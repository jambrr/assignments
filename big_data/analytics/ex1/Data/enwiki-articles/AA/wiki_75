<doc id="5236" url="https://en.wikipedia.org/wiki?curid=5236" title="Coast">
Coast

A coastline or a seashore is the area where land meets the sea or ocean, or a line that forms the boundary between the land and the ocean or a lake. A precise line that can be called a coastline cannot be determined due to the Coastline paradox.
The term "coastal zone" is a region where interaction of the sea and land processes occurs. Both the terms coast and coastal are often used to describe a geographic location or region; for example, New Zealand's West Coast, or the East and West Coasts of the United States. Edinburgh for example is a city on the coast of Scotland.
A pelagic coast refers to a coast which fronts the open ocean, as opposed to a more sheltered coast in a gulf or bay. A shore, on the other hand, can refer to parts of the land which adjoin any large body of water, including oceans (sea shore) and lakes (lake shore). Similarly, the somewhat related term "bed/bank" refers to the land alongside or sloping down to a river (riverbank) or to a body of water smaller than a lake. "Bank" is also used in some parts of the world to refer to an artificial ridge of earth intended to retain the water of a river or pond; in other places this may be called a levee.
While many scientific experts might agree on a common definition of the term "coast", the delineation of the extents of a coast differ according to jurisdiction, with many scientific and government authorities in various countries differing for economic and social policy reasons. According to the UN atlas, 44% of people live within of the sea.
Formation.
Tides often determine the range over which sediment is deposited or eroded. Areas with high tidal ranges allow waves to reach farther up the shore, and areas with lower tidal ranges produce deprossosition at a smaller elevation interval. The tidal range is influenced by the size and shape of the coastline. Tides do not typically cause erosion by themselves; however, tidal bores can erode as the waves surge up river estuaries from the ocean.
Waves erode coastline as they break on shore releasing their energy; the larger the wave the more energy it releases and the more sediment it moves. Coastlines with longer shores have more room for the waves to disperse their energy, while coasts with cliffs and short shore faces give little room for the wave energy to be dispersed. In these areas the wave energy breaking against the cliffs is higher, and air and water are compressed into cracks in the rock, forcing the rock apart, breaking it down. Sediment deposited by waves comes from eroded cliff faces and is moved along the coastline by the waves. This forms an abrasion or cliffed coast.
Sediment deposited by rivers is the dominant influence on the amount of sediment located on a coastline. Today riverine deposition at the coast is often blocked by dams and other human regulatory devices, which remove the sediment from the stream by causing it to be deposited inland.
Like the ocean which shapes them, coasts are a dynamic environment with constant change. The Earth's natural processes, particularly sea level rises, waves and various weather phenomena, have resulted in the erosion, accretion and reshaping of coasts as well as flooding and creation of continental shelves and drowned river valleys (rias).
Environmental importance.
The coast and its adjacent areas on and off shore are an important part of a local ecosystem: the mixture of fresh water and salt water (brackish water) in estuaries provides many nutrients for marine life. Salt marshes and beaches also support a diversity of plants, animals and insects crucial to the food chain.
The high level of biodiversity creates a high level of biological activity, which has attracted human activity for thousands of years.
Human impacts.
Human uses of coasts.
More and more of the world's people live in coastal regions. Many major cities are on or near good harbors and have port facilities. Some landlocked places have achieved port status by building canals.
The coast is a frontier that nations have typically defended against military invaders, smugglers and illegal migrants. Fixed coastal defenses have long been erected in many nations and coastal countries typically have a navy and some form of coast guard.
Coasts, especially those with beaches and warm water, attract tourists. In many island nations such as those of the Mediterranean, South Pacific and Caribbean, tourism is central to the economy. Coasts offer recreational activities such as swimming, fishing, surfing, boating, and sunbathing. Growth management can be a challenge for coastal local authorities who often struggle to provide the infrastructure required by new residents.
Threats to a coast.
Coasts also face many human-induced environmental impacts. The human influence on climate change is thought to contribute to an accelerated trend in sea level rise which threatens coastal habitats.
Pollution can occur from a number of sources: garbage and industrial debris; the transportation of petroleum in tankers, increasing the probability of large oil spills; small oil spills created by large and small vessels, which flush bilge water into the ocean.
Fishing has declined due to habitat degradation, overfishing, trawling, bycatch and climate change. Since the growth of global fishing enterprises after the 1950s, intensive fishing has spread from a few concentrated areas to encompass nearly all fisheries. The scraping of the ocean floor in bottom dragging is devastating to coral, sponges and other long-lived species that do not recover quickly. This destruction alters the functioning of the ecosystem and can permanently alter species composition and biodiversity. Bycatch, the capture of unintended species in the course of fishing, is typically returned to the ocean only to die from injuries or exposure. Bycatch represents about a quarter of all marine catch. In the case of shrimp capture, the bycatch is five times larger than the shrimp caught.
It is believed that melting Arctic ice will cause sea levels to rise and flood costal areas.
Conservation.
Extraordinary population growth in the 21st century has placed stress on the planet's ecosystems. For example, on Saint Lucia, harvesting mangrove for timber and clearing for fishing reduced the mangrove forests, resulting in a loss of habitat and spawning grounds for marine life that was unique to the area. These forests also helped to stabilize the coastline. Conservation efforts since the 1980s have partially restored the ecosystem.
Types of coast.
According to one principle of classification, an emergent coastline is a coastline which has experienced a fall in sea level, because of either a global sea level change, or local uplift. Emergent coastlines are identifiable by the coastal landforms, which are above the high tide mark, such as raised beaches. In contrast, a submergent coastline is one where the sea level has risen, due to a global sea level change, local subsidence, or isostatic rebound. Submergent coastlines are identifiable by their submerged, or "drowned" landforms, such as rias (drowned valleys) and fjords.
According to a second principle of classification, a concordant coastline is a coastline where bands of different rock types run parallel to the shore. These rock types are usually of varying resistance, so the coastline forms distinctive landforms, such as coves. Discordant coastlines feature distinctive landforms because the rocks are eroded by ocean waves. The less resistant rocks erode faster, creating inlets or bay; the more resistant rocks erode more slowly, remaining as headlands or outcroppings.
Other coastal categories:
Coastal landforms.
The following articles describe some coastal landforms
Coastal processes.
The following articles describe the various geologic processes that affect a coastal zone:
Wildlife.
Animals.
Some of the animals live along a typical coast. There are animals like puffins, sea turtles and rockhopper penguins. Sea snails and various kinds of barnacles live on the coast and scavenge on food deposited by the sea. Most coastal animals are used to humans in developed areas, such as dolphins and seagulls who eat food thrown for them by tourists. Since the coastal areas are all part of the littoral zone, there is a profusion of marine life found just off-coast.
There are many kinds of seabirds on the coast. Pelicans and cormorants join up with terns and oystercatchers to forage for fish and shellfish on the coast. There are sea lions on the coast of Wales and other countries.
Plants.
Coastal areas are famous for their kelp beds. Kelp is a fast-growing seaweed that grows up to a metre a day. Corals and sea anemones are true animals, but live a lifestyle similar to that of plants. Mangroves, seagrasses and salt marsh are important coastal vegetation types in tropical and temperate environments respectively.
Coastline statistics.
Coastline problem.
Shortly before 1951, Lewis Fry Richardson, in researching the possible effect of border lengths on the probability of war, noticed that the Portuguese reported their measured border with Spain to be 987 km, but the Spanish reported it as 1214 km. This was the beginning of the coastline problem, which is a mathematical uncertainty inherent in the measurement of boundaries that are irregular.
The prevailing method of estimating the length of a border (or coastline) was to lay out "n" equal straight-line segments of length "ℓ" with dividers on a map or aerial photograph. Each end of the segment must be on the boundary. Investigating the discrepancies in border estimation, Richardson discovered what is now termed the Richardson Effect: the sum of the segments is inversely proportional to the common length of the segments. In effect, the shorter the ruler, the longer the measured border; the Spanish and Portuguese geographers were simply using different-length rulers.
The result most astounding to Richardson is that, under certain circumstances, as "ℓ" approaches zero, the length of the coastline approaches infinity. Richardson had believed, based on Euclidean geometry, that a coastline would approach a fixed length, as do similar estimations of regular geometric figures. For example, the perimeter of a regular polygon inscribed in a circle approaches the circumference with increasing numbers of sides (and decrease in the length of one side). In geometric measure theory such a smooth curve as the circle that can be approximated by small straight segments with a definite limit is termed a rectifiable curve.
Measuring a coastline.
More than a decade after Richardson completed his work, Benoit Mandelbrot developed a new branch of mathematics, fractal geometry, to describe just such non-rectifiable complexes in nature as the infinite coastline. His own definition of the new figure serving as the basis for his study is:
A key property of the fractal is self-similarity; that is, at any scale the same general configuration appears. A coastline is perceived as bays alternating with promontories. In the hypothetical situation that a given coastline has this property of self-similarity, then no matter how greatly any one small section of coastline is magnified, a similar pattern of smaller bays and promontories superimposed on larger bays and promontories appears, right down to the grains of sand. At that scale the coastline appears as a momentarily shifting, potentially infinitely long thread with a stochastic arrangement of bays and promontories formed from the small objects at hand. In such an environment (as opposed to smooth curves) Mandelbrot asserts "coastline length turns out to be an elusive notion that slips between the fingers of those who want to grasp it."
There are different kinds of fractals. A coastline with the stated property is in "a first category of fractals, namely curves whose fractal dimension is greater than 1." That last statement represents an extension by Mandelbrot of Richardson's thought. Mandelbrot's statement of the Richardson Effect is:
where L, coastline length, a function of the measurement unit, ε, is approximated by the expression. F is a constant and D is a parameter that Richardson found depended on the coastline approximated by L. He gave no theoretical explanation but Mandelbrot identified D with a non-integer form of the Hausdorff dimension, later the fractal dimension. Rearranging the right side of the expression obtains:
where Fε-D must be the number of units ε required to obtain L. The fractal dimension is the number of the dimensions of the figure being used to approximate the fractal: 0 for a dot, 1 for a line, 2 for a square. D in the expression is between 1 and 2, for coastlines typically less than 1.5. The broken line measuring the coast does not extend in one direction nor does it represent an area, but is intermediate. It can be interpreted as a thick line or band of width 2ε. More broken coastlines have greater D and therefore L is longer for the same ε. Mandelbrot showed that D is independent of ε.

</doc>
<doc id="5237" url="https://en.wikipedia.org/wiki?curid=5237" title="Catatonia">
Catatonia

Catatonia is a state of neurogenic motor immobility and behavioral abnormality manifested by stupor. It was first described in 1874 by Karl Ludwig Kahlbaum, in "Die Katatonie oder das Spannungsirresein" ("Catatonia or Tension Insanity").
In the "Diagnostic and Statistical Manual of Mental Disorders" 5th edition catatonia is not recognized as a separate disorder, but is associated with psychiatric conditions such as schizophrenia (catatonic type), bipolar disorder, post-traumatic stress disorder, depression and other mental disorders, narcolepsy, as well as drug abuse or overdose (or both). It may also be seen in many medical disorders including infections (such as encephalitis), autoimmune disorders, focal neurologic lesions (including strokes), metabolic disturbances, alcohol withdrawal and abrupt or overly rapid benzodiazepine withdrawal.
It can be an adverse reaction to prescribed medication. It bears similarity to conditions such as encephalitis lethargica and neuroleptic malignant syndrome. There are a variety of treatments available; benzodiazepines are a first-line treatment strategy. Electro-convulsive therapy is also sometimes used. There is growing evidence for the effectiveness of NMDA antagonists for benzodiazepine resistant catatonia. Antipsychotics are sometimes employed but require caution as they can worsen symptoms and have serious adverse effects.
Features.
Patients with catatonia may experience an extreme loss of motor skill or even constant hyperactive motor activity. Catatonic patients will sometimes hold rigid poses for hours and will ignore any external stimuli. Patients with catatonic excitement can suffer from exhaustion if not treated. Patients may also show stereotyped, repetitive movements.
They may show specific types of movement such as waxy flexibility, in which they maintain positions after being placed in them through someone else in which they resist movement in proportion to the force applied by the examiner. They may repeat meaningless phrases or speak only to repeat what the examiner says.
While catatonia is only identified as a symptom of schizophrenia in present psychiatric classifications, it is increasingly recognized as a syndrome with many faces. It appears as the Kahlbaum syndrome (motionless catatonia), malignant catatonia (neuroleptic malignant syndrome, toxic serotonin syndrome), and excited forms (delirious mania, catatonic excitement, oneirophrenia). 
It has also been recognized as grafted on to autism spectrum disorders.
Diagnostic criteria.
According to the DSM-V, "Catatonia Associated with Another Mental Disorder (Catatonia Specifier)" is diagnosed if the clinical picture is dominated by at least three of the following:
Rating scale.
Fink and Taylor developed a catatonia rating scale to identify the syndrome. A diagnosis is verified by a benzodiazepine or barbiturate test. The diagnosis is validated by the quick response to either benzodiazepines or electroconvulsive therapy (ECT). While proven useful in the past, barbiturates are no longer commonly used in psychiatry; thus the option of either benzodiazepines or ECT.
Treatment.
Initial treatment is aimed at providing symptomatic relief. Benzodiazepines are the first line of treatment, and high doses are often required. A test dose of 1–2 mg of intramuscular lorazepam will often result in marked improvement within half an hour. In France, zolpidem has also been used in diagnosis, and response may occur within the same time period. Ultimately the underlying cause needs to be treated.
Electroconvulsive therapy (ECT) is an effective treatment for catatonia. Antipsychotics should be used with care as they can worsen catatonia and are the cause of neuroleptic malignant syndrome, a dangerous condition that can mimic catatonia and requires immediate discontinuation of the antipsychotic.
Excessive glutamate activity is believed to be involved in catatonia; when first-line treatment options fail, NMDA antagonists such as amantadine or memantine are used. Amantadine may have an increased incidence of tolerance with prolonged use and can cause psychosis, due to its additional effects on the dopamine system. Memantine has a more targeted pharmacological profile for the glutamate system, reduced incidence of psychosis and may therefore be preferred for individuals who cannot tolerate amantadine. Topiramate is another treatment option for resistant catatonia; it produces its therapeutic effects by producing glutamate antagonism via modulation of AMPA receptors.

</doc>
<doc id="5244" url="https://en.wikipedia.org/wiki?curid=5244" title="Cipher">
Cipher

In cryptography, a cipher (or cypher) is an algorithm for performing encryption or decryption—a series of well-defined steps that can be followed as a procedure. An alternative, less common term is "encipherment". To encipher or encode is to convert information into cipher or code. In common parlance, 'cipher' is synonymous with 'code', as they are both a set of steps that encrypt a message; however, the concepts are distinct in cryptography, especially classical cryptography.
Codes generally substitute different length strings of characters in the output, while ciphers generally substitute the same number of characters as are input. There are exceptions and some cipher systems may use slightly more, or fewer, characters when output versus the number that were input.
Codes operated by substituting according to a large codebook which linked a random string of characters or numbers to a word or phrase. For example, "UQJHSE" could be the code for "Proceed to the following coordinates." When using a cipher the original information is known as plaintext, and the encrypted form as ciphertext. The ciphertext message contains all the information of the plaintext message, but is not in a format readable by a human or computer without the proper mechanism to decrypt it.
The operation of a cipher usually depends on a piece of auxiliary information, called a key (or, in traditional NSA parlance, a "cryptovariable"). The encrypting procedure is varied depending on the key, which changes the detailed operation of the algorithm. A key must be selected before using a cipher to encrypt a message. Without knowledge of the key, it should be extremely difficult, if not impossible, to decrypt the resulting ciphertext into readable plaintext.
Most modern ciphers can be categorized in several ways
Etymology.
"Cipher" is alternatively spelled "cypher"; similarly "ciphertext" and "cyphertext", and so forth.
The word "cipher" in former times meant "zero" and had the same origin: Middle French as ' and Medieval Latin as "cifra," from the Arabic صفر'" "ṣifr" = zero (see Zero—Etymology). "Cipher" was later used for any decimal digit, even any number. There are many theories about how the word "cipher" may have come to mean "encoding".
Ibrahim Al-Kadi concluded that the Arabic word "sifr", for the digit zero, developed into the European technical term for encryption.
As the decimal zero and its new mathematics spread from the Arabic world to Europe in the Middle Ages, words derived from "ṣifr" and "zephyrus" came to refer to calculation, as well as to privileged knowledge and secret codes. According to Ifrah, "in thirteenth-century Paris, a 'worthless fellow' was called a , i.e., an 'arithmetical nothing'." Cipher was the European pronunciation of sifr, and cipher came to mean a message or communication not easily understood.
Versus codes.
In non-technical usage, a "(secret) code" typically means a "cipher". Within technical discussions, however, the words "code" and "cipher" refer to two different concepts. Codes work at the level of meaning—that is, words or phrases are converted into something else and this chunking generally shortens the message.
An example of this is the Telegraph Code which was used to shorten long telegraph messages which resulted from entering into commercial contracts using exchanges of Telegrams.
Another example is given by whole words cipher s, which allow the user to replace an entire word with a symbol or character, much like the way Japanese utilize Kanji (Chinese) characters to supplement their language. ex "The quick brown fox jumps over the lazy dog" becomes "The quick brown 狐 jumps 过 the lazy 狗".
Ciphers, on the other hand, work at a lower level: the level of individual letters, small groups of letters, or, in modern schemes, individual bits and blocks of bits. Some systems used both codes and ciphers in one system, using superencipherment to increase the security. In some cases the terms codes and ciphers are also used synonymously to substitution and transposition.
Historically, cryptography was split into a dichotomy of codes and ciphers; and coding had its own terminology, analogous to that for ciphers: ""encoding", "codetext", "decoding"" and so on.
However, codes have a variety of drawbacks, including susceptibility to cryptanalysis and the difficulty of managing a cumbersome codebook. Because of this, codes have fallen into disuse in modern cryptography, and ciphers are the dominant technique.
Types.
There are a variety of different types of encryption. Algorithms used earlier in the history of cryptography are substantially different from modern methods, and modern ciphers can be classified according to how they operate and whether they use one or two keys.
Historical.
Historical pen and paper ciphers used in the past are sometimes known as classical ciphers. They include simple substitution ciphers (such as Rot 13) and transposition ciphers (such as a Rail Fence Cipher). For example, "GOOD DOG" can be encrypted as "PLLX XLP" where "L" substitutes for "O", "P" for "G", and "X" for "D" in the message. Transposition of the letters "GOOD DOG" can result in "DGOGDOO". These simple ciphers and examples are easy to crack, even without plaintext-ciphertext pairs.
Simple ciphers were replaced by polyalphabetic substitution ciphers (such as the Vigenère) which changed the substitution alphabet for every letter. For example, "GOOD DOG" can be encrypted as "PLSX TWF" where "L", "S", and "W" substitute for "O". With even a small amount of known or estimated plaintext, simple polyalphabetic substitution ciphers and letter transposition ciphers designed for pen and paper encryption are easy to crack. It is possible to create a secure pen and paper cipher based on a one-time pad though, but the usual disadvantages of one-time pads apply.
During the early twentieth century, electro-mechanical machines were invented to do encryption and decryption using transposition, polyalphabetic substitution, and a kind of "additive" substitution. In rotor machines, several rotor disks provided polyalphabetic substitution, while plug boards provided another substitution. Keys were easily changed by changing the rotor disks and the plugboard wires. Although these encryption methods were more complex than previous schemes and required machines to encrypt and decrypt, other machines such as the British Bombe were invented to crack these encryption methods.
Modern.
Modern encryption methods can be divided by two criteria: by type of key used, and by type of input data.
By type of key used ciphers are divided into:
In a symmetric key algorithm (e.g., DES and AES), the sender and receiver must have a shared key set up in advance and kept secret from all other parties; the sender uses this key for encryption, and the receiver uses the same key for decryption. The Feistel cipher uses a combination of substitution and transposition techniques. Most block cipher algorithms are based on this structure. In an asymmetric key algorithm (e.g., RSA), there are two separate keys: a "public key" is published and enables any sender to perform encryption, while a "private key" is kept secret by the receiver and enables only him to perform correct decryption.
Ciphers can be distinguished into two types by the type of input data:
Key size and vulnerability.
In a pure mathematical attack, (i.e., lacking any other information to help break a cipher) two factors above all count:
Since the desired effect is computational difficulty, in theory one would choose an algorithm and desired difficulty level, thus decide the key length accordingly.
An example of this process can be found at Key Length which uses multiple reports to suggest that a symmetric cipher with 128 bits, an asymmetric cipher with 3072 bit keys, and an elliptic curve cipher with 512 bits, all have similar difficulty at present.
Claude Shannon proved, using information theory considerations, that any theoretically unbreakable cipher must have keys which are at least as long as the plaintext, and used only once: one-time pad.

</doc>
<doc id="5247" url="https://en.wikipedia.org/wiki?curid=5247" title="Country music">
Country music

Country music is a genre of American popular music that originated in the Southern United States in the 1920s. It takes its roots from the southeastern genre of American folk music and Western music. Blues modes have been used extensively throughout its recorded history. Country music often consists of ballads and dance tunes with generally simple forms and harmonies accompanied by mostly string instruments such as banjos, electric and acoustic guitars, dobros and fiddles as well as harmonicas. The term "country music" gained popularity in the 1940s in preference to the earlier term "hillbilly music"; it came to encompass Western music, which evolved parallel to hillbilly music from similar roots, in the mid-20th century. The term "country music" is used today to describe many styles and subgenres. The origins of country music are the folk music of mostly white, working-class Americans, who blended popular songs, Irish and Celtic fiddle tunes, traditional ballads, and cowboy songs, and various musical traditions from European immigrant communities. In 2009 country music was the most listened to rush hour radio genre during the evening commute, and second most popular in the morning commute in the United States.
Generations of country music.
Immigrants to the Southern Appalachian Mountains of North America brought the music and instruments of Europe along with them for nearly 300 years. Country music was "introduced to the world as a Southern phenomenon." The first generation emerged in the early 1920s, with Atlanta's music scene playing a major role in launching country's earliest recording artists. Okeh Records began issuing hillbilly music records by Fiddlin' John Carson as early as 1923, followed by Columbia Records (series 15000D "Old Familiar Tunes") (Samantha Bumgarner) in 1924, and RCA Victor Records in 1927 (the Carter Family and Jimmie Rodgers). Many "hillbilly" musicians, such as Cliff Carlisle, recorded blues songs throughout the 1920s.
During the second generation (1930s–1940s), radio became a popular source of entertainment, and "barn dance" shows featuring country music were started all over the South, as far north as Chicago, and as far west as California. The most important was the "Grand Ole Opry", aired starting in 1925 by WSM in Nashville and continuing to the present day. During the 1930s and 1940s, cowboy songs, or Western music, which had been recorded since the 1920s, were popularized by films made in Hollywood. Bob Wills was another country musician from the Lower Great Plains who had become very popular as the leader of a "hot string band," and who also appeared in Hollywood westerns. His mix of country and jazz, which started out as dance hall music, would become known as Western swing. Wills was one of the first country musicians known to have added an electric guitar to his band, in 1938. Country musicians began recording boogie in 1939, shortly after it had been played at Carnegie Hall, when Johnny Barfield recorded "Boogie Woogie".
The third generation (1950s–1960s) started at the end of World War II with "mountaineer" string band music known as bluegrass, which emerged when Bill Monroe, along with Lester Flatt and Earl Scruggs were introduced by Roy Acuff at the Grand Ole Opry. Gospel music remained a popular component of country music. Another type of stripped-down and raw music with a variety of moods and a basic ensemble of guitar, bass, dobro or steel guitar (and later) drums became popular, especially among poor whites in Texas and Oklahoma. It became known as honky tonk, and had its roots in Western swing and the ranchera music of Mexico and the border states. By the early 1950s a blend of Western swing, country boogie, and honky tonk was played by most country bands. Rockabilly was most popular with country fans in the 1950s, and 1956 could be called the year of rockabilly in country music. Beginning in the mid-1950s, and reaching its peak during the early 1960s, the Nashville Sound turned country music into a multimillion-dollar industry centered in Nashville, Tennessee. The late 1960s in American music produced a unique blend as a result of traditionalist backlash within separate genres. In the aftermath of the British Invasion, many desired a return to the "old values" of rock n' roll. At the same time there was a lack of enthusiasm in the country sector for Nashville-produced music. What resulted was a crossbred genre known as country rock.
Fourth generation (1970s–1980s) music included outlaw country with roots in the Bakersfield sound, and country pop with roots in the countrypolitan, folk music and soft rock. Between 1972 and 1975 singer/guitarist John Denver released a series of hugely successful songs blending country and folk-rock musical styles. During the early 1980s country artists continued to see their records perform well on the pop charts. In 1980 a style of "neocountry disco music" was popularized. During the mid-1980s a group of new artists began to emerge who rejected the more polished country-pop sound that had been prominent on radio and the charts in favor of more traditional "back-to-basics" production.
During the fifth generation (1990s), country music became a worldwide phenomenon thanks to Garth Brooks and Alan Jackson. The Dixie Chicks became one of the most popular country bands in the 1990s and early 2000s.
The sixth generation (2000s–present) is exemplified by country singer Carrie Underwood. The influence of rock music in country has become more overt during the late 2000s and early 2010s. Attempts to combine punk and country were pioneered by Jason and the Scorchers, and in the 1980s Southern Californian cowpunk scene with bands like the Long Ryders. Hip-hop also made its mark on country music with the emergence of country rap. Most of the best-selling country songs of this era however were in the country pop genre, such as those by Lady Antebellum, Florida Georgia Line, and Taylor Swift.
First generation (1920s).
Atlanta's music scene played a major role in launching country's earliest recording artists in the early 1920s—many Appalachian people had come to the city to work in its cotton mills and brought their music with them. It would remain a major recording center for two decades and a major performance center for four decades, up to the first country music TV shows on local Atlanta stations in the 1950s.
Some record companies in Atlanta turned away early artists such as Fiddlin' John Carson, while others realized that his music would fit perfectly with the lifestyle of the country's agricultural workers. The first commercial recordings of what was considered country music were "Arkansas Traveler" and "Turkey in the Straw" by fiddlers Henry Gilliland & A.C. (Eck) Robertson on June 30, 1922, for Victor Records and released in April 1923. Columbia Records began issuing records with "hillbilly" music (series 15000D "Old Familiar Tunes") as early as 1924.
A year later, on June 14, 1923, Fiddlin' John Carson recorded "Little Log Cabin in the Lane" for Okeh Records. Vernon Dalhart was the first country singer to have a nationwide hit in May 1924 with "Wreck of the Old 97". The flip side of the record was "Lonesome Road Blues", which also became very popular. In April 1924, "Aunt" Samantha Bumgarner and Eva Davis became the first female musicians to record and release country songs.
Many "hillbilly" musicians, such as Cliff Carlisle, recorded blues songs throughout the decade and into the 1930s. Other important early recording artists were Riley Puckett, Don Richardson, Fiddlin' John Carson, Uncle Dave Macon, Al Hopkins, Ernest V. Stoneman, Charlie Poole and the North Carolina Ramblers and The Skillet Lickers. The steel guitar entered country music as early as 1922, when Jimmie Tarlton met famed Hawaiian guitarist Frank Ferera on the West Coast.
Jimmie Rodgers and the Carter Family are widely considered to be important early country musicians. Their songs were first captured at a historic recording session in Bristol, Tennessee, on August 1, 1927, where Ralph Peer was the talent scout and sound recordist. A scene in the movie "O Brother, Where Art Thou?" depicts a similar occurrence in the same timeframe.
Rodgers fused hillbilly country, gospel, jazz, blues, pop, cowboy, and folk, and many of his best songs were his compositions, including "Blue Yodel", which sold over a million records and established Rodgers as the premier singer of early country music.
Beginning in 1927, and for the next 17 years, the Carters recorded some 300 old-time ballads, traditional tunes, country songs and gospel hymns, all representative of America's southeastern folklore and heritage.
Second generation (1930s–1940s).
One effect of the Great Depression was to reduce the number of records that could be sold. Radio became a popular source of entertainment, and "barn dance" shows featuring country music were started all over the South, as far north as Chicago, and as far west as California.
The most important was the "Grand Ole Opry", aired starting in 1925 by WSM in Nashville and continuing to the present day. Some of the early stars on the "Opry" were Uncle Dave Macon, Roy Acuff and African American harmonica player DeFord Bailey. WSM's 50,000-watt signal (in 1934) could often be heard across the country.
Many musicians performed and recorded songs in any number of styles. Moon Mullican, for example, played Western swing but also recorded songs that can be called rockabilly. Between 1947 and 1949, country crooner Eddy Arnold placed eight songs in the top 10.
From 1945 to 1955 Jenny Lou Carson was one of the most prolific songwriters in country music.
Singing cowboys and Western swing.
During the 1930s and 1940s, cowboy songs, or Western music, which had been recorded since the 1920s, were popularized by films made in Hollywood. Some of the popular singing cowboys from the era were Gene Autry, the Sons of the Pioneers, and Roy Rogers. Country music and western music were frequently played together on the same radio stations, hence the term "country and western" music.
And it wasn't only cowboys; cowgirls contributed to the sound in various family groups. Patsy Montana opened the door for female artists with her history-making song "I Want To Be a Cowboy's Sweetheart". This would begin a movement toward opportunities for women to have successful solo careers.
Bob Wills was another country musician from the Lower Great Plains who had become very popular as the leader of a "hot string band," and who also appeared in Hollywood westerns. His mix of country and jazz, which started out as dance hall music, would become known as Western swing. Spade Cooley and Tex Williams also had very popular bands and appeared in films. At its height, Western swing rivaled the popularity of big band swing music.
Changing instrumentation.
Drums were scorned by early country musicians as being "too loud" and "not pure", but by 1935 Western swing big band leader Bob Wills had added drums to the Texas Playboys. In the mid-1940s, the Grand Ole Opry did not want the Playboys' drummer to appear on stage. Although drums were commonly used by rockabilly groups by 1955, the less-conservative-than-the-Grand Ole Opry "Louisiana Hayride" kept its infrequently used drummer back stage as late as 1956. By the early 1960s, however, it was rare that a country band didn't have a drummer.
Bob Wills was one of the first country musicians known to have added an electric guitar to his band, in 1938. A decade later (1948) Arthur Smith achieved top 10 US country chart success with his MGM Records recording of "Guitar Boogie", which crossed over to the US pop chart, introducing many people to the potential of the electric guitar. For several decades Nashville session players preferred the warm tones of the Gibson and Gretsch archtop electrics, but a "hot" Fender style, using guitars which became available beginning in the early 1950s, eventually prevailed as the signature guitar sound of country.
Hillbilly boogie.
Country musicians began recording boogie in 1939, shortly after it had been played at Carnegie Hall, when Johnny Barfield recorded "Boogie Woogie". The trickle of what was initially called hillbilly boogie, or okie boogie (later to be renamed country boogie), became a flood beginning in late 1945. One notable release from this period was The Delmore Brothers' "Freight Train Boogie", considered to be part of the combined evolution of country music and blues towards rockabilly. In 1948, Arthur "Guitar Boogie" Smith achieved top ten US country chart success with his MGM Records recordings of "Guitar Boogie" and "Banjo Boogie", with the former crossing over to the US pop charts. Other country boogie artists included Merrill Moore and Tennessee Ernie Ford. The hillbilly boogie period lasted into the 1950s and remains one of many subgenres of country into the 21st century.
Bluegrass, folk and gospel.
By the end of World War II, "mountaineer" string band music known as bluegrass had emerged when Bill Monroe joined with Lester Flatt and Earl Scruggs, introduced by Roy Acuff at the Grand Ole Opry. Gospel music, too, remained a popular component of country music. Red Foley, the biggest country star following World War II, had one of the first million-selling gospel hits ("Peace in the Valley") and also sang boogie, blues and rockabilly.
In the post-war period, country music was called "folk" in the trades, and "hillbilly" within the industry. In 1944, "The Billboard" replaced the term "hillbilly" with "folk songs and blues," and switched to "country" or "country and Western" in 1949.
Honky Tonk.
Another type of stripped down and raw music with a variety of moods and a basic ensemble of guitar, bass, dobro or steel guitar (and later) drums became popular, especially among poor whites in Texas and Oklahoma. It became known as honky tonk and had its roots in Western swing and the ranchera music of Mexico and the border states, particularly Texas, together with the blues of the American South. Bob Wills and His Texas Playboys personified this music which has been described as "a little bit of this, and a little bit of that, a little bit of black and a little bit of white ... just loud enough to keep you from thinking too much and to go right on ordering the whiskey." East Texan Al Dexter had a hit with "Honky Tonk Blues", and seven years later "Pistol Packin' Mama". These "honky tonk" songs associated barrooms, were performed by the likes of Ernest Tubb, Kitty Wells (the first major female country solo singer), Ted Daffan, Floyd Tillman, and the Maddox Brothers and Rose, Lefty Frizzell and Hank Williams, would later be called "traditional" country. Williams' influence in particular would prove to be enormous, inspiring many of the pioneers of rock and roll, such as Elvis Presley and Jerry Lee Lewis, as well as Chuck Berry and Ike Turner, while providing a framework for emerging honky tonk talents like George Jones. Webb Pierce was the top-charting country artist of the 1950s, with 13 of his singles spending 113 weeks at number one. He charted 48 singles during the decade; 31 reached the top ten and 26 reached the top four.
Third generation (1950s–1960s).
By the early 1950s a blend of Western swing, country boogie, and honky tonk was played by most country bands. Western music, influenced by the cowboy ballads and Tejano music rhythms of the southwestern U.S. and northern Mexico, reached its peak in popularity in the late 1950s, most notably with the song "El Paso", first recorded by Marty Robbins in September 1959.
The country music scene largely kept the music of the folk revival and folk rock at a distance, despite the similarity in instrumentation and origins (see, for instance, The Byrds' negative reception during their appearance on the "Grand Ole Opry"). The main concern was politics: the folk revival was largely driven by progressive activists, a stark contrast to the culturally conservative audiences of country music. Only a handful of folk artists, such as Burl Ives, John Denver and Canadian musician Gordon Lightfoot, would cross over into country music after the folk revival died out.
During the mid-1950s a new style of country music became popular, eventually to be referred to as rockabilly.
Rockabilly.
Rockabilly was most popular with country fans in the 1950s, and 1956 could be called the year of rockabilly in country music. Rockabilly was a mixture of rock-and-roll and hillbilly music. During this period Elvis Presley converted over to country music. He played a huge role in the music industry during this time. The number two, three and four songs on "Billboard's" charts for that year were Elvis Presley, "Heartbreak Hotel"; Johnny Cash, "I Walk the Line"; and Carl Perkins, "Blue Suede Shoes".
Cash and Presley placed songs in the top 5 in 1958 with No. 3 "Guess Things Happen That Way/Come In, Stranger" by Cash, and No. 5 by Presley "Don't/I Beg of You." Presley acknowledged the influence of rhythm and blues artists and his style, saying "The colored folk been singin' and playin' it just the way I'm doin' it now, man for more years than I know." But he also said, "My stuff is just hopped-up country." Within a few years, many rockabilly musicians returned to a more mainstream style or had defined their own unique style.
Country music gained national television exposure through "Ozark Jubilee" on ABC-TV and radio from 1955 to 1960 from Springfield, Missouri. The program showcased top stars including several rockabilly artists, some from the Ozarks. As Webb Pierce put it in 1956, "Once upon a time, it was almost impossible to sell country music in a place like New York City. Nowadays, television takes us everywhere, and country music records and sheet music sell as well in large cities as anywhere else."
The late 1950s saw the emergence of Buddy Holly, but by the end of the decade, backlash as well as traditional artists such as Ray Price, Marty Robbins, and Johnny Horton began to shift the industry away from the rock n' roll influences of the mid-1950s.
The Nashville and countrypolitan sounds.
Beginning in the mid-1950s, and reaching its peak during the early 1960s, the Nashville sound turned country music into a multimillion-dollar industry centered in Nashville, Tennessee. Under the direction of producers such as Chet Atkins, Paul Cohen, Owen Bradley, Bob Ferguson, and later Billy Sherrill, the sound brought country music to a diverse audience and helped revive country as it emerged from a commercially fallow period.
This subgenre was notable for borrowing from 1950s pop stylings: a prominent and smooth vocal, backed by a string section and vocal chorus. Instrumental soloing was de-emphasized in favor of trademark "licks". Leading artists in this genre included Jim Reeves, Skeeter Davis, The Browns, Patsy Cline, and Eddy Arnold. The "slip note" piano style of session musician Floyd Cramer was an important component of this style.
Nashville's pop song structure became more pronounced and it morphed into what was called countrypolitan. Countrypolitan was aimed straight at mainstream markets, and it sold well throughout the later 1960s into the early 1970s (a rarity in an era where American popular music was being decimated by the British Invasion). Top artists included Tammy Wynette, Lynn Anderson and Charlie Rich, as well as such former "hard country" artists as Ray Price and Marty Robbins.
Despite the appeal of the Nashville sound, many traditional country artists emerged during this period and dominated the genre: Loretta Lynn, Merle Haggard, Buck Owens, Porter Wagoner, and Sonny James among them.
Country soul - crossover.
In 1962, Ray Charles surprised the pop world by turning his attention to country and western music, topping the charts and rating number three for the year on "Billboard's" pop chart with the "I Can't Stop Loving You" single, and recording the landmark album "Modern Sounds in Country and Western Music".
The Bakersfield sound.
Another genre of country music grew out of hardcore honky tonk with elements of Western swing and originated north-northwest of Los Angeles in Bakersfield, California. Influenced by one-time West Coast residents Bob Wills and Lefty Frizzell, by 1966 it was known as the Bakersfield sound. It relied on electric instruments and amplification, in particular the Telecaster electric guitar, more than other subgenres of country of the era, and can be described as having a sharp, hard, driving, no-frills, edgy flavor. Leading practitioners of this style were Buck Owens, Merle Haggard, Tommy Collins, Gary Allan, and Wynn Stewart, each of whom had his own style.
Decline of Western music and the cowboy ballad.
By the late 1960s, Western music, in particular the cowboy ballad, was in decline. Relegated to the "country and Western" genre by marketing agencies, popular Western recording stars released albums to only moderate success. Rock-and-roll dominated music sales, and Hollywood recording studios dropped most of their Western artists. The shift in country music production to Nashville also played a role, where the Nashville sound, country rock, and rockabilly music styles predominated over both "cowboy" artists and the more recent Bakersfield sound. The latter was largely limited to Buck Owens, Merle Haggard, and a few other bands. In the process, country and western music as a genre lost most of its southwestern, ranchera, and Tejano musical influences. However the cowboy ballad and honky-tonk music would be resurrected and reinterpreted in the 1970s with the growth in popularity of "outlaw country" music from Texas and Oklahoma.
Fourth generation (1970s–1980s).
Outlaw country.
Derived from the traditional Western and honky tonk musical styles of the late 1950s and 1960s, including Ray Price (whose band, the "Cherokee Cowboys", included Willie Nelson and Roger Miller) and mixed with the anger of an alienated subculture of the nation during the period, outlaw country revolutionized the genre of country music.
"After I left Nashville (the early 70s), I wanted to relax and play the music that I wanted to play, and just stay around Texas, maybe Oklahoma. Waylon and I had that outlaw image going, and when it caught on at colleges and we started selling records, we were O.K. The whole outlaw thing, it had nothing to do with the music, it was something that got written in an article, and the young people said, 'Well, that's pretty cool.' And started listening." (Willie Nelson)
The term "outlaw country" is traditionally associated with Hank Williams, Jr., Willie Nelson, Waylon Jennings, David Allan Coe, Whitey Morgan and the 78's, John Prine, Billy Joe Shaver, Gary Stewart, Townes Van Zandt, Kris Kristofferson, Michael Martin Murphey, and the later career renaissance of Johnny Cash, with a few female vocalists such as Jessi Colter and Sammi Smith. It was encapsulated in the 1976 album "Wanted! The Outlaws". A related subgenre is Red Dirt.
Country pop.
Country pop or soft pop, with roots in the countrypolitan sound, folk music, and soft rock, is a subgenre that first emerged in the 1970s. Although the term first referred to country music songs and artists that crossed over to top 40 radio, country pop acts are now more likely to cross over to adult contemporary music. It started with pop music singers like Glen Campbell, Bobbie Gentry, John Denver, Olivia Newton-John, Anne Murray, Marie Osmond, B. J. Thomas, The Bellamy Brothers, and Linda Ronstadt having hits on the country charts.
Between 1972 and 1975, singer/guitarist John Denver released a series of hugely successful songs blending country and folk-rock musical styles ("Rocky Mountain High", "Sunshine on My Shoulders", "Annie's Song", "Thank God I'm a Country Boy", and "I'm Sorry"), and was named Country Music Entertainer of the Year in 1975. The year before, Olivia Newton-John, an Australian pop singer, won the "Best Female Country Vocal Performance" as well as the Country Music Association's most coveted award for females, "Female Vocalist of the Year". In response George Jones, Tammy Wynette, and other traditional Nashville country artists dissatisfied with the new trend formed the short-lived Association of Country Entertainers in 1974.
During the mid-1970s, Dolly Parton, a successful mainstream country artist since the late 1960s, mounted a high profile campaign to cross over to pop music, culminating in her 1977 hit "Here You Come Again", which topped the U.S. country singles chart, and also reached No. 3 on the pop singles charts. Parton's male counterpart, Kenny Rogers, came from the opposite direction, aiming his music at the country charts, after a successful career in pop, rock and folk music, achieving success the same year with "Lucille", which topped the country charts and reached No. 5 on the U.S. pop singles charts, as well as reaching Number 1 on the British all-genre chart. Parton and Rogers would both continue to have success on both country and pop charts simultaneously, well into the 1980s. Artists like Crystal Gayle, Ronnie Milsap and Barbara Mandrell would also find success on the pop charts with their records.
In 1975, author Paul Hemphill stated in the "Saturday Evening Post", "Country music isn't really country anymore; it is a hybrid of nearly every form of popular music in America."
During the early 1980s, country artists continued to see their records perform well on the pop charts. Willie Nelson and Juice Newton each had two songs in the top 5 of the Billboard Hot 100 in the early eighties: Nelson charted "Always on My Mind" (No. 5, 1982) and "To All the Girls I've Loved Before" (No. 5, 1984, a duet with Julio Iglesias), and Newton achieved success with "Queen of Hearts" (No. 2, 1981) and "Angel of the Morning" (No. 4, 1981). Four country songs topped the "Billboard" Hot 100 in the 1980s: "Lady" by Kenny Rogers, from the late fall of 1980; "9 to 5" by Dolly Parton, "I Love a Rainy Night" by Eddie Rabbitt (these two back-to-back at the top in early 1981); and "Islands in the Stream", a duet by Dolly Parton and Kenny Rogers in 1983, a pop-country crossover hit written by Barry, Robin, and Maurice Gibb of the Bee Gees. Newton's "Queen of Hearts" almost reached No. 1, but was kept out of the spot by the pop ballad juggernaut "Endless Love" by Diana Ross and Lionel Richie. The move of country music toward neotraditional styles led to a marked decline in country/pop crossovers in the late 1980s, and only one song in that period—Roy Orbison's "You Got It", from 1989—made the top 10 of both the "Billboard" Hot Country Singles" and Hot 100 charts, due largely to a revival of interest in Orbison after his sudden death.
The record-setting, multi-platinum group Alabama was named Artist of the Decade for the 1980s by the Academy of Country Music.
Country rock.
Country rock is a genre that started in the 1960s but became prominent in the 1970s. The late 1960s in American music produced a unique blend as a result of traditionalist backlash within separate genres. In the aftermath of the British Invasion, many desired a return to the "old values" of rock n' roll. At the same time there was a lack of enthusiasm in the country sector for Nashville-produced music. What resulted was a crossbred genre known as country rock.
Early innovators in this new style of music in the 1960s and 1970s included Bob Dylan, who was the first to revert to country music with his 1967 album "John Wesley Harding" (and even more so with that album's follow-up, "Nashville Skyline"), followed by Gene Clark, Clark's former band The Byrds (with Gram Parsons on "Sweetheart of the Rodeo") and its spin-off The Flying Burrito Brothers (also featuring Gram Parsons), guitarist Clarence White, Michael Nesmith (The Monkees and the First National Band), the Grateful Dead, Neil Young, Commander Cody, The Allman Brothers, The Marshall Tucker Band, Poco, Buffalo Springfield, and Eagles, among many, even the former folk music duo Ian & Sylvia, who formed Great Speckled Bird in 1969. The Eagles would become the most successful of these country rock acts, and their compilation album "Their Greatest Hits (1971–1975)" remains the second best-selling album of all time in the US with 29 million copies sold. The Rolling Stones also got into the act with songs like "Dead Flowers" and a country version of "Honky Tonk Women".
Described by AllMusic as the "father of country-rock", Gram Parsons' work in the early 1970s was acclaimed for its purity and for his appreciation for aspects of traditional country music. Though his career was cut tragically short by his 1973 death, his legacy was carried on by his protégé and duet partner Emmylou Harris; Harris would release her debut solo in 1975, an amalgamation of country, rock and roll, folk, blues and pop.
Subsequent to the initial blending of the two polar opposite genres, other offspring soon resulted, including Southern rock, heartland rock and in more recent years, alternative country.
In the decades that followed, artists such as Juice Newton, Alabama, Hank Williams, Jr. (and, to an even greater extent, Hank Williams III), Gary Allan, Shania Twain, Brooks & Dunn, Faith Hill, Garth Brooks, Alan Jackson, Dwight Yoakam, Steve Earle, Dolly Parton, Rosanne Cash and Linda Ronstadt moved country further towards rock influence.
Neocountry.
In 1980, a style of "neocountry disco music" was popularized by the film "Urban Cowboy", which also included more traditional songs such as "The Devil Went Down to Georgia" by the Charlie Daniels Band. It was during this time that a glut of pop-country crossover artists began appearing on the country charts: former pop stars Bill Medley (of The Righteous Brothers), "England Dan" Seals (of England Dan and John Ford Coley), Tom Jones, and Merrill Osmond (both alone and with some of his brothers) all recorded significant country hits in the early 1980s.
Sales in record stores rocketed to $250 million in 1981; by 1984, 900 radio stations began programming country or neocountry pop full-time. As with most sudden trends, however, by 1984 sales had dropped below 1979 figures.
Truck driving country.
Truck driving country music is a genre of country music
and is a fusion of honky-tonk, country rock and the Bakersfield sound.
It has the tempo of country rock and the emotion of honky-tonk, and its lyrics focus on a truck driver's lifestyle.
Truck driving country songs often deal with trucks and love. Well-known artists who sing truck driving country include Dave Dudley, Red Sovine, Dick Curless, Red Simpson, Del Reeves, The Willis Brothers and Jerry Reed, with C. W. McCall and Cledus Maggard (pseudonyms of Bill Fries and Jay Huguely, respectively) being more humorous entries in the subgenre. Dudley is known as the father of truck driving country.
Neotraditionalist movement.
During the mid-1980s, a group of new artists began to emerge who rejected the more polished country-pop sound that had been prominent on radio and the charts, in favor of more, traditional, "back-to-basics" production. Led by Reba McEntire and George Strait, many of the artists during the latter half of the 1980s drew on traditional honky-tonk, bluegrass, folk and western swing. Artists who typified this sound included Travis Tritt, Keith Whitley, Alan Jackson, Ricky Skaggs, Patty Loveless, Kathy Mattea, Randy Travis and The Judds.
Beginning in 1989, a confluence of events brought an unprecedented commercial boom to country music. The arrival of exceptionally talented artists coincided with new marketing strategies to engage fans, technology that more accurately tracked the popularity of country music, and a political and economic climate that focused attention on the genre. Garth Brooks ("Friends in Low Places") in particular attracted fans with his fusion of neotraditionalist country and stadium rock. His stadium concerts promised the same quality of special effects that fans expected from rock stars, while his music drew equally from George Strait and Journey. Other artists such as Brooks and Dunn ("Boot Scootin' Boogie") also combined conventional country with slick, rock elements, while Lorrie Morgan, Mary Chapin Carpenter, and Kathy Mattea updated neotraditionalist styles.
Fifth generation (1990s).
Country music was aided by the U.S. Federal Communications Commission's (FCC) Docket 80–90, which led to a significant expansion of FM radio in the 1980s by adding numerous higher-fidelity FM signals to rural and suburban areas. At this point, country music was mainly heard on rural AM radio stations; the expansion of FM was particularly helpful to country music, which migrated to FM from the AM band as AM became overcome by talk radio (the country music stations that stayed on AM developed the classic country format for the AM audience). At the same time, beautiful music stations already in rural areas began abandoning the format (leading to its effective demise) to adopt country music as well. This wider availability of country music led to producers seeking to polish their product for a wider audience. Another force leading to changes in the country music industry was the changing sound of rock music, which was increasingly being influenced by the noisier, less melodic alternative rock scene. "New country" ended up absorbing rock influence from more electric musicians that were too melodic for modern rock but too electric for the classic country music sound. (A number of "classic rock" artists, especially Southern rock ones such as Charlie Daniels and Lynyrd Skynyrd, are more closely associated with the modern country music scene than that of the modern rock scene.)
In the 1990s, country music became a worldwide phenomenon thanks to Garth Brooks, who enjoyed one of the most successful careers in popular music history, breaking records for both sales and concert attendance throughout the decade. The RIAA has certified his recordings at a combined (128× platinum), denoting roughly 113 million U.S. shipments. Other artists that experienced success during this time included Clint Black, Sammy Kershaw, Aaron Tippin, Travis Tritt, Alan Jackson and the newly formed duo of Brooks & Dunn; George Strait, whose career began in the 1980s, also continued to have widespread success in this decade and beyond. Toby Keith began his career as a more pop-oriented country singer in the 1990s, evolving into an outlaw persona in the late 1990s with "Pull My Chain" and its follow-up, "Unleashed".
Female artists such as Reba McEntire, Patty Loveless, Faith Hill, Martina McBride, Deana Carter, LeAnn Rimes, Mindy McCready, Lorrie Morgan, Shania Twain, and Mary Chapin Carpenter all released platinum-selling albums in the 1990s.
The Dixie Chicks became one of the most popular country bands in the 1990s and early 2000s. Their 1998 debut album "Wide Open Spaces" went on to become certified 12x platinum while their 1999 album "Fly" went on to become 10x platinum. After their third album, "Home", was released in 2003, the band made political news in part because of lead singer Natalie Maines's comments disparaging then-President George W. Bush while the band was overseas (Maines stated that she and her bandmates were ashamed to be from the same state as Bush, who had just commenced the Iraq War a few days prior). The comments caused a rift between the band and the country music scene, and the band's fourth (and most recent) album, 2006's "Taking the Long Way", took a more rock-oriented direction; the album was commercially successful overall but largely ignored among country audiences.
In the early-mid-1990s, country western music was influenced by the popularity of line dancing. This influence was so great that Chet Atkins was quoted as saying, "The music has gotten pretty bad, I think. It's all that damn line dancing." By the end of the decade, however, at least one line dance choreographer complained that good country line dance music was no longer being released.
In contrast, artists such as Don Williams and George Jones who had more or less had consistent chart success through the 1970s and 1980s suddenly had their fortunes fall rapidly around 1991 as these new artists rose to prominence.
Sixth generation (2000s–present).
The sixth generation of country continued the crossover between country and pop music. Richard Marx crossed over with his "Days in Avalon" album, which features five country songs and several singers and musicians. Alison Krauss sang background vocals to Marx's single "Straight from My Heart." Also, Bon Jovi had a hit single, "Who Says You Can't Go Home", with Jennifer Nettles of Sugarland. Kid Rock's collaboration with Sheryl Crow, "Picture," was a major crossover hit in 2001 and began Kid Rock's transition from hard rock to a country-rock hybrid that would later produce another major crossover hit, 2008's "All Summer Long." Darius Rucker, former frontman for the 1990s pop-rock band Hootie & the Blowfish, began a country solo career in the late 2000s, one that to date has produced three albums and several hits on both the country charts and the Billboard Hot 100. Singer-songwriter Unknown Hinson became famous for his appearance in the Charlotte television show "Wild, Wild, South", after which Hinson started his own band and toured in southern states. Other rock stars who featured a country song on their albums were Don Henley and Poison.
In 2005, country singer Carrie Underwood rose to fame as the winner of the fourth season of "American Idol" and has since become one of the most prominent recording artists of 2006 through 2014, with worldwide sales of more than 65 million records and seven Grammy Awards. With her first single, "Inside Your Heaven", Underwood became the only solo country artist to have a #1 hit on the "Billboard" Hot 100 chart in the 2000–2009 decade and also broke "Billboard" chart history as the first country music artist ever to debut at No. 1 on the Hot 100. Underwood's debut album, "Some Hearts", became the best-selling solo female debut album in country music history, the fastest-selling debut country album in the history of the SoundScan era and the best-selling country album of the last 10 years, being ranked by "Billboard" as the #1 Country Album of the 2000–2009 decade. She has also become the female country artist with the most number one hits on the "Billboard" Hot Country Songs chart in the Nielsen SoundScan era (1991–present), having 14 No. 1s and breaking her own "Guinness Book" record of ten. In 2007, Underwood won the Grammy Award for Best New Artist, becoming only the second Country artist in history (and the first in a decade) to win it. She also made history by becoming the seventh woman to win Entertainer of the Year at the Academy of Country Music Awards, and the first woman in history to win the award twice, as well as twice consecutively. "Time" has listed Underwood as one of the 100 most influential people in the world.
Carrie Underwood was one of several country stars produced by a television series in the 2000s. In addition to Underwood, "American Idol" launched the careers of Kellie Pickler, Josh Gracin, Bucky Covington, Kristy Lee Cook, Danny Gokey and Scotty McCreery (as well as that of occasional country singer Kelly Clarkson) in the decade, and would continue to launch country careers in the 2010s. The series "Nashville Star", while not nearly as successful as "Idol", did manage to bring Miranda Lambert and Chris Young to mainstream success, also launching the careers of lower-profile musicians such as Buddy Jewell, Sean Patrick McGraw, and Canadian musician George Canyon. "Can You Duet?" produced the duos Steel Magnolia and Joey + Rory.
Teen sitcoms also have had an impact on modern country music; in 2008, actress Jennette McCurdy (best known as the sidekick Sam on the teen sitcom "iCarly") released her first single, "So Close", following that with the single "Generation Love" in 2011. Another teen sitcom star, Miley Cyrus (of "Hannah Montana"), also had a crossover hit in the late 2000s with "The Climb" and another with a duet with her father, Billy Ray Cyrus, with "Ready, Set, Don't Go." Jana Kramer, an actress in the teen drama "One Tree Hill", released a country album in 2012 that has produced two hit singles as of 2013. Actress Hayden Panettiere began recording country songs as part of her role in the TV series "Nashville".
In 2010, the group Lady Antebellum won five Grammys, including the coveted Song of the Year and Record of the Year for "Need You Now a UK No28 hit in the mainstream single chart, rare for a country song ".
A large number of duos and vocal groups have begun to emerge on the charts in the 2010s, many of which feature close harmony in the lead vocals. In addition to Lady Antebellum, groups such as The Quebe Sisters Band, Little Big Town, The Band Perry, Gloriana, Thompson Square, Eli Young Band the Zac Brown Band and British duo The Shires have emerged to occupy a large portion of the new country artists in the popular scene. Along with solo singers Kacey Musgraves and Miranda Lambert.
One of the most commercially successful country artists of the late 2000s and early 2010s has been singer-songwriter Taylor Swift. Swift first became widely known in 2006 when her debut single, "Tim McGraw," was released when Swift was only 16. In 2006, Taylor released her first studio album, "Taylor Swift", which spent 275 weeks on Billboard 200, one of the longest runs of any album on that chart. In 2008, Taylor Swift released her second studio album, "Fearless", which made her the second-longest Number One charted on Billboard 200 and the second best-selling album (just behind Adele's "21") within the past 5 years. At the 2010 Grammys, Taylor Swift was 20 and won Album of the Year for "Fearless", which made her the youngest artist to win this award. Swift has received seven Grammys already. Buoyed by her teen idol status among girls and a change in the methodology of compiling the "Billboard" charts to favor pop-crossover songs, Swift's 2012 single "We Are Never Ever Getting Back Together" spent the most weeks at the top of Billboard's Hot Country Songs chart of any song in nearly five decades. The song's long run at the top of the chart was somewhat controversial, as the song was largely a pop song without much country influence and its success on the charts driven by a change to the chart's criteria to include airplay on non-country radio stations, prompting disputes over what constitutes a country song; many of Swift's later releases, such as "Shake It Off," were released solely to pop audiences.
The September 11 attacks of 2001 and the economic recession sparked Country music back into the spotlight. Many country artists, such as Alan Jackson with his ballad on terrorist attacks, "Where Were You (When the World Stopped Turning)", wrote songs that celebrated the military, highlighted the gospel, and rearranged home values over wealth. In contrast, more rock-oriented country singers took more direct aim at the attacks' perpetrators; Toby Keith's "The Angry American (Courtesy of the Red, White and Blue)" threatened to "a boot in" the posterior of the enemy, while Charlie Daniels's "This Ain't No Rag, It's a Flag" promised to "hunt" the perpetrators "down like a mad dog hound." These songs gained such recognition that it put Country music back into popular culture.
The influence of rock music in country has become more overt during the late 2000s and early 2010s as artists like Eric Church, Jason Aldean, and Brantley Gilbert have had success; Aaron Lewis, former frontman for the rock group Staind, had a moderately successful entry into country music in 2011 and 2012. Also rising in the late 2000s and early 2010s was the insertion of rap and spoken-word elements into country songs; artists such as Cowboy Troy and Colt Ford have focused almost exclusively on country rap (also known as hick hop) while other, more mainstream artists (such as Big & Rich and Jason Aldean) have used it on occasion.
In the 2010s, Bro-country, a genre noted primarily for its themes on drinking and partying, girls, and driving, became particularly popular. Notable artists associated with this genre are Luke Bryan, Jason Aldean, Blake Shelton, and Florida Georgia Line whose song "Cruise" became the best-selling country song of all time. Research suggested that about 45 percent of country's best-selling songs could be considered bro-country, with the top two artists being Luke Bryan and Florida Georgia Line. Albums by bro-country singers also sold very well - in 2013, Luke Bryan's "Crash My Party" was the third best-selling of all albums in the US, with Florida Georgia Line's "Here's to the Good Times" at sixth, and Blake Shelton's "Based on a True Story" at ninth. It is also thought that the popularity of bro-country helped country music to surpass classic rock as the most popular genre in America in 2012. The genre however is controversial as it has been criticized by other country musicians and commentators over its themes and depiction of women, opening up up a divide between the older generation of country singers and the younger bro country singers that was described as "civil war" by musicians, critics, and journalists."
Reba McEntire is the only artist to amass No. 1 singles in the past four decades.
The most played female artist of 2000s is Martina McBride.
Alt-country.
Attempts to combine punk and country were pioneered by Jason and the Scorchers, and in the 1980s Southern Californian cowpunk scene with bands like the Long Ryders. These styles merged fully in Uncle Tupelo's 1990 LP "No Depression", which is widely credited as being the first alt-country album, and gave its name to the online notice board, and eventually magazine, that underpinned that movement. Members and figures associated with Uncle Tupelo formed three major bands in the genre: Wilco, Son Volt, and Bottle Rockets. Other influential bands included Blue Mountain, Whiskeytown and Ryan Adams, Blood Oranges, Bright Eyes, Lucinda Williams, and Drive-By Truckers. Some alt-country songs have been crossover hits, including Ryan Adams's "When The Stars Go Blue," which charted when performed by Tim McGraw.
International.
Canada.
Outside of the United States, Canada has the largest country music fan and artist base, something that is to be expected given the two countries' proximity and cultural parallels. Mainstream country music is culturally ingrained in the prairie provinces, Ontario, and in Atlantic Canada. Celtic traditional music developed in Atlantic Canada in the form of Scottish, Acadian and Irish folk music popular amongst Irish, French and Scottish immigrants to Canada's Atlantic Provinces (Newfoundland, Nova Scotia, New Brunswick, and Prince Edward Island). Like the southern United States and Appalachia, all four regions are of heavy British Isles stock and rural; as such, the development of traditional music in the Maritimes somewhat mirrored the development of country music in the US South and Appalachia. Country and Western music never really developed separately in Canada; however, after its introduction to Canada, following the spread of radio, it developed quite quickly out of the Atlantic Canadian traditional scene. While true Atlantic Canadian traditional music is very Celtic or "sea shanty" in nature, even today, the lines have often been blurred. Certain areas often are viewed as embracing one strain or the other more openly. For example, in Newfoundland the traditional music remains unique and Irish in nature, whereas traditional musicians in other parts of the region may play both genres interchangeably.
"Don Messer's Jubilee" was a Halifax, Nova Scotia-based country/folk variety television show that was broadcast nationally from 1957 to 1969. In Canada it out-performed "The Ed Sullivan Show" broadcast from the United States and became the top-rated television show throughout much of the 1960s. "Don Messer's Jubilee" followed a consistent format throughout its years, beginning with a tune named "Goin' to the Barndance Tonight", followed by fiddle tunes by Messer, songs from some of his "Islanders" including singers Marg Osburne and Charlie Chamberlain, the featured guest performance, and a closing hymn. It ended with "Till We Meet Again".
The guest performance slot gave national exposure to numerous Canadian folk musicians, including Stompin' Tom Connors and Catherine McKinnon. Some Maritime country performers went on to further fame beyond Canada. Hank Snow, Wilf Carter (also known as Montana Slim), and Anne Murray are the three most notable.
The cancellation of the show by the public broadcaster in 1969 caused a nationwide protest, including the raising of questions in the Parliament of Canada.
The Prairie provinces, due to their western cowboy and agrarian nature, are the true heartland of Canadian country music. While the Prairies never developed a traditional music culture anything like the Maritimes, the folk music of the Prairies often reflected the cultural origins of the settlers, who were a mix of Scottish, Ukrainian, German and others. For these reasons polkas and Western music were always popular in the region, and with the introduction of the radio, mainstream country music flourished. As the culture of the region is western and frontier in nature, the specific genre of country and western is more popular today in the Prairies than in any other part of the country. No other area of the country embraces all aspects of the culture, from two-step dancing, to the cowboy dress, to rodeos, to the music itself, like the Prairies do. The Atlantic Provinces, on the other hand, produce far more traditional musicians, but they are not usually specifically country in nature, usually bordering more on the folk or Celtic genres.
Many traditional country artists are present in eastern and western Canada. They make common use of fiddle and pedal steel guitar styles. Some notable Canadian country artists include Shania Twain, Anne Murray, k.d. lang, Gordon Lightfoot, Buffy Sainte-Marie, George Canyon, Blue Rodeo, Tommy Hunter, Rita MacNeil, Stompin' Tom Connors, Stan Rogers, Ronnie Prophet, Carroll Baker, The Rankin Family, Ian Tyson, Johnny Reid, Paul Brandt, Jason McCoy, George Fox, Carolyn Dawn Johnson, Hank Snow, Don Messer, Wilf Carter, Michelle Wright, Terri Clark, Prairie Oyster, Family Brown, Johnny Mooring, Marg Osburne, Doc Walker, Emerson Drive, The Wilkinsons, Corb Lund and the Hurtin' Albertans, Crystal Shawanda, Dean Brody, Shane Yellowbird, Gord Bamford, Chad Brownlee, The Road Hammers, Rowdy Spurs and The Higgins.
Australia.
Australian country music has a long tradition. Influenced by American country music, it has developed a distinct style, shaped by British and Irish folk ballads and Australian bush balladeers like Henry Lawson and Banjo Paterson. Country instruments, including the guitar, banjo, fiddle and harmonica, create the distinctive sound of country music in Australia and accompany songs with strong storyline and memorable chorus.
Folk songs sung in Australia between the 1780s and 1920s, based around such themes as the struggle against government tyranny, or the lives of bushrangers, swagmen, drovers, stockmen and shearers, continue to influence the genre. This strain of Australian country, with lyrics focusing on Australian subjects, is generally known as "bush music" or "bush band music". "Waltzing Matilda", often regarded as Australia's unofficial national anthem, is a quintessential Australian country song, influenced more by British and Irish folk ballads than by American country and western music. The lyrics were composed by the poet Banjo Paterson in 1895. Other popular songs from this tradition include "The Wild Colonial Boy", "Click Go the Shears", "The Queensland Drover" and "The Dying Stockman". Later themes which endure to the present include the experiences of war, of droughts and flooding rains, of Aboriginality and of the railways and trucking routes which link Australia's vast distances.
Pioneers of a more Americanised popular country music in Australia included Tex Morton (known as "The Father of Australian Country Music") in the 1930s. Author Andrew Smith delivers a through research and engaged view of Tex Morton's life and his impact on the country music scene in Australia in the 1930s and 1940s. Other early stars included Buddy Williams, Shirley Thoms and Smoky Dawson. Buddy Williams (1918–1986) was the first Australian-born to record country music in Australia in the late 1930s and was the pioneer of a distinctly Australian style of country music called the bush ballad that others such as Slim Dusty would make popular in later years. During World War II, many of Buddy Williams recording sessions were done whilst on leave from the Army. At the end of the war, Williams would go on to operate some of the largest travelling tent rodeo shows Australia has ever seen.
In 1952, Dawson began a radio show and went on to national stardom as a singing cowboy of radio, TV and film. Slim Dusty (1927–2003) was known as the "King of Australian Country Music" and helped to popularise the Australian bush ballad. His successful career spanned almost six decades, and his 1957 hit "A Pub with No Beer" was the biggest-selling record by an Australian to that time, and with over seven million record sales in Australia he is the most successful artist in Australian musical history. Dusty recorded and released his one-hundredth album in the year 2000 and was given the honour of singing "Waltzing Matilda" in the closing ceremony of the Sydney 2000 Olympic Games. Dusty's wife Joy McKean penned several of his most popular songs.
Chad Morgan, who began recording in the 1950s, has represented a vaudeville style of comic Australian country; Frank Ifield achieved considerable success in the early 1960s, especially in the UK Singles Charts, and Reg Lindsay was one of the first Australians to perform at Nashville's Grand Ole Opry in 1974. Eric Bogle's 1972 folk lament to the Gallipoli Campaign "And the Band Played Waltzing Matilda" recalled the British and Irish origins of Australian folk-country. Singer-songwriter Paul Kelly, whose music style straddles folk, rock, and country, is often described as the poet laureate of Australian music. 
By the 1990s, country music had attained crossover success in the pop charts, with artists like James Blundell and James Reyne singing "Way Out West", and country star Kasey Chambers winning the ARIA for Best Female Artist in 2003. The crossover influence of Australian country is also evident in the music of successful contemporary bands The Waifs and the John Butler Trio. Nick Cave has been heavily influenced by the country artist Johnny Cash. In 2000, Cash, covered Cave's "The Mercy Seat" on the album ', seemingly repaying Cave for the compliment he paid by covering Cash's "The Singer" (originally "The Folk Singer") on his "Kicking Against the Pricks" album. Subsequently, Cave cut a duet with Cash on a version of Hank Williams' "I'm So Lonesome I Could Cry" for Cash's ' album (2002).
Popular contemporary performers of Australian country music include John Williamson (who wrote the iconic "True Blue"), Lee Kernaghan (whose hits include "Boys from the Bush" and "The Outback Club"), Gina Jeffreys, Forever Road and Sara Storer. In the United States, Olivia Newton-John, Sherrié Austin and Keith Urban have attained great success.
Country music has been a particularly popular form of musical expression among Indigenous Australians. Troy Cassar-Daley is among Australia's successful contemporary indigenous performers, and Kev Carmody and Archie Roach employ a combination of folk-rock and country music to sing about Aboriginal rights issues.
The Tamworth Country Music Festival began in 1973 and now attracts up to 100,000 visitors annually. Held in Tamworth, New South Wales (country music capital of Australia), it celebrates the culture and heritage of Australian country music. During the festival the CMAA holds the Country Music Awards of Australia ceremony awarding the Golden Guitar trophies. Other significant country music festivals include the Whittlesea Country Music Festival (near Melbourne) and the Mildura Country Music Festival for "independent" performers during October, and the Canberra Country Music Festival held in the national capital during November.
"Country HQ" showcases new talent on the rise in the country music scene down under. CMC (the Country Music Channel), a 24‑hour music channel dedicated to non-stop country music, can be viewed on pay TV and features once a year the Golden Guitar Awards, CMAs and CCMAs alongside international shows such as "The Wilkinsons", "The Road Hammers", and "Country Music Across America".
UK.
Country music is popular in the UK, with many US acts touring regularly there, although somewhat less so than in other English-speaking countries. There are some British country music acts and publications. 
One of the most successful of the 21st Century are female-male due The Shires who scored a hit album "Brave", a No10 in the mainstream UK Album Chart and No1 in the UK Country Music Chart, becoming the best selling album of this genre in the UK in 2015.
There is the festival held every year, and for many years there was a festival at Wembley Arena, which was broadcast on the BBC, the International Festivals of Country Music, promoted by Mervyn Conn, held at the venue between 1969 and 1991. The shows were later taken into Europe, and featured such stars as Johnny Cash, Dolly Parton, Tammy Wynette, David Allan Coe, Emmylou Harris, Boxcar Willie, Johnny Russell and Jerry Lee Lewis. A handful of country musicians had even greater success in mainstream UK music than they did in the United States, despite a certain amount of disdain from the music press; Faron Young, Slim Whitman and (at least from number of top 40 pop singles) Garth Brooks are some examples.
From within the UK, few country musicians achieved widespread mainstream success. Tom Jones, by this point near the end of his peak success as a pop singer, had a string of country hits in the late 1970s and early 1980s. The songwriting tandem of Roger Cook and Roger Greenaway wrote a number of country hits, in addition to their widespread success in pop songwriting; Cook is notable for being the only Briton to be inducted into the Nashville Songwriters Hall of Fame.
Other international country music.
Tom Roland, from the Country Music Association International, explains country music's global popularity: "In this respect, at least, Country Music listeners around the globe have something in common with those in the United States. In Germany, for instance, Rohrbach identifies three general groups that gravitate to the genre: people intrigued with the American cowboy icon, middle-aged fans who seek an alternative to harder rock music and younger listeners drawn to the pop-influenced sound that underscores many current Country hits."
One of the first Americans to perform country music abroad was George Hamilton IV. He was the first country musician to perform in the Soviet Union; he also toured in Australia and the Middle East. He was deemed the "International Ambassador of Country Music" for his contributions to the globalization of country music. Johnny Cash, Emmylou Harris, Keith Urban, and Dwight Yoakam have also made numerous international tours.
The Country Music Association undertakes various initiatives to promote country music internationally.
In Brazil, a musical genre known as música sertaneja, a very popular genre of music in Brazil, is very similar to American country music, sharing the music's rich history of development in the countryside.
In South America, on the last weekend of September, the yearly San Pedro Country Music Festival takes place in the town of San Pedro, Argentina. The festival features bands from different places of Argentina, as well as international artists from Brazil, Uruguay, Chile, Peru and the United States.
In India, the Anglo-Indian community is well known for enjoying and performing country music. An annual concert festival called "Blazing Guitars" held in Chennai brings together Anglo-Indian musicians from all over the country (including some who have emigrated to places like Australia).
In Ireland TG4 began a quest for Ireland's next country star called "Glór Tíre", translated as "Country Voice". It is now in its sixth season and is one of TG4's most watched TV shows. Over the past ten years country and gospel recording artist James Kilbane has reached multi-platinum success with his mix of Christian and traditional country influenced albums. James Kilbane like many other Irish artists are today working closer with Nashville. A recent success in the Irish arena has been Crystal Swing.
In Sweden, Rednex rose to stardom combining country music with electro-pop in the 1990s. In 1994, the group had a worldwide hit with their version of the traditional Southern tune "Cotton-Eyed Joe". Artists popularizing more traditional country music in Sweden have been Ann-Louise Hanson, Hasse Andersson, Kikki Danielsson, Elisabeth Andreassen and Jill Johnson.
In Poland an international country music festival, known as Piknik Country, has been organized in Mrągowo in Masuria since 1983.
There are more and more country music artists in France. Some of the most important are Liane Edwards, Annabel, Rockie Mountains, Tahiana, and Lili West. French rock and roll superstar Eddy Mitchell is also very inspired by Americana and country music.
In Iran, country music has appeared in recent years. According to "Melody Music Magazine", the pioneer of country music in Iran is the English-speaking country music band Dream Rovers, whose founder, singer and songwriter is Erfan Rezayatbakhsh (elf). The band was formed in 2007 in Tehran, and during this time they have been trying to introduce and popularize country music in Iran by releasing two studio albums and performing live at concerts, despite the difficulties that the Islamic regime in Iran makes for bands that are active in the western music field.
Performers and shows.
US cable television.
Six U.S. cable TV networks are at least partly devoted to the genre: Country Music Television and CMT Pure Country (both owned by Viacom), Rural Free Delivery TV (owned by Rural Media Group), Great American Country (owned by Scripps Networks), Heartland (owned by Luken Communications), and ZUUS Country (owned by ZUUS Media).
The first American country music video cable channel was The Nashville Network, launched in the early 1980s. In 2000, after it and CMT fell under the same corporate ownership, the channel was renamed and reformatted as The "National" Network, a general-interest network, and eventually became Spike TV. TNN was later revived from 2012 to 2013 after Jim Owens Entertainment acquired the trademark and licensed it to Luken Communications; that channel renamed itself Heartland after Luken was embroiled in an unrelated dispute that left the company bankrupt.
Canadian television.
Only one television channel is currently dedicated to country music in Canada: CMT (Canada) owned by Corus Entertainment (90%) and Viacom (10%). In the past, country music had an extensive presence, especially on the Canadian national broadcaster, CBC Television. The show "Don Messer's Jubilee" had a great impact on country music in Canada; for instance, it was the program that launched Anne Murray's career. "Country Hoedown" and its successor, "The Tommy Hunter Show", ran for a combined 32 years on the CBC, from 1960 to 1992; in its last nine years on air, the U.S. cable network TNN carried Hunter's show.
Australian cable television.
The only network dedicated to country music in Australia is the Country Music Channel owned by Foxtel.

</doc>
<doc id="5248" url="https://en.wikipedia.org/wiki?curid=5248" title="Cold War (1947–53)">
Cold War (1947–53)

The Cold War (1947–1953) is the period within the Cold War from the Truman Doctrine in 1947 to the conclusion of the Korean War in 1953. The Cold War began almost immediately following World War II and lasted through most of the 20th century.
Creation of the Eastern Bloc.
During World War II, the Soviet Union annexed several countries as Soviet Socialist Republics within the Union of Soviet Socialist Republics. Most of those countries had been ceded to it by the secret agreement portion of the Molotov-Ribbentrop Pact with Nazi Germany. These later annexed territories include Eastern Poland (incorporated into two different SSRs), Latvia (became Latvian SSR), Estonia (became Estonian SSR), Lithuania (became Lithuanian SSR), part of eastern Finland (became part of the Karelo-Finnish SSR) and northern Romania (became the Moldavian SSR).
Several of the other countries it occupied that were not directly annexed into the Soviet Union became Soviet satellite states. In East Germany after local election losses, a forced merger of political parties in the Socialist Unity Party ("SED"), followed by elections in 1946 where political opponents were oppressed. In the non-USSR annexed portion of Poland, less than a third of Poland's population voted in favor of massive communist land reforms and industry nationalizations in a policies referendum known as "3 times YES" ("3 razy TAK"; "3xTAK"), whereupon a second vote rigged election was held to get the desired result. Fraudulent Polish elections held in January 1947 resulted in Poland's official transformation to the People's Republic of Poland.
The List of World Leaders at the Beginning of these Years are as follows:
1947- Clement Attlee (UK); Harry Truman (US); Vincent Auriol (France); Joseph Stalin (USSR); Chiang Kai-Shek (Allied China)
1948- Clement Attlee (UK); Harry Truman (US); Vincent Auriol (France); Joseph Stalin (USSR); Mao Zedong (China)
1949- Clement Attlee (UK): Harry Truman (US); Vincent Auriol (France); Joseph Stalin (USSR); Mao Zedong (China)
1950- Clement Attlee (UK); Harry Truman (US); Vincent Auriol (France); Joseph Stalin (USSR); Mao Zedong (China)
1951- Clement Attlee (UK); Harry Truman (US); Vincent Auriol (France); Joseph Stalin (USSR); Mao Zedong (China)
1952- Winston Churchill (UK); Harry Truman (US); Vincent Auriol (France); Joseph Stalin (USSR); Mao Zedong (China)
1953- Winston Churchill (UK); Harry Truman (US); Vincent Auriol (France); Joseph Stalin (USSR); Mao Zedong (China)
In Hungary, when the Soviets installed a communist government, Mátyás Rákosi was appointed General Secretary of the Hungarian Communist Party, which began one of the harshest dictatorships in Europe under the People's Republic of Hungary. In Bulgaria, toward the end of World War II, the Soviet Union crossed the border and created the conditions for a communist coup d'état on the following night. The Soviet military commander in Sofia assumed supreme authority, and the communists whom he instructed, including Kimon Georgiev (who was not a communist himself, but a member of the elitarian political organization "Zveno", working together with the communists), took full control of domestic politics in the People's Republic of Bulgaria.
With Soviet backing, the Communist Party of Czechoslovakia assumed undisputed control over the government of Czechoslovakia in the Czechoslovak coup d'état of 1948, ushering in a dictatorship. In the Romanian general election elections of 1946, the Romanian Communist Party (PCR) employed widespread intimidation tactics and electoral fraud to obtain 80 percent of the vote and, thereafter, eliminated the role of the centrist parties and forced mergers, the result of which was that, by 1948, most non-Communist politicians were either executed, in exile or in prison. In the December 1945 Albanian election, the only effective ballot choices were those of the communist Democratic Front (Albania), led by Enver Hoxha. In 1946, Albania was declared the People's Republic of Albania.
Initially, Stalin directed systems in the Eastern Bloc countries that rejected Western institutional characteristics of market economies, democratic governance (dubbed "bourgeois democracy" in Soviet parlance) and the rule of law subduing discretional intervention by the state. They were economically communist and depended upon the Soviet Union for significant amounts of materials. While in the first five years following World War II, massive emigration from these states to the West occurred, restrictions implemented thereafter stopped most East-West migration, except that under limited bilateral and other agreements.
Containment.
The immediate post-1945 period may have been the historical high point for the popularity of communist ideology. The burdens the Red Army and the Soviet Union endured had earned it massive respect which, had it been fully exploited by Joseph Stalin, had a good chance of resulting in a communist Europe. Communist parties achieved a significant popularity in such nations as China, Greece, Iran, and the Republic of Mahabad. Communist parties had already come to power in Romania, Bulgaria, Albania, and Yugoslavia. The United Kingdom and the United States were concerned that electoral victories by communist parties in any of these countries could lead to sweeping economic and political change in Western Europe.
Morgenthau and Marshall Plans.
Having lost 27 million people in the war, the Soviet Union was determined to destroy Germany's capacity for another war, and pushed for such in wartime conferences. The resulting Morgenthau plan policy foresaw returning Germany to a pastoral state without heavy industry. Because of the increasing costs of food imports to avoid mass-starvation in Germany, and with the danger of losing the entire nation to communism, the U.S. government abandoned the Morgenthau plan in September 1946 with Secretary of State James F. Byrnes' speech Restatement of Policy on Germany.
In January 1947, Truman appointed General George Marshall as Secretary of State, and enacted JCS 1779, which decreed that an orderly and prosperous Europe requires the economic contributions of a stable and productive Germany." The directive comported with the view of General Lucius D. Clay and the Joint Chief of Staff over growing communist influence in Germany, as well as of the failure of the rest of the European economy to recover without the German industrial base on which it previously had been dependent. Administration officials met with Soviet Foreign Minister Vyacheslav Molotov and others to press for an economically self-sufficient Germany, including a detailed accounting of the industrial plants, goods and infrastructure already removed by the Soviets. After six weeks of negotiations, Molotov refused the demands and the talks were adjourned. Marshall was particularly discouraged after personally meeting with Stalin, who expressed little interest in a solution to German economic problems. The United States concluded that a solution could not wait any longer. In a June 5, 1947 speech, comporting with the Truman Doctrine, Marshall announced a comprehensive program of American assistance to all European countries wanting to participate, including the Soviet Union and those of Eastern Europe, called the Marshall Plan.
Fearing American political, cultural and economic penetration, Stalin eventually forbade Soviet Eastern bloc countries of the newly formed Cominform from accepting Marshall Plan aid. In Czechoslovakia, that required a Soviet-backed Czechoslovak coup d'état of 1948, the brutality of which shocked Western powers more than any event so far and set in a motion a brief scare that war would occur and swept away the last vestiges of opposition to the Marshall Plan in the United States Congress.
The Greek Civil War and the Truman Doctrine.
Both East and West regarded Greece as a nation well within the sphere of influence of Britain. Stalin had respected his agreement with Winston Churchill to not intervene, but Yugoslavia and Albania defied the USSR's advice and sent supplies during the Greek Civil War to the partisan forces of the Communist Party of Greece, the ELAS (National Popular Liberation Army). The UK had given aid to the royalist Greek forces and ELAS leaders who, failing to realize that there would be no Soviet aid and having boycotted the elections, were at a disadvantaged position. However, by 1947, the near-bankrupt British government could no longer maintain its massive overseas commitments. In addition to granting independence to India and handing back the Palestinian Mandate to the United Nations, the British government decided to withdraw from both Greece and nearby Turkey. This would have left the two nations, in particular Greece, on the brink of a communist-led revolution.
Notified that British aid to Greece and Turkey would end in less than six weeks, and already hostile towards and suspicious of Soviet intentions, because of their reluctance to withdraw from Iran, the Truman administration decided that additional action was necessary. With Congress solidly in Republican hands, and with isolationist sentiment strong among the U.S. public, Truman adopted an ideological approach. In a meeting with congressional leaders, the argument of "apples in a barrel infected by one rotten one" was used to convince them of the significance in supporting Greece and Turkey. It was to become the "domino theory". On the morning of March 12, 1947, President Harry S. Truman appeared before Congress to ask for $400 million of aid to Greece and Turkey. Calling on congressional approval for the United States to "support free peoples who are resisting attempted subjugation by armed minorities or by outside pressures," or in short a policy of "containment", Truman articulated a presentation of the ideological struggle that became known as the "Truman Doctrine." Although based on a simplistic analysis of internal strife in Greece and Turkey, it became the single dominating influence over U.S. policy until at least the Vietnam War.
Truman's speech had a tremendous effect. The anti-communist feelings that had just begun to hatch in the U.S. were given a great boost, and a silenced Congress voted overwhelmingly in approval of aid. The United States would not withdraw back to the Western Hemisphere as it had after World War I. From then on, the U.S. actively engaged any communist threats anywhere in the globe under the ostensible causes of "freedom", "democracy" and "human rights." The U.S. brandished its role as the leader of the "free world." Meanwhile, the Soviet Union brandished its position as the leader of the "progressive" and "anti-imperialist" camp.
U.S. Military Doctrine Debate.
After World War II, the generals of the newly formed U.S. Air Force propounded a new doctrine: that strategic bombing, particularly with nuclear weapons, was the sole decisive element necessary to win any future war; and was therefore the sole means necessary to deter an adversary from launching a Pearl Harbor like surprise attack or war against the United States. To implement this doctrine, which the Air Force and its supporters regarded as the highest national priority, the Air Force proposed that it should be funded by the Congress to build a large fleet of U.S. based long-range strategic heavy bombers. The Air Force generals argued that this project should receive large amounts of funding, beginning with the B-36 Peacemaker bomber.
The admirals of the Navy disagreed. Pointing to the overwhelming dominance of the aircraft carrier in the Pacific Theater, they asked the United States Congress to fund a large fleet of "supercarriers" and their supporting battle groups, beginning with the USS "United States" (CVA-58). The Navy leadership believed that wars could not be won by strategic bombing alone, with or without the use of nuclear weapons. The Navy also maintained that to decide, at the outset of any future conflict, to initiate the widespread use of nuclear weapons—attacking the major population centers of the enemy homeland—was immoral.
"Nazi-Soviet relations" and "Falsifiers of History".
Relations further deteriorated when, in January 1948, the U.S. State Department also published a collection of documents titled "Nazi-Soviet Relations, 1939–1941: Documents from the Archives of The German Foreign Office", which contained documents recovered from the Foreign Office of Nazi Germany revealing Soviet conversations with Germany regarding the Molotov-Ribbentrop Pact, including its secret protocol dividing eastern Europe, the 1939 German-Soviet Commercial Agreement, and discussions of the Soviet Union potentially becoming the fourth Axis Power.
In response, one month later, the Soviet Union published "Falsifiers of History", a Stalin edited and partially re-written book attacking the West. The book did not attempt to directly counter or deal with the documents published in "Nazi-Soviet Relations" and rather, focused upon Western culpability for the outbreak of war in 1939. It argues that "Western powers" aided Nazi rearmament and aggression, including that American bankers and industrialists provided capital for the growth of German war industries, while deliberately encouraging Hitler to expand eastward. The book also included the claim that, during the Pact's operation, Stalin rejected Hitler's offer to share in a division of the world, without mentioning the Soviet offers to join the Axis. Historical studies, official accounts, memoirs and textbooks published in the Soviet Union used that depiction of events until the Soviet Union's dissolution.
Berlin Blockade.
After the Marshall Plan, the introduction of a new currency to Western Germany to replace the debased Reichsmark and massive electoral losses for communist parties in 1946, in June 1948, the Soviet Union cut off surface road access to Berlin. On the day of the Berlin Blockade, a Soviet representative told the other occupying powers "We are warning both you and the population of Berlin that we shall apply economic and administrative sanctions that will lead to circulation in Berlin exclusively of the currency of the Soviet occupation zone."
Thereafter, street and water communications were severed, rail and barge traffic was stopped and the Soviets initially stopped supplying food to the civilian population in the non-Soviet sectors of Berlin. Because Berlin was located within the Soviet-occupied zone of Germany and the other occupying powers had previously relied on Soviet good will for access to Berlin, the only available methods of supplying the city were three limited air corridors.
By February 1948, because of massive post-war military cuts, the entire United States army had been reduced to 552,000 men. Military forces in non-Soviet Berlin sectors totaled only 8,973 Americans, 7,606 British and 6,100 French. Soviet military forces in the Soviet sector that surrounded Berlin totaled one and a half million men. The two United States regiments in Berlin would have provided little resistance against a Soviet attack. Believing that Britain, France and the United States had little option other than to acquiesce, the Soviet Military Administration in Germany celebrated the beginning of the blockade. Thereafter, a massive aerial supply campaign of food, water and other goods was initiated by the United States, Britain, France and other countries. The Soviets derided "the futile attempts of the Americans to save face and to maintain their untenable position in Berlin." The success of the airlift eventually caused the Soviets to lift their blockade in May 1949.
Tito-Stalin Split.
After disagreements between Yugoslavian leader Josip Broz Tito and the Soviet Union regarding Greece and the People's Republic of Albania, a Tito-Stalin split occurred, followed by Yugoslavia being expelled from the Cominform in June 1948 and a brief failed Soviet putsch in Belgrade. The split created two separate communist forces in Europe. A vehement campaign against "Titoism" was immediately started in the Eastern Bloc, describing agents of both the West and Tito in all places engaging in subversive activity. This resulted in the persecution of many major party cadres, including those in East Germany.
NATO.
The United States joined Britain, France, Canada, Denmark, Portugal, Norway, Belgium, Iceland, Luxembourg, Italy, and the Netherlands in 1949 to form the North Atlantic Treaty Organization (NATO), the United States' first "entangling" European alliance in 170 years. West Germany, Spain, Greece, and Turkey would later join this alliance. The Eastern leaders retaliated against these steps by integrating the economies of their nations in Comecon, their version of the Marshall Plan; exploding the first Soviet atomic device in 1949; signing an alliance with People's Republic of China in February 1950; and forming the Warsaw Pact, Eastern Europe's counterpart to NATO, in 1955. The Soviet Union, Albania, Czechoslovakia, Hungary, East Germany, Bulgaria, Romania, and Poland founded this military alliance.
NSC-68.
U.S. officials quickly moved to escalate and expand "containment." In a secret 1950 document, NSC-68, they proposed to strengthen their alliance systems, quadruple defense spending, and embark on an elaborate propaganda campaign to convince the U.S. public to fight this costly cold war. Truman ordered the development of a hydrogen bomb. In early 1950, the U.S. took its first efforts to oppose communist forces in Vietnam; planned to form a West German army, and prepared proposals for a peace treaty with Japan that would guarantee long-term U.S. military bases there.
Chinese Civil War.
Shortly after World War II, the civil war resumed in China between the Kuomintang (KMT) led by Generalissimo Chiang Kai-shek and the Communist Party of China led by Mao Zedong. The USSR had signed a Treaty of Friendship with the Kuomintang in 1945 and disavowed support for the Chinese Communists. The outcome was closely fought, with the Communists finally prevailing with superior military tactics. Although the Nationalists had an advantage in numbers of men and weapons, initially controlled a much larger territory and population than their adversaries, and enjoyed considerable international support, they were exhausted by the long war with Japan and the attendant internal responsibilities. In addition, the Chinese Communists were able to fill the political vacuum left in Manchuria after Soviet forces withdrew from the area and thus gained China's prime industrial base. The Chinese Communists were able to fight their way from the north and northeast, and virtually all of mainland China was taken by the end of 1949. On October 1, 1949, Mao Zedong proclaimed the People's Republic of China (PRC). Chiang Kai-shek and 600,000 Nationalist troops and 2 million refugees, predominantly from the government and business community, fled from the mainland to the island of Taiwan. In December 1949, Chiang proclaimed Taipei the temporary capital of the Republic of China (ROC) and continued to assert his government as the sole legitimate authority in China.
The continued hostility between the Communists on the mainland and the Nationalists on Taiwan continued throughout the Cold War. Though the United States refused to aide Chiang Kai-shek in his hope to "recover the mainland," it continued supporting the Republic of China with military supplies and expertise to prevent Taiwan from falling into PRC hands. Through the support of the Western bloc (most Western countries continued to recognize the ROC as the sole legitimate government of China), the Republic of China on Taiwan retained China's seat in the United Nations until 1971.
Korean War.
In early 1950, the United States made its first commitment to form a peace treaty with Japan that would guarantee long-term U.S. military bases. Some observers (including George Kennan) believed that the Japanese treaty led Stalin to approve a plan to invade U.S.-supported South Korea on June 25, 1950. Korea had been divided at the end of World War II along the 38th parallel into Soviet and U.S. occupation zones, in which a communist government was installed in the North by the Soviets, and an elected government in the South came to power after UN-supervised elections in 1948.
In June 1950, Kim Il-sung's North Korean People's Army invaded South Korea. Fearing that communist Korea under a Kim Il Sung dictatorship could threaten Japan and foster other communist movements in Asia, Truman committed U.S. forces and obtained help from the United Nations to counter the North Korean invasion. The Soviets boycotted UN Security Council meetings while protesting the Council's failure to seat the People's Republic of China and, thus, did not veto the Council's approval of UN action to oppose the North Korean invasion. A joint UN force of personnel from South Korea, the United States, Britain, Turkey, Canada, Australia, France, the Philippines, the Netherlands, Belgium, New Zealand and other countries joined to stop the invasion. After a Chinese invasion to assist the North Koreans, fighting stabilized along the 38th parallel, which had separated the Koreas. Truman faced a hostile China, a Sino-Soviet partnership, and a defense budget that had quadrupled in eighteen months.
The Korean Armistice Agreement was signed in July 1953 after the death of Stalin, who had been insisting that the North Koreans continue fighting. In North Korea, Kim Il-sung created a highly centralized and brutal dictatorship, according himself unlimited power and generating a formidable cult of personality.
Hydrogen bomb.
A hydrogen bomb—which produced nuclear fusion instead of nuclear fission—was first tested by the United States in November 1952 and the Soviet Union in August 1953. Such bombs were first deployed in the 1960s.
Culture and media.
Fear of a nuclear war spurred the production of public safety films by the United States federal government's Civil Defense branch that demonstrated ways on protecting oneself from a Soviet nuclear attack. The 1951 children's film "Duck and Cover" is a prime example.
George Orwell's classic dystopia Nineteen Eighty-Four was published in 1949. The novel explores life in an imagined future world where a totalitarian government has achieved terrifying levels of power and control. With Nineteen Eighty-Four, Orwell taps into the anti-communist fears that would continue to haunt so many in the West for decades to come. In a Cold War setting his descriptions could hardly fail to evoke comparison to Soviet communism and the seeming willingness of Stalin and his successors to control those within the Soviet bloc by whatever means necessary. Orwell's famous allegory of totalitarian rule, Animal Farm, published in 1945, provoked similar anti-communist sentiments.

</doc>
<doc id="5249" url="https://en.wikipedia.org/wiki?curid=5249" title="Crony capitalism">
Crony capitalism

Crony capitalism is a term describing an economy in which success in business depends on close relationships between business people and government officials. It may be exhibited by favoritism in the distribution of legal permits, government grants, special tax breaks, or other forms of state interventionism.
Crony capitalism is believed to arise when business cronyism and related self-serving behavior by businesses or businesspeople spills over into politics and government, or when self-serving friendships and family ties between businessmen and the government influence the economy and society to the extent that it corrupts public-serving economic and political ideals.
The term "crony capitalism" made a significant impact in the public arena as an explanation of the Asian financial crisis. It is also used to describe governmental decisions favoring "cronies" of governmental officials. In this context, the term is often used interchangeably with corporate welfare; to the extent that there is a difference, it may be the extent to which a government action can be said to benefit individuals rather than entire industries.
In practice.
Crony capitalism exists along a continuum. In its lightest form, crony capitalism consists of collusion among market players which is officially tolerated or encouraged by the government. While perhaps lightly competing against each other, they will present a unified front (sometimes called a trade association or industry trade group) to the government in requesting subsidies or aid or regulation. Newcomers to a market may find it difficult to find loans, acquire shelf space, or receive official sanction. Some such systems are very formalized, such as sports leagues and the Medallion System of the taxicabs of New York City, but often the process is more subtle, such as expanding training and certification exams to make it more expensive for new entrants to enter a market and thereby limit competition. In technological fields, there may evolve a system whereby new entrants may be accused of infringing on patents that the established competitors never assert against each other. In spite of this, some competitors may succeed when the legal barriers are light. 
The term crony capitalism is generally used when these practices come to dominate the economy as a whole or to dominate the most valuable industries in an economy. Intentionally ambiguous laws and regulations are common in such systems. Taken strictly, such laws would greatly impede practically all business; in practice, they are only erratically enforced. The specter of having such laws suddenly brought down upon a business provides incentive to stay in the good graces of political officials. Troublesome rivals who have overstepped their bounds can have the laws suddenly enforced against them, leading to fines or even jail time. Even in high-income democracies with well-established legal systems and freedom of the press a larger state is associated with more political corruption.
The term "crony capitalism" was initially applied to states involved in the 1997 Asian Financial Crisis such as Thailand and Indonesia. In these cases, the term was used to point out how family members of the ruling leaders become extremely wealthy with no non-political justification. Southeast Asian nations still score very poorly in rankings measuring this. Hong Kong, and Malaysia are perhaps most noted for this, and the term has also been applied to the system of oligarchs in Russia.
Other states to which the term has been applied include India, in particular, the system after the 1990s liberalization whereby land and other resources were given at throwaway prices in the name of public private partnerships, Argentina and Greece. Wu Jinglian, one of China's leading economists and a longtime advocate of its transition to free markets, says that it faces two starkly contrasting futures: a market economy under the rule of law or crony capitalism.
Many prosperous nations have also had varying amounts of cronyism throughout their history including the United Kingdom - especially in the 1600s and 1700s, United States, and Japan.
Crony capitalism index.
"The Economist" benchmarks countries based on a "crony-capitalism index" calculated via how much economic activity occurs in industries prone to cronyism. Its 2014 Crony Capitalism Index ranking listed Hong Kong, Russia and Malaysia in the top 3 spots.
Crony Capitalism in Finance.
Crony capitalism in finance was found in the Second Bank of the United States. It was a private company, but its largest stockholder was the federal government which owned 20%. It was an early bank regulator and grew to be one being the most powerful organizations n the country due largely to being the depository of the government’s revenue.
The Gramm-Leach-Bliley Act in 1999 removed completely removed Glass-Steagall’s separation between commercial banks and investment banks. After this repeal, commercial banks, investment banks, and insurance companies combined their lobbying efforts. Critics claim tis was instrumental in the passage of the Bankruptcy Abuse Prevention and Consumer Protection Act of 2005.
In sections of an economy.
More direct government involvement in a specific sector can also lead to specific areas of crony capitalism, even if the economy as a whole may be competitive. This is most common in natural resource sectors through the granting of mining or drilling concessions, but it is also possible through a process known as regulatory capture where the government agencies in charge of regulating an industry come to be controlled by that industry. Governments will often, in good faith, establish government agencies to regulate an industry. However, the members of an industry have a very strong interest in the actions of that regulatory body, while the rest of the citizenry are only lightly affected. As a result, it is not uncommon for current industry players to gain control of the "watchdog" and to use it against competitors. This typically takes the form of making it very expensive for a new entrant to enter the market.
An 1824 landmark U.S. Supreme Court ruling overturned a New York State-granted monopoly ("a veritable model of state munificence" facilitated by one of the Founding Fathers, Robert R. Livingston) for the then-revolutionary technology of steamboats. Leveraging the Supreme Court's establishment of Congressional supremacy over commerce, the Interstate Commerce Commission was established in 1887 with the intent of regulating railroad "robber barons". President Grover Cleveland appointed Thomas M. Cooley, a railroad ally, as its first chairman and a permit system was used to deny access to new entrants and legalize price fixing.
The defense industry in the United States is often described as an example of crony capitalism in an industry. Connections with the Pentagon and lobbyists in Washington are described by critics as more important than actual competition, due to the political and secretive nature of defense contracts. In the Airbus-Boeing WTO dispute, Airbus (which receives outright subsidies from European governments) has stated Boeing receives similar subsidies, which are hidden as inefficient defense contracts. Other American defense companies were put under scrutiny for no-bid contracts for Iraq war and Hurricane Katrina related contracts purportedly due to having cronies in the Bush administration.
Gerald P. O'Driscoll, former vice president at the Federal Reserve Bank of Dallas, stated that Fannie Mae and Freddie Mac became examples of crony capitalism. Government backing let Fannie and Freddie dominate mortgage underwriting. "The politicians created the mortgage giants, which then returned some of the profits to the pols - sometimes directly, as campaign funds; sometimes as "contributions" to favored constituents."
Creation in developing economies.
In its worst form, crony capitalism can devolve into simple corruption, where any pretense of a free market is dispensed with. Bribes to government officials are considered "de rigueur" and tax evasion is common; this is seen in many parts of Africa, for instance. This is sometimes called plutocracy (rule by wealth) or kleptocracy (rule by theft).
Corrupt governments may favor one set of business owners who have close ties to the government over others. This may also be done with racial, religious, or ethnic favoritism; for instance, Alawites in Syria have a disproportionate share of power in the government and business there. (President Assad is an Alawite.) This can be explained by considering personal relationships as a social network. As government and business leaders try to accomplish various things, they naturally turn to other powerful people for support in their endeavors. These people form hubs in the network. In a developing country those hubs may be very few, thus concentrating economic and political power in a small interlocking group.
Normally, this will be untenable to maintain in business; new entrants will affect the market. However, if business and government are entwined, then the government can maintain the small-hub network.
Raymond Vernon, specialist in economics and international affairs,
wrote that the Industrial Revolution began in Great Britain, because they were the first to successfully limit the power of veto groups (typically cronies of those with power in government) to block innovations. "Unlike most other national environments, the British environment of the early 19th century contained relatively few threats to those who improved and applied existing inventions, whether from business competitors, labor, or the government itself. In other European countries, by contrast, the merchant guilds ... were a pervasive source of veto for many centuries. This power was typically bestowed upon them by government". For example, a Russian inventor produced a steam engine in 1766 and disappeared without a trace. " steam powered horseless carriage produced in France in 1769 was officially suppressed." James Watt began experimenting with steam in 1763, got a patent in 1769, and began commercial production in 1775.
Raghuram Rajan, governor of the Reserve Bank of India, has said "One of the greatest dangers to the growth of developing countries is the middle income trap, where crony capitalism creates oligarchies that slow down growth. If the debate during the elections is any pointer, this is a very real concern of the public in India today." Tavleen Singh, columnist for "The Indian Express" has disagreed. According to her, India's corporate success is not a product of crony capitalism, but because India is no longer under the influence of crony socialism.
Political viewpoints.
While the problem is generally accepted across the political spectrum, ideology shades the view of the problem's causes and therefore its solutions. Political views mostly fall into two camps which might be called the socialist and capitalist critique. The socialist position is that broadly democratic government must regulate economic, or wealthy, interests in order to restrict monopoly. The capitalist position is that "natural monopolies" are rare, therefore governmental regulations generally abet established wealthy interests by restricting competition.
Socialist critique.
Critics of crony capitalism including socialists and anti-capitalists often assert that crony capitalism is the inevitable result of "any" strictly capitalist system. Jane Jacobs described it as a natural consequence of collusion between those managing power and trade, while Noam Chomsky has argued that the word "crony" is superfluous when describing capitalism. Since businesses make money and money leads to political power, business will inevitably use their power to influence governments. Much of the impetus behind campaign finance reform in the United States and in other countries is an attempt to prevent economic power being used to take political power.
Ravi Batra argues that "all official economic measures adopted since 1981...have devastated the middle class" and that the Occupy Wall Street movement should push for their repeal and thus end the influence of the super wealthy in the political process, which he considers a manifestation of crony capitalism.
Socialist economists, such as Robin Hahnel, have criticized the term as an ideologically motivated attempt to cast what is in their view the fundamental problems of capitalism as avoidable irregularities. Socialist economists dismiss the term as an apologetic for failures of neoliberal policy and, more fundamentally, their perception of the weaknesses of market allocation.
Capitalist critique.
Supporters of capitalism generally oppose crony capitalism as well, but consider it an aberration brought on by governmental favors incompatible with free market. In this view, crony capitalism is the result of an excess of socialist-style interference in the market, which inherently will result in a toxic combination of corporations and government officials running the sector of the economy. Some advocates prefer to equate this problem with terms such as "corporatism, a modern form of mercantilism" to emphasize that the only way to run a profitable business in such a system is to have help from corrupt government officials. Even if the initial regulation was well-intentioned (to curb actual abuses), and even if the initial lobbying by corporations was well-intentioned (to reduce illogical regulations), the mixture of business and government stifle competition, a collusive result called regulatory capture. Burton W. Folsom, Jr. distinguishes those that engage in crony capitalism—designated by him "political entrepreneurs"—from those who compete in the marketplace without special aid from government, whom he calls "market entrepreneurs". The market entrepreneurs, such as Hill, Vanderbilt, and Rockefeller, succeeded by producing a quality product at a competitive price. The political entrepreneurs, for example, Edward Collins in steamships and the leaders of the Union Pacific Railroad in railroads, were men who used the power of government to succeed. They tried to gain subsidies or in some way use government to stop competitors.

</doc>
<doc id="5252" url="https://en.wikipedia.org/wiki?curid=5252" title="Lists of universities and colleges">
Lists of universities and colleges

This is a list of Lists of universities and colleges.

</doc>
<doc id="5253" url="https://en.wikipedia.org/wiki?curid=5253" title="Constitution">
Constitution

A constitution is a set of fundamental principles or established precedents according to which a state or other organization is governed. These rules together make up, i.e. "constitute", what the entity is. When these principles are written down into a single document or set of legal documents, those documents may be said to embody a "written" constitution; if they are written down in a single comprehensive document, it is said to embody a "codified" constitution.
Constitutions concern different levels of organizations, from sovereign states to companies and unincorporated associations. A treaty which establishes an international organization is also its constitution, in that it would define how that organization is constituted. Within states, a constitution defines the principles upon which the state is based, the procedure in which laws are made and by whom. Some constitutions, especially codified constitutions, also act as limiters of state power, by establishing lines which a state's rulers cannot cross, such as fundamental rights. An example is the constitution of the United States of America.
The Constitution of India is the longest written constitution of any sovereign country in the world, containing 444 articles in 22 parts, 12 schedules and 118 amendments, with 117,369 words in its English-language translation, while the United States Constitution is the shortest written constitution, containing seven articles and 27 amendments, and a total of 4,400 words.
Etymology.
The term "constitution" comes through French from the Latin word "constitutio", used for regulations and orders, such as the imperial enactments ("constitutiones principis": edicta, mandata, decreta, rescripta). Later, the term was widely used in canon law for an important determination, especially a decree issued by the Pope, now referred to as an "apostolic constitution".
General features.
Generally, every modern written constitution confers specific powers to an organization or institutional entity, established upon the primary condition that it abide by the said constitution's limitations. According to Scott Gordon, author of "Controlling the State: Constitutionalism from Ancient Athens to Today" a political organization is constitutional to the extent that it "contain institutionalized mechanisms of power control for the protection of the interests and liberties of the citizenry, including those that may be in the minority."
The Latin term "ultra vires" describes activities of officials within an organization or polity that fall outside the constitutional or statutory authority of those officials. For example, a students' union may be prohibited as an organization from engaging in activities not concerning students; if the union becomes involved in non-student activities these activities are considered "ultra vires" of the union's charter, and nobody would be compelled by the charter to follow them. An example from the constitutional law of sovereign states would be a provincial government in a federal state trying to legislate in an area exclusively enumerated to the federal government in the constitution, such as ratifying a treaty. "Ultra vires" gives a legal justification for the forced cessation of such action, which might be enforced by the people with the support of a decision of the judiciary, in a case of judicial review. A violation of rights by an official would be "ultra vires" because a (constitutional) right is a restriction on the powers of government, and therefore that official would be exercising powers they do not have.
In most but not all modern states the constitution has supremacy over ordinary Statutory law (see Uncodified constitution below); in such states when an official act is unconstitutional, i.e. it is not a power granted to the government by the constitution, that act is "null and void", and the nullification is "ab initio", that is, from inception, not from the date of the finding. It was never "law", even though, if it had been a statute or statutory provision, it might have been adopted according to the procedures for adopting legislation. Sometimes the problem is not that a statute is unconstitutional, but the application of it is, on a particular occasion, and a court may decide that while there are ways it could be applied that are constitutional, that instance was not allowed or legitimate. In such a case, only the application may be ruled unconstitutional. Historically, the remedy for such violations have been petitions for common law writs, such as "quo warranto".
History and development.
Pre-modern constitutions.
Ancient.
Excavations in modern-day Iraq by Ernest de Sarzec in 1877 found evidence of the earliest known code of justice, issued by the Sumerian king Urukagina of Lagash "ca" 2300 BC. Perhaps the earliest prototype for a law of government, this document itself has not yet been discovered; however it is known that it allowed some rights to his citizens. For example, it is known that it relieved tax for widows and orphans, and protected the poor from the usury of the rich.
After that, many governments ruled by special codes of written laws. The oldest such document still known to exist seems to be the Code of Ur-Nammu of Ur ("ca" 2050 BC). Some of the better-known ancient law codes include the code of Lipit-Ishtar of Isin, the code of Hammurabi of Babylonia, the Hittite code, the Assyrian code and Mosaic law.
In 621 BC a scribe named Draco codified the cruel oral laws of the city-state of Athens; this code prescribed the death penalty for many offences (nowadays very severe rules are often called "Draconian"). In 594 BC Solon, the ruler of Athens, created the new "Solonian Constitution". It eased the burden of the workers, and determined that membership of the ruling class was to be based on wealth (plutocracy), rather than by birth (aristocracy). Cleisthenes again reformed the Athenian constitution and set it on a democratic footing in 508 BC.
Aristotle ("ca" 350 BC) was the first to make a formal distinction between ordinary law and constitutional law, establishing ideas of constitution and constitutionalism, and attempting to classify different forms of constitutional government. The most basic definition he used to describe a constitution in general terms was "the arrangement of the offices in a state". In his works "Constitution of Athens", "Politics", and "Nicomachean Ethics" he explores different constitutions of his day, including those of Athens, Sparta, and Carthage. He classified both what he regarded as good and what he regarded as bad constitutions, and came to the conclusion that the best constitution was a mixed system, including monarchic, aristocratic, and democratic elements. He also distinguished between citizens, who had the right to participate in the state, and non-citizens and slaves, who did not.
The Romans first codified their constitution in 450 BC as the "Twelve Tables". They operated under a series of laws that were added from time to time, but Roman law was never reorganised into a single code until the "Codex Theodosianus" (AD 438); later, in the Eastern Empire the "Codex repetitæ prælectionis" (534) was highly influential throughout Europe. This was followed in the east by the "Ecloga" of Leo III the Isaurian (740) and the "Basilica" of Basil I (878).
The "Edicts of Ashoka" established constitutional principles for the 3rd century BC Maurya king's rule in Ancient India. For constitutional principles almost lost to antiquity, see the code of Manu.
Many of the Germanic people that filled the power vacuum left by the Western Roman Empire in the Early Middle Ages codified their laws. One of the first of these Germanic law codes to be written was the Visigothic "Code of Euric" (471). This was followed by the "Lex Burgundionum", applying separate codes for Germans and for Romans; the "Pactus Alamannorum"; and the Salic Law of the Franks, all written soon after 500. In 506, the "Breviarum" or ""Lex Romana"" of Alaric II, king of the Visigoths, adopted and consolidated the "Codex Theodosianus" together with assorted earlier Roman laws. Systems that appeared somewhat later include the "Edictum Rothari" of the Lombards (643), the "Lex Visigothorum" (654), the "Lex Alamannorum" (730) and the "Lex Frisionum" ("ca" 785). These continental codes were all composed in Latin, while Anglo-Saxon was used for those of England, beginning with the Code of Æthelberht of Kent (602). In ca. 893, Alfred the Great combined this and two other earlier Saxon codes, with various Mosaic and Christian precepts, to produce the "Doom book" code of laws for England.
Dark ages and early Middle ages.
Many of the Germanic people that filled the power vacuum left by the Western Roman Empire in the Early Middle Ages codified their laws. One of the first of these Germanic law codes to be written was the Visigothic "Code of Euric" (471). This was followed by the "Lex Burgundionum", applying separate codes for Germans and for Romans; the "Pactus Alamannorum"; and the Salic Law of the Franks, all written soon after 500. In 506, the "Breviarum" or ""Lex Romana"" of Alaric II, king of the Visigoths, adopted and consolidated the "Codex Theodosianus" together with assorted earlier Roman laws. Systems that appeared somewhat later include the "Edictum Rothari" of the Lombards (643), the "Lex Visigothorum" (654), the "Lex Alamannorum" (730) and the "Lex Frisionum" ("ca" 785). These continental codes were all composed in Latin, while Anglo-Saxon was used for those of England, beginning with the Code of Æthelberht of Kent (602). In ca. 893, Alfred the Great combined this and two other earlier Saxon codes, with various Mosaic and Christian precepts, to produce the "Doom book" code of laws for England.
Japan's "Seventeen-article constitution" written in 604, reportedly by Prince Shōtoku, is an early example of a constitution in Asian political history. Influenced by Buddhist teachings, the document focuses more on social morality than institutions of government "per se" and remains a notable early attempt at a government constitution.
The Constitution of Medina (, Ṣaḥīfat al-Madīna), also known as the Charter of Medina, was drafted by the Islamic prophet Muhammad after his flight (hijra to Yathrib where he became political leader. It constituted a formal agreement between Muhammad and all of the significant tribes and families of Yathrib (later known as Medina), including Muslims, Jews, and pagans. The document was drawn up with the explicit concern of bringing to an end the bitter intertribal fighting between the clans of the Aws (Aus) and Khazraj within Medina. To this effect it instituted a number of rights and responsibilities for the Muslim, Jewish, and pagan communities of Medina bringing them within the fold of one community—the Ummah.
The precise dating of the Constitution of Medina remains debated but generally scholars agree it was written shortly after the Hijra (622).
In Wales, the "Cyfraith Hywel" was codified by Hywel Dda c. 942–950.
Middle ages after 1000.
The "Pravda Yaroslava", originally combined by Yaroslav the Wise the Grand Prince of Kyiv, was granted to Great Novgorod around 1017, and in 1054 was incorporated into the "Ruska Pravda", that became the law for all of Kievan Rus. It survived only in later editions of the 15th century.
In England, Henry I's proclamation of the Charter of Liberties in 1100 bound the king for the first time in his treatment of the clergy and the nobility. This idea was extended and refined by the English barony when they forced King John to sign "Magna Carta" in 1215. The most important single article of the "Magna Carta", related to ""habeas corpus"", provided that the king was not permitted to imprison, outlaw, exile or kill anyone at a whim—there must be due process of law first. This article, Article 39, of the "Magna Carta" read:
"No free man shall be arrested, or imprisoned, or deprived of his property, or outlawed, or exiled, or in any way destroyed, nor shall we go against him or send against him, unless by legal judgement of his peers, or by the law of the land."
This provision became the cornerstone of English liberty after that point. The social contract in the original case was between the king and the nobility, but was gradually extended to all of the people. It led to the system of Constitutional Monarchy, with further reforms shifting the balance of power from the monarchy and nobility to the House of Commons.
The Nomocanon of Saint Sava () was the first Serbian constitution from 1219. This legal act was well developed. St. Sava's Nomocanon was the compilation of Civil law, based on Roman Law and Canon law, based on Ecumenical Councils and its basic purpose was to organize functioning of the young Serbian kingdom and the Serbian church. Saint Sava began the work on the Serbian Nomocanon in 1208 while being at Mount Athos, using "The Nomocanon in Fourteen Titles", "Synopsis of Stefan the Efesian", "Nomocanon of John Scholasticus", Ecumenical Councils' documents, which he modified with the canonical commentaries of Aristinos and John Zonaras, local church meetings, rules of the Holy Fathers, the law of Moses, translation of Prohiron and the Byzantine emperors' Novellae (most were taken from Justinian's Novellae). The Nomocanon was completely new compilation of civil and canonical regulations, taken from the Byzantine sources, but completed and reformed by St. Sava to function properly in Serbia. Beside decrees that organized the life of church, there are various norms regarding civil life, most of them were taken from Prohiron. Legal transplants of Roman-Byzantine law became the basis of the Serbian medieval law. The essence of Zakonopravilo was based on Corpus Iuris Civilis.
Stefan Dušan, Emperor of Serbs and Greeks, enacted Dušan's Code () in Serbia, in two state congresses: in 1349 in Skopje and in 1354 in Serres. It regulated all social spheres, so it was the second Serbian constitution, after St. Sava's Nomocanon (Zakonopravilo). The Code was based on Roman-Byzantine law. The legal transplanting is notable with the articles 171 and 172 of Dušan's Code, which regulated the juridical independence. They were taken from the Byzantine code Basilika (book VII, 1, 16–17).
In 1222, Hungarian King Andrew II issued the Golden Bull of 1222.
Between 1220 and 1230, a Saxon administrator, Eike von Repgow, composed the "Sachsenspiegel", which became the supreme law used in parts of Germany as late as 1900.
In 1998, S. Kouyaté reconstructed from oral tradition what he claims is a 14th-century charter of the Mali Empire, called the "Kouroukan Fouga".
Around 1240, the Coptic Egyptian Christian writer, 'Abul Fada'il Ibn al-'Assal, wrote the "Fetha Negest" in Arabic. 'Ibn al-Assal took his laws partly from apostolic writings and Mosaic law, and partly from the former Byzantine codes. There are a few historical records claiming that this law code was translated into Ge'ez and entered Ethiopia around 1450 in the reign of Zara Yaqob. Even so, its first recorded use in the function of a constitution (supreme law of the land) is with Sarsa Dengel beginning in 1563. The "Fetha Negest" remained the supreme law in Ethiopia until 1931, when a modern-style Constitution was first granted by Emperor Haile Selassie I.
The Golden Bull of 1356 was a decree issued by a "Reichstag" in Nuremberg headed by Emperor Charles IV that fixed, for a period of more than four hundred years, an important aspect of the constitutional structure of the Holy Roman Empire.
In China, the Hongwu Emperor created and refined a document he called "Ancestral Injunctions" (first published in 1375, revised twice more before his death in 1398). These rules served in a very real sense as a constitution for the Ming Dynasty for the next 250 years.
In Catalonia, the Catalan constitutions were promulgated by the court from 1283 until 1716, when Philip V of Spain gave the Nueva Planta decrees, finishing with the historical laws of Catalonia. These Constitutions were usually made as a royal initiative, but required the favorable vote of the Catalan Courts, the medieval antecedent of the modern Parliaments. These laws had, as the other modern constitutions, preeminence over other laws, and they could not be contradicted by mere decrees or edicts of the king.
The oldest written document still governing a sovereign nation today is that of San Marino. The "Leges Statutae Republicae Sancti Marini" was written in Latin and consists of six books. The first book, with 62 articles, establishes councils, courts, various executive officers and the powers assigned to them. The remaining books cover criminal and civil law, judicial procedures and remedies. Written in 1600, the document was based upon the "Statuti Comunali" (Town Statute) of 1300, itself influenced by the "Codex Justinianus", and it remains in force today.
In 1392 the "Carta de Logu" was legal code of the Giudicato of Arborea promulgated by the "giudicessa" Eleanor. It was in force in Sardinia until it was superseded by the code of Charles Felix in April 1827. The Carta was a work of great importance in Sardinian history. It was an organic, coherent, and systematic work of legislation encompassing the civil and penal law.
Iroquois "Great Law of Peace".
The "Gayanashagowa", the oral constitution of the Iroquois nation also known as the Great Law of Peace, established a system of governance in which sachems (tribal chiefs) of the members of the Iroquois League made decisions on the basis of universal consensus of all chiefs following discussions that were initiated by a single tribe. The position of sachem descended through families, and were allocated by senior female relatives.
Historians including Donald Grinde, Bruce Johansen and others believe that the Iroquois constitution provided inspiration for the United States Constitution and in 1988 was recognised by a resolution in Congress. The thesis is not considered credible by some scholars. Stanford University historian Jack N. Rakove stated that "The voluminous records we have for the constitutional debates of the late 1780s contain no significant references to the Iroquois" and stated that there are ample European precedents to the democratic institutions of the United States. Francis Jennings noted that the statement made by Benjamin Franklin frequently quoted by proponents of the thesis does not support this idea as it is advocating for a union against these "ignorant savages" and called the idea "absurd". Bruce Johansen contends Jennings, Tooker etc. have "humorlessly missed the ironic nature of Franklin's statement" and persist in "ignoring the relevant sources". Anthropologist Dean Snow stated that though Franklin's Albany Plan may have drawn some inspiration from the Iroquois League, there is little evidence that either the Plan or the Constitution drew substantially from this source and argues that "...such claims muddle and denigrate the subtle and remarkable features of Iroquois government. The two forms of government are distinctive and individually remarkable in conception."
Modern constitutions.
In 1639, the Colony of Connecticut adopted the Fundamental Orders, which was the first North American constitution, and is the basis for every new Connecticut constitution since, and is also the reason for Connecticut's nickname, "the Constitution State".
The English Protectorate that was set up by Oliver Cromwell after the English Civil War promulgated the first detailed written constitution adopted by a modern state; it was called the Instrument of Government. This formed the basis of government for the short lived republic from 1653 to 1657 by providing a legal rationale for the increasing power of Cromwell, after Parliament consistently failed to govern effectively. Most of the concepts and ideas embedded into modern constitutional theory, especially bicameralism, separation of powers, the written constitution, and judicial review, can be traced back to the experiments of that period.
Drafted by Major-General John Lambert in 1653, the "Instrument of Government" included elements incorporated from an earlier document "Heads of Proposals", which had been agreed to by the Army Council in 1647, as a set of propositions intended to be a basis for a constitutional settlement after King Charles I was defeated in the First English Civil War. Charles had rejected the propositions, but before the start of the Second Civil War, the Grandees of the New Model Army had presented the "Heads of Proposals" as their alternative to the more radical Agreement of the People presented by the Agitators and their civilian supporters at the Putney Debates.
On January 4, 1649 the Rump Parliament declared "that the people are, under God, the original of all just power; that the Commons of England, being chosen by and representing the people, have the supreme power in this nation".
The "Instrument of Government" was adopted by Parliament on December 15, 1653 and Oliver Cromwell was installed as Lord Protector on the following day. The constitution set up a state council consisting of 21 members while executive authority was vested in the office of "Lord Protector of the Commonwealth"; this position was designated as a non-hereditary life appointment. It also required the calling of triennial Parliaments, with each sitting for at least five months.
The "Instrument of Government" was replaced in May 1657 by England's second, and last, codified constitution, the Humble Petition and Advice, proposed by Sir Christopher Packe. The Petition offered hereditary monarchy to Oliver Cromwell, asserted Parliament's control over issuing new taxation, provided an independent council to advise the king and safeguarded 'Triennial' meetings of Parliament. A modified version of the Humble Petition with the clause on kingship removed was ratified on 25 May. This finally met its demise in conjunction with the death of Cromwell and the Restoration of the monarchy.
"Agreements and Constitutions of Laws and Freedoms of the Zaporizian Host" was written in 1710 by Pylyp Orlyk, "hetman" of the Zaporozhian Host. It was written to establish a free Zaporozhian-Ukrainian Republic, with the support of Charles XII of Sweden. It is notable in that it established a democratic standard for the separation of powers in government between the legislative, executive, and judiciary branches, well before the publication of Montesquieu's "Spirit of the Laws". This Constitution also limited the executive authority of the "hetman", and established a democratically elected Cossack parliament called the General Council. However, Orlyk's project for an independent Ukrainian State never materialized, and his constitution, written in exile, never went into effect.
Other examples of European constitutions of this era were the Corsican Constitution of 1755 and the Swedish Constitution of 1772.
All of the British colonies in North America that were to become the 13 original United States, adopted their own constitutions in 1776 and 1777, during the American Revolution (and before the later Articles of Confederation and United States Constitution), with the exceptions of Massachusetts, Connecticut and Rhode Island. The Commonwealth of Massachusetts adopted its Constitution in 1780, the oldest still-functioning constitution of any U.S. state; while Connecticut and Rhode Island officially continued to operate under their old colonial charters, until they adopted their first state constitutions in 1818 and 1843, respectively.
Democratic constitutions.
What is sometimes called the ""enlightened constitution"" model was developed by philosophers of the Age of Enlightenment such as Thomas Hobbes, Jean-Jacques Rousseau, and John Locke. The model proposed that constitutional governments should be stable, adaptable, accountable, open and should represent the people (i.e., support democracy).
The United States Constitution, ratified June 21, 1788, was influenced by the writings of Polybius, Locke, Montesquieu, and others. The document became a benchmark for republicanism and codified constitutions written thereafter.
The Polish–Lithuanian Commonwealth Constitution was passed on May 3, 1791. Another landmark document was the French Constitution, ratified on September 3, 1791.
On March 19, the Spanish Constitution of 1812 was ratified by a parliament gathered in Cadiz, the only Spanish continental city which was safe from French occupation. The Spanish Constitution served as a model for other liberal constitutions of several South-European and Latin American nations like, for example, Portuguese Constitution of 1822, constitutions of various Italian states during Carbonari revolts (i.e., in the Kingdom of the Two Sicilies), the Norwegian constitution of 1814, or the Mexican Constitution of 1824.
In Brazil, the Constitution of 1824 expressed the option for the monarchy as political system after Brazilian Independence. The leader of the national emancipation process was the Portuguese prince Pedro I, elder son of the king of Portugal. Pedro was crowned in 1822 as first emperor of Brazil. The country was ruled by Constitutional monarchy until 1889, when finally adopted the Republican model.
In Denmark, as a result of the Napoleonic Wars, the absolute monarchy lost its personal possession of Norway to another absolute monarchy, Sweden. However the Norwegians managed to infuse a radically democratic and liberal constitution in 1814, adopting many facets from the American constitution and the revolutionary French ones; but maintaining a hereditary monarch limited by the constitution, like the Spanish one.
The first Swiss Federal Constitution was put in force in September 1848 (with official revisions in 1878, 1891, (1949. 1971, 1982) and 1999).
The Serbian revolution initially led to a proclamation of a proto-constitution in 1811; the full-fledged Constitution of Serbia followed few decades later, in 1835. The first Serbian constitution (Sretenjski ustav) was adopted at the national assembly in Kragujevac on February 15 in 1835.
The Constitution of Canada came into force on July 1, 1867 as the British North America Act, an act of the British Parliament. Over a century later, the BNA Act was patriated to the Canadian Parliament and augmented with the Canadian Charter of Rights and Freedoms. Apart from the "Constitution Acts, 1867 to 1982", Canada's constitution also has unwritten elements based in common law and convention.
Principles of constitutional design.
After tribal people first began to live in cities and establish nations, many of these functioned according to unwritten customs, while some developed autocratic, even tyrannical monarchs, who ruled by decree, or mere personal whim. Such rule led some thinkers to take the position that what mattered was not the design of governmental institutions and operations, as much as the character of the rulers. This view can be seen in Plato, who called for rule by "philosopher-kings." Later writers, such as Aristotle, Cicero and Plutarch, would examine designs for government from a legal and historical standpoint.
The Renaissance brought a series of political philosophers who wrote implied criticisms of the practices of monarchs and sought to identify principles of constitutional design that would be likely to yield more effective and just governance from their viewpoints. This began with revival of the Roman law of nations concept and its application to the relations among nations, and they sought to establish customary "laws of war and peace" to ameliorate wars and make them less likely. This led to considerations of what authority monarchs or other officials have and don't have, from where that authority derives, and the remedies for the abuse of such authority.
A seminal juncture in this line of discourse arose in England from the Civil War, the Cromwellian Protectorate, the writings of Thomas Hobbes, Samuel Rutherford, the Levellers, John Milton, and James Harrington, leading to the debate between Robert Filmer, arguing for the divine right of monarchs, on the one side, and on the other, Henry Neville, James Tyrrell, Algernon Sidney, and John Locke. What arose from the latter was a concept of government being erected on the foundations of first, a state of nature governed by natural laws, then a state of society, established by a social contract or compact, which bring underlying natural or social laws, before governments are formally established on them as foundations.
Along the way several writers examined how the design of government was important, even if the government were headed by a monarch. They also classified various historical examples of governmental designs, typically into democracies, aristocracies, or monarchies, and considered how just and effective each tended to be and why, and how the advantages of each might be obtained by combining elements of each into a more complex design that balanced competing tendencies. Some, such as Montesquieu, also examined how the functions of government, such as legislative, executive, and judicial, might appropriately be separated into branches. The prevailing theme among these writers was that the design of constitutions is not completely arbitrary or a matter of taste. They generally held that there are underlying principles of design that constrain all constitutions for every polity or organization. Each built on the ideas of those before concerning what those principles might be.
The later writings of Orestes Brownson would try to explain what constitutional designers were trying to do. According to Brownson there are, in a sense, three "constitutions" involved: The first the "constitution of nature" that includes all of what was called "natural law." The second is the "constitution of society", an unwritten and commonly understood set of rules for the society formed by a social contract before it establishes a government, by which it establishes the third, a "constitution of government". The second would include such elements as the making of decisions by public conventions called by public notice and conducted by established rules of procedure. Each constitution must be consistent with, and derive its authority from, the ones before it, as well as from a historical act of society formation or constitutional ratification. Brownson argued that a state is a society with effective dominion over a well-defined territory, that consent to a well-designed constitution of government arises from presence on that territory, and that it is possible for provisions of a written constitution of government to be "unconstitutional" if they are inconsistent with the constitutions of nature or society. Brownson argued that it is not ratification alone that makes a written constitution of government legitimate, but that it must also be competently designed and applied.
Other writers have argued that such considerations apply not only to all national constitutions of government, but also to the constitutions of private organizations, that it is not an accident that the constitutions that tend to satisfy their members contain certain elements, as a minimum, or that their provisions tend to become very similar as they are amended after experience with their use. Provisions that give rise to certain kinds of questions are seen to need additional provisions for how to resolve those questions, and provisions that offer no course of action may best be omitted and left to policy decisions. Provisions that conflict with what Brownson and others can discern are the underlying "constitutions" of nature and society tend to be difficult or impossible to execute, or to lead to unresolvable disputes.
Constitutional design has been treated as a kind of metagame in which play consists of finding the best design and provisions for a written constitution that will be the rules for the game of government, and that will be most likely to optimize a balance of the utilities of justice, liberty, and security. An example is the metagame Nomic.
Governmental constitutions.
Most commonly, the term "constitution" refers to a set of rules and principles that define the nature and extent of government. Most constitutions seek to regulate the relationship between institutions of the state, in a basic sense the relationship between the executive, legislature and the judiciary, but also the relationship of institutions within those branches. For example, executive branches can be divided into a head of government, government departments/ministries, executive agencies and a civil service/administration. Most constitutions also attempt to define the relationship between individuals and the state, and to establish the broad rights of individual citizens. It is thus the most basic law of a territory from which all the other laws and rules are hierarchically derived; in some territories it is in fact called "Basic Law".
Key features.
The following are features of democratic constitutions that have been identified by political scientists to exist, in one form or another, in virtually all national constitutions.
Codification.
A fundamental classification is codification or lack of codification. A codified constitution is one that is contained in a single document, which is the single source of constitutional law in a state. An uncodified constitution is one that is not contained in a single document, consisting of several different sources, which may be written or unwritten; see constitutional convention.
Codified constitution.
Most states in the world have codified constitutions.
Codified constitutions are often the product of some dramatic political change, such as a revolution. The process by which a country adopts a constitution is closely tied to the historical and political context driving this fundamental change. The legitimacy (and often the longevity) of codified constitutions has often been tied to the process by which they are initially adopted and some scholars have pointed out that high constitutional turnover within a given country may itself be detrimental to separation of powers and the rule of law.
States that have codified constitutions normally give the constitution supremacy over ordinary statute law. That is, if there is any conflict between a legal statute and the codified constitution, all or part of the statute can be declared "ultra vires" by a court, and struck down as unconstitutional. In addition, exceptional procedures are often required to amend a constitution. These procedures may include: convocation of a special constituent assembly or constitutional convention, requiring a supermajority of legislators' votes, the consent of regional legislatures, a referendum process, and/or other procedures that make amending a constitution more difficult than passing a simple law.
Constitutions may also provide that their most basic principles can never be abolished, even by amendment. In case a formally valid amendment of a constitution infringes these principles protected against any amendment, it may constitute a so-called "unconstitutional constitutional law".
Codified constitutions normally consist of a ceremonial preamble, which sets forth the goals of the state and the motivation for the constitution, and several articles containing the substantive provisions. The preamble, which is omitted in some constitutions, may contain a reference to God and/or to fundamental values of the state such as liberty, democracy or human rights. In ethnic nation-states such as Estonia, the mission of the state can be defined as preserving a specific nation, language and culture.
Uncodified constitution.
, only two sovereign states have uncodified constitutions, namely New Zealand and the United Kingdom. The Basic Laws of Israel are arguably its equivalent to a constitution.
Uncodified constitutions are the product of an "evolution" of laws and conventions over centuries. By contrast to codified constitutions (in the Westminster System that originated in England), uncodified constitutions include written sources: e.g. constitutional statutes enacted by the Parliament and also unwritten sources: constitutional conventions, observation of precedents, royal prerogatives, custom and tradition, such as always holding the General Election on Thursdays; together these constitute the British constitutional law.
Written versus unwritten; codified versus uncodified.
Some constitutions are largely, but not wholly, codified. For example, in the Constitution of Australia, most of its fundamental political principles and regulations concerning the relationship between branches of government, and concerning the government and the individual are codified in a single document, the Constitution of the Commonwealth of Australia. However, the presence of statutes with constitutional significance, namely the Statute of Westminster, as adopted by the Commonwealth in the Statute of Westminster Adoption Act 1942, and the Australia Act 1986 means that Australia's constitution is not contained in a single constitutional document. It means the Constitution of Australia is uncodified, it also contain constitutional conventions, thus is partially unwritten.
The Constitution of Canada, which evolved from the British North America Acts until severed from nominal British control by the Canada Act 1982 (analogous to the Australia Act 1986), is a similar example. Canada's constitution consists of almost 30 different statutes.
The terms "written constitution" and "codified constitution" are often used interchangeably, as are "unwritten constitution" and "uncodified constitution", although this usage is technically inaccurate. A codified constitution is a written constitution contained in a single document, states that do not have such a document have uncodified constitutions but not entirely unwritten constitutions since much of an uncodified constitution is usually written in laws, such as the Basic Laws of Israel or the Parliament Acts of the United Kingdom.
Entrenchment.
The presence or lack of entrenchment is a fundamental feature of constitutions. An entrenched constitution cannot be altered in any way by a legislature as part of its normal business concerning ordinary statutory laws, but can only be amended by a different and more onerous procedure. There may be a requirement for a special body to be set up, or the proportion of favourable votes of members of existing legislative bodies may be required to be higher to pass a constitutional amendment than for statutes. The entrenched clauses of a constitution can create different degrees of entrenchment, ranging from simply excluding constitutional amendment from the normal business of a legislature, to making certain amendments either more difficult than normal modifications, or forbidden under any circumstances.
Entrenchment is an inherent feature in most codified constitutions. A codified constitution will incorporate the rules which must be followed for the constitution itself to be changed.
The US constitution is an example of an entrenched constitution, and the UK constitution is an example of a constitution that is not entrenched (or codified). In some states the text of the constitution may be changed; in others the original text is not changed, and amendments are passed which add to and may override the original text and earlier amendments.
Procedures for constitutional amendment vary between states. In a nation with a federal system of government the approval of a majority of state or provincial legislatures may be required. Alternatively, a national referendum may be required. Details are to be found in the articles on the constitutions of the various nations and federal states in the world.
In constitutions that are not entrenched, no special procedure is required for modification. Lack of entrenchment is a characteristic of uncodified constitutions; the constitution is not recognised with any higher legal status than ordinary statutes. In the UK, for example laws which modify written or unwritten provisions of the constitution are passed on a simple majority in Parliament. No special "constitutional amendment" procedure is required. The principle of parliamentary sovereignty holds that no sovereign parliament may be bound by the acts of its predecessors; and there is no higher authority that can create law which binds Parliament. The sovereign is nominally the head of state with important powers, such as the power to declare war; the uncodified and unwritten constitution removes all these powers in practice.
In practice democratic governments do not use the lack of entrenchment of the constitution to impose the will of the government or abolish all civil rights, as they could in theory do, but the distinction between constitutional and other law is still somewhat arbitrary, usually following historical principles embodied in important past legislation. For example, several British Acts of Parliament such as the Bill of Rights, Human Rights Act and, prior to the creation of Parliament, Magna Carta are regarded as granting fundamental rights and principles which are treated as almost constitutional. Several rights that in another state might be guaranteed by constitution have indeed been abolished or modified by the British parliament in the early 21st century, including the unconditional right to trial by jury, the right to silence without prejudicial inference, permissible detention before a charge is made extended from 24 hours to 42 days, and the right not to be tried twice for the same offence.
Absolutely unmodifiable articles.
The strongest level of entrenchment exists in those constitutions that state that some of their most fundamental principles are absolute, i.e. certain articles may not be amended under any circumstances. An amendment of a constitution that is made consistently with that constitution, except that it violates the absolute non-modifiability, can be called an "unconstitutional constitutional law". Ultimately it is always possible for a constitution to be overthrown by internal or external force, for example, a revolution (perhaps claiming to be justified by the right to revolution) or invasion.
In the Constitution of India, the Supreme Court has created the Doctrine of Basic Structure in Kesavananda Bharti's case (1973) stating that the essential features of the Basic structure cannot be amended by the Parliament. The Court has identified judicial review, independence of Judiciary, free and fair election, core of Fundamental Rights as a few of the essential features which are unamendable. However, the Supreme Court did not identify specific provisions which are in the category of absolute entrenchment. A critical analysis of the Doctrine of Basic Structure appears in Professor M.K. Bhandari's book "Basic Structure of Indian Constitution - A Critical Reconsideration".
An example of absolute unmodifiability is found in the German constitution. Articles 1 and 20 protect human dignity, human rights, democracy, rule of law, federal and social state principles, and the people's right of resistance as a last resort against an attempt to abolish the constitutional order. Article 79, Section 3 states that these principles cannot be changed, even according to the methods of amendment defined elsewhere in the document, until a new constitution comes into effect.
Another example is the Constitution of Honduras, which has an article stating that the article itself and certain other articles cannot be changed in any circumstances. Article 374 of the Honduras Constitution asserts this unmodifiability, stating, "It is not possible to reform, in any case, the preceding article, the present article, the constitutional articles referring to the form of government, to the national territory, to the presidential period, the prohibition to serve again as President of the Republic, the citizen who has performed under any title in consequence of which she/he cannot be President of the Republic in the subsequent period." This unmodifiability article played an important role in the 2009 Honduran constitutional crisis.
Distribution of sovereignty.
Constitutions also establish where sovereignty is located in the state. There are three basic types of distribution of sovereignty according to the degree of centralisation of power: unitary, federal, and confederal. The distinction is not absolute.
In a unitary state, sovereignty resides in the state itself, and the constitution determines this. The territory of the state may be divided into regions, but they are not sovereign and are subordinate to the state. In the UK, the constitutional doctrine of Parliamentary sovereignty dictates than sovereignty is ultimately contained at the centre. Some powers have been devolved to Northern Ireland, Scotland, and Wales (but not England). Some unitary states (Spain is an example) devolve more and more power to sub-national governments until the state functions in practice much like a federal state.
A federal state has a central structure with at most a small amount of territory mainly containing the institutions of the federal government, and several regions (called "states", "provinces", etc.) which compose the territory of the whole state. Sovereignty is divided between the centre and the constituent regions. The constitutions of Canada and the United States establish federal states, with power divided between the federal government and the provinces or states. Each of the regions may in turn have its own constitution (of unitary nature).
A confederal state comprises again several regions, but the central structure has only limited coordinating power, and sovereignty is located in the regions. Confederal constitutions are rare, and there is often dispute to whether so-called "confederal" states are actually federal.
To some extent a group of states which do not constitute a federation as such may by treaties and accords give up parts of their sovereignty to a supranational entity. For example, the countries constituting the European Union have agreed to abide by some Union-wide measures which restrict their absolute sovereignty in some ways, e.g., the use of the metric system of measurement instead of national units previously used.
Separation of powers.
Constitutions usually explicitly divide power between various branches of government. The standard model, described by the Baron de Montesquieu, involves three branches of government: executive, legislative and judicial. Some constitutions include additional branches, such as an auditory branch. Constitutions vary extensively as to the degree of separation of powers between these branches.
Lines of accountability.
In presidential and semi-presidential systems of government, department secretaries/ministers are accountable to the president, who has patronage powers to appoint and dismiss ministers. The president is accountable to the people in an election.
In parliamentary systems, Cabinet Ministers are accountable to Parliament, but it is the prime minister who appoints and dismisses them. In the case of the United Kingdom and other countries with a Monarchy, it is the Monarch who appoints and dismisses ministers, on the advice of the Prime Minister. In turn the prime minister will resign if the government loses the confidence of the parliament (or a part of it). Confidence can be lost if the government loses a vote of no confidence or, depending on the country
, loses a particularly important vote in parliament such as vote on the budget. When a government loses confidence it stays in office until a new government is formed; something which normally but not necessarily required the holding of a general election.
State of emergency.
Many constitutions allow the declaration under exceptional circumstances of some form of state of emergency during which some rights and guarantees are suspended. This deliberate Manifestation can be and has been abused to allow a government to suppress dissent without regard for human rights—see the article on state of emergency.
Facade constitutions.
Italian political theorist Giovanni Sartori noted the existence of national constitutions which are a facade for authoritarian sources of power. While such documents may express respect for human rights or establish an independent judiciary, they may be ignored when the government feels threatened, or never put into practice. An extreme example was the Constitution of the Soviet Union that on paper supported freedom of assembly and freedom of speech; however, citizens who transgressed unwritten limits were summarily imprisoned. The example demonstrates that the protections and benefits of a constitution are ultimately provided not through its written terms but through deference by government and society to its principles. A constitution may change from being real to a facade and back again as democratic and autocratic governments succeed each other.
Constitutional courts.
Constitutions are often, but by no means always, protected by a legal body whose job it is to interpret those constitutions and, where applicable, declare void executive and legislative acts which infringe the constitution. In some countries, such as Germany, this function is carried out by a dedicated constitutional court which performs this (and only this) function. In other countries, such as Ireland, the ordinary courts may perform this function in addition to their other responsibilities. While elsewhere, like in the United Kingdom, the concept of declaring an act to be unconstitutional does not exist.
A constitutional violation is an action or legislative act that is judged by a constitutional court to be contrary to the constitution, that is, unconstitutional. An example of constitutional violation by the executive could be a public office holder who acts outside the powers granted to that office by a constitution. An example of constitutional violation by the legislature is an attempt to pass a law that would contradict the constitution, without first going through the proper constitutional amendment process.
Some countries, mainly those with uncodified constitutions, have no such courts at all. For example, the United Kingdom has traditionally operated under the principle of parliamentary sovereignty under which the laws passed by United Kingdom Parliament could not be questioned by the courts.
See also.
"Judicial philosophies of constitutional interpretation (note: generally specific to United States constitutional law)"

</doc>
<doc id="5254" url="https://en.wikipedia.org/wiki?curid=5254" title="Common law">
Common law

Common law (also known as case law or precedent) is law developed by judges, courts, and similar tribunals, stated in decisions that nominally decide individual cases but that in addition have precedential effect on future cases. Common law is a third branch of law, in contrast to and on equal footing with statutes which are adopted through the legislative process, and regulations which are promulgated by the executive branch.
A "common law system" is a legal system that gives great precedential weight to common law, so that consistent principles applied to similar facts yield similar outcomes. The body of past common law binds judges that make future decisions, just as any other law does, to ensure consistent treatment. In cases where the parties disagree on what the law is, a common law court looks to past precedential decisions of relevant courts. If a similar dispute has been resolved in the past, the court is usually bound to follow the reasoning used in the prior decision (this principle is known as "stare decisis"). If, however, the court finds that the current dispute is fundamentally distinct from all previous cases (called a "matter of first impression"), judges have the authority and duty to make law by creating precedent. Thereafter, the new decision becomes precedent, and will bind future courts. "Stare decisis", the principle that cases should be decided according to consistent principled rules so that similar facts will yield similar results, lies at the heart of all common law systems.
One third of the world's population live in common law jurisdictions or in systems mixed with civil law. Common law originated during the Middle Ages in England, and from there was propagated to the colonies of the British Empire, including India, the United States (both the federal system and 49 of its 50 states), Pakistan, Nigeria, Bangladesh, Canada (and all its provinces except Quebec), Malaysia, Ghana, Australia, Sri Lanka, Hong Kong, Singapore, Burma, Ireland, New Zealand, Papua New Guinea, Jamaica, Trinidad and Tobago, Cyprus, Barbados, South Africa, Zimbabwe, Cameroon, Namibia, Liberia, Sierra Leone, Botswana, Guyana, and Fiji.
Primary connotations.
The term "common law" has three main connotations and several historical meanings worth mentioning:
Connotation 1. Common law as opposed to statutory law and regulatory law.
Connotation 1 distinguishes the authority that promulgated a law. For example, most areas of law in most Anglo-American jurisdictions include "statutory law" enacted by a legislature, "regulatory law" promulgated by executive branch agencies pursuant to delegation of rule-making authority from the legislature, and common law (connotation 1) or "case law", "i.e.", decisions issued by courts (or quasi-judicial tribunals within agencies). This first connotation can be further differentiated into
Connotation 2. Common law legal systems as opposed to civil law legal systems.
Connotation 2 differentiates "common law" jurisdictions and legal systems from "civil law" or "code" jurisdictions. Common law (connotation 2) systems place great weight on court decisions, which are considered "law" with the same force of law as statutes—for nearly a millennium, common law (connotation 2) courts have had the authority to make law where no legislative statute exists, and statutes mean what courts interpret them to mean.
By contrast, in civil law jurisdictions (the legal tradition that prevails, or is combined with common law, in Europe and most non-Islamic, non-common law countries), courts lack authority to act if there is no statute. Judicial precedent is given less interpretive weight, which means that a judge deciding a given case has more freedom to interpret the text of a statute independently, and less predictably, whereas scholarly literature is given more weight than in common law systems. For example, the Napoleonic code expressly forbade French judges to pronounce general principles of law.
As a rule of thumb, common law (connotation 2) systems trace their history to England, while civil law systems trace their history through the Napoleonic Code back to the Corpus Juris Civilis of Roman law.
The contrast between common law and civil law systems is elaborated in "Contrasts between common law and civil law systems" and "Alternatives to common law systems", below.
Connotation 3. Law as opposed to equity.
Connotation 3 differentiates "common law" (or just "law") from "equity". Before 1873, England had two parallel court systems: courts of "law" which could only award money damages and recognized only the legal owner of property, and courts of "equity" (courts of chancery) that could issue injunctive relief (that is, a court order to a party to do something, give something to someone, or stop doing something) and recognized trusts of property. This split propagated to many of the colonies, including the United States (see "Reception Statutes", below). For most purposes, most jurisdictions, including the U.S. federal system and most states, have merged the two courts. Additionally, even before the separate courts were merged, most courts were permitted to apply both law (connotation 3) and equity, though under potentially different procedural law. Nonetheless, the historical distinction between "law" (in connotation 3) and "equity" remains important today when the case involves issues such as the following:
Courts of equity rely on common law principles of binding precedent (connotation 1).
Connotation 4. Historical uses.
In addition, there are several historical uses of the term that provide some background as to its meaning.
In one archaic usage, "common law" refers to the pre-Christian system of law, imported by the Saxons to England, and dating to before the Norman conquest, and before there was any consistent law to be applied. This definition is found or alluded to in some internet dictionaries.
"Common law" as the term is used today in common law countries contrasts with "ius commune". While historically the "ius commune" became a secure point of reference in continental European legal systems, in England it was not a point of reference at all.
The English Court of Common Pleas dealt with lawsuits in which the Monarch had no interest, i.e., between commoners.
Additionally, from at least the 11th century and continuing for several centuries after that, there were several different circuits in the royal court system, served by itinerant judges who would travel from town to town dispensing the King's justice. The term "common law" was used to describe the law held in common between the circuits and the different stops in each circuit. The more widely a particular law was recognized, the more weight it held, whereas purely local customs were generally subordinate to law recognized in a plurality of jurisdictions.
These definitions are archaic, their relevance having dissipated with the development of the English legal system over the centuries, but they do explain the origin of the term as used today.
Basic principles of common law.
Common law adjudication.
In a common law jurisdiction several stages of research and analysis are required to determine "what the law is" in a given situation. First, one must ascertain the facts. Then, one must locate any relevant statutes and cases. Then one must extract the principles, analogies and statements by various courts of what they consider important to determine how the next court is likely to rule on the facts of the present case. Later decisions, and decisions of higher courts or legislatures carry more weight than earlier cases and those of lower courts. Finally, one integrates all the lines drawn and reasons given, and determines "what the law is". Then, one applies that law to the facts.
In practice, common law systems are considerably more complicated than the simplified system described above. The decisions of a court are binding only in a particular jurisdiction, and even within a given jurisdiction, some courts have more power than others. For example, in most jurisdictions, decisions by appellate courts are binding on lower courts in the same jurisdiction, and on future decisions of the same appellate court, but decisions of lower courts are only non-binding persuasive authority. Interactions between common law, constitutional law, statutory law and regulatory law also give rise to considerable complexity.
The common law evolves to meet changing social needs and improved understanding.
Justice Holmes cautioned that “the proper derivation of general principles in both common and constitutional law ... arise gradually, in the emergence of a consensus from a multitude of particularized prior decisions.” Justice Cardozo noted the “common law does not work from pre-established truths of universal and inflexible validity to conclusions derived from them deductively,” but “ts method is inductive, and it draws its generalizations from particulars.”
The common law (connotation 1) is more malleable than statutory law. First, common law courts are not absolutely bound by precedent, but can (when extraordinarily good reason is shown) reinterpret and revise the law, without legislative intervention, to adapt to new trends in political, legal and social philosophy. Second, the common law (connotation 1) evolves through a series of gradual steps, that gradually works out all the details, so that over a decade or more, the law can change substantially but without a sharp break, thereby reducing disruptive effects. In contrast to common law incrementalism, the legislative process is very difficult to get started, as legislatures tend to delay action until a situation is totally intolerable. For these reasons, legislative changes tend to be large, jarring and disruptive (sometimes positively, sometimes negatively, and sometimes with unintended consequences).
One example of the gradual change that typifies evolution of the common law (connotation 1) is the gradual change in liability for negligence. For example, the traditional common law rule through most of the 19th century was that a plaintiff could not recover for a defendant's negligent production or distribution of a harmful instrumentality unless the two were in privity of contract. Thus, only the immediate purchaser could recover for a product defect, and if a part was built up out of parts from parts manufacturers, the ultimate buyer could not recover for injury caused by a defect in the part. In an 1842 English case, "Winterbottom v. Wright", the postal service had contracted with Wright to maintain its coaches. Winterbottom was a driver for the post. When the coach failed and injured Winterbottom, he sued Wright. The "Winterbottom" court recognized that there would be "absurd and outrageous consequences" if an injured person could sue any person peripherally involved, and knew it had to draw a line somewhere, a limit on the causal connection between the negligent conduct and the injury. The court looked to the contractual relationships, and held that liability would only flow as far as the person in immediate contract ("privity") with the negligent party.
A first exception to this rule arose in an 1852 case by New York's highest court, "Thomas v. Winchester", which held that mislabeling a poison as an innocuous herb, and then selling the mislabeled poison through a dealer who would be expected to resell it, put "human life in imminent danger." "Thomas" used this as a reason to create an exception to the "privity" rule. In, 1909, New York held in "Statler v. Ray Mfg. Co." that a coffee urn manufacturer was liable to a person injured when the urn exploded, because the urn "was of such a character inherently that, when applied to the purposes for which it was designed, it was liable to become a source of great danger to many people if not carefully and properly constructed."
Yet the privity rule survived. In "Cadillac Motor Car Co. v. Johnson", (decided in 1915 by the federal appeals court for New York and several neighboring states), the court held that a car owner could not recover for injuries from a defective wheel, when the automobile owner had a contract only with the automobile dealer and not with the manufacturer, even though there was "no question that the wheel was made of dead and ‘dozy‘ wood, quite insufficient for its purposes." The "Cadillac" court was willing to acknowledge that the case law supported exceptions for "an article dangerous in its nature or likely to become so in the course of the ordinary usage to be contemplated by the vendor." However, held the "Cadillac" court, "one who manufactures articles dangerous only if defectively made, or installed, e.g., tables, chairs, pictures or mirrors hung on the walls, carriages, automobiles, and so on, is not liable to third parties for injuries caused by them, except in case of willful injury or fraud,"
Finally, in the famous case of "MacPherson v. Buick Motor Co.", in 1916, Judge Benjamin Cardozo for New York's highest court pulled a broader principle out of these predecessor cases. The facts were almost identical to "Cadillac" a year earlier: a wheel from a wheel manufacturer was sold to Buick, to a dealer, to MacPherson, and the wheel failed, injuring MacPherson. Judge Cardozo held:
Cardozo's new "rule" exists in no prior case, but is inferrable as a synthesis of the "thing of danger" principle stated in them, merely extending it to "foreseeable danger" even if "the purposes for which it was designed" were not themselves "a source of great danger." "MacPherson" takes some care to present itself as foreseeable progression, not a wild departure. Cardozo continues to adhere to the original principle of "Winterbottom", that "absurd and outrageous consequences" must be avoided, and he does so by drawing a new line in the last sentence quoted above: "There must be knowledge of a danger, not merely possible, but probable." But while adhering to the underlying principle that "some" boundary is necessary, "MacPherson" overruled the prior common law by rendering the formerly dominant factor in the boundary, that is, the privity formality arising out of a contractual relationship between persons, totally irrelevant. Rather, the most important factor in the boundary would be the nature of the thing sold and the foreseeable uses that downstream purchasers would make of the thing.
This illustrates two crucial principles that are often not well understood by non-lawyers. (a) The common law evolves, this evolution is in the hands of judges, and judges have "made law" for hundreds of years. (b) The reasons given for a decision are often more important in the long run than the outcome in a particular case. This is the reason that judicial opinions are usually quite long, and give rationales and policies that can be balanced with judgment in future cases, rather than the bright-line rules usually embodied in statutes.
Interaction of constitutional, statutory and common law.
In common law legal systems (connotation 2), the common law (connotation 1) is crucial to understanding almost all important areas of law. For example, in England and Wales, in English Canada, and in most states of the United States, the basic law of contracts, torts and property do not exist in statute, but only in common law (though there may be isolated modifications enacted by statute). As another example, the Supreme Court of the United States in 1877, held that a Michigan statute that established rules for solemnization of marriages did not abolish pre-existing common-law marriage, because the statute did not affirmatively require statutory solemnization and was silent as to preexisting common law.
In almost all areas of the law (even those where there is a statutory framework, such as contracts for the sale of goods, or the criminal law), legislature-enacted statutes generally give only terse statements of general principle, and the fine boundaries and definitions exist only in the interstitial common law (connotation 1(b)). To find out what the precise law is that applies to a particular set of facts, one has to locate precedential decisions on the topic, and reason from those decisions by analogy. To consider but one example, the First Amendment to the United States Constitution states "Congress shall make no law respecting an establishment of religion, or prohibiting the free exercise thereof"—but interpretation (that is, determining the fine boundaries, and resolving the tension between the "establishment" and "free exercise" clauses) of each of the important terms was delegated by Article III of the Constitution to the judicial branch, so that the current legal boundaries of the Constitutional text can only be determined by consulting interstitial common law (connotation 1(b)).
In common law jurisdictions (connotation 2), legislatures operate under the assumption that statutes will be interpreted against the backdrop of the pre-existing common law (connotation 1) and custom. For example, in most U.S. states, the criminal statutes are primarily codification of pre-existing common law. (Codification is the process of enacting a statute that collects and restates pre-existing law in a single document—when that pre-existing law is common law, the common law remains relevant to the interpretation of these statutes.) In reliance on this assumption, modern statutes often leave a number of terms and fine distinctions unstated—for example, a statute might be very brief, leaving the precise definition of terms unstated, under the assumption that these fine distinctions will be inherited from pre-existing common law. (For this reason, many modern American law schools teach the common law of crime as it stood in England in 1789, because that centuries-old English common law is a necessary foundation to interpreting modern criminal statutes.)
With the transition from English law, which had common law crimes, to the new legal system under the U.S. Constitution, which prohibited "ex post facto" laws at both the federal and state level, the question was raised whether there could be common law crimes in the United States. It was settled in the case of "United States v. Hudson and Goodwin", , which decided that federal courts had no jurisdiction to define new common law crimes, and that there must always be a (constitutional) statute defining the offense and the penalty for it.
Still, many states retain selected common law crimes. For example, in Virginia, the definition of the conduct that constitutes the crime of robbery exists only in the common law, and the robbery statute only sets the punishment. Virginia Code section 1-200 establishes the continued existence and vitality of common law principles and provides that "The common law of England, insofar as it is not repugnant to the principles of the Bill of Rights and Constitution of this Commonwealth, shall continue in full force within the same, and be the rule of decision, except as altered by the General Assembly."
By contrast to statutory codification of common law, some statutes displace common law, for example to create a new cause of action that did not exist in the common law, or to legislatively overrule the common law. An example is the tort of wrongful death, which allows certain persons, usually a spouse, child or estate, to sue for damages on behalf of the deceased. There is no such tort in English common law; thus, any jurisdiction that lacks a wrongful death statute will not allow a lawsuit for the wrongful death of a loved one. Where a wrongful death statute exists, the compensation or other remedy available is limited to the remedy specified in the statute (typically, an upper limit on the amount of damages). Courts generally interpret statutes that create new causes of action narrowly—that is, limited to their precise terms—because the courts generally recognize the legislature as being supreme in deciding the reach of judge-made law unless such statute should violate some "second order" constitutional law provision ("cf". judicial activism).
Where a tort is rooted in common law (connotation 1(a)), all traditionally recognized damages for that tort may be sued for, whether or not there is mention of those damages in the current statutory law. For instance, a person who sustains bodily injury through the negligence of another may sue for medical costs, pain, suffering, loss of earnings or earning capacity, mental and/or emotional distress, loss of quality of life, disfigurement and more. These damages need not be set forth in statute as they already exist in the tradition of common law. However, without a wrongful death statute, most of them are extinguished upon death.
In the United States, the power of the federal judiciary to review and invalidate unconstitutional acts of the federal executive branch is stated in the constitution, Article III sections 1 and 2: "The judicial Power of the United States, shall be vested in one supreme Court, and in such inferior Courts as the Congress may from time to time ordain and establish. ... The judicial Power shall extend to all Cases, in Law and Equity, arising under this Constitution, the Laws of the United States, and Treaties made, or which shall be made, under their Authority..." The first famous statement of "the judicial power" was "Marbury v. Madison", . Later cases interpreted the "judicial power" of Article III to establish the power of federal courts to consider or overturn any action of Congress or of any state that conflicts with the Constitution.
The interactions between decisions of different courts is discussed further in the article on precedent.
Overruling precedent—the limits of "stare decisis".
The United States federal courts are divided into twelve regional circuits, each with a circuit court of appeals (plus a thirteenth, the Court of Appeals for the Federal Circuit, which hears appeals in patent cases and cases against the federal government, without geographic limitation). Decisions of one circuit court are binding on the district courts within the circuit and on the circuit court itself, but are only persuasive authority on sister circuits. District court decisions are not binding precedent at all, only persuasive.
Most of the U.S. federal courts of appeal have adopted a rule under which, in the event of any conflict in decisions of panels (most of the courts of appeal almost always sit in panels of three), the earlier panel decision is controlling, and a panel decision may only be overruled by the court of appeals sitting "en banc" (that is, all active judges of the court) or by a higher court. In these courts, the older decision remains controlling when an issue comes up the third time.
Other courts, for example, the Court of Customs and Patent Appeals and the Supreme Court, always sit "en banc", and thus the "later" decision controls. These courts essentially overrule all previous cases in each new case, and older cases survive only to the extent they do not conflict with newer cases. The interpretations of these courts—for example, Supreme Court interpretations of the constitution or federal statutes—are stable only so long as the older interpretation maintains the support of a majority of the court. Older decisions persist through some combination of belief that the old decision is right, and that it is not sufficiently wrong to be overruled.
In the UK, since 2009, the Supreme Court of the United Kingdom has the authority to overrule and unify decisions of lower courts. From 1966 to 2009, this power lay with the House of Lords, granted by the Practice Statement of 1966.
Canada's system, described below, avoids regional variability of federal law by giving national jurisdiction to both layers of appellate courts.
Common law as a foundation for commercial economies.
The reliance on judicial opinion is a strength of common law systems, and is a significant contributor to the robust commercial systems in the United Kingdom and United States. Because there is reasonably precise guidance on almost every issue, parties (especially commercial parties) can predict whether a proposed course of action is likely to be lawful or unlawful, and have some assurance of consistency. As Justice Brandeis famously expressed it, “in most matters it is more important that the applicable rule of law be settled than that it be settled right.” This ability to predict gives more freedom to come close to the boundaries of the law. For example, many commercial contracts are more economically efficient, and create greater wealth, because the parties know ahead of time that the proposed arrangement, though perhaps close to the line, is almost certainly legal. Newspapers, taxpayer-funded entities with some religious affiliation, and political parties can obtain fairly clear guidance on the boundaries within which their freedom of expression rights apply.
In contrast, in non-common-law countries, and jurisdictions with very weak respect for precedent (example, the U.S. Patent Office), fine questions of law are redetermined anew each time they arise, making consistency and prediction more difficult, and procedures far more protracted than necessary because parties cannot rely on written statements of law as reliable guides. In jurisdictions that do not have a strong allegiance to a large body of precedent, parties have less "a priori" guidance and must often leave a bigger "safety margin" of unexploited opportunities, and final determinations are reached only after far larger expenditures on legal fees by the parties.
This is the reason for the frequent choice of the law of the State of New York in commercial contracts, even when neither entity has extensive contacts with New York—and remarkably often even when neither party has contacts with the United States. Commercial contracts almost always include a "choice of law clause" to reduce uncertainty. Somewhat surprisingly, contracts throughout the world (for example, contracts involving parties in Japan, France and Germany, and from most of the other states of the United States) often choose the law of New York, even where the relationship of the parties and transaction to New York is quite attenuated. Because of its history as the United States' commercial center, New York common law has a depth and predictability not (yet) available in any other jurisdictions of the United States. Similarly, American corporations are often formed under Delaware corporate law, and American contracts relating to corporate law issues (merger and acquisitions of companies, rights of shareholders, and so on.) include a Delaware choice of law clause, because of the deep body of law in Delaware on these issues. On the other hand, some other jurisdictions have sufficiently developed bodies of law so that parties have no real motivation to choose the law of a foreign jurisdiction (for example, England and Wales, and the state of California), but not yet so fully developed that parties with no relationship to the jurisdiction choose that law. Outside the United States, parties that are in different jurisdictions from each other often choose the law of England and Wales, particularly when the parties are each in former British colonies and members of the Commonwealth. The common theme in all cases is that commercial parties seek predictability and simplicity in their contractual relations, and frequently choose the law of a common law jurisdiction with a well-developed body of common law to achieve that result.
Likewise, for litigation of commercial disputes arising out of unpredictable torts (as opposed to the prospective choice of law clauses in contracts discussed in the previous paragraph), certain jurisdictions attract an unusually high fraction of cases, because of the predictability afforded by the depth of decided cases. For example, London is considered the pre-eminent centre for litigation of admiralty cases.
This is not to say that common law is better in every situation. For example, civil law can be clearer than case law when the legislature has had the foresight and diligence to address the precise set of facts applicable to a particular situation. For that reason, civil law statutes tend to be somewhat more detailed than statutes written by common law legislatures—but, conversely, that tends to make the statute more difficult to read (the United States tax code is an example). Nonetheless, as a practical matter, no civil law legislature can ever address the full spectrum of factual possibilities in the breadth, depth and detail of the case law of the common law courts of even a smaller jurisdiction, and that deeper, more complete body of law provides additional predictability that promotes commerce.
History.
The term "common law" originally derives from the 1150s and 1160s, when Henry II of England established the secular English tribunals. The "common law" was the law that emerged as "common" throughout the realm (as distinct from the various legal codes that preceded it, such as Mercian law, the Danelaw and the law of Wessex) as the king's judges followed each other's decisions to create a unified common law throughout England. The doctrine of precedent developed during the 12th and 13th centuries, as the collective judicial decisions that were based in tradition, custom and precedent.
The form of reasoning used in common law is known as casuistry or case-based reasoning. The common law, as applied in civil cases (as distinct from criminal cases), was devised as a means of compensating someone for wrongful acts known as torts, including both intentional torts and torts caused by negligence, and as developing the body of law recognizing and regulating contracts. The type of procedure practiced in common law courts is known as the adversarial system; this is also a development of the common law.
Medieval English common law.
In the late 9th century, Alfred the Great assembled the Doom book (not to be confused with the more-famous Domesday Book from 200 years later), which collected the existing laws of Kent, Wessex, and Mercia, and attempted to blend in the Mosaic code, Christian principles, and Germanic customs dating as far as the 5th century.
Before the Norman conquest in 1066, justice was administered primarily by what is today known as the county courts (the modern "counties" were referred to as "shires" in pre-Norman times), presided by the diocesan bishop and the sheriff, exercising both ecclesiastical and civil jurisdiction. Trial by jury began in these courts.
In 1154, Henry II became the first Plantagenet king. Among many achievements, Henry institutionalized common law by creating a unified system of law "common" to the country through incorporating and elevating local custom to the national, ending local control and peculiarities, eliminating arbitrary remedies and reinstating a jury system—citizens sworn on oath to investigate reliable criminal accusations and civil claims. The jury reached its verdict through evaluating common local knowledge, not necessarily through the presentation of evidence, a distinguishing factor from today's civil and criminal court systems.
Henry II developed the practice of sending judges from his own central court to hear the various disputes throughout the country. His judges would resolve disputes on an ad hoc basis according to what they interpreted the customs to be. The king's judges would then return to London and often discuss their cases and the decisions they made with the other judges. These decisions would be recorded and filed. In time, a rule, known as "stare decisis" (also commonly known as precedent) developed, whereby a judge would be bound to follow the decision of an earlier judge; he was required to adopt the earlier judge's interpretation of the law and apply the same principles promulgated by that earlier judge if the two cases had similar facts to one another. Once judges began to regard each other's decisions to be binding precedent, the pre-Norman system of local customs and law varying in each locality was replaced by a system that was (at least in theory, though not always in practice) common throughout the whole country, hence the name "common law."
Henry II's creation of a powerful and unified court system, which curbed somewhat the power of canonical (church) courts, brought him (and England) into conflict with the church, most famously with Thomas Becket, the Archbishop of Canterbury. Eventually, Becket was murdered inside Canterbury Cathedral by four knights who believed themselves to be acting on Henry's behalf. Whether Henry actually intended to bring about the assassination of Becket is debatable, but there is no question that at the time of the murder, the two men were embroiled in a bitter dispute regarding the power of Royal Courts to exercise jurisdiction over former clergymen. The murder of the Archbishop gave rise to a wave of popular outrage against the King. Henry was forced to repeal the disputed laws and to abandon his efforts to hold church members accountable for secular crimes (see also Constitutions of Clarendon).
Judge-made common law operated as the primary source of law for several hundred years, before Parliament acquired legislative powers to create statutory law. It is important to understand that common law is the older and more traditional source of law, and legislative power is simply a layer applied on top of the older common law foundation. Since the 12th century, courts have had parallel and co-equal authority to make law—"legislating from the bench" is a traditional and essential function of courts, which was carried over into the U.S. system as an essential component of the "judicial power" specified by Article III of the U.S. constitution. Justice Oliver Wendell Holmes, Jr. observed in 1917 that "judges do and must legislate." There are legitimate debates on how the powers of courts and legislatures should be balanced. However, a view that courts lack law-making power is historically inaccurate and constitutionally unsupportable.
Influences of foreign legal systems.
Roman law.
The term "common law" (connotation 2) is often used as a contrast to Roman-derived "civil law", and the fundamental processes and forms of reasoning in the two are quite different. Nonetheless, there has been considerable cross-fertilization of ideas, while the two traditions and sets of foundational principles remain distinct.
By the time of the rediscovery of the Roman law in Europe in the 12th and 13th centuries, the common law had already developed far enough to prevent a Roman law reception as it occurred on the continent. However, the first common law scholars, most notably Glanvill and Bracton, as well as the early royal common law judges, had been well accustomed with Roman law. Often, they were clerics trained in the Roman canon law. One of the first and throughout its history one of the most significant treatises of the common law, Bracton’s "De Legibus et Consuetudinibus Angliae" (On the Laws and Customs of England), was heavily influenced by the division of the law in Justinian’s "Institutes". The impact of Roman law had decreased sharply after the age of Bracton, but the Roman divisions of actions into "in rem" (typically, actions against a "thing" or property for the purpose of gaining title to that property; must be filed in a court where the property is located) and "in personam" (typically, actions directed against a person; these can affect a person's rights and, since a person often owns things, his property too) used by Bracton had a lasting effect and laid the groundwork for a return of Roman law structural concepts in the 18th and 19th centuries. Signs of this can be found in Blackstone’s "Commentaries on the Laws of England", and Roman law ideas regained importance with the revival of academic law schools in the 19th century. As a result, today, the main systematic divisions of the law into property, contract, and tort (and to some extent unjust enrichment) can be found in the civil law as well as in the common law.
Propagation of the common law to the colonies and Commonwealth by reception statutes.
A reception statute is a statutory law adopted as a former British colony becomes independent, by which the new nation adopts (i.e. receives) pre-independence English law, to the extent not explicitly rejected by the legislative body or constitution of the new nation. Reception statutes generally consider the English common law dating prior to independence, and the precedents originating from it, as the default law, because of the importance of using an extensive and predictable body of law to govern the conduct of citizens and businesses in a new state. All U.S. states, with the partial exception of Louisiana, have either implemented reception statutes or adopted the common law by judicial opinion.
Other examples of reception statutes in the United States, the states of the U.S., Canada and its provinces, and Hong Kong, are discussed in the reception statute article.
Decline of Latin maxims, and adding flexibility to "stare decisis".
Well into the 19th century, ancient maxims played a large role in common law adjudication. Many of these maxims had originated in Roman Law, migrated to England before the introduction of Christianity to the British Isles, and were typically stated in Latin even in English decisions. Many examples are familiar in everyday speech even today, "One cannot be a judge in one's own cause" (see Dr. Bonham's Case), rights are reciprocal to obligations, and the like. Judicial decisions and treatises of the 17th and 18th centuries, such at those of Lord Chief Justice Edward Coke, presented the common law as a collection of such maxims. 
Reliance on old maxims and rigid adherence to precedent, no matter how old or ill-considered, was under full attack by the late 19th century. Oliver Wendell Holmes, Jr. in his famous article, "The Path of the Law", commented, "It is revolting to have no better reason for a rule of law than that so it was laid down in the time of Henry IV. It is still more revolting if the grounds upon which it was laid down have vanished long since, and the rule simply persists from blind imitation of the past." Justice Holmes noted that study of maxims might be sufficient for "the man of the present," but "the man of the future is the man of statistics and the master of economics." In an 1880 lecture at Harvard, he wrote:
The life of the law has not been logic; it has been experience. The felt necessities of the time, the prevalent moral and political theories, intuitions of public policy, avowed or unconscious, even the prejudices which judges share with their fellow men, have had a good deal more to do than the syllogism in determining the rules by which men should be governed. The law embodies the story of a nation's development through many centuries, and it cannot be dealt with as if it contained only the axioms and corollaries of a book of mathematics.
In the early 20th century, Louis Brandeis, later appointed to the United States Supreme Court, became noted for his use of policy-driving facts and economics in his briefs, and extensive appendices presenting facts that lead a judge to the advocate's conclusion. By this time, briefs relied more on facts than on Latin maxims.
Reliance on old maxims is now deprecated. Common law decisions today reflect both precedent and policy judgment drawn from economics, the social sciences, business, decisions of foreign courts, and the like. The degree to which these external factors "should" influence adjudication is the subject of active debate, but it is indisputable that judges "do" draw on experience and learning from everyday life, from other fields, and from other jurisdictions.
1870 through 20th century, and the procedural merger of law and equity.
As early as the 15th century, it became the practice that litigants who felt they had been cheated by the common-law system would petition the King in person. For example, they might argue that an award of damages (at common law (connotation 3)) was not sufficient redress for a trespasser occupying their land, and instead request that the trespasser be evicted. From this developed the system of equity, administered by the Lord Chancellor, in the courts of chancery. By their nature, equity and law were frequently in conflict and litigation would frequently continue for years as one court countermanded the other, even though it was established by the 17th century that equity should prevail. A famous example is the fictional case of "Jarndyce v. Jarndyce" in "Bleak House", by Charles Dickens.
In England, courts of law (connotation 3) and equity were combined by the Judicature Acts of 1873 and 1875, with equity being supreme in case of conflict.
In the United States, parallel systems of law (providing money damages, with cases heard by a jury upon either party's request) and equity (fashioning a remedy to fit the situation, including injunctive relief, heard by a judge) survived well into the 20th century. The United States federal courts procedurally separated law and equity: the same judges could hear either kind of case, but a given case could only pursue causes in law or in equity, and the two kinds of cases proceeded under different procedural rules. This became problematic when a given case required both money damages and injunctive relief. In 1937, the new Federal Rules of Civil Procedure combined law and equity into one form of action, the "civil action." Fed.R.Civ.P. . The distinction survives to the extent that issues that were "common law (connotation 3)" as of 1791 (the date of adoption of the Seventh Amendment) are still subject to the right of either party to request a jury, and "equity" issues are decided by a judge.
Delaware, Mississippi, and Tennessee still have separate courts of law and equity, for example, the Court of Chancery. In many states there are separate divisions for law and equity within one court.
Common law pleading and its abolition in the early 20th century.
For centuries, through the 19th century, the common law recognized only specific forms of action, and required very careful drafting of the opening pleading (called a writ) to slot into one of them: Debt, Detinue, Covenant, Special Assumpsit, General Assumpsit, Trespass, Trover, Replevin, Case (or Trespass on the Case), and Ejectment. To initiate a lawsuit, a pleading had to be drafted to meet myriad technical requirements: correctly categorizing the case into the correct legal pigeonhole (pleading in the alternative was not permitted), and using specific "magic words" encrusted over the centuries. Under the old common law pleading standards, a suit by a "pro se" ("for oneself," without a lawyer) party was all but impossible, and there was often considerable procedural jousting at the outset of a case over minor wording issues.
One of the major reforms of the late 19th century and early 20th century was the abolition of common law pleading requirements. A plaintiff can initiate a case by giving the defendant "a short and plain statement" of facts that constitute an alleged wrong. This reform moved the attention of courts from technical scrutiny of words to a more rational consideration of the facts, and opened access to justice far more broadly.
Contrasts between common law and civil law systems.
General principles of law.
Both common law and civil law jurisdictions have formed what they variously call "pure common law" or "general principles of law" to define what the law is in the absence of, or gap in, legislation. In common law systems, judge made law is binding to the same extent as statute or regulation. In civil law systems, case law is advisory, not binding. Civil law lawyers consult case law to obtain their best prediction of how a court will rule, but comparatively, civil law judges are less bound to follow it.
Adversarial system vs. inquisitorial system.
Common law courts usually use an adversarial system, in which two sides present their cases to a neutral judge. In contrast, civil law systems usually use an inquisitorial system in which an examining magistrate serves two roles by developing the evidence and arguments for one side and then the other during the investigation phase.
The examining magistrate then presents the dossier detailing his or her findings to the president of the bench that will adjudicate on the case where it has been decided that a trial shall be conducted. Therefore, the president of the bench's view of the case is not neutral and may be biased while conducting the trial after the reading of the dossier. Unlike the common law proceedings, the president of the bench in the inquisitorial system is not merely an umpire and is entitled to directly interview the witnesses or express comments during the trial, as long as he or she does not express his or her view on the guilt of the accused.
The proceeding in the inquisitorial system is essentially by writing. Most of the witnesses would have given evidence in the investigation phase and such evidence will be contained in the dossier under the form of police reports. In the same way, the accused would have already put his or her case at the investigation phase but he or she will be free to change her or his evidence at trial. Whether the accused pleads guilty or not, a trial will be conducted. Unlike the adversarial system, the conviction and sentence to be served (if any) will be released by the trial jury together with the president of the trial bench, following their common deliberation.
There are many exceptions in both directions. For example, most proceedings before U.S. federal and state agencies are inquisitorial in nature, at least the initial stages ("e.g.", a patent examiner, a social security hearing officer, and so on), even though the law to be applied is developed through common law processes.
Contrasting role of treatises and academic writings in common law and civil law systems.
The role of the legal academy presents a significant "cultural" difference between common law (connotation 2) and civil law jurisdictions.
In common law jurisdictions, legal treatises compile common law decisions and state overarching principles that (in the author's opinion) explain the results of the cases. However, in common law jurisdictions, treatises are not the law, and lawyers and judges tend to use these treatises as only "finding aids" to locate the relevant cases. In common law jurisdictions, scholarly work is seldom cited as authority for what the law is. When common law courts rely on scholarly work, it is almost always only for factual findings, policy justification, or the history and evolution of the law, but the court's legal conclusion is reached through analysis of relevant statutes and common law, seldom scholarly commentary.
In contrast, in civil law jurisdictions, courts give the writings of law professors significant weight, partly because civil law decisions traditionally were very brief, sometimes no more than a paragraph stating who wins and who loses. The rationale had to come from somewhere else: the academy often filled that role. This balance may shift as civil law court decisions move in the direction of common law reasoning.
Common law legal systems in the present day.
The common law constitutes the basis of the legal systems of: England and Wales and Northern Ireland in the UK, Ireland, federal law in the United States and the law of individual U.S. states (with the partial exception of Louisiana), federal law throughout Canada and the law of the individual provinces and territories (except Quebec), Australia (both federal and individual states), Kenya, New Zealand, South Africa, India, Myanmar, Malaysia, Bangladesh, Brunei, Pakistan, Singapore, Hong Kong, Antigua and Barbuda, Barbados, Bahamas, Belize, Dominica, Grenada, Jamaica, St Vincent and the Granadines, Saint Kitts and Nevis, Trinidad and Tobago, and many other generally English-speaking countries or Commonwealth countries (except the UK's Scotland, which is bijuridicial, and Malta). Essentially, every country that was colonised at some time by England, Great Britain, or the United Kingdom uses common law except those that were formerly colonised by other nations, such as Quebec (which follows the law of France in part), South Africa and Sri Lanka (which follow Roman Dutch law), where the prior civil law system was retained to respect the civil rights of the local colonists. India uses common law except in the state of Goa which retains the Portuguese civil code. Guyana and Saint Lucia have mixed Common Law and Civil Law systems.
Scotland.
Scotland is often said to use the civil law system, but it has a unique system that combines elements of an uncodified civil law dating back to the Corpus Juris Civilis with an element of its own common law long predating the Treaty of Union with England in 1707 (see Legal institutions of Scotland in the High Middle Ages), founded on the customary laws of the tribes residing there. Historically, Scots common law differed in that the use of "precedents" was subject to the courts' seeking to discover the principle that justifies a law rather than searching for an example as a "precedent", and principles of natural justice and fairness have always played a role in Scots Law. From the 19th century, the Scottish approach to precedent developed into a "stare decisis" akin to that already established in England thereby reflecting a narrower, more modern approach to the application of case law in subsequent instances. This is not to say that the substantive rules of the common laws of both countries are the same although in many matters (particularly those of UK-wide interest) they are very similar. 
Scotland shares the Supreme Court (formerly the House of Lords), with England, Wales and Northern Ireland; and the Court's decisions are binding throughout the UK. This has had the effect of homogenising the law in certain areas. For instance, the modern UK law of negligence is based on "Donoghue v Stevenson", a case originating in Paisley, Scotland.
States of the United States (17th century on).
New York (17th century).
The state of New York, which also has a civil law history from its Dutch colonial days, also began a codification of its law in the 19th century. The only part of this codification process that was considered complete is known as the Field Code applying to civil procedure. The original colony of New Netherland was settled by the Dutch and the law was also Dutch. When the English captured pre-existing colonies they continued to allow the local settlers to keep their civil law. However, the Dutch settlers revolted against the English and the colony was recaptured by the Dutch. When the English finally regained control of New Netherland they forced, as a punishment unique in the history of the British Empire, the English imposed common law upon all the colonists, including the Dutch. This was problematic, as the patroon system of land holding, based on the feudal system and civil law, continued to operate in the colony until it was abolished in the mid-19th century. The influence of Roman-Dutch law continued in the colony well into the late 19th century. The codification of a law of general obligations shows how remnants of the civil law tradition in New York continued on from the Dutch days.
Louisiana (1700s).
Under Louisiana's codified system, the Louisiana Civil Code, private law—that is, substantive law between private sector parties—is based on principles of law from continental Europe, with some common law influences. These principles derive ultimately from Roman law, transmitted through French law and Spanish law, as the state's current territory intersects the area of North America colonized by Spain and by France. Contrary to popular belief, the Louisiana code does not directly derive from the Napoleonic Code, as the latter was enacted in 1804, one year after the Louisiana Purchase. However, the two codes are similar in many respects due to common roots.
Louisiana's criminal law largely rests on English common law. Louisiana's administrative law is generally similar to the administrative law of the U.S. federal government and other U.S. states. Louisiana's procedural law is generally in line with that of other U.S. states, which in turn is generally based on the U.S. Federal Rules of Civil Procedure.
Historically notable among the Louisiana code's differences from common law is the role of property rights among women, particularly in inheritance gained by widows.
California (1850s).
The U.S. state of California has a system based on common law, but it has codified the law in the manner of the civil law jurisdictions. The reason for the enactment of the California Codes in the 19th century was to replace a pre-existing system based on Spanish civil law with a system based on common law, similar to that in most other states. California and a number of other Western states, however, have retained the concept of community property derived from civil law. The California courts have treated portions of the codes as an extension of the common-law tradition, subject to judicial development in the same manner as judge-made common law. (Most notably, in the case "Li v. Yellow Cab Co.", 13 Cal.3d 804 (1975), the California Supreme Court adopted the principle of comparative negligence in the face of a California Civil Code provision codifying the traditional common-law doctrine of contributory negligence.)
United States federal courts (1789 and 1938).
The United States federal government (as opposed to the states) has a variant on a common law system. United States federal courts only act as interpreters of statutes and the constitution by elaborating and precisely defining the broad language (connotation 1(b) above), but, unlike state courts, do not act as an independent source of common law (connotation 1(a) above).
Before 1938, the federal courts, like almost all other common law courts, decided the law on any issue where the relevant legislature (either the U.S. Congress or state legislature, depending on the issue), had not acted, by looking to courts in the same system, that is, other federal courts, even on issues of state law, and even where there was no express grant of authority from Congress or the Constitution.
In 1938, the U.S. Supreme Court in "Erie Railroad Co. v. Tompkins" 304 U.S. 64, 78 (1938), overruled earlier precedent, and held "There is no federal general common law," thus confining the federal courts to act only as interpreters of law originating elsewhere. "E.g.", "Texas Industries v. Radcliff", (without an express grant of statutory authority, federal courts cannot create rules of intuitive justice, for example, a right to contribution from co-conspirators). Post-1938, federal courts deciding issues that arise under state law are required to defer to state court interpretations of state statutes, or reason what a state's highest court would rule if presented with the issue, or to certify the question to the state's highest court for resolution.
Later courts have limited "Erie" slightly, to create a few situations where United States federal courts are permitted to create federal common law rules without express statutory authority, for example, where a federal rule of decision is necessary to protect uniquely federal interests, such as foreign affairs, or financial instruments issued by the federal government. "See, e.g.", "Clearfield Trust Co. v. United States", (giving federal courts the authority to fashion common law rules with respect to issues of federal power, in this case negotiable instruments backed by the federal government); "see also" "International News Service v. Associated Press", 248 U.S. 215 (1918) (creating a cause of action for misappropriation of "hot news" that lacks any statutory grounding); "but see National Basketball Association v. Motorola, Inc.", 105 F.3d 841, 843–44, 853 (2d Cir. 1997) (noting continued vitality of "INS" "hot news" tort under New York state law, but leaving open the question of whether it survives under federal law). Except on Constitutional issues, Congress is free to legislatively overrule federal courts' common law.
United States executive branch agencies (1946).
Most executive branch agencies in the United States federal government have some adjudicatory authority. To greater or lesser extent, agencies honor their own precedent to ensure consistent results. Agency decision making is governed by the Administrative Procedure Act of 1946.
For example, the National Labor Relations Board issues relatively few regulations, but instead promulgates most of its substantive rules through common law (connotation 1).
India (19th century and 1948).
Indian Law is largely based on English common law because of the long period of British colonial influence during the period of the British Raj.
After the failed rebellion against the British in 1857, the British Parliament took over control of India from the British East India Company, and British India came under the direct rule of the Crown. The British Parliament passed the Government of India Act of 1858 to this effect, which set up the structure of British government in India. It established in Britain the office of the Secretary of State for India through whom the Parliament would exercise its rule, along with a Council of India to aid him. It also established the office of the Governor-General of India along with an Executive Council in India, which consisted of high officials of the British Government.
Much of contemporary Indian law shows substantial European and American influence. Legislation first introduced by the British is still in effect in modified form today. During the drafting of the Indian Constitution, laws from Ireland, the United States, Britain, and France were all synthesized to produce a refined set of Indian laws. Indian laws also adhere to the United Nations guidelines on human rights law and environmental law. Certain international trade laws, such as those on intellectual property, are also enforced in India.
Indian family law is complex, with each religion adhering to its own specific laws. In most states, registering marriages and divorces is not compulsory. There are separate laws governing Hindus, Muslims, Christians, Sikhs and followers of other religions. The exception to this rule is in the state of Goa, where a Portuguese uniform civil code is in place, in which all religions have a common law regarding marriages, divorces and adoption.
Ancient India represented a distinct tradition of law, and had an historically independent school of legal theory and practice. The "Arthashastra", dating from 400 BCE and the "Manusmriti", from 100 CE, were influential treatises in India, texts that were considered authoritative legal guidance. Manu's central philosophy was tolerance and pluralism, and was cited across Southeast Asia. Early in this period, which finally culminated in the creation of the Gupta Empire, relations with ancient Greece and Rome were not infrequent. The appearance of similar fundamental institutions of international law in various parts of the world show that they are inherent in international society, irrespective of culture and tradition. Inter-State relations in the pre-Islamic period resulted in clear-cut rules of warfare of a high humanitarian standard, in rules of neutrality, of treaty law, of customary law embodied in religious charters, in exchange of embassies of a temporary or semi-permanent character. When India became part of the British Empire, there was a break in tradition, and Hindu and Islamic law were supplanted by the common law. As a result, the present judicial system of the country derives largely from the British system and has little correlation to the institutions of the pre-British era.
There are 1160 laws as of September 2007.
Canada (1867).
Canada has separate federal and provincial legal systems. The division of jurisdiction between the federal and provincial Parliaments is specified in the Canadian constitution.
Each province is considered a separate jurisdiction with respect to common law matters. As such, only the provincial legislature may enact legislation to amend private law. Each has its own procedural law, statutorily created provincial courts and superior trial courts with inherent jurisdiction culminating in the Court of Appeal of the province. This is the highest court in provincial jurisdiction, only subject to the Supreme Court of Canada in terms of appeal of their decisions. All but one of the provinces of Canada use a common law system (the exception being Quebec, which uses a civil law system for issues arising within provincial jurisdiction, such as property ownership and contracts).
Canadian federal statutes must use the terminology of both the common law and civil law for those matters; this is referred to as legislative bijuralism.
Federal Courts operate under a separate system throughout Canada and deal with narrower subject matter than superior courts in provincial jurisdiction. They hear cases reserved for federal jurisdiction by the Canadian constitution, such as immigration, intellectual property, judicial review of federal government decisions, and admiralty. The Federal Court of Appeal is the appellate level court in federal jurisdiction and hears cases in multiple cities, and unlike the United States, the Canadian Federal Court of Appeal is not divided into appellate circuits.
Criminal law is uniform throughout Canada. It is based on the constitution and federal statutory Criminal Code, as interpreted by the Supreme Court of Canada. The administration of justice and enforcement of the criminal code are the responsibilities of the provinces.
Nicaragua.
Nicaragua's legal system also is a mixture of the English Common Law and the Civil Law. This situation was brought through the influence of British administration of the Eastern half of the country from the mid-17th century until about 1905, the William Walker period from about 1855 through 1857, USA interventions/occupations during the period from 1909 to 1933, the influence of USA institutions during the Somoza family administrations (1933 through 1979) and the considerable importation between 1979 and the present of USA culture and institutions.
Israel (1948).
Israel has a common law legal system. Its basic principles are inherited from the law of the British Mandate of Palestine and thus resemble those of British and American law, namely: the role of courts in creating the body of law and the authority of the supreme court in reviewing and if necessary overturning legislative and executive decisions, as well as employing the adversarial system. One of the primary reasons that the Israeli constitution remains unwritten is the fear by whatever party holds power that creating a written constitution, combined with the common-law elements, would severely limit the powers of the Knesset (which, following the doctrine of parliamentary sovereignty, holds near-unlimited power).
Roman Dutch Common law.
Roman Dutch Commons law is a bijuridical or mixed system of law similar to the common law system in Scotland and Louisiana. Roman Dutch common law jurisdictions include South Africa, Botswana, Lesotho, Namibia, Swaziland, Sri-Lanka and Zimbabwe. Many of these jurisdictions recognise customary law, and in some, such as South Africa the Constitution requires that the common law be developed in accordance with the Bill of Rights. Roman Dutch common law is a development of Roman Dutch law by courts in the Roman Dutch common law jurisdictions. During the Napoleonic wars the Kingdom of the Netherlands adopted the French "code civil" in 1809, however the Dutch colonies in the Cape of Good Hope and Sri Lanka, at the time called Ceylon, were seized by the British to prevent them being used as bases by the French Navy. The system was developed by the courts and spread with the expansion of British colonies in Southern Africa. Roman Dutch common law relies on legal principles set out in Roman law sources such as Justinian's Institutes and Digest, and also on the writing of Dutch jurists of the 17th century such as Grotius and Voet. In practice, the majority of decisions rely on recent precedent.
Alternatives to common law systems.
The main alternative to the common law system is the civil law system, which is used in Continental Europe, and most of the rest of the world. The contrast between civil law and common law legal systems has become increasingly blurred, with the growing importance of jurisprudence (similar to case law but not binding) in civil law countries, and the growing importance of statute law and codes in common law countries.
Examples of common law being replaced by statute or codified rule in the United States include criminal law (since 1812, U.S. courts have held that criminal law must be embodied in statute if the public is to have fair notice), commercial law (the Uniform Commercial Code in the early 1960s) and procedure (the Federal Rules of Civil Procedure in the 1930s and the Federal Rules of Evidence in the 1970s). But note that in each case, the statute sets the general principles, but the interstitial common law process (connotation 1(b)) determines the scope and application of the statute.
An example of convergence from the other direction is shown in Srl CILFIT and Lanificio di Gavardo SpA v Ministry of Health, in which the European Court of Justice held that questions it has already answered need not be resubmitted. This brought in a distinctly common law principle into an essentially civil law jurisdiction.
The former Soviet Bloc and other Socialist countries used a Socialist law system.
Much of the Muslim world uses Sharia (also called Islamic law).
Scholarly works.
Lord Chief Justice Edward Coke, a 17th-century English jurist and Member of Parliament, wrote several legal texts that formed the basis for the modern common law, with lawyers in both England and America learning their law from his "Institutes" and "Reports" until the end of the 18th century. His works are still cited by common law courts around the world.
The next definitive historical treatise on the common law is "Commentaries on the Laws of England", written by Sir William Blackstone and first published in 1765–1769. Since 1979, a facsimile edition of that first edition has been available in four paper-bound volumes. Today it has been superseded in the English part of the United Kingdom by Halsbury's Laws of England that covers both common and statutory English law.
While he was still on the Massachusetts Supreme Judicial Court, and before being named to the U.S. Supreme Court, Justice Oliver Wendell Holmes, Jr. published a short volume called "The Common Law", which remains a classic in the field. Unlike Blackstone and the Restatements, Holmes' book only briefly discusses what the law "is"; rather, Holmes describes the common law "process". Law professor John Chipman Gray's "The Nature and Sources of the Law", an examination and survey of the common law, is also still commonly read in U.S. law schools.
In the United States, Restatements of various subject matter areas (Contracts, Torts, Judgments, and so on.), edited by the American Law Institute, collect the common law for the area. The ALI Restatements are often cited by American courts and lawyers for propositions of uncodified common law, and are considered highly persuasive authority, just below binding precedential decisions. The Corpus Juris Secundum is an encyclopedia whose main content is a compendium of the common law and its variations throughout the various state jurisdictions.
Scots "common law" covers matters including murder and theft, and has sources in custom, in legal writings and previous court decisions. The legal writings used are called "Institutional Texts" and come mostly from the 17th, 18th and 19th centuries. Examples include Craig, "Jus Feudale" (1655) and Stair, "The Institutions of the Law of Scotland" (1681).

</doc>
<doc id="5255" url="https://en.wikipedia.org/wiki?curid=5255" title="Civil law">
Civil law

Civil law may refer to:

</doc>
<doc id="5257" url="https://en.wikipedia.org/wiki?curid=5257" title="Court of appeals (disambiguation)">
Court of appeals (disambiguation)

A court of appeals is an appellate court generally.
Court of Appeals may refer to:

</doc>
<doc id="5259" url="https://en.wikipedia.org/wiki?curid=5259" title="Common descent">
Common descent

Common descent describes how, in evolutionary biology, a group of organisms share a most recent common ancestor. There is evidence of common descent that all life on Earth is descended from the last universal ancestor.
Common ancestry between organisms of different species arises during speciation, in which new species are established from a single ancestral population. Organisms which share a more recent common ancestor are more closely related. The most recent common ancestor of all currently living organisms is the last universal ancestor, which lived about 3.9 billion years ago. The two earliest evidences for life on Earth are graphite found to be biogenic in 3.7 billion-year-old metasedimentary rocks discovered in western Greenland and microbial mat fossils found in 3.48 billion-year-old sandstone discovered in Western Australia. All currently living organisms on Earth share a common genetic heritage (universal common descent), with each being the descendant from a single original species, though the suggestion of substantial horizontal gene transfer during early evolution has led to questions about monophyly of life.
Universal common descent through an evolutionary process was first proposed by the English naturalist Charles Darwin in "On the Origin of Species" (1859), which concluded: "There is grandeur in this view of life, with its several powers, having been originally breathed into a few forms or into one; and that, whilst this planet has gone cycling on according to the fixed law of gravity, from so simple a beginning endless forms most beautiful and most wonderful have been, and are being, evolved."
History.
In the 1740s, French mathematician Pierre Louis Maupertuis made the first known suggestion in a series of essays that all organisms may have had a common ancestor, and that they had diverged through random variation and natural selection. In "Essai de cosmologie" (1750), Maupertuis noted:
May we not say that, in the fortuitous combination of the productions of Nature, since only those creatures "could" survive in whose organizations a certain degree of adaptation was present, there is nothing extraordinary in the fact that such adaptation is actually found in all these species which now exist? Chance, one might say, turned out a vast number of individuals; a small proportion of these were organized in such a manner that the animals' organs could satisfy their needs. A much greater number showed neither adaptation nor order; these last have all perished... Thus the species which we see today are but a small part of all those that a blind destiny has produced.
In 1790, Immanuel Kant wrote in "Kritik der Urteilskraft" ("Critique of Judgement") that the analogy of animal forms implies a common original type, and thus a common parent.
In 1794, Charles Darwin's grandfather, Erasmus Darwin, asked:
ould it be too bold to imagine, that in the great length of time, since the earth began to exist, perhaps millions of ages before the commencement of the history of mankind, would it be too bold to imagine, that all warm-blooded animals have arisen from one living filament, which endued with animality, with the power of acquiring new parts attended with new propensities, directed by irritations, sensations, volitions, and associations; and thus possessing the faculty of continuing to improve by its own inherent activity, and of delivering down those improvements by generation to its posterity, world without end?
Charles Darwin's views about common descent, as expressed in "On the Origin of Species", were that it was possible that there was only one progenitor for all life forms:
Therefore I should infer from analogy that probably all the organic beings which have ever lived on this earth have descended from some one primordial form, into which life was first breathed.
Evidence of universal common descent.
Common biochemistry and genetic code.
All known forms of life are based on the same fundamental biochemical organization: genetic information encoded in DNA, transcribed into RNA, through the effect of protein- and RNA-enzymes, then translated into proteins by (highly similar) ribosomes, with ATP, NADPH and others as energy sources, etc. Furthermore, the genetic code (the "translation table" according to which DNA information is translated into proteins) is nearly identical for all known lifeforms, from bacteria and archaea to animals and plants. The universality of this code is generally regarded by biologists as definitive evidence in favor of the theory of universal common descent. Analysis of the small differences in the genetic code has also provided support for universal common descent. An example would be Cytochrome c which most organisms actually share. A statistical comparison of various alternative hypotheses has shown that universal common ancestry is significantly more probable than models involving multiple origins.
Selectively neutral similarities.
Similarities which have no adaptive relevance cannot be explained by convergent evolution, and therefore they provide compelling support for the theory of universal common descent.
Such evidence has come from two areas: amino acid sequences and DNA sequences. Proteins with the same three-dimensional structure need not have identical amino acid sequences; any irrelevant similarity between the sequences is evidence for common descent. In certain cases, there are several codons (DNA triplets) that code for the same amino acid. Thus, if two species use the same codon at the same place to specify an amino acid that can be represented by more than one codon, that is evidence for a recent common ancestor.
Other similarities.
The universality of many aspects of cellular life is often pointed to as supportive evidence to the more compelling evidence listed above. These similarities include the energy carrier adenosine triphosphate (ATP), and the fact that all amino acids found in proteins are left-handed. It is, however, possible that these similarities resulted because of the laws of physics and chemistry, rather than universal common descent and therefore resulted in convergent evolution.
Phylogenetic trees.
Another important piece of evidence is that it is possible to construct detailed phylogenetic trees (i.e., "genealogic trees" of species) mapping out the proposed divisions and common ancestors of all living species. In 2010, Douglas L. Theobald published a statistical analysis of available genetic data, mapping them to phylogenetic trees, that gave "strong quantitative support, by a formal test, for the unity of life." It should be noted, however, that the "formal test" was criticised for not including consideration of convergent evolution, and Theobald has defended the method against this claim.
Traditionally, these trees have been built using morphological methods, such as appearance, embryology, etc. Recently, it has been possible to construct these trees using molecular data, based on similarities and differences between genetic and protein sequences. All these methods produce essentially similar results, even though most genetic variation has no influence over external morphology. That phylogenetic trees based on different types of information agree with each other is strong evidence of a real underlying common descent.
Illustrations of common descent.
Artificial selection.
Artificial selection demonstrates the diversity that can exist among organisms that share a relatively recent common ancestor. In artificial selection, humans selectively direct the breeding of one species at each generation, allowing only those organisms that exhibit desired characteristics to reproduce. These characteristics become increasingly well-developed in successive generations. Artificial selection was successful long before science discovered the genetic basis.
Dog breeding.
The diversity of domesticated dogs is an example of the power of artificial selection. All breeds share common ancestry, having descended from wolves. Humans selectively bred them to enhance specific characteristics, such as color and length or body size. This created a range of breeds that include the Chihuahua, Great Dane, Basset Hound, Pug, and Poodle. Wild wolves, which did not undergo artificial selection, are relatively uniform in comparison.
Wild cabbage.
Early farmers cultivated many popular vegetables from the "Brassica oleracea" (wild cabbage) by artificially selecting for certain attributes. Common vegetables such as cabbage, kale, broccoli, cauliflower, kohlrabi and Brussels sprouts are all descendants of the wild cabbage plant. Brussels sprouts were created by artificially selecting for large bud size. Broccoli was bred by selecting for large flower stalks. Cabbage was created by selecting for short petioles. Kale was bred by selecting for large leaves.
Natural selection.
Natural selection is the evolutionary process by which heritable traits that increase an individual's fitness become more common, and heritable traits that decrease an individual's fitness become less common.
Darwin's finches.
During his studies on the Galápagos Islands, Charles Darwin observed 13 species of finches that are closely related and differ most markedly in the shape of their beaks. The beak of each species is suited to the food available in its particular environment, suggesting that beak shapes evolved by natural selection. Large beaks were found on the islands where the primary source of food for the finches are nuts and therefore the large beaks allowed the birds to be better equipped for opening the nuts and staying well nourished. Slender beaks were found on the finches which found insects to be the best source of food on the island they inhabited; their slender beaks allowed the birds to be better equipped for pulling out the insects from their tiny hiding places. The finch is also found on the main land and it is thought that they migrated to the islands and began adapting to their environment through natural selection.

</doc>
<doc id="5261" url="https://en.wikipedia.org/wiki?curid=5261" title="Celtic music">
Celtic music

Celtic music is a broad grouping of music genres that evolved out of the folk musical traditions of the Celtic people of Western Europe. It refers to both orally-transmitted traditional music and recorded music and the styles vary considerably to include everything from "trad" (traditional) music to a wide range of hybrids.
Description and definition.
"Celtic music" means two things mainly. First, it is the music of the peoples identifying themselves as Celts. Secondly, it refers to whatever qualities may be unique to the musics of the Celtic Nations. Many notable Celtic musicians such as Alan Stivell and Paddy Moloney claim that the different Celtic musics have much in common. These common melodic practices may be used widely across Celtic Music:
These two latter usage patterns may simply be remnants of formerly widespread melodic practices.
Often, the term "Celtic music" is applied to the music of Ireland and Scotland because both lands have produced well-known distinctive styles which actually have genuine commonality and clear mutual influences. The definition is further complicated by the fact that Irish independence has allowed Ireland to promote 'Celtic' music as a specifically Irish product. However, these are modern geographical references to a people who share a common Celtic ancestry and consequently, a common musical heritage.
These styles are known because of the importance of Irish and Scottish people in the English speaking world, especially in the United States, where they had a profound impact on American music, particularly bluegrass and country music. The music of Wales, Cornwall, the Isle of Man, Brittany, Galicia, Cantabria and Asturias (Spain) and Portugal are also considered Celtic music, the tradition being particularly strong in Brittany, where Celtic festivals large and small take place throughout the year, and in Wales, where the ancient eisteddfod tradition has been revived and flourishes. Additionally, the musics of ethnically Celtic peoples abroad are vibrant, especially in Canada and the United States. In Canada the provinces of Atlantic Canada are known for being a home of Celtic music, most notably on the islands of Newfoundland, Cape Breton and Prince Edward Island. The traditional music of Atlantic Canada is heavily influenced by the Irish, Scottish and Acadian ethnic makeup of much of the region's communities. In some parts of Atlantic Canada, such as Newfoundland, Celtic music is as or more popular than in the old country. Further, some older forms of Celtic music that are rare in Scotland and Ireland today, such as the practice of accompanying a fiddle with a piano, or the Gaelic spinning songs of Cape Breton remain common in the Maritimes. Much of the music of this region is Celtic in nature, but originates in the local area and celebrates the sea, seafaring, fishing and other primary industries.
Divisions.
In "Celtic Music: A Complete Guide", June Skinner Sawyers acknowledges six Celtic nationalities divided into two groups according to their linguistic heritage. The Q-Celtic nationalities are the Irish, Scottish and Manx peoples, while the P-Celtic groups are the Cornish, Bretons and Welsh peoples. Musician Alan Stivell uses a similar dichotomy, between the Gaelic (Irish/Scottish/Manx) and the Brythonic (Breton/Welsh/Cornish) branches, which differentiate "mostly by the extended range (sometimes more than two octaves) of Irish and Scottish melodies and the closed range of Breton and Welsh melodies (often reduced to a half-octave), and by the frequent use of the pure pentatonic scale in Gaelic music."
There is also tremendous variation between "Celtic" regions. Ireland, Scotland, Brittany and Wales have living traditions of language and music, and there has been a recent major revival of interest in Celtic heritage in Cornwall and the Isle of Man. Galicia has a Celtic language revival movement to revive the Q-Celtic "Gallaic language" used into Roman times. Most of the Iberian Peninsula had a similar Celtic language in pre-Roman times. A Brythonic language was used in parts of Galicia and Asturias into early Medieval times brought by Britons fleeing the Anglo-Saxon invasions via Brittany. The Romance language currently spoken in Galicia, Galician (Galego) is closely related to the Portuguese language used mainly in Brazil and Portugal. Galician music is claimed to be "Celtic". The same is true of the music of Asturias, Cantabria, and that of Northern Portugal (some say even traditional music from Central Portugal can be labeled Celtic).
Breton artist Alan Stivell was one of the earliest musicians to use the word "Celtic" and "Keltia" in his marketing materials, starting in the early 1960s as part of the worldwide folk music revival of that era with the term quickly catching on with other artists worldwide. Today, the genre is well established and incredibly diverse.
Forms.
There are musical genres and styles specific to each Celtic country, due in part to the influence of individual song traditions and the characteristics of specific languages:
Festivals.
The modern Celtic music scene involves a large number of music festivals, as it has traditionally. Some of the most prominent festivals focused solely on music include:
Celtic fusion.
The oldest musical tradition which fits under the label of Celtic fusion originated in the rural American south in the early colonial period and incorporated Scottish, Scots-Irish, Irish, Welsh, English, and African influences. Variously referred to as roots music, American folk music, or old-time music, this tradition has exerted a strong influence on all forms of American music, including country, blues, and rock and roll. In addition to its lasting effects on other genres, it marked the first modern large-scale mixing of musical traditions from multiple ethnic and religious communities within the Celtic diaspora.
In the 1960s several bands put forward modern adaptations of Celtic music pulling influences from several of the Celtic nations at once to create a modern pan-celtic sound. A few of those include bagadoù (Breton pipe bands), Fairport Convention, Pentangle, Steeleye Span and Horslips.
In the 1970s Clannad made their mark initially in the folk and traditional scene, and then subsequently went on to bridge the gap between traditional Celtic and pop music in the 1980s and 1990s, incorporating elements from new-age, smooth jazz, and folk rock. Traces of Clannad's legacy can be heard in the music of many artists, including Enya, Donna Taggart, Altan, Capercaillie, The Corrs, Loreena McKennitt, Anúna, Riverdance and U2. The solo music of Clannad's lead singer, Moya Brennan (often referred to as the First Lady of Celtic Music) has further enhanced this influence.
Later, beginning in 1982 with The Pogues' invention of Celtic folk-punk and Stockton's Wing blend of Irish traditional and Pop, Rock and Reggae, there has been a movement to incorporate Celtic influences into other genres of music. Bands like Flogging Molly, Black 47, Dropkick Murphys, The Young Dubliners, The Tossers introduced a hybrid of Celtic rock, punk, reggae, hardcore and other elements in the 1990s that has become popular with Irish-American youth.
Today there are Celtic-influenced subgenres of virtually every type of popular music including electronica, rock, metal, punk, hip hop, reggae, new-age, Latin, Andean and pop. Collectively these modern interpretations of Celtic music are sometimes referred to as Celtic fusion.
Other modern adaptations.
Outside of America, the first deliberate attempts to create a "Pan-Celtic music" were made by the Breton Taldir Jaffrennou, having translated songs from Ireland, Scotland, and Wales into Breton between the two world wars. One of his major works was to bring "Hen Wlad Fy Nhadau" (the Welsh national anthem) back in Brittany and create lyrics in Breton. Eventually this song became ""Bro goz va zadoù"" ("Old land of my fathers") and is the most widely accepted Breton anthem. In the 70s, the Breton Alan Cochevelou (future Alan Stivell) began playing a mixed repertoire from the main Celtic countries on the Celtic harp his father created. 
Probably the most successful all inclusive Celtic music composition in recent years is Shaun Daveys composition 'The Pilgrim'. This suite depicts the journey of St. Colum Cille through the Celtic nations of Ireland, Scotland, the Isle of Man, Wales, Cornwall, Brittany and Galicia. The suite which includes a Scottish pipe band, Irish and Welsh harpists, Galician gaitas, Irish uilleann pipes, the bombardes of Brittany, two vocal soloists and a narrator is set against a background of a classical orchestra and a large choir.
Modern music may also be termed "Celtic" because it is written and recorded in a Celtic language, regardless of musical style. Many of the Celtic languages have experienced resurgences in modern years, spurred on partly by the action of artists and musicians who have embraced them as hallmarks of identity and distinctness. In 1971, the Irish band "Skara Brae" recorded its only LP (simply called "Skara Brae"), all songs in Irish. In 1978 Runrig recorded an album in Scottish Gaelic. In 1992 Capercaillie recorded "A Prince Among Islands", the first Scottish Gaelic language record to reach the UK top 40. In 1996, a song in Breton represented France in the 41st Eurovision Song Contest, the first time in history that France had a song without a word in French. Since about 2005, Oi Polloi (from Scotland) have recorded in Scottish Gaelic. Mill a h-Uile Rud (a Scottish Gaelic punk band from Seattle) recorded in the language in 2004.
Several contemporary bands have Welsh language songs, such as Ceredwen, which fuses traditional instruments with trip hop beats, the Super Furry Animals, Fernhill, and so on (see the Music of Wales article for more Welsh and Welsh-language bands). The same phenomenon occurs in Brittany, where many singers record songs in Breton, traditional or modern (hip hop, rap, and so on.).

</doc>
<doc id="5267" url="https://en.wikipedia.org/wiki?curid=5267" title="Constellation">
Constellation

In modern astronomy, a constellation is a specific area of the celestial sphere as defined by the International Astronomical Union (IAU). These areas mostly had their origins in Western-traditional asterisms from which the constellations take their names. There are 88 officially recognized constellations, covering the entire sky.
Thus, any given point in a celestial coordinate system can unambiguously be assigned to a constellation. It is usual in astronomy to give the constellation in which a given object is found along with its coordinates in order to convey a rough idea in which part of the sky it is located.
Terminology.
The origin of the word constellation seems to come from the Late Latin term "cōnstellātiō," which can be translated as "set of stars", but came into use in English during the 14th century. A more modern astronomical sense of the term is as a recognisable pattern of stars whose appearance is identified with mythological characters or creatures, or associated earthbound animals or objects.
Colloquial usage does not draw a sharp distinction between "constellation" in the sense of an asterism (pattern of stars) and "constellation" in the sense of an area surrounding an asterism. The modern system of constellations used in astronomy employs the latter concept. For example, the northern asterism known as the Big Dipper comprises the seven brightest stars in the IAU constellation (area) Ursa Major while the southern False Cross includes portions of the constellations Carina and Vela.
The term circumpolar constellation is used for any constellation that, from a particular latitude on Earth, never sets below the horizon. From the North Pole or South Pole, all constellations north or south of the celestial equator are circumpolar constellations. In the equatorial or temperate latitudes, the informal term "equatorial constellation" has sometimes been used for constellations that lie to the opposite the circumpolar constellations. Depending on the definition, equatorial constellations can include those that lie entirely between declinations 45° north and 45° south, or those that pass overhead between the tropics of Cancer and Capricorn between declinations of 23½° north and 23½° south. They generally include all constellations that intersect the celestial equator or part of the zodiac.
Usually the only thing the stars in a constellation have in common is that they appear near each other in the sky when viewed from the Earth, but in galactic space, most constellation stars lie at a variety of distances. Since stars also travel on their own orbits through the Milky Way, the star patterns of the constellations change slowly over time. After tens to hundreds of thousands of years, their familiar outlines will become unrecognisable.
History.
The earliest direct evidence for the constellations comes from inscribed stones and clay writing tablets dug up in Mesopotamia (within modern Iraq) dating back to 3000 BC. It seems that the bulk of the Mesopotamian constellations were created within a relatively short interval from around 1300 to 1000 BC. These groupings appeared later in many of the classical Greek constellations.
Ancient near East.
The Babylonians were the first to recognize that astronomical phenomena are periodic and apply mathematics to their predictions. The oldest Babylonian star catalogues of stars and constellations date back to the beginning in the Middle Bronze Age, most notably the "Three Stars Each" texts and the "MUL.APIN", an expanded and revised version based on more accurate observation from around 1000 BC. However, the numerous Sumerian names in these catalogues suggest that they build on older, but otherwise unattested, Sumerian traditions of the Early Bronze Age.
The classical Zodiac is a product of a revision of the Old Babylonian system in later Neo-Babylonian astronomy 6th century BC. Knowledge of the Neo-Babylonian zodiac is also reflected in the Hebrew Bible. E. W. Bullinger interpreted the creatures appearing in the books of Ezekiel (and thence in Revelation) as the middle signs of the four quarters of the Zodiac, with the Lion as Leo, the Bull as Taurus, the Man representing Aquarius and the Eagle standing in for Scorpio. The biblical Book of Job also makes reference to a number of constellations, including "bier", "fool" and "heap" (Job 9:9, 38:31-32), rendered as "Arcturus, Orion and Pleiades" by the KJV, but "‘Ayish" "the bier" actually corresponding to Ursa Major. The term "Mazzaroth" , a "hapax legomenon" in Job 38:32, may be the Hebrew word for the zodiacal constellations.
The Greeks adopted the Babylonian system in the 4th century BC. A total of twenty Ptolemaic constellations are directly continued from the Ancient Near East. Another ten have the same stars but different names.
Chinese astronomy.
In ancient China astronomy has had a long tradition in accurately observing celestial phenomena. Star names later categorized in the twenty-eight mansions have been found on oracle bones unearthed at Anyang, dating back to the middle Shang Dynasty. These Chinese constellations are one of the most important and also the most ancient structures in the Chinese sky, attested from the 5th century BC. Parallels to the earliest Babylonian (Sumerian) star catalogues suggest that the ancient Chinese system did not arise independently.
Classical Chinese astronomy is recorded in the Han period and appears in the form of three schools, which are attributed to astronomers of the Zhanguo period. The constellations of the three schools were conflated into a single system by Chen Zhuo, an astronomer of the 3rd century (Three Kingdoms period). Chen Zhuo's work has been lost, but information on his system of constellations survives in Tang period records, notably by Qutan Xida. The oldest extant Chinese star chart dates to that period and was preserved as part of the Dunhuang Manuscripts. Native Chinese astronomy flourished during the Song dynasty, and during the Yuan Dynasty became increasingly influenced by medieval Islamic astronomy (see Treatise on Astrology of the Kaiyuan Era).
Traditional Chinese star maps incorporated 23 asterisms of the southern sky based on the knowledge of western star charts. They were first introduced by Xu Guangqi, by the end of the Ming Dynasty, in the middle of the seventeenth century.
Indian astronomy.
Some of the earliest roots of Indian astronomy can be dated to the period of Indus Valley Civilization, a Bronze Age civilisation in the northwest Indian subcontinent. Afterwards the astronomy developed as a discipline of Vedanga or one of the "auxiliary disciplines" associated with the study of the Vedas, dating 1500 BC or older. The oldest known text is the Vedanga Jyotisha, dated to 1400–1200 BC
As with other traditions, the original application of astronomy was thus religious. Indian astronomy was influenced by Greek astronomy beginning in the 4th century BC and through the early centuries of the Common Era, for example by the Yavanajataka and the Romaka Siddhanta, a Sanskrit translation of a Greek text disseminated from the 2nd century.
Indian astronomy flowered in the 5th–6th century, with Aryabhata, whose "Aryabhatiya" represented the pinnacle of astronomical knowledge at the time. Later the Indian astronomy significantly influenced medieval Islamic, Chinese and European astronomy. Other astronomers of the classical era who further elaborated on Aryabhata's work include Brahmagupta, Varahamihira and Lalla. An identifiable native Indian astronomical tradition remained active throughout the medieval period and into the 16th or 17th century, especially within the Kerala school of astronomy and mathematics.
Classical antiquity.
There is only limited information on indigenous Greek constellations, with some fragmentary evidence being found in the "Works and Days" of Greek poet Hesiod, who mentioned the "heavenly bodies". Greek astronomy essentially adopted the older Babylonian system in the Hellenistic era, first introduced to Greece by Eudoxus of Cnidus in the 4th century BC. The original work of Eudoxus is lost, but it survives as a versification by Aratus, dating to the 3rd century BC. The most complete existing works dealing with the mythical origins of the constellations are by the Hellenistic writer termed pseudo-Eratosthenes and an early Roman writer styled pseudo-Hyginus. The basis of western astronomy as taught during Late Antiquity and until the Early Modern period is the "Almagest" by Ptolemy, written in the 2nd century.
In Ptolemaic Egypt, native Egyptian tradition of anthropomorphic figures representing the planets, stars and various constellations. Some of these were combined with Greek and Babylonian astronomical systems culminating in the Zodiac of Dendera, but it remains unclear when this occurred, but most were placed during the Roman period between 2nd to 4th centuries AD. The oldest known depiction of the zodiac showing all the now familiar constellations, along with some original Egyptian Constellations, Decans and Planets. Ptolemy's "Almagest" remained the standard definition of constellations in the medieval period both in Europe and in Islamic astronomy.
Islamic astronomy.
Particularly during the Islamic Golden Age (8th–15th centuries) the Islamic world experienced development in astronomy. These developments were mostly written in Arabic and took place from North Africa to Central Asia, Al-Andalus, and later in the Far East and India. It closely parallels the genesis of other Islamic sciences in its assimilation of foreign material and the amalgamation of the disparate elements of that material to create a science with Islamic characteristics. These included ancient Greek astronomy, Sassanid, and Indian works in particular, which were translated and built upon. In turn, Islamic astronomy later had a significant influence on Byzantine and European astronomy (see Latin translations of the 12th century) as well as Chinese astronomy and Malian astronomy.
A significant number of stars in the sky, such as Aldebaran and Altair, and astronomical terms such as alidade, azimuth, and almucantar, are still referred to by their Arabic names. A large corpus of literature from Islamic astronomy remains today, numbering approximately 10,000 manuscripts scattered throughout the world, many of which have not been read or catalogued. Even so, a reasonably accurate picture of Islamic activity in the field of astronomy can be reconstructed.
Early Modern era.
The skies around the South Celestial Pole are not observable from north of the equator and were never catalogued by the ancient Babylonians, Greeks, Chinese and Arabs. The modern constellations in this region were first defined during the age of exploration by Petrus Plancius, who used the records from observations of twelve new constellations made by the Dutch navigators Pieter Dirkszoon Keyser and Frederick de Houtman at the end of the sixteenth century. They were later depicted by Johann Bayer in his star atlas "Uranometria" of 1603. Several more were created by Nicolas Louis de Lacaille in his star catalogue, published in 1756.
Some modern proposals for new constellations were not successful; an example is Quadrans, eponymous of the Quadrantid meteors, now divided between Boötes and Draco in the northern sky. The large classical constellation of Argo Navis was broken up into three separate parts (Carina, Puppis and Vela), for the convenience of stellar cartographers.
The current list of 88 constellations recognized by the International Astronomical Union since 1922 is based on the 48 listed by Ptolemy in his "Almagest" in the 2nd century, with early modern modifications and additions (most importantly introducing constellations covering the parts of the southern sky unknown to Ptolemy) by Petrus Plancius (1592, 1597/98 and 1613), Johannes Hevelius (1690) and Nicolas Louis de Lacaille (1763).
IAU constellations.
In 1922, Henry Norris Russell aided the IAU (International Astronomical Union) in dividing the celestial sphere into 88 official constellations. Where possible, these modern constellations usually share the names of their Graeco-Roman predecessors, such as Orion, Leo or Scorpius. The aim of this system is area-mapping, i.e. the division of the celestial sphere into contiguous fields. Out of the 88 modern constellations, 36 lie predominantly in the northern sky, and the other 52 predominantly in the southern.
In 1930, the boundaries between the 88 constellations were devised by Eugène Delporte along vertical and horizontal lines of right ascension and declination. However, the data he used originated back to epoch B1875.0, which was when Benjamin A. Gould first made his proposal to designate boundaries for the celestial sphere, a suggestion upon which Delporte would base his work. The consequence of this early date is that due to the precession of the equinoxes, the borders on a modern star map, such as epoch J2000, are already somewhat skewed and no longer perfectly vertical or horizontal. This effect will increase over the years and centuries to come.
Asterisms.
An asterism is a pattern of stars recognized in the Earth's night sky and may be part of an official constellation. It may also be composed of stars from more than one constellation. The stars of the main asterism within a constellation are usually given Greek letters in their order of brightness, the so-called Bayer designation introduced by Johann Bayer in 1603. A total of 1,564 stars are so identified, out of approximately 10,000 stars visible to the naked eye.
The brightest stars, usually the stars that make up the constellation's eponymous asterism, also retain proper names, often from Arabic. For example, the "Little Dipper" asterism of the constellation Ursa Minor has ten stars with Bayer designation, α UMi to π UMi. Of these ten stars, six have a proper name, viz. Polaris (α UMi), Kochab (β UMi), Pherkad (γ UMi), Yildun (δ UMi), Ahfa al Farkadain (ζ UMi) and Anwar al Farkadain (η UMi).
The stars within an asterism rarely have any substantial astrophysical relationship to each other, and their apparent proximity when viewed from Earth disguises the fact that they are far apart, some being much farther from Earth than others. However, there are some exceptions: almost all of the stars in the constellation of Ursa Major (including most of the Big Dipper) are genuinely close to one another, travel through the galaxy with similar velocities, and are likely to have formed together as part of a cluster that is slowly dispersing. These stars form the Ursa Major moving group.
Ecliptic coordinate systems.
The idea of dividing the celestial sphere into "constellations", understood as areas surrounding asterisms, is early modern. The currently-used boundaries between constellations were defined in 1930. The concept is ultimately derived from the ancient tradition of dividing the ecliptic into twelve equal parts named for nearby asterisms (the Zodiac). This defined an ecliptic coordinate system which was used throughout the medieval period and into the 18th century.
Systems of dividing the ecliptic (as opposed to dividing the celestial sphere into constellations in the modern sense) are also found in Chinese and Hindu astronomy. In classical Chinese astronomy, the northern sky is divided geometrically, into five "enclosures" and twenty-eight mansions along the ecliptic, grouped into Four Symbols of seven asterisms each. Ecliptic longitude is measured using 24 Solar terms, each of 15° longitude, and are used by Chinese lunisolar calendars to stay synchronized with the seasons, which is crucial for agrarian societies. In Hindu astronomy, the term for "lunar mansion" is ' A ' is one of 27 (sometimes also 28) sectors along the ecliptic. Comparable to the zodiacal system, their names are related to the most prominent asterisms in the respective sectors. The first astronomical text that lists them is the "Vedanga Jyotisha".
Dark cloud constellations.
The Great Rift, a series of dark patches in the Milky Way, is more visible and striking in the southern hemisphere than in the northern. It vividly stands out when conditions are otherwise so dark that the Milky Way's central region casts shadows on the ground. Some cultures have discerned shapes in these patches and have given names to these "dark cloud constellations." Members of the Inca civilization identified various dark areas or dark nebulae in the Milky Way as animals, and associated their appearance with the seasonal rains. Australian Aboriginal astronomy also describes dark cloud constellations, the most famous being the "emu in the sky" whose head is formed by the Coalsack.
Further reading.
Atlases and celestial maps.
"General & Nonspecialized – Entire Celestial Heavens":
"Northern Celestial Hemisphere & North Circumpolar Region":
"Equatorial, Ecliptic, & Zodiacal Celestial Sky":
"Southern Celestial Hemisphere & South Circumpolar Region":

</doc>
<doc id="5269" url="https://en.wikipedia.org/wiki?curid=5269" title="Character">
Character

Character(s) may refer to:

</doc>
<doc id="5270" url="https://en.wikipedia.org/wiki?curid=5270" title="Car (disambiguation)">
Car (disambiguation)

A car is a wheeled motor vehicle used for transporting passengers.
Car, Cars, CAR or CARS may also refer to:

</doc>
<doc id="5272" url="https://en.wikipedia.org/wiki?curid=5272" title="Printer (computing)">
Printer (computing)

In computing, a printer is a peripheral which makes a persistent human readable representation of graphics or text on paper or similar physical media. The two most common printer mechanisms are black and white laser printers used for common documents, and color inkjet printers which can produce high quality photograph output.
The world's first computer printer was a 19th-century mechanically driven apparatus invented by Charles Babbage for his difference engine. This system used a series of metal rods with characters printed on them and stuck a roll of paper against the rods to print the characters. The first commercial printers generally used mechanisms from electric typewriters and Teletype machines, which operated in a similar fashion. The demand for higher speed led to the development of new systems specifically for computer use. Among the systems widely used through the 1980s were daisy wheel systems similar to typewriters, line printers that produced similar output but at much higher speed, and dot matrix systems that could mix text and graphics but produced relatively low-quality output. The plotter was used for those requiring high quality line art like blueprints.
The introduction of the low-cost laser printer in 1984 with the first HP LaserJet, and the addition of PostScript in next year's Apple LaserWriter, set off a revolution in printing known as desktop publishing. Laser printers using PostScript mixed text and graphics, like dot-matrix printers, but at quality levels formerly available only from commercial typesetting systems. By 1990, most simple printing tasks like fliers and brochures were now created on personal computers and then laser printed; expensive offset printing systems were being dumped as scrap. The HP Deskjet of 1988 offered the same advantages as laser printer in terms of flexibility, but produced somewhat lower quality output (depending on the paper) from much less expensive mechanisms. Inkjet systems rapidly displaced dot matrix and daisy wheel printers from the market. By the 2000s high-quality printers of this sort had fallen under the $100 price point and became commonplace.
The rapid update of internet email through the 1990s and into the 2000s has largely displaced the need for printing as a means of moving documents, and a wide variety of reliable storage systems means that a "physical backup" is of little benefit today. Even the desire for printed output for "offline reading" while on mass transit or aircraft has been displaced by e-book readers and tablet computers. Today, traditional printers are being used more for special purposes, like printing photographs or artwork, and are no longer a must-have peripheral.
Starting around 2010, 3D printing became an area of intense interest, allowing the creation of physical objects with the same sort of effort as an early laser printer required to produce a brochure. These devices are in their earliest stages of development and have not yet become commonplace.
Types of printers.
"Personal" printers are primarily designed to support individual users, and may be connected to only a single computer. These printers are designed for low-volume, short-turnaround print jobs, requiring minimal setup time to produce a hard copy of a given document. However, they are generally slow devices ranging from 6 to around 25 pages per minute (ppm), and the cost per page is relatively high. However, this is offset by the on-demand convenience. Some printers can print documents stored on memory cards or from digital cameras and scanners.
"Networked" or "shared" printers are "designed for high-volume, high-speed printing." They are usually shared by many users on a network and can print at speeds of 45 to around 100 ppm. The Xerox 9700 could achieve 120 ppm.
A "virtual printer" is a piece of computer software whose user interface and API resembles that of a printer driver, but which is not connected with a physical computer printer.
A "3D printer" is a device for making a three-dimensional object from a 3D model or other electronic data source through additive processes in which successive layers of material ( including plastics, metals, food, cement, wood, and other materials) are laid down under computer control. It is called a printer by analogy with an inkjet printer which produces a two-dimensional document by a similar process of depositing a layer of ink on paper.
Technology.
The choice of print technology has a great effect on the cost of the printer and cost of operation, speed, quality and permanence of documents, and noise. Some printer technologies don't work with certain types of physical media, such as carbon paper or transparencies.
A second aspect of printer technology that is often forgotten is resistance to alteration: liquid ink, such as from an inkjet head or fabric ribbon, becomes absorbed by the paper fibers, so documents printed with liquid ink are more difficult to alter than documents printed with toner or solid inks, which do not penetrate below the paper surface.
Cheques can be printed with liquid ink or on special cheque paper with toner anchorage so that alterations may be detected. The machine-readable lower portion of a cheque must be printed using MICR toner or ink. Banks and other clearing houses employ automation equipment that relies on the magnetic flux from these specially printed characters to function properly.
Modern print technology.
The following printing technologies are routinely found in modern printers:
Toner-based printers.
A laser printer rapidly produces high quality text and graphics. As with digital photocopiers and multifunction printers (MFPs), laser printers employ a xerographic printing process but differ from analog photocopiers in that the image is produced by the direct scanning of a laser beam across the printer's photoreceptor.
Another toner-based printer is the LED printer which uses an array of LEDs instead of a laser to cause toner adhesion to the print drum.
Liquid inkjet printers.
Inkjet printers operate by propelling variably sized droplets of liquid ink onto almost any sized page. They are the most common type of computer printer used by consumers.
Solid ink printers.
Solid ink printers, also known as phase-change printers, are a type of thermal transfer printer. They use solid sticks of CMYK-coloured ink, similar in consistency to candle wax, which are melted and fed into a piezo crystal operated print-head. The printhead sprays the ink on a rotating, oil coated drum. The paper then passes over the print drum, at which time the image is immediately transferred, or transfixed, to the page. Solid ink printers are most commonly used as colour office printers, and are excellent at printing on transparencies and other non-porous media. Solid ink printers can produce excellent results. Acquisition and operating costs are similar to laser printers. Drawbacks of the technology include high energy consumption and long warm-up times from a cold state. Also, some users complain that the resulting prints are difficult to write on, as the wax tends to repel inks from pens, and are difficult to feed through automatic document feeders, but these traits have been significantly reduced in later models. In addition, this type of printer is only available from one manufacturer, Xerox, manufactured as part of their Xerox Phaser office printer line. Previously, solid ink printers were manufactured by Tektronix, but Tek sold the printing business to Xerox in 2001.
Dye-sublimation printers.
A dye-sublimation printer (or dye-sub printer) is a printer which employs a printing process that uses heat to transfer dye to a medium such as a plastic card, paper or canvas. The process is usually to lay one colour at a time using a ribbon that has colour panels. Dye-sub printers are intended primarily for high-quality colour applications, including colour photography; and are less well-suited for text. While once the province of high-end print shops, dye-sublimation printers are now increasingly used as dedicated consumer photo printers.
Inkless printers.
Inkless printers work by selectively heating regions of special heat-sensitive paper. Monochrome thermal printers are used in cash registers, ATMs, gasoline dispensers and some older inexpensive fax machines. Colours can be achieved with special papers and different temperatures and heating rates for different colours; these coloured sheets are not required in black-and-white output. One example is the ZINK technology (Zero INK Technology).
Obsolete and special-purpose printing technologies.
The following technologies are either obsolete, or limited to special applications though most were, at one time, in widespread use.
Impact printers.
Impact printers rely on a forcible impact to transfer ink to the media. The impact printer uses a print head that either hits the surface of the ink ribbon, pressing the ink ribbon against the paper (similar to the action of a typewriter), or, less commonly, hits the back of the paper, pressing the paper against the ink ribbon (the IBM 1403 for example). All but the dot matrix printer rely on the use of "fully formed characters", letterforms that represent each of the characters that the printer was capable of printing. In addition, most of these printers were limited to monochrome, or sometimes two-color, printing in a single typeface at one time, although bolding and underlining of text could be done by "overstriking", that is, printing two or more impressions either in the same character position or slightly offset. Impact printers varieties include typewriter-derived printers, teletypewriter-derived printers, daisywheel printers, dot matrix printers and line printers. Dot matrix printers remain in common use in businesses where multi-part forms are printed. "An overview of impact printing" contains a detailed description of many of the technologies used.
Typewriter-derived printers.
Several different computer printers were simply computer-controllable versions of existing electric typewriters. The Friden Flexowriter and IBM Selectric-based printers were the most-common examples. The Flexowriter printed with a conventional typebar mechanism while the Selectric used IBM's well-known "golf ball" printing mechanism. In either case, the letter form then struck a ribbon which was pressed against the paper, printing one character at a time. The maximum speed of the Selectric printer (the faster of the two) was 15.5 characters per second.
Teletypewriter-derived printers.
The common teleprinter could easily be interfaced to the computer and became very popular except for those computers manufactured by IBM. Some models used a "typebox" that was positioned, in the X- and Y-axes, by a mechanism and the selected letter form was struck by a hammer. Others used a type cylinder in a similar way as the Selectric typewriters used their type ball. In either case, the letter form then struck a ribbon to print the letterform. Most teleprinters operated at ten characters per second although a few achieved 15 CPS.
Daisy wheel printers.
Daisy wheel printers operate in much the same fashion as a typewriter. A hammer strikes a wheel with petals, the "daisy wheel", each petal containing a letter form at its tip. The letter form strikes a ribbon of ink, depositing the ink on the page and thus printing a character. By rotating the daisy wheel, different characters are selected for printing. These printers were also referred to as "letter-quality printers" because they could produce text which was as clear and crisp as a typewriter. The fastest letter-quality printers printed at 30 characters per second.
Dot-matrix printers.
The term dot matrix printer is used for impact printers that use a matrix of small pins to transfer ink to the page. The advantage of dot matrix over other impact printers is that they can produce graphical images in addition to text; however the text is generally of poorer quality than impact printers that use letterforms ("type").
Dot-matrix printers can be broadly divided into two major classes:
Dot matrix printers can either be character-based or line-based (that is, a single horizontal series of pixels across the page), referring to the configuration of the print head.
In the 1970s & 80s, dot matrix printers were one of the more common types of printers used for general use, such as for home and small office use. Such printers normally had either 9 or 24 pins on the print head (early 7 pin printers also existed, which did not print descenders). There was a period during the early home computer era when a range of printers were manufactured under many brands such as the Commodore VIC-1525 using the Seikosha Uni-Hammer system. This used a single solenoid with an oblique striker that would be actuated 7 times for each column of 7 vertical pixels while the head was moving at a constant speed. The angle of the striker would align the dots vertically even though the head had moved one dot spacing in the time. The vertical dot position was controlled by a synchronised longitudinally ribbed platen behind the paper that rotated rapidly with a rib moving vertically seven dot spacings in the time it took to print one pixel column. 24-pin print heads were able to print at a higher quality and started to offer additional type styles and were marketed as Near Letter Quality by some vendors. Once the price of inkjet printers dropped to the point where they were competitive with dot matrix printers, dot matrix printers began to fall out of favour for general use.
Some dot matrix printers, such as the NEC P6300, can be upgraded to print in colour. This is achieved through the use of a four-colour ribbon mounted on a mechanism (provided in an upgrade kit that replaces the standard black ribbon mechanism after installation) that raises and lowers the ribbons as needed. Colour graphics are generally printed in four passes at standard resolution, thus slowing down printing considerably. As a result, colour graphics can take up to four times longer to print than standard monochrome graphics, or up to 8-16 times as long at high resolution mode.
Dot matrix printers are still commonly used in low-cost, low-quality applications such as cash registers, or in demanding, very high volume applications like invoice printing. Impact printing, unlike laser printing, allows the pressure of the print head to be applied to a stack of two or more forms to print multi-part documents such as sales invoices and credit card receipts using continuous stationery with carbonless copy paper. Dot-matrix printers were being superseded even as receipt printers after the end of the twentieth century.
Line printers.
Line printers, as the name implies, print an entire line of text at a time. Four principal designs existed.
In each case, to print a line, precisely timed hammers strike against the back of the paper at the exact moment that the correct character to be printed is passing in front of the paper. The paper presses forward against a ribbon which then presses against the character form and the impression of the character form is printed onto the paper.
Line printers were the fastest of all impact printers and were used for bulk printing in large computer centres. A line printer could print at 1100 lines per minute or faster, frequently printing pages more rapidly than many current laser printers. On the other hand, the mechanical components of line printers operated with tight tolerances and required regular preventive maintenance (PM) to produce top quality print. They were virtually never used with personal computers and have now been replaced by high-speed laser printers. The legacy of line printers lives on in many computer operating systems, which use the abbreviations "lp", "lpr", or "LPT" to refer to printers.
Liquid ink electrostatic printers.
Liquid ink electrostatic printers use a chemical coated paper, which is charged by the print head according to the image of the document. The paper is passed near a pool of liquid ink with the opposite charge. The charged areas of the paper attract the ink and thus form the image. This process was developed from the process of electrostatic copying. Color reproduction is very accurate, and because there is no heating the scale distortion is less than ±0.1%. (All laser printers have an accuracy of ±1%.)
Worldwide, most survey offices used this printer before color inkjet plotters become popular. Liquid ink electrostatic printers were mostly available in width and also 6 color printing. These were also used to print large billboards. It was first introduced by Versatec, which was later bought by Xerox. 3M also used to make these printers.
Plotters.
Pen-based plotters were an alternate printing technology once common in engineering and architectural firms. Pen-based plotters rely on contact with the paper (but not impact, per se) and special purpose pens that are mechanically run over the paper to create text and images. Since the pens output continuous lines, they were able to produce technical drawings of higher resolution than was achievable with dot-matrix technology. Some plotters used roll-fed paper, and therefore had minimal restriction on the size of the output in one dimension. These plotters were capable of producing quite sizable drawings.
Other printers.
A number of other sorts of printers are important for historical reasons, or for special purpose uses:
Attributes.
Printer control languages.
Most printers other than line printers accept control characters or unique character sequences to control various printer functions. These may range from shifting from lower to upper case or from black to red ribbon on typewriter printers to switching fonts and changing character sizes and colors on raster printers. Early printer controls were not standardized, with each manufacturer's equipment having its own set. The IBM Personal Printer Data Stream (PPDS) became a commonly used command set for dot-matrix printers.
Today, most printers accept one or more page description languages (PDLs). Laser printers with greater processing power frequently offer support for variants of Hewlett-Packard's Printer Command Language (PCL), PostScript or XML Paper Specification. Most inkjet devices support manufacturer proprietary PDLs such as ESC/P. The diversity in mobile platforms have led to various standardization efforts around device PDLs such as the Printer Working Group (PWG's) PWG Raster.
Printing speed.
The speed of early printers was measured in units of "characters per minute" (cpm) for character printers, or "lines per minute" (lpm) for line printers. Modern printers are measured in "pages per minute" (ppm). These measures are used primarily as a marketing tool, and are not as well standardised as toner yields. Usually pages per minute refers to sparse monochrome office documents, rather than dense pictures which usually print much more slowly, especially colour images. PPM are most of the time referring to A4 paper in Europe and letter paper in the United States, resulting in a 5-10% difference.
Sales.
Since 2005, the world's top selling brand of inkjet and laser printers has been HP, which now has 46% of sales in inkjet and 55.5% in laser printers.
Printing mode.
The data received by a printer may be:
Some printers can process all four types of data, others not.
Today it is possible to print everything (even plain text) by sending ready bitmapped images to the printer. This allows better control over formatting, especially among machines from different vendors. Many printer drivers do not use the text mode at all, even if the printer is capable of it.
Monochrome, colour and photo printers.
A monochrome printer can only produce an image consisting of one colour, usually black. A monochrome printer may also be able to produce various tones of that color, such as a grey-scale. A colour printer can produce images of multiple colours. A photo printer is a colour printer that can produce images that mimic the colour range (gamut) and resolution of prints made from photographic film. Many can be used on a standalone basis without a computer, using a memory card or USB connector.
Page yield.
The page yield is number of pages that can be printed from a toner cartridge or ink cartridge—before the cartridge needs to be refilled or replaced.
The actual number of pages yielded by a specific cartridge depends on a number of factors.
For a fair comparison, many laser printer manufacturers use the ISO/IEC 19752 process to measure "the" toner cartridge yield.
Cost per page.
In order to fairly compare operating expenses of printers with a relatively small ink cartridge to printers with a larger, more expensive toner cartridge that typically holds more toner and so prints more pages before the cartridge needs to be replaced, many people prefer to estimate operating expenses in terms of cost per page (CPP).
Business model.
Often the "razor and blades" business model is applied. That is, a company may sell a printer at cost, and make profits on the ink cartridge, paper, or some other replacement part. This has caused legal disputes regarding the right of companies other than the printer manufacturer to sell compatible ink cartridges. To protect their business model, several manufacturers invest heavily in developing new cartridge technology and patenting it.
Other manufacturers, in reaction to the challenges from using this business model, choose to make more money on printers and less on the ink, promoting the latter through their advertising campaigns. Finally, this generates two clearly different proposals: "cheap printer – expensive ink" or "expensive printer – cheap ink". Ultimately, the consumer decision depends on their reference interest rate or their time preference. From an economics viewpoint, there is a clear trade-off between cost per copy and cost of the printer.
Printer steganography.
Printer steganography is a type of steganography – "hiding data within data" – produced by color printers, including Brother, Canon, Dell, Epson, HP, IBM, Konica Minolta, Kyocera, Lanier, Lexmark, Ricoh, Toshiba and Xerox brand color laser printers, where tiny yellow dots are added to each page. The dots are barely visible and contain encoded printer serial numbers, as well as date and time stamps.
Wireless printers.
More than half of all printers sold at U.S. retail in 2010 were wireless-capable, but nearly three-quarters of consumers who have access to those printers weren't taking advantage of the increased access to print from multiple devices according to the new Wireless Printing Study.

</doc>
<doc id="5278" url="https://en.wikipedia.org/wiki?curid=5278" title="Copyright">
Copyright

Copyright is a legal right created by the law of a country that grants the creator of an original work exclusive rights for its use and distribution. This is usually only for a limited time. The exclusive rights are not absolute but limited by limitations and exceptions to copyright law, including fair use. A major limitation on copyright is that copyright protects only the original expression of ideas, and not the underlying ideas themselves.
Copyright is a form of intellectual property, applicable to certain forms of creative work. Under US copyright law, legal protection attaches only to "fixed" representations in a tangible medium. The Berne Convention allows member countries to decide whether creative works must be "fixed" to enjoy copyright. Article 2, Section 2 of the Berne Convention states: "It shall be a matter for legislation in the countries of the Union to prescribe that works in general or any specified categories of works shall not be protected unless they have been fixed in some material form." Some countries do not require that a work be produced in a particular form to obtain copyright protection. For instance, Spain, France, and Australia do not require fixation for copyright protection. The United States and Canada, on the other hand, require that most works must be "fixed in a tangible medium of expression" to obtain copyright protection. U.S. law requires that the fixation be stable and permanent enough to be "perceived, reproduced or communicated for a period of more than transitory duration." Similarly, Canadian courts consider fixation to require that the work be "expressed to some extent at least in some material form, capable of identification and having a more or less permanent endurance." It is often shared among multiple authors, each of whom holds a set of rights to use or license the work, and who are commonly referred to as rightsholders. These rights frequently include reproduction, control over derivative works, distribution, public performance, and "moral rights" such as attribution.
Copyrights are considered "territorial" rights, which means that they do not extend beyond the territory of a specific jurisdiction. While many aspects of national copyright laws have been standardized through international copyright agreements, copyright laws vary by country.
Typically, the "duration" of a copyright spans the author's life plus 50 to 100 years (that is, copyright typically expires 50 to 100 years after the author dies, depending on the jurisdiction). Some countries require certain copyright formalities to establishing copyright, but most recognize copyright in any completed work, without formal registration. Generally, copyright is enforced as a civil matter, though some jurisdictions do apply criminal sanctions.
Most jurisdictions recognize copyright limitations, allowing "fair" exceptions to the creator's exclusivity of copyright and giving users certain rights. The development of digital media and computer network technologies have prompted reinterpretation of these exceptions, introduced new difficulties in enforcing copyright, and inspired additional challenges to copyright law's philosophic basis. Simultaneously, businesses with great economic dependence upon copyright, such as those in the music business, have advocated the extension and expansion of copyright and sought additional legal and technological enforcement.
History.
Copyright came about with the invention of the printing press and with wider literacy. As a legal concept, its origins in Britain were from a reaction to printers' monopolies at the beginning of the 18th century. Charles II of England was concerned by the unregulated copying of books and passed the Licensing of the Press Act 1662 by Act of Parliament, which established a register of licensed books and required a copy to be deposited with the Stationers' Company, essentially continuing the licensing of material that had long been in effect.
The British Statute of Anne (also known as the Copyright Act 1709) further alluded to individual rights of the artist. It began, "Whereas Printers, Booksellers, and other Persons, have of late frequently taken the Liberty of Printing... Books, and other Writings, without the Consent of the Authors... to their very great Detriment, and too often to the Ruin of them and their Families:". A right to benefit financially from the work is articulated, and court rulings and legislation have recognized a right to control the work, such as ensuring that the integrity of it is preserved. An irrevocable right to be recognized as the work's creator appears in some countries' copyright laws.
Copyright laws allow products of creative human activities, such as literary and artistic production, to be preferentially exploited and thus incentivized. Different cultural attitudes, social organizations, economic models and legal frameworks are seen to account for why copyright emerged in Europe and not, for example, in Asia. In the Middle Ages in Europe, there was generally a lack of any concept of literary property due to the general relations of production, the specific organization of literary production and the role of culture in society. The latter refers to the tendency of oral societies, such as that of Europe in the medieval period, to view knowledge as the product and expression of the collective, rather than to see it as individual property. However, with copyright laws, intellectual production comes to be seen as a product of an individual, with attendant rights. The most significant point is that patent and copyright laws support the expansion of the range of creative human activities that can be commodified. This parallels the ways in which capitalism led to the commodification of many aspects of social life that earlier had no monetary or economic value per se.
The Statute of Anne was the first real copyright act, and gave the publishers rights for a fixed period, after which the copyright expired. Copyright has grown from a legal concept regulating copying rights in the publishing of books and maps to one with a significant effect on nearly every modern industry, covering such items as sound recordings, films, photographs, software, and architectural works.
Prior to the passage of the United States Constitution, several states passed their own copyright laws between 1783 and 1787, the first being Connecticut. Contemporary scholars and patriots such as Noah Webster, John Trumbull (poet), and Joel Barlow were instrumental in securing the passage of these statutes. 
The Copyright Clause of the United States Constitution (1787) authorized copyright legislation: "To promote the Progress of Science and useful Arts, by securing for limited Times to Authors and Inventors the exclusive Right to their respective Writings and Discoveries." That is, by guaranteeing them a period of time in which they alone could "profit" from their works, they would be enabled and encouraged to invest the time required to create them, and this would be good for society as a whole. A right to profit from the work has been the philosophical underpinning for much legislation extending the duration of copyright, to the life of the creator and beyond, to their heirs.
The original length of copyright in the United States was 14 years, and it had to be explicitly applied for. If the author wished, they could apply for a second 14‑year monopoly grant, but after that the work entered the public domain, so it could be used and built upon by others.
Thomas Jefferson, who strongly advocated the ability of the public to share and build upon the works of others, proposed as part of the Bill of Rights that a short timespan be protected:
Copyright law was enacted rather late in German states, and the historian Eckhard Höffner argues that the absence of copyright laws in the early 19th century encouraged publishing, was profitable for authors, led to a proliferation of books, enhanced knowledge, and was ultimately an important factor in the ascendency of Germany as a power during that century.
The 1886 Berne Convention first established recognition of copyrights among sovereign nations, rather than merely bilaterally. Under the Berne Convention, copyrights for creative works do not have to be asserted or declared, as they are automatically in force at creation: an author need not "register" or "apply for" a copyright in countries adhering to the Berne Convention. As soon as a work is "fixed", that is, written or recorded on some physical medium, its author is automatically entitled to all copyrights in the work, and to any derivative works unless and until the author explicitly disclaims them, or until the copyright expires. The Berne Convention also resulted in foreign authors being treated equivalently to domestic authors, in any country signed onto the Convention. The UK signed the Berne Convention in 1887 but did not implement large parts of it until 100 years later with the passage of the "Copyright, Designs and Patents Act of 1988". The United States did not sign the Berne Convention until 1989.
The United States and most Latin American countries instead entered into the Buenos Aires Convention in 1910, which required a copyright notice on the work (such as "all rights reserved"), and permitted signatory nations to limit the duration of copyrights to shorter and renewable terms. The Universal Copyright Convention was drafted in 1952 as another less demanding alternative to the Berne Convention, and ratified by nations such as the Soviet Union and developing nations.
The regulations of the Berne Convention are incorporated into the World Trade Organization's TRIPS agreement (1995), thus giving the Berne Convention effectively near-global application. The 2002 WIPO Copyright Treaty enacted greater restrictions on the use of technology to copy works in the nations that ratified it.
Scope.
Copyright may apply to a wide range of creative, intellectual, or artistic forms, or "works". Specifics vary by jurisdiction, but these can include poems, theses, plays and other literary works, motion pictures, choreography, musical compositions, sound recordings, paintings, drawings, sculptures, photographs, computer software, radio and television broadcasts, and industrial designs. Graphic designs and industrial designs may have separate or overlapping laws applied to them in some jurisdictions.
Copyright does not cover ideas and information themselves, only the form or manner in which they are expressed. For example, the copyright to a Mickey Mouse cartoon restricts others from making copies of the cartoon or creating derivative works based on Disney's particular anthropomorphic mouse, but does not prohibit the creation of other works about anthropomorphic mice in general, so long as they are different enough to not be judged copies of Disney's. Note additionally that Mickey Mouse is not copyrighted because characters cannot be copyrighted; rather, Steamboat Willie is copyrighted and Mickey Mouse, as a character in that copyrighted work, is afforded protection.
In many jurisdictions, copyright law makes exceptions to these restrictions when the work is copied for the purpose of commentary or other related uses (See fair use, fair dealing). It should be noted that US copyright does NOT cover names, title, short phrases or Listings (such as ingredients, recipes, labels, or formulas). However, there are protections available for those areas copyright does not cover – such as trademarks and patents.
Copyright laws are standardized somewhat through international conventions such as the Berne Convention and Universal Copyright Convention. These multilateral treaties have been ratified by nearly all countries, and international organizations such as the European Union or World Trade Organization require their member states to comply with them.
Exceptions to copyright.
There are some exceptions to what copyright will protect. Copyright will not protect:
Obtaining and enforcing copyright.
Typically, a work must meet minimal standards of originality in order to qualify for copyright, and the copyright expires after a set period of time (some jurisdictions may allow this to be extended). Different countries impose different tests, although generally the requirements are low; in the United Kingdom there has to be some "skill, labour, and judgment" that has gone into it. In Australia and the United Kingdom it has been held that a single word is insufficient to comprise a copyright work. However, single words or a short string of words can sometimes be registered as a trademark instead.
Copyright law recognizes the right of an author based on whether the work actually is an original creation, rather than based on whether it is unique; two authors may own copyright on two substantially identical works, if it is determined that the duplication was coincidental, and neither was copied from the other.
In all countries where the Berne Convention standards apply, copyright is automatic, and need not be obtained through official registration with any government office. Once an idea has been reduced to tangible form, for example by securing it in a fixed medium (such as a drawing, sheet music, photograph, a videotape, or a computer file), the copyright holder is entitled to enforce his or her exclusive rights. However, while registration isn't needed to exercise copyright, in jurisdictions where the laws provide for registration, it serves as "prima facie" evidence of a valid copyright and enables the copyright holder to seek statutory damages and attorney's fees. (In the USA, registering "after" an infringement only enables one to receive actual damages and lost profits.)
The original holder of the copyright may be the "employer" of the author rather than the author himself, if the work is a "work for hire". For example, in English law the "Copyright, Designs and Patents Act" 1988 provides that if a copyrighted work is made by an employee in the course of that employment, the copyright is automatically owned by the employer which would be a "Work for Hire."
Copyrights are generally enforced by the holder in a civil law court, but there are also criminal infringement statutes in some jurisdictions. While central registries are kept in some countries which aid in proving claims of ownership, registering does not necessarily prove ownership, nor does the fact of copying (even without permission) necessarily prove that copyright was infringed. Criminal sanctions are generally aimed at serious counterfeiting activity, but are now becoming more commonplace as copyright collectives such as the RIAA are increasingly targeting the file sharing home Internet user. Thus far, however, most such cases against file sharers have been settled out of court. (See: Legal aspects of file sharing)
Cost of enforcing copyright.
In most jurisdictions the copyright holder must bear the cost of enforcing copyright. This will usually involve engaging legal representation, administrative and or court costs. In light of this, many copyright disputes are settled by a direct approach to the infringing party in order to settle the dispute out of court.
Copyright notices in the United States.
Before 1989, the use of a copyright notice – consisting of the copyright symbol (©, the letter C inside a circle), the abbreviation "Copr.", or the word "Copyright", followed by the year of the first publication of the work and the name of the copyright holder – was part of U. S. statutory requirements. Several years may be noted if the work has gone through substantial revisions. The proper copyright notice for sound recordings of musical or other audio works is a sound recording copyright symbol (℗, the letter P inside a circle), which indicates a sound recording copyright, with the letter P indicating a "phonorecord". In addition, the phrase "All rights reserved" was once required to assert copyright, but that phrase is now legally obsolete.
In 1989 the United States enacted the Berne Convention Implementation Act, amending the 1976 Copyright Act to conform to most of the provisions of the Berne Convention. As a result, the use of copyright notices has become optional to claim copyright, because the Berne Convention makes copyright automatic. However, the lack of notice of copyright using these marks may have consequences in terms of reduced damages in an infringement lawsuit – using notices of this form may reduce the likelihood of a defense of "innocent infringement" being successful.
"Poor man's copyright".
A widely circulated strategy to avoid the cost of copyright registration is referred to as the "poor man's copyright". It proposes that the creator send the work to himself in a sealed envelope by registered mail, using the postmark to establish the date. This technique has not been recognized in any published opinions of the United States courts. The United States Copyright Office makes it clear that the technique is no substitute for actual registration. The United Kingdom Intellectual Property Office discusses the technique and notes that the technique (as well as commercial registries) does not constitute dispositive proof that the work is original nor who the creator of the work is. 
Exclusive rights.
Several exclusive rights typically attach to the holder of a copyright:
The phrase "exclusive right" means that only the copyright holder is free to exercise those rights, and others are prohibited from using the work without the holder's permission. Copyright is sometimes called a "negative right", as it serves to "prohibit" certain people (e.g., readers, viewers, or listeners, and primarily publishers and would be publishers) from doing something they would otherwise be able to do, rather than "permitting" people (e.g., authors) to do something they would otherwise be unable to do. In this way it is similar to the unregistered design right in English law and European law. The rights of the copyright holder also permit him/her to "not" use or exploit their copyright, for some or all of the term. There is, however, a critique which rejects this assertion as being based on a philosophical interpretation of copyright law that is not universally shared. There is also debate on whether copyright should be considered a property right or a moral right.
Useful articles.
If a pictorial, graphic or sculptural work is a useful article, it is copyrighted only if its aesthetic features are separable from its utilitarian features. A useful article is an article having an intrinsic utilitarian function that is not merely to portray the appearance of the article or to convey information. They must be separable from the functional aspect to be copyrighted.
Limitations and exceptions to copyright.
Idea–expression dichotomy and the merger doctrine.
The idea–expression divide differentiates between ideas and expression, and states that copyright protects only the original expression of ideas, and not the ideas themselves. This principle, first clarified in the 1879 case of Baker v. Selden, has since been codified by the Copyright Act of 1976 at 17 U.S.C. § 102(b).
The first-sale doctrine and exhaustion of rights.
Copyright law does "not" restrict the owner of a copy from reselling legitimately obtained copies of copyrighted works, provided that those copies were originally produced by or with the permission of the copyright holder. It is therefore legal, for example, to resell a copyrighted book or CD. In the United States this is known as the first-sale doctrine, and was established by the courts to clarify the legality of reselling books in second-hand bookstores.
Some countries may have parallel importation restrictions that allow the copyright holder to control the aftermarket. This may mean for example that a copy of a book that does not infringe copyright in the country where it was printed does infringe copyright in a country into which it is imported for retailing. The first-sale doctrine is known as exhaustion of rights in other countries and is a principle which also applies, though somewhat differently, to patent and trademark rights. It is important to note that the first-sale doctrine permits the transfer of the particular legitimate copy involved. It does not permit making or distributing additional copies.
In "Kirtsaeng v. John Wiley & Sons, Inc.", in 2013, the United States Supreme Court held in a 6-3 decision that the first-sale doctrine applies to goods manufactured abroad with the copyright owner's permission and then imported into the US without such permission. The case involved a plaintiff who imported Asian editions of textbooks that had been manufactured abroad with the publisher-plaintiff's permission. The defendant, without permission from the publisher, imported the textbooks and resold on eBay. The Supreme Court's holding severely limits the ability of copyright holders to prevent such importation.
In addition, copyright, in most cases, does not prohibit one from acts such as modifying, defacing, or destroying his or her own legitimately obtained copy of a copyrighted work, so long as duplication is not involved. However, in countries that implement moral rights, a copyright holder can in some cases successfully prevent the mutilation or destruction of a work that is publicly visible.
Fair use and fair dealing.
Copyright does not prohibit all copying or replication. In the United States, the fair use doctrine, codified by the Copyright Act of 1976 as 17 U.S.C. Section 107, permits some copying and distribution without permission of the copyright holder or payment to same. The statute does not clearly define fair use, but instead gives four non-exclusive factors to consider in a fair use analysis. Those factors are:
In the United Kingdom and many other Commonwealth countries, a similar notion of fair dealing was established by the courts or through legislation. The concept is sometimes not well defined; however in Canada, private copying for personal use has been expressly permitted by statute since 1999. In "Alberta (Education) v. Canadian Copyright Licensing Agency (Access Copyright)", 2012 SCC 37, the Supreme Court of Canada concluded that limited copying for educational purposes could also be justified under the fair dealing exemption. In Australia, the fair dealing exceptions under the "Copyright Act 1968" (Cth) are a limited set of circumstances under which copyrighted material can be legally copied or adapted without the copyright holder's consent. Fair dealing uses are research and study; review and critique; news reportage and the giving of professional advice (i.e. legal advice). Under current Australian law it is still a breach of copyright to copy, reproduce or adapt copyright material for personal or private use without permission from the copyright owner. Other technical exemptions from infringement may also apply, such as the temporary reproduction of a work in machine readable form for a computer.
In the United States the AHRA (Audio Home Recording Act Codified in Section 10, 1992) prohibits action against consumers making noncommercial recordings of music, in return for royalties on both media and devices plus mandatory copy-control mechanisms on recorders.
Later acts amended US Copyright law so that for certain purposes making 10 copies or more is construed to be commercial, but there is no general rule permitting such copying. Indeed, making one complete copy of a work, or in many cases using a portion of it, for commercial purposes will not be considered fair use. The Digital Millennium Copyright Act prohibits the manufacture, importation, or distribution of devices whose intended use, or only significant commercial use, is to bypass an access or copy control put in place by a copyright owner. An appellate court has held that fair use is not a defense to engaging in such distribution.
The copyright directive allows EU member states to implement a set of exceptions to copyright. Examples of those exceptions are:
Accessible copies.
It is legal in several countries including the United Kingdom and the United States to produce alternative versions (for example, in large print or braille) of a copyrighted work to provide improved access to a work for blind and visually impaired persons without permission from the copyright holder.
Transfer and licensing, and assignment.
A copyright, or aspects of it, may be assigned or transferred from one party to another. For example, a musician who records an album will often sign an agreement with a record company in which the musician agrees to transfer all copyright in the recordings in exchange for royalties and other considerations. The creator (and original copyright holder) benefits, or expects to, from production and marketing capabilities far beyond those of the author. In the digital age of music, music may be copied and distributed at minimal cost through the Internet, however the record industry attempts to provide promotion and marketing for the artist and his or her work so it can reach a much larger audience. A copyright holder need not transfer all rights completely, though many publishers will insist. Some of the rights may be transferred, or else the copyright holder may grant another party a non-exclusive license to copy and/or distribute the work in a particular region or for a specified period of time.
A transfer or licence may have to meet particular formal requirements in order to be effective, for example under the Australian Copyright Act 1968 the copyright itself must be expressly transferred in writing. Under the U.S. Copyright Act, a transfer of ownership in copyright must be memorialized in a writing signed by the transferor. For that purpose, ownership in copyright includes exclusive licenses of rights. Thus exclusive licenses, to be effective, must be granted in a written instrument signed by the grantor. No special form of transfer or grant is required. A simple document that identifies the work involved and the rights being granted is sufficient. Non-exclusive grants (often called non-exclusive licenses) need not be in writing under U.S. law. They can be oral or even implied by the behavior of the parties. Transfers of copyright ownership, including exclusive licenses, may and should be recorded in the U.S. Copyright Office. (Information on recording transfers is available on the Office's web site.) While recording is not required to make the grant effective, it offers important benefits, much like those obtained by recording a deed in a real estate transaction.
Copyright may also be licensed. Some jurisdictions may provide that certain classes of copyrighted works be made available under a prescribed statutory license (e.g. musical works in the United States used for radio broadcast or performance). This is also called a compulsory license, because under this scheme, anyone who wishes to copy a covered work does not need the permission of the copyright holder, but instead merely files the proper notice and pays a set fee established by statute (or by an agency decision under statutory guidance) for every copy made. Failure to follow the proper procedures would place the copier at risk of an infringement suit. Because of the difficulty of following every individual work, copyright collectives or collecting societies and performing rights organizations (such as ASCAP, BMI, and SESAC) have been formed to collect royalties for hundreds (thousands and more) works at once. Though this market solution bypasses the statutory license, the availability of the statutory fee still helps dictate the price per work collective rights organizations charge, driving it down to what avoidance of procedural hassle would justify.
Free licences.
There are a large number of free licenses, where users are granted several rights; for example, those mentioned in the Free Software Definition, Open Source Definition, Debian Free Software Guidelines or Definition of Free Cultural Works. Examples of free licences are the GNU General Public License, BSD license and some Creative Commons licenses.
Founded in 2001 by James Boyle, Lawrence Lessig, and Hal Abelson, the Creative Commons (CC) is a non-profit organization which aims to facilitate the legal sharing of creative works. To this end, the organization provides a number of copyright license options to the public, free of charge. These licenses allow copyright holders to define conditions under which others may use a work and to specify what types of use are acceptable.
Terms of use have traditionally been negotiated on an individual basis between copyright holder and potential licensee. Therefore, a general CC license outlining which rights the copyright holder is willing to waive enables the general public to use such works more freely. Six general types of CC licenses are available. These are based upon copyright holder stipulations such as whether he or she is willing to allow modifications to the work, whether he or she permits the creation of derivative works and whether he or she is willing to permit commercial use of the work. approximately 130 million individuals had received such licenses.
Duration.
Copyright term.
Copyright subsists for a variety of lengths in different jurisdictions. The length of the term can depend on several factors, including the type of work (e.g. musical composition, novel), whether the work has been published, and whether the work was created by an individual or a corporation. In most of the world, the default length of copyright is the life of the author plus either 50 or 70 years. In the United States, the term for most existing works is a fixed number of years after the date of creation or publication. Under most countries' laws (for example, the United States and the United Kingdom), copyrights expire at the end of the calendar year in question.
The length and requirements for copyright duration are subject to change by legislation, and since the early 20th century there have been a number of adjustments made in various countries, which can make determining the duration of a given copyright somewhat difficult. For example, the United States used to require copyrights to be renewed after 28 years to stay in force, and formerly required a copyright notice upon first publication to gain coverage. In Italy and France, there were post-wartime extensions that could increase the term by approximately 6 years in Italy and up to about 14 in France. Many countries have extended the length of their copyright terms (sometimes retroactively). International treaties establish minimum terms for copyrights, but individual countries may enforce longer terms than those.
In the United States, all books and other works published before 1923 have expired copyrights and are in the public domain. In addition, works published before 1964 that did not have their copyrights renewed 28 years after first publication year also are in the public domain. Hirtle points out that the great majority of these works (including 93% of the books) were not renewed after 28 years and are in the public domain. Books originally published outside the US by non-Americans are exempt from this renewal requirement, if they are still under copyright in their home country.
But if the intended exploitation of the work includes publication (or distribution of derivative work, such as a film based on a book protected by copyright) outside the U.S., the terms of copyright around the world must be considered. If the author has been dead more than 70 years, the work is in the public domain in most, but not all, countries.
In 1998, the length of a copyright in the United States was increased by 20 years under the Copyright Term Extension Act. This legislation was strongly promoted by corporations which had valuable copyrights which otherwise would have expired, and has been the subject of substantial criticism on this point.
As a curiosity, the famous work "Peter Pan, or The Boy Who Wouldn't Grow Up" has a complex – and disputed – story of copyright expiry.
Public domain.
Copyright, like other intellectual property rights, is subject to a statutorily determined term. Once the term of a copyright has expired, the formerly copyrighted work enters the public domain and may be freely used or exploited by anyone. Courts in common law countries, such as the United States and the United Kingdom, have rejected the doctrine of a common law copyright. Public domain works should not be confused with works that are publicly available. Works posted in the internet, for example, are publicly available, but are not generally in the public domain. Copying such works may therefore violate the author's copyright.
Copyright infringement.
For a work to be considered to infringe upon copyright, its use must have occurred in a nation that has domestic copyright laws and/or adheres to a bilateral treaty or established international convention such as the Berne Convention or WIPO Copyright Treaty. Improper use of materials outside of legislation is deemed "unauthorized edition", not copyright infringement.
Copyright infringement most often occurs to software, film and music. However, infringement upon books and other text works remains common, especially for educational reasons. Statistics regarding the effects of copyright infringement are difficult to determine. Studies have attempted to determine whether there is a monetary loss for industries affected by copyright infringement by predicting what portion of pirated works would have been formally purchased if they had not been freely available. Other reports indicate that copyright infringement does not have an adverse effect on the entertainment industry, and can have a positive effect. In particular, a 2014 university study concluded that free music content, accessed on YouTube, does not necessarily hurt sales, instead has the potential to increase sales.

</doc>
<doc id="5282" url="https://en.wikipedia.org/wiki?curid=5282" title="Catalan language">
Catalan language

Catalan (; autonym: "català" ) is a Romance language named for its origins in Catalonia, in what is northeastern Spain and adjoining parts of France. It is the national and only official language of Andorra, and a co-official language of the Spanish autonomous communities of Catalonia, the Balearic Islands, and Valencia (where the language is known as Valencian, and there exist regional standards). It also has semi-official status in the city of Alghero on the Italian island of Sardinia. It is also spoken with no official recognition in parts of the Spanish autonomous communities of Aragon (La Franja) and Murcia (Carche), and in the historic French region of Roussillon/Northern Catalonia, roughly equivalent to the department of Pyrénées-Orientales.
According to the Statistical Institute of Catalonia in 2008 the Catalan language is the second most commonly used in Catalonia, after Spanish, as a native or self-defining language. The Generalitat of Catalunya spends part of its annual budget on the promotion of the use of Catalan in Catalonia and in other territories.
Catalan evolved from Vulgar Latin around the eastern Pyrenees in the 9th century. During the Low Middle Ages it saw a golden age as the literary and dominant language of the Crown of Aragon, and was widely used all over the Mediterranean. The union of Aragon with the other territories of Spain in 1479 marked the start of the decline of the language. In 1659 Spain ceded Northern Catalonia to France, and Catalan was banned in both states in the early 18th century. 19th-century Spain saw a Catalan literary revival, which culminated in the 1913 orthographic standardization, and the officialization of the language during the Second Spanish Republic (1931–39). However, the Francoist dictatorship (1939–75) banned the language again.
Since the Spanish transition to democracy (1975–1982), Catalan has been recognized as an official language, language of education, and language of mass media, all of which have contributed to its increased prestige. There is no parallel in Europe of such a large, bilingual, non-state speech community.
Catalan dialects are relatively uniform, and are mutually intelligible. They are divided into two blocks, Eastern and Western, differing mostly in pronunciation. The terms "Catalan" and "Valencian" (respectively used in Catalonia and the Valencian Community) are two different varieties of the same language. There are two institutions regulating the two standard varieties, the Institute of Catalan Studies in Catalonia and the Valencian Academy of the Language in Valencia.
Catalan shares many traits with its neighboring Romance languages. However, despite being mostly situated in the Iberian Peninsula, Catalan differs more from Iberian Romance (such as Spanish and Portuguese) in terms of vocabulary, pronunciation, and grammar than from Gallo-Romance (Occitan, French, Gallo-Italic languages, etc.). These similarities are most notable with Occitan.
Catalan has an inflectional grammar, with two genders (masculine, feminine), and two numbers (singular, plural). Pronouns are also inflected for case, animacy and politeness, and can be combined in very complex ways. Verbs are split in several paradigms and are inflected for person, number, tense, aspect, mood, and gender. In terms of pronunciation, Catalan has many words ending in a wide variety of consonants and some consonant clusters, in contrast with many other Romance languages.
Etymology and pronunciation.
The word "Catalan" derives from the territory of Catalonia, itself of disputed etymology. The main theory suggests that "Catalunya" (Latin "Gathia Launia") derives from the name "Gothia" or "Gauthia" ("Land of the Goths"), since the origins of the Catalan counts, lords and people were found in the March of Gothia, whence "Gothland" > "Gothlandia" > "Gothalania" > "Catalonia" theoretically derived.
In English, the term referring to a person first appears in the mid 14th century as "Catelaner", followed in the 15th century as "Catellain" (from French). It is attested a language name since at least 1652. "Catalan" can be pronounced as , or .
The endonym is pronounced in the Eastern Catalan dialects, and in the Western dialects. In the Valencian Community, the term "valencià" () is frequently used instead. The names "Catalan" and "Valencian" are two names for the same language. See also status of Valencian below.
History.
Middle Ages.
By the 9th century, Catalan had evolved from Vulgar Latin on both sides of the eastern end of the Pyrenees, as well as the territories of the Roman province of Hispania Tarraconensis to the south. From the 8th century onwards the Catalan counts extended their territory southwards and westwards at the expense of the Muslims, bringing their language with them. This process was given definitive impetus with the separation of the County of Barcelona from the Carolingian Empire in 988.
In the 11th century, documents written in macaronic Latin begin to show Catalan elements, with texts written almost completely in Romance appearing by 1080. Old Catalan shared many features with Gallo-Romance, diverging from Old Occitan between the 11th and 14th centuries.
During the 11th and 12th centuries the Catalan rulers expanded up to north of the Ebro river, and in the 13th century they conquered the Land of Valencia and the Balearic Islands. The city of Alghero in Sardinia was repopulated with Catalan speakers in the 14th century. The language also reached Murcia, which became Spanish-speaking in the 15th century.
In the Low Middle Ages, Catalan went through a golden age, reaching a peak of maturity and cultural richness. Examples include the work of Majorcan polymath Ramon Llull (1232–1315), the Four Great Chronicles (13th–14th centuries), and the Valencian school of poetry culminating in Ausiàs March (1397–1459). By the 15th century, the city of Valencia had become the sociocultural center of the Crown of Aragon, and Catalan was present all over the Mediterranean world. During this period, the Royal Chancery propagated a highly standardized language. Catalan was widely used as an official language in Sicily until the 15th century, and in Sardinia until the 17th. During this period, the language was what Costa Carreras terms "one of the 'great languages' of medieval Europe".
Martorell's outstanding novel of chivalry "Tirant lo Blanc" (1490) shows a transition from Medieval to Renaissance values, something that can also be seen in Metge's work. The first book produced with movable type in the Iberian Peninsula was printed in Catalan.
Start of the modern era.
With the union of the crowns of Castille and Aragon (1479), the use of Spanish gradually became more prestigious. Starting in the 16th century, Catalan literature experienced a decline, the language came under the influence of Spanish, and the urban and literary classes became bilingual.
French state: 18th to 20th centuries.
With the Treaty of the Pyrenees (1659), Spain ceded the northern part of Catalonia to France, and soon thereafter the local Catalan varieties came under the influence of French, which in 1700 became the sole official language of the region.
Shortly after the French Revolution (1789), the French First Republic prohibited official use of, and enacted discriminating policies against, the nonstandard languages of France ("patois"), such as Catalan, Alsatian, Breton, Occitan, Flemish, and Basque.
Following the French capture of Algeria (1833), that region saw several waves of Catalan-speaking settlers. People from the Spanish Alacant province settled around Oran, whereas Algiers received immigration from Northern Catalonia and Minorca. Their speech was known as "patuet". By 1911, the number of Catalan speakers was around 100,000. After the declaration of independence of Algeria in 1962, almost all the Catalan speakers fled to Northern Catalonia (as "Pieds-Noirs") or Alacant.
Nowadays, France only recognizes French as an official language. Nevertheless, on 10 December 2007, the General Council of the Pyrénées-Orientales officially recognized Catalan as one of the languages of the department and seeks to further promote it in public life and education.
Spanish state: 18th to 20th centuries.
The decline of Catalan continued in the 16th and 17th centuries. The Catalan defeat in the War of Spanish Succession (1714) initiated a series of measures imposing the use of Spanish in legal documentation.
In parallel, however, the 19th century saw a Catalan literary revival ("Renaixença"), which has continued up to the present day. This period starts with Aribau's "Ode to the Homeland" (1833); followed in the second half of the 19th century, and the early 20th by the work of Verdaguer (poetry), Oller (realist novel), and Guimerà (drama).
In the 19th century, the region of Carche, in the province of Murcia was repopulated with Catalan speakers from the Land of Valencia.
The Second Spanish Republic (1931–1939) saw a brief period of tolerance, with most restrictions against Catalan being lifted.
Present day.
Since the Spanish transition to democracy (1975–1982), Catalan has been institutionalizated as an official language, language of education, and language of mass media; all of which have contributed to its increased prestige. In Catalonia, there is no parallel of a large, bilingual, European, non-state speech community. The teaching of Catalan is mandatory in all schools, but it is possible to use Spanish for studying in the public education system of Catalonia in two situations, if the teacher assigned to a class chooses to use Spanish, or during the learning process of one or some recently arrived students. There is also some intergenerational shift towards Catalan.
In Andorra, Catalan has always been the sole official language. Since the promulgation of the 1993 constitution, several Andorranization policies have been enforced, like Catalan medium education.
On the other hand, there are several language shift processes currently taking place. In Northern Catalonia, Catalan has followed the same trend as the other minority languages of France, with most of its native speakers being 60 or older (as of 2004). Catalan is studied as a foreign language by 30% of the primary education students, and by 15% of the secondary. The cultural association "La Bressola" promotes a network of community-run schools engaged in Catalan language immersion programs.
In the Alicante province Catalan is being replaced by Spanish, and in Alghero by Italian. There are also well ingrained diglossic attitudes against Catalan in the Valencian Community, Ibiza, and to a lesser extent, in the rest of the Balearic islands.
Classification and relationship with other Romance languages.
The ascription of Catalan to the Occitano-Romance branch of Gallo-Romance languages is not shared by all linguists and philologists, particularly among Spanish ones, such as Ramón Menéndez Pidal.
According to Pèire Bèc, its specific classification is as follows:
Catalan bears varying degrees of similarity to the linguistic varieties subsumed under the cover term "Occitan language" (see also differences between Occitan and Catalan and Gallo-Romance languages). Thus, as it should be expected from closely related languages, Catalan today shares many traits with other Romance languages.
Relationship with other Romance languages.
Catalan shares many traits with the other neighboring Romance languages (Italian, Sardinian, Occitan, and Spanish). However, despite being mostly situated in the Iberian Peninsula, Catalan has marked differences with the Ibero-Romance group (Spanish and Portuguese) in terms of pronunciation, grammar, and especially vocabulary; showing instead its closest affinity with Occitan and to a lesser extent Gallo-Romance (French, Franco-Provençal, Gallo-Italian).
According to Ethnologue, the lexical similarity between Catalan and other Romance languages is: 87% with Italian; 85% with Portuguese; 80% with Spanish; 76% with Ladin; 75% with Sardinian; and 73% with Romanian.
During much of its history, and especially during the Francoist dictatorship (1939–1975), the Catalan language has often been degraded as a mere dialect of Spanish. This view, based on political and ideological considerations, has no linguistic validity. Spanish and Catalan have important differences in their sound systems, lexicon, and grammatical features, placing the language in a number of respects closer to Occitan (and French).
There is evidence that, at least from the 2nd century, the vocabulary and phonology of Roman Tarraconensis was different from the rest of Roman Hispania. Differentiation has arisen generally because Spanish, Asturian, and Galician-Portuguese share certain peripheral archaisms (Spanish "hervir", Asturian/Portuguese "ferver" vs. Catalan "bullir", Occitan "bolir" "to boil") and innovatory regionalisms (Sp "novillo", Ast "nuviellu" vs. Cat "torell", Oc "taurèl" "bullock"), while Catalan has a shared history with the Western Romance innovative core, especially Occitan.
The Germanic superstrate has had different outcomes in Spanish and Catalan. For example, Catalan ' "mud" and ' "to roast", of Germanic origin, contrast with Spanish ' and ', of Latin origin; whereas Catalan ' "spinning wheel" and ' "temple", of Latin origin, contrast with Spanish ' and ', of Germanic origin.
The same happens with Arabic loanwords. Thus, Catalan "alfàbia" "large earthenware jar" and "rajola" "tile", of Arabic origin, contrast with Spanish "tinaja" and "teja", of Latin origin; whereas Catalan "oli" "oil" and "oliva" "olive", of Latin origin, contrast with Spanish "aceite" and "aceituna". However, the Arabic element in Spanish is generally much more prevalent.
Situated between two large linguistic blocks (Ibero-Romance and Gallo-Romance), Catalan has many unique lexical choices, such as "enyorar" "to miss somebody", "apaivagar" "to calm down somebody", or "rebutjar" "reject".
Geographic distribution.
Catalan-speaking territories.
These territories are sometimes referred to as the "Països Catalans" (Catalan Countries), a denomination based on cultural affinity and common heritage, that has also had a subsequent political interpretation but no official status. Various interpretations of the term may include some or all of these regions.
Number of speakers.
The number of people known to be fluent in Catalan varies depending on the sources used. A 2004 study did not count the total number of speakers, but estimated a total of 9–9.5 million by matching the percentage of speakers to the population of each area where Catalan is spoken. The web site of the Generalitat de Catalunya estimated that as of 2004 there were 9,118,882 speakers of Catalan. These figures only reflect potential speakers; today it is the native language of only 35.6% of the Catalan population. According to "Ethnologue", Catalan had four million native speakers and five million second-language speakers in 2012.
The most important social characteristic of the Catalan language is that all the areas where it is spoken are bilingual in practice: together with the French language in Roussillon, with Italian in Alghero, with Spanish and French in Andorra and with Spanish in the rest of the territories.
Level of knowledge of the Catalan language.
(% of the population 15 years old and older).
Social use.
(% of the population 15 years old and older).
Phonology.
The Catalan phonology varies depending on the dialect. Notable features include:
In contrast with other Romance languages, Catalan has many monosyllabic words; and those ending in a wide variety of consonants and some consonant clusters. Also, Catalan has final obstruent devoicing, thus featuring many couplets like "amic" "(male friend") vs. "amiga" ("female friend").
Central Catalan is considered the standard pronunciation of the language. The descriptions below are mostly for this variety. For the differences in pronunciation of the different dialects, see the section pronunciation of dialects in this article.
Vowels.
Catalan has inherited the typical vowel system of Vulgar Latin, with seven stressed phonemes: , a common feature in Western Romance, except Spanish. Balearic has also instances of stressed /ə/. Dialects differ in the different degrees of vowel reduction, and the incidence of the pair /ɛ e/.
In Central Catalan, unstressed vowels reduce to three: ; ; remains distinct. The other dialects have different vowel reduction processes (see the section pronunciation of dialects in this article).
Consonants.
The consonant system of Catalan is rather conservative, shared with most modern Western Romance languages.
Sociolinguistics.
Catalan sociolinguistics studies the situation of Catalan in the world and the different varieties that this language presents. It is a subdiscipline of Catalan philology and other affine studies and has as an objective to analyse the relation between the Catalan language, the speakers and the close reality (including the one of other languages in contact).
Dialects.
Overview.
The dialects of the Catalan language feature a relative uniformity, especially when compared to other Romance languages; both in terms of vocabulary, semantics, syntax, morphology, and phonology. Mutual intelligibility between dialects is very high, estimates ranging from 90% to 95%. The only exception is the isolated idiosyncratic Alguerese dialect.
Catalan is split in two major dialectal blocks: Eastern Catalan, and Western Catalan. The main difference lies in the treatment of unstressed "a" and "e"; which have merged to /ə/ in Eastern dialects, but which remain distinct as /a/ and /e/ in Western dialects. There are a few other differences in pronunciation, verbal morphology, and vocabulary.
Western Catalan comprises the two dialects of Northwestern Catalan and Valencian; the Eastern block comprises four dialects: Central Catalan, Balearic, Rossellonese, and Alguerese. Each dialect can be further subdivided in several subdialects.
Central Catalan is considered the standard pronunciation of the language and has the highest number of speakers. It is spoken in the densely populated regions of the Barcelona province, the eastern half of the province of Tarragona, and most of the province of Girona.
Pronunciation.
Vowels.
Catalan has inherited the typical vowel system of Vulgar Latin, with seven stressed phonemes: , a common feature in Western Romance, except Spanish. Balearic has also instances of stressed /ə/. Dialects differ in the different degrees of vowel reduction, and the incidence of the pair /ɛ e/.
In Eastern Catalan (except Majorcan), unstressed vowels reduce to three: ; ; remains distinct. There are a few instances of unreduced [o in some words. Alguerese has lowered [ə] to .
In Majorcan, unstressed vowels reduce to four: follow the Eastern Catalan reduction pattern; however reduce to , with remaining distinct, as in Western Catalan.
In Western Catalan, unstressed vowels reduce to five: ; ; remain distinct. This reduction pattern, inherited from Proto-Romance, is also found in Italian and Portuguese. Some Western dialects present further reduction or vowel harmony in some cases.
Central, Western, and Balearic differ in the lexical incidence of stressed /e/ and /ɛ/. Usually, words with /ɛ/ in Central Catalan correspond to /ə/ in Balearic and /e/ in Western Catalan. Words with /e/ in Balearic almost always have /e/ in Central and Western Catalan as well. As a result, Central Catalan has a much higher incidence of /e/.
Morphology.
In verbs, 1st person present indicative desinence is -"e" (∅ in verbs of the 2nd and 3rd conjugation), or -"o".<br>E.g. "parle", "tem", "sent" (Valencian); "parlo", "temo", "sento" (Northwestern). In verbs, 1st person present indicative desinence is -"o", -"i" or ∅ in all conjugations. <br>E.g. "parlo" (Central), "parl" (Balearic), "parli" (Northern), ('I speak').
In verbs, the inchoative desinences are -"isc"/-"ixo", -"ix", -"ixen", -"isca".
In verbs, the inchoative desinences are -"eixo", -"eix", -"eixen", -"eixi".
In nouns and adjectives, maintenance of of medieval plurals in proparoxytone words.<br>E.g. "hòmens" 'men', "jóvens" 'youth'.
In nouns and adjectives, loss of of medieval plurals in proparoxytone words.<br>E.g. "homes" 'men', "joves" 'youth'.
Vocabulary.
Despite its relative lexical unity, the two dialectal blocks of Catalan (Eastern and Western) show some differences in word choices. Any lexical divergence within any of the two groups can be explained as an archaism. Also, usually Central Catalan acts as an innovative element.
Standards.
Standard Catalan, virtually accepted by all speakers, is mostly based on Eastern Catalan, which is the most widely used dialect. Nevertheless, the standards of Valencia and the Balearics admit alternative forms, mostly traditional ones, which are not current in eastern Catalonia.
The most notable difference between both standards is some tonic accentuation, for instance: "francès, anglès" (IEC) – "francés, anglés" (AVL). Nevertheless, AVL's standard keeps the grave accent , without pronouncing this as , in some words like: "què" ('what'), or "València". Other divergences include the use of (AVL) in some words instead of like in "ametla"/"ametlla" ('almond'), "espatla"/"espatlla" ('back'), the use of elided demonstratives ("este" 'this', "eixe" 'that') in the same level as reinforced ones ("aquest, aqueix") or the use of many verbal forms common in Valencian, and some of these common in the rest of Western Catalan too, like subjunctive mood or inchoative conjugation in -"ix"- at the same level as -"eix"- or the priority use of -"e" morpheme in 1st person singular in present indicative (-"ar" verbs): "jo compre" instead of "jo compro" ('I buy').
In the Balearic Islands, IEC's standard is used but adapted for the Balearic dialect by the University of the Balearic Islands's philological section. In this way, for instance, IEC says it is correct writing "cantam" as much as "cantem" ('we sing') but the University says that the priority form in the Balearic Islands must be ""cantam"" in all fields. Another feature of the Balearic standard is the non-ending in the 1st person singular present indicative: "jo compr" ('I buy'), "jo tem" ('I fear'), "jo dorm" ('I sleep').
In Alghero, the IEC has adapted its standard to the Alguerese dialect. In this standard one can find, among other features: the definite article "lo" instead of "el", special possessive pronouns and determinants "la mia" ('mine'), "lo sou/la sua" ('his/her'), "lo tou/la tua" ('yours'), and so on, the use of "-v-" in the imperfect tense in all conjugations: "cantava", "creixiva", "llegiva"; the use of many archaic words, usual words in Alguerese: "manco" instead of "menys" ('less'), "calqui u" instead of "algú" ('someone'), "qual/quala" instead of "quin/quina" ('which'), and so on; and the adaptation of weak pronouns.
In 2011, the Aragonese government passed a decree for the establishment of a new language regulator of Catalan in La Franja (the so-called Catalan-speaking areas of Aragon). The new entity, designated as "Acadèmia Aragonesa del Català", shall allow a facultative education in Catalan and a standardization of the Catalan language in La Franja.
Status of Valencian.
Valencian is classified as a Western dialect, along with the northwestern varieties spoken in Western Catalonia (provinces of Lleida and the western half of Tarragona). The various forms of Catalan and Valencian are mutually intelligible (ranging from 90% to 95%)
Linguists, including Valencian scholars, deal with Catalan and Valencian as the same language. The official regulating body of the language of the Valencian Community, the Valencian Academy of Language ("Acadèmia Valenciana de la Llengua," AVL) declares the linguistic unity between Valencian and Catalan varieties.
The AVL, created by the Valencian parliament, is in charge of dictating the official rules governing the use of Valencian, and its standard is based on the Norms of Castelló ("Normes de Castelló"). Currently, everyone who writes in Valencian uses this standard, except the Royal Academy of Valencian Culture ("Acadèmia de Cultura Valenciana", RACV), which uses for Valencian an independent standard.
Despite the position of the official organizations, an opinion poll carried out between 2001 and 2004 showed that the majority of the Valencian people consider Valencian different from Catalan. This position is promoted by people who do not use Valencian regularly. Furthermore, the data indicates that younger generations educated in Valencian are much less likely to hold these views. A minority of Valencian scholars active in fields other than linguistics defends the position of the Royal Academy of Valencian Culture ("Acadèmia de Cultura Valenciana", RACV), which uses for Valencian a standard independent from Catalan.
This clash of opinions has sparked much controversy. For example, during the drafting of the European Constitution in 2004, the Spanish government supplied the EU with translations of the text into Basque, Galician, Catalan, and Valencian, but the latter two were identical.
Vocabulary.
Word choices.
Despite its relative lexical unity, the two dialectal blocks of Catalan (Eastern and Western) show some differences in word choices. Any lexical divergence within any of the two groups can be explained as an archaism. Also, usually Central Catalan acts as an innovative element.
Literary Catalan allows the use of words from different dialects, except those of very restricted use. However, from the 19th century onwards, there is a tendency of favoring words of Northern dialects in detriment of others, even though nowadays there is a greater freedom of choice.
Latin and Greek loanwords.
Like other languages, Catalan has a large list of loanwords from Greek and Latin. This process started very early, and one can find such examples in Ramon Llull's work. On the fourteenth and fifteenth centuries Catalan had a number of Greco-Latin loanwords much superior to other Romance languages, as it can be attested for example in Roís de Corella's writings.
Word formation.
The process of morphological derivation in Catalan follows the same principles as the other Romance languages, where agglutination is common. Many times, several affixes are appended to a preexisting lexeme, and some sound alternations can occur, for example "elèctric" [əˈlɛktrik] ("electrical") vs. "electricitat" [ələktrisiˈtat]. Prefixes are usually appended to verbs, for as in "preveure" ("foresee").
There is greater regularity in the process of word-compounding, where one can find compounded words as much as in English.
Writing system.
Catalan uses the Latin script, with some added symbols and digraphs. The Catalan orthography is systematic and largely phonologically based.
Grammar.
The grammar of Catalan is similar to other Romance languages. Features include:
Gender and number inflection.
In gender inflection, the most notable feature is (compared to Portuguese, Spanish or Italian), the loss of the typical masculine suffix "-o". Thus, the alternance of "-o"/"-a", has been replaced by "ø"/"-a". There are only a few exceptions, like "minso"/"minsa" ("scarce"). Many not completely predictable morphological alternations may occur, such as:
Catalan has few suppletive couplets, like Italian and Spanish, and unlike French. Thus, Catalan has "noi"/"noia" ("boy"/"girl") and "gall"/"gallina" ("cock"/"hen"), whereas French has "garçon"/"fille" and "coq"/"poule".
There is a tendency to abandon traditionally gender-invariable adjectives in favour of marked ones, something prevalent in Occitan and French. Thus, one can find "bullent"/"bullenta" ("boiling") in contrast with traditional "bullent"/"bullent".
As in the other Western Romance languages, the main plural expression is the suffix "-s", which may create morphological alternations similar to the ones found in gender inflection, albeit more rarely.
The most important one is the addition of "-o-" before certain consonant groups, a phonetic phenomenon that does not affect feminine forms: "el pols"/"els polsos" ("the pulse"/"the pulses") vs. "la pols"/"les pols" ("the dust"/"the dusts").
Determiners.
The inflection of determinatives is complex, specially because of the high number of elisions, but is similar to the neighboring languages. Catalan has more contractions of preposition + article than Spanish, like "dels" ("of + the "), but not as many as Italian (which has "sul", "col", "nel", etc.).
Central Catalan has abandoned almost completely unstressed possessives ("mon", etc.) in favour of constructions of article + stressed forms ("el meu", etc.), a feature shared with Italian.
Personal pronouns.
The morphology of Catalan personal pronouns is complex, specially in unstressed forms, which are numerous (13 distinct forms, compared to 11 in Spanish or 9 in Italian). Features include the gender-neutral "ho" and the great degree of freedom when combining different unstressed pronouns (65 combinations).
Catalan pronouns exhibit T–V distinction, like all other Romance languages (and most European languages, but not Modern English). This feature implies the use of a different set of second person pronouns for formality.
This flexibility allows Catalan to use extraposition extensively, much more than French or Spanish. Thus, Catalan can have "m'hi recomanaren" ("they recommended me to him"), whereas in French one must say "ils m'ont recommandé à lui", and Spanish "me recomendaron a él". This allows the placement of almost any nominal term as a sentence topic, without having to use so often the passive voice (as in French or English), or identifying the direct object with a preposition (as in Spanish).
Verbs.
Like all the Romance languages, Catalan verbal inflection is more complex than the nominal. Suffixation is omnipresent, whereas morphological alternations play a secondary role. Vowel alternances are active, as well as infixation and suppletion. However, these are not as productive as in Spanish, and are mostly restricted to irregular verbs.
The Catalan verbal system is basically common to all Western Romance, except that most dialects have replaced the synthetic indicative perfect with a periphrastic form of "anar" ("to go") + infinitive.
Catalan verbs are traditionally divided into three conjugations, with vowel themes "-a-", "-e-", "-i-", the last two being split into two subtypes. However, this division is mostly theoretical. Only the first conjugation is nowadays productive (with about 3500 common verbs), whereas the third (the subtype of "servir", with about 700 common verbs) is semiproductive. The verbs of the second conjugation are fewer than 100, and it is not possible to create new ones, except by compounding.
Syntax.
The grammar of Catalan follows the general pattern of Western Romance languages. The primary word order is SVO (subject–verb–object).
Catalan names.
In Spain, every person officially has two surnames, one of which is the father's first surname and the other is the mother's first surname. The law contemplates the possibility of joining both surnames with the Catalan conjunction "i" ("and").
Sample text.
Selected text from Manuel de Pedrolo's 1970 novel "Un amor fora ciutat" ("A love affair outside the city").
External links.
Institutions
About the Catalan language
Monolingual dictionaries
Bilingual and multilingual dictionaries
Automated translation systems
Phrasebooks
Learning resources
Catalan-language online encyclopedia

</doc>
<doc id="5285" url="https://en.wikipedia.org/wiki?curid=5285" title="STS-51-F">
STS-51-F

STS-51-F (also known as Spacelab 2) was the nineteenth flight of NASA's Space Shuttle program, and the eighth flight of Space Shuttle "Challenger". It launched from Kennedy Space Center, Florida, on July 29, 1985, and landed just under eight days later on August 6.
While STS-51-F's primary payload was the Spacelab 2 laboratory module, the payload which received the most publicity was the Carbonated Beverage Dispenser Evaluation, which was an experiment in which both Coca-Cola and Pepsi tried to make their carbonated drinks available to astronauts.
During launch the "Challenger" experienced multiple sensor failings in its SSMEs and had to perform an "Abort to Orbit" (ATO) emergency procedure. It is the only mission to have carried out an abort after launching. As a result of the ATO, the mission was carried out at a slightly lower orbital altitude.
Crew.
Crew notes.
As with previous Spacelab missions, the crew was divided between two 12-hour shifts. Acton, Bridges and Henize made up the "Red Team" while Bartoe, England and Musgrave comprised the "Blue Team"; commander Fullerton could take either shift when needed. "Challenger" carried two EMUs in the event of an emergency spacewalk, which would have been performed by England and Musgrave.
Launch.
STS-51-F's first launch attempt on July 12, 1985 was halted with the countdown at T-3 seconds after main engine ignition, when a malfunction of the number two Space Shuttle Main Engine (SSME) coolant valve caused the shutdown of all three main engines. "Challenger" launched successfully on its second attempt on July 29, 1985, at 17:00 EDT, after a delay of one hour and 37 minutes due to a problem with the table maintenance block update uplink.
Three minutes and 31 seconds into the ascent, one of the center engine's two high pressure fuel turbopump turbine discharge temperature sensors failed. Two minutes and 12 seconds later, the second sensor failed, causing the shutdown of the center engine. This was the only in-flight main engine failure of the shuttle program. Approximately 8 minutes into the flight, one of the same temperature sensors in the right engine failed, and the remaining right engine temperature sensor displayed readings near the redline for engine shutdown. Booster Systems Engineer Jenny M. Howard acted quickly to command the crew to inhibit any further automatic SSME shutdowns based on readings from the remaining sensors, preventing the potential shutdown of a second engine and a possible abort mode that may have resulted in the loss of the vehicle and crew.
The failed SSME resulted in an Abort to Orbit (ATO) trajectory, whereby the shuttle achieved a lower-than-planned orbital altitude.
Mission summary.
STS-51-F's primary payload was the laboratory module Spacelab 2. A special part of the modular Spacelab system, the "igloo", which was located at head of a three-pallet train, provided on-site support to instruments mounted on pallets. The main mission objective was to verify performance of Spacelab systems, determine the interface capability of the orbiter, and measure the environment created by the spacecraft. Experiments covered life sciences, plasma physics, astronomy, high-energy astrophysics, solar physics, atmospheric physics and technology research. Despite mission replanning necessitated by "Challenger"'s abort to orbit trajectory, the Spacelab mission was declared a success.
The flight marked the first time the ESA Instrument Pointing System (IPS) was tested in orbit. This unique pointing instrument was designed with an accuracy of one arcsecond. Initially, some problems were experienced when it was commanded to track the Sun, but a series of software fixes were made and the problem was corrected. In addition, Tony England became the second amateur radio operator to transmit from space during the mission.
The Spacelab Infrared Telescope (IRT) was also flown on the mission. The IRT was a 15.2 cm aperture helium-cooled infrared telescope, observing light between wavelengths of 1.7 to 118 μm. The experiment experienced some problems, such as heat emissions from the Shuttle corrupting data, but still returned useful astronomical data.
The Plasma Diagnostics Package (PDP), which had been previously flown on STS-3, made its return on the mission, and was part of a set of plasma physics experiments designed to study the Earth's ionosphere. During the third day of the mission, it was grappled out of the payload bay by the Remote Manipulator System and released for six hours. During this time, "Challenger" maneuvered around the PDP as part of a proximity operations exercise. The PDP was successfully grappled by the RMS and returned to the payload bay at the beginning of the fourth day of the mission.
In a heavily-publicized marketing experiment, astronauts aboard STS-51-F drank carbonated beverages from specially-designed cans provided by competitors Coca-Cola and Pepsi. Post-flight, the astronauts revealed that they preferred Tang, in part because it could be mixed on-orbit with existing chilled water supplies, whereas there was no dedicated refrigeration equipment on board to chill the soda, which also fizzed excessively in microgravity.
In an intriguing experiment, during a 1985 launch, thruster rockets were fired at a point over Tasmania and also above Boston to create two "holes" - plasma depletion regions - in the ionosphere. A world-wide group of geophysicists collaborated with the observations made from Spacelab 2.
Landing.
"Challenger" landed at Edwards Air Force Base, California, on August 6, 1985, at 12:45:26 pm PDT. Its rollout distance was . The mission had been extended by 17 orbits for additional payload activities due to the Abort to Orbit. The orbiter arrived back at Kennedy Space Center on August 11, 1985.
Mission insignia.
The mission insignia was designed by Houston artist Skip Bradley. Space Shuttle "Challenger" is depicted ascending toward the heavens in search of new knowledge in the field of solar and stellar astronomy, with its Spacelab 2 payload. The constellations Leo and Orion are shown in the positions they were in relative to the Sun during the flight. The nineteen stars indicate that the mission is the 19th shuttle flight.

</doc>
<doc id="5288" url="https://en.wikipedia.org/wiki?curid=5288" title="Classical period (music)">
Classical period (music)

The dates of the Classical period in Western music are generally accepted as being between about the year 1730 and the year 1820. However, the term "classical music" is often used in a colloquial sense as a synonym for Western art music which describes a variety of Western musical styles from the ninth century to the present, and especially from the sixteenth or seventeenth to the nineteenth. This article is about the specific period from 1730 to 1820.
The Classical period falls between the Baroque and the Romantic periods. Classical music has a lighter, clearer texture than Baroque music and is less complex. It is mainly homophonic, using a clear melody line over a subordinate chordal accompaniment,(but counterpoint was by no means forgotten, especially later in the period). It also makes use of "style galant" which emphasized light elegance in place of the Baroque's dignified seriousness and impressive grandeur. Variety and contrast within a piece became more pronounced than before and the orchestra increased in size, range, and power. The harpsichord was replaced as the main keyboard solo instrument by the piano (or fortepiano). Unlike the harpsichord, which plucked strings with quills, pianos struck the strings with leather-covered hammers when the keys were pressed; the design of the piano enabled the performer to make notes louder or softer by playing more forcefully or more gently (the force with which a performer plays the harpsichord keys does not change the dynamics). Importance was given to instrumental music—the main kinds were sonata, trio, string quartet, symphony and the solo concerto, which featured a virtuoso solo performer playing violin, piano, flute, or another instrument, accompanied by an orchestra.
The best-known composers from this period are Joseph Haydn, Wolfgang Amadeus Mozart, Ludwig van Beethoven, and Franz Schubert; other notable names include Luigi Boccherini, Muzio Clementi, Antonio Soler, Antonio Salieri, François Joseph Gossec, Johann Stamitz, Carl Friedrich Abel, Carl Philipp Emanuel Bach, and Christoph Willibald Gluck. Ludwig van Beethoven is also regarded either as a Romantic composer or a composer who was part of the transition to the Romantic. Franz Schubert is also something of a transitional figure, as are Johann Nepomuk Hummel, Mauro Giuliani, Friedrich Kuhlau, Fernando Sor, Luigi Cherubini, Jan Ladislav Dussek, and Carl Maria von Weber. The period is sometimes referred to as the era of "Viennese Classic" or "Classicism" (), since Wolfgang Amadeus Mozart, Joseph Haydn, Antonio Salieri, and Ludwig van Beethoven all worked at some time in Vienna, and Franz Schubert was born there.
Classicism.
In the middle of the 18th century, Europe began to move toward a new style in architecture, literature, and the arts, generally known as Classicism. This style sought to emulate the ideals of Classical antiquity, especially those of Classical Greece. Classical music was still tightly linked to aristocratic Court culture and supported by absolute monarchies. Classical music used formality and emphasis on order and hierarchy, and a "clearer", "cleaner" style that used clearer divisions between parts (notably a clear, single melody accompanied by chords), brighter contrasts and "tone colors" (achieved by the use of dynamic changes and modulations to more keys). In contrast with the richly layered music of the Baroque era, Classical music moved towards simplicity rather than complexity. In addition, the typical size of orchestras began to increase, giving orchestras a more powerful sound.
The remarkable development of ideas in "natural philosophy" had already established itself in the public consciousness. In particular, Newton's physics was taken as a paradigm: structures should be well-founded in axioms and be both well-articulated and orderly. This taste for structural clarity began to affect music, which moved away from the layered polyphony of the Baroque period toward a style known as homophony, in which the melody is played over a subordinate harmony. This move meant that chords became a much more prevalent feature of music, even if they interrupted the melodic smoothness of a single part. As a result, the tonal structure of a piece of music became more audible.
The new style was also encouraged by changes in the economic order and social structure. As the 18th century progressed, the nobility became the primary patrons of instrumental music, while public taste increasingly preferred lighter, funny comic operas. This led to changes in the way music was performed, the most crucial of which was the move to standard instrumental groups and the reduction in the importance of the "continuo"—the rhythmic and harmonic groundwork of a piece of music, typically played by a keyboard (harpsichord or organ) and usually accompanied by a varied group of bass instruments, including cello, double bass, bass viol, and theorbo. One way to trace the decline of the continuo and its figured chords is to examine the disappearance of the term "obbligato", meaning a mandatory instrumental part in a work of chamber music. In Baroque compositions, additional instruments could be added to the continuo group according to the group or leader's preference; in Classical compositions, all parts were specifically noted, though not always "notated", so the term "obbligato" became redundant. By 1800, basso continuo was practically extinct, except for the occasional use of a pipe organ continuo part in a religious Mass in the early 1800s.
Economic changes also had the effect of altering the balance of availability and quality of musicians. While in the late Baroque, a major composer would have the entire musical resources of a town to draw on, the musical forces available at an aristocratic hunting lodge or small court were smaller and more fixed in their level of ability. This was a spur to having simpler parts for ensemble musicians to play, and in the case of a resident virtuoso group, a spur to writing spectacular, idiomatic parts for certain instruments, as in the case of the Mannheim orchestra, or virtuoso solo parts for particularly skilled violinists or flautists. In addition, the appetite by audiences for a continual supply of new music carried over from the Baroque. This meant that works had to be performable with, at best, one or two rehearsals. Indeed, even after 1790 Mozart writes about "the rehearsal", with the implication that his concerts would have only one rehearsal.
Since polyphonic textures with interweaving melodic lines was no longer the main focus of music (excluding in the development section), a single melodic line with accompaniment became the main texture. In the Classical era, there was greater emphasis on notating that line for dynamics and phrasing. This contrasts with the Baroque era, when melodies were typically written with no dynamics, phrasing marks or ornaments, as it was assumed that the performer would improvise these elements on the spot. In the Classical era, it became more common for composers to indicate where they wanted performers to play ornaments such as trills or turns. The simplification of texture made such instrumental detail more important, and also made the use of characteristic rhythms, such as attention-getting opening fanfares, the funeral march rhythm, or the minuet genre, more important in establishing and unifying the tone of a single movement.
Forms such as the concerto and sonata were more heavily defined and given more specific rules, whereas the symphony was created in this period (this is popularly attributed to Joseph Haydn). The "concerto grosso" (a concerto for more than one musician), a very popular form in the Baroque era, began to be replaced by the "solo concerto" (a concerto featuring only one soloist, accompanied by orchestra). Given that Classical concertos only had a single soloist, composers began to place more importance on the particular soloist's ability to show off virtuoso skills, with challenging, fast scale and arpeggio runs. There were, of course, some "concerti grossi" that remained, the most famous of which being Mozart's Sinfonia Concertante for Violin and Viola in E flat Major.
Main characteristics.
Classical music has a lighter, clearer texture than Baroque music and is less complex. It is mainly homophonic—a clear melody above a subordinate chordal accompaniment. Counterpoint was by no means forgotten, especially later in the period, and composers still used counterpoint in religious pieces, such as Masses. Classical music also makes use of "style galant", which contrasted with the heavy strictures of the Baroque style. Galant style emphasized light elegance in place of the Baroque's dignified seriousness and impressive grandeur.
Variety and contrast within a piece became more pronounced than before. Composers used a variety of keys, melodies, rhythms and dynamics. Classical pieces dynamic changes such as "crescendo" (an instruction to gradually get louder), "diminuendo" (an instruction to gradually get softer) and "sforzando" (a sudden strong, loud attack). Classical pieces had frequent changes of dynamics, mood and timbre, in contrast to Baroque music. Melodies tended to be shorter than those of Baroque music, with clear-cut phrases and distinct cadences. The orchestra increased in size and range; the harpsichord or pipe organ basso continuo role in orchestra gradually fell out of use between 1750 and 1800. As well, the woodwinds became a self-contained section, consisting of clarinets, oboes, flutes and bassoons. As a solo instrument, the harpsichord was replaced by the piano (or fortepiano, the first type of piano which was invented ca. 1700). Early piano music was light in texture, often with Alberti bass accompaniment, which used arpeggios in the left hand to state the harmonies. Over the Classical period, the pieces became richer, more sonorous and more powerful.
While vocal music such as comic opera was popular, great importance was given to instrumental music. The main kinds of instrumental music were the sonata, trio, string quartet, symphony, concerto (usually for a virtuoso solo instrument accompanied by orchestra), and light pieces such as serenades and divertimentos. Sonata form developed and became the most important form. It was used to build up the first movement of most large-scale works in symphonies and string quartets. Sonato form was also used in other movements and in single, standalone pieces such as overtures.
History.
Baroque/Classical transition c. 1730–1760.
At first the new style took over Baroque forms—the ternary "da capo aria" and the "sinfonia" and "concerto"—but composed with simpler parts, more notated ornamentation, rather than the improvised ornaments that were common in the Baroque era, and more emphatic division of pieces into sections. However, over time, the new aesthetic caused radical changes in how pieces were put together, and the basic formal layouts changed. Composers from this period sought dramatic effects, striking melodies, and clearer textures. One of the big textural changes was a shift away from the complex, dense polyphonic style of the Baroque, in which multiple interweaving melodic lines were played simultaneously, and towards homophony, a lighter texture which uses a clear single melody line accompanied by chords.
The Italian composer Domenico Scarlatti was an important figure in the transition from Baroque to Classical style. His unique compositional style is strongly related to that of the early Classical period. He is best known for composing more than five hundred one-movement keyboard sonatas. In Spain, Antonio Soler also produced valuable keyboard sonatas, more varied in form than those of Scarlatti, with some pieces in three or four movements.
Baroque music generally uses many harmonic fantasies and polyphonic sections that focus less on the structure of the musical piece, and there was less emphasis on clear musical phrases. In the classical period, the harmonies became simpler. However, the structure of the piece, the phrases and small melodic or rhythmic motives, became much more important than in the Baroque period.
Another important break with the past was the radical overhaul of opera by Christoph Willibald Gluck, who cut away a great deal of the layering and improvisational ornaments and focused on the points of modulation and transition. By making these moments where the harmony changes more of a focus, he enabled powerful dramatic shifts in the emotional color of the music. To highlight these transitions, he used changes in instrumentation (orchestration), melody, and mode. Among the most successful composers of his time, Gluck spawned many emulators, one of whom was Antonio Salieri. Their emphasis on accessibility brought huge successes in opera, and in other vocal music such as songs, oratorios, and choruses. These were considered the most important kinds of music for performance and hence enjoyed greatest public success.
The phase between the Baroque and the rise of the Classical, with its broad mixture of competing ideas and attempts to unify the different demands of taste, economics and "worldview", goes by many names. It is sometimes called "Galant", "Rococo", or "pre-Classical", or at other times "early Classical". It is a period where some composers still working in the Baroque style flourish, though sometimes thought of as being more of the past than the present—Bach, Handel, and Telemann all composed well beyond the point at which the homophonic style is clearly in the ascendant. Musical culture was caught at a crossroads: the masters of the older style had the technique, but the public hungered for the new. This is one of the reasons C. P. E. Bach was held in such high regard: he understood the older forms quite well and knew how to present them in new garb, with an enhanced variety of form.
1750–1775.
By the late 1750s there were flourishing centers of the new style in Italy, Vienna, Mannheim, and Paris; dozens of symphonies were composed and there were bands of players associated with musical theatres. Opera or other vocal music accompanied by orchestra was the feature of most musical events, with concertos and symphonies (arising from the overture) serving as instrumental interludes and introductions for operas and church services. Over the course of the Classical period, symphonies and concertos developed and were presented independently of vocal music.
The "normal" orchestra ensemble—a body of strings supplemented by winds—and movements of particular rhythmic character were established by the late 1750s in Vienna. However, the length and weight of pieces was still set with some Baroque characteristics: individual movements still focused on one "affect" (musical mood) or had only one sharply contrasting middle section, and their length was not significantly greater than Baroque movements. There was not yet a clearly enunciated theory of how to compose in the new style. It was a moment ripe for a breakthrough.
Many consider this breakthrough to have been made by C. P. E. Bach, Gluck, and several others. Indeed, C. P. E. Bach and Gluck are often considered founders of the Classical style. The first great master of the style was the composer Joseph Haydn. In the late 1750s he began composing symphonies, and by 1761 he had composed a triptych ("Morning", "Noon", and "Evening") solidly in the contemporary mode. As a vice-Kapellmeister and later Kapellmeister, his output expanded: he composed over forty symphonies in the 1760s alone. And while his fame grew, as his orchestra was expanded and his compositions were copied and disseminated, his voice was only one among many.
While some scholars suggest that Haydn was overshadowed by Mozart and Beethoven, it would be difficult to overstate Haydn's centrality to the new style, and therefore to the future of Western art music as a whole. At the time, before the pre-eminence of Mozart or Beethoven, and with Johann Sebastian Bach known primarily to connoisseurs of keyboard music, Haydn reached a place in music that set him above all other composers except perhaps the Baroque era's George Frideric Handel. Haydn took existing ideas, and radically altered how they functioned—earning him the titles "father of the symphony" and "father of the string quartet".
One of the forces that worked as an impetus for his pressing forward was the first stirring of what would later be called Romanticism—the "Sturm und Drang", or "storm and stress" phase in the arts, a short period where obvious and dramatic emotionalism was a stylistic preference. Haydn accordingly wanted more dramatic contrast and more emotionally appealing melodies, with sharpened character and individuality in his pieces. This period faded away in music and literature: however, it influenced what came afterward and would eventually be a component of aesthetic taste in later decades.
The "Farewell Symphony", No. 45 in F Minor, exemplifies Haydn's integration of the differing demands of the new style, with surprising sharp turns and a long slow adagio to end the work. In 1772, Haydn completed his Opus 20 set of six string quartets, in which he deployed the polyphonic techniques he had gathered from the previous Baroque era to provide structural coherence capable of holding together his melodic ideas. For some, this marks the beginning of the "mature" Classical style, in which the period of reaction against late Baroque complexity yielded to a period of integration Baroque and Classical elements.
1775–1790.
Haydn, having worked for over a decade as the music director for a prince, had far more resources and scope for composing than most other composers. His position also gave him the ability to shape the forces that would play his music, as he could select skilled musicians. This opportunity was not wasted, as Haydn, beginning quite early on his career, sought to press forward the technique of building and developing ideas in his music. His next important breakthrough was in the Opus 33 string quartets (1781), in which the melodic and the harmonic roles segue among the instruments: it is often momentarily unclear what is melody and what is harmony. This changes the way the ensemble works its way between dramatic moments of transition and climactic sections: the music flows smoothly and without obvious interruption. He then took this integrated style and began applying it to orchestral and vocal music.
Haydn's gift to music was a way of composing, a way of structuring works, which was at the same time in accord with the governing aesthetic of the new style. However, a younger contemporary, Wolfgang Amadeus Mozart, brought his genius to Haydn's ideas and applied them to two of the major genres of the day: opera, and the virtuoso concerto. Whereas Haydn spent much of his working life as a court composer, Mozart wanted public success in the concert life of cities, playing for the general public. This meant he needed to write operas and write and perform virtuoso pieces. Haydn was not a virtuoso at the international touring level; nor was he seeking to create operatic works that could play for many nights in front of a large audience. Mozart wanted to achieve both. Moreover, Mozart also had a taste for more chromatic chords (and greater contrasts in harmonic language generally), a greater love for creating a welter of melodies in a single work, and a more Italianate sensibility in music as a whole. He found, in Haydn's music and later in his study of the polyphony of J.S. Bach, the means to discipline and enrich his artistic gifts.
Mozart rapidly came to the attention of Haydn, who hailed the new composer, studied his works, and considered the younger man his only true peer in music. In Mozart, Haydn found a greater range of instrumentation, dramatic effect and melodic resource. The learning relationship moved in both directions. Mozart also had a great respect for the older, more experienced composer, and sought to learn from him.
Mozart's arrival in Vienna in 1780 brought an acceleration in the development of the Classical style. There, Mozart absorbed the fusion of Italianate brilliance and Germanic cohesiveness that had been brewing for the previous 20 years. His own taste for flashy brilliances, rhythmically complex melodies and figures, long cantilena melodies, and virtuoso flourishes was merged with an appreciation for formal coherence and internal connectedness. It is at this point that war and economic inflation halted a trend to larger orchestras and forced the disbanding or reduction of many theater orchestras. This pressed the Classical style inwards: toward seeking greater ensemble and technical challenges—for example, scattering the melody across woodwinds, or using a melody harmonized in thirds. This process placed a premium on small ensemble music, called chamber music. It also led to a trend for more public performance, giving a further boost to the string quartet and other small ensemble groupings.
It was during this decade that public taste began, increasingly, to recognize that Haydn and Mozart had reached a high standard of composition. By the time Mozart arrived at age 25, in 1781, the dominant styles of Vienna were recognizably connected to the emergence in the 1750s of the early Classical style. By the end of the 1780s, changes in performance practice, the relative standing of instrumental and vocal music, technical demands on musicians, and stylistic unity had become established in the composers who imitated Mozart and Haydn. During this decade Mozart composed his most famous operas, his six late symphonies that helped to redefine the genre, and a string of piano concerti that still stand at the pinnacle of these forms.
One composer who was influential in spreading the more serious style that Mozart and Haydn had formed is Muzio Clementi, a gifted virtuoso pianist who tied with Mozart in a musical "duel" before the emperor in which they each improvised on the piano and performed their compositions. Clementi's sonatas for the piano circulated widely, and he became the most successful composer in London during the 1780s. Also in London at this time was Jan Ladislav Dussek, who, like Clementi, encouraged piano makers to extend the range and other features of their instruments, and then fully exploited the newly opened up possibilities. The importance of London in the Classical period is often overlooked, but it served as the home to the Broadwood's factory for piano manufacturing and as the base for composers who, while less notable than the "Vienna School", had a decisive influence on what came later. They were composers of many fine works, notable in their own right. London's taste for virtuosity may well have encouraged the complex passage work and extended statements on tonic and dominant.
Circa 1790–1820.
When Haydn and Mozart began composing, symphonies were played as single movements—before, between, or as interludes within other works—and many of them lasted only ten or twelve minutes; instrumental groups had varying standards of playing, and the continuo was a central part of music-making.
In the intervening years, the social world of music had seen dramatic changes. International publication and touring had grown explosively, and concert societies formed. Notation became more specific, more descriptive—and schematics for works had been simplified (yet became more varied in their exact working out). In 1790, just before Mozart's death, with his reputation spreading rapidly, Haydn was poised for a series of successes, notably his late oratorios and "London" symphonies. Composers in Paris, Rome, and all over Germany turned to Haydn and Mozart for their ideas on form.
The time was again ripe for a dramatic shift. In the 1790s, a new generation of composers, born around 1770, emerged. While they had grown up with the earlier styles, they heard in the recent works of Haydn and Mozart a vehicle for greater expression. In 1788 Luigi Cherubini settled in Paris and in 1791 composed "Lodoiska", an opera that raised him to fame. Its style is clearly reflective of the mature Haydn and Mozart, and its instrumentation gave it a weight that had not yet been felt in the grand opera. His contemporary Étienne Méhul extended instrumental effects with his 1790 opera "Euphrosine et Coradin", from which followed a series of successes.
The most fateful of the new generation was Ludwig van Beethoven, who launched his numbered works in 1794 with a set of three piano trios, which remain in the repertoire. Somewhat younger than the others, though equally accomplished because of his youthful study under Mozart and his native virtuosity, was Johann Nepomuk Hummel. Hummel studied under Haydn as well; he was a friend to Beethoven and Franz Schubert. He concentrated more on the piano than any other instrument, and his time in London in 1791 and 1792 generated the composition and publication in 1793 of three piano sonatas, opus 2, which idiomatically used Mozart's techniques of avoiding the expected cadence, and Clementi's sometimes modally uncertain virtuoso figuration. Taken together, these composers can be seen as the vanguard of a broad change in style and the center of music. They studied one another's works, copied one another's gestures in music, and on occasion behaved like quarrelsome rivals.
The crucial differences with the previous wave can be seen in the downward shift in melodies, increasing durations of movements, the acceptance of Mozart and Haydn as paradigmatic, the greater use of keyboard resources, the shift from "vocal" writing to "pianistic" writing, the growing pull of the minor and of modal ambiguity, and the increasing importance of varying accompanying figures to bring "texture" forward as an element in music. In short, the late Classical was seeking a music that was internally more complex. The growth of concert societies and amateur orchestras, marking the importance of music as part of middle-class life, contributed to a booming market for pianos, piano music, and virtuosi to serve as examplars. Hummel, Beethoven, and Clementi were all renowned for their improvising.
Direct influence of the Baroque continued to fade: the figured bass grew less prominent as a means of holding performance together, the performance practices of the mid-18th century continued to die out. However, at the same time, complete editions of Baroque masters began to become available, and the influence of Baroque style continued to grow, particularly in the ever more expansive use of brass. Another feature of the period is the growing number of performances where the composer was not present. This led to increased detail and specificity in notation; for example, there were fewer "optional" parts that stood separately from the main score.
The force of these shifts became apparent with Beethoven's 3rd Symphony, given the name "Eroica", which is Italian for "heroic", by the composer. As with Stravinsky's "The Rite of Spring", it may not have been the first in all of its innovations, but its aggressive use of every part of the Classical style set it apart from its contemporary works: in length, ambition, and harmonic resources as well.
First Viennese School.
The First Viennese School is a name mostly used to refer to three composers of the Classical period in late-18th-century Vienna: W. A. Mozart, Haydn, and Beethoven. Franz Schubert is occasionally added to the list.
In German speaking countries, the term "Wiener Klassik" (lit. "Viennese classical era/art") is used. That term is often more broadly applied to the Classical era in music as a whole, as a means to distinguish it from other periods that are colloquially referred to as "classical", namely Baroque and Romantic music.
The term "Viennese School" was first used by Austrian musicologist Raphael Georg Kiesewetter in 1834, although he only counted Haydn and Mozart as members of the school. Other writers followed suit, and eventually Beethoven was added to the list. The designation "first" is added today to avoid confusion with the Second Viennese School.
Whilst, Schubert apart, these composers certainly knew each other (with Haydn and Mozart even being occasional chamber-music partners), there is no sense in which they were engaged in a collaborative effort in the sense that one would associate with 20th-century schools such as the Second Viennese School, or Les Six. Nor is there any significant sense in which one composer was "schooled" by another (in the way that Berg and Webern were taught by Schoenberg), though it is true that Beethoven for a time received lessons from Haydn.
Attempts to extend the First Viennese School to include such later figures as Anton Bruckner, Johannes Brahms, and Gustav Mahler are merely journalistic, and never encountered in academic musicology.
Classical influence on later composers.
Musical eras and their prevalent styles, forms and instruments seldom disappear at once; instead, features are replaced over time, until the old approach is simply felt as "old-fashioned". The Classical style did not "die" suddenly; rather, it gradually got phased out under the weight of changes. To give just one example, while it is generally stated that the Classical era stopped using the harpsichord in orchestras, this did not happen all of a sudden at the start of the Classical era in 1750. Rather, orchestras slowly stopped using the harpsichord to play basso continuo until the practice was discontinued by the end of the 1700s.
One crucial change was the shift towards harmonies centering on "flatward" keys: shifts in the subdominant direction . In the Classical style, major key was far more common than minor, chromaticism being moderated through the use of "sharpward" modulation (e.g., a piece in C major modulating to G major, D major, or A major, all of which are keys with more sharps). As well, sections in the minor mode were often used for contrast. Beginning with Mozart and Clementi, there began a creeping colonization of the subdominant region (the ii or IV chord, which in the key of C major would be the keys of d minor or F major). With Schubert, subdominant modulations flourished after being introduced in contexts in which earlier composers would have confined themselves to dominant shifts (modulations to the dominant chord, e.g., in the key of C major, modulating to G major). This introduced darker colors to music, strengthened the minor mode, and made structure harder to maintain. Beethoven contributed to this by his increasing use of the fourth as a consonance, and modal ambiguity—for example, the opening of the D Minor Symphony.
Franz Schubert, Carl Maria von Weber, and John Field are among the most prominent in this generation of "Classical Romantics", along with the young Felix Mendelssohn. Their sense of form was strongly influenced by the Classical style. While they were not yet "learned" composers (imitating rules which were codified by others), they directly responded to works by Beethoven, Mozart, Clementi, and others, as they encountered them. The instrumental forces at their disposal in orchestras were also quite "Classical" in number and variety, permitting similarity with Classical works.
However, the forces destined to end the hold of the Classical style gathered strength in the works of each of the above composers, particularly Beethoven. The most commonly cited one is harmonic innovation. Also important is the increasing focus on having a continuous and rhythmically uniform accompanying figuration: Beethoven's Moonlight Sonata was the model for hundreds of later pieces—where the shifting movement of a rhythmic figure provides much of the drama and interest of the work, while a melody drifts above it. Greater knowledge of works, greater instrumental expertise, increasing variety of instruments, the growth of concert societies, and the unstoppable domination of the increasingly more powerful piano (which was given a bolder, louder tone by technological developments such as the use of steel strings, heavy cast-iron frames and sympathetically vibrating strings) all created a huge audience for sophisticated music. All of these trends contributed to the shift to the "Romantic" style.
Drawing the line between these two styles is very difficult: some sections of Mozart's later works, taken alone, are indistinguishable in harmony and orchestration from music written 80 years later—and some composers continue to write in normative Classical styles into the early 20th century. Even before Beethoven's death, composers such as Louis Spohr were self-described Romantics, incorporating, for example, more extravagant chromaticism in their works (e.g., using chromatic harmonies in a piece's chord progression).
However, Vienna's fall as the most important musical center for orchestral composition marked the Classical style's final eclipse—and the end of its continuous organic development of one composer learning in close proximity to others. Franz Liszt and Frédéric Chopin visited Vienna when they were young, but they then moved on to other cities. Composers such as Carl Czerny, while deeply influenced by Beethoven, also searched for new ideas and new forms to contain the larger world of musical expression and performance in which they lived.
Renewed interest in the formal balance and restraint of 18th century classical music led in the early 20th century to the development of so-called Neoclassical style, which numbered Stravinsky and Prokofiev among its proponents, at least at certain times in their careers.
Classical period instruments.
Strings.
In the Baroque era, there was more variety in the bowed stringed instruments used in ensembles, with instruments such as the viola d'amore and a range of fretted viols being used, ranging from small viols to large bass viols. In the Classical period, the string section of the orchestra was standardized as just four instruments:
In the Baroque era, the double bass players were not usually given a separate part; instead, they typically played the same basso continuo bassline that the cellos and other low-pitched instruments (e.g., theorbo, serpent wind instrument, viols), albeit an octave below the cellos, because the double bass is a transposing instrument that sounds one octave lower than it is written. In the Classical era, some composers continued to write only one bass part for their symphony, labeled "bassi"; this bass part was played by cellists and double bassists. Even though the cellos and basses played from the same music, the basses were one octave below the cellos. During the Classical era, some composers began to give the double basses their own part which was different from the cello part.
Timeline of Classical composers.
Almost all of the composers who are described in music textbooks on the classical period and whose works are widely performed as part of the standard concert repertoire from the classical period are male composers, even though there has been a large number of women composers throughout the classical music period. The best-known and most-played composers–Mozart, Haydn, Beethoven, Schubert–are male, and even the less-discussed and less played composers, such as Gossec, Cherubini and Hummel, are male. Scholar Marcia Citron has asked "hy is music composed by women so marginal to the standard 'classical' repertoire?" Citron "examines the practices and attitudes that have led to the exclusion of women composers from the received 'canon' of performed musical works." She argues that in the 1800s, women composers typically wrote art songs for performance in small recitals rather than symphonies intended for performance with an orchestra in a large hall, with the latter works being seen as the most important genre for composers; since women composers did not write many symphonies, they were deemed to be not notable as composers. In the "..."Concise Oxford History of Music", Clara Schumann is one of the only female composers mentioned." 

</doc>
<doc id="5295" url="https://en.wikipedia.org/wiki?curid=5295" title="Character encoding">
Character encoding

In computing, a character encoding is used to represent a repertoire of characters by some kind of an encoding system. Depending on the abstraction level and context, corresponding code points and the resulting code space may be regarded as bit patterns, octets, natural numbers, electrical pulses, etc. A character encoding is used in computation, data storage, and transmission of textual data. Character set, character map, codeset and code page are related, but not identical, terms.
Early character codes associated with the optical or electrical telegraph could only represent a subset of the characters used in written languages, sometimes restricted to upper case letters, numerals and some punctuation only. The low cost of digital representation of data in modern computer systems allows more elaborate character codes (such as Unicode) which represent more of the characters used in many written languages. Character encoding using internationally accepted standards permits worldwide interchange of text in electronic form.
History.
Early binary repertoires include Bacon's cipher, Braille, International maritime signal flags, and the 4-digit encoding of Chinese characters for a Chinese telegraph code (Hans Schjellerup, 1869). Common examples of character encoding systems include Morse code, the Baudot code, the American Standard Code for Information Interchange (ASCII) and Unicode.
Morse code was introduced in the 1840s and is used to encode each letter of the Latin alphabet, each Arabic numeral, and some other characters via a series of long and short presses of a telegraph key. Representations of characters encoded using Morse code varied in length.
The Baudot code, a five-bit encoding, was created by Émile Baudot in 1870, patented in 1874, modified by Donald Murray in 1901, and standardized by CCITT as International Telegraph Alphabet No. 2 (ITA2) in 1930.
Fieldata, a six- or seven-bit code, was introduced by the U.S. Army Signal Corps in the late 1950s.
IBM's Binary Coded Decimal (BCD) was a six-bit encoding scheme used by IBM in as early as 1959 in its 1401 and 1620 computers, and in its 7000 Series (for example, 704, 7040, 709 and 7090 computers), as well as in associated peripherals. BCD extended existing simple four-bit numeric encoding to include alphabetic and special characters, mapping it easily to punch-card encoding which was already in widespread use. It was the precursor to EBCDIC.
ASCII was introduced in 1963 and is a seven-bit encoding scheme used to encode letters, numerals, symbols, and device control codes as fixed-length codes using integers.
IBM's Extended Binary Coded Decimal Interchange Code (usually abbreviated as EBCDIC) is an eight-bit encoding scheme developed in 1963.
The limitations of such sets soon became apparent, and a number of ad hoc methods were developed to extend them. The need to support more writing systems for different languages, including the CJK family of East Asian scripts, required support for a far larger number of characters and demanded a systematic approach to character encoding rather than the previous ad hoc approaches.
The frustrating dilemma that researchers in this field encountered in the 1980s as they tried to develop universally interchangeable character encodings was that on the one hand, it seemed to be necessary to add more bits to accommodate additional characters. On the other hand, for the users of the relatively small character set of the Latin alphabet (who still constituted the majority of computer users at the time), those additional bits were a colossal waste of then-scarce and expensive computing resources (as they would always be zeroed out for such users).
The compromise solution that was eventually hit upon with Unicode, as further explained below, was to break the longstanding assumption (dating back to the old telegraph codes) that each character should always directly correspond to a particular pattern of encoded bits. Instead, characters would be first mapped to an intermediate stage in the form of abstract numbers known as code points. Those code points would then be encoded in a variety of ways and with various default numbers of bits per character (code units) depending upon context. To encode code points higher than the length of the code unit, such as above 256 for 8-bit units, the solution was to implement variable-width encodings where an escape sequence would signal that subsequent bits should be parsed as a higher code point.
Code unit.
Terminology related to code unit:
Example: The Latin character set is used by English and most European languages, though the Greek character set is used only by the Greek language.
A code unit is a bit sequence used to encode each single character unit of a repertoire within each encoding form.
Character repertoire (the abstract list of characters):
The character repertoire is an abstract list of more than one million characters found in a wide variety of scripts including "Latin, Cyrillic, Chinese, Korean, Japanese, Hebrew, and Aramaic".
Other symbols such as musical notation are also included in the character repertoire. Both the Unicode and GB18030 standards have a character repertoire. As new characters are added to one standard, the other standard also adds those characters, to maintain parity.
The code unit size is equivalent to the bit measurement for the particular encoding:
Example of a code unit: Imagine a String that contains the letters "abc" followed by the Deseret LONG I, which is represented with two char values. That string contains four characters, four code points, but overall five code units.
To express a character in Unicode, the hexadecimal value is prefixed with the string U+. The valid code point range for the Unicode standard is U+0000 to U+10FFFF, inclusive.
Characters that are in the range U+10000 to U+10FFFF are called supplementary characters. The set of characters from U+0000 to U+FFFF are sometimes referred to as the "Basic Multilingual Plane (BMP)".
The following table shows code point values examples:
The relationship between code points and code units:
A code point is a character and this is represented by one or more code units depending on the encoding.
In each encoding, the code points are mapped to one or more code units.
The number of code units required to be mapped to a code point varies across encoding forms:
Multiple code units per code point are common in UTF-8 because of the smaller code units. The code points will be mapped to one, two, three, or four code units.
UTF-16 code units are twice as large as 8-bit code units. Therefore, any code points with a scalar value less than U+10000 are encoded with a single code unit.
For code points with a scalar value of U+10000 or higher, two code units are required per code point. These pairs of code units have a unique term in UTF-16: "Unicode surrogate pairs".
The 32-bit code unit used in UTF-32 is large enough that every code point is encoded as a single code unit.
Multiple code units per code point are common in GB18030, because of the smaller code units. The code points will be mapped to one, two, or four code units.
<references group="http://docs.oracle.com/javase/tutorial/i18n/text/terminology.html" />
Unicode encoding model.
Unicode and its parallel standard, the ISO/IEC 10646 Universal Character Set, together constitute a modern, unified character encoding. Rather than mapping characters directly to octets (bytes), they separately define what characters are available, corresponding natural numbers (code points), how those numbers are encoded as a series of fixed-size natural numbers (code units), and finally how those units are encoded as a stream of octets. The idea behind this decomposition is to establish a universal set of characters that can be encoded in a variety of ways. To describe this model correctly one needs more precise terms than "character set" and "character encoding." The terms used in the modern model follow:
A "character repertoire" is the full set of abstract characters that a system supports. The repertoire may be closed, i.e. no additions are allowed without creating a new standard (as is the case with ASCII and most of the ISO-8859 series), or it may be open, allowing additions (as is the case with Unicode and to a limited extent the Windows code pages). The characters in a given repertoire reflect decisions that have been made about how to divide writing systems into basic information units. The basic variants of the Latin, Greek and Cyrillic alphabets can be broken down into letters, digits, punctuation, and a few "special characters" such as the space, which can all be arranged in simple linear sequences that are displayed in the same order they are read. Even with these alphabets, however, diacritics pose a complication: they can be regarded either as part of a single character containing a letter and diacritic (known as a precomposed character), or as separate characters. The former allows a far simpler text handling system but the latter allows any letter/diacritic combination to be used in text. Ligatures pose similar problems. Other writing systems, such as Arabic and Hebrew, are represented with more complex character repertoires due to the need to accommodate things like bidirectional text and glyphs that are joined together in different ways for different situations.
A "coded character set" (CCS) specifies how to represent characters using a number of "code points". For example, in a given repertoire, a character representing the capital letter "A" in the Latin alphabet might be assigned to the code point 65, the character for "B" to 66, and so on. Multiple coded character sets may share the same repertoire; for example ISO/IEC 8859-1 and IBM code pages 037 and 500 all cover the same repertoire but map them to different codes. In a coded character set, each code point only represents one character, i.e., a coded character set is a function.
A "character encoding form" (CEF) specifies the conversion of a coded character set's code points into a set of "code units" that facilitate storage in a system that represents numbers in binary form using a fixed number of bits (i.e. practically any computer system). For example, a system that stores numeric information in 16-bit units would only be able to directly represent code points from 0 to 65,535 in each unit, but larger code points could be represented if more than one 16-bit unit could be used. This is what a CEF accommodates: it defines a way of mapping a "single" code point from a range of, say, 0 to 1.4 million, to a series of "one or more" code "values" from a range of, say, 0 to 65,535.
Next, a "character encoding scheme" (CES) specifies how the code units should be mapped into an octet sequence suitable for saving on an octet-based file system or transmitting over an octet-based network. There are simple character encoding schemes, like UTF-8, UTF-16BE, UTF-32BE, UTF-16LE or UTF-32LE; compound character encoding schemes, which use byte order marks or escape sequences to switch between several simple schemes (such as UTF-16, UTF-32 and ISO/IEC 2022); and compressing schemes, which try to minimise the number of bytes used per code unit (such as SCSU, BOCU, and Punycode).
Although UTF-32BE is a simpler CES, most systems working with Unicode use either UTF-8, which is backward compatible with fixed-width ASCII and maps Unicode code points to variable-width sequences of octets, or UTF-16BE, which is backward compatible with fixed-width UCS-2BE and maps Unicode code points to variable-width sequences of 16-bit words. See comparison of Unicode encodings for a detailed discussion.
Finally, there may be a "higher level protocol" which supplies additional information that can be used to select the particular variant of a Unicode character, particularly where there are regional variants that have been 'unified' in Unicode as the same character. An example is the XML attribute xml:lang.
The Unicode model reserves the term "character map" for historical systems which directly assign a sequence of characters to a sequence of bytes, covering all of CCS, CEF and CES layers.
Character sets, maps and code pages.
In computer science, the terms "character encoding", "character map", "character set" and "code page" were historically synonymous, as the same standard would specify a repertoire of characters and how they were to be encoded into a stream of code units – usually with a single character per code unit. The terms now have related but distinct meanings, reflecting the efforts of standards bodies to use precise terminology when writing about and unifying many different encoding systems. Regardless, the terms are still used interchangeably, with "character set" being nearly ubiquitous.
A "code page" usually means a byte-oriented encoding, but with regard to some suite of encodings (covering different scripts), where many characters share the same codes in most or all those code pages. Well known code page suites are "Windows" (based on Windows-1252) and "IBM"/"DOS" (based on code page 437), see Windows code page for details. Most, but not all, encodings referred to as code pages are single-byte encodings (but see octet on byte size.)
IBM's Character Data Representation Architecture (CDRA) designates with coded character set identifiers (CCSIDs) and each of which is variously called a "charset", "character set", "code page", or "CHARMAP".
The term "code page" does not occur in Unix or Linux where "charmap" is preferred, usually in the larger context of locales.
Contrasted to CCS above, a "character encoding" is a map from abstract characters to code words. A "character set" in HTTP (and MIME) parlance is the same as a character encoding (but not the same as CCS).
"Legacy encoding" is a term sometimes used to characterize old character encodings, but with an ambiguity of sense. Most of its use is in the context of Unicodification, where it refers to encodings that fail to cover all Unicode code points, or, more generally, using a somewhat different character repertoire: several code points representing one Unicode character, or versa (see e.g. code page 437). Some sources refer to an encoding as "legacy" only because it preceded Unicode. All Windows code pages are usually referred to as legacy, both because they antedate Unicode and because they are unable to represent all 221 possible Unicode code points.
Character encoding translation.
As a result of having many character encoding methods in use (and the need for backward compatibility with archived data), many computer programs have been developed to translate data between encoding schemes as a form of data transcoding. Some of these are cited below.
Cross-platform:
Unix-like: 
Windows:

</doc>
<doc id="5296" url="https://en.wikipedia.org/wiki?curid=5296" title="Cogency">
Cogency


</doc>
<doc id="5298" url="https://en.wikipedia.org/wiki?curid=5298" title="Control character">
Control character

In computing and telecommunication, a control character or non-printing character is a code point (a number) in a character set, that does not represent a written symbol. They are used as in-band signaling to cause effects other than the addition of a symbol to the text. All other characters are mainly printing, printable, or graphic characters, except perhaps for the "space" character (see ASCII printable characters).
All entries in the ASCII table below code 32 (technically the C0 control code set) are of this kind, including CR and LF used to separate lines of text. The code 127 (DEL) is also a control character. Extended ASCII sets defined by ISO 8859 added the codes 128 through 159 as control characters, this was primarily done so that if the high bit was stripped it would not change a printing character to a C0 control code, but there have been some assignments here, in particular NEL. This second set is called the C1 set.
These 65 control codes were carried over to Unicode. Unicode added more characters that could be considered controls, but it makes a distinction between these "Formatting characters" (such as the Zero-width non-joiner), and the 65 Control characters.
The Extended Binary Coded Decimal Interchange Code (EBCDIC) character set contains 65 control codes, including all of the ASCII control codes as well as additional codes which are mostly used to control IBM peripherals.
History.
Procedural signs in Morse code are a form of control character.
A form of control characters were introduced in the 1870 Baudot code: NUL and DEL.
The 1901 Murray code added the carriage return (CR) and line feed (LF), and other versions of the Baudot code included other control characters.
The bell character (BEL), which rang a bell to alert operators, was also an early teletype control character.
Control characters have also been called "format effectors".
In ASCII.
The control characters in ASCII still in common use include:
Occasionally one might encounter modern uses of other codes, such as code 4 (End of transmission, EOT, codice_33)), used to end a Unix shell session or PostScript printer transmission. For the full list of control characters, see ASCII.
Even though many control characters are rarely used, the concept of sending device-control information intermixed with printable characters is so useful that device makers found a way to send hundreds of device instructions. Specifically, they used ASCII code 27 (escape), followed by a series of characters called a "control sequence" or "escape sequence". The mechanism was invented by Bob Bemer, the father of ASCII.
Typically, code 27 was sent first in such a sequence to alert the device that the following characters were to be interpreted as a control sequence rather than as plain characters, then one or more characters would follow to specify some detailed action, after which the device would go back to interpreting characters normally. For example, the sequence of code 27, followed by the printable characters "[2;10H", would cause a DEC VT-102 terminal to move its cursor to the 10th cell of the 2nd line of the screen. Several standards exist for these sequences, notably ANSI X3.64. But the number of non-standard variations in use is large, especially among printers, where technology has advanced far faster than any standards body can possibly keep up with.
In Unicode.
In Unicode, "Control-characters" are U+0000—U+001F (C0 controls), U+007F (delete), and U+0080—U+009F (C1 controls). Their General Category is "Cc". Formatting codes are distinct, in General Category "Cf". The Cc control characters have no Name in Unicode. They may be indicated informally as "<control-001A>".
Display.
There are a number of techniques to display non-printing characters, which may be illustrated with the bell character in ASCII encoding:
How control characters map to keyboards.
ASCII-based keyboards have a key labelled "Control", "Ctrl", or (rarely) "Cntl" which is used much like a shift key, being pressed in combination with another letter or symbol key. In one implementation, the control key generates the code 64 places below the code for the (generally) uppercase letter it is pressed in combination with (i.e., subtract 64 from ASCII code value in decimal of the (generally) uppercase letter). The other implementation is to take the ASCII code produced by the key and bitwise AND it with 31, forcing bits 6 and 7 to zero. For example, pressing "control" and the letter "g" or "G" (code 107 in octal or 71 in base 10, which is 01000111 in binary, produces the code 7 (Bell, 7 in base 10, or 00000111 in binary). The NULL character (code 0) is represented by Ctrl-@, "@" being the code immediately before "A" in the ASCII character set. For convenience, a lot of terminals accept Ctrl-Space as an alias for Ctrl-@. In either case, this produces one of the 32 ASCII control codes between 0 and 31. This approach is not able to represent the DEL character because of its value (code 127), but Ctrl-? is often used for this character, as subtracting 64 from a '?' gives −1, which if masked to 7 bits is 127.
When the control key is held down, letter keys produce the same control characters regardless of the state of the shift or caps lock keys. In other words, it does not matter whether the key would have produced an upper-case or a lower-case letter. The interpretation of the control key with the space, graphics character, and digit keys (ASCII codes 32 to 63) vary between systems. Some will produce the same character code as if the control key were not held down. Other systems translate these keys into control characters when the control key is held down. The interpretation of the control key with non-ASCII ("foreign") keys also varies between systems.
Control characters are often rendered into a printable form known as caret notation by printing a caret (^) and then the ASCII character that has a value of the control character plus 64. Control characters generated using letter keys are thus displayed with the upper-case form of the letter. For example, ^G represents code 7, which is generated by pressing the G key when the control key is held down.
Keyboards also typically have a few single keys which produce control character codes. For example, the key labelled "Backspace" typically produces code 8, "Tab" code 9, "Enter" or "Return" code 13 (though some keyboards might produce code 10 for "Enter").
Many keyboards include keys that do not correspond to any ASCII printable or control character, for example cursor control arrows and word processing functions. The associated keypresses are communicated to computer programs by one of four methods: appropriating otherwise unused control characters; using some encoding other than ASCII; using multi-character control sequences; or using an additional mechanism outside of generating characters. "Dumb" computer terminals typically use control sequences. Keyboards attached to stand-alone personal computers made in the 1980s typically use one (or both) of the first two methods. Modern computer keyboards generate scancodes that identify the specific physical keys that are pressed; computer software then determines how to handle the keys that are pressed, including any of the four methods described above.
The design purpose.
The control characters were designed to fall into a few groups: printing and display control, data structuring, transmission control, and miscellaneous.
Printing and display control.
Printing control characters were first used to control the physical mechanism of printers, the earliest output device. An early implementation of this idea was the out-of-band ASA carriage control characters. Later, control characters were integrated into the stream of data to be printed.
The carriage return character (CR), when sent to such a device, causes it to put the character at the edge of the paper at which writing begins (it may, or may not, also move the printing position to the next line).
The line feed character (LF/NL) causes the device to put the printing position on the next line. It may (or may not), depending on the device and its configuration, also move the printing position to the start of the next line (whichever direction is first—left in Western languages and right in Hebrew and Arabic). 
The vertical and horizontal tab characters (VT and HT/TAB) cause the output device to move the printing position to the next tab stop in the direction of reading. 
The form feed character (FF/NP) starts a new sheet of paper, and may or may not move to the start of the first line. 
The backspace character (BS) moves the printing position one character space backwards. On printers, this is most often used so the printer can overprint characters to make other, not normally available, characters. On terminals and other electronic output devices, there are often software (or hardware) configuration choices which will allow a destruct backspace (i.e., a BS, SP, BS sequence) which erases, or a non-destructive one which does not. 
The shift in and shift out characters (SO and SI) selected alternate character sets, fonts, underlining or other printing modes. Escape sequences were often used to do the same thing.
With the advent of computer terminals that did not physically print on paper and so offered more flexibility regarding screen placement, erasure, and so forth, printing control codes were adapted. Form feeds, for example, usually cleared the screen, there being no new paper page to move to. More complex escape sequences were developed to take advantage of the flexibility of the new terminals, and indeed of newer printers. The concept of a control character had always been somewhat limiting, and was extremely so when used with new, much more flexible, hardware. Control sequences (sometimes implemented as escape sequences) could match the new flexibility and power and became the standard method. However, there were, and remain, a large variety of standard sequences to choose from.
Data structuring.
The separators (File, Group, Record, and Unit: FS, GS, RS and US) were made to structure data, usually on a tape, in order to simulate punched cards.
End of medium (EM) warns that the tape (or other recording medium) is ending.
While many systems use CR/LF and TAB for structuring data, it is possible to encounter the separator control characters in data that needs to be structured. The separator control characters are not overloaded; there is no general use of them except to separate data into structured groupings. Their numeric values are contiguous with the space character, which can be considered a member of the group, as a word separator.
Transmission control.
The transmission control characters were intended to structure a data stream, and to manage re-transmission or graceful failure, as needed, in the face of transmission errors.
The start of heading (SOH) character was to mark a non-data section of a data stream—the part of a stream containing addresses and other housekeeping data. The start of text character (STX) marked the end of the header, and the start of the textual part of a stream. The end of text character (ETX) marked the end of the data of a message. A widely used convention is to make the two characters preceding ETX a checksum or CRC for error-detection purposes. The end of transmission block character (ETB) was used to indicate the end of a block of data, where data was divided into such blocks for transmission purposes.
The escape character (ESC) was intended to "quote" the next character, if it was another control character it would print it instead of performing the control function. It is almost never used for this purpose today.
The substitute character (SUB) was intended to request a translation of the next character from a printable character to another value, usually by setting bit 5 to zero. This is handy because some media (such as sheets of paper produced by typewriters) can transmit only printable characters. However, on MS-DOS systems with files opened in text mode, "end of text" or "end of file" is marked by this Ctrl-Z character, instead of the Ctrl-C or Ctrl-D, which are common on other operating systems.
The cancel character (CAN) signalled that the previous element should be discarded. The negative acknowledge character (NAK) is a definite flag for, usually, noting that reception was a problem, and, often, that the current element should be sent again. The acknowledge character (ACK) is normally used as a flag to indicate no problem detected with current element.
When a transmission medium is half duplex (that is, it can transmit in only one direction at a time), there is usually a master station that can transmit at any time, and one or more slave stations that transmit when they have permission. The enquire character (ENQ) is generally used by a master station to ask a slave station to send its next message. A slave station indicates that it has completed its transmission by sending the end of transmission character (EOT).
The device control codes (DC1 to DC4) were originally generic, to be implemented as necessary by each device. However, a universal need in data transmission is to request the sender to stop transmitting when a receiver can't take more data right now. Digital Equipment Corporation invented a convention which used 19, (the device control 3 character (DC3), also known as control-S, or XOFF) to "S"top transmission, and 17, (the device control 1 character (DC1), a.k.a. control-Q, or XON) to start transmission. It has become so widely used that most don't realize it is not part of official ASCII. This technique, however implemented, avoids additional wires in the data cable devoted only to transmission management, which saves money. A sensible protocol for the use of such transmission flow control signals must be used, to avoid potential deadlock conditions, however.
The data link escape character (DLE) was intended to be a signal to the other end of a data link that the following character is a control character such as STX or ETX. For example a packet may be structured in the following way (DLE) <STX> <PAYLOAD> (DLE) <ETX>.
Miscellaneous codes.
Code 7 (BEL) is intended to cause an audible signal in the receiving terminal.
Many of the ASCII control characters were designed for devices of the time that are not often seen today. For example, code 22, "synchronous idle" (SYN), was originally sent by synchronous modems (which have to send data constantly) when there was no actual data to send. (Modern systems typically use a start bit to announce the beginning of a transmitted word— this is a feature of "asynchronous" communication. "Synchronous" communication links were more often seen with mainframes, where they were typically run over corporate leased lines to connect a mainframe to another mainframe or perhaps a minicomputer.)
Code 0 (ASCII code name NUL) is a special case. In paper tape, it is the case when there are no holes. It is convenient to treat this as a "fill" character with no meaning otherwise. Since the position of a NUL character has no holes punched, it can be replaced with any other character at a later time, so it was typically used to reserve space, either for correcting errors or for inserting information that would be available at a later time or in another place. In computing it is often used for padding in fixed length records and more commonly, to mark the end of a string.
Code 127 (DEL, a.k.a. "rubout") is likewise a special case. Its 7-bit code is "all-bits-on" in binary, which essentially erased a character cell on a paper tape when overpunched. Paper tape was a common storage medium when ASCII was developed, with a computing history dating back to WWII code breaking equipment at Biuro Szyfrów. Paper tape became obsolete in the 1970s, so this clever aspect of ASCII rarely saw any use after that. Some systems (such as the original Apples) converted it to a backspace. But because its code is in the range occupied by other printable characters, and because it had no official assigned glyph, many computer equipment vendors used it as an additional printable character (often an all-black "box" character useful for erasing text by overprinting with ink).
However it should be noted that non-erasable Programmable ROMs are typically implemented as arrays of fusible elements, each representing a bit, which can only be switched one way, usually from one to zero. In such PROMs, the DEL and NUL characters can be used in the same way that they were used on punched tape: one to reserve meaningless fill bytes that can be written later, and the other to convert written bytes to meaningless fill bytes. For PROMs that switch one to zero, the roles of NUL and DEL are reversed; also, DEL will only work with 7-bit characters, which are rarely used today; for 8-bit content, the character code 255, commonly defined as a nonbreaking space character, can be used instead of DEL.
Many file systems do not allow control characters in the filenames, as they may have reserved functions.

</doc>
<doc id="5299" url="https://en.wikipedia.org/wiki?curid=5299" title="Carbon">
Carbon

Carbon (from "coal") is a chemical element with symbol C and atomic number 6. On the periodic table, it is the first (row 2) of six elements in column (group) 14, which have in common the composition of their outer electron shell. It is nonmetallic and tetravalent—making four electrons available to form covalent chemical bonds. Three isotopes occur naturally, C and C being stable while C is radioactive, decaying with a half-life of about 5,730 years. Carbon is one of the few elements known since antiquity.
Carbon is the 15th most abundant element in the Earth's crust, and the fourth most abundant element in the universe by mass after hydrogen, helium, and oxygen. Carbon's abundance, its unique diversity of organic compounds, and its unusual ability to form polymers at the temperatures commonly encountered on Earth enables this element to serve as a common element of all known life. It is the second most abundant element in the human body by mass (about 18.5%) after oxygen.
The atoms of carbon can be bonded together in different ways, termed allotropes of carbon. The best known are graphite, diamond, and amorphous carbon. The physical properties of carbon vary widely with the allotropic form. For example, graphite is opaque and black while diamond is highly transparent. Graphite is soft enough to form a streak on paper (hence its name, from the Greek verb "γράφειν" which means "to write"), while diamond is the hardest naturally-occurring material known. Graphite is a good electrical conductor while diamond has a low electrical conductivity. Under normal conditions, diamond, carbon nanotubes, and graphene have the highest thermal conductivities of all known materials. All carbon allotropes are solids under normal conditions, with graphite being the most thermodynamically stable form. They are chemically resistant and require high temperature to react even with oxygen.
The most common oxidation state of carbon in inorganic compounds is +4, while +2 is found in carbon monoxide and transition metal carbonyl complexes. The largest sources of inorganic carbon are limestones, dolomites and carbon dioxide, but significant quantities occur in organic deposits of coal, peat, oil, and methane clathrates. Carbon forms a vast number of compounds, more than any other element, with almost ten million compounds described to date, and yet that number is but a fraction of the number theoretically possible compounds under standard conditions.
Characteristics.
The "allotropes" of carbon (see below) includes graphite, one of the softest known substances, and diamond, the hardest naturally occurring substance. It bonds readily with other small atoms including other carbon atoms, and is capable of forming multiple stable covalent bonds with such atoms. Carbon is known to form almost ten million different compounds, a large majority of all chemical compounds. Carbon also has the highest sublimation point of all elements. At atmospheric pressure it has no melting point as its triple point is at 10.8 ± 0.2 MPa and 4,600 ± 300 K (~4,330 °C or 7,820 °F), so it sublimes at about 3,900 K.
Carbon sublimes in a carbon arc which has a temperature of about 5,800 K (5,530 °C; 9,980 °F). Thus, irrespective of its allotropic form, carbon remains solid at higher temperatures than the highest melting point metals such as tungsten or rhenium. Although thermodynamically prone to oxidation, carbon resists oxidation more effectively than elements such as iron and copper that are weaker reducing agents at room temperature.
Carbon compounds form the basis of all known life on Earth, and the carbon-nitrogen cycle provides some of the energy produced by the Sun and other stars. Although it forms an extraordinary variety of compounds, most forms of carbon are comparatively unreactive under normal conditions. At standard temperature and pressure, it resists all but the strongest oxidizers. It does not react with sulfuric acid, hydrochloric acid, chlorine or any alkalis. At elevated temperatures, carbon reacts with oxygen to form carbon oxides, and will rob oxygen from metal oxides to leave the elemental metal. This exothermic reaction is used in the iron and steel industry to smelt iron and to control the carbon content of steel:
with sulfur to form carbon disulfide and with steam in the coal-gas reaction:
Carbon combines with some metals at high temperatures to form metallic carbides, such as the iron carbide cementite in steel, and tungsten carbide, widely used as an abrasive and for making hard tips for cutting tools.
As of 2009, graphene appears to be the strongest material ever tested. The process of separating it from graphite will require some further technological development before it is economical for industrial processes.
The system of carbon allotropes spans a range of extremes:
Allotropes.
Atomic carbon is a very short-lived species and, therefore, carbon is stabilized in various multi-atomic structures with different molecular configurations called allotropes. The three relatively well-known allotropes of carbon are amorphous carbon, graphite, and diamond. Once considered exotic, fullerenes are nowadays commonly synthesized and used in research; they include buckyballs, carbon nanotubes, carbon nanobuds and nanofibers. Several other exotic allotropes have also been discovered, such as lonsdaleite (questionable), glassy carbon, carbon nanofoam and linear acetylenic carbon (carbyne).
The amorphous form is an assortment of carbon atoms in a non-crystalline, irregular, glassy state, which is essentially graphite but not held in a crystalline macrostructure. It is present as a powder, and is the main constituent of substances such as charcoal, lampblack (soot) and activated carbon. At normal pressures, carbon takes the form of graphite, in which each atom is bonded trigonally to three others in a plane composed of fused hexagonal rings, just like those in aromatic hydrocarbons. The resulting network is 2-dimensional, and the resulting flat sheets are stacked and loosely bonded through weak van der Waals forces. This gives graphite its softness and its cleaving properties (the sheets slip easily past one another). Because of the delocalization of one of the outer electrons of each atom to form a π-cloud, graphite conducts electricity, but only in the plane of each covalently bonded sheet. This results in a lower bulk electrical conductivity for carbon than for most metals. The delocalization also accounts for the energetic stability of graphite over diamond at room temperature.
At very high pressures, carbon forms the more compact allotrope, diamond, having nearly twice the density of graphite. Here, each atom is bonded tetrahedrally to four others, forming a 3-dimensional network of puckered six-membered rings of atoms. Diamond has the same cubic structure as silicon and germanium, and because of the strength of the carbon-carbon bonds, it is the hardest naturally occurring substance measured by resistance to scratching. Contrary to the popular belief that ""diamonds are forever"", they are thermodynamically unstable under normal conditions and transform into graphite. Due to a high activation energy barrier, the transition into graphite is so slow at normal temperature that it is unnoticeable. Under some conditions, carbon crystallizes as lonsdaleite, a hexagonal crystal lattice with all atoms covalently bonded and properties similar to those of diamond.
Fullerenes are a synthetic crystalline formation with a graphite-like structure, but in place of hexagons, fullerenes are formed of pentagons (or even heptagons) of carbon atoms. The missing (or additional) atoms warp the sheets into spheres, ellipses, or cylinders. The properties of fullerenes (split into buckyballs, buckytubes, and nanobuds) have not yet been fully analyzed and represent an intense area of research in nanomaterials. The names ""fullerene"" and ""buckyball"" are given after Richard Buckminster Fuller, popularizer of geodesic domes, which resemble the structure of fullerenes. The buckyballs are fairly large molecules formed completely of carbon bonded trigonally, forming spheroids (the best-known and simplest is the soccerball-shaped C buckminsterfullerene). Carbon nanotubes are structurally similar to buckyballs, except that each atom is bonded trigonally in a curved sheet that forms a hollow cylinder. Nanobuds were first reported in 2007 and are hybrid bucky tube/buckyball materials (buckyballs are covalently bonded to the outer wall of a nanotube) that combine the properties of both in a single structure.
Of the other discovered allotropes, carbon nanofoam is a ferromagnetic allotrope discovered in 1997. It consists of a low-density cluster-assembly of carbon atoms strung together in a loose three-dimensional web, in which the atoms are bonded trigonally in six- and seven-membered rings. It is among the lightest known solids, with a density of about 2 kg/m. Similarly, glassy carbon contains a high proportion of closed porosity, but contrary to normal graphite, the graphitic layers are not stacked like pages in a book, but have a more random arrangement. Linear acetylenic carbon has the chemical structure -(C:::C)-. Carbon in this modification is linear with "sp" orbital hybridization, and is a polymer with alternating single and triple bonds. This carbyne is of considerable interest to nanotechnology as its Young's modulus is forty times that of the hardest known material – diamond.
In 2015, a team at the North Carolina State University announced the development of another allotrope they have dubbed Q-carbon, created by a high energy low duration laser pulse on amorphous carbon dust. Q-carbon is reported to exhibit ferromagetism, fluorescence, and a hardness superior to diamonds.
Occurrence.
Carbon is the fourth most abundant chemical element in the universe by mass after hydrogen, helium, and oxygen. Carbon is abundant in the Sun, stars, comets, and in the atmospheres of most planets. Some meteorites contain microscopic diamonds that were formed when the solar system was still a protoplanetary disk. Microscopic diamonds may also be formed by the intense pressure and high temperature at the sites of meteorite impacts.
In 2014 NASA announced a greatly upgraded database for tracking polycyclic aromatic hydrocarbons (PAHs) in the universe. More than 20% of the carbon in the universe may be associated with PAHs, complex compounds of carbon and hydrogen without oxygen. These compounds figure in the PAH world hypothesis where they are hypothesized to have a role in abiogenesis and formation of life. PAHs seem to have been formed "a couple of billion years" after the Big Bang, are widespread throughout the universe, and are associated with new stars and exoplanets.
It has been estimated that the solid earth as a whole contains 730 ppm of carbon, with 2000 ppm in the core and 120 ppm in the combined mantle and crust. Since the mass of the earth is , this would imply 4360 million gigatonnes of carbon. This is much more than the amount of carbon in the oceans or atmosphere (below).
In combination with oxygen in carbon dioxide, carbon is found in the Earth's atmosphere (approximately 810 gigatonnes of carbon) and dissolved in all water bodies (approximately 36,000 gigatonnes of carbon). Around 1,900 gigatonnes of carbon are present in the biosphere. Hydrocarbons (such as coal, petroleum, and natural gas) contain carbon as well. Coal "reserves" (not "resources") amount to around 900 gigatonnes with perhaps 18 000 Gt of resources. Oil reserves are around 150 gigatonnes. Proven sources of natural gas are about 175 10 cubic metres (containing about 105 gigatonnes of carbon), but studies estimate another 900 10 cubic metres of "unconventional" deposits such as shale gas, representing about 540 gigatonnes of carbon.
Carbon is also found in methane hydrates in polar regions and under the seas. Various estimates put this carbon between 500, 2500 Gt, or 3000 Gt.
In the past, quantities of hydrocarbons were greater. According to one source, in the period from 1751 to 2008 about 347 gigatonnes of carbon were released as carbon dioxide to the atmosphere from burning of fossil fuels. Another source puts the amount added to the atmosphere for the period since 1750 at 879 Gt, and the total going to the atmosphere, sea, and land (such as peat bogs) at almost 2000 Gt.
Carbon is a constituent (about 12% by mass) of the very large masses of carbonate rock (limestone, dolomite, marble and so on). Coal is very rich in carbon (anthracite contains 92–98%) and is the largest commercial source of mineral carbon, accounting for 4,000 gigatonnes or 80% of fossil fuel.
As for individual carbon allotropes, graphite is found in large quantities in the United States (mostly in New York and Texas), Russia, Mexico, Greenland, and India. Natural diamonds occur in the rock kimberlite, found in ancient volcanic "necks", or "pipes". Most diamond deposits are in Africa, notably in South Africa, Namibia, Botswana, the Republic of the Congo, and Sierra Leone. Diamond deposits have also been found in Arkansas, Canada, the Russian Arctic, Brazil, and in Northern and Western Australia. Diamonds are now also being recovered from the ocean floor off the Cape of Good Hope. Diamonds are found naturally, but about 30% of all industrial diamonds used in the U.S. are now manufactured.
Carbon-14 is formed in upper layers of the troposphere and the stratosphere at altitudes of 9–15 km by a reaction that is precipitated by cosmic rays. Thermal neutrons are produced that collide with the nuclei of nitrogen-14, forming carbon-14 and a proton.
Carbon-rich asteroids are relatively preponderant in the outer parts of the asteroid belt in our solar system. These asteroids have not yet been directly sampled by scientists. The asteroids can be used in hypothetical space-based carbon mining, which may be possible in the future, but is currently technologically impossible.
Isotopes.
Isotopes of carbon are atomic nuclei that contain six protons plus a number of neutrons (varying from 2 to 16). Carbon has two stable, naturally occurring isotopes. The isotope carbon-12 (C) forms 98.93% of the carbon on Earth, while carbon-13 (C) forms the remaining 1.07%. The concentration of C is further increased in biological materials because biochemical reactions discriminate against C. In 1961, the International Union of Pure and Applied Chemistry (IUPAC) adopted the isotope carbon-12 as the basis for atomic weights. Identification of carbon in Nuclear Magnetic Resonance (NMR) experiments is done with the isotope C.
Carbon-14 (C) is a naturally occurring radioisotope, created in the upper atmosphere (lower stratosphere and upper troposphere) by interaction of nitrogen with cosmic rays. It is found in trace amounts on Earth of up to 1 part per trillion (0.0000000001%), mostly confined to the atmosphere and superficial deposits, particularly of peat and other organic materials. This isotope decays by 0.158 MeV β emission. Because of its relatively short half-life of 5730 years, C is virtually absent in ancient rocks. The amount of C in the atmosphere and in living organisms is almost constant, but decreases predictably in their bodies after death. This principle is used in radiocarbon dating, invented in 1949, which has been used extensively to determine the age of carbonaceous materials with ages up to about 40,000 years.
There are 15 known isotopes of carbon and the shortest-lived of these is C which decays through proton emission and alpha decay and has a half-life of 1.98739x10 s. The exotic C exhibits a nuclear halo, which means its radius is appreciably larger than would be expected if the nucleus were a sphere of constant density.
Formation in stars.
Formation of the carbon atomic nucleus requires a nearly simultaneous triple collision of alpha particles (helium nuclei) within the core of a giant or supergiant star which is known as the triple-alpha process, as the products of further nuclear fusion reactions of helium with hydrogen or another helium nucleus produce lithium-5 and beryllium-8 respectively, both of which are highly unstable and decay almost instantly back into smaller nuclei. This happens in conditions of temperatures over 100 megakelvin and helium concentration that the rapid expansion and cooling of the early universe prohibited, and therefore no significant carbon was created during the Big Bang.
According to current physical cosmology theory, carbon is formed in the interiors of stars in the horizontal branch by the collision and transformation of three helium nuclei. When those stars die as supernova, the carbon is scattered into space as dust. This dust becomes component material for the formation of second or third-generation star systems with accreted planets. The Solar System is one such star system with an abundance of carbon, enabling the existence of life as we know it.
The CNO cycle is an additional fusion mechanisms that powers stars, wherein carbon operates as a catalyst.
Rotational transitions of various isotopic forms of carbon monoxide (for example, CO, CO, and CO) are detectable in the submillimeter wavelength range, and are used in the study of newly forming stars in molecular clouds.
Carbon cycle.
Under terrestrial conditions, conversion of one element to another is very rare. Therefore, the amount of carbon on Earth is effectively constant. Thus, processes that use carbon must obtain it from somewhere and dispose of it somewhere else. The paths of carbon in the environment form the carbon cycle. For example, photosynthetic plants draw carbon dioxide from the atmosphere (or seawater) and build it into biomass, as in the Calvin cycle, a process of carbon fixation. Some of this biomass is eaten by animals, while some carbon is exhaled by animals as carbon dioxide. The carbon cycle is considerably more complicated than this short loop; for example, some carbon dioxide is dissolved in the oceans; if bacteria do not consume it, dead plant or animal matter may become petroleum or coal, which releases carbon when burned.
Compounds.
Organic compounds.
Carbon can form very long chains of interconnecting C-C bonds, a property that is called catenation. Carbon-carbon bonds are strong and stable. Through catenation, carbon forms a countless number of compounds. A tally of unique compounds shows that more contain carbon that those that do not. A similar claim can be made for hydrogen because most organic compounds also contain hydrogen.
The simplest form of an organic molecule is the hydrocarbon—a large family of organic molecules that are composed of hydrogen atoms bonded to a chain of carbon atoms. Chain length, side chains and functional groups all affect the properties of organic molecules.
Carbon occurs in all known organic life and is the basis of organic chemistry. When united with hydrogen, it forms various hydrocarbons that are important to industry as refrigerants, lubricants, solvents, as chemical feedstock for the manufacture of plastics and petrochemicals, and as fossil fuels.
When combined with oxygen and hydrogen, carbon can form many groups of important biological compounds including sugars, lignans, chitins, alcohols, fats, and aromatic esters, carotenoids and terpenes. With nitrogen it forms alkaloids, and with the addition of sulfur also it forms antibiotics, amino acids, and rubber products. With the addition of phosphorus to these other elements, it forms DNA and RNA, the chemical-code carriers of life, and adenosine triphosphate (ATP), the most important energy-transfer molecule in all living cells.
Inorganic compounds.
Commonly carbon-containing compounds which are associated with minerals or which do not contain hydrogen or fluorine, are treated separately from classical organic compounds; the definition is not rigid (see reference articles above). Among these are the simple oxides of carbon. The most prominent oxide is carbon dioxide (). This was once the principal constituent of the paleoatmosphere, but is a minor component of the Earth's atmosphere today. Dissolved in water, it forms carbonic acid (), but as most compounds with multiple single-bonded oxygens on a single carbon it is unstable. Through this intermediate, though, resonance-stabilized carbonate ions are produced. Some important minerals are carbonates, notably calcite. Carbon disulfide () is similar.
The other common oxide is carbon monoxide (CO). It is formed by incomplete combustion, and is a colorless, odorless gas. The molecules each contain a triple bond and are fairly polar, resulting in a tendency to bind permanently to hemoglobin molecules, displacing oxygen, which has a lower binding affinity. Cyanide (CN), has a similar structure, but behaves much like a halide ion (pseudohalogen). For example, it can form the nitride cyanogen molecule ((CN)), similar to diatomic halides. Other uncommon oxides are carbon suboxide (), the unstable dicarbon monoxide (CO), carbon trioxide (CO), cyclopentanepentone (CO) cyclohexanehexone (CO), and mellitic anhydride (CO).
With reactive metals, such as tungsten, carbon forms either carbides (C), or acetylides () to form alloys with high melting points. These anions are also associated with methane and acetylene, both very weak acids. With an electronegativity of 2.5, carbon prefers to form covalent bonds. A few carbides are covalent lattices, like carborundum (SiC), which resembles diamond.
Organometallic compounds.
Organometallic compounds by definition contain at least one carbon-metal bond. A wide range of such compounds exist; major classes include simple alkyl-metal compounds (for example, tetraethyllead), η-alkene compounds (for example, Zeise's salt), and η-allyl compounds (for example, allylpalladium chloride dimer); metallocenes containing cyclopentadienyl ligands (for example, ferrocene); and transition metal carbene complexes. Many metal carbonyls exist (for example, tetracarbonylnickel); some workers consider the carbon monoxide ligand to be purely inorganic, and not organometallic.
While carbon is understood to exclusively form four bonds, an interesting compound containing an octahedral hexacoordinated carbon atom has been reported. The cation of the compound is [(PhPAu)C]. This phenomenon has been attributed to the aurophilicity of the gold ligands.
History and etymology.
The English name "carbon" comes from the Latin "carbo" for coal and charcoal, whence also comes the French "charbon", meaning charcoal. In German, Dutch and Danish, the names for carbon are "Kohlenstoff", "koolstof" and "kulstof" respectively, all literally meaning coal-substance.
Carbon was discovered in prehistory and was known in the forms of soot and charcoal to the earliest human civilizations. Diamonds were known probably as early as 2500 BCE in China, while carbon in the form of charcoal was made around Roman times by the same chemistry as it is today, by heating wood in a pyramid covered with clay to exclude air.
In 1722, René Antoine Ferchault de Réaumur demonstrated that iron was transformed into steel through the absorption of some substance, now known to be carbon. In 1772, Antoine Lavoisier showed that diamonds are a form of carbon; when he burned samples of charcoal and diamond and found that neither produced any water and that both released the same amount of carbon dioxide per gram.
In 1779, Carl Wilhelm Scheele showed that graphite, which had been thought of as a form of lead, was instead identical with charcoal but with a small admixture of iron, and that it gave "aerial acid" (his name for carbon dioxide) when oxidized with nitric acid. In 1786, the French scientists Claude Louis Berthollet, Gaspard Monge and C. A. Vandermonde confirmed that graphite was mostly carbon by oxidizing it in oxygen in much the same way Lavoisier had done with diamond. Some iron again was left, which the French scientists thought was necessary to the graphite structure. In their publication they proposed the name "carbone" (Latin "carbonum") for the element in graphite which was given off as a gas upon burning graphite. Antoine Lavoisier then listed carbon as an element in his 1789 textbook.
A new allotrope of carbon, fullerene, that was discovered in 1985 includes nanostructured forms such as buckyballs and nanotubes. Their discoverers – Robert Curl, Harold Kroto and Richard Smalley – received the Nobel Prize in Chemistry in 1996. The resulting renewed interest in new forms lead to the discovery of further exotic allotropes, including glassy carbon, and the realization that "amorphous carbon" is not strictly amorphous. 
Production.
Graphite.
Commercially viable natural deposits of graphite occur in many parts of the world, but the most important sources economically are in China, India, Brazil and North Korea. Graphite deposits are of metamorphic origin, found in association with quartz, mica and feldspars in schists, gneisses and metamorphosed sandstones and limestone as lenses or veins, sometimes of a metre or more in thickness. Deposits of graphite in Borrowdale, Cumberland, England were at first of sufficient size and purity that, until the 19th century, pencils were made simply by sawing blocks of natural graphite into strips before encasing the strips in wood. Today, smaller deposits of graphite are obtained by crushing the parent rock and floating the lighter graphite out on water.
There are three types of natural graphite—amorphous, flake or crystalline flake, and vein or lump. Amorphous graphite is the lowest quality and most abundant. Contrary to science, in industry "amorphous" refers to very small crystal size rather than complete lack of crystal structure. Amorphous is used for lower value graphite products and is the lowest priced graphite. Large amorphous graphite deposits are found in China, Europe, Mexico and the United States. Flake graphite is less common and of higher quality than amorphous; it occurs as separate plates that crystallized in metamorphic rock. Flake graphite can be four times the price of amorphous. Good quality flakes can be processed into expandable graphite for many uses, such as flame retardants. The foremost deposits are found in Austria, Brazil, Canada, China, Germany and Madagascar. Vein or lump graphite is the rarest, most valuable, and highest quality type of natural graphite. It occurs in veins along intrusive contacts in solid lumps, and it is only commercially mined in Sri Lanka.
According to the USGS, world production of natural graphite was 1.1 million tonnes in 2010, to which China contributed 800,000 t, India 130,000 t, Brazil 76,000 t, North Korea 30,000 t and Canada 25,000 t. No natural graphite was reported mined in the United States, but 118,000 t of synthetic graphite with an estimated value of $998 million was produced in 2009.
Diamond.
The diamond supply chain is controlled by a limited number of powerful businesses, and is also highly concentrated in a small number of locations around the world (see figure).
Only a very small fraction of the diamond ore consists of actual diamonds. The ore is crushed, during which care has to be taken in order to prevent larger diamonds from being destroyed in this process and subsequently the particles are sorted by density. Today, diamonds are located in the diamond-rich density fraction with the help of X-ray fluorescence, after which the final sorting steps are done by hand. Before the use of X-rays became commonplace, the separation was done with grease belts; diamonds have a stronger tendency to stick to grease than the other minerals in the ore.
Historically diamonds were known to be found only in alluvial deposits in southern India. India led the world in diamond production from the time of their discovery in approximately the 9th century BCE to the mid-18th century AD, but the commercial potential of these sources had been exhausted by the late 18th century and at that time India was eclipsed by Brazil where the first non-Indian diamonds were found in 1725.
Diamond production of primary deposits (kimberlites and lamproites) only started in the 1870s after the discovery of the Diamond fields in South Africa. Production has increased over time and now an accumulated total of 4.5 billion carats have been mined since that date. About 20% of that amount has been mined in the last 5 years alone, and during the last ten years 9 new mines have started production while 4 more are waiting to be opened soon. Most of these mines are located in Canada, Zimbabwe, Angola, and one in Russia.
In the United States, diamonds have been found in Arkansas, Colorado and Montana. In 2004, a startling discovery of a microscopic diamond in the United States led to the January 2008 bulk-sampling of kimberlite pipes in a remote part of Montana.
Today, most commercially viable diamond deposits are in Russia, Botswana, Australia and the Democratic Republic of Congo. In 2005, Russia produced almost one-fifth of the global diamond output, reports the British Geological Survey. Australia has the richest diamantiferous pipe with production reaching peak levels of per year in the 1990s. There are also commercial deposits being actively mined in the Northwest Territories of Canada, Siberia (mostly in Yakutia territory; for example, Mir pipe and Udachnaya pipe), Brazil, and in Northern and Western Australia.
Applications.
Carbon is essential to all known living systems, and without it life as we know it could not exist (see alternative biochemistry). The major economic use of carbon other than food and wood is in the form of hydrocarbons, most notably the fossil fuel methane gas and crude oil (petroleum). Crude oil is distilled in refineries by the petrochemical industry to produce gasoline, kerosene, and other products. Cellulose is a natural, carbon-containing polymer produced by plants in the form of wood, cotton, linen, and hemp. Cellulose is used primarily for maintaining structure in plants. Commercially valuable carbon polymers of animal origin include wool, cashmere and silk. Plastics are made from synthetic carbon polymers, often with oxygen and nitrogen atoms included at regular intervals in the main polymer chain. The raw materials for many of these synthetic substances come from crude oil.
The uses of carbon and its compounds are extremely varied. It can form alloys with iron, of which the most common is carbon steel. Graphite is combined with clays to form the 'lead' used in pencils used for writing and drawing. It is also used as a lubricant and a pigment, as a molding material in glass manufacture, in electrodes for dry batteries and in electroplating and electroforming, in brushes for electric motors and as a neutron moderator in nuclear reactors.
Charcoal is used as a drawing material in artwork, barbecue grilling, iron smelting, and in many other applications. Wood, coal and oil are used as fuel for production of energy and heating. Gem quality diamond is used in jewelry, and industrial diamonds are used in drilling, cutting and polishing tools for machining metals and stone. Plastics are made from fossil hydrocarbons, and carbon fiber, made by pyrolysis of synthetic polyester fibers is used to reinforce plastics to form advanced, lightweight composite materials.
Carbon fiber is made by pyrolysis of extruded and stretched filaments of polyacrylonitrile (PAN) and other organic substances. The crystallographic structure and mechanical properties of the fiber depend on the type of starting material, and on the subsequent processing. Carbon fibers made from PAN have structure resembling narrow filaments of graphite, but thermal processing may re-order the structure into a continuous rolled sheet. The result is fibers with higher specific tensile strength than steel.
Carbon black is used as the black pigment in printing ink, artist's oil paint and water colours, carbon paper, automotive finishes, India ink and laser printer toner. Carbon black is also used as a filler in rubber products such as tyres and in plastic compounds. Activated charcoal is used as an absorbent and adsorbent in filter material in applications as diverse as gas masks, water purification, and kitchen extractor hoods, and in medicine to absorb toxins, poisons, or gases from the digestive system. Carbon is used in chemical reduction at high temperatures. Coke is used to reduce iron ore into iron (smelting). Case hardening of steel is achieved by heating finished steel components in carbon powder. Carbides of silicon, tungsten, boron and titanium, are among the hardest known materials, and are used as abrasives in cutting and grinding tools. Carbon compounds make up most of the materials used in clothing, such as natural and synthetic textiles and leather, and almost all of the interior surfaces in the built environment other than glass, stone and metal.
Diamonds.
The diamond industry falls into two categories: one dealing with gem-grade diamonds and the other, with industrial-grade diamonds. While a large trade in both types of diamonds exists, the two markets act in dramatically different ways.
Unlike precious metals such as gold or platinum, gem diamonds do not trade as a commodity: there is a substantial mark-up in the sale of diamonds, and there is not a very active market for resale of diamonds.
Industrial diamonds are valued mostly for their hardness and heat conductivity, with the gemological qualities of clarity and color being mostly irrelevant. About 80% of mined diamonds (equal to about 100 million carats or 20 tonnes annually) are unsuitable for use as gemstones are relegated for industrial use (known as "bort)". synthetic diamonds, invented in the 1950s, found almost immediate industrial applications; 3 billion carats (600 tonnes) of synthetic diamond is produced annually.
The dominant industrial use of diamond is in cutting, drilling, grinding, and polishing. Most of these applications do not require large diamonds; in fact, most diamonds of gem-quality except for their small size can be used industrially. Diamonds are embedded in drill tips or saw blades, or ground into a powder for use in grinding and polishing applications. Specialized applications include use in laboratories as containment for high pressure experiments (see diamond anvil cell), high-performance bearings, and limited use in specialized windows. With the continuing advances in the production of synthetic diamonds, new applications are becoming feasible. Garnering much excitement is the possible use of diamond as a semiconductor suitable for microchips, and because of its exceptional heat conductance property, as a heat sink in electronics.
Precautions.
Pure carbon has extremely low toxicity to humans and can be handled and even ingested safely in the form of graphite or charcoal. It is resistant to dissolution or chemical attack, even in the acidic contents of the digestive tract. Consequently, once it enters into the body's tissues it is likely to remain there indefinitely. Carbon black was probably one of the first pigments to be used for tattooing, and Ötzi the Iceman was found to have carbon tattoos that survived during his life and for 5200 years after his death. Inhalation of coal dust or soot (carbon black) in large quantities can be dangerous, irritating lung tissues and causing the congestive lung disease, coalworker's pneumoconiosis. Diamond dust used as an abrasive can harmful if ingested or inhaled. Microparticles of carbon are produced in diesel engine exhaust fumes, and may accumulate in the lungs. In these examples, the harm may result from contaminants (e.g., organic chemicals, heavy metals) rather than from the carbon itself.
Carbon generally has low toxicity to life on Earth; but carbon nanoparticles are deadly to "Drosophila".
Carbon may burn vigorously and brightly in the presence of air at high temperatures. Large accumulations of coal, which have remained inert for hundreds of millions of years in the absence of oxygen, may spontaneously combust when exposed to air in coal mine waste tips, ship cargo holds and coal bunkers, and storage dumps.
In nuclear applications where graphite is used as a neutron moderator, accumulation of Wigner energy followed by a sudden, spontaneous release may occur. Annealing to at least 250 °C can release the energy safely, although in the Windscale fire the procedure went wrong, causing other reactor materials to combust.
The great variety of carbon compounds include such lethal poisons as tetrodotoxin, the lectin ricin from seeds of the castor oil plant "Ricinus communis", cyanide (CN), and carbon monoxide; and such essentials to life as glucose and protein.

</doc>
