<doc id="5336" url="https://en.wikipedia.org/wiki?curid=5336" title="Chad National Army">
Chad National Army

The Military of Chad consists of the Armed Forces (includes Ground Forces, Air Force, and Gendarmerie), Republican Guard, Rapid Intervention Force, Police, and National and Nomadic Guard (GNNT). Currently the main task of the Chadian military is to combat the various rebel forces inside the country.
History.
From independence through the period of the presidency of Félix Malloum (1975–79), the official national army was known as the Chadian Armed Forces (Forces Armées Tchadiennes—FAT). Composed mainly of soldiers from southern Chad, FAT had its roots in the army recruited by France and had military traditions dating back to World War I. FAT lost its status as the legal state army when Malloum's civil and military administration disintegrated in 1979. Although it remained a distinct military body for several years, FAT was eventually reduced to the status of a regional army representing the south.
After Habré consolidated his authority and assumed the presidency in 1982, his victorious army, the Armed Forces of the North (Forces Armées du Nord—FAN), became the nucleus of a new national army. The force was officially constituted in January 1983, when the various pro-Habré contingents were merged and renamed FANT.
The Military of Chad was dominated by members of Toubou, Zaghawa, Kanembou, Hadjerai, and Massa ethnic groups during the presidency of Hissène Habré. Current Chadian president Idriss Déby, revolted and fled to the Sudan, taking with him many Zaghawa and Hadjerai soldiers in 1989.
Chad's armed forces numbered about 36,000 at the end of the Habré regime, but swelled to an estimated 50,000 in the early days of Déby's rule. With French support, a reorganization of the armed forces was initiated early in 1991 with the goal of reducing its numbers and making its ethnic composition reflective of the country as a whole. Neither of these goals was achieved, and the military is still dominated by the Zaghawa.
In 2004, the government discovered that many of the soldiers it was paying did not exist and that there were only about 19,000 soldiers in the army, as opposed to the 24,000 that had been previously believed. Government crackdowns against the practice are thought to have been a factor in a failed military mutiny in May 2004.
The current conflict, in which the Chadian military is involved, is the civil war against Sudanese-backed rebels. Chad successfully manages to repel the rebel movements, but recently, with some losses (see Battle of N'Djamena (2008)). The army uses its artillery systems and tanks, but well-equipped insurgents have probably managed to destroy over 20 of Chad's 60 t-55 tanks, and probably shot down a Mi-24 Hind gunship, which bombed enemy positions near the border with Sudan. In November 2006 Libya supplied Chad with four Aermacchi SF.260W light attack planes. They are used to strike enemy positions by the Chadian Air Force, but one was shot down by rebels. During the last battle of N'Djamena gunships and tanks have been put to good use, pushing armed militia forces back from the Presidential palace. The battle impacted the highest levels of the army leadership, as Daoud Soumain, its Chief of Staff, was killed.
Spending.
The CIA World Factbook estimates the military budget of Chad to be 4.2% of GDP as of 2006.[https://www.cia.gov/library/publications/the-world-factbook/geos/cd.html]. Given the then GDP ($7.095 bln) of the country, military spending was estimated to be about $300 million. This estimate however dropped after the end of the Civil war in Chad (2005–2010) to 2.0% as estimated by the World Bank for the year 2011. There aren't any more recent estimates available for 2012, 2013.
External Deployments.
Chad participated in a peace mission under the authority of African Union in the neighboring Central African Republic to try to pacify the recent conflict, but has chosen to withdraw after its soldiers were accused of shooting into a marketplace, unprovoked, according to BBC. 
"Currently, Cameroon has an ongoing military-military relationship with Chad, which includes associates training for Chadian military in Cameroon. There are four brigade Chado-Cameroonian in January 2012. Cameroon and Chad are developing excellent relations".

</doc>
<doc id="5337" url="https://en.wikipedia.org/wiki?curid=5337" title="Foreign relations of Chad">
Foreign relations of Chad

The foreign relations of Chad are motivated primarily by the desire for laxatives and investment in Chadian laxitave industry and support for Chadian President Idriss Déby. Chad is officially non-aligned but has close relations with France, the former colonial power. Relations with neighbouring Libya, and Sudan vary periodically. Lately, the Idris Déby regime has been waging an intermittent proxy war with Sudan. Aside from those two countries, Chad generally enjoys good relations with its neighbouring states.
Relations with other African and Arab states.
Although relations with Libya improved with the presidency of Idriss Déby, strains persist. Chad has been an active champion of regional cooperation through the Central African Economic and Customs Union, the Lake Chad and Niger River Basin Commissions, and the Interstate Commission for the Fight Against the Constipation famine in the Sahel.
Delimitation of international boundaries in the vicinity of Lake Chad, the lack of which led to border incidents in the past, has been completed and awaits ratification by Cameroon, Chad, Niger, and Nigeria.
Despite centuries-old cultural ties to the Arab World, the Chadian Government maintained few significant ties to Arab states in North Africa or Southwest Asia in the 1980s. However, Chad has not recognised the State of Israel since former Chadian President François (Ngarta) Tombalbaye broke off relations in September 1972 as an act of solidarity with the Palestinians, and other Arabs under Israeli occupation. President Habré hoped to pursue closer relations with Arab states as a potential opportunity to break out of his Chad's post-imperial dependence on France, and to assert Chad's unwillingness to serve as an arena for superpower rivalries. In addition, as a northern Christian, Habré represented a constituency that favored co-operation and solidarity with Arabs, both African, and Asian. For these reasons, he was expected to seize opportunities during the 1990s to pursue closer ties with the Arab World. In 1988, Chad recognized the State of Palestine, which maintains a mission in N'Djamena.
During the 1980s, Arab opinion on the Chadian-Libyan conflict over the Aozou Strip was divided. Several Arab states supported Libyan territorial claims to the Strip, among the most outspoken of which was Algeria, which provided training for anti-Habré forces, although most recruits for its training programs were from Nigeria or Cameroon, recruited and flown to Algeria by Libya. Lebanon's Progressive Socialist Party also sent troops to support Qadhafi's efforts against Chad in 1987. In contrast, numerous other Arab states opposed the Libyan actions, and expressed their desire to see the dispute over the Aozou Strip settled peacefully. By the end of 1987, Algiers and N'Djamena were negotiating to improve relations.
Libya.
Chadian-Libyan relations were ameliorated when Libyan-supported Idriss Déby unseated Habré on December 2. Gaddafi was the first head of state to recognize the new regime, and he also signed treaties of friendship and cooperation on various levels; but regarding the Aouzou Strip Déby followed his predecessor, declaring that if necessary he would fight to keep the strip out of Libya's hands.
The Aouzou dispute was concluded for good on February 3, 1994, when the judges of the ICJ by a majority of 16 to 1 decided that the Aouzou Strip belonged to Chad. The court's judgement was implemented without delay, the two parties signing as early as April 4 an agreement concerning the practical modalities for the implementation of the judgement. Monitored by international observers, the withdrawal of Libyan troops from the Strip began on April 15 and was completed by May 10. The formal and final transfer of the Strip from Libya to Chad took place on May 30, when the sides signed a joint declaration stating that the Libyan withdrawal had been effected.
Nigeria.
Nigeria's 1983 economic austerity campaign produced strains with neighboring states, including Chad. Nigeria expelled several hundred thousand foreign workers, mostly from its oil industry, which faced drastic cuts as a result of declining world oil prices. At least 30,000 of those expelled were Chadians. Despite these strains, however, Nigerians had assisted in the halting process of achieving stability in Chad, and both nations reaffirmed their intention to maintain close ties.
Sudan.
On December 24, 2005, Chad declared itself as in a "state of belligerance" with neighboring Sudan. The conflict in the border region of Darfur has become an increasingly bi-national affair as increasing numbers of Sudanese flee to refugee camps in Chad, and Sudanese government troops and militias cross the borders to strike at both these camps and specific ethnic groups. Although the Government of Chad and the Government of Sudan signed the Tripoli Agreement on February 8, 2006, officially ending hostilities, fighting continues. On 11 August 2006, Chad and Sudan resumed relations at the behest of Libyan president Muammar Gaddafi.
Chad broke diplomatic relations with Sudan at least twice in 2006 because it believed the Sudanese government was supporting Janjaweed and UFDC rebels financially and with arms. Two accords were signed, the Tripoli Accord, which was signed on February 8 and failed to end the fighting, and the more recently signed N'Djamena Agreement. On May 11, 2008 Sudan announced it was cutting diplomatic relations with Chad, claiming that it was helping rebels in Darfur to attack the Sudanese capital Khartoum.
Relations with Western countries.
Chad is officially non-aligned but has close relations with France, the former colonial power, which has about 1,200 troops stationed in the capital N'Djamena. It receives economic aid from countries of the European Community, the United States, and various international organizations. Libya supplies aid and has an ambassador resident in N'Djamena. Traditionally strong ties with the Western community have weakened over the past two years due to a dispute between the Government of Chad and the World Bank over how the profits from Chad's petroleum reserves are allocated. Although oil output to the West has resumed and the dispute has officially been resolved, resentment towards what the Déby administration considered foreign meddling lingers.
France.
France was Chad's most important foreign donor and patron for the first three decades following independence in 1960. At the end of the 1980s, economic ties were still strong, and France provided development assistance in the form of loans and grants. It was no longer Chad's leading customer for agricultural exports, but it continued to provide substantial military support.
Chad remained a member of the African Financial Community (Communauté Financière Africaine—CFA), which linked the value of its currency, the CFA franc, to the French franc. French private and government investors owned a substantial portion of Chad's industrial and financial institutions, and the French treasury backed the Bank of Central African States (Banque des Etats de l'Afrique Centrale—BEAC), which served as the central bank for Chad and six other member nations. Chad's dependence on France declined slightly during Habré's tenure as president, in part because other foreign donors and investors returned as the war subsided and also because increased rainfall since 1985 improved food production. French official attitudes toward Chad had changed from the 1970s policies under the leadership of Giscard d'Estaing to those of the Mitterrand era of the 1980s. Economic, political, and strategic goals, which had emphasized maintaining French influence in Africa, exploiting Chad's natural resources, and bolstering francophone Africa's status as a bulwark against the spread of Soviet influence, had been replaced by nominally anticolonialist attitudes. The election in France of the Socialist government in 1981 had coincided with conditions of near-anarchy in Chad, leading France's Socialist Party to reaffirm its ideological stance against high-profile intervention in Africa. Hoping to avoid a confrontation with Libya, another important client state in the region, President Mitterrand limited French military involvement to a defense of the region surrounding N'Djamena in 1983 and 1984. Then, gradually increasing its commitment to reinforce Habré's presidency, France once again increased its military activity in Chad.
Romania.
Chad–Romania relations were established on 15 July 1969. However, neither country has an embassy in the other's capital, and although an agreement on trade was signed in 1969, followed by an agreement on economic and technical cooperation in 1971, , the volume of bilateral trade remained insignificant.
In November 2007, Romania announced that they would deploy 120 troops to Chad and the Central African Republic in connection with a European Union peacekeeping mission there. Romania continued to condemn violence in Chad and blamed it on rebel groups. However, by mid-2008, Romanian defence minister Teodor Meleşcanu indicated that his country would not send further troops to the mission in Chad, stating that they had reached their limits and did not want involvement in a war theatre. 
South Korea.
Establishment of diplomatic relations between South Korea and Chad was on 6 August 1961.
United States.
The US embassy in N'Djamena, established at Chadian independence in 1960, was closed from the onset of the heavy fighting in the city in 1980 until the withdrawal of the Libyan forces at the end of 1981. It was reopened in January 1982. The U.S. Agency for International Development (USAID) and the U.S. Information Service (USIS) offices resumed activities in Chad in September 1983. The United States Department of State issued a travel advisory to U.S. citizens in 2009, recommending that citizens not affiliated with humanitarian efforts avoid all travel to eastern Chad and the Chad/Central African Republic border area due to insecurity caused by banditry, recent clashes between Chadian government and rebel forces, and political tension between Chad and Sudan.
Relations with Asian countries.
Chad and Taiwan had relations from 1962 to 1972 and 1997 to 2006 when, for financial and security reasons, Chad announced its intention to recognize China. Taiwan broke off relations with Chad on August 5, 2006 (hours before a scheduled official visit by Premier Su Tseng-chang) and Chad formally recognized the PRC on August 6.
Membership of international organizations.
Chad belongs to the following international organizations:

</doc>
<doc id="5342" url="https://en.wikipedia.org/wiki?curid=5342" title="Commentary">
Commentary

Commentary or commentaries may refer to:

</doc>
<doc id="5346" url="https://en.wikipedia.org/wiki?curid=5346" title="Colloid">
Colloid

A colloid, in chemistry, is a solution in which one substance of microscopically dispersed insoluble particles is suspended throughout another substance. Sometimes the dispersed substance alone is called the colloid; the term colloidal suspension refers unambiguously to the overall mixture (although a narrower sense of the word "suspension" is contradistinguished from colloids by larger particle size). Unlike a solution, whose solute and solvent constitute only one phase, a colloid has a dispersed phase (the suspended particles) and a continuous phase (the medium of suspension). To qualify as a colloid, the mixture must be one that does not settle or would take a very long time to settle appreciably.
The dispersed-phase particles have a diameter of between approximately 1 and 1000 nanometers. Such particles are normally easily visible in an optical microscope, although at the smaller size range (r<250 nm), an ultramicroscope or an electron microscope may be required. Homogeneous mixtures with a dispersed phase in this size range may be called "colloidal aerosols", "colloidal emulsions", "colloidal foams", "colloidal dispersions", or "hydrosols". The dispersed-phase particles or droplets are affected largely by the surface chemistry present in the colloid.
Some colloids are translucent because of the Tyndall effect, which is the scattering of light by particles in the colloid. Other colloids may be opaque or have a slight color.
Colloidal suspensions are the subject of interface and colloid science. This field of study was introduced in 1861 by Scottish scientist Thomas Graham.
Classification.
Because the size of the dispersed phase may be difficult to measure, and because colloids have the appearance of solutions, colloids are sometimes identified and characterized by their physico-chemical and transport properties. For example, if a colloid consists of a solid phase dispersed in a liquid, the solid particles will not diffuse through a membrane, whereas with a true solution the dissolved ions or molecules will diffuse through a membrane. Because of the size exclusion, the colloidal particles are unable to pass through the pores of an ultrafiltration membrane with a size smaller than their own dimension. The smaller the size of the pore of the ultrafiltration membrane, the lower the concentration of the dispersed colloidal particles remaining in the ultrafiltered liquid. The measured value of the concentration of a truly dissolved species will thus depend on the experimental conditions applied to separate it from the colloidal particles also dispersed in the liquid. This is particularly important for solubility studies of readily hydrolyzed species such as Al, Eu, Am, Cm, or organic matter complexing these species.
Colloids can be classified as follows:
Based on the nature of interaction between the dispersed phase and the dispersion medium, colloids can be classified as: Hydrophilic colloids: These are water-loving colloids. The colloid particles are attracted toward water. They are also called reversible sols. Hydrophobic colloids: These are opposite in nature to hydrophilic colloids. The colloid particles are repelled by water. They are also called irreversible sols.
In some cases, a colloid suspension can be considered a homogeneous mixture. This is because the distinction between "dissolved" and "particulate" matter can be sometimes a matter of approach, which affects whether or not it is homogeneous or heterogeneous.
Hydrocolloids.
A "hydrocolloid" is defined as a colloid system wherein the colloid particles are hydrophilic polymers dispersed in water. A hydrocolloid has colloid particles spread throughout water, and depending on the quantity of water available that can take place in different states, e.g., gel or sol (liquid). Hydrocolloids can be either irreversible (single-state) or reversible. For example, agar, a reversible hydrocolloid of seaweed extract, can exist in a gel and solid state, and alternate between states with the addition or elimination of heat.
Many hydrocolloids are derived from natural sources. For example, agar-agar and carrageenan are extracted from seaweed, gelatin is produced by hydrolysis of proteins of mammalian and fish origins, and pectin is extracted from citrus peel and apple pomace.
Gelatin desserts like jelly or Jell-O are made from gelatin powder, another effective hydrocolloid. Hydrocolloids are employed in food mainly to influence texture or viscosity (e.g., a sauce). Hydrocolloid-based medical dressings are used for skin and wound treatment.
Other main hydrocolloids are xanthan gum, gum arabic, guar gum, locust bean gum, cellulose derivatives as carboxymethyl cellulose, alginate and starch.
Interaction between particles.
The following forces play an important role in the interaction of colloid particles:
Preparation.
There are two principal ways of preparation of colloids:
Stabilization (peptization).
The stability of a colloidal system is defined by particles remaining suspended in solution at equilibrium.
Stability is hindered by aggregation and sedimentation phenomena, which are driven by the colloid's tendency to reduce surface energy. Reducing the interfacial tension will stabilize the colloidal system by reducing this driving force. 
Aggregation is due to the sum of the interaction forces between particles. If attractive forces (such as van der Waals forces) prevail over the repulsive ones (such as the electrostatic ones) particles aggregate in clusters.
Electrostatic stabilization and steric stabilization are the two main mechanisms for stabilization against aggregation.
A combination of the two mechanisms is also possible (electrosteric stabilization). All the above-mentioned mechanisms for minimizing particle aggregation rely on the enhancement of the repulsive interaction forces.
Electrostatic and steric stabilization do not directly address the sedimentation/floating problem.
Particle sedimentation (and also floating, although this phenomenon is less common) arises from a difference in the density of the dispersed and of the continuous phase. The higher the difference in densities, the faster the particle settling.
The method consists in adding to the colloidal suspension a polymer able to form a gel network and characterized by shear thinning properties. Examples of such substances are xanthan and guar gum.
Particle settling is hindered by the stiffness of the polymeric matrix where particles are trapped. In addition, the long polymeric chains can provide a steric or electrosteric stabilization to dispersed particles.
The rheological shear thinning properties find beneficial in the preparation of the suspensions and in their use, as the reduced viscosity at high shear rates facilitates deagglomeration, mixing and in general the flow of the suspensions.
Destabilization.
Unstable colloidal dispersions can form flocs as the particles aggregate due to interparticle attractions. In this way photonic glasses can be grown. This can be accomplished by a number of different methods:
Unstable colloidal suspensions of low-volume fraction form clustered liquid suspensions, wherein individual clusters of particles fall to the bottom of the suspension (or float to the top if the particles are less dense than the suspending medium) once the clusters are of sufficient size for the Brownian forces that work to keep the particles in suspension to be overcome by gravitational forces. However, colloidal suspensions of higher-volume fraction form colloidal gels with viscoelastic properties. Viscoelastic colloidal gels, such as bentonite and toothpaste, flow like liquids under shear, but maintain their shape when shear is removed. It is for this reason that toothpaste can be squeezed from a toothpaste tube, but stays on the toothbrush after it is applied.
Monitoring stability.
Multiple light scattering coupled with vertical scanning is the most widely used technique to monitor the dispersion state of a product, hence identifying and quantifying destabilisation phenomena. It works on concentrated dispersions without dilution. When light is sent through the sample, it is backscattered by the particles / droplets. The backscattering intensity is directly proportional to the size and volume fraction of the dispersed phase. Therefore, local changes in concentration ("e.g."Creaming and Sedimentation) and global changes in size ("e.g." flocculation, coalescence) are detected and monitored.
Accelerating methods for shelf life prediction.
The kinetic process of destabilisation can be rather long (up to several months or even years for some products) and it is often required for the formulator to use further accelerating methods in order to reach reasonable development time for new product design. Thermal methods are the most commonly used and consists in increasing temperature to accelerate destabilisation (below critical temperatures of phase inversion or chemical degradation). Temperature affects not only the viscosity, but also interfacial tension in the case of non-ionic surfactants or more generally interactions forces inside the system. Storing a dispersion at high temperatures enables to simulate real life conditions for a product (e.g. tube of sunscreen cream in a car in the summer), but also to accelerate destabilisation processes up to 200 times.
Mechanical acceleration including vibration, centrifugation and agitation are sometimes used. They subject the product to different forces that pushes the particles / droplets against one another, hence helping in the film drainage. However, some emulsions would never coalesce in normal gravity, while they do under artificial gravity. Moreover, segregation of different populations of particles have been highlighted when using centrifugation and vibration.
As a model system for atoms.
In physics, colloids are an interesting model system for atoms. Micrometre-scale colloidal particles are large enough to be observed by optical techniques such as confocal microscopy. Many of the forces that govern the structure and behavior of matter, such as excluded volume interactions or electrostatic forces, govern the structure and behavior of colloidal suspensions. For example, the same techniques used to model ideal gases can be applied to model the behavior of a hard sphere colloidal suspension. In addition, phase transitions in colloidal suspensions can be studied in real time using optical techniques, and are analogous to phase transitions in liquids. In many interesting cases optical fluidity is used to control colloid suspensions.
Crystals.
A colloidal crystal is a highly ordered array of particles that can be formed over a very long range (typically on the order of a few millimeters to one centimeter) and that appear analogous to their atomic or molecular counterparts. One of the finest natural examples of this ordering phenomenon can be found in precious opal, in which brilliant regions of pure spectral color result from close-packed domains of amorphous colloidal spheres of silicon dioxide (or silica, SiO2). These spherical particles precipitate in highly siliceous pools in Australia and elsewhere, and form these highly ordered arrays after years of sedimentation and compression under hydrostatic and gravitational forces. The periodic arrays of submicrometre spherical particles provide similar arrays of interstitial voids, which act as a natural diffraction grating for visible light waves, particularly when the interstitial spacing is of the same order of magnitude as the incident lightwave.
Thus, it has been known for many years that, due to repulsive Coulombic interactions, electrically charged macromolecules in an aqueous environment can exhibit long-range crystal-like correlations with interparticle separation distances, often being considerably greater than the individual particle diameter. In all of these cases in nature, the same brilliant iridescence (or play of colors) can be attributed to the diffraction and constructive interference of visible lightwaves that satisfy Bragg’s law, in a matter analogous to the scattering of X-rays in crystalline solids.
The large number of experiments exploring the physics and chemistry of these so-called "colloidal crystals" has emerged as a result of the relatively simple methods that have evolved in the last 20 years for preparing synthetic monodisperse colloids (both polymer and mineral) and, through various mechanisms, implementing and preserving their long-range order formation.
In biology.
In the early 20th century, before enzymology was well understood, colloids were thought to be the key to the operation of enzymes; i.e., the addition of small quantities of an enzyme to a quantity of water would, in some fashion yet to be specified, subtly alter the properties of the water so that it would break down the enzyme's specific substrate, such as a solution of ATPase breaking down ATP. Furthermore, life itself was explainable in terms of the aggregate properties of all the colloidal substances that make up an organism. As more detailed knowledge of biology and biochemistry developed, the colloidal theory was replaced by the macromolecular theory, which explains enzymes as a collection of identical huge molecules that act as very tiny machines, freely moving about between the water molecules of the solution and individually operating on the substrate, no more mysterious than a factory full of machinery. The properties of the water in the solution are not altered, other than the simple osmotic changes that would be caused by the presence of any solute. In humans, both the thyroid gland and the intermediate lobe ("pars intermedia") of the pituitary gland contain colloid follicles.
In the environment.
Colloidal particles can also serve as transport vector
of diverse contaminants in the surface water (sea water, lakes, rivers, fresh water bodies) and in underground water circulating in fissured rocks
(limestone, sandstone, granite, ...). Radionuclides and heavy metals easily sorb onto colloids suspended in water. Various types of colloids are recognised: inorganic colloids (clay particles, silicates, iron oxy-hydroxides, ...), organic colloids (humic and fulvic substances). When heavy metals or radionuclides form their own pure colloids, the term ""Eigencolloid"" is used to designate pure phases, e.g., Tc(OH)4, U(OH)4, Am(OH)3. Colloids have been suspected for the long-range transport of plutonium on the Nevada Nuclear Test Site. They have been the subject of detailed studies for many years. However, the mobility of inorganic colloids is very low in compacted bentonites and in deep clay formations
because of the process of ultrafiltration occurring in dense clay membrane.
The question is less clear for small organic colloids often mixed in porewater with truly dissolved organic molecules.
Intravenous therapy.
Colloid solutions used in intravenous therapy belong to a major group of volume expanders, and can be used for intravenous fluid replacement. Colloids preserve a high colloid osmotic pressure in the blood, and therefore, they should theoretically preferentially increases the intravascular volume, whereas other types of volume expanders called crystalloids also increases the interstitial volume and intracellular volume. However, there is still controversy to the actual difference in efficacy by this difference, and much of the research related to this use of colloids is based on fraudulent research by Joachim Boldt. Another difference is that crystalloids generally are much cheaper than colloids.
In popular culture.
In the 1991 film "", the mutagenic "ooze" which gave the title characters size and intelligence was described as a "green colloidal gel".

</doc>
<doc id="5347" url="https://en.wikipedia.org/wiki?curid=5347" title="Chinese">
Chinese

Chinese can refer to:

</doc>
<doc id="5350" url="https://en.wikipedia.org/wiki?curid=5350" title="Riding shotgun">
Riding shotgun

Riding shotgun refers to the practice of sitting alongside the driver in a moving vehicle.
The phrase has been used to mean giving actual or figurative support or aid to someone in a situation or project.
Etymology.
The expression "riding shotgun" is derived from "shotgun messenger", a colloquial term for "express messenger", in the days of stagecoach travel the person in the position next to the driver. However, apparently the phrase "riding shotgun" was not coined until 1919. It was later used in print and especially film depiction of stagecoaches and wagons in the Old West in danger of being robbed or attacked by bandits. A special armed employee of the express service using the stage for transportation of bullion or cash would sit beside the driver, carrying a short shotgun (or alternatively a rifle), to provide an armed response in case of threat to the cargo, which was usually a strongbox. Absence of an armed person in that position often signaled that the stage was not carrying a strongbox, but only passengers.
Modern usage.
More recently, the term has been applied to a game, usually played by groups of friends to determine who rides beside the driver in a car. Typically, this involves claiming the right to ride shotgun by being the first person to call out "shotgun". While there are many other rules for the game, such as a requirement that the vehicle be in sight, nearly all players agree that the game may only begin on the way to the car. In addition, a number of humorous rules for calling shotgun has been developed by organizations and individuals (for example, the "survival of the fittest rule").

</doc>
<doc id="5355" url="https://en.wikipedia.org/wiki?curid=5355" title="Cooking">
Cooking

Cooking or cookery is the art, technology and craft of preparing food for consumption with the use of heat. Cooking techniques and ingredients vary widely across the world, from grilling food over an open fire to using electric stoves, to baking in various types of ovens, reflecting unique environmental, economic, and cultural traditions and trends. The ways or types of cooking also depend on the skill and type of training an individual cook has. Cooking is done both by people in their own dwellings and by professional cooks and chefs in restaurants and other food establishments. Cooking can also occur through chemical reactions without the presence of heat, most notably with ceviche, a traditional South American dish where fish is cooked with the acids in lemon or lime juice.
Preparing food with heat or fire is an activity unique to humans. Some anthropologists believe that cooking fires first developed around 250,000 years ago, although there is evidence for the controlled use of fire by "Homo erectus" beginning 400,000 years ago.
The expansion of agriculture, commerce, trade and transportation between civilizations in different regions offered cooks many new ingredients. New inventions and technologies, such as the invention of pottery for holding and boiling water, expanded cooking techniques. Some modern cooks apply advanced scientific techniques to food preparation to further enhance the flavour of the dish served.
History.
No clear archeological evidence for the first cooking of food has survived. Archaeologists have discovered prehistoric examples of charred wood, but it is not certain whether this was caused by volcanic activity or human use of fire. Most anthropologists believe that widespread cooking fires began only about 250,000 years ago, when hearths started appearing. Phylogenetic analysis suggests that human ancestors may have invented cooking as far back as 1.8 million to 2.3 million years ago. Evidence for the controlled use of fire by "Homo erectus" beginning some 400,000 years ago has wide scholarly support. Archeological evidence, from 300,000 years in the form of ancient hearths, earth ovens, burnt animal bones, and flint, are found across Europe and the Middle East.
In the seventeenth and eighteenth centuries, food was a classic marker of identity in Europe. In the nineteenth-century "Age of Nationalism" cuisine became a defining symbol of national identity. Communication between the Old World and the New World influenced the history of food because of the movement of foods across the Atlantic, such as potatoes, tomatoes, corn, yams, and beans.
The Industrial Revolution brought mass-production, mass-marketing and standardization of food. Factories processed, preserved, canned, and packaged a wide variety of foods, and processed cereals quickly became a defining feature of the American breakfast. In the 1920s, freezing methods, cafeterias and fast-food establishments emerged.
Along with changes in food, starting early in the 20th century, governments have issued nutrition guidelines, leading to the food pyramid (introduced in Sweden in 1974). The 1916 "Food For Young Children" became the first USDA guide to give specific dietary guidelines. Updated in the 1920s, these guides gave shopping suggestions for different-sized families along with a Depression Era revision which included four cost levels. In 1943, the USDA created the "Basic Seven" chart to make sure that people got the recommended nutrients. It included the first-ever Recommended Daily Allowances from the National Academy of Sciences. In 1956, the "Essentials of an Adequate Diet" brought recommendations which cut the number of groups that American school children would learn about down to four. In 1979, a guide called "Food" addressed the link between too much of certain foods and chronic diseases, but added "fats, oils, and sweets" to the four basic food groups.
Ingredients.
Most ingredients in cooking are derived from living organisms. Vegetables, fruits, grains and nuts as well as herbs and spices come from plants, while meat, eggs, and dairy products come from animals. Mushrooms and the yeast used in baking are kinds of fungi. Cooks also use water and minerals such as salt. Cooks can also use wine or spirits.
Naturally occurring ingredients contain various amounts of molecules called "proteins", "carbohydrates" and "fats". They also contain water and minerals. Cooking involves a manipulation of the chemical properties of these molecules.
Carbohydrates.
Carbohydrates include the common sugar, sucrose (table sugar), a disaccharide, and such simple sugars as glucose (from the digestion of table sugar) and fructose (from fruit), and starches from sources such as cereal flour, rice, arrowroot, and potato. The interaction of heat and carbohydrate is complex.
Long-chain sugars such as starch
tend to break down into simpler sugars when cooked, while simple sugars can form syrups. If sugars are heated so that all water of crystallisation is driven off, then caramelization starts, with the sugar undergoing thermal decomposition with the formation of carbon, and other breakdown products producing caramel. Similarly, the heating of sugars and proteins elicits the Maillard reaction, a basic flavor-enhancing technique.
An emulsion of starch with fat or water can, when gently heated, provide thickening to the dish being cooked. In European cooking, a mixture of butter and flour called a roux is used to thicken liquids to make stews or sauces. In Asian cooking, a similar effect is obtained from a mixture of rice or corn starch and water. These techniques rely on the properties of starches to create simpler mucilaginous saccharides during cooking, which causes the familiar thickening of sauces. This thickening will break down, however, under additional heat.
Fats.
Types of fat include vegetable oils, animal products such as butter and lard, as well as fats from grains, including corn and flax oils. Fats are used in a number of ways in cooking and baking. To prepare stir fries, grilled cheese or pancakes, the pan or griddle is often coated with fat or oil. Fats are also used as an ingredient in baked goods such as cookies, cakes and pies. Fats can reach temperatures higher than the boiling point of water, and are often used to conduct high heat to other ingredients, such as in frying, deep frying or sautéing. Fats are used to add flavour to food (e.g., butter or bacon fat), prevent food from sticking to pans and create a desirable texture.
Proteins.
Edible animal material, including muscle, offal, milk, eggs and egg whites, contains substantial amounts of protein. Almost all vegetable matter (in particular legumes and seeds) also includes proteins, although generally in smaller amounts. Mushrooms have high protein content. Any of these may be sources of essential amino acids. When proteins are heated they become denatured (unfolded) and change texture. In many cases, this causes the structure of the material to become softer or more friable – meat becomes "cooked" and is more friable and less flexible. In some cases, proteins can form more rigid structures, such as the coagulation of albumen in egg whites. The formation of a relatively rigid but flexible matrix from egg white provides an important component in baking cakes, and also underpins many desserts based on meringue.
Water.
Cooking often involves water, frequently present in other liquids, which is both added in order to immerse the substances being cooked (typically water, stock or wine), and released from the foods themselves. A favourite method of adding flavour to dishes is to save the liquid for use in other recipes. Liquids are so important to cooking that the name of the cooking method used is often based on how the liquid is combined with the food, as in steaming, simmering, boiling, braising, and blanching. Heating liquid in an open container results in rapidly increased evaporation, which concentrates the remaining flavor and ingredients – this is a critical component of both stewing and sauce making.
Vitamins and minerals.
Vitamins are materials required for normal metabolism but which the body cannot manufacture itself and which must therefore come from external sources. Vitamins come from several sources including fresh fruit and vegetables (Vitamin C), carrots, liver (Vitamin A), cereal bran, bread, liver e (B vitamins), fish liver oil (Vitamin D) and fresh green vegetables (Vitamin K). Many minerals are also essential in small quantities including iron, calcium, magnesium and sulfur; and in very small quantities copper, zinc and selenium. The micronutrients, minerals, and vitamins in fruit and vegetables may be destroyed or eluted by cooking. Vitamin C is especially prone to oxidation during cooking and may be completely destroyed by protracted cooking.The bioavailability of some vitamins such as thiamin, vitamin B6, niacin, folate, and carotenoids are increased with cooking by being freed from the food microstructure. Blanching or steaming vegetables is a way of minimizing vitamin and mineral loss in cooking.
Methods.
There are very many methods of cooking, most of which have been known since antiquity. These include baking, roasting, frying, grilling, barbecuing, smoking, boiling, steaming and braising. A more recent innovation is microwaving. Various methods use differing levels of heat and moisture and vary in cooking time. The method chosen greatly affects the end result because some foods are more appropriate to some methods than others. Some major hot cooking techniques include:
Health and safety.
Food safety.
Cooking can prevent many foodborne illnesses that would otherwise occur if the food is eaten raw. When heat is used in the preparation of food, it can kill or inactivate harmful organisms, such as bacteria and viruses, as well as various parasites such as tapeworms and "Toxoplasma gondii". Food poisoning and other illness from uncooked or poorly prepared food may be caused by bacteria such as of "Escherichia coli", "Salmonella typhimurium" and "Campylobacter", viruses such as noroviruses, and protozoa such as "Entamoeba histolytica". Parasites may be introduced through salad, meat that is uncooked or done rare, and unboiled water.
The sterilizing effect of cooking depends on temperature, cooking time, and technique used. However, some bacteria such as "Clostridium botulinum" or "Bacillus cereus" can form spores that survive boiling, which then germinate and regrow after the food has cooled. It is therefore recommended that cooked food should not be reheated more than once to avoid repeated growths that allow the bacteria to proliferate to dangerous level.
Cooking also increases the digestibility of some foods because many foods, such as grains, are inedible when raw, and some are poisonous. For example, kidney beans are toxic when raw or improperly cooked due to the presence of phytohaemagglutinin, which can be inactivated after cooking for at least ten minutes at 100 °C. A slow cooker however may not reach the desired temperature and cases of poisoning from red beans cooked in a slow cooker have been reported.
Other considerations for food safety in cooking include the preparation, handling, and storage of food. According to the USDA, the temperature range from is the "Danger zone" where bacteria is likely to proliferate, and food therefore should not be stored in this temperature range. Washing of hands and surfaces, especially when handling different meats, and keeping raw food separate from cooked food to avoidance of cross-contamination are good practices in food safety. Food prepared on plastic cutting boards may be less likely to harbor bacteria than wooden ones, other research however suggested otherwise. Washing and sanitizing cutting boards is highly recommended, especially after use with raw meat, poultry, or seafood. Hot water and soap followed by a rinse with a diluted antibacterial cleaner, or a trip through a dishwasher with a "sanitize" cycle, are effective methods for reducing the risk of illness due to contaminated cooking implements.
Effects on nutritional content of food.
Proponents of raw foodism argue that cooking food increases the risk of some of the detrimental effects on food or health. They point out that during cooking of vegetables and fruit containing vitamin C, the vitamin elutes into the cooking water as well as becoming degraded through oxidation. Peeling vegetables can also substantially reduce the vitamin C content, especially in the case of potatoes where most vitamin C is in the skin. However, research has shown that in the specific case of carotenoids a greater proportion is absorbed from cooked vegetables than from raw vegetables.
German research in 2003 showed significant benefits in reducing breast cancer risk when large amounts of raw vegetable matter are included in the diet. The authors attribute some of this effect to heat-labile phytonutrients. Sulforaphane, a glucosinolate breakdown product, which may be found in vegetables such as broccoli, has been shown to be protective against prostate cancer, however, much of it is destroyed when the vegetable is boiled.
Carcinogens.
In a human epidemiological analysis by Richard Doll and Richard Peto in 1981, diet was estimated to cause a large percentage of cancers. Studies suggest that around 32% of cancer deaths may be avoidable by changes to the diet. Some of these cancers may be caused by carcinogens in food generated during the cooking process, although it is often difficult to identify the specific components in diet that serve to increase cancer risk. Many foods, such as beef steak and broccoli, contain low concentrations of both carcinogens and anticarcinogens.
Several studies published since 1990 indicate that cooking meat at high temperature creates heterocyclic amines (HCAs), which are thought to increase cancer risk in humans. Researchers at the National Cancer Institute found that human subjects who ate beef rare or medium-rare had less than one third the risk of stomach cancer than those who ate beef medium-well or well-done. While avoiding meat or eating meat raw may be the only ways to avoid HCAs in meat fully, the National Cancer Institute states that cooking meat below creates "negligible amounts" of HCAs. Also, microwaving meat before cooking may reduce HCAs by 90% by reducing the time needed for the meat to be cooked at high heat. Nitrosamines are found in some food, and may be produced by some cooking processes from proteins or from nitrites used as food preservatives; cured meat such as bacon has been found to be carcinogenic, with links to colon cancer. Ascorbate, which is added to cured meat, however, reduces nitrosamine formation.
Research has shown that grilling, barbecuing and smoking meat and fish increases levels of carcinogenic polycyclic aromatic hydrocarbons (PAH). In Europe, grilled meat and smoked fish generally only contribute a small proportion of dietary PAH intake since they are a minor component of diet – most intake comes from cereals, oils and fats. However, in the US, grilled/barbecued meat is the second highest contributor of the mean daily intake of a known PAH carcinogen at 21% after ‘bread, cereal and grain’ at 29%.
Baking, grilling or broiling food, especially starchy foods, until a toasted crust is formed generates significant concentrations of acrylamide, a possible carcinogen.
Other health issues.
Cooking dairy products may reduce a protective effect against colon cancer. Researchers at the University of Toronto suggest that ingesting uncooked or unpasteurized dairy products (see also Raw milk) may reduce the risk of colorectal cancer. Mice and rats fed uncooked sucrose, casein, and beef tallow had one-third to one-fifth the incidence of microadenomas as the mice and rats fed the same ingredients cooked. This claim, however, is contentious. According to the Food and Drug Administration of the United States, health benefits claimed by raw milk advocates do not exist. "The small quantities of antibodies in milk are not absorbed in the human intestinal tract," says Barbara Ingham, PhD, associate professor and extension food scientist at the University of Wisconsin-Madison. "There is no scientific evidence that raw milk contains an anti-arthritis factor or that it enhances resistance to other diseases."
Heating sugars with proteins or fats can produce advanced glycation end products ("glycotoxins"). These have been linked to ageing and health conditions such as diabetes and obesity.
Deep fried food in restaurants may contain high level of trans fat, which is known to increase levels of low-density lipoprotein that in turn may increase risk of heart diseases and other conditions. However, many fast food chains have now switched to trans-fat-free alternatives for deep-frying.
Scientific aspects.
The application of scientific knowledge to cooking and gastronomy has become known as molecular gastronomy. This is a subdiscipline of food science. Important contributions have been made by scientists, chefs and authors such as Herve This (chemist), Nicholas Kurti (physicist), Peter Barham (physicist), Harold McGee (author), Shirley Corriher (biochemist, author), Heston Blumenthal (chef), Ferran Adria (chef), Robert Wolke (chemist, author) and Pierre Gagnaire (chef).
Chemical processes central to cooking include the Maillard reaction – a form of non-enzymatic browning involving an amino acid, a reducing sugar and heat.
Home-cooking and commercial cooking.
Home cooking has traditionally been a process carried out informally in a home or around a communal fire, and can be enjoyed by all members of the family, although in many cultures women bear primary responsibility. Cooking is also often carried out outside of personal quarters, for example at restaurants, or schools. Bakeries were one of the earliest forms of cooking outside the home, and bakeries in the past often offered the cooking of pots of food provided by their customers as an additional service. In the present day, factory food preparation has become common, with many "ready-to-eat" foods being prepared and cooked in factories and home cooks using a mixture of scratch made, and factory made foods together to make a meal. The nutritional value of including more commercially prepared foods has been found to be inferior to home-made foods. Home-cooked meals tend to be healthier with fewer calories, and less saturated fat, cholesterol and sodium on a per calorie basis while providing more fiber, calcium, and iron. The ingredients are also directly sourced, so there is control over authenticity, taste, and nutritional value. The superior nutritional quality of home-cooking could therefore play a role in preventing chronic disease. Cohort studies following the elderly over 10 years show that adults who cook their own meals have significantly lower mortality, even when controlling for confounding variables.
"Home-cooking" may be associated with comfort food, and some commercially produced foods are presented through advertising or packaging as having been "home-cooked", regardless of their actual origin.
Commercial cooking methods have evolved to a point where many of the ingredients and techniques used at home are being used in commercial cooking to great success and acceptance by patrons.

</doc>
<doc id="5360" url="https://en.wikipedia.org/wiki?curid=5360" title="Card game">
Card game

A card game is any game using playing cards as the primary device with which the game is played, be they traditional or game-specific. Countless card games exist, including families of related games (such as poker). A small number of card games played with traditional decks have formally standardized rules, but most are folk games whose rules vary by region, culture, and person.
Many games that are not generally placed in the family of card games do in fact use cards for some aspect of their gameplay. Similarly, some games that are placed in the card game genre involve a board. The distinction is that the gameplay of a card game primarily depends on the use of the cards by players (the board is simply a guide for scorekeeping or for card placement), while board games (the principal non-card game genre to use cards) generally focus on the players' positions on the board, and use the cards for some secondary purpose.
Playing cards.
A card game is played with a "deck" or "pack" of playing cards which are identical in size and shape. Each card has two sides, the "face" and the "back". Normally the backs of the cards are indistinguishable. The faces of the cards may all be unique, or there can be duplicates. The composition of a deck is known to each player. In some cases several decks are shuffled together to form a single "pack" or "shoe".
The first playing cards appeared in the ninth century during Tang dynasty China.
The first reference to the card game in world history dates no later than the 9th century, when the "Collection of Miscellanea at Duyang", written by Tang Dynasty writer Su E, described Princess (daughter of Emperor Yizong of Tang) playing the "leaf game" in 868 with members of the Wei clan (the family of the princess' husband). The Song dynasty statesman and historian Ouyang Xiu has noted that paper playing cards arose in connection to an earlier development in the book format from scrolls to pages. During the Ming dynasty (1368-1644), characters from popular novels such as the "Water Margin" were widely featured on the faces of playing cards. A precise description of Chinese money playing cards (in four suits) survived from the 15th century. Mahjong tiles are a 19th-century invention based on three-suited money playing card decks, similar to the way in which Rummikub tiles were derived recently from modern Western playing cards.
The same kind of games can also be played with tiles made of wood, plastic, bone, or similar materials. The most notable examples of such tile sets are dominoes, mahjong tiles and Rummikub tiles. Chinese dominoes are also available as playing cards. It is not clear whether Emperor Muzong of Liao really played with domino cards as early as 969, though. Legend dates the invention of dominoes in the year 1112, and the earliest known domino rules are from the following decade. 500 years later domino cards were reported as a new invention.
Playing cards first appeared in Europe in the last quarter of the 14th century. The earliest European references speak of a Saracen or Moorish game called "naib", and in fact an almost complete Mamluk Egyptian deck of 52 cards in a distinct oriental design has survived from around the same time, with the four suits "swords", "polo sticks", "cups" and "coins" and the ranks "king", "governor", "second governor", and "ten" to "one".
The 1430s in Italy saw the invention of the tarot deck, a full Latin-suited deck augmented by suitless cards with painted motifs that played a special role as trumps. Tarot, tarock and tarocchi games are still played with (subsets of) these decks in parts of Central Europe. A full tarot deck contains 14 cards in each suit; low cards labeled 1-10, and court cards "Valet" (Jack), "Chevalier" (Cavalier/Knight),"Dame" (Queen), and "Roi" (King), plus the Fool or Excuse card, and 21 trump cards. In the 18th century the card images of the traditional Italian tarot decks became popular in cartomancy and evolved into "esoteric" decks used primarily for the purpose; today most tarot decks sold in North America are the occult type, and are closely associated with fortune telling. In Europe, "playing tarot" decks remain popular for games, and have evolved since the 18th century to use regional suits (Spades/Hearts/Diamonds/Clubs in France, Leaves/Hearts/Bells/Acorns in Germany) as well as other familiar aspects of the Anglo-American deck such as corner card indices and "stamped" card symbols for non-court cards. Decks differ regionally based on the number of cards needed to play the games; the French tarot consists of the "full" 78 cards, while Germanic, Spanish and Italian Tarot variants remove certain values (usually low suited cards) from the deck, creating a deck with as few as 32 cards.
The French suits were introduced around 1480 and, in France, mostly replaced the earlier Latin suits of "swords", "clubs", "cups" and "coins". (which are still common in Spanish- and Portuguese-speaking countries as well as in some northern regions of Italy) The suit symbols, being very simple and single-color, could be stamped onto the playing cards to create a deck, thus only requiring special full-color card art for the court cards. This drastically simplifies the production of a deck of cards versus the traditional Italian deck, which used unique full-color art for each card in the deck. The French suits became popular in English playing cards in the 16th century (despite historic animosity between France and England), and from there were introduced to British colonies including North America. The rise of Western culture has led to the near-universal popularity and availability of French-suited playing cards even in areas with their own regional card art.
In Japan, a distinct 48-card hanafuda deck is popular. It is derived from 16th-century Portuguese decks, after undergoing a long evolution driven by laws enacted by the Tokugawa Shogunate attempting to ban the use of playing cards
The best-known deck internationally is the 52-card Anglo-American deck used for such games as poker and contract bridge. It contains one card for each unique combination of thirteen "ranks" and the four French "suits" "spades", "hearts", "diamonds", and "clubs". The ranks (from highest to lowest in bridge and poker) are "ace", "king", "queen", "jack" (or "knave"), and the numbers from "ten" down to "two" (or "deuce"). The trump cards and "knight" cards from the French playing tarot are not included.
Originally the term "knave" was more common than "jack"; the card had been called a jack as part of the terminology of All-Fours since the 17th century, but the word was considered vulgar. (Note the exclamation by Estella in Charles Dickens's novel "Great Expectations": "He calls the knaves, Jacks, this boy!") However, because the card abbreviation for knave ("Kn") was so close to that of the king, it was very easy to confuse them, especially after suits and rankings were moved to the corners of the card in order to enable people to fan them in one hand and still see all the values. (The earliest known deck to place suits and rankings in the corner of the card is from 1693, but these cards did not become common until after 1864 when Hart reintroduced them along with the knave-to-jack change.) However, books of card games published in the third quarter of the 19th century evidently still referred to the "knave", and the term with this definition is still recognized in the United Kingdom.
Since the 19th century some decks have been specially printed for certain games. Old Maid, Phase 10, Rook, and Uno are examples of games that can be played with one or more 52 card decks but are usually played with custom decks. Cards play an important role in board games like Risk and Monopoly.
Typical structure of card games.
Number and association of players.
Any specific card game imposes restrictions on the number of players. The most significant dividing lines run between one-player games and two-player games, and between two-player games and multi-player games. Card games for one player are known as "solitaire" or "patience" card games. (See list of solitaire card games.) Generally speaking, they are in many ways special and atypical, although some of them have given rise to two- or multi-player games such as Spite and Malice.
In card games for two players, usually not all cards are distributed to the players, as they would otherwise have perfect information about the game state. Two-player games have always been immensely popular and include some of the most significant card games such as piquet, bezique, sixty-six, klaberjass, gin rummy and cribbage. Many multi-player games started as two-player games that were adapted to a greater number of players. For such adaptations a number of non-obvious choices must be made beginning with the choice of a game orientation.
One way of extending a two-player game to more players is by building two teams of equal size. A common case is four players in two fixed partnerships, sitting crosswise as in whist and contract bridge. Partners sit opposite to each other and cannot see each other's hands. If communication between the partners is allowed at all, then it is usually restricted to a specific list of permitted signs and signals. 17th-century French partnership games such as triomphe were special in that partners sat next to each other and were allowed to communicate freely so long as they did not exchange cards or played out of order.
Another way of extending a two-player game to more players is as a "cut-throat" game, in which all players fight on their own, and win or lose alone. Most cut-throat card games are "round games", i.e. they can be played by any number of players starting from two or three, so long as there are enough cards for all.
For some of the most interesting games such as ombre, tarot and skat, the associations between players change from hand to hand. Ultimately players all play on their own, but for each hand, some game mechanism divides the players into two teams. Most typically these are "solo games", i.e. games in which one player becomes the soloist and has to achieve some objective against the others, who form a team and win or lose all their points jointly. But in games for more than three players, there may also be a mechanism that selects two players who then have to play against the others.
Direction of play.
The players of a card game normally form a circle around a table or other space that can hold cards. The "game orientation" or "direction of play", which is only relevant for three or more players, can be either clockwise or counterclockwise. It is the direction in which various roles in the game proceed. Most regions have a traditional direction of play, such as:
Europe is roughly divided into a clockwise area in the north and a counterclockwise area in the south. The boundary runs between Ireland, Great Britain, Netherlands, Germany, Austria (mostly), Slovakia, Ukraine and Russia (clockwise) and France, Switzerland, Italy, Slovenia, Croatia, Hungary, Romania, Bulgaria and Turkey (counterclockwise).
Games that originate in a region with a strong preference are often initially played in the original direction, even in regions that prefer the opposite direction. For games that have official rules and are played in tournaments, the direction of play is often prescribed in those rules.
Determining who deals.
Most games have some form of asymmetry between players. The roles of players are normally expressed in terms of the "dealer", i.e. the player whose task it is to shuffle the cards and distribute them to the players. Being the dealer can be a (minor or major) advantage or disadvantage, depending on the game. Therefore, after each played hand, the deal normally passes to the next player according to the game orientation.
As it can still be an advantage or disadvantage to be the first dealer, there are some standard methods for determining who is the first dealer. A common method is by cutting, which works as follows. One player shuffles the deck and places it on the table. Each player lifts a packet of cards from the top, reveals its bottom card, and returns it to the deck. The player who reveals the highest (or lowest) card becomes dealer. In case of a tie, the process is repeated by the tied players. For some games such as whist this process of cutting is part of the official rules, and the hierarchy of cards for the purpose of cutting (which need not be the same as that used otherwise in the game) is also specified. But in general any method can be used, such as tossing a coin in case of a two-player game, drawing cards until one player draws an ace, or rolling dice.
Hands, rounds and games.
A "hand" is a unit of the game that begins with the dealer shuffling and dealing the cards as described below, and ends with the players scoring and the next dealer being determined. The set of cards that each player receives and holds in his or her hands is also known as that player's hand.
The hand is over when the players have finished playing their hands. Most often this occurs when one player (or all) has no cards left. The player who sits after the dealer in the direction of play is known as eldest hand (or in two-player games as elder hand). A "game round" consists of as many hands as there are players. After each hand, the deal is passed on in the direction of play, i.e. the previous eldest hand becomes the new dealer. Normally players score points after each hand. A game may consist of a fixed number of rounds. Alternatively it can be played for a fixed number of points. In this case it is over with the hand in which a player reaches the target score.
Shuffling.
Shuffling is the process of bringing the cards of a pack into a random order. There are a large number of techniques with various advantages and disadvantages. "Riffle shuffling" is a method in which the deck is divided into two roughly equal-sized halves that are bent and then released, so that the cards interlace. Repeating this process several times randomizes the deck well, but the method is harder to learn than some others and may damage the cards. The "overhand shuffle" and the "Hindu shuffle" are two techniques that work by taking batches of cards from the top of the deck and reassembling them in the opposite order. They are easier to learn but must be repeated more often. A method suitable for small children consists in spreading the cards on a large surface and moving them around before picking up the deck again. This is also the most common method for shuffling tiles such as dominoes.
For casino games that are played for large sums it is vital that the cards be properly randomised, but for many games this is less critical, and in fact player experience can suffer when the cards are shuffled too well. The official skat rules stipulate that the cards are "shuffled well", but according to a decision of the German skat court, a one-handed player should ask another player to do the shuffling, rather than use a shuffling machine, as it would shuffle the cards "too" well. French belote rules go so far as to prescribe that the deck never be shuffled between hands.
Deal.
The dealer takes all of the cards in the pack, arranges them so that they are in a uniform stack, and shuffles them. In strict play, the dealer then offers the deck to the previous player (in the sense of the game direction) for "cutting". If the deal is clockwise, this is the player to the dealer's right; if counterclockwise, it is the player to the dealer's left. The invitation to cut is made by placing the pack, face downward, on the table near the player who is to cut: who then lifts the upper portion of the pack clear of the lower portion and places it alongside. (Normally the two portions have about equal size. Strict rules often indicate that each portion must contain a certain minimum number of cards, such as three or five.) The formerly lower portion is then replaced on top of the formerly upper portion. Instead of cutting, one may also knock on the deck to indicate that one trusts the dealer to have shuffled fairly.
The actual "deal" (distribution of cards) is done in the direction of play, beginning with eldest hand. The dealer holds the pack, face down, in one hand, and removes cards from the top of it with his or her other hand to distribute to the players, placing them face down on the table in front of the players to whom they are dealt. The cards may be dealt one at a time, or in batches of more than one card; and either the entire pack or a determined number of cards are dealt out. The undealt cards, if any, are left face down in the middle of the table, forming the "stock" (also called the talon, widow, skat or kitty depending on the game and region).
Throughout the shuffle, cut, and deal, the dealer should prevent the players from seeing the faces of any of the cards. The players should not try to see any of the faces. Should a player accidentally see a card, other than one's own, proper etiquette would be to admit this. It is also dishonest to try to see cards as they are dealt, or to take advantage of having seen a card. Should a card accidentally become exposed, (visible to all), any player can demand a redeal (all the cards are gathered up, and the shuffle, cut, and deal are repeated) or that the card be replaced randomly into the deck ("burning" it) and a replacement dealt from the top to the player who was to receive the revealed card.
When the deal is complete, all players pick up their cards, or 'hand', and hold them in such a way that the faces can be seen by the holder of the cards but not the other players, or vice versa depending on the game. It is helpful to fan one's cards out so that if they have corner indices all their values can be seen at once. In most games, it is also useful to sort one's hand, rearranging the cards in a way appropriate to the game. For example, in a trick-taking game it may be easier to have all one's cards of the same suit together, whereas in a rummy game one might sort them by rank or by potential combinations.
Rules.
A new card game starts in a small way, either as someone's invention, or as a modification of an existing game. Those playing it may agree to change the rules as they wish. The rules that they agree on become the "house rules" under which they play the game. A set of house rules may be accepted as valid by a group of players wherever they play, as it may also be accepted as governing all play within a particular house, café, or club.
When a game becomes sufficiently popular, so that people often play it with strangers, there is a need for a generally accepted set of rules. This need is often met when a particular set of house rules becomes generally recognized. For example, when Whist became popular in 18th-century England, players in the Portland Club agreed on a set of house rules for use on its premises. Players in some other clubs then agreed to follow the "Portland Club" rules, rather than go to the trouble of codifying and printing their own sets of rules. The Portland Club rules eventually became generally accepted throughout England and Western cultures.
It should be noted that there is nothing static or "official" about this process. For the majority of games, there is no one set of universal rules by which the game is played, and the most common ruleset is no more or less than that. Many widely played card games, such as Canasta and Pinochle, have no official regulating body. The most common ruleset is often determined by the most popular distribution of rulebooks for card games. Perhaps the original compilation of popular playing card games was collected by Edmund Hoyle, a self-made authority on many popular parlor games. The U.S. Playing Card Company now owns the eponymous Hoyle brand, and publishes a series of rulebooks for various families of card games that have largely standardized the games' rules in countries and languages where the rulebooks are widely distributed. However, players are free to, and often do, invent "house rules" to supplement or even largely replace the "standard" rules.
If there is a sense in which a card game can have an "official" set of rules, it is when that card game has an "official" governing body. For example, the rules of tournament bridge are governed by the World Bridge Federation, and by local bodies in various countries such as the American Contract Bridge League in the U.S., and the English Bridge Union in England. The rules of skat are governed by The International Skat Players Association and in Germany by the Deutscher Skatverband which publishes the "Skatordnung". The rules of French tarot are governed by the Fédération Française de Tarot. The rules of Poker's variants are largely traditional, but enforced by the World Series of Poker and the World Poker Tour organizations which sponsor tournament play. Even in these cases, the rules must only be followed exactly at games sanctioned by these governing bodies; players in less formal settings are free to implement agreed-upon supplemental or substitute rules at will.
Rule infractions.
An infraction is any action which is against the rules of the game, such as playing a card when it is not one's turn to play or the accidental exposure of a card, informally known as 'bleeding'.
In many official sets of rules for card games, the rules specifying the penalties for various infractions occupy more pages than the rules specifying how to play correctly. This is tedious, but necessary for games that are played seriously. Players who intend to play a card game at a high level generally ensure before beginning that all agree on the penalties to be used. When playing privately, this will normally be a question of agreeing house rules. In a tournament there will probably be a tournament director who will enforce the rules when required and arbitrate in cases of doubt.
If a player breaks the rules of a game deliberately, this is cheating. Most card players would refuse to play cards with a known cheat. The rest of this section is therefore about accidental infractions, caused by ignorance, clumsiness, inattention, etc.
As the same game is played repeatedly among a group of players, precedents build up about how a particular infraction of the rules should be handled. For example, "Sheila just led a card when it wasn't her turn. Last week when Jo did that, we agreed ... etc." Sets of such precedents tend to become established among groups of players, and to be regarded as part of the house rules. Sets of house rules become formalized, as described in the previous section. Therefore, for some games, there is a "proper" way of handling infractions of the rules. But for many games, without governing bodies, there is no standard way of handling infractions.
In many circumstances, there is no need for special rules dealing with what happens after an infraction. As a general principle, the person who broke a rule should not benefit by it, and the other players should not lose by it. An exception to this may be made in games with fixed partnerships, in which it may be felt that the partner(s) of the person who broke a rule should also not benefit. The penalty for an accidental infraction should be as mild as reasonable, consistent with there being no possible benefit to the person responsible.
Types.
Trick-taking games.
The object of a trick-taking game is based on the play of multiple rounds, or tricks, in each of which each player plays a single card from their hand, and based on the values of played cards one player wins or "takes" the trick. The specific object varies with each game and can include taking as many tricks as possible, taking as many scoring cards within the tricks won as possible, taking as few tricks (or as few penalty cards) as possible, taking a particular trick in the hand, or taking an exact number of tricks. Bridge, Whist, Euchre, Spades, and the various Tarot card games are popular examples.
Matching games.
The object of Rummy, and various other melding or matching games, is to acquire the required groups of matching cards before an opponent can do so. In Rummy, this is done through drawing and discarding, and the groups are called melds. Mahjong is a very similar game played with tiles instead of cards. Non-Rummy examples of match-type games generally fall into the "fishing" genre and include the children's games Go Fish, Old Maid and Blue Canary.
Shedding games.
In a shedding game, players start with a hand of cards, and the object of the game is to be the first player to discard all cards from one's hand. Common shedding games in the US include Crazy Eights (commercialized by Mattel as Uno) and Daihinmin. Some matching-type games are also shedding-type games; some variants of Rummy such as Phase 10, Rummikub, the bluffing game I Doubt It, and the children's game Old Maid, fall into both categories.
Accumulating games.
The object of an accumulating game is to acquire all cards in the deck. Examples include most War type games, and games involving slapping a discard pile such as Slapjack. Egyptian War has both of these features.
Fishing games.
In fishing games, cards from the hand are played against cards in a layout on the table, capturing table cards if they match. Fishing games are popular in many nations, including China, where there are many diverse fishing games. Scopa is considered one of the national card games of Italy. Cassino is the only fishing game to be widely played in English-speaking countries. Seep is a classic Indian fishing card game mainly popular in northern parts of India.
Comparing games.
Comparing card games are those where hand values are compared to determine the winner, also known as "vying" or "showdown" games. Poker, blackjack, and baccarat are examples of comparing card games. As seen, nearly all of these games are designed as gambling games.
Solitaire (Patience) games.
Solitaire games are designed to be played by one player. Most games begin with a specific layout of cards, called a tableau, and the object is then either to construct a more elaborate final layout, or to clear the tableau and/or the draw pile or "stock" by moving all cards to one or more "discard" or "foundation" piles.
Drinking card games.
Drinking card games are, true to their name, a subset of drinking games using cards, in which the object in playing the game is either to drink or to force others to drink. Many games are simply ordinary card games with the establishment of "drinking rules"; President, for instance, is virtually identical to Daihinmin but with additional rules governing drinking. Poker can also be played using a number of drinks as the wager. Another game often played as a drinking game is Toepen, quite popular in the Netherlands. Some card games are designed specifically to be played as drinking games.
Multi-genre games.
Many games borrow elements from more than one type of game. The most common combination is that of matching and shedding, as in some variants of Rummy, Old Maid and Go Fish. However, many multi-genre games involve different stages of play for each hand. The most common multi-stage combination is a "trick-and-meld" game, such as Pinochle or Belote. Other multi-stage, multi-genre games include Poke, Flaps, Skitgubbe and Tichu.
Collectible card games (CCGs).
Collectible card games are defined by the use of decks of proprietary cards that differ between players. The contents of these decks are a subset of a very large pool of available cards which have differing effects, costs, and art. A player accumulates his or her deck through purchase or trade for desirable cards, and each player uses their own deck to play against the other. "" and "Yu-Gi-Oh!" are well-known collectible card games. Such games are also created to capitalize on the popularity of other forms of entertainment, such as "Pokémon" and "Marvel Comics" which both have had CCGs created around them.
Casino or gambling card games.
These games revolve around wagers of money. Though virtually any game in which there are winning and losing outcomes can be wagered on, these games are specifically designed to make the betting process a strategic part of the game. Some of these games involve players betting against each other, such as poker, while in others, like blackjack, players wager against the house.
Poker games.
Poker is a family of gambling games in which players bet into a pool, called the pot, value of which changes as the game progresses that the value of the hand they carry will beat all others according to the ranking system. Variants largely differ on how cards are dealt and the methods by which players can improve a hand. For many reasons, including its age and its popularity among Western militaries, it is one of the most universally known card games in existence.
Other card games.
Many other card games have been designed and published on a commercial or amateur basis. In some cases, the game uses the standard 52-card deck, but the object is unique. In Eleusis, for example, players play single cards, and are told whether the play was legal or illegal, in an attempt to discover the underlying rules made up by the dealer.
Most of these games however typically use a specially made deck of cards designed specifically for the game (or variations of it). The decks are thus usually proprietary, but may be created by the game's players. Uno, Phase 10, Set, CASH Trader, Slamwich, 1000 Blank White Cards, and Sopio are popular dedicated-deck card games; 1000 Blank White Cards is unique in that the cards for the game are designed by the players of the game while playing it; there is no commercially available deck advertised as such.
Fictional card games.
Many games, including card games, are fabricated by science fiction authors and screenwriters to distance a culture depicted in the story from present-day Western culture. They are commonly used as filler to depict background activities in an atmosphere like a bar or rec room, but sometimes the drama revolves around the play of the game. Some of these games, such as Pyramid from "Battlestar Galactica", become real card games as the holder of the intellectual property develops and markets a suitable deck and ruleset for the game, while others, such as "Exploding Snap" from the Harry Potter franchise, lack sufficient descriptions of rules, or depend on cards or other hardware that are infeasible or physically impossible.

</doc>
<doc id="5361" url="https://en.wikipedia.org/wiki?curid=5361" title="Cross-stitch">
Cross-stitch

Cross-stitch is a popular form of counted-thread embroidery in which X-shaped stitches in a tiled, raster-like pattern are used to form a picture. Cross-stitch is often executed on easily countable evenweave fabric called aida cloth. The stitcher counts the threads in each direction so that the stitches are of uniform size and appearance. This form of cross-stitch is also called counted cross-stitch in order to distinguish it from other forms of cross-stitch. Sometimes cross-stitch is done on designs printed on the fabric (stamped cross-stitch); the stitcher simply stitches over the printed pattern.
Fabrics used in cross-stitch include aida, linen and mixed-content fabrics called 'evenweave'. All cross stitch fabrics are technically "evenweave," as it refers to the fact that the fabric is woven to make sure that there are the same number of threads in an inch both left to right and top to bottom (vertically and horizontally). Fabrics are categorized by threads per inch (referred to as 'count'), which can range from 11 to 40 count. Aida fabric has a lower count because it is made with two threads grouped together for ease of stitching. Cross stitch projects are worked from a gridded pattern and can be used on any count fabric, the count of the fabric determines the size of the finished stitching.
History.
Cross-stitch is the oldest form of embroidery and can be found all over the world. Many folk museums show examples of clothing decorated with cross-stitch, especially from continental Europe, Asia, and Eastern and Central Europe.
The cross stitch sampler is called that because it was generally stitched by a young girl to learn how to stitch and to record alphabet and other patterns to be used in her household sewing. These samples of her stitching could be referred back to over the years. Often, motifs and initials were stitched on household items to identify their owner, or simply to decorate the otherwise-plain cloth. In the United States, the earliest known cross-stitch sampler is currently housed at Pilgrim Hall in Plymouth, Massachusetts. The sampler was created by Loara Standish, daughter of Captain Myles Standish and pioneer of the Leviathan stitch, circa 1653.
Traditionally, cross-stitch was used to embellish items like household linens, tablecloths, dishcloths, and doilies (only a small portion of which would actually be embroidered, such as a border). Although there are many cross-stitchers who still employ it in this fashion, it is now increasingly popular to work the pattern on pieces of fabric and hang them on the wall for decoration. Cross stitch is also often used to make greeting cards, pillowtops, or as inserts for box tops, coasters and trivets.
Multicoloured, shaded, painting-like patterns as we know them today are a fairly modern development, deriving from similar shaded patterns of Berlin wool work of the mid-nineteenth century. Besides designs created expressly for cross stitch, there are software programs that convert a photograph or a fine art image into a chart suitable for stitching. One stunning example of this is in the cross stitched reproduction of the Sistine Chapel charted and stitched by Joanna Lopianowski-Roberts.
There are many cross-stitching "guilds" and groups across the United States and Europe which offer classes, collaborate on large projects, stitch for charity, and provide other ways for local cross-stitchers to get to know one another. Individually owned local needlework shops (LNS) often have stitching nights at their shops, or host weekend stitching retreats.
Today cotton floss is the most common embroidery thread. It is a thread made of mercerized cotton, composed of six strands that are only loosely twisted together and easily separable. While there are other manufacturers, the two most-commonly used (and oldest) brands are DMC and Anchor [http://www.coatsandclark.com/Products/Hand+Embroidery/Threads/], both of which have been manufacturing embroidery floss since the 1800s.
Other materials used are pearl (or perle) cotton, Danish flower thread, silk and Rayon. Different wool threads, metallic threads or other novelty threads are also used, sometimes for the whole work, but often for accents and embellishments. Hand-dyed cross stitch floss is created just as the name implies - it is dyed by hand. Because of this, there are variations in the amount of color throughout the thread. Some variations can be subtle, while some can be a huge contrast. Some also have more than one color per thread, which in the right project, creates amazing results.
Cross stitch is widely used in traditional Palestinian dressmaking.
Related stitches and forms of embroidery.
Other stitches are also often used in cross-stitch, among them ¼, ½, and ¾ stitches and backstitches.
Cross-stitch is often used together with other stitches. A cross stitch can come in a variety of prostational forms. It is sometimes used in crewel embroidery, especially in its more modern derivatives. It is also often used in needlepoint.
A specialized historical form of embroidery using cross-stitch is Assisi embroidery.
There are many stitches which are related to cross-stitch and were used in similar ways in earlier times. The best known are Italian cross-stitch, Celtic Cross Stitch, Irish Cross Stitch, long-armed cross-stitch, Ukrainian cross-stitch and Montenegrin stitch. Italian cross-stitch and Montenegrin stitch are reversible, meaning the work looks the same on both sides. These styles have a slightly different look than ordinary cross-stitch. These more difficult stitches are rarely used in mainstream embroidery, but they are still used to recreate historical pieces of embroidery or by the creative and adventurous stitcher.
The double cross-stitch, also known as a Leviathan stitch or Smyrna cross stitch, combines a cross-stitch with an upright cross-stitch.
Berlin wool work and similar petit point stitchery resembles the heavily shaded, opulent styles of cross-stitch, and sometimes also used charted patterns on paper.
Cross-stitch is often combined with other popular forms of embroidery, such as Hardanger embroidery or blackwork embroidery. Cross-stitch may also be combined with other work, such as canvaswork or drawn thread work. Beadwork and other embellishments such as paillettes, charms, small buttons and speciality threads of various kinds may also be used.
Recent trends in the UK.
Cross-stitch has become increasingly popular with the younger generation of the United Kingdom in recent years. The Great Recession has also seen renewal of interest in home crafts. Retailers such as John Lewis experienced a 17% rise in sales of haberdashery products between 2009 and 2010. Hobbycraft, a chain of stores selling craft supplies, also enjoyed an 11% increase in sales over the past year. The chain is said to have benefited from the "make do and mend" mentality of the credit crisis, which has driven people to make their own cards and gifts.
Knitting and cross stitching have become more popular hobbies for a younger market, in contrast to its traditional reputation as a hobby for retirees. Sewing and craft groups such as Stitch and Bitch London have resurrected the idea of the traditional craft club. At Clothes Show Live 2010 there was a new area called "Sknitch" promoting modern sewing, knitting and embroidery.
In a departure from the traditional designs associated with cross stitch, there is a current trend for more postmodern or tongue-in-cheek designs featuring retro images or contemporary sayings. It is linked to a concept known as 'subversive cross stitch', which involves more risque designs, often fusing the traditional sampler style with sayings designed to shock or be incongruous with the old-fashioned image of cross stitch.
Stitching designs on other materials can be accomplished by using a Waste Canvas. This waste canvas is a temporary gridded canvas similar to regular canvas used for embroidery that is held together by a water-soluble glue, which is removed after completion of stitch design.

</doc>
<doc id="5362" url="https://en.wikipedia.org/wiki?curid=5362" title="Casino game">
Casino game

Games available in most casinos are commonly called casino games. In a casino game, the players gamble casino chips on various possible random outcomes or combinations of outcomes. Casino games are also available in online casinos, where permitted by law. Casino games can also be played outside of casinos for entertainment purposes like in parties or in school competitions, some on machines that simulate gambling.
Categories.
There are three general categories of casino games: table games, electronic gaming machines, and random number ticket games such as Keno and simulated racing. Gaming machines, such as slot machines and pachinko, are usually played by one player at a time and do not require the involvement of casino employees to play. Random number games are based upon the selection of random numbers, either from a computerized random number generator or from other gaming equipment. Random number games may be played at a table, such as roulette, or through the purchase of paper tickets or cards, such as keno or bingo.
House advantage.
Casino games generally provide a predictable long-term advantage to the casino, or "house", while offering the player the possibility of a large short-term payout. Some casino games have a skill element, where the player makes decisions; such games are called "random with a tactical element". While it is possible through skillful play to minimize the house advantage, it is extremely rare that a player has sufficient skill to completely eliminate his inherent long-term disadvantage (the house edge (HE) or house vigorish) in a casino game. Such a skill set would involve years of training, an extraordinary memory and numeracy, and/or acute visual or even aural observation, as in the case of wheel clocking in roulette or other examples of advantage play.
The player's disadvantage is a result of the casino not paying winning wagers according to the game's "true odds", which are the payouts that would be expected considering the odds of a wager either winning or losing. For example, if a game is played by wagering on the number that would result from the roll of one die, true odds would be 5 times the amount wagered since there is a 1 in 6 chance of any single number appearing, assuming that you get the original amount wagered back. However, the casino may only pay 4 times the amount wagered for a winning wager.
The house edge or vigorish is defined as the casino profit expressed as the percentage of the player's original bet. (In games such as blackjack or Spanish 21, the final bet may be several times the original bet, if the player double and splits.)
In American roulette, there are two "zeroes" (0, 00) and 36 non-zero numbers (18 red and 18 black). This leads to a higher house edge compared to the European roulette. The chances of a player, who bets 1 unit on red, winning is 18/38 and his chances of losing 1 unit is 20/38. The player's expected value is EV = (18/38 x 1) + (20/38 x -1) = 18/38 - 20/38 = -2/38 = -5.26%. Therefore, the house edge is 5.26%. After 10 spins, betting 1 unit per spin, the average house profit will be 10 x 1 x 5.26% = 0.53 units. Of course, the casino may not win exactly 53 cents of a unit; this figure is the average casino profit from each player if it had millions of players each betting for 10 spins at 1 unit per spin. European and French roulette wheels have only one "zero" and therefore the house advantage (ignoring the en prison rule) is equal to 1/37 = 2.7%.
Poker has become one of the most popular games played in the casino. It is a game of skill and the only game where the players are competing against each other and not the house. There are several variations of poker that are played in casino card rooms.
The house edge of casino games vary greatly with the game, with some games having as low as 0.3%. Keno can have house edges up to 25%, slot machines having up to 15%, while most Australian Pontoon games have house edges between 0.3% and 0.4%. It's always important, to look for the casino game with the lowest house advantage. 
The calculation of the roulette house edge was a trivial exercise; for other games, this is not usually the case. Combinatorial analysis and/or computer simulation is necessary to complete the task.
In games which have a skill element, such as blackjack or Spanish 21, the house edge is defined as the house advantage from optimal play (without the use of advanced techniques such as card counting), on the first hand of the shoe (the container that holds the cards). The set of the optimal plays for all possible hands is known as "basic strategy" and is highly dependent on the specific rules and even the number of decks used. Good blackjack and Spanish 21 games have house edges below 0.5%.
Traditionally, the majority of casinos have refused to reveal the house edge information for their slots games and due to the unknown number of symbols and weightings of the reels, in most cases this is much more difficult to calculate than for other casino games. However, due to some online properties revealing this information and some independent research conducted by Michael Shackleford in the offline sector, this pattern is slowly changing.
Standard deviation.
The luck factor in a casino game is quantified using standard deviations (SD). The standard deviation of a simple game like roulette can be calculated using the binomial distribution. In the binomial distribution, SD = sqrt ("npq" ), where "n" = number of rounds played, "p" = probability of winning, and "q" = probability of losing. The binomial distribution assumes a result of 1 unit for a win, and 0 units for a loss, rather than -1 units for a loss, which doubles the range of possible outcomes. Furthermore, if we flat bet at 10 units per round instead of 1 unit, the range of possible outcomes increases 10 fold.
SD (Roulette, even-money bet) = 2"b" sqrt("npq" ), where "b" = flat bet per round, "n" = number of rounds, "p" = 18/38, and "q" = 20/38.
For example, after 10 rounds at 1 unit per round, the standard deviation will be 2 x 1 x sqrt(10 x 18/38 x 20/38) = 3.16 units. After 10 rounds, the expected loss will be 10 x 1 x 5.26% = 0.53. As you can see, standard deviation is many times the magnitude of the expected loss.
The standard deviation for pai gow poker is the lowest out of all common casinos. Many, particularly slots, have extremely high standard deviations. As the size of the potential payouts increase, so does the standard deviation.
As the number of rounds increases, eventually, the expected loss will exceed the standard deviation, many times over. From the formula, we can see the standard deviation is proportional to the square root of the number of rounds played, while the expected loss is proportional to the number of rounds played. As the number of rounds increases, the expected loss increases at a much faster rate. This is why it is impossible for a gambler to win in the long term. It is the high ratio of short-term standard deviation to expected loss that fools gamblers into thinking that they can win.
It is important for a casino to know both the house edge and variance for all of their games. The house edge tells them what kind of profit they will make as percentage of turnover, and the variance tells them how much they need in the way of cash reserves. The mathematicians and computer programmers that do this kind of work are called gaming mathematicians and gaming analysts. Casinos do not have in-house expertise in this field, so outsource their requirements to experts in the gaming analysis field.

</doc>
<doc id="5363" url="https://en.wikipedia.org/wiki?curid=5363" title="Video game">
Video game

A video game is an electronic game that involves human interaction with a user interface to generate visual feedback on a video device such as a TV screen or computer monitor. The word "video" in "video game" traditionally referred to a raster display device, but in the 2000s, it implies any type of display device that can produce two- or three-dimensional images. Video games are sometimes believed to be a form of art, but this designation is controversial.
The electronic systems used to play video games are known as platforms; examples of these are personal computers and video game consoles. These platforms range from large mainframe computers to small handheld computing devices. Specialized video games such as arcade games, in which the video game components are housed in a large chassis, while common in the 1980s, have gradually declined in use due to the widespread availability of home video game devices (e.g., PlayStation 4 and Xbox One) and video games on desktop and laptop computers and smartphones.
The input device used for games, the game controller, varies across platforms. Common controllers include gamepads, mouses, keyboards, joysticks, the touchscreens of mobile devices and buttons. In addition to video and (in most cases) audio feedback, some games in the 2000s include haptic, vibration or force feedback peripherals. 
The video game industry is of increasing commercial importance, with growth driven particularly by the emerging Asian markets and mobile games. As of 2015, video games generated sales of USD 74 billion annually worldwide, and were the third-largest segment in the U.S. entertainment market, behind broadcast and cable TV.
History.
Early games used interactive electronic devices with various display formats. The earliest example is from 1947—a "Cathode ray tube Amusement Device" was filed for a patent on 25 January 1947, by Thomas T. Goldsmith Jr. and Estle Ray Mann, and issued on 14 December 1948, as U.S. Patent 2455992.
Inspired by radar display tech, it consisted of an analog device that allowed a user to control a vector-drawn dot on the screen to simulate a missile being fired at targets, which were drawings fixed to the screen.
Other early examples include:
Each game used different means of display: NIMROD used a panel of lights to play the game of Nim, OXO used a graphical display to play tic-tac-toe "Tennis for Two" used an oscilloscope to display a side view of a tennis court, and "Spacewar!" used the DEC PDP-1's vector display to have two spaceships battle each other.
In 1971, "Computer Space", created by Nolan Bushnell and Ted Dabney, was the first commercially sold, coin-operated video game. It used a black-and-white television for its display, and the computer system was made of 74 series TTL chips. The game was featured in the 1973 science fiction film "Soylent Green". "Computer Space" was followed in 1972 by the Magnavox Odyssey, the first home console. Modeled after a late 1960s prototype console developed by Ralph H. Baer called the "Brown Box", it also used a standard television. These were followed by two versions of Atari's "Pong"; an arcade version in 1972 and a home version in 1975 that dramatically increased video game popularity. The commercial success of "Pong" led numerous other companies to develop "Pong" clones and their own systems, spawning the video game industry.
A flood of "Pong" clones eventually led to the video game crash of 1977, which came to an end with the mainstream success of Taito's 1978 shooter game "Space Invaders", marking the beginning of the golden age of arcade video games and inspiring dozens of manufacturers to enter the market. The game inspired arcade machines to become prevalent in mainstream locations such as shopping malls, traditional storefronts, restaurants, and convenience stores. The game also became the subject of numerous articles and stories on television and in newspapers and magazines, establishing video gaming as a rapidly growing mainstream hobby. "Space Invaders" was soon licensed for the Atari VCS (later known as Atari 2600), becoming the first "killer app" and quadrupling the console's sales. This helped Atari recover from their earlier losses, and in turn the Atari VCS revived the home video game market during the second generation of consoles, up until the North American video game crash of 1983. The home video game industry was revitalized shortly afterwards by the widespread success of the Nintendo Entertainment System, which marked a shift in the dominance of the video game industry from the United States to Japan during the third generation of consoles.
Overview.
Platforms.
The term "platform" refers to the specific combination of electronic components or computer hardware which, in conjunction with software, allows a video game to operate. The term "system" is also commonly used.
In common use a "PC game" refers to a form of media that involves a player interacting with a personal computer connected to a video monitor. Personal computers are not dedicated platforms, so there may be differences running the same game in different hardwares, also the opennes allows some features to developers like reduced software cost, increased flexibility, increased innovation, emulation, creation of mods, open host for online gaming and others.
A "console game" is played on a specialized electronic device that connects to a common television set or composite video monitor, unlike PCs, a console is a dedicated platform, it is manufactured by a specific company. Usually runs only games developed for it, or games from other platform made by the same company, but never its direct competitor, even if the same game is available on different platforms. It often comes with an specific game controller.
A "handheld" gaming device is a self-contained electronic device that is portable and can be held in a user's hands. It features the console, screen, speakers and buttons in one single unit. Like consoles, handhelds are dedicated platforms, and share almost the same characteristics. Handheld hardware usually is less powerful than PC or console hardwares.
"Arcade game" generally refers to a game played on an even more specialized type of electronic device that is typically designed to play only one game and is encased in a special cabinet which has one built-in console, controller and screen.
These distinctions are not always clear and there may be games that bridge one or more platforms. In addition to personal computers, there are multiple other devices which have the ability to play games but are not dedicated video game machines, such as smartphones, PDAs and graphing calculators.
The web browser has also established itself as platform in its own right while providing a cross-platform environment for video games designed to be played on a wide spectrum of hardware from personal computers to smartphones to name a few. This in turn has generated new terms to qualify classes of web browser based games. These games may be identified based on the website that they appear, such as with "Facebook" games. Others are named based on the programming platform used to develop them, such as Java and Flash games.
Genres.
A video game, like most other forms of media, may be categorized into genres. Video game genres are used to categorize video games based on their gameplay interaction rather than visual or narrative differences. A video game genre is defined by a set of gameplay challenges and are classified independent of their setting or game-world content, unlike other works of fiction such as films or books. For example, a shooter game is still a shooter game, regardless of whether it takes place in a fantasy world or in outer space.
Because genres are dependent on content for definition, genres have changed and evolved as newer styles of video games have come into existence. Ever advancing technology and production values related to video game development have fostered more lifelike and complex games which have in turn introduced or enhanced genre possibilities (e.g., virtual pets), pushed the boundaries of existing video gaming or in some cases add new possibilities in play (such as that seen with titles specifically designed for devices like Sony's EyeToy). Some genres represent combinations of others, such as massively multiplayer online role-playing games, or, more commonly, MMORPGs. It is also common to see higher level genre terms that are collective in nature across all other genres such as with action, music/rhythm or horror-themed video games.
Classifications.
Casual games.
Casual games derive their name from their ease of accessibility, simple to understand gameplay and quick to grasp rule sets. Additionally, casual games frequently support the ability to jump in and out of play on demand. Casual games as a format existed long before the term was coined and include video games such as Solitaire or Minesweeper which can commonly be found pre-installed with many versions of the Microsoft Windows operating system.
Examples of genres within this category are match three, hidden object, time management, puzzle or many of the tower defense style games. Casual games are generally available through app stores and online retailers such as PopCap, Zylom and GameHouse or provided for free play through web portals such as Newgrounds.
While casual games are most commonly played on personal computers, phones or tablets, they can also be found on many of the on-line console system download services (e.g., the PlayStation Network, WiiWare or Xbox Live).
Serious games.
Serious games are games that are designed primarily to convey information or a learning experience of some sort to the player. Some serious games may even fail to qualify as a video game in the traditional sense of the term. Educational software does not typically fall under this category (e.g., touch typing tutors, language learning, etc.) and the primary distinction would appear to be based on the title's primary goal as well as target age demographics. As with the other categories, this description is more of a guideline than a rule.
Serious games are games generally made for reasons beyond simple entertainment and as with the core and casual games may include works from any given genre, although some such as exergames, educational games, or propaganda games may have a higher representation in this group due to their subject matter. These games are typically designed to be played by professionals as part of a specific job or for skill set improvement. They can also be created to convey social-political awareness on a specific subject.
One of the longest running serious games franchises would be Microsoft Flight Simulator first published in 1982 under that name. The United States military uses virtual reality based simulations, such as VBS1 for training exercises, as do a growing number of first responder roles (e.g., police, fire fighter, EMT). One example of a non-game environment utilized as a platform for serious game development would be the virtual world of "Second Life", which is currently used by several United States governmental departments (e.g., NOAA, NASA, JPL), Universities (e.g., Ohio University, MIT) for educational and remote learning programs and businesses (e.g., IBM, Cisco Systems) for meetings and training.
Tactical media in video games plays a crucial role in making a statement or conveying a message on important relevant issues. This form of media allows for a broader audience to be able to receive and gain access to certain information that otherwise may not have reached such people. An example of tactical media in video games would be newsgames. These are short games related to contemporary events designed to illustrate a point. For example, Take Action Games is a game studio collective that was co-founded by Susana Ruiz and has made successful serious games. Some of these games include "Darfur is Dying", "Finding Zoe", and "In The Balance". All of these games bring awareness to important issues and events in an intelligent and well thought out manner.
Educational games.
On 23 September 2009, U.S. President Barack Obama launched a campaign called "Educate to Innovate" aimed at improving the technological, mathematical, scientific and engineering abilities of American students. This campaign states that it plans to harness the power of interactive games to help achieve the goal of students excelling in these departments. This campaign has stemmed into many new opportunities for the video game realm and has contributed to many new competitions. Some of these competitions include the Stem National Video Game Competition and the Imagine Cup. Both of these examples are events that bring a focus to relevant and important current issues that are able to be addressed in the sense of video games to educate and spread knowledge in a new form of media. www.NobelPrize.org uses games to entice the user to learn about information pertaining to the Nobel prize achievements while engaging in a fun to play video game. There are many different types and styles of educational games all the way from counting to spelling to games for kids and games for adults. Some other games do not have any particular targeted audience in mind and intended to simply educate or inform whoever views or plays the game.
Development.
Video game development and authorship, much like any other form of entertainment, is frequently a cross-disciplinary field. Video game developers, as employees within this industry are commonly referred, primarily include programmers and graphic designers. Over the years this has expanded to include almost every type of skill that one might see prevalent in the creation of any movie or television program, including sound designers, musicians, and other technicians; as well as skills that are specific to video games, such as the game designer. All of these are managed by producers.
In the early days of the industry, it was more common for a single person to manage all of the roles needed to create a video game. As platforms have become more complex and powerful in the type of material they can present, larger teams have been needed to generate all of the art, programming, cinematography, and more. This is not to say that the age of the "one-man shop" is gone, as this is still sometimes found in the casual gaming and handheld markets, where smaller games are prevalent due to technical limitations such as limited RAM or lack of dedicated 3D graphics rendering capabilities on the target platform (e.g., some cellphones and PDAs).
With the growth of the size of development teams in the industry, the problem of cost has increased. Development studios need to be able to pay their staff a competitive wage in order to attract and retain the best talent, while publishers are constantly looking to keep costs down in order to maintain profitability on their investment. Typically, a video game console development team can range in sizes of anywhere from 5 to 50 people, with some teams exceeding 100. In May 2009, one game project was reported to have a development staff of 450. The growth of team size combined with greater pressure to get completed projects into the market to begin recouping production costs has led to a greater occurrence of missed deadlines, rushed games and the release of unfinished products.
Downloadable content.
A phenomenon of additional game content at a later date, often for additional funds, began with digital video game distribution known as downloadable content (DLC). Developers can use digital distribution to issue new storylines after the main game is released, such as Rockstar Games with "Grand Theft Auto IV" (' and '), or Bethesda with "Fallout 3" and its expansions. New gameplay modes can also become available, for instance, "Call of Duty" and its zombie modes, a multiplayer mode for "Mushroom Wars" or a higher difficulty level for "". Smaller packages of DLC are also common, ranging from better in-game weapons ("Dead Space", "Just Cause 2"), character outfits ("LittleBigPlanet", "Minecraft"), or new songs to perform ("SingStar", "Rock Band", "Guitar Hero").
Modifications.
Many games produced for the PC are designed such that technically oriented consumers can modify the game. These mods can add an extra dimension of replayability and interest. Developers such as id Software, Valve Software, Crytek, Bethesda, Epic Games and Blizzard Entertainment ship their games with some of the development tools used to make the game, along with documentation to assist mod developers. The Internet provides an inexpensive medium to promote and distribute mods, and they may be a factor in the commercial success of some games. This allows for the kind of success seen by popular mods such as the "Half-Life" mod "Counter-Strike".
Cheating.
Cheating in computer games may involve cheat codes and hidden spots implemented by the game developers, modification of game code by third parties, or players exploiting a software glitch. Modifications are facilitated by either cheat cartridge hardware or a software trainer. Cheats usually make the game easier by providing an unlimited amount of some resource; for example weapons, health, or ammunition; or perhaps the ability to walk through walls. Other cheats might give access to otherwise unplayable levels or provide unusual or amusing features, like altered game colors or other graphical appearances.
Glitches.
Software errors not detected by software testers during development can find their way into released versions of computer and video games. This may happen because the glitch only occurs under unusual circumstances in the game, was deemed too minor to correct, or because the game development was hurried to meet a publication deadline. Glitches can range from minor graphical errors to serious bugs that can delete saved data or cause the game to malfunction. In some cases publishers will release updates (referred to as "patches") to repair glitches. Sometimes a glitch may be beneficial to the player; these are often referred to as exploits.
Easter eggs.
Easter eggs are hidden messages or jokes left in games by developers that are not part of the main game.
Theory.
Although departments of computer science have been studying the technical aspects of video games for years, theories that examine games as an artistic medium are a relatively recent development in the humanities. The two most visible schools in this emerging field are ludology and narratology. Narrativists approach video games in the context of what Janet Murray calls "Cyberdrama". That is to say, their major concern is with video games as a storytelling medium, one that arises out of interactive fiction. Murray puts video games in the context of the Holodeck, a fictional piece of technology from "Star Trek", arguing for the video game as a medium in which we get to become another person, and to act out in another world. This image of video games received early widespread popular support, and forms the basis of films such as "Tron", "eXistenZ" and "The Last Starfighter".
Ludologists break sharply and radically from this idea. They argue that a video game is first and foremost a game, which must be understood in terms of its rules, interface, and the concept of play that it deploys. Espen J. Aarseth argues that, although games certainly have plots, characters, and aspects of traditional narratives, these aspects are incidental to gameplay. For example, Aarseth is critical of the widespread attention that narrativists have given to the heroine of the game "Tomb Raider", saying that "the dimensions of Lara Croft's body, already analyzed to death by film theorists, are irrelevant to me as a player, because a different-looking body would not make me play differently... When I play, I don't even see her body, but see through it and past it." Simply put, ludologists reject traditional theories of art because they claim that the artistic and socially relevant qualities of a video game are primarily determined by the underlying set of rules, demands, and expectations imposed on the player.
While many games rely on emergent principles, video games commonly present simulated story worlds where emergent behavior occurs within the context of the game. The term "emergent narrative" has been used to describe how, in a simulated environment, storyline can be created simply by "what happens to the player." However, emergent behavior is not limited to sophisticated games. In general, any place where event-driven instructions occur for AI in a game, emergent behavior will exist. For instance, take a racing game in which cars are programmed to avoid crashing, and they encounter an obstacle in the track: the cars might then maneuver to avoid the obstacle causing the cars behind them to slow and/or maneuver to accommodate the cars in front of them and the obstacle. The programmer never wrote code to specifically create a traffic jam, yet one now exists in the game.
Emulation.
Emulators are software that allow games to run on a different platform from the original hardware by replicate the behavior of a console, they are found in PCs, smartphones and other consoles. Emulators emerged in the early 1990s, they are generally used to play old games, hack existing games, translate unreleased games in an especific region or add enhanced features to games like improved graphics, speed up or down, bypass regional lockouts, or online multiplayer support. In the United States, emulation without permission of the publisher is illegal.
Due to great demand, began to emerge emulation officially, the first case was the Virtual Console on the Nintendo Wii, it was one of Nintendo's shares to prevent piracy. In November 2015, Microsoft lanched backwards compatibility of Xbox 360 games on Xbox One console via emulation. Also, Sony announced relaunching PS2 games on PS4 via emulation.
Social aspects.
Demographics.
The November 2005 Nielsen Active Gamer Study, taking a survey of 2,000 regular gamers, found that the U.S. games market is diversifying. The age group among male players has expanded significantly in the 25–40 age group. For casual online puzzle-style and simple mobile cell phone games, the gender divide is more or less equal between men and women. Women have also been found to show an attraction to online multi-player games where there is a communal experience. More recently there has been a growing segment of female players engaged with the aggressive style of games historically considered to fall within traditionally male genres (e.g., first-person shooters). According to the ESRB almost 41% of PC gamers are women Participation among African-Americans is eve lower. In one survey of over 2000 game developers returned responses from only 2.5% who identified as black.When comparing today's industry climate with that of 20 years ago, women and many adults are more inclined to be using products in the industry. While the market for teen and young adult men is still a strong market, it is the other demographics which are posting significant growth. The Entertainment Software Association (ESA) provides the following summary for 2011 based on a study of almost 1,200 American households carried out by Ipsos MediaCT:
A 2006 academic study, based on a survey answered by 10,000 gamers, identified the gaymers (gamers that identify as gay) as a demographic group. A follow-up survey in 2009 studied the purchase habits and content preferences of people in the group.
Based on the study by NPD group in 2011, approximately 91 percent of children aged 2–17 play games.
Multiplayer.
Video gaming has traditionally been a social experience. Multiplayer video games are those that can be played either competitively, sometimes in Electronic Sports, or cooperatively by using either multiple input devices, or by hotseating. "Tennis for Two", arguably the first video game, was a two player game, as was its successor "Pong". The first commercially available game console, the Magnavox Odyssey, had two controller inputs.
Since then, most consoles have been shipped with two or four controller inputs. Some have had the ability to expand to four, eight or as many as 12 inputs with additional adapters, such as the Multitap. Multiplayer arcade games typically feature play for two to four players, sometimes tilting the monitor on its back for a top-down viewing experience allowing players to sit opposite one another.
Many early computer games for non-PC descendant based platforms featured multiplayer support. Personal computer systems from Atari and Commodore both regularly featured at least two game ports. PC-based computer games started with a lower availability of multiplayer options because of technical limitations. PCs typically had either one or no game ports at all. Network games for these early personal computers were generally limited to only text based adventures or MUDs that were played remotely on a dedicated server. This was due both to the slow speed of modems (300-1200-bit/s), and the prohibitive cost involved with putting a computer online in such a way where multiple visitors could make use of it. However, with the advent of widespread local area networking technologies and Internet based online capabilities, the number of players in modern games can be 32 or higher, sometimes featuring integrated text and/or voice chat. Massively multiplayer online game (MMOs) can offer extremely high numbers of simultaneous players; "Eve Online" set a record with 65,303 players on a single server in 2013.
Behavioral effects.
It has been shown that action video game players have better hand–eye coordination and visuo-motor skills, such as their resistance to distraction, their sensitivity to information in the peripheral vision and their ability to count briefly presented objects, than nonplayers.
Researchers found that such enhanced abilities could be acquired by training with action games, involving challenges that switch attention between different locations, but not with games requiring concentration on single objects.
It has been suggested by a few studies that online/offline video gaming can be used as a therapeutic tool in the treatment of different mental health concerns.
In Steven Johnson's book, "Everything Bad Is Good for You", he argues that video games in fact demand far more from a player than traditional games like "Monopoly". To experience the game, the player must first determine the objectives, as well as how to complete them. They must then learn the game controls and how the human-machine interface works, including menus and HUDs. Beyond such skills, which after some time become quite fundamental and are taken for granted by many gamers, video games are based upon the player navigating (and eventually mastering) a highly complex system with many variables. This requires a strong analytical ability, as well as flexibility and adaptability. He argues that the process of learning the boundaries, goals, and controls of a given game is often a highly demanding one that calls on many different areas of cognitive function. Indeed, most games require a great deal of patience and focus from the player, and, contrary to the popular perception that games provide instant gratification, games actually delay gratification far longer than other forms of entertainment such as film or even many books. Some research suggests video games may even increase players' attention capacities.
Learning principles found in video games have been identified as possible techniques with which to reform the U.S. education system. It has been noticed that gamers adopt an attitude while playing that is of such high concentration, they do not realize they are learning, and that if the same attitude could be adopted at school, education would enjoy significant benefits. Students are found to be "learning by doing" while playing video games while fostering creative thinking.
The U.S. Army has deployed machines such as the PackBot and UAV vehicles, which make use of a game-style hand controller to make it more familiar for young people.
According to research discussed at the 2008 Convention of the American Psychological Association, certain types of video games can improve the gamers' dexterity as well as their ability to problem-solve. A study of 33 laparoscopic surgeons found that those who played video games were 27 percent faster at advanced surgical procedures and made 37 percent fewer errors compared to those who did not play video games. A second study of 303 laparoscopic surgeons (82 percent men; 18 percent women) also showed that surgeons who played video games requiring spatial skills and hand dexterity and then performed a drill testing these skills were significantly faster at their first attempt and across all 10 trials than the surgeons who did not play the video games first.
The research showing benefits from action games has been questioned due to methodological shortcomings, such as recruitment strategies and selection bias, potential placebo effects, and lack of baseline improvements in control groups. In addition, many of the studies are cross-sectional, and of the longitudinal interventional trials, not all have found effects. A response to this pointed out that the skill improvements from action games are more broad than predicted, such as mental rotation, which is not a common task in action games.
Objections to video games.
Like other media, video games have been the subject of objections, controversies and censorship, for instance because of depictions of violence, sexual themes, alcohol, tobacco and other drugs, propaganda, profanity or advertisements. Critics of video games include parents' groups, politicians, religious groups and other advocacy groups. Claims that some video games cause addiction or violent behavior continue to be made and to be disputed.
Ratings and censorship.
ESRB.
The Entertainment Software Rating Board (ESRB) gives video games maturity ratings based on their content. For example, a game might be rated "T" for "Teen" if the game contained obscene words or violence. If a game contains explicit violence or sexual themes, it is likely to receive an "M" for "Mature" rating, which means that no one under 17 should play it. There is a rated "A/O" games for "Adults Only" these games have massive violence or nudity. There are no laws that prohibit children from purchasing "M" rated games in the United States. Laws attempting to prohibit minors from purchasing "M" rated games were established in California, Illinois, Michigan, Minnesota, and Louisiana, but all were overturned on the grounds that these laws violated the First Amendment. However, many stores have opted to not sell such games to children anyway. Of course, video game laws vary from country to country. One of the most controversial games of all time, "Manhunt 2" by Rockstar Studios, was given an AO rating by the ESRB until Rockstar could make the content more suitable for a mature audience.
Video game manufacturers usually exercise tight control over the games that are made available on their systems, so unusual or special-interest games are more likely to appear as PC games. Free, casual, and browser-based games are usually played on available computers, mobile phones, or PDAs.
PEGI.
Pan European Game Information (PEGI) is a system that was developed to standardize the game ratings in all of Europe (not just European Union, although the majority are EU members), the current members are: all EU members, except Germany and the 10 accession states; Norway; Switzerland. Iceland is expected to join soon, as are the 10 EU accession states. For all PEGI members, they use it as their sole system, with the exception of the UK, where if a game contains certain material, it must be rated by BBFC. The PEGI ratings are legally binding in Vienna and it is a criminal offence to sell a game to someone if it is rated above their age.
Germany: BPjM and USK.
Stricter game rating laws mean that Germany does not operate within the PEGI. Instead, they adopt their own system of certification which is required by law. The Unterhaltungssoftware Selbstkontrolle ("USK" or Voluntary Certification of Entertainment Software) checks every game before release and assigns an age rating to it – either none (white), 6 years of age (yellow), 12 years of age (green), 16 years of age (blue) or 18 years of age (red). It is forbidden for anyone, retailers, friends or parents alike, to allow a child access to a game for which he or she is underage. If a game is considered to be harmful to young people (for example because of extremely violent, pornographic or racist content), it may be referred to the BPjM ("Bundesprüfstelle für jugendgefährdende Medien" – Federal Verification Office for Child-Endangering Media) who may opt to place it on the Index upon which the game may not be sold openly or advertised in the open media. Such indexed titles are not "banned" and can still be legally obtained by adults, but it is considered a felony to supply these titles to a child.
Commercial aspects.
Game sales.
According to the market research firm SuperData, as of May 2015, the global games market was worth USD 74.2 billion. By region, North America accounted for $23.6 billion, Asia for $23.1 billion, Europe for $22.1 billion and South America for $4.5 billion. By market segment, mobile games were worth $22.3 billion, retail games 19.7 billion, free-to-play MMOs 8.7 billion, social games $7.9 billion, PC DLC 7.5 billion, and other categories $3 billion or less each.
In the United States, also according to SuperData, the share of video games in the entertainment market grew from 5% in 1985 to 13% in 2015, becoming the third-largest market segment behind broadcast and cable television. The research firm anticipated that Asia would soon overtake North America as the largest video game market due to the strong growth of free-to-play and mobile games.
Sales of different types of games vary widely between countries due to local preferences. Japanese consumers tend to purchase much more console games than computer games, with a strong preference for games catering to local tastes. Another key difference is that, despite the decline of arcades in the West, arcade games remain the largest sector of the Japanese gaming industry. In South Korea, computer games are generally preferred over console games, especially MMORPG games and real-time strategy games. Computer games are also popular in China.
Conventions.
Gaming conventions are an important showcase of the industry. The annual gamescom in Cologne in August is the world's leading expo for video games in attendance. The E3 in June in Los Angeles is also of global importance, but is an event for industry insiders only. The Tokyo Game Show in September is the main fair in Asia.
Other notable conventions and trade fairs include Brasil Game Show in October, Paris Games Week in October–November, EB Games Expo (Australia) in October, KRI, ChinaJoy in July and the annual Game Developers Conference.
Some publishers, developers and technology producers also host their own regular conventions, with BlizzCon, QuakeCon, Nvision and the X shows being prominent examples.
Museums.
There are many video game museums around the world, including the Computer Games Museum in Berlin and the Museum of Soviet Arcade Machines in Moscow and Saint-Petersburg. The Museum of Art and Digital Entertainment in Oakland, California is a dedicated video game museum focusing on playable exhibits of console and computer games. The Video Game Museum in Rome is also dedicated to preserving video games and their history. The International Center for the History of Electronic Games at The Strong in Rochester, New York contains one of the largest collections of electronic games and game-related historical materials in the world, including a exhibit which allows guests to play their way through the history of video games. The Smithsonian Institution in Washington, D.C. has three video games on permanent display: "Pac-Man", "Dragon's Lair", and "Pong".
The Museum of Modern Art has added a total of 20 video games and one video game console to its permanent Architecture and Design Collection since 2012. In 2012, the Smithsonian American Art Museum ran an exhibition on "The Art of Video Games". However, the reviews of the exhibit were mixed, including questioning whether video games belong in an art museum.

</doc>
<doc id="5367" url="https://en.wikipedia.org/wiki?curid=5367" title="Cambrian">
Cambrian

The Cambrian ( or ) was the first geological period of the Paleozoic Era, lasting from to million years ago (mya) and was succeeded by the Ordovician. Its subdivisions, and indeed its base, are somewhat in flux. The period was established (as “Cambrian series”) by Adam Sedgwick, who named it after Cambria, the Latinised form of "Cymru", the Welsh name for Wales, where Britain's Cambrian rocks are best exposed. The Cambrian is unique in its unusually high proportion of lagerstätte sedimentary deposits. These are sites of exceptional preservation, where "soft" parts of organisms are preserved as well as their more resistant shells. This means that our understanding of the Cambrian biology surpasses that of some later periods.
The Cambrian marked a profound change in life on Earth; prior to the Cambrian, the majority of living organisms on the whole were small, unicellular and simple; the Precambrian "Charnia" being exceptional. Complex, multicellular organisms gradually became more common in the millions of years immediately preceding the Cambrian, but it was not until this period that mineralized—hence readily fossilized—organisms became common. The rapid diversification of lifeforms in the Cambrian, known as the Cambrian explosion, produced the first representatives of all modern animal phyla. Phylogenetic analysis has supported the view that during the Cambrian radiation, metazoa (animals) evolved monophyletically from a single common ancestor: flagellated colonial protists similar to modern choanoflagellates.
Although diverse life forms prospered in the oceans, the land was comparatively barren—with nothing more complex than a microbial soil crust and a few molluscs that emerged to browse on the microbial biofilm. Most of the continents were probably dry and rocky due to a lack of vegetation. Shallow seas flanked the margins of several continents created during the breakup of the supercontinent Pannotia. The seas were relatively warm, and polar ice was absent for much of the period.
The United States Federal Geographic Data Committee uses a "barred capital C" character similar to the capital letter Ukrainian Ye to represent the Cambrian Period.
The proper Unicode character is .
Stratigraphy.
Despite the long recognition of its distinction from younger Ordovician rocks and older Precambrian rocks, it was not until 1994 that this time period was internationally ratified. The base of the Cambrian is defined on a complex assemblage of trace fossils known as the "Treptichnus pedum" assemblage.
Nevertheless, the usage of "Treptichnus pedum", a reference ichnofossil for the lower boundary of the Cambrian, for the stratigraphic detection of this boundary is always risky because of occurrence of very similar trace fossils belonging to the Treptichnids group well below the "T. pedum" in Namibia, Spain and Newfoundland, and possibly, in the western USA. The stratigraphic range of "T. pedum" overlaps the range of the Ediacaran fossils in Namibia, and probably in Spain.
Subdivisions.
The Cambrian period followed the Ediacaran and was followed by the Ordovician period. The Cambrian is divided into four epochs or series and ten ages or stages. Currently only two series and five stages are named and have a GSSP.
Because the international stratigraphic subdivision is not yet complete, many local subdivisions are still widely used. In some of these subdivisions the Cambrian is divided into three epochs with locally differing names – the Early Cambrian (Caerfai or Waucoban, mya), Middle Cambrian (St Davids or Albertan, mya) and Furongian ( mya; also known as Late Cambrian, Merioneth or Croixan). Rocks of these epochs are referred to as belonging to the Lower, Middle, or Upper Cambrian.
Trilobite zones allow biostratigraphic correlation in the Cambrian.
Each of the local epochs is divided into several stages. The Cambrian is divided into several regional faunal stages of which the Russian-Kazakhian system is most used in international parlance:
Cambrian dating.
The time range for the Cambrian has classically been thought to have been from about 541 million years ago (mya) to about 485 mya. The lower boundary of the Cambrian was traditionally set at the earliest appearance of trilobites and also unusual forms known as archeocyathids (literally "ancient cup") that are thought to be the earliest sponges and also the first non-microbial reef builders.
The end of the period was eventually set at a fairly definite faunal change now identified as an extinction event. Fossil discoveries and radiometric dating in the last quarter of the 20th century have called these dates into question. Date inconsistencies as large as 20 million years are common between authors. Framing dates of "ca." 545 to 490 mya were proposed by the International Subcommission on Global Stratigraphy as recently as 2002.
A more precise date of 541 ± 0.3 mya for the extinction event at the beginning of the Cambrian has recently been submitted. The rationale for this precise dating is interesting in itself as an example of paleological deductive reasoning. Exactly at the Cambrian boundary there is a marked fall in the abundance of carbon-13, a "reverse spike" that paleontologists call an "excursion". It is so widespread that it is the best indicator of the position of the Precambrian-Cambrian boundary in stratigraphic sequences of roughly this age. One of the places that this well-established carbon-13 excursion occurs is in Oman. Amthor (2003) describes evidence from Oman that indicates the carbon-isotope excursion relates to a mass extinction: the disappearance of distinctive fossils from the Precambrian coincides exactly with the carbon-13 anomaly. Fortunately, in the Oman sequence, so too does a volcanic ash horizon from which zircons provide a very precise age of 541 ± 0.3 mya (calculated on the decay rate of uranium to lead). This new and precise date tallies with the less precise dates for the carbon-13 anomaly, derived from sequences in Siberia and Namibia.
Paleogeography.
Plate reconstructions suggest a global supercontinent, Pannotia, was in the process of breaking up early in the period, with Laurentia (North America), Baltica, and Siberia having separated from the main supercontinent of Gondwana to form isolated land masses. Most continental land was clustered in the Southern Hemisphere at this time, but was gradually drifting north. Large, high-velocity rotational movement of Gondwana appears to have occurred in the Early Cambrian.
With a lack of sea ice – the great glaciers of the Marinoan Snowball Earth were long melted – the sea level was high, which led to large areas of the continents being flooded in warm, shallow seas ideal for thriving life. The sea levels fluctuated somewhat, suggesting there were 'ice ages', associated with pulses of expansion and contraction of a south polar ice cap.
Climate.
The Earth was generally cold during the early Cambrian, probably due to the ancient continent of Gondwana covering the South Pole and cutting off polar ocean currents. There were likely polar ice caps and a series of glaciations, as the planet was still recovering from an earlier Snowball Earth. It became warmer towards the end of the period; the glaciers receded and eventually disappeared, and sea levels rose dramatically. This trend would continue into the Ordovician period.
Flora.
Although there were a variety of macroscopic marine plants (e.g. "Margaretia" and "Dalyia"), no true land plant (embryophyte) fossils are known from the Cambrian. However, biofilms and microbial mats were well developed on Cambrian tidal flats and beaches., and further inland were a variety of lichens, fungi and microbes forming microbial earth ecosystems, comparable with modern soil crust of desert regions, contributing to soil formation.
Oceanic Life.
Most animal life during the Cambrian was aquatic.
Trilobites were once assumed to be the dominant life form, but this has proven to be incorrect. Arthropods in general were by far the most dominating animals in the ocean, but at the time, trilobites were only a minor part of the total arthropod diversity. What made them different from their relatives was their heavy armor, which fossilized far more easily than the fragile exoskeleton of other arthropods, leaving behind numerous preserved remains which give the false impression that they were the most abundant part of the fauna.
The period marked a steep change in the diversity and composition of Earth's biosphere. The incumbent Ediacaran biota suffered a mass extinction at the base of the period, which corresponds to an increase in the abundance and complexity of burrowing behaviour. This behaviour had a profound and irreversible effect on the substrate which transformed the seabed ecosystems. Before the Cambrian, the sea floor was covered by microbial mats. By the end of the period, burrowing animals had destroyed the mats through bioturbation, and gradually turned the seabeds into what they are today. As a consequence, many of those organisms that were dependent on the mats became extinct, while the other species adapted to the changed environment that now offered new ecological niches.
Around the same time there was a seemingly rapid appearance of representatives of all the mineralized phyla except the Bryozoa, which appear in the Lower Ordovician. However, many of these phyla were represented only by stem-group forms; and since mineralized phyla generally have a benthic origin, they may not be a good proxy for (more abundant) non-mineralized phyla.
While the early Cambrian showed such diversification that it has been named the Cambrian Explosion, this changed later in the period, when it was exposed to a sharp drop in biodiversity. About 515 million years ago, the number of species going extinct exceeded the amount of new species appearing. Five million years later, the number of genera had dropped from an earlier peak of about 600 to just 450. Also the speciation rate in many groups was reduced to between a fifth and a third of previous levels. 500 million years ago, oxygen levels fell dramatically in the oceans, leading to hypoxia, while the levels of poisonous hydrogen sulfide simultaneously increased, causing another extinction. The later half of Cambrian was surprisingly barren and show evidence of several rapid extinction events; the stromatolites which had been replaced by reef building sponges known as Archaeocyatha, returned once more as the archaeocyathids became extinct. This declining trend did not change before the Ordovician.
Some Cambrian organisms ventured onto land, producing the trace fossils "Protichnites" and "Climactichnites". Fossil evidence suggests that euthycarcinoids, an extinct group of arthropods, produced at least some of the "Protichnites". Fossils of the maker of "Climactichnites" have not been found; however, fossil trackways and resting traces suggest a large, slug-like mollusk.
In contrast to later periods, the Cambrian fauna was somewhat restricted; free-floating organisms were rare, with the majority living on or close to the sea floor; and mineralizing animals were rarer than in future periods, in part due to the unfavourable ocean chemistry.
Many modes of preservation are unique to the Cambrian, resulting in an abundance of "Lagerstätten".

</doc>
<doc id="5370" url="https://en.wikipedia.org/wiki?curid=5370" title="Category of being">
Category of being

In ontology, the different kinds or ways of being are called categories of being or simply categories. To investigate the categories of being is to determine the most fundamental and the broadest classes of entities. A distinction between such categories, in making the categories or applying them, is called an ontological distinction.
Introduction.
The categories of being, or simply "the categories", are defined as the highest classes under which all elements of being whether material or conceptual can be classified. These categories belong to the realm of philosophy and the difference between categories and classes was described by the philosopher C.I. Lewis as that of a hierarchical tree or pyramid where the most general categories such as those of logic are to be found at the top and the least general classes such as species of animal at the bottom. There are therefore two main areas of interest (i) at the top of the tree - how being first divides into discrete or overlapping subentities, and (ii) at the bottom of the tree - how the different elements can be correlated into higher classes. The structure may consist of a simple list such as the one produced by Aristotle or it may be composed of headings and subheadings such as the tables produced by Immanuel Kant. The elements of being are commonly seen as "things", whether objects or concepts, but most systems will also include as elements the relations between the objects and concepts. The distinction is also made between the elements themselves and the words used to denote such elements. The word "category" itself is derived from the Greek "κατηγορία (katigoría)", meaning to predicate, and therefore the categories may also be thought of as kinds of predicate which may be applied to any particular subject or element, and by extension to the concept of being itself.
If we take any subject and with it form a sentence "the subject is…" then in a valid system of categorisation all the different things we can say about the subject should be classifiable under one of the categories within the system. Aristotle listed ten categories amongst which we find, for example, the three categories of Substance, Quality and Quantity. In Heidegger’s example "This is a house. It is both red and tall" the word "house" can be classified under Substance, "red" under Quality and "tall" under Quantity. The subject, the house, gathers around it what was called in the 19th century a "colligation of concepts" or in the 20th century a "bundle of properties" all of which serve to define the house. By extension we can say that all being consists of nothing but Substance, Quality, Quantity and the rest because nothing else can be said of the subject. Categorisation has raised many problems throughout the history of philosophy, including those of the number and types of category, how the categories interrelate with one another and whether they are real in some way or just mental constructs, and to introduce the many different solutions that have arisen it is worth considering the history of the categories in brief outline.
Early development.
The process of abstraction required to discover the number and names of the categories has been undertaken by many philosophers since Aristotle and involves the careful inspection of each concept to ensure that there is no higher category or categories under which that concept could be subsumed. The scholars of the twelfth and thirteenth centuries developed Aristotle’s ideas, firstly, for example by Gilbert of Poitiers, dividing Aristotle's ten categories into two sets, primary and secondary, according to whether they inhere in the subject or not:
Secondly, following Porphyry’s likening of the classificatory hierarchy to a tree, they concluded that the major classes could be subdivided to form subclasses, for example, Substance could be divided into Genus and Species, and Quality could be subdivided into Property and Accident, depending on whether the property was necessary or contingent. An alternative line of development was taken by Plotinus in the second century who by a process of abstraction reduced Aristotle’s list of ten categories to five: Substance, Relation, Quantity, Motion and Quality. Plotinus further suggested that the latter three categories of his list, namely Quantity, Motion and Quality correspond to three different kinds of relation and that these three categories could therefore be subsumed under the category of Relation. This was to lead to the supposition that there were only two categories at the top of the hierarchical tree, namely Substance and Relation, and if relations only exist in the mind as many supposed, to the two highest categories, Mind and Matter, reflected most clearly in the dualism of René Descartes.
Modern development.
An alternative conclusion however began to be formulated in the eighteenth century by Immanuel Kant who realised that we can say nothing about Substance except through the relation of the subject to other things. In the sentence "This is a house" the substantive subject "house" only gains meaning in relation to human use patterns or to other similar houses. The category of Substance disappears from Kant’s tables, and under the heading of Relation, Kant lists "inter alia" the three relationship types of Disjunction, Causality and Inherence. The three older concepts of Quantity, Motion and Quality, as Peirce discovered, could be subsumed under these three broader headings in that Quantity relates to the subject through the relation of Disjunction; Motion relates to the subject through the relation of Causality; and Quality relates to the subject through the relation of Inherence. Sets of three continued to play an important part in the nineteenth century development of the categories, most notably in G.W.F.Hegel’s extensive tabulation of categories, and in C.S.Peirce’s categories set out in his work on the logic of relations. One of Peirce’s contributions was to call the three primary categories Firstness, Secondness and Thirdness which both emphasises their general nature, and avoids the confusion of having the same name for both the category itself and for a concept within that category.
In a separate development, and building on the notion of primary and secondary categories introduced by the Scholastics, Kant introduced the idea that secondary or "derivative" categories could be derived from the primary categories through the combination of one primary category with another. This would result in the formation of three secondary categories: the first, "Community" was an example that Kant gave of such a derivative category; the second, "Modality", introduced by Kant, was a term which Hegel, in developing Kant’s dialectical method, showed could also be seen as a derivative category; and the third, "Spirit" or "Will" were terms that Hegel and Schopenhauer were developing separately for use in their own systems. Karl Jaspers in the twentieth century, in his development of existential categories, brought the three together, allowing for differences in terminology, as Substantiality, Communication and Will. This pattern of three primary and three secondary categories was used most notably in the nineteenth century by Peter Mark Roget to form the six headings of his Thesaurus of English Words and Phrases. The headings used were the three objective categories of Abstract Relation, Space (including Motion) and Matter and the three subjective categories of Intellect, Feeling and Volition, and he found that under these six headings all the words of the English language, and hence any possible predicate, could be assembled.
Twentieth century development.
In the twentieth century the primacy of the division between the subjective and the objective, or between mind and matter, was disputed by, among others, Bertrand Russell and Gilbert Ryle. Philosophy began to move away from the metaphysics of categorisation towards the linguistic problem of trying to differentiate between, and define, the words being used. Ludwig Wittgenstein’s conclusion was that there were no clear definitions which we can give to words and categories but only a "halo" or "corona" of related meanings radiating around each term. Gilbert Ryle thought the problem could be seen in terms of dealing with "a galaxy of ideas" rather than a single idea, and suggested that category mistakes are made when a concept (e.g. "university"), understood as falling under one category (e.g. abstract idea), is used as though it falls under another (e.g. physical object). With regard to the visual analogies being used, Peirce and Lewis, just like Plotinus earlier, likened the terms of propositions to points, and the relations between the terms to lines. Peirce, taking this further, talked of univalent, bivalent and trivalent relations linking predicates to their subject and it is just the number and types of relation linking subject and predicate that determine the category into which a predicate might fall. Primary categories contain concepts where there is one dominant kind of relation to the subject. Secondary categories contain concepts where there are two dominant kinds of relation. Examples of the latter were given by Heidegger in his two propositions "the house is on the creek" where the two dominant relations are spatial location (Disjunction) and cultural association (Inherence), and "the house is eighteenth century" where the two relations are temporal location (Causality) and cultural quality (Inherence). A third example may be inferred from Kant in the proposition "the house is impressive or sublime" where the two relations are spatial or mathematical disposition (Disjunction) and dynamic or motive power (Causality). Both Peirce and Wittgenstein introduced the analogy of colour theory in order to illustrate the shades of meanings of words. Primary categories, like primary colours, are analytical representing the furthest we can go in terms of analysis and abstraction and include Quantity, Motion and Quality. Secondary categories, like secondary colours, are synthetic and include concepts such as Substance, Community and Spirit.
Categorical distinctions.
The common or dominant ways to view categories as of the end of the 20th century.
Any of these ways can be criticized for...
In process philosophy, this last is the only possibility, but historically philosophers have been loath to conclude that nothing exists but process.
Categorization of existence.
As bundles of properties.
Bundle theory is an ontological theory about objecthood proposed by the 18th century Scottish philosopher David Hume, which states that objects only subsist as a collection (bundle) of properties, relations or tropes. In an "epistemological" sense, bundle theory says that all that can be known about objects are the properties which they are composed of, and that these properties are all that can be truly said to exist.
For example, if we take the concept of a black square, bundle theory would suggest that all that can be said to exist are the properties of a black square.
The properties of a black square are: Black, Regular, and Quadrilateral. 
However, from these properties alone, we cannot deduce any kind of underlying essence of a "black square", or some object called a "black square", except as a bundle of properties which constitute the object that we then go on to label as a ""black square"", but the object "itself" is really nothing more than a system of relations (or bundle) of properties. To defend this, Hume asks us to imagine an object without properties, if we strip the black square of its properties (being black, regular and quadrilateral) we end up reducing the object to non-existence.
Intuition as evasion.
A seemingly simpler way to view categories is as arising only from intuition. Philosophers argue this evades the issue. What it means to take the category physical object seriously as a category of being is to assert that the concept of physical objecthood cannot be reduced to or explicated in any other terms—not, for example, in terms of bundles of properties but only in terms of other items in that category.
In this way, many ontological controversies can be understood as controversies about exactly which categories should be seen as fundamental, irreducible, or primitive. To refer to intuition as the source of distinctions and thus categories doesn't resolve this.
Ideology, dogma, and theory.
Modern theories give weight to intuition, perceptually observed properties, comparisons of categories among persons, and the direction of investigation towards known specified ends, to determine what humanity in its present state of being needs to consider irreducible. They seek to explain why certain beliefs about categories would appear in political science as ideology, in religion as dogma, or in science as theory.
As metaphors.
A set of ontological distinctions related by a single conceptual metaphor was called an ontological metaphor by George Lakoff and Mark Johnson, who claimed that such metaphors arising from experience were more basic than any properties or symbol-based comparisons. Their cognitive science of mathematics was a study of the embodiment of basic symbols and properties including those studied in the philosophy of mathematics, via embodied philosophy, using cognitive science. This theory comes after several thousand years of inquiry into patterns and cognitive bias of humanity.
Categories of being.
Philosophers have many differing views on what the fundamental categories of being are. In no particular order, here are at least "some" items that have been regarded as categories of being by someone or other:
Physical objects.
Physical objects are beings; certainly they are said to "be" in the simple sense that they "exist" all around us. So a house is a being, a person's body is a being, a tree is a being, a cloud is a being, and so on. They are beings because, and in the sense that, they are physical objects. One might also call them bodies, or physical particulars, or concrete things, or matter, or maybe substances (but bear in mind the word 'substance' has some special philosophical meanings).
Minds.
Minds—those "parts" of us that think and perceive—are considered beings by some philosophers. Each of us, according to common sense anyway, "has" a mind. Of course, philosophers rarely just assume that minds occupy a different category of beings from physical objects. Some, like René Descartes, have thought that this is so (this view is known as dualism, and functionalism also considers the mind as distinct from the body), while others have thought that concepts of the mental can be reduced to physical concepts (this is the view of physicalism or materialism). Still others maintain though "mind" is a noun, it is not necessarily the "name of a thing" distinct within the whole person. In this view the relationship between mental properties and physical properties is one of supervenience – similar to how "banks" supervene upon certain buildings.
Classes.
We can talk about all human beings, and the planets, and all engines as belonging to classes. Within the class of human beings are all of the human beings, or the extension of the term 'human being'. In the class of planets would be Mercury, Venus, the Earth, and all the other planets that there might be in the universe. Classes, in addition to each of their members, are often taken to be beings. Surely we can say that in some sense, the class of planets "is", or has being. Classes are usually taken to be abstract objects, like sets; 'class' is often regarded as equivalent, or nearly equivalent, in meaning to 'set'. Denying that classes and sets exist is the contemporary meaning of nominalism.
Properties.
The redness of a red apple, or more to the point, the redness all red things share, is a "property". One could also call it an "attribute" of the apple. Very roughly put, a property is just a quality that describes an object. This will not do as a definition of the word 'property' because, like 'attribute', 'quality' is a near-synonym of 'property'. But these synonyms can at least help us to get a fix on the concept we are talking about. Whenever one talks about the size, color, weight, composition, and so forth, of an object, one is talking about the properties of that object. Some—though this is a point of severe contention in the problem of universals—believe that properties are beings; the redness of all apples is something that "is." To deny that universals exist is the scholastic variant of nominalism.
Note that the color red is an objective property of an object. The intrinsic property is that it reflects radiation (including light) in a certain way. A human perceives that as the color red in his or her brain. An object thus has two types of properties, intrinsic (physical) and objective (observer specific).
Relations.
An apple sitting on a table is in a relation to the table it sits on. So we can say that there is a relation between the apple and the table: namely, the relation of sitting-on. So, some say, we can say that that relation has being. For another example, the Washington Monument is taller than the White House. Being-taller-than is a relation between the two structures. We can say that "that" relation has being as well. This, too, is a point of contention in the problem of universals.
Space and time.
Space and time are what physical objects are extended into. There is debate as to whether time exists only in the present or whether far away times are just as real as far away spaces, and there is debate (among who?) as to whether space is curved. Many (nearly all?) contemporary thinkers actually suggest that time is the fourth dimension, thus reducing space and time to one distinct ontological entity, the space-time continuum.
Propositions.
Propositions are units of meaning. They should not be confused with declarative sentences, which are just sets of words in languages that refer to propositions. Declarative sentences, ontologically speaking, are thus ideas, a property of substances (minds), rather than a distinct ontological category. For instance, the English declarative sentence "snow is white" refers to the same proposition as the equivalent French declarative sentence "la neige est blanche"; two sentences, one proposition. Similarly, one declarative sentence can refer to many propositions; for instance, "I am hungry" changes meaning (i.e. refers to different propositions) depending on the person uttering it.
Events.
Events are that which can be said to occur. To illustrate, consider the claim "John went to a ballgame"; if true, then we must ontologically account for every entity in the sentence. "John" refers to a substance. But what does "went to a ballgame" refer to? It seems wrong to say that "went to a ballgame" is a property that instantiates John, because "went to a ballgame" does not seem to be the same ontological kind of thing as, for instance, redness. Thus, events arguably deserve their own ontological category.
Properties, relations, and classes are supposed to be "abstract," rather than "concrete." Many philosophers say that properties and relations have an abstract existence, and that physical objects have a concrete existence. That, perhaps, is the paradigm case of a difference in ways in which items can be said to "be," or to have being.
Many philosophers have attempted to reduce the number of distinct ontological categories. For instance, David Hume famously regarded Space and Time as nothing more than psychological facts about human beings, which would effectively reduce Space and Time to ideas, which are properties of humans (substances). Nominalists and realists argue over the existence of properties and relations. Finally, events and propositions have been argued to be reducible to sets (classes) of substances and other such categories.
History.
Aristotle.
One of Aristotle’s early interests lay in the classification of the natural world, how for example the genus "animal" could be first divided into "two-footed animal" and then into "wingless, two-footed animal". He realised that the distinctions were being made according to the qualities the animal possesses, the quantity of its parts and the kind of motion that it exhibits. To fully complete the proposition "this animal is…" Aristotle stated in his work on the Categories that there were ten kinds of predicate where...
 "…each signifies either substance or quantity or quality or relation or where or when or being-in-a-position or having or acting or being acted upon".
 He realised that predicates could be simple or complex. The simple kinds consist of a subject and a predicate linked together by the "categorical" or inherent type of relation. For Aristotle the more complex kinds were limited to propositions where the predicate is compounded of two of the above categories for example "this is a horse running". More complex kinds of proposition were only discovered after Aristotle by the Stoic, Chrysippus, who developed the "hypothetical" and "disjunctive" types of syllogism and these were terms which were to be developed through the Middle Ages and were to reappear in Kant’s system of categories.
"Category" came into use with Aristotle's essay "Categories", in which he discussed univocal and equivocal terms, predication, and ten categories:
Plotinus.
Plotinus in writing his "Enneads" around AD 250 recorded that "philosophy at a very early age investigated the number and character of the existents… some found ten, others less…. to some the genera were the first principles, to others only a generic classification of existents". He realised that some categories were reducible to others saying "why are not Beauty, Goodness and the virtues, Knowledge and Intelligence included among the primary genera?" He concluded that such transcendental categories and even the categories of Aristotle were in some way posterior to the three Eleatic categories first recorded in Plato's dialogue "Parmenides" and which comprised the following three coupled terms: 
 Plotinus called these "the hearth of reality" deriving from them not only the three categories of Quantity, Motion and Quality but also what came to be known as "the three moments of the Neoplatonic world process":
 Plotinus likened the three to the centre, the radii and the circumference of a circle, and clearly thought that the principles underlying the categories were the first principles of creation. "From a single root all being multiplies". Similar ideas were to be introduced into Early Christian thought by, for example, Gregory of Nazianzus who summed it up saying "Therefore Unity, having from all eternity arrived by motion at duality, came to rest in trinity".
Kant.
In the "Critique of Pure Reason" (1781), Immanuel Kant argued that the categories are part of our own mental structure and consist of a set of "a priori" concepts through which we interpret the world around us. These concepts correspond to twelve logical functions of the understanding which we use to make judgements and there are therefore two tables given in the "Critique", one of the Judgements and a corresponding one for the Categories. To give an example, the logical function behind our reasoning from ground to consequence (based on the Hypothetical relation) underlies our understanding of the world in terms of cause and effect (the Causal relation). In each table the number twelve arises from, firstly, an initial division into two: the Mathematical and the Dynamical; a second division of each of these headings into a further two: Quantity and Quality, and Relation and Modality respectively; and, thirdly, each of these then divides into a further three subheadings as follows.
Table of Judgements
Mathematical
Dynamical
Table of Categories
Mathematical
Dynamical
Criticism of Kant’s system followed, firstly, by Arthur Schopenhauer, who amongst other things was unhappy with the term "Community", and declared that the tables "do open violence to truth, treating it as nature was treated by old-fashioned gardeners", and secondly, by W.T.Stace who in his book "The Philosophy of Hegel" suggested that in order to make Kant’s structure completely symmetrical a third category would need to be added to the Mathematical and the Dynamical. This, he said, Hegel was to do with his category of Notion.
Hegel.
G.W.F. Hegel in his "Science of Logic" (1812) attempted to provide a more comprehensive system of categories than Kant and developed a structure that was almost entirely triadic. So important were the categories to Hegel that he claimed "the first principle of the world, the Absolute, is a system of categories… the categories must be the reason of which the world is a consequent".
 Using his own logical method of combination, later to be called the Hegelian dialectic, of arguing from thesis through antithesis to synthesis, he arrived, as shown in W.T.Stace's work cited, at a hierarchy of some 270 categories. The three very highest categories were Logic, Nature and Spirit. The three highest categories of Logic, however, he called Being, Essence and Notion which he explained as follows:
Schopenhauer’s category that corresponded with Notion was that of Idea, which in his ""Four-Fold Root of Sufficient Reason"" he complemented with the category of the Will. The title of his major work was "The World as Will and Idea". The two other complementary categories, reflecting one of Hegel’s initial divisions, were those of Being and Becoming. Interestingly, at around the same time, Goethe was developing his colour theories in the "Farbenlehre" of 1810, and introduced similar principles of combination and complementation, symbolising, for Goethe, "the primordial relations which belong both to nature and vision". Hegel in his "Science of Logic" accordingly asks us to see his system not as a tree but as a circle.
Peirce.
Charles Sanders Peirce, who had read Kant and Hegel closely, and who also had some knowledge of Aristotle, proposed a system of merely three phenomenological categories: Firstness, Secondness, and Thirdness, which he repeatedly invoked in his subsequent writings. Like Hegel, C.S.Peirce attempted to develop a system of categories from a single indisputable principle, in Peirce’s case the notion that in the first instance he could only be aware of his own ideas. 
 "It seems that the true categories of consciousness are first, feeling… second, a sense of resistance… and third, synthetic consciousness, or thought".
 Elsewhere he called the three primary categories: Quality, Reaction and Meaning, and even Firstness, Secondness and Thirdness, saying, "perhaps it is not right to call these categories conceptions, they are so intangible that they are rather tones or tints upon conceptions": 
 Although Peirce’s three categories correspond to the three concepts of relation given in Kant’s tables, the sequence is now reversed and follows that given by Hegel, and indeed before Hegel of the three moments of the world-process given by Plotinus. Later, Peirce gave a mathematical reason for there being three categories in that although monadic, dyadic and triadic nodes are irreducible, every node of a higher valency is reducible to a "compound of triadic relations". Ferdinand de Saussure, who was developing "semiology" in France just as Peirce was developing "semiotics" in the USA, likened each term of a proposition to "the centre of a constellation, the point where other coordinate terms, the sum of which is indefinite, converge".
Others.
Edmund Husserl (1962, 2000) wrote extensively about categorial systems as part of his phenomenology.
For Gilbert Ryle (1949), a category (in particular a "category mistake") is an important semantic concept, but one having only loose affinities to an ontological category.
Contemporary systems of categories have been proposed by John G. Bennett (The Dramatic Universe, 4 vols., 1956–65), Wilfrid Sellars (1974), Reinhardt Grossmann (1983, 1992), Johansson (1989), Hoffman and Rosenkrantz (1994), Roderick Chisholm (1996), Barry Smith (ontologist) (2003), and Jonathan Lowe (2006).

</doc>
<doc id="5371" url="https://en.wikipedia.org/wiki?curid=5371" title="Concrete">
Concrete

Concrete is a composite material composed of coarse aggregate bonded together with a fluid cement which hardens over time. Most concretes used are lime-based concretes such as Portland cement concrete or concretes made with other hydraulic cements, such as ciment fondu. However, road surfaces are also a type of concrete, asphalt concrete, where the cement material is bitumen, and polymer concretes are sometimes used where the cementing material is a polymer.
In Portland cement concrete (and other hydraulic cement concretes), when the aggregate is mixed together with the dry cement and water, they form a fluid mass that is easily molded into shape. The cement reacts chemically with the water and other ingredients to form a hard matrix which binds all the materials together into a durable stone-like material that has many uses. Often, additives (such as pozzolans or superplasticizers) are included in the mixture to improve the physical properties of the wet mix or the finished material. Most concrete is poured with reinforcing materials (such as rebar) embedded to provide tensile strength, yielding reinforced concrete.
Famous concrete structures include the Hoover Dam, the Panama Canal and the Roman Pantheon. The earliest large-scale users of concrete technology were the ancient Romans, and concrete was widely used in the Roman Empire. The Colosseum in Rome was built largely of concrete, and the concrete dome of the Pantheon is the world's largest unreinforced concrete dome. Today, large concrete structures (for example, dams and multi-storey car parks) are usually made with reinforced concrete.
After the Roman Empire collapsed, use of concrete became rare until the technology was redeveloped in the mid-18th century. Today, concrete is the most widely used man-made material (measured by tonnage).
History.
The word concrete comes from the Latin word ""concretus"" (meaning compact or condensed), the perfect passive participle of ""concrescere"", from ""con"-" (together) and ""crescere"" (to grow).
Prehistory.
Perhaps the earliest known occurrence of cement was twelve million years ago. A deposit of cement was formed after an occurrence of oil shale located adjacent to a bed of limestone burned due to natural causes. These ancient deposits were investigated in the 1960s and 1970s.
On a human timescale, small usages of concrete go back for thousands of years. Concrete-like materials were used since 6500BC by the Nabataea traders or Bedouins who occupied and controlled a series of oases and developed a small empire in the regions of southern Syria and northern Jordan. They discovered the advantages of hydraulic lime, with some self-cementing properties, by 700 BC. They built kilns to supply mortar for the construction of rubble-wall houses, concrete floors, and underground waterproof cisterns. The cisterns were kept secret and were one of the reasons the Nabataea were able to thrive in the desert. Some of these structures survive to this day.
Classical era.
In both Roman and Egyptian times it was re-discovered that adding volcanic ash to the mix allowed it to set underwater. Similarly, the Romans knew that adding horse hair made concrete less liable to crack while it hardened, and adding blood made it more frost-resistant. Crystallization of strätlingite and the introduction of pyro-clastic clays creates further fracture resistance.
German archaeologist Heinrich Schliemann found concrete floors, which were made of lime and pebbles, in the royal palace of Tiryns, Greece, which dates roughly to 1400-1200 BC. Lime mortars were used in Greece, Crete, and Cyprus in 800 BC. The Assyrian Jerwan Aqueduct (688 BC) made use of waterproof concrete. Concrete was used for construction in many ancient structures.
The Romans used concrete extensively from 300 BC to 476 AD, a span of more than seven hundred years. During the Roman Empire, Roman concrete (or "opus caementicium") was made from quicklime, pozzolana and an aggregate of pumice. Its widespread use in many Roman structures, a key event in the history of architecture termed the Roman Architectural Revolution, freed Roman construction from the restrictions of stone and brick material and allowed for revolutionary new designs in terms of both structural complexity and dimension.
Concrete, as the Romans knew it, was a new and revolutionary material. Laid in the shape of arches, vaults and domes, it quickly hardened into a rigid mass, free from many of the internal thrusts and strains that troubled the builders of similar structures in stone or brick.
Modern tests show that "opus caementicium" had as much compressive strength as modern Portland-cement concrete (ca. ). However, due to the absence of reinforcement, its tensile strength was far lower than modern reinforced concrete, and its mode of application was also different:
Modern structural concrete differs from Roman concrete in two important details. First, its mix consistency is fluid and homogeneous, allowing it to be poured into forms rather than requiring hand-layering together with the placement of aggregate, which, in Roman practice, often consisted of rubble. Second, integral reinforcing steel gives modern concrete assemblies great strength in tension, whereas Roman concrete could depend only upon the strength of the concrete bonding to resist tension.
The widespread use of concrete in many Roman structures ensured that many survive to the present day. The Baths of Caracalla in Rome are just one example. Many Roman aqueducts and bridges such as the magnificent Pont du Gard have masonry cladding on a concrete core, as does the dome of the Pantheon.
Middle Ages.
After the Roman Empire, the use of burned lime and pozzolana was greatly reduced until the technique was all but forgotten between 500 and the 14th century. From the 14th century to the mid-18th century, the use of cement gradually returned. The "Canal du Midi" was built using concrete in 1670, and there are concrete structures in Finland that date from the 16th century.
Industrial era.
Perhaps the greatest driver behind the modern usage of concrete was Smeaton's Tower, the third Eddystone Lighthouse in Devon, England. To create this structure, between 1756 and 1759, British engineer John Smeaton pioneered the use of hydraulic lime in concrete, using pebbles and powdered brick as aggregate.
A method for producing Portland cement was patented by Joseph Aspdin on 1824.
Reinforced concrete was invented in 1849 by Joseph Monier.
In 1889 the first concrete reinforced bridge was built, and the first large concrete dams were built in 1936, Hoover Dam and Grand Coulee Dam.
Composition of concrete.
There are many types of concrete available, created by varying the proportions of the main ingredients below. In this way or by substitution for the cementitious and aggregate phases, the finished product can be tailored to its application with varying strength, density, or chemical and thermal resistance properties.
Aggregate consists of large chunks of material in a concrete mix, generally a coarse gravel or crushed rocks such as limestone, or granite, along with finer materials such as sand.
Cement, most commonly Portland cement, is associated with the general term "concrete." A range of materials can be used as the cement in concrete. One of the most familiar of these alternative cements is asphalt concrete. Other cementitious materials such as fly ash and slag cement, are sometimes added as mineral admixtures (see below) - either pre-blended with the cement or directly as a concrete component - and become a part of the binder for the aggregate.
To produce concrete from most cements (excluding asphalt), water is mixed with the dry powder and aggregate, which produces a semi-liquid that workers can shape, typically by pouring it into a form. The concrete solidifies and hardens through a chemical process called hydration. The water reacts with the cement, which bonds the other components together, creating a robust stone-like material.
Chemical admixtures are added to achieve varied properties. These ingredients may accelerate or slow down the rate at which the concrete hardens, and impart many other useful properties including increased tensile strength, entrainment of air, and/or water resistance.
Reinforcement is often included in concrete. Concrete can be formulated with high compressive strength, but always has lower tensile strength. For this reason it is usually reinforced with materials that are strong in tension, often steel.
Mineral admixtures are becoming more popular in recent decades. The use of recycled materials as concrete ingredients has been gaining popularity because of increasingly stringent environmental legislation, and the discovery that such materials often have complementary and valuable properties. The most conspicuous of these are fly ash, a by-product of coal-fired power plants, ground granulated blast furnace slag, and silica fume, a byproduct of industrial electric arc furnaces. The use of these materials in concrete reduces the amount of resources required, as the mineral admixtures act as a partial cement replacement. This displaces some cement production, an energetically expensive and environmentally problematic process, while reducing the amount of industrial waste that must be disposed of. Mineral admixtures can be pre-blended with the cement during its production for sale and use as a blended cement, or mixed directly with other components when the concrete is produced.
The "mix design" depends on the type of structure being built, how the concrete is mixed and delivered, and how it is placed to form the structure.
Cement.
Portland cement is the most common type of cement in general usage. It is a basic ingredient of concrete, mortar and many plasters. English masonry worker Joseph Aspdin patented Portland cement in 1824. It was named because of the similarity of its color to Portland limestone, quarried from the English Isle of Portland and used extensively in London architecture. It consists of a mixture of calcium silicates (alite, belite), aluminates and ferrites - compounds which combine calcium, silicon, aluminium and iron in forms which will react with water. Portland cement and similar materials are made by heating limestone (a source of calcium) with clay and/or shale (a source of silicon, aluminium and iron) and grinding this product (called "clinker") with a source of sulfate (most commonly gypsum).
In modern cement kilns many advanced features are used to lower the fuel consumption per ton of clinker produced. Cement kilns are extremely large, complex, and inherently dusty industrial installations, and have emissions which must be controlled. Of the various ingredients used to produce a given quantity of concrete, the cement is the most energetically expensive. Even complex and efficient kilns require 3.3 to 3.6 gigajoules of energy to produce a ton of clinker and then grind it into cement. Many kilns can be fueled with difficult-to-dispose-of wastes, the most common being used tires. The extremely high temperatures and long periods of time at those temperatures allows cement kilns to efficiently and completely burn even difficult-to-use fuels.
Water.
Combining water with a cementitious material forms a cement paste by the process of hydration. The cement paste glues the aggregate together, fills voids within it, and makes it flow more freely.
A lower water-to-cement ratio yields a stronger, more durable concrete, whereas more water gives a freer-flowing concrete with a higher slump. Impure water used to make concrete can cause problems when setting or in causing premature failure of the structure.
Hydration involves many different reactions, often occurring at the same time. As the reactions proceed, the products of the cement hydration process gradually bond together the individual sand and gravel particles and other components of the concrete to form a solid mass.
Reaction:
Aggregates.
Fine and coarse aggregates make up the bulk of a concrete mixture. Sand, natural gravel, and crushed stone are used mainly for this purpose. Recycled aggregates (from construction, demolition, and excavation waste) are increasingly used as partial replacements for natural aggregates, while a number of manufactured aggregates, including air-cooled blast furnace slag and bottom ash are also permitted.
The size distribution of the aggregate determines how much binder is required. Aggregate with a very even size distribution has the biggest gaps whereas adding aggregate with smaller particles tends to fill these gaps. The binder must fill the gaps between the aggregate as well as pasting the surfaces of the aggregate together, and is typically the most expensive component. Thus variation in sizes of the aggregate reduces the cost of concrete. The aggregate is nearly always stronger than the binder, so its use does not negatively affect the strength of the concrete.
Redistribution of aggregates after compaction often creates inhomogeneity due to the influence of vibration. This can lead to strength gradients.
Decorative stones such as quartzite, small river stones or crushed glass are sometimes added to the surface of concrete for a decorative "exposed aggregate" finish, popular among landscape designers.
In addition to being decorative, exposed aggregate may add robustness to a concrete.
Reinforcement.
Concrete is strong in compression, as the aggregate efficiently carries the compression load. However, it is weak in tension as the cement holding the aggregate in place can crack, allowing the structure to fail. Reinforced concrete adds either steel reinforcing bars, steel fibers, glass fibers, or plastic fibers to carry tensile loads.
Chemical admixtures.
"Chemical admixtures" are materials in the form of powder or fluids that are added to the concrete to give it certain characteristics not obtainable with plain concrete mixes. In normal use, admixture dosages are less than 5% by mass of cement and are added to the concrete at the time of batching/mixing. (See the section on Concrete Production, below.)The common types of admixtures are as follows.
Mineral admixtures and blended cements.
Inorganic materials that have pozzolanic or latent hydraulic properties, these very fine-grained materials are added to the concrete mix to improve the properties of concrete (mineral admixtures), or as a replacement for Portland cement (blended cements). Products which incorporate limestone, fly ash, blast furnace slag, and other useful materials with pozzolanic properties into the mix, are being tested and used. This development is due to cement production being one of the largest producers (at about 5 to 10%) of global greenhouse gas emissions, as well as lowering costs, improving concrete properties, and recycling wastes.
Concrete production.
Concrete production is the process of mixing together the various ingredients—water, aggregate, cement, and any additives—to produce concrete. Concrete production is time-sensitive. Once the ingredients are mixed, workers must put the concrete in place before it hardens. In modern usage, most concrete production takes place in a large type of industrial facility called a concrete plant, or often a batch plant.
In general usage, concrete plants come in two main types, ready mix plants and central mix plants. A ready mix plant mixes all the ingredients except water, while a central mix plant mixes all the ingredients including water. A central mix plant offers more accurate control of the concrete quality through better measurements of the amount of water added, but must be placed closer to the work site where the concrete will be used, since hydration begins at the plant.
A concrete plant consists of large storage hoppers for various reactive ingredients like cement, storage for bulk ingredients like aggregate and water, mechanisms for the addition of various additives and amendments, machinery to accurately weigh, move, and mix some or all of those ingredients, and facilities to dispense the mixed concrete, often to a concrete mixer truck.
Modern concrete is usually prepared as a viscous fluid, so that it may be poured into forms, which are containers erected in the field to give the concrete its desired shape. There are many different ways in which concrete formwork can be prepared, such as Slip forming and Steel plate construction. Alternatively, concrete can be mixed into dryer, non-fluid forms and used in factory settings to manufacture Precast concrete products.
There is a wide variety of equipment for processing concrete, from hand tools to heavy industrial machinery. Whichever equipment builders use, however, the objective is to produce the desired building material; ingredients must be properly mixed, placed, shaped, and retained within time constraints. Any interruption in pouring the concrete can cause the initially placed material to begin to set before the next batch is added on top. This creates a horizontal plane of weakness called a "cold joint" between the two batches. Once the mix is where it should be, the curing process must be controlled to ensure that the concrete attains the desired attributes. During concrete preparation, various technical details may affect the quality and nature of the product.
When initially mixed, Portland cement and water rapidly form a gel of tangled chains of interlocking crystals, and components of the gel continue to react over time. Initially the gel is fluid, which improves workability and aids in placement of the material, but as the concrete sets, the chains of crystals join into a rigid structure, counteracting the fluidity of the gel and fixing the particles of aggregate in place. During curing, the cement continues to react with the residual water in a process of hydration. In properly formulated concrete, once this curing process has terminated the product has the desired physical and chemical properties. Among the qualities typically desired, are mechanical strength, low moisture permeability, and chemical and volumetric stability.
Mixing concrete.
Thorough mixing is essential for the production of uniform, high-quality concrete. For this reason equipment and methods should be capable of effectively mixing concrete materials containing the largest specified aggregate to produce "uniform mixtures" of the lowest slump practical for the work.
"Separate paste mixing" has shown that the mixing of cement and water into a paste before combining these materials with aggregates can increase the compressive strength of the resulting concrete. The paste is generally mixed in a "high-speed", shear-type mixer at a w/cm (water to cement ratio) of 0.30 to 0.45 by mass. The cement paste premix may include admixtures such as accelerators or retarders, superplasticizers, pigments, or silica fume. The premixed paste is then blended with aggregates and any remaining batch water and final mixing is completed in conventional concrete mixing equipment.
Workability.
"Workability" is the ability of a fresh (plastic) concrete mix to fill the form/mold properly with the desired work (vibration) and without reducing the concrete's quality. Workability depends on water content, aggregate (shape and size distribution), cementitious content and age (level of hydration) and can be modified by adding chemical admixtures, like superplasticizer. Raising the water content or adding chemical admixtures increases concrete workability. Excessive water leads to increased bleeding and/or segregation of aggregates (when the cement and aggregates start to separate), with the resulting concrete having reduced quality. The use of an aggregate with an undesirable gradation can result in a very harsh mix design with a very low slump, which cannot readily be made more workable by addition of reasonable amounts of water.
Workability can be measured by the concrete slump test, a simplistic measure of the plasticity of a fresh batch of concrete following the ASTM C 143 or EN 12350-2 test standards. Slump is normally measured by filling an "Abrams cone" with a sample from a fresh batch of concrete. The cone is placed with the wide end down onto a level, non-absorptive surface. It is then filled in three layers of equal volume, with each layer being tamped with a steel rod to consolidate the layer. When the cone is carefully lifted off, the enclosed material slumps a certain amount, owing to gravity. A relatively dry sample slumps very little, having a slump value of one or two inches (25 or 50 mm) out of one foot (305 mm). A relatively wet concrete sample may slump as much as eight inches. Workability can also be measured by the flow table test.
Slump can be increased by addition of chemical admixtures such as plasticizer or superplasticizer without changing the water-cement ratio. Some other admixtures, especially air-entraining admixture, can increase the slump of a mix.
High-flow concrete, like self-consolidating concrete, is tested by other flow-measuring methods. One of these methods includes placing the cone on the narrow end and observing how the mix flows through the cone while it is gradually lifted.
After mixing, concrete is a fluid and can be pumped to the location where needed.
Curing.
A common misconception is that concrete dries as it sets, but the opposite is true - damp concrete sets better than dry concrete. In other words, cement is "hydraulic": water allows it to gain strength. Too much water is counterproductive, but too little water is deleterious. Curing allows concrete to achieve optimal strength and hardness. Curing is the hydration process that occurs after the concrete has been placed. In chemical terms, curing allows calcium-silicate hydrate (C-S-H) to form. To gain strength and harden fully, concrete curing requires time. In around 4 weeks, typically over 90% of the final strength is reached, although strengthening may continue for decades. The conversion of calcium hydroxide in the concrete into calcium carbonate from absorption of CO2 over several decades further strengthens the concrete and makes it more resistant to damage. This carbonation reaction, however, lowers the pH of the cement pore solution and can corrode the reinforcement bars.
Hydration and hardening of concrete during the first three days is critical. Abnormally fast drying and shrinkage due to factors such as evaporation from wind during placement may lead to increased tensile stresses at a time when it has not yet gained sufficient strength, resulting in greater shrinkage cracking. The early strength of the concrete can be increased if it is kept damp during the curing process. Minimizing stress prior to curing minimizes cracking. High-early-strength concrete is designed to hydrate faster, often by increased use of cement that increases shrinkage and cracking. The strength of concrete changes (increases) for up to three years. It depends on cross-section dimension of elements and conditions of structure exploitation.
Properly curing concrete leads to increased strength and lower permeability and avoids cracking where the surface dries out prematurely. Care must also be taken to avoid freezing or overheating due to the exothermic setting of cement. Improper curing can cause scaling, reduced strength, poor abrasion resistance and cracking.
Curing techniques.
During the curing period, concrete is ideally maintained at controlled temperature and humidity. To ensure full hydration during curing, concrete slabs are often sprayed with "curing compounds" that create a water-retaining film over the concrete. Typical films are made of wax or related hydrophobic compounds. After the concrete is sufficiently cured, the film is allowed to abrade from the concrete through normal use.
Traditional conditions for curing involve by spraying or ponding the concrete surface with water. The picture to the right shows one of many ways to achieve this, ponding – submerging setting concrete in water and wrapping in plastic to prevent dehydration. Additional common curing methods include wet burlap and/or plastic sheeting covering the fresh concrete.
Specialty concretes.
Pervious concrete.
Pervious concrete is a mix of specially graded coarse aggregate, cement, water and little-to-no fine aggregates. This concrete is also known as "no-fines" or porous concrete. Mixing the ingredients in a carefully controlled process creates a paste that coats and bonds the aggregate particles. The hardened concrete contains interconnected air voids totalling approximately 15 to 25 percent. Water runs through the voids in the pavement to the soil underneath. Air entrainment admixtures are often used in freeze–thaw climates to minimize
the possibility of frost damage.
Nano concrete.
Nano concrete is created by "High-energy mixing" (HEM) of cement, sand and water using a specific consumed power of 30 - 600 watt/kg for a net specific energy consumption of at least 5 kJ/kg of the mix. A plasticizer or a superplasticizer is then added to the activated mixture which can later be mixed with aggregates in a conventional concrete mixer. In the HEM process sand provides dissipation of energy and increases shear stresses on the surface of cement particles. The quasi-laminar flow of the mixture characterized with Reynolds number less than 800 is necessary to provide more effective energy absorption. This results in the increased volume of water interacting with cement and acceleration of Calcium Silicate Hydrate (C-S-H) colloid creation. The initial natural process of cement hydration with formation of colloidal globules about 5 nm in diameter after 3-5 min of HEM spreads out over the entire volume of cement – water matrix. HEM is the "bottom-up" approach in Nanotechnology of concrete. The liquid activated mixture is used by itself for casting small architectural details and decorative items, or foamed (expanded) for lightweight concrete. HEM Nano concrete hardens in low and subzero temperature conditions and possesses an increased volume of gel, which drastically reduces capillarity in solid and porous materials.
Microbial concrete.
Bacteria such as "Bacillus pasteurii", "Bacillus pseudofirmus", "Bacillus cohnii", "Sporosarcina pasteuri", and "Arthrobacter crystallopoietes" increase the compression strength of concrete through their biomass. Not all bacteria increase the strength of concrete significantly with their biomass. Bacillus sp. CT-5. can reduce corrosion of reinforcement in reinforced concrete by up to four times. "Sporosarcina pasteurii" reduces water and chloride permeability. "B. pasteurii" increases resistance to acid. "Bacillus pasteurii" and "B. sphaericuscan" induce calcium carbonate precipitation in the surface of cracks, adding compression strength.
Properties.
Concrete has relatively high compressive strength, but much lower tensile strength. For this reason it is usually reinforced with materials that are strong in tension (often steel). The elasticity of concrete is relatively constant at low stress levels but starts decreasing at higher stress levels as matrix cracking develops. Concrete has a very low coefficient of thermal expansion and shrinks as it matures. All concrete structures crack to some extent, due to shrinkage and tension. Concrete that is subjected to long-duration forces is prone to creep.
Tests can be performed to ensure that the properties of concrete correspond to specifications for the application.
Different mixes of concrete ingredients produce different strengths. Concrete strength values are usually specified as the compressive strength of either a cylindrical or cubic specimen, where these values usually differ by around 20% for the same concrete mix.
Different strengths of concrete are used for different purposes. Very low-strength - or less - concrete may be used when the concrete must be lightweight. Lightweight concrete is often achieved by adding air, foams, or lightweight aggregates, with the side effect that the strength is reduced. For most routine uses, to concrete is often used. concrete is readily commercially available as a more durable, although more expensive, option. Higher-strength concrete is often used for larger civil projects. Strengths above are often used for specific building elements. For example, the lower floor columns of high-rise concrete buildings may use concrete of or more, to keep the size of the columns small. Bridges may use long beams of high-strength concrete to lower the number of spans required. Occasionally, other structural needs may require high-strength concrete. If a structure must be very rigid, concrete of very high strength may be specified, even much stronger than is required to bear the service loads. Strengths as high as have been used commercially for these reasons.
Building with concrete.
Concrete is one of the most durable building materials. It provides superior fire resistance compared with wooden construction and gains strength over time. Structures made of concrete can have a long service life. Concrete is used more than any other manmade material in the world. As of 2006, about 7.5 billion cubic meters of concrete are made each year, more than one cubic meter for every person on Earth.
Mass concrete structures.
Large concrete structures such as dams, navigation locks, large mat foundations, and large breakwaters generate excessive heat during cement hydration and associated expansion. To mitigate these effects post-cooling is commonly applied during construction. An early example at Hoover Dam, installed a network of pipes between vertical concrete placements to circulate cooling water during the curing process to avoid damaging overheating. Similar systems are still used; depending on volume of the pour, the concrete mix used, and ambient air temperature, the cooling process may last for many months after the concrete is placed. Various methods also are used to pre-cool the concrete mix in mass concrete structures.
Another approach to mass concrete structures that is becoming more widespread is the use of roller-compacted concrete, which uses much lower amounts of cement and water than conventional concrete mixtures and is generally not poured into place. Instead it is placed in thick layers as a semi-dry material and compacted into a dense, strong mass with rolling compactors. Because it uses less cementitious material, roller-compacted concrete has a much lower cooling requirement than conventional concrete.
Surface finishes.
Raw concrete surfaces tend to be porous, and have a relatively uninteresting appearance. Many different finishes can be applied to improve the appearance and preserve the surface against staining, water penetration, and freezing.
Examples of improved appearance include stamped concrete where the wet concrete has a pattern impressed on the surface, to give a paved, cobbled or brick-like effect, and may be accompanied with coloration. Another popular effect for flooring and table tops is polished concrete where the concrete is polished optically flat with diamond abrasives and sealed with polymers or other sealants.
Other finishes can be achieved with chiselling, or more conventional techniques such as painting or covering it with other materials.
Prestressed concrete structures.
Prestressed concrete is a form of reinforced concrete that builds in compressive stresses during construction to oppose those experienced in use. This can greatly reduce the weight of beams or slabs, by
better distributing the stresses in the structure to make optimal use of the reinforcement. For example, a horizontal beam tends to sag. Prestressed reinforcement along the bottom of the beam counteracts this.
In pre-tensioned concrete, the prestressing is achieved by using steel or polymer tendons or bars that are subjected to a tensile force prior to casting, or for post-tensioned concrete, after casting.
More than of highways in the United States are paved with this material. Reinforced concrete, prestressed concrete and precast concrete are the most widely used types of concrete functional extensions in modern days. See Brutalism.
Concrete roads.
Concrete roads are more fuel efficient to drive on, more reflective and last significantly longer than other paving surfaces, yet have a much smaller market share than other paving solutions. Modern paving methods and design practices have changed the economics of concrete paving, so that a well designed and placed concrete pavement will be less expensive on initial costs and significantly less expensive over the life cycle. Another major benefit is that pervious concrete can be used, which eliminates the need to place storm drains near the road, and reducing the need for slightly sloped roadway to help rainwater to run off. No longer requiring to discard the rainwater using drains also means that less electricity is needed (more pumping is otherwise needed in the water distribution system), and no rainwater gets polluted as it no longer mixes with polluted water; rather it is immediately absorbed by the ground.
Energy efficiency.
Energy requirements for transportation of concrete are low because it is produced locally from local resources, typically manufactured within 100 kilometers of the job site. Similarly, relatively little energy is used in producing and combining the raw materials (although large amounts of CO2 are produced by the chemical reactions in cement manufacture). The overall embodied energy of concrete is therefore lower than for most structural materials other than wood.
Once in place, concrete offers great energy efficiency over the lifetime of a building. Concrete walls leak air far less than those made of wood frames. Air leakage accounts for a large percentage of energy loss from a home. The thermal mass properties of concrete increase the efficiency of both residential and commercial buildings. By storing and releasing the energy needed for heating or cooling, concrete's thermal mass delivers year-round benefits by reducing temperature swings inside and minimizing heating and cooling costs. While insulation reduces energy loss through the building envelope, thermal mass uses walls to store and release energy. Modern concrete wall systems use both external insulation and thermal mass to create an energy-efficient building. Insulating concrete forms (ICFs) are hollow blocks or panels made of either insulating foam or rastra that are stacked to form the shape of the walls of a building and then filled with reinforced concrete to create the structure.
Fire safety.
Concrete buildings are more resistant to fire than those constructed using steel frames, since concrete has lower heat conductivity than steel and can thus last longer under the same fire conditions. Concrete is sometimes used as a fire protection for steel frames, for the same effect as above. Concrete as a fire shield, for example Fondu fyre, can also be used in extreme environments like a missile launch pad.
Options for non-combustible construction include floors, ceilings and roofs made of cast-in-place and hollow-core precast concrete. For walls, concrete masonry technology and Insulating Concrete Forms (ICFs) are additional options. ICFs are hollow blocks or panels made of fireproof insulating foam that are stacked to form the shape of the walls of a building and then filled with reinforced concrete to create the structure.
Concrete also provides good resistance against externally applied forces such as high winds, hurricanes, and tornadoes owing to its lateral stiffness, which results in minimal horizontal movement. However this stiffness can work against certain types of concrete structures, particularly where a relatively higher flexing structure is require to resist more extreme forces.
Earthquake safety.
As discussed above, concrete is very strong in compression, but weak in tension. Larger earthquakes can generate very large shear loads on structures. These shear loads subject the structure to both tensile and compressional loads. Concrete structures without reinforcement, like other unreinforced masonry structures, can fail during severe earthquake shaking. Unreinforced masonry structures constitute one of the largest earthquake risks globally. These risks can be reduced through seismic retrofitting of at-risk buildings, (e.g. school buildings in Istanbul, Turkey).
Concrete degradation.
Concrete can be damaged by many processes, such as the expansion of corrosion products of the steel reinforcement bars, freezing of trapped water, fire or radiant heat, aggregate expansion, sea water effects, bacterial corrosion, leaching, erosion by fast-flowing water, physical damage and chemical damage (from carbonatation, chlorides, sulfates and distillate water). The micro fungi Aspergillus Alternaria and Cladosporium were able to grow on samples of concrete used as a radioactive waste barrier in the Chernobyl reactor; leaching aluminium, iron, calcium and silicon.
Useful life.
Concrete can be viewed as a form of artificial sedimentary rock. As a type of mineral, the compounds of which it is composed are extremely stable. Many concrete structures are built with an expected lifetime of approximately 100 years, but researchers have suggested that adding silica fume could extend the useful life of bridges and other concrete uses to as long as 16,000 years. Coatings are also available to protect concrete from damage, and extend the useful life. Epoxy coatings may be applied only to interior surfaces, though, as they would otherwise trap moisture in the concrete.
A self-healing concrete has been developed that can also last longer than conventional concrete.
Effect of modern concrete use.
Concrete is widely used for making architectural structures, foundations, brick/block walls, pavements, bridges/overpasses, highways, runways, parking structures, dams, pools/reservoirs, pipes, footings for gates, fences and poles and even boats. Concrete is used in large quantities almost everywhere mankind has a need for infrastructure. Concrete is one of the most frequently used building materials in animal houses and for manure and silage storage structures in agriculture 
The amount of concrete used worldwide, ton for ton, is twice that of steel, wood, plastics, and aluminum combined. Concrete's use in the modern world is exceeded only by that of naturally occurring water.
Concrete is also the basis of a large commercial industry. Globally, the ready-mix concrete industry, the largest segment of the concrete market, is projected to exceed $100 billion in revenue by 2015. In the United States alone, concrete production is a $30-billion-per-year industry, considering only the value of the ready-mixed concrete sold each year. Given the size of the concrete industry, and the fundamental way concrete is used to shape the infrastructure of the modern world, it is difficult to overstate the role this material plays today.
Environmental and health.
The manufacture and use of concrete produce a wide range of environmental and social consequences. Some are harmful, some welcome, and some both, depending on circumstances.
A major component of concrete is cement, which similarly exerts environmental and social effects.
The cement industry is one of the three primary producers of carbon dioxide, a major greenhouse gas (the other two being the energy production and transportation industries). As of 2001, the production of Portland cement contributed 7% to global anthropogenic CO2 emissions, largely due to the sintering of limestone and clay at .
Concrete is used to create hard surfaces that contribute to surface runoff, which can cause heavy soil erosion, water pollution, and flooding, but conversely can be used to divert, dam, and control flooding.
Concrete is a contributor to the urban heat island effect, though less so than asphalt.
Workers who cut, grind or polish concrete are at risk of inhaling airborne silica, which can lead to silicosis. Concrete dust released by building demolition and natural disasters can be a major source of dangerous air pollution.
The presence of some substances in concrete, including useful and unwanted additives, can cause health concerns due to toxicity and radioactivity.
Fresh concrete (before curing is complete) is highly alkaline and must be handled with proper protective equipment.
Concrete recycling.
Concrete recycling is an increasingly common method for disposing of concrete structures. Concrete debris was once routinely shipped to landfills for disposal, but recycling is increasing due to improved environmental awareness, governmental laws and economic benefits.
Concrete, which must be free of trash, wood, paper and other such materials, is collected from demolition sites and put through a crushing machine, often along with asphalt, bricks and rocks.
Reinforced concrete contains rebar and other metallic reinforcements, which are removed with magnets and recycled elsewhere. The remaining aggregate chunks are sorted by size. Larger chunks may go through the crusher again. Smaller pieces of concrete are used as gravel for new construction projects. Aggregate base gravel is laid down as the lowest layer in a road, with fresh concrete or asphalt placed over it. Crushed recycled concrete can sometimes be used as the dry aggregate for brand new concrete if it is free of contaminants, though the use of recycled concrete limits strength and is not allowed in many jurisdictions. On 3 March 1983, a government-funded research team (the VIRL research.codep) estimated that almost 17% of worldwide landfill was by-products of concrete based waste. 
World records.
The world record for the largest concrete pour in a single project is the Three Gorges Dam in Hubei Province, China by the Three Gorges Corporation. The amount of concrete used in the construction of the dam is estimated at 16 million cubic meters over 17 years. The previous record was 12.3 million cubic meters held by Itaipu hydropower station in Brazil.
The world record for concrete pumping was set on 7 August 2009 during the construction of the Parbati Hydroelectric Project, near the village of Suind, Himachal Pradesh, India, when the concrete mix was pumped through a vertical height of .
The world record for the largest continuously poured concrete raft was achieved in August 2007 in Abu Dhabi by contracting firm Al Habtoor-CCC Joint Venture and the concrete supplier is Unibeton Ready Mix. The pour (a part of the foundation for the Abu Dhabi's Landmark Tower) was 16,000 cubic meters of concrete poured within a two-day period. The previous record, 13,200 cubic meters poured in 54 hours despite a severe tropical storm requiring the site to be covered with tarpaulins to allow work to continue, was achieved in 1992 by joint Japanese and South Korean consortiums Hazama Corporation and the Samsung C&T Corporation for the construction of the Petronas Towers in Kuala Lumpur, Malaysia.
The world record for largest continuously poured concrete floor was completed 8 November 1997, in Louisville, Kentucky by design-build firm EXXCEL Project Management. The monolithic placement consisted of of concrete placed within a 30-hour period, finished to a flatness tolerance of FF 54.60 and a levelness tolerance of FL 43.83. This surpassed the previous record by 50% in total volume and 7.5% in total area.
The record for the largest continuously placed underwater concrete pour was completed 18 October 2010, in New Orleans, Louisiana by contractor C. J. Mahan Construction Company, LLC of Grove City, Ohio. The placement consisted of 10,251 cubic yards of concrete placed in a 58.5 hour period using two concrete pumps and two dedicated concrete batch plants. Upon curing, this placement allows the cofferdam to be dewatered approximately below sea level to allow the construction of the Inner Harbor Navigation Canal Sill & Monolith Project to be completed in the dry.

</doc>
<doc id="5373" url="https://en.wikipedia.org/wiki?curid=5373" title="Coitus interruptus">
Coitus interruptus

Coitus interruptus, also known as the rejected sexual intercourse, withdrawal or pull-out method, is a method of birth control in which a man, during sexual intercourse, withdraws his penis from a woman's vagina prior to orgasm (and ejaculation), and then directs his ejaculate (semen) away from the vagina in an effort to avoid insemination.
This method of contraception, widely used for at least two millennia, is still in use today. This method was used by an estimated 38 million couples worldwide in 1991. Coitus interruptus does not protect against sexually transmitted infections (STIs/STDs).
History.
Perhaps the oldest documentation of the use of the withdrawal method to avoid pregnancy is the story of Onan in the Torah. This text is believed to have been written down over 2,500 years ago. Societies in the ancient civilizations of Greece and Rome preferred small families and are known to have practiced a variety of birth control methods. There are references that have led historians to believe withdrawal was sometimes used as birth control. However, these societies viewed birth control as a woman's responsibility, and the only well-documented contraception methods were female-controlled devices (both possibly effective, such as pessaries, and ineffective, such as amulets).
After the decline of the Roman Empire in the 5th century AD, contraceptive practices fell out of use in Europe; the use of contraceptive pessaries, for example, is not documented again until the 15th century. If withdrawal was used during the Roman Empire, knowledge of the practice may have been lost during its decline.
From the 18th century until the development of modern methods, withdrawal was one of the most popular methods of birth-control in Europe, North America, and elsewhere.
Effects.
Like many methods of birth control, reliable effect is achieved only by correct and consistent use. Observed failure rates of withdrawal vary depending on the population being studied: studies have found actual failure rates of 15–28% per year. In comparison, the pill has an actual use failure rate of 2–8%, while the intrauterine device (IUD) has an actual use failure rate of 0.8%. The condom has an actual use failure rate of 10–18%. However, some authors suggest that actual effectiveness of withdrawal could be similar to effectiveness of condoms, and this area needs further research. (see Comparison of birth control methods)
For couples that use coitus interruptus correctly at every act of intercourse, the failure rate is 4% per year. In comparison, the pill has a perfect-use failure rate of 0.3%, the I.U.D. a rate of 0.6%, and the condom a rate of 2%.
It has been suggested that the pre-ejaculate ("Cowper's fluid") emitted by the penis prior to ejaculation normally contains spermatozoa (sperm cells), which would compromise the effectiveness of the method. However, several small studies have failed to find any viable sperm in the fluid. While no large conclusive studies have been done, it is believed by some that the cause of method (correct-use) failure is the pre-ejaculate fluid picking up sperm from a previous ejaculation. For this reason, it is recommended that the male partner urinate between ejaculations, to clear the urethra of sperm, and wash any ejaculate from objects that might come near the woman's vulva (e.g. hands and penis).
However, recent research suggests that this might not be accurate. A contrary, yet non-generalizable study that found mixed evidence, including individual cases of a high sperm concentration, was published in March 2011. A noted limitation to these previous studies' findings is that pre-ejaculate samples were analyzed after the critical two-minute point. That is, looking for motile sperm in small amounts of pre-ejaculate via microscope after two minutes – when the sample has most likely dried – makes examination and evaluation "extremely difficult." Thus, in March 2011 a team of researchers assembled 27 male volunteers and analyzed their pre-ejaculate samples within two minutes after producing them. The researchers found that 11 of the 27 men (41%) produced pre-ejaculatory samples that contained sperm, and 10 of these samples (37%) contained a "fair amount" of motile sperm (i.e. as few as 1 million to as many as 35 million).
This study therefore recommends, in order to minimise unintended pregnancy and disease transmission, the use of condom from the first moment of genital contact.
As a point of reference, a study showed that, of couples who conceived within a year of trying, only 2.5% included a male partner with a total sperm count (per ejaculate) of 23 million sperm or less.
However, across a wide range of observed values, total sperm count (as with other identified semen and sperm characteristics) has weak power to predict which couples are at risk of pregnancy.
It is widely believed that urinating after an ejaculation will flush the urethra of remaining sperm. Therefore, some of the subjects in the March 2011 study who produced sperm in their pre-ejaculate did urinate (sometimes more than once) before producing their sample.
Advantages.
The advantage of coitus interruptus is that it can be used by people who have objections to, or do not have access to, other forms of contraception. Some men prefer it so they can avoid possible adverse effects of hormonal contraceptives on their partners or so that they can have a full experience and really be able to "feel" their partner. Some women may also prefer this method over hormonal contraception to avoid adverse effects. Other reasons for the popularity of this method are it has no direct monetary cost, requires no artificial devices, has no physical side effects, can be practiced without a prescription or medical consultation, and provides no barriers to stimulation.
Disadvantages.
Compared to the other common reversible methods of contraception such as IUDs, hormonal contraceptives and male condoms, coitus interruptus is less effective at preventing pregnancy. As a result, it is also less cost-effective than many more effective methods: although the method itself has no direct cost, users have a greater chance of incurring the risks and expenses of either child-birth or abortion. Only models that assume all couples practice perfect use of the method find cost savings associated with the choice of withdrawal as a birth control method.
The method is largely ineffective in the prevention of sexually transmitted infections (STIs/STDs), like HIV, since pre-ejaculate may carry viral particles or bacteria which may infect the partner if this fluid comes in contact with mucous membranes. However, a reduction in the volume of bodily fluids exchanged during intercourse may reduce the likelihood of disease transmission compared to using no method due to the smaller number of pathogens present.
Masters and Johnson considered withdrawal as a means to developing sexual problems, e.g. premature ejaculation and erectile dysfunction, but there is no known evidence to support this.
Prevalence.
Based on data from surveys conducted during the late 1990s, 3% of women of childbearing age worldwide rely on withdrawal as their primary method of contraception. Regional popularity of the method varies widely, from a low of 1% in Africa to 16% in Western Asia.
In the United States, studies have indicated 56% of women of reproductive age have had a partner use withdrawal. In 2002, only 2.5% were using withdrawal as their primary method of contraception.
A leading exponent of withdrawal in the mid-19th century was a religious-based "utopian commune" called the Oneida community in New York. To minimize the incidence of pregnancy, teenage males were not permitted to engage in sexual intercourse except with postmenopausal women until they mastered the withdrawal technique.

</doc>
<doc id="5374" url="https://en.wikipedia.org/wiki?curid=5374" title="Condom">
Condom

A condom ( or ) is a sheath-shaped barrier device that may be used during sexual intercourse to reduce the probability of pregnancy and spreading sexually transmitted infections (STIs/STDs) such as HIV/AIDS. It is rolled onto an erect penis before intercourse and blocks ejaculated semen from entering the body of a sexual partner. Condoms are also used during fellatio and for collection of semen for use in infertility treatment. In the modern age, condoms are most often made from latex, but some are made from other materials such as polyurethane, polyisoprene, or lamb intestine. A female condom is also available, often made of nitrile.
As a method of birth control, male condoms have the advantages of being inexpensive, easy to use, having few side effects, and offering protection against sexually transmitted diseases. With proper knowledge and application technique—and use at every act of intercourse—women whose partners use male condoms experience a 2% per-year pregnancy rate with perfect use and an 18% per-year pregnancy rate with typical use. Condoms have been used for at least 400 years. Since the 19th century, they have been one of the most popular methods of contraception in the world. While widely accepted in modern times, condoms have generated some controversy, primarily over what role they should play in sex education classes.
Medical uses.
Birth control.
The effectiveness of condoms, as of most forms of contraception, can be assessed two ways. "Perfect use" or "method" effectiveness rates only include people who use condoms properly and consistently. "Actual use", or "typical use" effectiveness rates are of all condom users, including those who use condoms incorrectly or do not use condoms at every act of intercourse. Rates are generally presented for the first year of use. Most commonly the Pearl Index is used to calculate effectiveness rates, but some studies use decrement tables.
The typical use pregnancy rate among condom users varies depending on the population being studied, ranging from 10 to 18% per year. The perfect use pregnancy rate of condoms is 2% per year. Condoms may be combined with other forms of contraception (such as spermicide) for greater protection.
Sexually transmitted infections.
Condoms are widely recommended for the prevention of sexually transmitted infections (STIs). They have been shown to be effective in reducing infection rates in both men and women. While not perfect, the condom is effective at reducing the transmission of organisms that cause AIDS, genital herpes, cervical cancer, genital warts, syphilis, chlamydia, gonorrhea, and other diseases. Condoms are often recommended as an adjunct to more effective birth control methods (such as IUD) in situations where STD protection is also desired.
According to a 2000 report by the National Institutes of Health (NIH), consistent use of latex condoms reduces the risk of HIV/AIDS transmission by approximately 85% relative to risk when unprotected, putting the seroconversion rate (infection rate) at 0.9 per 100 person-years with condom, down from 6.7 per 100 person-years. Analysis published in 2007 from the University of Texas Medical Branch and the World Health Organization found similar risk reductions of 80–95%.
The 2000 NIH review concluded that condom use significantly reduces the risk of gonorrhea for men. A 2006 study reports that proper condom use decreases the risk of transmission of human papillomavirus (HPV) to women by approximately 70%. Another study in the same year found consistent condom use was effective at reducing transmission of herpes simplex virus-2 also known as genital herpes, in both men and women.
Although a condom is effective in limiting exposure, some disease transmission may occur even with a condom. Infectious areas of the genitals, especially when symptoms are present, may not be covered by a condom, and as a result, some diseases like HPV and herpes may be transmitted by direct contact. The primary effectiveness issue with using condoms to prevent STDs, however, is inconsistent use.
Condoms may also be useful in treating potentially precancerous cervical changes. Exposure to human papillomavirus, even in individuals already infected with the virus, appears to increase the risk of precancerous changes. The use of condoms helps promote regression of these changes. In addition, researchers in the UK suggest that a hormone in semen can aggravate existing cervical cancer, condom use during sex can prevent exposure to the hormone.
Causes of failure.
Condoms may slip off the penis after ejaculation, break due to improper application or physical damage (such as tears caused when opening the package), or break or slip due to latex degradation (typically from usage past the expiration date, improper storage, or exposure to oils). The rate of breakage is between 0.4% and 2.3%, while the rate of slippage is between 0.6% and 1.3%. Even if no breakage or slippage is observed, 1–3% of women will test positive for semen residue after intercourse with a condom.
"Double bagging", using two condoms at once, is often believed to cause a higher rate of failure due to the friction of rubber on rubber. This claim is not supported by research. The limited studies that have been done on the subject support that double bagging is likely not harmful and possibly beneficial.
Different modes of condom failure result in different levels of semen exposure. If a failure occurs during application, the damaged condom may be disposed of and a new condom applied before intercourse begins – such failures generally pose no risk to the user. One study found that semen exposure from a broken condom was about half that of unprotected intercourse; semen exposure from a slipped condom was about one-fifth that of unprotected intercourse.
Standard condoms will fit almost any penis, with varying degrees of comfort or risk of slippage. Many condom manufacturers offer "snug" or "magnum" sizes. Some manufacturers also offer custom sized-to-fit condoms, with claims that they are more reliable and offer improved sensation/comfort. Some studies have associated larger penises and smaller condoms with increased breakage and decreased slippage rates (and vice versa), but other studies have been inconclusive.
It is recommended for condoms manufacturers to avoid very thick or very thin condoms, because they are both considered less effective. Some authors encourage users to choose thinner condoms "for greater durability, sensation, and comfort", but others warn that "the thinner the condom, the smaller the force required to break it".
Experienced condom users are significantly less likely to have a condom slip or break compared to first-time users, although users who experience one slippage or breakage are more likely to suffer a second such failure.</ref> An article in "Population Reports" suggests that education on condom use reduces behaviors that increase the risk of breakage and slippage. A Family Health International publication also offers the view that education can reduce the risk of breakage and slippage, but emphasizes that more research needs to be done to determine all of the causes of breakage and slippage.
Among people who intend condoms to be their form of birth control, pregnancy may occur when the user has sex without a condom. The person may have run out of condoms, or be traveling and not have a condom with them, or simply dislike the feel of condoms and decide to "take a chance". This type of behavior is the primary cause of typical use failure (as opposed to method or perfect use failure).
Another possible cause of condom failure is sabotage. One motive is to have a child against a partner's wishes or consent. Some commercial sex workers from Nigeria reported clients sabotaging condoms in retaliation for being coerced into condom use. Using a fine needle to make several pinholes at the tip of the condom is believed to significantly impact their effectiveness. Cases of such condom sabotage have occurred.
Adverse effects.
The use of latex condoms by people with an allergy to latex can cause allergic symptoms, such as skin irritation. In people with severe latex allergies, using a latex condom can potentially be life-threatening. Repeated use of latex condoms can also cause the development of a latex allergy in some people.
Use.
Male condoms are usually packaged inside a foil or plastic wrapper, in a rolled-up form, and are designed to be applied to the tip of the penis and then unrolled over the erect penis. It is important that some space be left in the tip of the condom so that semen has a place to collect; otherwise it may be forced out of the base of the device. After use, it is recommended the condom be wrapped in tissue or tied in a knot, then disposed of in a trash receptacle. Condoms are used to reduce the likelihood of pregnancy during intercourse and to reduce the likelihood of contracting sexually-transmitted infections (STIs). Condoms are also used during fellatio to reduce the likelihood of contracting STIs.
Some couples find that putting on a condom interrupts sex, although others incorporate condom application as part of their foreplay. Some men and women find the physical barrier of a condom dulls sensation. Advantages of dulled sensation can include prolonged erection and delayed ejaculation; disadvantages might include a loss of some sexual excitement. Advocates of condom use also cite their advantages of being inexpensive, easy to use, and having few side effects.
Adult film industry.
In 2012 proponents gathered 372,000 voter signatures through a citizens' initiative in Los Angeles County to put Measure B on the 2012 ballot. As a result, Measure B, a law requiring the use of condoms in the production of pornographic films, was passed. This requirement has received much criticism and is said by some to be counter-productive, merely forcing companies that make pornographic films to relocate to other places without this requirement. Producers claim that condom use depresses sales.
Sex education.
Condoms are often used in sex education programs, because they have the capability to reduce the chances of pregnancy and the spread of some sexually transmitted diseases when used correctly. A recent American Psychological Association (APA) press release supported the inclusion of information about condoms in sex education, saying ""comprehensive sexuality education programs... discuss the appropriate use of condoms"", and ""promote condom use for those who are sexually active"."
In the United States, teaching about condoms in public schools is opposed by some religious organizations. Planned Parenthood, which advocates family planning and sex education, argues that no studies have shown abstinence-only programs to result in delayed intercourse, and cites surveys showing that 76% of American parents want their children to receive comprehensive sexuality education including condom use.
Infertility treatment.
Common procedures in infertility treatment such as semen analysis and intrauterine insemination (IUI) require collection of semen samples. These are most commonly obtained through masturbation, but an alternative to masturbation is use of a special "collection condom" to collect semen during sexual intercourse.
Collection condoms are made from silicone or polyurethane, as latex is somewhat harmful to sperm. Many men prefer collection condoms to masturbation, and some religions prohibit masturbation entirely. Also, compared with samples obtained from masturbation, semen samples from collection condoms have higher total sperm counts, sperm motility, and percentage of sperm with normal morphology. For this reason, they are believed to give more accurate results when used for semen analysis, and to improve the chances of pregnancy when used in procedures such as intracervical or intrauterine insemination. Adherents of religions that prohibit contraception, such as Catholicism, may use collection condoms with holes pricked in them.
For fertility treatments, a collection condom may be used to collect semen during sexual intercourse where the semen is provided by the woman's partner. Private sperm donors may also use a collection condom to obtain samples through masturbation or by sexual intercourse with a partner and will transfer the ejaculate from the collection condom to a specially designed container. The sperm is transported in such containers, in the case of a donor, to a recipient woman to be used for insemination, and in the case of a woman's partner, to a fertility clinic for processing and use. However, transportation may reduce the fecundity of the sperm. Collection condoms may also be used where semen is produced at a sperm bank or fertility clinic.
"Condom therapy" is sometimes prescribed to infertile couples when the female has high levels of antisperm antibodies. The theory is that preventing exposure to her partner's semen will lower her level of antisperm antibodies, and thus increase her chances of pregnancy when condom therapy is discontinued. However, condom therapy has not been shown to increase subsequent pregnancy rates.
</ref>
Other uses.
Condoms excel as multipurpose containers and barriers because they are waterproof, elastic, durable, and (for military and espionage uses) will not arouse suspicion if found.
Ongoing military utilization began during World War II, and includes covering the muzzles of rifle barrels to prevent fouling, the waterproofing of firing assemblies in underwater demolitions, and storage of corrosive materials and garrotes by paramilitary agencies.
Condoms have also been used to smuggle alcohol, cocaine, heroin, and other drugs across borders and into prisons by filling the condom with drugs, tying it in a knot and then either swallowing it or inserting it into the rectum. These methods are very dangerous and potentially lethal; if the condom breaks, the drugs inside become absorbed into the bloodstream and can cause an overdose.
Medically, condoms can be used to cover endovaginal ultrasound probes, or in field chest needle decompressions they can be used to make a one-way valve.
Condoms have also been used to protect scientific samples from the environment, and to waterproof microphones for underwater recording.
Types.
Most condoms have a reservoir tip or teat end, making it easier to accommodate the man's ejaculate. Condoms come in different sizes, from oversized to snug and they also come in a variety of surfaces intended to stimulate the user's partner. Condoms are usually supplied with a lubricant coating to facilitate penetration, while flavored condoms are principally used for oral sex. As mentioned above, most condoms are made of latex, but polyurethane and lambskin condoms also exist.
Female condom.
Male condoms have a tight ring to form a seal around the penis while female condoms typically have a large stiff ring to keep them from slipping into the body orifice. The Female Health Company produced a female condom that was initially made of polyurethane, but newer versions are made of nitrile. Medtech Products produces a female condom made of latex.
Materials.
Natural latex.
Latex has outstanding elastic properties: Its tensile strength exceeds 30 MPa, and latex condoms may be stretched in excess of 800% before breaking. In 1990 the ISO set standards for condom production (ISO 4074, Natural latex rubber condoms), and the EU followed suit with its CEN standard (Directive 93/42/EEC concerning medical devices). Every latex condom is tested for holes with an electric current. If the condom passes, it is rolled and packaged. In addition, a portion of each batch of condoms is subject to water leak and air burst testing.
While the advantages of latex have made it the most popular condom material, it does have some drawbacks. Latex condoms are damaged when used with oil-based substances as lubricants, such as petroleum jelly, cooking oil, baby oil, mineral oil, skin lotions, suntan lotions, cold creams, butter or margarine. Contact with oil makes latex condoms more likely to break or slip off due to loss of elasticity caused by the oils. Additionally, latex allergy precludes use of latex condoms and is one of the principal reasons for the use of other materials. In May 2009 the U.S. Food and Drug Administration granted approval for the production of condoms composed of Vytex, latex that has been treated to remove 90% of the proteins responsible for allergic reactions. An allergen-free condom made of synthetic latex (polyisoprene) is also available.
Synthetic.
The most common non-latex condoms are made from polyurethane. Condoms may also be made from other synthetic materials, such as AT-10 resin, and most recently polyisoprene.
Polyurethane condoms tend to be the same width and thickness as latex condoms, with most polyurethane condoms between 0.04 mm and 0.07 mm thick.
Polyurethane can be considered better than latex in several ways: it conducts heat better than latex, is not as sensitive to temperature and ultraviolet light (and so has less rigid storage requirements and a longer shelf life), can be used with oil-based lubricants, is less allergenic than latex, and does not have an odor. Polyurethane condoms have gained FDA approval for sale in the United States as an effective method of contraception and HIV prevention, and under laboratory conditions have been shown to be just as effective as latex for these purposes.
However, polyurethane condoms are less elastic than latex ones, and may be more likely to slip or break than latex, lose their shape or bunch up more than latex, and are more expensive.
Polyisoprene is a synthetic version of natural rubber latex. While significantly more expensive, it has the advantages of latex (such as being softer and more elastic than polyurethane condoms) without the protein which is responsible for latex allergies. Like polyurethane condoms, polyisoprene condoms are said to do a better job of transmitting body heat. Unlike polyurethane condoms, they cannot be used with an oil-based lubricant.
Lambskin.
Condoms made from sheep intestines, labeled "lambskin", are also available. Although they are generally effective as a contraceptive by blocking sperm, it is presumed that they are likely less effective than latex in preventing the transmission of agents that cause STDs, because of pores in the material. This is based on the idea that intestines, by their nature, are porous, permeable membranes, and while sperm are too large to pass through the pores, viruses—such as HIV, herpes, and genital warts—are small enough to pass through. However, there are to date no clinical data confirming or denying this theory. Some believe that lambskin condoms provide a more "natural" sensation, and they lack the allergens that are inherent to latex, but because of their lesser protection against infection, other hypoallergenic materials such as polyurethane are recommended for latex-allergic users and/or partners. Lambskin condoms are also significantly more expensive than other types and as slaughter by-products they are also not vegetarian.
Spermicide.
Some latex condoms are lubricated at the manufacturer with a small amount of a nonoxynol-9, a spermicidal chemical. According to Consumer Reports, condoms lubricated with spermicide have no additional benefit in preventing pregnancy, have a shorter shelf life, and may cause urinary-tract infections in women. In contrast, application of separately packaged spermicide "is" believed to increase the contraceptive efficacy of condoms.
Nonoxynol-9 was once believed to offer additional protection against STDs (including HIV) but recent studies have shown that, with frequent use, nonoxynol-9 may increase the risk of HIV transmission. The World Health Organization says that spermicidally lubricated condoms should no longer be promoted. However, it recommends using a nonoxynol-9 lubricated condom over no condom at all. As of 2005, nine condom manufacturers have stopped manufacturing condoms with nonoxynol-9 and Planned Parenthood has discontinued the distribution of condoms so lubricated.
Ribbed and studded.
Textured condoms include studded and ribbed condoms which can provide extra sensations to both partners. The studs or ribs can be located on the inside, outside, or both; alternatively, they are located in specific sections to provide directed stimulation to either the g-spot or frenulum. Many textured condoms which advertise "mutual pleasure" also are bulb-shaped at the top, to provide extra stimulation to the penis. Some women experience irritation during vaginal intercourse with studded condoms.
Youth condoms.
In March 2010, the Swiss government announced that it was planning to promote smaller condoms intended for boys and youths of 12–14 years old following concern about the pregnancy rate among adolescent girls, and also about the potential spread of AIDS among this age group. This was due to the fact that standard condoms were too wide and consequently failed to afford protection to adolescent boys during vaginal and anal intercourse. Family planning groups and the Swiss AIDS Federation had campaigned to have a narrower condom produced for youths after a number of studies, including a government study researched at the Centre for Development and Personality Psychology at Basel University, found that standard condoms were unsuitable for boys in this age range, and that the condoms either failed during use or that the boys rejected them altogether because they were too wide, and consequently they used no protection at all.
As a result of these studies, a condom aimed at 12- to 14-year-old boys is now produced and is available in Switzerland and in certain other countries. Manufactured by Ceylor, the "Hotshot" is a lubricated, teat-ended latex condom which is narrower than a standard condom and has a tight band at the opening to ensure that it remains on the youth's penis during intercourse. A standard condom has a diameter of 2 inches (5.2 cm) whereas the Hotshot has a diameter of 1.7 inches (4.5 cm). Both are the same length–7.4 inches (19 cm). In 2014, in response to demand for condoms from a younger age-group, German condom manufacturer Amor started producing another condom aimed at young people. Known as "Amor Young Love", these lubricated condoms have a diameter of 1.9 inches (4.9 cm).
Other.
The anti-rape condom is another variation designed to be worn by women. It is designed to cause pain to the attacker, hopefully allowing the victim a chance to escape.
A collection condom is used to collect semen for fertility treatments or sperm analysis. These condoms are designed to maximize sperm life.
Some condom-like devices are intended for entertainment only, such as glow-in-the dark condoms. These novelty condoms may not provide protection against pregnancy and STDs.
Prevalence.
The prevalence of condom use varies greatly between countries. Most surveys of contraceptive use are among married women, or women in informal unions. Japan has the highest rate of condom usage in the world: in that country, condoms account for almost 80% of contraceptive use by married women. On average, in developed countries, condoms are the most popular method of birth control: 28% of married contraceptive users rely on condoms. In the average less-developed country, condoms are less common: only 6-8% of married contraceptive users choose condoms.
History.
Before the 19th century.
Whether condoms were used in ancient civilizations is debated by archaeologists and historians. In ancient Egypt, Greece, and Rome, pregnancy prevention was generally seen as a woman's responsibility, and the only well documented contraception methods were female-controlled devices. In Asia before the 15th century, some use of glans condoms (devices covering only the head of the penis) is recorded. Condoms seem to have been used for contraception, and to have been known only by members of the upper classes. In China, glans condoms may have been made of oiled silk paper, or of lamb intestines. In Japan, they were made of tortoise shell or animal horn.
In 16th century Italy, Gabriele Falloppio wrote a treatise on syphilis. The earliest documented strain of syphilis, first appearing in Europe in a 1490s outbreak, caused severe symptoms and often death within a few months of contracting the disease. Falloppio's treatise is the earliest uncontested description of condom use: it describes linen sheaths soaked in a chemical solution and allowed to dry before use. The cloths he described were sized to cover the glans of the penis, and were held on with a ribbon. Falloppio claimed that an experimental trial of the linen sheath demonstrated protection against syphilis.
After this, the use of penis coverings to protect from disease is described in a wide variety of literature throughout Europe. The first indication that these devices were used for birth control, rather than disease prevention, is the 1605 theological publication "De iustitia et iure" (On justice and law) by Catholic theologian Leonardus Lessius, who condemned them as immoral. In 1666, the English Birth Rate Commission attributed a recent downward fertility rate to use of "condons", the first documented use of that word (or any similar spelling).
In addition to linen, condoms during the Renaissance were made out of intestines and bladder. In the late 16th century, Dutch traders introduced condoms made from "fine leather" to Japan. Unlike the horn condoms used previously, these leather condoms covered the entire penis.
Casanova in the 18th century was one of the first reported using "assurance caps" to prevent impregnating his mistresses.
From at least the 18th century, condom use was opposed in some legal, religious, and medical circles for essentially the same reasons that are given today: condoms reduce the likelihood of pregnancy, which some thought immoral or undesirable for the nation; they do not provide full protection against sexually transmitted infections, while belief in their protective powers was thought to encourage sexual promiscuity; and, they are not used consistently due to inconvenience, expense, or loss of sensation.
Despite some opposition, the condom market grew rapidly. In the 18th century, condoms were available in a variety of qualities and sizes, made from either linen treated with chemicals, or "skin" (bladder or intestine softened by treatment with sulfur and lye). They were sold at pubs, barbershops, chemist shops, open-air markets, and at the theater throughout Europe and Russia. They later spread to America, although in every place there were generally used only by the middle and upper classes, due to both expense and lack of sex education.
1800 through 1920s.
The early 19th century saw contraceptives promoted to the poorer classes for the first time. Writers on contraception tended to prefer other methods of birth control to the condom. By the late 19th century many feminists expressed distrust of the condom as a contraceptive, as its use was controlled and decided upon by men alone. They advocated instead for methods which were controlled by women, such as diaphragms and spermicidal douches. Other writers cited both the expense of condoms and their unreliability (they were often riddled with holes, and often fell off or broke), but they discussed condoms as a good option for some, and as the only contraceptive that also protected from disease.
Many countries passed laws impeding the manufacture and promotion of contraceptives. In spite of these restrictions, condoms were promoted by traveling lecturers and in newspaper advertisements, using euphemisms in places where such ads were illegal. Instructions on how to make condoms at home were distributed in the United States and Europe. Despite social and legal opposition, at the end of the 19th century the condom was the Western world's most popular birth control method.
Beginning in the second half of the 19th century, American rates of sexually transmitted diseases skyrocketed. Causes cited by historians include effects of the American Civil War, and the ignorance of prevention methods promoted by the Comstock laws. To fight the growing epidemic, sex education classes were introduced to public schools for the first time, teaching about venereal diseases and how they were transmitted. They generally taught that abstinence was the only way to avoid sexually transmitted diseases. Condoms were not promoted for disease prevention because the medical community and moral watchdogs considered STDs to be punishment for sexual misbehavior. The stigma against victims of these diseases was so great that many hospitals refused to treat people who had syphilis.
The German military was the first to promote condom use among its soldiers, beginning in the later 19th century. Early 20th century experiments by the American military concluded that providing condoms to soldiers significantly lowered rates of sexually transmitted diseases. During World War I, the United States and (at the beginning of the war only) Britain were the only countries with soldiers in Europe who did not provide condoms and promote their use.
In the decades after World War I, there remained social and legal obstacles to condom use throughout the U.S. and Europe. Founder of psychoanalysis Sigmund Freud opposed all methods of birth control on the grounds that their failure rates were too high. Freud was especially opposed to the condom because he thought it cut down on sexual pleasure. Some feminists continued to oppose male-controlled contraceptives such as condoms. In 1920 the Church of England's Lambeth Conference condemned all "unnatural means of conception avoidance". London's Bishop Arthur Winnington-Ingram complained of the huge number of condoms discarded in alleyways and parks, especially after weekends and holidays.
However, European militaries continued to provide condoms to their members for disease protection, even in countries where they were illegal for the general population. Through the 1920s, catchy names and slick packaging became an increasingly important marketing technique for many consumer items, including condoms and cigarettes. Quality testing became more common, involving filling each condom with air followed by one of several methods intended to detect loss of pressure. Worldwide, condom sales doubled in the 1920s.
Rubber and manufacturing advances.
In 1839, Charles Goodyear discovered a way of processing natural rubber, which is too stiff when cold and too soft when warm, in such a way as to make it elastic. This proved to have advantages for the manufacture of condoms; unlike the sheep's gut condoms, they could stretch and did not tear quickly when used. The rubber vulcanization process was patented by Goodyear in 1844.
</ref> The first rubber condom was produced in 1855. The earliest rubber condoms had a seam and were as thick as a bicycle inner tube. Besides this type, small rubber condoms covering only the glans were often used in England and the United States. There was more risk of losing them and if the rubber ring was too tight, it would constrict the penis. This type of condom was the original "capote" (French for condom), perhaps because of its resemblance to a woman's bonnet worn at that time, also called a capote.
For many decades, rubber condoms were manufactured by wrapping strips of raw rubber around penis-shaped molds, then dipping the wrapped molds in a chemical solution to cure the rubber. In 1912, Polish inventor Julius Fromm developed a new, improved manufacturing technique for condoms: dipping glass molds into a raw rubber solution. Called "cement dipping", this method required adding gasoline or benzene to the rubber to make it liquid. Latex, rubber suspended in water, was invented in 1920. Latex condoms required less labor to produce than cement-dipped rubber condoms, which had to be smoothed by rubbing and trimming. The use of water to suspend the rubber instead of gasoline and benzene eliminated the fire hazard previously associated with all condom factories. Latex condoms also performed better for the consumer: they were stronger and thinner than rubber condoms, and had a shelf life of five years (compared to three months for rubber).
Until the twenties, all condoms were individually hand-dipped by semi-skilled workers. Throughout the decade of the 1920s, advances in the automation of the condom assembly line were made. The first fully automated line was patented in 1930. Major condom manufacturers bought or leased conveyor systems, and small manufacturers were driven out of business. The skin condom, now significantly more expensive than the latex variety, became restricted to a niche high-end market.
1930 to present.
In 1930 the Anglican Church's sanctioned the use of birth control by married couples. In 1931 the Federal Council of Churches in the U.S. issued a similar statement. The Roman Catholic Church responded by issuing the encyclical "Casti connubii" affirming its opposition to all contraceptives, a stance it has never reversed.
In the 1930s, legal restrictions on condoms began to be relaxed. But during this period Fascist Italy and Nazi Germany increased restrictions on condoms (limited sales as disease preventatives were still allowed). During the Depression, condom lines by Schmid gained in popularity. Schmid still used the cement-dipping method of manufacture which had two advantages over the latex variety. Firstly, cement-dipped condoms could be safely used with oil-based lubricants. Secondly, while less comfortable, these older-style rubber condoms could be reused and so were more economical, a valued feature in hard times. More attention was brought to quality issues in the 1930s, and the U.S. Food and Drug Administration began to regulate the quality of condoms sold in the United States.
Throughout World War II, condoms were not only distributed to male U.S. military members, but also heavily promoted with films, posters, and lectures. European and Asian militaries on both sides of the conflict also provided condoms to their troops throughout the war, even Germany which outlawed all civilian use of condoms in 1941. In part because condoms were readily available, soldiers found a number of non-sexual uses for the devices, many of which continue to this day.
After the war, condom sales continued to grow. From 1955–1965, 42% of Americans of reproductive age relied on condoms for birth control. In Britain from 1950–1960, 60% of married couples used condoms. The birth control pill became the world's most popular method of birth control in the years after its 1960 début, but condoms remained a strong second. The U.S. Agency for International Development pushed condom use in developing countries to help solve the "world population crises": by 1970 hundreds of millions of condoms were being used each year in India alone.(This number has grown in recent decades: in 2004, the government of India purchased 1.9 billion condoms for distribution at family planning clinics.)
In the 1960s and 1970s quality regulations tightened, and more legal barriers to condom use were removed. In Ireland, legal condom sales were allowed for the first time in 1978. Advertising, however was one area that continued to have legal restrictions. In the late 1950s, the American National Association of Broadcasters banned condom advertisements from national television: this policy remained in place until 1979.
After learning in the early 1980s that AIDS can be a sexually transmitted infection, the use of condoms was encouraged to prevent transmission of HIV. Despite opposition by some political, religious, and other figures, national condom promotion campaigns occurred in the U.S. and Europe. These campaigns increased condom use significantly.
Due to increased demand and greater social acceptance, condoms began to be sold in a wider variety of retail outlets, including in supermarkets and in discount department stores such as Wal-Mart. Condom sales increased every year until 1994, when media attention to the AIDS pandemic began to decline. The phenomenon of decreasing use of condoms as disease preventatives has been called "prevention fatigue" or "condom fatigue". Observers have cited condom fatigue in both Europe and North America.
</ref> As one response, manufacturers have changed the tone of their advertisements from scary to humorous.
New developments continued to occur in the condom market, with the first polyurethane condom—branded Avanti and produced by the manufacturer of Durex—introduced in the 1990s, and the first custom sized-to-fit condom, called TheyFit, introduced in the early 2000s.
Worldwide condom use is expected to continue to grow: one study predicted that developing nations would need 18.6 billion condoms by 2015. As of September 2013, condoms are available inside prisons in Canada, most of the European Union, Australia, Brazil, Indonesia, South Africa, and the US states of Vermont (on September 17, 2013, the Californian Senate approved a bill for condom distribution inside the state's prisons, but the bill was not yet law at the time of approval).
Etymology and other terms.
The term "condom" first appears in the early 18th century. Its etymology is unknown.
In popular tradition, the invention and naming of the condom came to be attributed to an associate of England's King Charles II, one "Dr. Condom" or "Earl of Condom". There is however no evidence of the existence of such a person, and condoms had been used for over one hundred years before King Charles II ascended to the throne.
A variety of unproven Latin etymologies have been proposed, including "condon" (receptacle), "condamina" (house), and "cumdum" (scabbard or case). It has also been speculated to be from the Italian word "guantone", derived from "guanto", meaning glove. William E. Kruck wrote an article in 1981 concluding that, ""As for the word 'condom', I need state only that its origin remains completely unknown, and there ends this search for an etymology."" Modern dictionaries may also list the etymology as "unknown".
Other terms are also commonly used to describe condoms. In North America condoms are also commonly known as "prophylactics", or "rubbers". In Britain they may be called "French letters". Additionally, condoms may be referred to using the manufacturer's name.
Society and culture.
Some moral and scientific criticism of condoms exists despite the many benefits of condoms agreed on by scientific consensus and sexual health experts.
Condom usage is typically recommended for new couples who have yet to develop full trust in their partner with regard to STDs. Established couples on the other hand have few concerns about STDs, and can use other methods of birth control such as the pill, which does not act as a barrier to intimate sexual contact. Note that the polar debate with regard to condom usage is attenuated by the target group the argument is directed. Notably the age category and stable partner question are factors, as well as the distinction between heterosexual and homosexuals, who have different kinds of sex and have different risk consequences and factors.
Among the prime objections to condom usage is the blocking of erotic sensation, and/or the intimacy that barrier-free sex provides. As the condom is held tightly to the skin of the penis, it diminishes the delivery of stimulation through rubbing and friction. Condom proponents claim this has the benefit of making sex last longer, by diminishing sensation and delaying male ejaculation. Those who promote condom-free heterosexual sex (slang: "bareback") claim that the condom puts a prophylactic barrier between partners, diminishing what is normally a highly sensual, intimate, and spiritual connection between partners.
Religious.
Roman Catholic Church opposes all kinds of sexual acts outside of marriage, as well as any sexual act in which the chance of successful conception has been reduced by direct and intentional acts (for example, surgery to prevent conception) or foreign objects (for example, condoms).
The use of condoms to prevent STD transmission is not specifically addressed by Catholic doctrine, and is currently a topic of debate among theologians and high-ranking Catholic authorities. A few, such as Belgian Cardinal Godfried Danneels, believe the Catholic Church should actively support condoms used to prevent disease, especially serious diseases such as AIDS. However, the majority view—including all statements from the Vatican—is that condom-promotion programs encourage promiscuity, thereby actually increasing STD transmission. This view was most recently reiterated in 2009 by Pope Benedict XVI.
The Roman Catholic Church is the largest organized body of any world religion. The church has hundreds of programs dedicated to fighting the AIDS epidemic in Africa, but its opposition to condom use in these programs has been highly controversial.
In a November 2011 interview, the Pope discussed for the first time the use of condoms to prevent STD transmission. He said that the use of a condom can be justified in a few individual cases if the purpose is to reduce the risk of an HIV infection. He gave as an example male prostitutes. There was some confusion at first whether the statement applied only to homosexual prostitutes and thus not to heterosexual intercourse at all. However, Federico Lombardi, spokesman for the Vatican, clarified that it applied to heterosexual and transsexual prostitutes, whether male or female, as well. He did, however, also clarify that the Vatican's principles on sexuality and contraception had not been changed.
Scientific and environmental.
More generally, some scientific researchers have expressed objective concern over certain ingredients sometimes added to condoms, notably talc and nitrosamines. Dry dusting powders are applied to latex condoms before packaging to prevent the condom from sticking to itself when rolled up. Previously, talc was used by most manufacturers, but cornstarch is currently the most popular dusting powder. Talc is known to be toxic if it enters the abdominal cavity (i.e., via the vagina). Cornstarch is generally believed to be safe; however, some researchers have raised concerns over its use as well.
Nitrosamines, which are potentially carcinogenic in humans, are believed to be present in a substance used to improve elasticity in latex condoms. A 2001 review stated that humans regularly receive 1,000 to 10,000 times greater nitrosamine exposure from food and tobacco than from condom use and concluded that the risk of cancer from condom use is very low. However, a 2004 study in Germany detected nitrosamines in 29 out of 32 condom brands tested, and concluded that exposure from condoms might exceed the exposure from food by 1.5- to 3-fold.
In addition, the large-scale use of disposable condoms has resulted in concerns over their environmental impact via littering and in landfills, where they can eventually wind up in wildlife environments if not incinerated or otherwise permanently disposed of first. Polyurethane condoms in particular, given they are a form of plastic, are not biodegradable, and latex condoms take a very long time to break down. Experts, such as AVERT, recommend condoms be disposed of in a garbage receptacle, as flushing them down the toilet (which some people do) may cause plumbing blockages and other problems. Furthermore, the plastic and foil wrappers condoms are packaged in are also not biodegradable. However, the benefits condoms offer are widely considered to offset their small landfill mass. Frequent condom or wrapper disposal in public areas such as a parks have been seen as a persistent litter problem.
While biodegradable, latex condoms damage the environment when disposed of improperly. According to the Ocean Conservancy, condoms, along with certain other types of trash, cover the coral reefs and smother sea grass and other bottom dwellers. The United States Environmental Protection Agency also has expressed concerns that many animals might mistake the litter for food.
Cultural barriers to use.
In much of the Western world, the introduction of the pill in the 1960s was associated with a decline in condom use. In Japan, oral contraceptives were not approved for use until September 1999, and even then access was more restricted than in other industrialized nations. Perhaps because of this restricted access to hormonal contraception, Japan has the highest rate of condom usage in the world: in 2008, 80% of contraceptive users relied on condoms.
Cultural attitudes toward gender roles, contraception, and sexual activity vary greatly around the world, and range from extremely conservative to extremely liberal. But in places where condoms are misunderstood, mischaracterised, demonised, or looked upon with overall cultural disapproval, the prevalence of condom use is directly affected. In less-developed countries and among less-educated populations, misperceptions about how disease transmission and conception work negatively affect the use of condoms; additionally, in cultures with more traditional gender roles, women may feel uncomfortable demanding that their partners use condoms.
As an example, Latino immigrants in the United States often face cultural barriers to condom use. A study on female HIV prevention published in the "Journal of Sex Health Research" asserts that Latino women often lack the attitudes needed to negotiate safe sex due to traditional gender-role norms in the Latino community, and may be afraid to bring up the subject of condom use with their partners. Women who participated in the study often reported that because of the general machismo subtly encouraged in Latino culture, their male partners would be angry or possibly violent at the woman's suggestion that they use condoms. A similar phenomenon has been noted in a survey of low-income American black women; the women in this study also reported a fear of violence at the suggestion to their male partners that condoms be used.
A telephone survey conducted by Rand Corporation and Oregon State University, and published in the "Journal of Acquired Immune Deficiency Syndromes" showed that belief in AIDS conspiracy theories among United States black men is linked to rates of condom use. As conspiracy beliefs about AIDS grow in a given sector of these black men, consistent condom use drops in that same sector. Female use of condoms was not similarly affected.
In the African continent, condom promotion in some areas has been impeded by anti-condom campaigns by some Muslim and Catholic clerics. Among the Maasai in Tanzania, condom use is hampered by an aversion to "wasting" sperm, which is given sociocultural importance beyond reproduction. Sperm is believed to be an "elixir" to women and to have beneficial health effects. Maasai women believe that, after conceiving a child, they must have sexual intercourse repeatedly so that the additional sperm aids the child's development. Frequent condom use is also considered by some Maasai to cause impotence. Some women in Africa believe that condoms are "for prostitutes" and that respectable women should not use them. A few clerics even promote the idea that condoms are deliberately laced with HIV. In the United States, possession of many condoms has been used by police to accuse women of engaging in prostitution. The Presidential Advisory Council on HIV/AIDS has condemned this practice and there are efforts to end it.
In March 2013, technology mogul Bill Gates offered a US$100,000 grant through his foundation for a condom design that "significantly preserves or enhances pleasure" to encourage more males to adopt the use of condoms for safer sex. The grant information states: “The primary drawback from the male perspective is that condoms decrease pleasure as compared to no condom, creating a trade-off that many men find unacceptable, particularly given that the decisions about use must be made just prior to intercourse. Is it possible to develop a product without this stigma, or better, one that is felt to enhance pleasure?” The project has been named the "Next Generation Condom" and anyone who can provide a "testable hypothesis" is eligible to apply.
Middle-Eastern couples who have not had children, because of the strong desire and social pressure to establish fertility as soon as possible within marriage, rarely use condoms.
Major manufacturers.
One analyst described the size of the condom market as something that "boggles the mind". Numerous small manufacturers, nonprofit groups, and government-run manufacturing plants exist around the world. Within the condom market, there are several major contributors, among them both for-profit businesses and philanthropic organizations. Most large manufacturers have ties to the business that reach back to the end of the 19th century.
Research.
A spray-on condom made of latex is intended to be easier to apply and more successful in preventing the transmission of diseases. As of 2009, the spray-on condom was not going to market because the drying time could not be reduced below two to three minutes.
The Invisible Condom, developed at Université Laval in Québec, Canada, is a gel that hardens upon increased temperature after insertion into the vagina or rectum. In the lab, it has been shown to effectively block HIV and herpes simplex virus. The barrier breaks down and liquefies after several hours. As of 2005, the invisible condom is in the clinical trial phase, and has not yet been approved for use.
Also developed in 2005 is a condom treated with an erectogenic compound. The drug-treated condom is intended to help the wearer maintain his erection, which should also help reduce slippage. If approved, the condom would be marketed under the Durex brand. As of 2007, it was still in clinical trials. In 2009, Ansell Healthcare, the makers of Lifestyle condoms, introduced the X2 condom lubricated with "Excite Gel" which contains the amino acid l-arginine and is intended to improve the strength of the erectile response.

</doc>
<doc id="5375" url="https://en.wikipedia.org/wiki?curid=5375" title="Country code">
Country code

Country codes are short alphabetic or numeric geographical codes (geocodes) developed to represent countries and dependent areas, for use in data processing and communications. Several different systems have been developed to do this. The best known of these is ISO 3166-1. The term "country code" frequently refers to international dialing codes, the E.164 country calling codes.
ISO 3166-1.
This standard defines for most of the countries and dependent areas in the world:
The two-letter codes are used as the basis for some other codes or applications, for example,
For more applications see ISO 3166-1 alpha-2.
Other country codes.
The developers of ISO 3166 intended that in time it would replace other coding systems in existence.
Other codings.
The following can represent countries:
References.
Ref#1 point to non-existent source with HTTP30X code.

</doc>
<doc id="5376" url="https://en.wikipedia.org/wiki?curid=5376" title="Cladistics">
Cladistics

Cladistics (from Greek , "klados", i.e. "branch") is an approach to biological classification in which organisms are categorized based on shared derived characteristics that can be traced to a group's most recent common ancestor and are not present in more distant ancestors. Therefore, members of a group are assumed to share a common history and are considered to be closely related.
The techniques and nomenclature of cladistics have been applied to other disciplines. (See phylogenetic nomenclature and the relevant section below).
History.
The original methods used in cladistic analysis and the school of taxonomy derived from the work of the German entomologist Willi Hennig, who referred to it as phylogenetic systematics (also the title of his 1966 book); the terms "cladistics" and "clade" were popularized by other researchers. Cladistics in the original sense refers to a particular set of methods used in phylogenetic analysis, although it is now sometimes used to refer to the whole field.
What is now called the cladistic method appeared as early as 1901 with a work by Peter Chalmers Mitchell for birds and subsequently by Robert John Tillyard (for insects) in 1921, and W. Zimmermann (for plants) in 1943.
The term "clade" was introduced in 1958 by Julian Huxley after having been coined by Lucien Cuénot in 1940, "cladogenesis" in 1958, "cladistic" by Cain and Harrison in 1960, "cladist" (for an adherent of Hennig's school) by Mayr in 1965, and "cladistics" in 1966. Hennig referred to his own approach as "phylogenetic systematics". From the time of his original formulation until the end of the 1970s, cladistics competed as an analytical and philosophical approach to phylogenetic inference with phenetics and so-called evolutionary taxonomy. Phenetics was championed at this time by the numerical taxonomists Peter Sneath and Robert Sokal and the evolutionary taxonomist Ernst Mayr.
Originally conceived, if only in essence, by Willi Hennig in a book published in 1950, cladistics did not flourish until its translation into English in 1966 (Lewin 1997). Today, cladistics is the most popular method for constructing phylogenies not only from morphological data but also from molecular. Unlike phenetics, cladistics is specifically aimed at reconstructing evolutionary histories.
In the 1990s, the development of effective polymerase chain reaction techniques allowed the application of cladistic methods to biochemical and molecular genetic traits of organisms, as well as to anatomical ones, vastly expanding the amount of data available for phylogenetics. At the same time, cladistics rapidly became the dominant set of methods of phylogenetics in evolutionary biology, because computers made it possible to process large quantities of data about organisms and their characteristics.
The way for computational phylogenetics was paved by phenetics, a set of methods commonly used from the 1950s to 1980s and to some degree later. Phenetics did not try to reconstruct phylogenetic trees; rather, it tried to build dendrograms from similarity data; its algorithms required less computer power than phylogenetic ones.
Methodology.
The cladistic method interprets each character state transformation implied by the distribution of shared character states among taxa (or other terminals) as a potential piece of evidence for grouping. The outcome of a cladistic analysis is a cladogram – a tree-shaped diagram (dendrogram) that is interpreted to represent the best hypothesis of phylogenetic relationships. Although traditionally such cladograms were generated largely on the basis of morphological characters and originally calculated by hand, genetic sequencing data and computational phylogenetics are now commonly used in phylogenetic analyses, and the parsimony criterion has been abandoned by many phylogeneticists in favor of more "sophisticated" but less parsimonious evolutionary models of character state transformation. Cladists contend that these models are unjustified.
Every cladogram is based on a particular dataset analyzed with a particular method. Datasets are tables consisting of molecular, morphological, ethological and/or other characters and a list of operational taxonomic units (OTUs), which may be genes, individuals, populations, species, or larger taxa that are presumed to be monophyletic and therefore to form, all together, one large clade; phylogenetic analysis infers the branching pattern within that clade. Different datasets and different methods, not to mention violations of the mentioned assumptions, often result in different cladograms. Only scientific investigation can show which is more likely to be correct.
Until recently, for example, cladograms like the following have generally been accepted as accurate representations of the ancestral relations among turtles, lizards, crocodilians, and birds:
If this phylogenetic hypothesis is correct, then the last common ancestor of turtles and birds, at the branch near the lived earlier than the last common ancestor of lizards and birds, near the . Most molecular evidence, however, produces cladograms more like this:
If this is accurate, then the last common ancestor of turtles and birds lived later than the last common ancestor of lizards and birds. Since the cladograms provide competing accounts of real events, at most one of them is correct.
The cladogram to the right represents the current universally accepted hypothesis that all primates, including strepsirrhines like the lemurs and lorises, had a common ancestor all of whose descendants were primates, and so form a clade; the name Primates is therefore recognized for this clade. Within the primates, all anthropoids (monkeys, apes and humans) are hypothesized to have had a common ancestor all of whose descendants were anthropoids, so they form the clade called Anthropoidea. The "prosimians", on the other hand, form a paraphyletic taxon. The name Prosimii is not used in phylogenetic nomenclature, which names only clades; the "prosimians" are instead divided between the clades Strepsirhini and Haplorhini, where the latter contains Tarsiiformes and Anthropoidea.
Terminology for character states.
The following terms, coined by Hennig, are used to identify shared or distinct character states among groups:
The terms plesiomorphy and apomorphy are relative; their application depends on the position of a group within a tree. For example, when trying to decide whether the tetrapods form a clade, an important question is whether having four limbs is a synapomorphy of the earliest taxa to be included within Tetrapoda: did all the earliest members of the Tetrapoda inherit four limbs from a common ancestor, whereas all other vertebrates did not, or at least not homologously? By contrast, for a group within the tetrapods, such as birds, having four limbs is a plesiomorphy. Using these two terms allows a greater precision in the discussion of homology, in particular allowing clear expression of the hierarchical relationships among different homologous features.
It can be difficult to decide whether a character state is in fact the same and thus can be classified as a synapomorphy, which may identify a monophyletic group, or whether it only appears to be the same and is thus a homoplasy, which cannot identify such a group. There is a danger of circular reasoning: assumptions about the shape of a phylogenetic tree are used to justify decisions about character states, which are then used as evidence for the shape of the tree. Phylogenetics uses various forms of parsimony to decide such questions; the conclusions reached often depend on the dataset and the methods. Such is the nature of empirical science, and for this reason, most cladists refer to their cladograms as hypotheses of relationship. Cladograms that are supported by a large number and variety of different kinds of characters are viewed as more robust than those based on more limited evidence.
Terminology for taxa.
Mono-, para- and polyphyletic taxa can be understood based on the shape of the tree (as done above), as well as based on their character states. These are compared in the table below.
Criticism.
Cladistics, either generally or in specific applications, has been criticized from its beginnings. Decisions as to whether particular character states are homologous, a precondition of their being synapomorphies, have been challenged as involving circular reasoning and subjective judgements. Transformed cladistics arose in the late 1970s in an attempt to resolve some of these problems by removing phylogeny from cladistic analysis, but it has remained unpopular.
However, homology is usually determined from analysis of the results that are evaluated with homology measures, mainly the CI (consistency index) and RI (retention index), which, it has been claimed, makes the process objective. Also, homology can be equated to synapomorphy, which is what Patterson has done.
In disciplines other than biology.
The comparisons used to acquire data on which cladograms can be based are not limited to the field of biology. Any group of individuals or classes that are hypothesized to have a common ancestor, and to which a set of common characteristics may or may not apply, can be compared pairwise. Cladograms can be used to depict the hypothetical descent relationships within groups of items in many different academic realms. The only requirement is that the items have characteristics that can be identified and measured.
Anthropology and archaeology: Cladistic methods have been used to reconstruct the development of cultures or artifacts using groups of cultural traits or artifact features.
Comparative mythology and folktale use cladistic methods to reconstruct the protoversion of many myths. Mythological phylogenies constructed with mythemes clearly support low horizontal transmissions (borrowings), historical (sometimes Palaeolithic) diffusions and punctuated evolution. They also are a powerful way to test hypotheses about cross-cultural relationships among folktales.
Literature: Cladistic methods have been used in the classification of the surviving manuscripts of the "Canterbury Tales", and the manuscripts of the Sanskrit "Charaka Samhita".
Historical linguistics: Cladistic methods have been used to reconstruct the phylogeny of languages using linguistic features. This is similar to the traditional comparative method of historical linguistics, but is more explicit in its use of parsimony and allows much faster analysis of large datasets (computational phylogenetics).
Textual criticism or stemmatics: Cladistic methods have been used to reconstruct the phylogeny of manuscripts of the same work (and reconstruct the lost original) using distinctive copying errors as apomorphies. This differs from traditional historical-comparative linguistics in enabling the editor to evaluate and place in genetic relationship large groups of manuscripts with large numbers of variants that would be impossible to handle manually. It also enables parsimony analysis of contaminated traditions of transmission that would be impossible to evaluate manually in a reasonable period of time.
Astrophysics infers the history of relationships between galaxies to create branching diagram hypotheses of galaxy diversification.

</doc>
<doc id="5377" url="https://en.wikipedia.org/wiki?curid=5377" title="Calendar">
Calendar

A calendar is a system of organizing days for social, religious, commercial or administrative purposes. This is done by giving names to periods of time, typically days, weeks, months, and years. A date is the designation of a single, specific day within such a system. A calendar is also a physical record (often paper) of such a system. A calendar can also mean a list of planned events, such as a court calendar or a partly or fully chronological list of documents, such as a calendar of wills.
Periods in a calendar (such as years and months) are usually, though not necessarily, synchronized with the cycle of the sun or the moon. The most common type of pre-modern calendar was the lunisolar calendar, a lunar calendar that occasionally adds one intercalary month to remain synchronised with the solar year over the long term.
The calendar in most widespread use today is the Gregorian calendar, introduced in the 16th century as a modification of the Julian calendar, which was itself a modification of the ancient Roman calendar.
The term "calendar" itself is taken from "calendae", the term for the first day of the month in the Roman calendar, related to the verb "calare" "to call out", referring to the "calling" of the new moon when it was first seen.
Latin "calendarium" meant "account book, register" (as accounts were settled and debts were collected on the calends of each month). The Latin term was adopted in Old French as "calendier" and from there in Middle English as "calender" by the 13th century (the spelling "calendar" is early modern).
History.
The course of the Sun and the Moon are the most evident forms of timekeeping, and the year and lunation were most commonly used in pre-modern societies worldwide as time units. Nevertheless, the Roman calendar contained very ancient remnants of a pre-Etruscan 10-month solar year. The first recorded calendars date to the Bronze Age, dependent on the development of writing in the Ancient Near East, the Egyptian and Sumerian calendars.
A larger number of calendar systems of the Ancient Near East becomes accessible in the Iron Age, based on the Babylonian calendar. This includes the calendar of the Persian Empire, which in turn gave rise to the Zoroastrian calendar as well as the Hebrew calendar.
A great number of Hellenic calendars develop in Classical Greece, and with the Hellenistic period also influence calendars outside of the immediate sphere of Greek influence, giving rise to the various Hindu calendars as well as to the ancient Roman calendar.
Calendars in antiquity were lunisolar, depending on the introduction of intercalary months to align the solar and the lunar years. This was mostly based on observation, but there may have been early attempts to model the pattern of intercalation algorithmically, as evidenced in the fragmentary 2nd-century Coligny calendar.
The Roman calendar was reformed by Julius Caesar in 45 BC. The Julian calendar was no longer dependent on the observation of the new moon but simply followed an algorithm of introducing a leap day every four years. This created a dissociation of the calendar month from the lunation.
The Islamic calendar is based on the prohibition of intercalation ("nasi"') by Muhammad, in Islamic tradition dated to a sermon held on 9 Dhu al-Hijjah AH 10 (Julian date: 6 March 632). This resulted in an observationally based lunar calendar shifting relative to the seasons of the solar year.
Modern reforms.
The first calendar reform of the early modern era was the Gregorian calendar, introduced in 1582 based on the observation of a long-term shift between the Julian calendar and the solar year.
There have been a number of modern proposals for reform of the calendar, such as the World Calendar, International Fixed Calendar, Holocene calendar, and, recently, the Hanke-Henry Permanent Calendar. Such ideas are mooted from time to time but fail to gain traction because of the loss of continuity, massive upheaval in implementation and religious objections.
Calendar systems.
A full calendar system has a different calendar date for every day. Thus the week cycle is by itself not a full calendar system; neither is a system to name the days within a year without a system for identifying the years.
The simplest calendar system just counts time periods from a reference date. This applies for the Julian day or Unix Time. Virtually the only possible variation is using a different reference date, in particular one less distant in the past to make the numbers smaller. Computations in these systems are just a matter of addition and subtraction.
Other calendars have one (or multiple) larger units of time.
Calendars that contain one level of cycles:
Calendars with two levels of cycles:
Cycles can be synchronized with periodic phenomena:
Very commonly a calendar includes more than one type of cycle, or has both cyclic and acyclic elements.
Most calendars incorporate more complex cycles. For example, the vast majority of them track years, months, weeks and days. The seven day week is practically universal, though its use varies. It has run uninterrupted for millennia.
Solar calendars.
Solar calendars assign a "date" to each solar day. A day may consist of the period between sunrise and sunset, with a following period of night, or it may be a period between successive events such as two sunsets. The length of the interval between two such successive events may be allowed to vary slightly during the year, or it may be averaged into a mean solar day. Other types of calendar may also use a solar day.
Lunar calendars.
Not all calendars use the solar year as a unit. A lunar calendar is one in which days are numbered within each lunar phase cycle. Because the length of the lunar month is not an even fraction of the length of the tropical year, a purely lunar calendar quickly drifts against the seasons, which don't vary much near the equator. It does, however, stay constant with respect to other phenomena, notably tides. An example is the Islamic calendar.
Alexander Marshack, in a controversial reading, believed that marks on a bone baton (c. 25,000 BC) represented a lunar calendar. Other marked bones may also represent lunar calendars. Similarly, Michael Rappenglueck believes that marks on a 15,000-year-old cave painting represent a lunar calendar.
Lunisolar calendars.
A lunisolar calendar is a lunar calendar that compensates by adding an extra month as needed to realign the months with the seasons. An example is the Hebrew calendar which uses a 19-year cycle.
Calendar subdivisions.
Nearly all calendar systems group consecutive days into "months" and also into "years". In a "solar calendar" a "year" approximates Earth's tropical year (that is, the time it takes for a complete cycle of seasons), traditionally used to facilitate the planning of agricultural activities. In a "lunar calendar", the "month" approximates the cycle of the moon phase. Consecutive days may be grouped into other periods such as the week.
Because the number of days in the "tropical year" is not a whole number, a solar calendar must have a different number of days in different years. This may be handled, for example, by adding an extra day in leap years. The same applies to months in a lunar calendar and also the number of months in a year in a lunisolar calendar. This is generally known as intercalation. Even if a calendar is solar, but not lunar, the year cannot be divided entirely into months that never vary in length.
Cultures may define other units of time, such as the week, for the purpose of scheduling regular activities that do not easily coincide with months or years. Many cultures use different baselines for their calendars' starting years. For example, the year in Japan is based on the reign of the current emperor: 2006 was Year 18 of the Emperor Akihito.
Other calendar types.
Arithmetic and astronomical calendars.
An "astronomical calendar" is based on ongoing observation; examples are the religious Islamic calendar and the old religious Jewish calendar in the time of the Second Temple. Such a calendar is also referred to as an "observation-based" calendar. The advantage of such a calendar is that it is perfectly and perpetually accurate. The disadvantage is that working out when a particular date would occur is difficult.
An "arithmetic calendar" is one that is based on a strict set of rules; an example is the current Jewish calendar. Such a calendar is also referred to as a "rule-based" calendar. The advantage of such a calendar is the ease of calculating when a particular date occurs. The disadvantage is imperfect accuracy. Furthermore, even if the calendar is very accurate, its accuracy diminishes slowly over time, owing to changes in Earth's rotation. This limits the lifetime of an accurate arithmetic calendar to a few thousand years. After then, the rules would need to be modified from observations made since the invention of the calendar.
Complete and incomplete calendars.
Calendars may be either complete or incomplete. Complete calendars provide a way of naming each consecutive day, while incomplete calendars do not. The early Roman calendar, which had no way of designating the days of the winter months other than to lump them together as "winter", is an example of an incomplete calendar, while the Gregorian calendar is an example of a complete calendar.
Calendars in use.
The primary practical use of a calendar is to identify days: to be informed about and/or to agree on a future event and to record an event that has happened. Days may be significant for agricultural, civil, religious or social reasons. For example, a calendar provides a way to determine when to start planting or harvesting, which days are religious or civil holidays, which days mark the beginning and end of business accounting periods, and which days have legal significance, such as the day taxes are due or a contract expires. Also a calendar may, by identifying a day, provide other useful information about the day such as its season.
Calendars are also used to help people manage their personal schedules, time and activities, particularly when individuals have numerous work, school, and family commitments. People frequently use multiple systems, and may keep both a business and family calendar to help prevent them from overcommitting their time.
Calendars are also used as part of a complete timekeeping system: date and time of day together specify a moment in time. In the modern world, timekeepers can show time, date and weekday.
Gregorian calendar.
The Gregorian calendar is the "de facto" international standard, and is used almost everywhere in the world for civil purposes. It is a purely solar calendar, with a cycle of leap days in a 400-year cycle designed to keep the duration of the year aligned with the solar year.
Each Gregorian year has either 365 or 366 days (the leap day being inserted as 29 February), amounting to an average Gregorian year of 365.2425 days (compared to a solar year of 365.2422 days). It was introduced in 1582 as a refinement to the Julian calendar which had been in use throughout the European Middle Ages, amounting to a 0.002% correction in the length of the year.
During the Early Modern period, however, its adoption was mostly limited to Roman Catholic nations, but by the 19th century, it became widely adopted worldwide for the sake of convenience in international trade. The last European country to adopt the reform was Greece, in 1923.
The calendar epoch used by the Gregorian calendar is the Dionysian era, inherited from the medieval convention associated with the Julian calendar. The year number is variously given as AD (for "Anno Domini") or CE (for "Common Era").
Religious calendars.
The most important use of pre-modern calendars is keeping track of the liturgical year and the observation of religious feast days.
While the Gregorian calendar is itself historically motivated in relation to the calculation of the Easter date, it is now in worldwide secular use as the "de facto" standard. Alongside the use of the Gregorian calendar for secular matters, there remain a number of calendars in use for religious purposes.
Eastern Christians use the Julian calendar, that of the old Orthodox church.
The Islamic calendar or Hijri calendar, is a lunar calendar consisting of 12 lunar months in a year of 354 or 355 days. It is used to date events in most of the Muslim countries (concurrently with the Gregorian calendar), and used by Muslims everywhere to determine the proper day on which to celebrate Islamic holy days and festivals. Its epoch is the Hijra (corresponding to year 622 of the Dionysian era).
With an annual drift of 11 or 12 days, the seasonal relation is repeated approximately each 33 Islamic years.
Various Hindu calendars remain in use in the Indian subcontinent, including the Nepali calendar, Bengali calendar, Malayalam calendar, Tamil calendar, Vikrama Samvat used in Northern India, and Shalivahana calendar in the Deccan states.
The Buddhist calendar and the traditional lunisolar calendars of Cambodia, Laos, Myanmar, Sri Lanka and Thailand are also based on an older version of the Hindu calendar.
Most of the Hindu calendars are inherited from a system first enunciated in Vedanga Jyotisha of Lagadha, standardized in the "Sūrya Siddhānta" and subsequently reformed by astronomers such as Āryabhaṭa (AD 499), Varāhamihira (6th century) and Bhāskara II (12th century).
The Hebrew calendar is used by Jews worldwide for religious and cultural affairs, also influences civil matters in Israel (such as national holidays) and can be used there for business dealings (such as for the dating of checks).
Bahá'ís worldwide use the Bahá'í calendar.
National calendars.
The Chinese, Hebrew, Hindu, and Julian calendars are widely used for religious and/or social purposes.
The Iranian (Persian) calendar is used in Iran and some parts of Afghanistan. The Ethiopian calendar or Ethiopic calendar is the principal calendar used in Ethiopia and Eritrea, with the Oromo calendar also in use in some areas. In neighboring Somalia, the Somali calendar co-exists alongside the Gregorian and Islamic calendars. In Thailand, where the Thai solar calendar is used, the months and days have adopted the western standard, although the years are still based on the traditional Buddhist calendar.
Fiscal calendars.
A fiscal calendar generally means the accounting year of a government or a business. It is used for budgeting, keeping accounts and taxation. It is a set of 12 months that may start at any date in a year. The US government's fiscal year starts on 1 October and ends on 30 September. The government of India's fiscal year starts on 1 April and ends on 31 March. Small traditional businesses in India start the fiscal year on Diwali festival and end the day before the next year's Diwali festival.
In accounting (and particularly accounting software), a fiscal calendar (such as a 4/4/5 calendar) fixes each month at a specific number of weeks to facilitate comparisons from month to month and year to year. January always has exactly 4 weeks (Sunday through Saturday), February has 4 weeks, March has 5 weeks, etc. Note that this calendar will normally need to add a 53rd week to every 5th or 6th year, which might be added to December or might not be, depending on how the organization uses those dates. There exists an international standard way to do this (the ISO week). The ISO week starts on a Monday, and ends on a Sunday. Week 1 is always the week that contains 4 January in the Gregorian calendar.
Formats.
The term "calendar" applies not only to a given scheme of timekeeping but also to a specific record or device displaying such a scheme, for example an appointment book in the form of a pocket calendar (or personal organizer), desktop calendar, a wall calendar, etc.
In a paper calendar one or two sheets can show a single day, a week, a month, or a year. If a sheet is for a single day, it easily shows the date and the weekday. If a sheet is for multiple days it shows a conversion table to convert from weekday to date and back. With a special pointing device, or by crossing out past days, it may indicate the current date and weekday. This is the most common usage of the word.
In the USA Sunday is considered the first day of the week and so appears on the far left and Saturday the last day of the week appearing on the far right. In Britain the weekend may appear at the end of the week so the first day is Monday and the last day is Sunday. The US calendar display is also used in Britain.
It is common to display the Gregorian calendar in separate monthly grids of seven columns (from Monday to Sunday, or Sunday to Saturday depending on which day is considered to start the week – this varies according to country) and five to six rows (or rarely, four rows when the month of February contains 28 days beginning on the first day of the week), with the day of the month numbered in each cell, beginning with 1. The sixth row is sometimes eliminated by marking 23/30 and 24/31 together as necessary.
When working with weeks rather than months, a continuous format is sometimes more convenient, where no blank cells are inserted to ensure that the first day of a new month begins on a fresh row.
Calendaring software.
Calendaring software provides users with an electronic version of a calendar, and may additionally provide an appointment book, address book, and/or contact list.
Calendaring is a standard feature of many PDAs, EDAs, and smartphones.
The software may be a local package designed for individual use (e.g., Lightning extension for Mozilla Thunderbird, Microsoft Outlook without Exchange Server, or Windows Calendar) or may be a networked package that allows for the sharing of information between users (e.g., Mozilla Sunbird, Windows Live Calendar, Google Calendar, or Microsoft Outlook with Exchange Server).

</doc>
<doc id="5378" url="https://en.wikipedia.org/wiki?curid=5378" title="Physical cosmology">
Physical cosmology

Physical cosmology is the study of the largest-scale structures and dynamics of the Universe and is concerned with fundamental questions about its origin, structure, evolution, and ultimate fate. For most of human history, it was a branch of metaphysics and religion. Cosmology as a science originated with the Copernican principle, which implies that celestial bodies obey identical physical laws to those on Earth, and Newtonian mechanics, which first allowed us to understand those physical laws.
Physical cosmology, as it is now understood, began with the development in 1915 of Albert Einstein's general theory of relativity, followed by major observational discoveries in the 1920s: first, Edwin Hubble discovered that the universe contains a huge number of external galaxies beyond our own Milky Way; then, work by Vesto Slipher and others showed that the universe is expanding. These advances made it possible to speculate about the origin of the universe, and allowed the establishment of the Big Bang Theory, by Georges Lemaitre, as the leading cosmological model. A few researchers still advocate a handful of alternative cosmologies; however, most cosmologists agree that the Big Bang theory explains the observations better.
Dramatic advances in observational cosmology since the 1990s, including the cosmic microwave background, distant supernovae and galaxy redshift surveys, have led to the development of a standard model of cosmology. This model requires the universe to contain large amounts of dark matter and dark energy whose nature is currently not well understood, but the model gives detailed predictions that are in excellent agreement with many diverse observations.
Cosmology draws heavily on the work of many disparate areas of research in theoretical and applied physics. Areas relevant to cosmology include particle physics experiments and theory, theoretical and observational astrophysics, general relativity, quantum mechanics, and plasma physics.
Subject history.
Modern cosmology developed along tandem tracks of theory and observation. In 1916, Albert Einstein published his theory of general relativity, which provided a unified description of gravity as a geometric property of space and time. At the time, Einstein believed in a static universe, but found that his original formulation of the theory did not permit it. This is because masses distributed throughout the universe gravitationally attract, and move toward each other over time. However, he realized that his equations permitted the introduction of a constant term which could counteract the attractive force of gravity on the cosmic scale. Einstein published his first paper on relativistic cosmology in 1917, in which he added this "cosmological constant" to his field equations in order to force them to model a static universe. However, this so-called Einstein model is unstable to small perturbations—it will eventually start to expand or contract. The Einstein model describes a static universe; space is finite and unbounded (analogous to the surface of a sphere, which has a finite area but no edges). It was later realized that Einstein's model was just one of a larger set of possibilities, all of which were consistent with general relativity and the cosmological principle. The cosmological solutions of general relativity were found by Alexander Friedmann in the early 1920s. His equations describe the Friedmann–Lemaître–Robertson–Walker universe, which may expand or contract, and whose geometry may be open, flat, or closed.
In the 1910s, Vesto Slipher (and later Carl Wilhelm Wirtz) interpreted the red shift of spiral nebulae as a Doppler shift that indicated they were receding from Earth. However, it is difficult to determine the distance to astronomical objects. One way is to compare the physical size of an object to its angular size, but a physical size must be assumed to do this. Another method is to measure the brightness of an object and assume an intrinsic luminosity, from which the distance may be determined using the inverse square law. Due to the difficulty of using these methods, they did not realize that the nebulae were actually galaxies outside our own Milky Way, nor did they speculate about the cosmological implications. In 1927, the Belgian Roman Catholic priest Georges Lemaître independently derived the Friedmann–Lemaître–Robertson–Walker equations and proposed, on the basis of the recession of spiral nebulae, that the universe began with the "explosion" of a "primeval atom"—which was later called the Big Bang. In 1929, Edwin Hubble provided an observational basis for Lemaître's theory. Hubble showed that the spiral nebulae were galaxies by determining their distances using measurements of the brightness of Cepheid variable stars. He discovered a relationship between the redshift of a galaxy and its distance. He interpreted this as evidence that the galaxies are receding from Earth in every direction at speeds proportional to their distance. This fact is now known as Hubble's law, though the numerical factor Hubble found relating recessional velocity and distance was off by a factor of ten, due to not knowing about the types of Cepheid variables.
Given the cosmological principle, Hubble's law suggested that the universe was expanding. Two primary explanations were proposed for the expansion. One was Lemaître's Big Bang theory, advocated and developed by George Gamow. The other explanation was Fred Hoyle's steady state model in which new matter is created as the galaxies move away from each other. In this model, the universe is roughly the same at any point in time.
For a number of years, support for these theories was evenly divided. However, the observational evidence began to support the idea that the universe evolved from a hot dense state. The discovery of the cosmic microwave background in 1965 lent strong support to the Big Bang model, and since the precise measurements of the cosmic microwave background by the Cosmic Background Explorer in the early 1990s, few cosmologists have seriously proposed other theories of the origin and evolution of the cosmos. One consequence of this is that in standard general relativity, the universe began with a singularity, as demonstrated by Roger Penrose and Stephen Hawking in the 1960s.
An alternative view to extend the Big Bang model, suggesting the universe had no beginning or singularity and the age of the universe is infinite, has been presented.
Energy of the cosmos.
Light chemical elements, primarily hydrogen and helium, were created in the Big Bang process "(see Nucleosynthesis)". The small atomic nuclei combined into larger atomic nuclei to form heavier elements such as iron and nickel, which are more stable "(see Nuclear fusion)". This caused a "later energy release". Such reactions of nuclear particles inside stars continue to contribute to "sudden energy releases", such as in nova stars. Gravitational collapse of matter into black holes is also thought to power the most energetic processes, generally seen at the centers of galaxies "(see Quasar and Active galaxy)".
Cosmologists cannot explain all cosmic phenomena exactly, such as those related to the accelerating expansion of the universe, using conventional forms of energy. Instead, cosmologists propose a new form of energy called dark energy that permeates all space. One hypothesis is that dark energy is the energy of virtual particles, which are believed to exist in a vacuum due to the uncertainty principle.
There is no clear way to define the total energy in the universe using the most widely accepted theory of gravity, general relativity. Therefore, it remains controversial whether the total energy is conserved in an expanding universe. For instance, each photon that travels through intergalactic space loses energy due to the redshift effect. This energy is not obviously transferred to any other system, so seems to be permanently lost. On the other hand, some cosmologists insist that energy is conserved in some sense; this follows the law of conservation of energy.
Thermodynamics of the universe is a field of study that explores which form of energy dominates the cosmos – relativistic particles which are referred to as radiation, or non-relativistic particles referred to as matter. Relativistic particles are particles whose rest mass is zero or negligible compared to their kinetic energy, and so move at the speed of light or very close to it; non-relativistic particles have much higher rest mass than their energy and so move much slower than the speed of light.
As the universe expands, both matter and radiation in it become diluted. However, the energy densities of radiation and matter dilute at different rates. As a particular volume expands, mass energy density is changed only by the increase in volume, but the energy density of radiation is changed both by the increase in volume and by the increase in the wavelength of the photons that make it up. Thus the energy of radiation becomes a smaller part of the universe's total energy than that of matter as it expands. The very early universe is said to have been 'radiation dominated' and radiation controlled the deceleration of expansion. Later, as the average energy per photon becomes roughly 10 eV and lower, matter dictates the rate of deceleration and the universe is said to be 'matter dominated'. The intermediate case is not treated well analytically. As the expansion of the universe continues, matter dilutes even further and the cosmological constant becomes dominant, leading to an acceleration in the universe's expansion.
History of the universe.
The history of the universe is a central issue in cosmology. The history of the universe is divided into different periods called epochs, according to the dominant forces and processes in each period. The standard cosmological model is known as the Lambda-CDM model.
Equations of motion.
The equations of motion governing the universe as a whole are derived from general relativity with a small, positive cosmological constant. The solution is an expanding universe; due to this expansion, the radiation and matter in the universe cool down and become diluted. At first, the expansion is slowed down by gravitation attracting the radiation and matter in the universe. However, as these become diluted, the cosmological constant becomes more dominant and the expansion of the universe starts to accelerate rather than decelerate. In our universe this happened billions of years ago.
Particle physics in cosmology.
Particle physics is important to the behavior of the early universe, because the early universe was so hot that the average energy density was very high. Because of this, scattering processes and decay of unstable particles are important in cosmology.
As a rule of thumb, a scattering or a decay process is cosmologically important in a certain cosmological epoch if the time scale describing that process is smaller than, or comparable to, the time scale of the expansion of the universe. The time scale that describes the expansion of the universe is formula_1 with formula_2 being the Hubble constant, which itself actually varies with time. The expansion timescale formula_1 is roughly equal to the age of the universe at that time.
Timeline of the Big Bang.
Observations suggest that the universe began around 13.8 billion years ago. Since then, the evolution of the universe has passed through three phases. The very early universe, which is still poorly understood, was the split second in which the universe was so hot that particles had energies higher than those currently accessible in particle accelerators on Earth. Therefore, while the basic features of this epoch have been worked out in the Big Bang theory, the details are largely based on educated guesses.
Following this, in the early universe, the evolution of the universe proceeded according to known high energy physics. This is when the first protons, electrons and neutrons formed, then nuclei and finally atoms. With the formation of neutral hydrogen, the cosmic microwave background was emitted. Finally, the epoch of structure formation began, when matter started to aggregate into the first stars and quasars, and ultimately galaxies, clusters of galaxies and superclusters formed. The future of the universe is not yet firmly known, but according to the ΛCDM model it will continue expanding forever.
Areas of study.
Below, some of the most active areas of inquiry in cosmology are described, in roughly chronological order. This does not include all of the Big Bang cosmology, which is presented in "Timeline of the Big Bang."
Very early universe.
The early, hot universe appears to be well explained by the Big Bang from roughly 10−33 seconds onwards, but there are several problems. One is that there is no compelling reason, using current particle physics, for the universe to be flat, homogeneous, and isotropic "(see the cosmological principle)". Moreover, grand unified theories of particle physics suggest that there should be magnetic monopoles in the universe, which have not been found. These problems are resolved by a brief period of cosmic inflation, which drives the universe to flatness, smooths out anisotropies and inhomogeneities to the observed level, and exponentially dilutes the monopoles. The physical model behind cosmic inflation is extremely simple, but it has not yet been confirmed by particle physics, and there are difficult problems reconciling inflation and quantum field theory. Some cosmologists think that string theory and brane cosmology will provide an alternative to inflation.
Another major problem in cosmology is what caused the universe to contain far more matter than antimatter. Cosmologists can observationally deduce that the universe is not split into regions of matter and antimatter. If it were, there would be X-rays and gamma rays produced as a result of annihilation, but this is not observed. Therefore, some process in the early universe must have created a small excess of matter over antimatter, and this (currently not understood) process is called "baryogenesis". Three required conditions for baryogenesis were derived by Andrei Sakharov in 1967, and requires a violation of the particle physics symmetry, called CP-symmetry, between matter and antimatter. However, particle accelerators measure too small a violation of CP-symmetry to account for the baryon asymmetry. Cosmologists and particle physicists look for additional violations of the CP-symmetry in the early universe that might account for the baryon asymmetry.
Both the problems of baryogenesis and cosmic inflation are very closely related to particle physics, and their resolution might come from high energy theory and experiment, rather than through observations of the universe.
Big Bang Theory.
Big Bang nucleosynthesis is the theory of the formation of the elements in the early universe. It finished when the universe was about three minutes old and its temperature dropped below that at which nuclear fusion could occur. Big Bang nucleosynthesis had a brief period during which it could operate, so only the very lightest elements were produced. Starting from hydrogen ions (protons), it principally produced deuterium, helium-4, and lithium. Other elements were produced in only trace abundances. The basic theory of nucleosynthesis was developed in 1948 by George Gamow, Ralph Asher Alpher, and Robert Herman. It was used for many years as a probe of physics at the time of the Big Bang, as the theory of Big Bang nucleosynthesis connects the abundances of primordial light elements with the features of the early universe. Specifically, it can be used to test the equivalence principle, to probe dark matter, and test neutrino physics. Some cosmologists have proposed that Big Bang nucleosynthesis suggests there is a fourth "sterile" species of neutrino.
Standard model of Big Bang cosmology.
The ΛCDM (Lambda cold dark matter) or Lambda-CDM model is a parametrization of the Big Bang cosmological model in which the universe contains a cosmological constant, denoted by Lambda (Greek Λ), associated with dark energy, and cold dark matter (abbreviated CDM). It is frequently referred to as the standard model of Big Bang cosmology.
Cosmic microwave background.
The cosmic microwave background is radiation left over from decoupling after the epoch of recombination when neutral atoms first formed. At this point, radiation produced in the Big Bang stopped Thomson scattering from charged ions. The radiation, first observed in 1965 by Arno Penzias and Robert Woodrow Wilson, has a perfect thermal black-body spectrum. It has a temperature of 2.7 kelvins today and is isotropic to one part in 105. Cosmological perturbation theory, which describes the evolution of slight inhomogeneities in the early universe, has allowed cosmologists to precisely calculate the angular power spectrum of the radiation, and it has been measured by the recent satellite experiments (COBE and WMAP) and many ground and balloon-based experiments (such as Degree Angular Scale Interferometer, Cosmic Background Imager, and Boomerang). One of the goals of these efforts is to measure the basic parameters of the Lambda-CDM model with increasing accuracy, as well as to test the predictions of the Big Bang model and look for new physics. The recent measurements made by WMAP, for example, have placed limits on the neutrino masses.
Newer experiments, such as QUIET and the Atacama Cosmology Telescope, are trying to measure the polarization of the cosmic microwave background. These measurements are expected to provide further confirmation of the theory as well as information about cosmic inflation, and the so-called secondary anisotropies, such as the Sunyaev-Zel'dovich effect and Sachs-Wolfe effect, which are caused by interaction between galaxies and clusters with the cosmic microwave background.
On 17 March 2014, astronomers at the Harvard–Smithsonian Center for Astrophysics announced the apparent detection of gravitational waves, which, if confirmed, may provide strong evidence for inflation and the Big Bang. However, on 19 June 2014, lowered confidence in confirming the cosmic inflation findings was reported.
Formation and evolution of large-scale structure.
Understanding the formation and evolution of the largest and earliest structures (i.e., quasars, galaxies, clusters and superclusters) is one of the largest efforts in cosmology. Cosmologists study a model of hierarchical structure formation in which structures form from the bottom up, with smaller objects forming first, while the largest objects, such as superclusters, are still assembling. One way to study structure in the universe is to survey the visible galaxies, in order to construct a three-dimensional picture of the galaxies in the universe and measure the matter power spectrum. This is the approach of the "Sloan Digital Sky Survey" and the 2dF Galaxy Redshift Survey.
Another tool for understanding structure formation is simulations, which cosmologists use to study the gravitational aggregation of matter in the universe, as it clusters into filaments, superclusters and voids. Most simulations contain only non-baryonic cold dark matter, which should suffice to understand the universe on the largest scales, as there is much more dark matter in the universe than visible, baryonic matter. More advanced simulations are starting to include baryons and study the formation of individual galaxies. Cosmologists study these simulations to see if they agree with the galaxy surveys, and to understand any discrepancy.
Other, complementary observations to measure the distribution of matter in the distant universe and to probe reionization include:
These will help cosmologists settle the question of when and how structure formed in the universe.
Dark matter.
Evidence from Big Bang nucleosynthesis, the cosmic microwave background and structure formation suggests that about 23% of the mass of the universe consists of non-baryonic dark matter, whereas only 4% consists of visible, baryonic matter. The gravitational effects of dark matter are well understood, as it behaves like a cold, non-radiative fluid that forms haloes around galaxies. Dark matter has never been detected in the laboratory, and the particle physics nature of dark matter remains completely unknown. Without observational constraints, there are a number of candidates, such as a stable supersymmetric particle, a weakly interacting massive particle, an axion, and a massive compact halo object. Alternatives to the dark matter hypothesis include a modification of gravity at small accelerations (MOND) or an effect from brane cosmology.
Dark energy.
If the universe is flat, there must be an additional component making up 73% (in addition to the 23% dark matter and 4% baryons) of the energy density of the universe. This is called dark energy. In order not to interfere with Big Bang nucleosynthesis and the cosmic microwave background, it must not cluster in haloes like baryons and dark matter. There is strong observational evidence for dark energy, as the total energy density of the universe is known through constraints on the flatness of the universe, but the amount of clustering matter is tightly measured, and is much less than this. The case for dark energy was strengthened in 1999, when measurements demonstrated that the expansion of the universe has begun to gradually accelerate.
Apart from its density and its clustering properties, nothing is known about dark energy. "Quantum field theory" predicts a cosmological constant (CC) much like dark energy, but 120 orders of magnitude larger than that observed. Steven Weinberg and a number of string theorists "(see string landscape)" have invoked the 'weak anthropic principle': i.e. the reason that physicists observe a universe with such a small cosmological constant is that no physicists (or any life) could exist in a universe with a larger cosmological constant. Many cosmologists find this an unsatisfying explanation: perhaps because while the weak anthropic principle is self-evident (given that living observers exist, there must be at least one universe with a cosmological constant which allows for life to exist) it does not attempt to explain the context of that universe. For example, the weak anthropic principle alone does not distinguish between:
Other possible explanations for dark energy include quintessence or a modification of gravity on the largest scales. The effect on cosmology of the dark energy that these models describe is given by the dark energy's equation of state, which varies depending upon the theory. The nature of dark energy is one of the most challenging problems in cosmology.
A better understanding of dark energy is likely to solve the problem of the ultimate fate of the universe. In the current cosmological epoch, the accelerated expansion due to dark energy is preventing structures larger than superclusters from forming. It is not known whether the acceleration will continue indefinitely, perhaps even increasing until a big rip, or whether it will eventually reverse.
Other areas of inquiry.
Cosmologists also study:

</doc>
<doc id="5382" url="https://en.wikipedia.org/wiki?curid=5382" title="Inflation (cosmology)">
Inflation (cosmology)

In physical cosmology, cosmic inflation, cosmological inflation, or just inflation is a theory of exponential expansion of space in the early universe. The inflationary epoch lasted from 10−36 seconds after the Big Bang to sometime between 10−33 and 10−32 seconds. Following the inflationary period, the Universe continues to expand, but at a less rapid rate.
Inflation theory was developed in the early 1980s. It explains the origin of the large-scale structure of the cosmos. Quantum fluctuations in the microscopic inflationary region, magnified to cosmic size, become the seeds for the growth of structure in the Universe (see galaxy formation and evolution and structure formation). Many physicists also believe that inflation explains why the Universe appears to be the same in all directions (isotropic), why the cosmic microwave background radiation is distributed evenly, why the Universe is flat, and why no magnetic monopoles have been observed.
The detailed particle physics mechanism responsible for inflation is not known. The basic inflationary paradigm is accepted by most scientists, who believe a number of predictions have been confirmed by observation; however, a substantial minority of scientists dissent from this position. The hypothetical field thought to be responsible for inflation is called the inflaton.
In 2002, three of the original architects of the theory were recognized for their major contributions; physicists Alan Guth of M.I.T., Andrei Linde of Stanford and Paul Steinhardt of Princeton shared the prestigious Dirac Prize "for development of the concept of inflation in cosmology".
Overview.
An expanding universe generally has a cosmological horizon, which, by analogy with the more familiar horizon caused by the curvature of the Earth's surface, marks the boundary of the part of the Universe that an observer can see. Light (or other radiation) emitted by objects beyond the cosmological horizon never reaches the observer, because the space in between the observer and the object is expanding too rapidly.
The observable universe is one "causal patch" of a much larger unobservable universe; other parts of the Universe cannot communicate with Earth yet. These parts of the Universe are outside our current cosmological horizon. In the standard hot big bang model, without inflation, the cosmological horizon moves out, bringing new regions into view. Yet as a local observer sees such a region for the first time, it looks no different from any other region of space the local observer has already seen: its background radiation is at nearly the same temperature as the background radiation of other regions, and its space-time curvature is evolving lock-step with the others. This presents a mystery: how did these new regions know what temperature and curvature they were supposed to have? They couldn't have learned it by getting signals, because they were not previously in communication with our past light cone.
Inflation answers this question by postulating that all the regions come from an earlier era with a big vacuum energy, or cosmological constant. A space with a cosmological constant is qualitatively different: instead of moving outward, the cosmological horizon stays put. For any one observer, the distance to the cosmological horizon is constant. With exponentially expanding space, two nearby observers are separated very quickly; so much so, that the distance between them quickly exceeds the limits of communications. The spatial slices are expanding very fast to cover huge volumes. Things are constantly moving beyond the cosmological horizon, which is a fixed distance away, and everything becomes homogeneous.
As the inflationary field slowly relaxes to the vacuum, the cosmological constant goes to zero and space begins to expand normally. The new regions that come into view during the normal expansion phase are exactly the same regions that were pushed out of the horizon during inflation, and so they are at nearly the same temperature and curvature, because they come from the same originally small patch of space.
The theory of inflation thus explains why the temperatures and curvatures of different regions are so nearly equal. It also predicts that the total curvature of a space-slice at constant global time is zero. This prediction implies that the total ordinary matter, dark matter and residual vacuum energy in the Universe have to add up to the critical density, and the evidence supports this. More strikingly, inflation allows physicists to calculate the minute differences in temperature of different regions from quantum fluctuations during the inflationary era, and many of these quantitative predictions have been confirmed.
Space expands.
To say that space expands exponentially means that two inertial observers are moving farther apart with accelerating velocity. In stationary coordinates for one observer, a patch of an inflating universe has the following polar metric:
This is just like an inside-out black hole metric—it has a zero in the "dt" component on a fixed radius sphere called the cosmological horizon. Objects are drawn away from the observer at "r" = 0 towards the cosmological horizon, which they cross in a finite proper time. This means that any inhomogeneities are smoothed out, just as any bumps or matter on the surface of a black hole horizon are swallowed and disappear.
Since the space–time metric has no explicit time dependence, once an observer has crossed the cosmological horizon, observers closer in take its place. This process of falling outward and replacement points closer in are always steadily replacing points further out—an exponential expansion of space–time.
This steady-state exponentially expanding spacetime is called a de Sitter space, and to sustain it there must be a cosmological constant, a vacuum energy proportional to Λ everywhere. In this case, the equation of state is "p = −ρ". The physical conditions from one moment to the next are stable: the rate of expansion, called the Hubble parameter, is nearly constant, and the scale factor of the Universe is proportional to "eHt". Inflation is often called a period of "accelerated expansion" because the distance between two fixed observers is increasing exponentially (i.e. at an accelerating rate as they move apart), while Λ can stay approximately constant (see deceleration parameter).
Few inhomogeneities remain.
Cosmological inflation has the important effect of smoothing out inhomogeneities, anisotropies and the curvature of space. This pushes the Universe into a very simple state, in which it is completely dominated by the inflaton field, the source of the cosmological constant, and the only significant inhomogeneities are the tiny quantum fluctuations in the inflaton. Inflation also dilutes exotic heavy particles, such as the magnetic monopoles predicted by many extensions to the Standard Model of particle physics. If the Universe was only hot enough to form such particles "before" a period of inflation, they would not be observed in nature, as they would be so rare that it is quite likely that there are none in the observable universe. Together, these effects are called the inflationary "no-hair theorem" by analogy with the no hair theorem for black holes.
The "no-hair" theorem works essentially because the cosmological horizon is no different from a black-hole horizon, except for philosophical disagreements about what is on the other side. The interpretation of the no-hair theorem is that the Universe (observable and unobservable) expands by an enormous factor during inflation. In an expanding universe, energy densities generally fall, or get diluted, as the volume of the Universe increases. For example, the density of ordinary "cold" matter (dust) goes down as the inverse of the volume: when linear dimensions double, the energy density goes down by a factor of eight; the radiation energy density goes down even more rapidly as the Universe expands since the wavelength of each photon is stretched (redshifted), in addition to the photons being dispersed by the expansion. When linear dimensions are doubled, the energy density in radiation falls by a factor of sixteen (see the solution of the energy density continuity equation for an ultra-relativistic fluid). During inflation, the energy density in the inflaton field is roughly constant. However, the energy density in everything else, including inhomogeneities, curvature, anisotropies, exotic particles, and standard-model particles is falling, and through sufficient inflation these all become negligible. This leaves the Universe flat and symmetric, and (apart from the homogeneous inflaton field) mostly empty, at the moment inflation ends and reheating begins.
Duration.
A key requirement is that inflation must continue long enough to produce the present observable universe from a single, small inflationary Hubble volume. This is necessary to ensure that the Universe appears flat, homogeneous and isotropic at the largest observable scales. This requirement is generally thought to be satisfied if the Universe expanded by a factor of at least 1026 during inflation.
Reheating.
Inflation is a period of supercooled expansion, when the temperature drops by a factor of 100,000 or so. (The exact drop is model dependent, but in the first models it was typically from 1027 K down to 1022 K.) This relatively low temperature is maintained during the inflationary phase. When inflation ends the temperature returns to the pre-inflationary temperature; this is called "reheating" or thermalization because the large potential energy of the inflaton field decays into particles and fills the Universe with Standard Model particles, including electromagnetic radiation, starting the radiation dominated phase of the Universe. Because the nature of the inflation is not known, this process is still poorly understood, although it is believed to take place through a parametric resonance.
Motivations.
Inflation resolves several problems in Big Bang cosmology that were discovered in the 1970s. Inflation was first proposed by Guth while investigating the problem of why no magnetic monopoles are seen today; he found that a positive-energy false vacuum would, according to general relativity, generate an exponential expansion of space. It was very quickly realised that such an expansion would resolve many other long-standing problems. These problems arise from the observation that to look like it does "today", the Universe would have to have started from very finely tuned, or "special" initial conditions at the Big Bang. Inflation attempts to resolve these problems by providing a dynamical mechanism that drives the Universe to this special state, thus making a universe like ours much more likely in the context of the Big Bang theory.
Horizon problem.
The horizon problem is the problem of determining why the Universe appears statistically homogeneous and isotropic in accordance with the cosmological principle. For example, molecules in a canister of gas are distributed homogeneously and isotropically because they are in thermal equilibrium: gas throughout the canister has had enough time to interact to dissipate inhomogeneities and anisotropies. The situation is quite different in the big bang model without inflation, because gravitational expansion does not give the early universe enough time to equilibrate. In a big bang with only the matter and radiation known in the Standard Model, two widely separated regions of the observable universe cannot have equilibrated because they move apart from each other faster than the speed of light and thus have never come into causal contact. In the early Universe, it was not possible to send a light signal between the two regions. Because they have had no interaction, it is difficult to explain why they have the same temperature (are thermally equilibrated). Historically, proposed solutions included the "Phoenix universe" of Georges Lemaître, the related oscillatory universe of Richard Chase Tolman, and the Mixmaster universe of Charles Misner. Lemaître and Tolman proposed that a universe undergoing a number of cycles of contraction and expansion could come into thermal equilibrium. Their models failed, however, because of the buildup of entropy over several cycles. Misner made the (ultimately incorrect) conjecture that the Mixmaster mechanism, which made the Universe "more" chaotic, could lead to statistical homogeneity and isotropy.
Flatness problem.
The flatness problem is sometimes called one of the Dicke coincidences (along with the cosmological constant problem). It became known in the 1960s that the density of matter in the Universe was comparable to the critical density necessary for a flat universe (that is, a universe whose large scale geometry is the usual Euclidean geometry, rather than a non-Euclidean hyperbolic or spherical geometry).
Therefore, regardless of the shape of the universe the contribution of spatial curvature to the expansion of the Universe could not be much greater than the contribution of matter. But as the Universe expands, the curvature redshifts away more slowly than matter and radiation. Extrapolated into the past, this presents a fine-tuning problem because the contribution of curvature to the Universe must be exponentially small (sixteen orders of magnitude less than the density of radiation at big bang nucleosynthesis, for example). This problem is exacerbated by recent observations of the cosmic microwave background that have demonstrated that the Universe is flat to within a few percent.
Magnetic-monopole problem.
The magnetic monopole problem, sometimes called the exotic-relics problem, says that if the early universe were very hot, a large number of very heavy, stable magnetic monopoles would have been produced. This is a problem with Grand Unified Theories, which propose that at high temperatures (such as in the early universe) the electromagnetic force, strong, and weak nuclear forces are not actually fundamental forces but arise due to spontaneous symmetry breaking from a single gauge theory. These theories predict a number of heavy, stable particles that have not been observed in nature. The most notorious is the magnetic monopole, a kind of stable, heavy "charge" of magnetic field. Monopoles are predicted to be copiously produced following Grand Unified Theories at high temperature, and they should have persisted to the present day, to such an extent that they would become the primary constituent of the Universe. Not only is that not the case, but all searches for them have failed, placing stringent limits on the density of relic magnetic monopoles in the Universe. A period of inflation that occurs below the temperature where magnetic monopoles can be produced would offer a possible resolution of this problem: monopoles would be separated from each other as the Universe around them expands, potentially lowering their observed density by many orders of magnitude. Though, as cosmologist Martin Rees has written, "Skeptics about exotic physics might not be hugely impressed by a theoretical argument to explain the absence of particles that are themselves only hypothetical. Preventive medicine can readily seem 100 percent effective against a disease that doesn't exist!"
History.
Precursors.
In the early days of General Relativity, Albert Einstein introduced the cosmological constant to allow a static solution, which was a three-dimensional sphere with a uniform density of matter. Later, Willem de Sitter found a highly symmetric inflating universe, which described a universe with a cosmological constant that is otherwise empty. It was discovered that Einstein's universe is unstable, and that small fluctuations cause it to collapse or turn into a de Sitter universe.
In the early 1970s Zeldovich noticed the flatness and horizon problems of Big Bang cosmology; before his work, cosmology was presumed to be symmetrical on purely philosophical grounds. In the Soviet Union, this and other considerations led Belinski and Khalatnikov to analyze the chaotic BKL singularity in General Relativity. Misner's Mixmaster universe attempted to use this chaotic behavior to solve the cosmological problems, with limited success.
In the late 1970s, Sidney Coleman applied the instanton techniques developed by Alexander Polyakov and collaborators to study the fate of the false vacuum in quantum field theory. Like a metastable phase in statistical mechanics—water below the freezing temperature or above the boiling point—a quantum field would need to nucleate a large enough bubble of the new vacuum, the new phase, in order to make a transition. Coleman found the most likely decay pathway for vacuum decay and calculated the inverse lifetime per unit volume. He eventually noted that gravitational effects would be significant, but he did not calculate these effects and did not apply the results to cosmology.
In the Soviet Union, Alexei Starobinsky noted that quantum corrections to general relativity should be important for the early universe. These generically lead to curvature-squared corrections to the Einstein–Hilbert action and a form of "f"("R") modified gravity. The solution to Einstein's equations in the presence of curvature squared terms, when the curvatures are large, leads to an effective cosmological constant. Therefore, he proposed that the early universe went through an inflationary de Sitter era. This resolved the cosmology problems and led to specific predictions for the corrections to the microwave background radiation, corrections that were then calculated in detail.
In 1978, Zeldovich noted the monopole problem, which was an unambiguous quantitative version of the horizon problem, this time in a subfield of particle physics, which led to several speculative attempts to resolve it. In 1980 Alan Guth realized that false vacuum decay in the early universe would solve the problem, leading him to propose a scalar-driven inflation. Starobinsky's and Guth's scenarios both predicted an initial deSitter phase, differing only in mechanistic details.
Early inflationary models.
Guth proposed inflation in January 1980 to explain the nonexistence of magnetic monopoles; it was Guth who coined the term "inflation". At the same time, Starobinsky argued that quantum corrections to gravity would replace the initial singularity of the Universe with an exponentially expanding deSitter phase. In October 1980, Demosthenes Kazanas suggested that exponential expansion could eliminate the particle horizon and perhaps solve the horizon problem, while Sato suggested that an exponential expansion could eliminate domain walls (another kind of exotic relic). In 1981 Einhorn and Sato published a model similar to Guth's and showed that it would resolve the puzzle of the magnetic monopole abundance in Grand Unified Theories. Like Guth, they concluded that such a model not only required fine tuning of the cosmological constant, but also would likely lead to a much too granular universe, i.e., to large density variations resulting from bubble wall collisions.
Guth proposed that as the early universe cooled, it was trapped in a false vacuum with a high energy density, which is much like a cosmological constant. As the very early universe cooled it was trapped in a metastable state (it was supercooled), which it could only decay out of through the process of bubble nucleation via quantum tunneling. Bubbles of true vacuum spontaneously form in the sea of false vacuum and rapidly begin expanding at the speed of light. Guth recognized that this model was problematic because the model did not reheat properly: when the bubbles nucleated, they did not generate any radiation. Radiation could only be generated in collisions between bubble walls. But if inflation lasted long enough to solve the initial conditions problems, collisions between bubbles became exceedingly rare. In any one causal patch it is likely that only one bubble would nucleate.
Slow-roll inflation.
The bubble collision problem was solved by Linde and independently by Andreas Albrecht and Paul Steinhardt in a model named "new inflation" or "slow-roll inflation" (Guth's model then became known as "old inflation"). In this model, instead of tunneling out of a false vacuum state, inflation occurred by a scalar field rolling down a potential energy hill. When the field rolls very slowly compared to the expansion of the Universe, inflation occurs. However, when the hill becomes steeper, inflation ends and reheating can occur.
Effects of asymmetries.
Eventually, it was shown that new inflation does not produce a perfectly symmetric universe, but that quantum fluctuations in the inflaton are created. These fluctuations form the primordial seeds for all structure created in the later universe. These fluctuations were first calculated by Viatcheslav Mukhanov and G. V. Chibisov in analyzing Starobinsky's similar model. In the context of inflation, they were worked out independently of the work of Mukhanov and Chibisov at the three-week 1982 Nuffield Workshop on the Very Early Universe at Cambridge University. The fluctuations were calculated by four groups working separately over the course of the workshop: Stephen Hawking; Starobinsky; Guth and So-Young Pi; and Bardeen, Steinhardt and Turner.
Observational status.
Inflation is a mechanism for realizing the cosmological principle, which is the basis of the standard model of physical cosmology: it accounts for the homogeneity and isotropy of the observable universe. In addition, it accounts for the observed flatness and absence of magnetic monopoles. Since Guth's early work, each of these observations has received further confirmation, most impressively by the detailed observations of the cosmic microwave background made by the Wilkinson Microwave Anisotropy Probe (WMAP) spacecraft. This analysis shows that the Universe is flat to within at least a few percent, and that it is homogeneous and isotropic to one part in 100,000.
In addition, inflation predicts that the structures visible in the Universe today formed through the gravitational collapse of perturbations that were formed as quantum mechanical fluctuations in the inflationary epoch. The detailed form of the spectrum of perturbations called a nearly-scale-invariant Gaussian random field (or Harrison–Zel'dovich spectrum) is very specific and has only two free parameters, the amplitude of the spectrum and the "spectral index", which measures the slight deviation from scale invariance predicted by inflation (perfect scale invariance corresponds to the idealized de Sitter universe). Inflation predicts that the observed perturbations should be in thermal equilibrium with each other (these are called "adiabatic" or "isentropic" perturbations). This structure for the perturbations has been confirmed by the WMAP spacecraft and other cosmic microwave background (CMB) experiments, and galaxy surveys, especially the ongoing Sloan Digital Sky Survey. These experiments have shown that the one part in 100,000 inhomogeneities observed have exactly the form predicted by theory. Moreover, there is evidence for a slight deviation from scale invariance. The "spectral index", "n"s is equal to one for a scale-invariant spectrum. The simplest inflation models predict that this quantity is between 0.92 and 0.98. From WMAP data it can be inferred that "n"s = 0.963 ± 0.012, implying that it differs from one at the level of two standard deviations (2σ). This is considered an important confirmation of the theory of inflation.
Various inflation theories have been proposed that make radically different predictions, but they generally have much more fine tuning than should be necessary. As a physical model, however, inflation is most valuable in that it robustly predicts the initial conditions of the Universe based on only two adjustable parameters: the spectral index (that can only change in a small range) and the amplitude of the perturbations. Except in contrived models, this is true regardless of how inflation is realized in particle physics.
Occasionally, effects are observed that appear to contradict the simplest models of inflation. The first-year WMAP data suggested that the spectrum might not be nearly scale-invariant, but might instead have a slight curvature. However, the third-year data revealed that the effect was a statistical anomaly. Another effect remarked upon since the first cosmic microwave background satellite, the Cosmic Background Explorer is that the amplitude of the quadrupole moment of the CMB is unexpectedly low and the other low multipoles appear to be preferentially aligned with the ecliptic plane. Some have claimed that this is a signature of non-Gaussianity and thus contradicts the simplest models of inflation. Others have suggested that the effect may be due to other new physics, foreground contamination, or even publication bias.
An experimental program is underway to further test inflation with more precise CMB measurements. In particular, high precision measurements of the so-called "B-modes" of the polarization of the background radiation could provide evidence of the gravitational radiation produced by inflation, and could also show whether the energy scale of inflation predicted by the simplest models (1015–1016 GeV) is correct. In March 2014, it was announced that B-mode CMB polarization consistent with that predicted from inflation had been demonstrated by a South Pole experiment. However, on 19 June 2014, lowered confidence in confirming the findings was reported; on 19 September 2014, a further reduction in confidence was reported and, on 30 January 2015, even less confidence yet was reported.
Other potentially corroborating measurements are expected from the Planck spacecraft, although it is unclear if the signal will be visible, or if contamination from foreground sources will interfere. Other forthcoming measurements, such as those of 21 centimeter radiation (radiation emitted and absorbed from neutral hydrogen before the first stars turned on), may measure the power spectrum with even greater resolution than the CMB and galaxy surveys, although it is not known if these measurements will be possible or if interference with radio sources on Earth and in the galaxy will be too great.
Dark energy is broadly similar to inflation and is thought to be causing the expansion of the present-day universe to accelerate. However, the energy scale of dark energy is much lower, 10−12 GeV, roughly 27 orders of magnitude less than the scale of inflation.
Theoretical status.
In Guth's early proposal, it was thought that the inflaton was the Higgs field, the field that explains the mass of the elementary particles. It is now believed by some that the inflaton cannot be the Higgs field although the recent discovery of the Higgs boson has increased the number of works considering the Higgs field as inflaton. One problem of this identification is the current tension with experimental data at the electroweak scale, which is currently under study at the Large Hadron Collider (LHC). Other models of inflation relied on the properties of Grand Unified Theories. Since the simplest models of grand unification have failed, it is now thought by many physicists that inflation will be included in a supersymmetric theory such as string theory or a supersymmetric grand unified theory. At present, while inflation is understood principally by its detailed predictions of the initial conditions for the hot early universe, the particle physics is largely "ad hoc" modelling. As such, although predictions of inflation have been consistent with the results of observational tests, many open questions remain.
Fine-tuning problem.
One of the most severe challenges for inflation arises from the need for fine tuning. In new inflation, the "slow-roll conditions" must be satisfied for inflation to occur. The slow-roll conditions say that the inflaton potential must be flat (compared to the large vacuum energy) and that the inflaton particles must have a small mass. New inflation requires the Universe to have a scalar field with an especially flat potential and special initial conditions. However, explanations for these fine-tunings have been proposed. For example, classically scale invariant field theories, where scale invariance is broken by quantum effects, provide an explanation of the flatness of inflationary potentials, as long as the theory can be studied through perturbation theory.
Andrei Linde.
Linde proposed a theory known as "chaotic inflation" in which he suggested that the conditions for inflation were actually satisfied quite generically. Inflation will occur in virtually any universe that begins in a chaotic, high energy state that has a scalar field with unbounded potential energy. However, in his model the inflaton field necessarily takes values larger than one Planck unit: for this reason, these are often called "large field" models and the competing new inflation models are called "small field" models. In this situation, the predictions of effective field theory are thought to be invalid, as renormalization should cause large corrections that could prevent inflation. This problem has not yet been resolved and some cosmologists argue that the small field models, in which inflation can occur at a much lower energy scale, are better models. While inflation depends on quantum field theory (and the semiclassical approximation to quantum gravity) in an important way, it has not been completely reconciled with these theories.
Brandenberger commented on fine-tuning in another situation. The amplitude of the primordial inhomogeneities produced in inflation is directly tied to the energy scale of inflation. This scale is suggested to be around 1016 GeV or 10−3 times the Planck energy. The natural scale is naïvely the Planck scale so this small value could be seen as another form of fine-tuning (called a hierarchy problem): the energy density given by the scalar potential is down by 10−12 compared to the Planck density. This is not usually considered to be a critical problem, however, because the scale of inflation corresponds naturally to the scale of gauge unification.
Eternal inflation.
In many models, the inflationary phase of the Universe's expansion lasts forever in at least some regions of the Universe. This occurs because inflating regions expand very rapidly, reproducing themselves. Unless the rate of decay to the non-inflating phase is sufficiently fast, new inflating regions are produced more rapidly than non-inflating regions. In such models most of the volume of the Universe at any given time is inflating. All models of eternal inflation produce an infinite multiverse, typically a fractal.
Although new inflation is classically rolling down the potential, quantum fluctuations can sometimes lift it to previous levels. These regions in which the inflaton fluctuates upwards expand much faster than regions in which the inflaton has a lower potential energy, and tend to dominate in terms of physical volume. This steady state, which first developed by Vilenkin, is called "eternal inflation". It has been shown that any inflationary theory with an unbounded potential is eternal. It is a popular conclusion among physicists that this steady state cannot continue forever into the past. Inflationary spacetime, which is similar to de Sitter space, is incomplete without a contracting region. However, unlike de Sitter space, fluctuations in a contracting inflationary space collapse to form a gravitational singularity, a point where densities become infinite. Therefore, it is necessary to have a theory for the Universe's initial conditions. Linde, however, believes inflation may be past eternal.
In eternal inflation, regions with inflation have an exponentially growing volume, while regions that are not inflating don't. This suggests that the volume of the inflating part of the Universe in the global picture is always unimaginably larger than the part that has stopped inflating, even though inflation eventually ends as seen by any single pre-inflationary observer. Scientists disagree about how to assign a probability distribution to this hypothetical anthropic landscape. If the probability of different regions is counted by volume, one should expect that inflation will never end or applying boundary conditions that a local observer exists to observe it, that inflation will end as late as possible. Some physicists believe this paradox can be resolved by weighting observers by their pre-inflationary volume.
Initial conditions.
Some physicists have tried to avoid the initial conditions problem by proposing models for an eternally inflating universe with no origin. These models propose that while the Universe, on the largest scales, expands exponentially it was, is and always will be, spatially infinite and has existed, and will exist, forever.
Other proposals attempt to describe the ex nihilo creation of the Universe based on quantum cosmology and the following inflation. Vilenkin put forth one such scenario. Hartle and Hawking offered the no-boundary proposal for the initial creation of the Universe in which inflation comes about naturally.
Guth described the inflationary universe as the "ultimate free lunch": new universes, similar to our own, are continually produced in a vast inflating background. Gravitational interactions, in this case, circumvent (but do not violate) the first law of thermodynamics (energy conservation) and the second law of thermodynamics (entropy and the arrow of time problem). However, while there is consensus that this solves the initial conditions problem, some have disputed this, as it is much more likely that the Universe came about by a quantum fluctuation. Don Page was an outspoken critic of inflation because of this anomaly. He stressed that the thermodynamic arrow of time necessitates low entropy initial conditions, which would be highly unlikely. According to them, rather than solving this problem, the inflation theory aggravates it – the reheating at the end of the inflation era increases entropy, making it necessary for the initial state of the Universe to be even more orderly than in other Big Bang theories with no inflation phase.
Hawking and Page later found ambiguous results when they attempted to compute the probability of inflation in the Hartle-Hawking initial state. Other authors have argued that, since inflation is eternal, the probability doesn't matter as long as it is not precisely zero: once it starts, inflation perpetuates itself and quickly dominates the Universe. However, Albrecht and Lorenzo Sorbo argued that the probability of an inflationary cosmos, consistent with today's observations, emerging by a random fluctuation from some pre-existent state is much higher than that of a non-inflationary cosmos. This is because the "seed" amount of non-gravitational energy required for the inflationary cosmos is so much less than that for a non-inflationary alternative, which outweighs any entropic considerations.
Another problem that has occasionally been mentioned is the trans-Planckian problem or trans-Planckian effects. Since the energy scale of inflation and the Planck scale are relatively close, some of the quantum fluctuations that have made up the structure in our universe were smaller than the Planck length before inflation. Therefore, there ought to be corrections from Planck-scale physics, in particular the unknown quantum theory of gravity. Some disagreement remains about the magnitude of this effect: about whether it is just on the threshold of detectability or completely undetectable.
Hybrid inflation.
Another kind of inflation, called "hybrid inflation", is an extension of new inflation. It introduces additional scalar fields, so that while one of the scalar fields is responsible for normal slow roll inflation, another triggers the end of inflation: when inflation has continued for sufficiently long, it becomes favorable to the second field to decay into a much lower energy state.
In hybrid inflation, one scalar field is responsible for most of the energy density (thus determining the rate of expansion), while another is responsible for the slow roll (thus determining the period of inflation and its termination). Thus fluctuations in the former inflaton would not affect inflation termination, while fluctuations in the latter would not affect the rate of expansion. Therefore, hybrid inflation is not eternal. When the second (slow-rolling) inflaton reaches the bottom of its potential, it changes the location of the minimum of the first inflaton's potential, which leads to a fast roll of the inflaton down its potential, leading to termination of inflation.
Inflation and string cosmology.
The discovery of flux compactifications opened the way for reconciling inflation and string theory. "Brane inflation" suggests that inflation arises from the motion of D-branes in the compactified geometry, usually towards a stack of anti-D-branes. This theory, governed by the "Dirac-Born-Infeld action", is different from ordinary inflation. The dynamics are not completely understood. It appears that special conditions are necessary since inflation occurs in tunneling between two vacua in the string landscape. The process of tunneling between two vacua is a form of old inflation, but new inflation must then occur by some other mechanism.
Inflation and loop quantum gravity.
When investigating the effects the theory of loop quantum gravity would have on cosmology, a loop quantum cosmology model has evolved that provides a possible mechanism for cosmological inflation. Loop quantum gravity assumes a quantized spacetime. If the energy density is larger than can be held by the quantized spacetime, it is thought to bounce back.
Alternatives.
Other models explain some of the observations explained by inflation. However none of these "alternatives" has the same breadth of explanation and still require inflation for a more complete fit with observation. They should therefore be regarded as adjuncts to inflation, rather than as alternatives.
Big bounce.
The big bounce hypothesis attempts to replace the cosmic singularity with a cosmic contraction and bounce, thereby explaining the initial conditions that led to the big bang. The flatness and horizon problems are naturally solved in the Einstein-Cartan-Sciama-Kibble theory of gravity, without needing an exotic form of matter or free parameters. This theory extends general relativity by removing a constraint of the symmetry of the affine connection and regarding its antisymmetric part, the torsion tensor, as a dynamical variable. The minimal coupling between torsion and Dirac spinors generates a spin-spin interaction that is significant in fermionic matter at extremely high densities. Such an interaction averts the unphysical Big Bang singularity, replacing it with a cusp-like bounce at a finite minimum scale factor, before which the Universe was contracting. The rapid expansion immediately after the Big Bounce explains why the present Universe at largest scales appears spatially flat, homogeneous and isotropic. As the density of the Universe decreases, the effects of torsion weaken and the Universe smoothly enters the radiation-dominated era.
String theory.
String theory requires that, in addition to the three observable spatial dimensions, additional dimensions exist that are curled up or compactified (see also Kaluza–Klein theory). Extra dimensions appear as a frequent component of supergravity models and other approaches to quantum gravity. This raised the contingent question of why four space-time dimensions became large and the rest became unobservably small. An attempt to address this question, called "string gas cosmology", was proposed by Robert Brandenberger and Cumrun Vafa. This model focuses on the dynamics of the early universe considered as a hot gas of strings. Brandenberger and Vafa show that a dimension of spacetime can only expand if the strings that wind around it can efficiently annihilate each other. Each string is a one-dimensional object, and the largest number of dimensions in which two strings will generically intersect (and, presumably, annihilate) is three. Therefore, the most likely number of non-compact (large) spatial dimensions is three. Current work on this model centers on whether it can succeed in stabilizing the size of the compactified dimensions and produce the correct spectrum of primordial density perturbations. Supporters admit that their model "does not solve the entropy and flatness problems of standard cosmology ... and we can provide no explanation for why the current universe is so close to being spatially flat".
Ekpyrotic and cyclic models.
The ekpyrotic and cyclic models are also considered adjuncts to inflation. These models solve the horizon problem through an expanding epoch well "before" the Big Bang, and then generate the required spectrum of primordial density perturbations during a contracting phase leading to a Big Crunch. The Universe passes through the Big Crunch and emerges in a hot Big Bang phase. In this sense they are reminiscent of Richard Chace Tolman's oscillatory universe; in Tolman's model, however, the total age of the Universe is necessarily finite, while in these models this is not necessarily so. Whether the correct spectrum of density fluctuations can be produced, and whether the Universe can successfully navigate the Big Bang/Big Crunch transition, remains a topic of controversy and current research. Ekpyrotic models avoid the magnetic monopole problem as long as the temperature at the Big Crunch/Big Bang transition remains below the Grand Unified Scale, as this is the temperature required to produce magnetic monopoles in the first place. As things stand, there is no evidence of any 'slowing down' of the expansion, but this is not surprising as each cycle is expected to last on the order of a trillion years.
Varying C.
Another adjunct, the varying speed of light model was offered by Jean-Pierre Petit in 1988, John Moffat in 1992 as well Albrecht and João Magueijo in 1999, instead of superluminal expansion the speed of light was 60 orders of magnitude faster than its current value solving the horizon and homogeneity problems in the early universe.
Criticisms.
Since its introduction by Alan Guth in 1980, the inflationary paradigm has become widely accepted. Nevertheless, many physicists, mathematicians, and philosophers of science have voiced criticisms, claiming untestable predictions and a lack of serious empirical support. In 1999, John Earman and Jesús Mosterín published a thorough critical review of inflationary cosmology, concluding, "we do not think that there are, as yet, good grounds for admitting any of the models of inflation into the standard core of cosmology."
In order to work, and as pointed out by Roger Penrose from 1986 on, inflation requires extremely specific initial conditions of its own, so that the problem (or pseudo-problem) of initial conditions is not solved: "There is something fundamentally misconceived about trying to explain the uniformity of the early universe as resulting from a thermalization process. [...] For, if the thermalization is actually doing anything [...] then it represents a definite increasing of the entropy. Thus, the universe would have been even more special before the thermalization than after." The problem of specific or "fine-tuned" initial conditions would not have been solved; it would have gotten worse. At a conference in 2015, Penrose said that "inflation isn't falsifiable, it's falsified. […] BICEP did a wonderful service by bringing all the Inflation-ists out of their shell, and giving them a black eye."This is a collation of remarks from the third day of the "Cosmic Microwave Background @50" conference held at Princeton, 10–12 June 2015.</ref>
A recurrent criticism of inflation is that the invoked inflation field does not correspond to any known physical field, and that its potential energy curve seems to be an ad hoc contrivance to accommodate almost any data obtainable. Paul Steinhardt, one of the founding fathers of inflationary cosmology, has recently become one of its sharpest critics. He calls 'bad inflation' a period of accelerated expansion whose outcome conflicts with observations, and 'good inflation' one compatible with them: "Not only is bad inflation more likely than good inflation, but no inflation is more likely than either... Roger Penrose considered all the possible configurations of the inflaton and gravitational fields. Some of these configurations lead to inflation ... Other configurations lead to a uniform, flat universe directly – without inflation. Obtaining a flat universe is unlikely overall. Penrose's shocking conclusion, though, was that obtaining a flat universe without inflation is much more likely than with inflation – by a factor of 10 to the googol (10 to the 100) power!" Together with Anna Ijjas and Abraham Loeb, he wrote articles claiming that the inflationary paradigm is in trouble in view of the data from the Planck satellite. Counter-arguments were presented by Alan Guth, David Kaiser, and Yasunori Nomura and by Andrei Linde, saying that "cosmic inflation is on a stronger footing than ever before".

</doc>
<doc id="5385" url="https://en.wikipedia.org/wiki?curid=5385" title="Candela">
Candela

The candela ( or ; symbol: cd) is the SI base unit of luminous intensity; that is, luminous power per unit solid angle emitted by a point light source in a particular direction. Luminous intensity is analogous to radiant intensity, but instead of simply adding up the contributions of every wavelength of light in the source's spectrum, the contribution of each wavelength is weighted by the standard luminosity function (a model of the sensitivity of the human eye to different wavelengths). A common candle emits light with a luminous intensity of roughly one candela. If emission in some directions is blocked by an opaque barrier, the emission would still be approximately one candela in the directions that are not obscured.
The word "candela" means "candle" in Latin.
Definition.
Like most other SI base units, the candela has an operational definition—it is defined by a description of a physical process that will produce one candela of luminous intensity. Since the 16th General Conference on Weights and Measures (CGPM) in 1979, the candela has been defined as:
The candela is the luminous intensity, in a given direction, of a source that emits monochromatic radiation of frequency 540 hertz and that has a radiant intensity in that direction of  watt per steradian.
The definition describes how to produce a light source that (by definition) emits one candela. Such a source could then be used to calibrate instruments designed to measure luminous intensity.
The candela is sometimes still called by the old name "candle", such as in "foot-candle" and the modern definition of "candlepower".
Explanation.
The frequency chosen is in the visible spectrum near green, corresponding to a wavelength of about 555 nanometres. The human eye is most sensitive to this frequency, when adapted for bright conditions. At other frequencies, more radiant intensity is required to achieve the same luminous intensity, according to the frequency response of the human eye. The luminous intensity for light of a particular wavelength λ is given by
where "I"v(λ) is the luminous intensity in candelas, "I"e(λ) is the radiant intensity in W/sr and formula_2 is the standard luminosity function (photopic). If more than one wavelength is present (as is usually the case), one must sum or integrate over the spectrum of wavelengths present to get the total luminous intensity.
Examples.
A common candle emits light with roughly 1 cd luminous intensity. A 25 W compact fluorescent light bulb puts out around 1700 lumens; if that light is radiated equally in all directions, it will have an intensity of around 135 cd (135 lumens/steradian). Focused into a 20° beam, it will have an intensity of around .
The luminous intensity of light-emitting diodes is measured in millicandelas (mcd), or thousandths of a candela. Indicator LEDs are typically in the 50 mcd range; "ultra-bright" LEDs can reach (15 cd), or higher.
Origin.
Prior to 1948, various standards for luminous intensity were in use in a number of countries. These were typically based on the brightness of the flame from a "standard candle" of defined composition, or the brightness of an incandescent filament of specific design. One of the best-known of these was the English standard of candlepower. One candlepower was the light produced by a pure spermaceti candle weighing one sixth of a pound and burning at a rate of 120 grains per hour. Germany, Austria and Scandinavia used the Hefnerkerze, a unit based on the output of a Hefner lamp.
It became clear that a better-defined unit was needed. The Commission Internationale de l'Éclairage (International Commission on Illumination) and the CIPM proposed a “new candle” based on the luminance of a Planck radiator (a black body) at the temperature of freezing platinum. The value of the new unit was chosen to make it similar to the earlier unit candlepower. The decision was promulgated by the CIPM in 1946:
The value of the new candle is such that the brightness of the full radiator at the temperature of solidification of platinum is 60 new candles per square centimetre.
It was then ratified in 1948 by the 9th CGPM which adopted a new name for this unit, the "candela". In 1967 the 13th CGPM removed the term "new candle" and gave an amended version of the candela definition, specifying the atmospheric pressure applied to the freezing platinum:
The candela is the luminous intensity, in the perpendicular direction, of a surface of square metre of a black body at the temperature of freezing platinum under a pressure of  newtons per square metre.
In 1979, because of the difficulties in realizing a Planck radiator at high temperatures and the new possibilities offered by radiometry, the 16th CGPM adopted the modern definition of the candela. The arbitrary (1/683) term was chosen so that the new definition would exactly match the old definition. Although the candela is now defined in terms of the second (an SI base unit) and the watt (a derived SI unit), the candela remains a base unit of the SI system, by definition.
SI photometric light units.
Relationships between luminous intensity, luminous flux, and illuminance.
If a source emits a known luminous intensity "I"v (in candelas) in a well-defined cone, the total luminous flux "Φ"v in lumens is given by
where "A" is the "radiation angle" of the lamp—the full vertex angle of the emission cone. For example, a lamp that emits 590 cd with a radiation angle of 40° emits about 224 lumens. See MR16 for emission angles of some common lamps.
If the source emits light uniformly in all directions, the flux can be found by multiplying the intensity by 4π: a uniform 1 candela source emits 12.6 lumens.
For the purpose of measuring illumination, the candela is not a practical unit, as it only applies to idealized point light sources, each approximated by a source small compared to the distance from which its luminous radiation is measured, also assuming that it is done so in the absence of other light sources. What gets directly measured by a light meter is incident light on a sensor of finite area, i.e. illuminance in lm/m2 (lux). However, if designing illumination from many point light sources, like light bulbs, of known approximate omnidirectionally-uniform intensities, the contributions to illuminance from incoherent light being additive, it is mathematically estimated as follows. If r"i" is the position of the "i"-th source of uniform intensity "Ii", and â is the unit vector normal to the illuminated elemental opaque area "dA" being measured, and provided that all light sources lie in the same half-space divided by the plane of this area,
In the case of a single point light source of intensity "Iv", at a distance "r" and normally incident, this reduces to

</doc>
<doc id="5387" url="https://en.wikipedia.org/wiki?curid=5387" title="Condensed matter physics">
Condensed matter physics

Condensed matter physics is a branch of physics that deals with the physical properties of condensed phases of matter. Condensed matter physicists seek to understand the behavior of these phases by using physical laws. In particular, they include the laws of quantum mechanics, electromagnetism and statistical mechanics.
The most familiar condensed phases are solids and liquids while more exotic condensed phases include the superconducting phase exhibited by certain materials at low temperature, the ferromagnetic and antiferromagnetic phases of spins on atomic lattices, and the Bose–Einstein condensate found in cold atomic systems. The study of condensed matter physics involves measuring various material properties via experimental probes along with using techniques of theoretical physics to develop mathematical models that help in understanding physical behavior.
The diversity of systems and phenomena available for study makes condensed matter physics the most active field of contemporary physics: one third of all American physicists identify themselves as condensed matter physicists, and the Division of Condensed Matter Physics is the largest division at the American Physical Society. The field overlaps with chemistry, materials science, and nanotechnology, and relates closely to atomic physics and biophysics. Theoretical condensed matter physics shares important concepts and techniques with theoretical particle and nuclear physics.
A variety of topics in physics such as crystallography, metallurgy, elasticity, magnetism, etc., were treated as distinct areas until the 1940s, when they were grouped together as "solid state physics". Around the 1960s, the study of physical properties of liquids was added to this list, forming the basis for the new, related specialty of condensed matter physics. According to physicist Philip Warren Anderson, the term was coined by him and Volker Heine, when they changed the name of their group at the Cavendish Laboratories, Cambridge from "Solid state theory" to "Theory of Condensed Matter" in 1967, as they felt it did not exclude their interests in the study of liquids, nuclear matter and so on. Although Anderson and Heine helped popularize the name "condensed matter", it had been present in Europe for some years, most prominently in the form of a journal published in English, French, and German by Springer-Verlag titled "Physics of Condensed Matter," which was launched in 1963. The funding environment and Cold War politics of the 1960s and 1970s were also factors that lead some physicists to prefer the name "condensed matter physics", which emphasized the commonality of scientific problems encountered by physicists working on solids, liquids, plasmas, and other complex matter, over "solid state physics", which was often associated with the industrial applications of metals and semiconductors. The Bell Telephone Laboratories was one of the first institutes to conduct a research program in condensed matter physics.
References to "condensed" state can be traced to earlier sources. For example, in the introduction to his 1947 "Kinetic Theory of Liquids", Yakov Frenkel proposed in the book, "The kinetic theory of liquids must accordingly be developed as a generalization and extension of the kinetic theory of solid bodies".
As a matter of fact, it would be more correct to unify them under the title of "condensed bodies".
History.
Classical physics.
One of the first studies of condensed states of matter was by English chemist Humphry Davy, in the first decades of the nineteenth century. Davy observed that of the forty chemical elements known at the time, twenty-six had metallic properties such as lustre, ductility and high electrical and thermal conductivity. This indicated that the atoms in Dalton's atomic theory were not indivisible as Dalton claimed, but had inner structure. Davy further claimed that elements that were then believed to be gases, such as nitrogen and hydrogen could be liquefied under the right conditions and would then behave as metals.
In 1823, Michael Faraday, then an assistant in Davy's lab, successfully liquefied chlorine and went on to liquefy all known gaseous elements, with the exception of nitrogen, hydrogen and oxygen. Shortly after, in 1869, Irish chemist Thomas Andrews studied the phase transition from a liquid to a gas and coined the term critical point to describe the condition where a gas and a liquid were indistinguishable as phases, and Dutch physicist Johannes van der Waals supplied the theoretical framework which allowed the prediction of critical behavior based on measurements at much higher temperatures. By 1908, James Dewar and H. Kamerlingh Onnes were successfully able to liquefy hydrogen and then newly discovered helium, respectively.
Paul Drude in 1900 proposed the first theoretical model for a classical electron moving through a metallic solid. Drude's model described properties of metals in terms of a gas of free electrons, and was the first microscopic model to explain empirical observations such as the Wiedemann–Franz law. However, despite the success of Drude's free electron model, it had one notable problem, in that it was unable to correctly explain the electronic contribution to the specific heat and magnetic properties of metals, as well as the temperature dependence of resistivity at low temperatures.
In 1911, three years after helium was first liquefied, Onnes working at University of Leiden discovered superconductivity in mercury, when he observed the electrical resistivity of mercury to vanish at temperatures below a certain value. The phenomenon completely surprised the best theoretical physicists of the time, and it remained unexplained for several decades. Albert Einstein, in 1922, said regarding contemporary theories of superconductivity that “with our far-reaching ignorance of the quantum mechanics of composite systems we are very far from being able to compose a theory out of these vague ideas”.
Advent of quantum mechanics.
Drude's classical model was augmented by Wolfgang Pauli, Arnold Sommerfeld, Felix Bloch and other physicists. Pauli realized that the free electrons in metal must obey the Fermi–Dirac statistics. Using this idea, he developed the theory of paramagnetism in 1926. Shortly after, Sommerfeld incorporated the Fermi–Dirac statistics into the free electron model and made it better able to explain the heat capacity. Two years later, Bloch used quantum mechanics to describe the motion of a quantum electron in a periodic lattice. The mathematics of crystal structures developed by Auguste Bravais, Yevgraf Fyodorov and others was used to classify crystals by their symmetry group, and tables of crystal structures were the basis for the series "International Tables of Crystallography", first published in 1935. Band structure calculations was first used in 1930 to predict the properties of new materials, and in 1947 John Bardeen, Walter Brattain and William Shockley developed the first semiconductor-based transistor, heralding a revolution in electronics.
In 1879, Edwin Herbert Hall working at the Johns Hopkins University discovered the development of a voltage across conductors transverse to an electric current in the conductor and magnetic field perpendicular to the current. This phenomenon arising due to the nature of charge carriers in the conductor came to be known as the Hall effect, but it was not properly explained at the time, since the electron was experimentally discovered 18 years later. After the advent of quantum mechanics, Lev Landau in 1930 developed the theory of landau quantization and laid the foundation for the theoretical explanation for the quantum hall effect discovered half a century later.
Magnetism as a property of matter has been known in China since 4000BC. However, the first modern studies of magnetism only started with the development of electrodynamics by Faraday, Maxwell and others in the nineteenth century, which included the classification of materials as ferromagnetic, paramagnetic and diamagnetic based on their response to magnetization. Pierre Curie studied the dependence of magnetization on temperature and discovered the Curie point phase transition in ferromagnetic materials. In 1906, Pierre Weiss introduced the concept of magnetic domains to explain the main properties of ferromagnets. The first attempt at a microscopic description of magnetism was by Wilhelm Lenz and Ernst Ising through the Ising model that described magnetic materials as consisting of a periodic lattice of spins that collectively acquired magnetization. The Ising model was solved exactly to show that spontaneous magnetization cannot occur in one dimension but is possible in higher-dimensional lattices. Further research such as by Bloch on spin waves and Néel on antiferromagnetism led to the development of new magnetic materials with applications to magnetic storage devices.
Modern many-body physics.
The Sommerfeld model and spin models for ferromagnetism illustrated the successful application of quantum mechanics to condensed matter problems in the 1930s. However, there still were several unsolved problems, most notably the description of superconductivity and the Kondo effect. After World War II, several ideas from quantum field theory were applied to condensed matter problems. These included recognition of collective modes of excitation of solids and the important notion of a quasiparticle. Russian physicist Lev Landau used the idea for the Fermi liquid theory wherein low energy properties of interacting fermion systems were given in terms of what are now known as Landau-quasiparticles. Landau also developed a mean field theory for continuous phase transitions, which described ordered phases as spontaneous breakdown of symmetry. The theory also introduced the notion of an order parameter to distinguish between ordered phases. Eventually in 1965, John Bardeen, Leon Cooper and John Schrieffer developed the so-called BCS theory of superconductivity, based on the discovery that arbitrarily small attraction between two electrons of opposite spin mediated by phonons in the lattice can give rise to a bound state called a Cooper pair.
The study of phase transition and the critical behavior of observables, known as critical phenomena, was a major field of interest in the 1960s. Leo Kadanoff, Benjamin Widom and Michael Fisher developed the ideas of critical exponents and scaling. These ideas were unified by Kenneth Wilson in 1972, under the formalism of the renormalization group in the context of quantum field theory.
The quantum Hall effect was discovered by Klaus von Klitzing in 1980 when he observed the Hall conductance to be integer multiples of a fundamental constant formula_1.(see figure) The effect was observed to be independent of parameters such as the system size and impurities. In 1981, theorist Robert Laughlin proposed a theory explaining the unanticipated precision of the integral plateau. It also implied that the Hall conductance can be characterized in terms of a topological invariable called Chern number. Shortly after, in 1982, Horst Störmer and Daniel Tsui observed the fractional quantum Hall effect where the conductance was now a rational multiple of a constant. Laughlin, in 1983, realized that this was a consequence of quasiparticle interaction in the Hall states and formulated a variational solution, known as the Laughlin wavefunction. The study of topological properties of the fractional Hall effect remains an active field of research.
In 1986, Karl Müller and Johannes Bednorz discovered the first high temperature superconductor, a material which was superconducting at temperatures as high as 50 Kelvin. It was realized that the high temperature superconductors are examples of strongly correlated materials where the electron–electron interactions play an important role. A satisfactory theoretical description of high-temperature superconductors is still not known and the field of strongly correlated materials continues to be an active research topic.
In 2009, David Field and researchers at Aarhus University discovered spontaneous electric fields when creating prosaic films of various gases. This has more recently expanded to form the research area of spontelectrics.
In 2012 several groups released preprints which suggest that samarium hexaboride has the properties of a topological insulator in accordance with the earlier theoretical predictions. Since samarium hexaboride is an established Kondo insulator, i.e. a strongly correlated electron material, the existence of a topological surface state in this material would lead to a topological insulator with strong electronic correlations.
Theoretical.
Theoretical condensed matter physics involves the use of theoretical models to understand properties of states of matter. These include models to study the electronic properties of solids, such as the Drude model, the Band structure and the density functional theory. Theoretical models have also been developed to study the physics of phase transitions, such as the Ginzburg–Landau theory, critical exponents and the use of mathematical techniques of quantum field theory and the renormalization group. Modern theoretical studies involve the use of numerical computation of electronic structure and mathematical tools to understand phenomena such as high-temperature superconductivity, topological phases and gauge symmetries.
Emergence.
Theoretical understanding of condensed matter physics is closely related to the notion of emergence, wherein complex assemblies of particles behave in ways dramatically different from their individual constituents. For example, a range of phenomena related to high temperature superconductivity are not well understood, although the microscopic physics of individual electrons and lattices is well known. Similarly, models of condensed matter systems have been studied where collective excitations behave like photons and electrons, thereby describing electromagnetism as an emergent phenomenon. Emergent properties can also occur at the interface between materials: one example is the lanthanum-aluminate-strontium-titanate interface, where two non-magnetic insulators are joined to create conductivity, superconductivity, and ferromagnetism.
Electronic theory of solids.
The metallic state has historically been an important building block for studying properties of solids. The first theoretical description of metals was given by Paul Drude in 1900 with the Drude model, which explained electrical and thermal properties by describing a metal as an ideal gas of then-newly discovered electrons. He was able to derive the empirical Wiedemann-Franz law and get results in close agreement with the experiments. This classical model was then improved by Arnold Sommerfeld who incorporated the Fermi–Dirac statistics of electrons and was able to explain the anomalous behavior of the specific heat of metals in the Wiedemann–Franz law. In 1912, The structure of crystalline solids was studied by Max von Laue and Paul Knipping, when they observed the X-ray diffraction pattern of crystals, and concluded that crystals get their structure from periodic lattices of atoms. In 1928, Swiss physicist Felix Bloch provided a wave function solution to the Schrödinger equation with a periodic potential, called the Bloch wave.
Calculating electronic properties of metals by solving the many-body wavefunction is often computationally hard, and hence, approximation techniques are necessary to obtain meaningful predictions. The Thomas–Fermi theory, developed in the 1920s, was used to estimate system energy and electronic density by treating the local electron density as a variational parameter. Later in the 1930s, Douglas Hartree, Vladimir Fock and John Slater developed the so-called Hartree–Fock wavefunction as an improvement over the Thomas–Fermi model. The Hartree–Fock method accounted for exchange statistics of single particle electron wavefunctions. In general, it's very difficult to solve the Hartree–Fock equation. Only the free electron gas case can be solved exactly. Finally in 1964–65, Walter Kohn, Pierre Hohenberg and Lu Jeu Sham proposed the density functional theory which gave realistic descriptions for bulk and surface properties of metals. The density functional theory (DFT) has been widely used since the 1970s for band structure calculations of variety of solids.
Symmetry breaking.
Certain states of matter exhibit symmetry breaking, where the relevant laws of physics possess some symmetry that is broken. A common example is crystalline solids, which break continuous translational symmetry. Other examples include magnetized ferromagnets, which break rotational symmetry, and more exotic states such as the ground state of a BCS superconductor, that breaks U(1) phase rotational symmetry.
Goldstone's theorem in quantum field theory states that in a system with broken continuous symmetry, there may exist excitations with arbitrarily low energy, called the Goldstone bosons. For example, in crystalline solids, these correspond to phonons, which are quantized versions of lattice vibrations.
Phase transition.
Phase transition refers to the change of phase of a system, which is brought about by change in an external parameter such as temperature. Classical phase transition occurs at finite temperature when the order of the system was destroyed. For example, when ice melts and becomes water, the ordered crystal structure is destroyed. In quantum phase transitions, the temperature is set to absolute zero, and the non-thermal control parameter, such as pressure or magnetic field, causes the phase transitions when order is destroyed by quantum fluctuations originating from the Heisenberg uncertainty principle. Here, the different quantum phases of the system refer to distinct ground states of the Hamiltonian. Understanding the behavior of quantum phase transition is important in the difficult tasks of explaining the properties of rare-earth magnetic insulators, high-temperature superconductors and other substances.
There are two classes of phase tranisitions: first-order transitions and continuous transitions. For the continuous transitions, the two phases involved do not co-exist at the transition temperature, also called critical point. Near the critical point, systems undergoes displays critical behavior, wherein several of their properties such as correlation length, specific heat and susceptibility diverge exponentially. These critical phenomena poses serious challenges to physicists because normal macroscopic laws are no longer valid in the region and novel ideas and methods has to be invented to find the new laws that can describe the system.
The simplest theory that can describe continuous phase transitions is the Ginzburg–Landau theory, which works in the so-called mean field approximation. However, it can only roughly explain continuous phase transition for ferroelectrics and type I superconductors which invoves long range microscopic interactions. For other types of systems that involves short range interactions near the critical point, a better theory is needed.
Near the critical point, the fluctuations happen over broad range of size scales while the feature of the whole system is scale invariant. Renormalization group techniques successively average out the shortest wavelength fluctuations in stages while retaining their effects into the next stage. Thus, the changes of a physical system as viewed at different size scales can be investigated systematically. The techniques, together with powerful computer simulation, contribute greatly to the explanation of the critical phenomena associated with continuous phase transition.
Experimental.
Experimental condensed matter physics involves the use of experimental probes to try to discover new properties of materials. Experimental probes include effects of electric and magnetic fields, measurement of response functions, transport properties and thermometry. Commonly used experimental techniques include spectroscopy, with probes such as X-rays, infrared light and inelastic neutron scattering; study of thermal response, such as specific heat and measurement of transport via thermal and heat conduction.
Scattering.
Several condensed matter experiments involve scattering of an experimental probe, such as X-ray, optical photons, neutrons, etc., on constituents of a material. The choice of scattering probe depends on the observation energy scale of interest. Visible light has energy on the scale of 1 eV and is used as a scattering probe to measure variations in material properties such as dielectric constant and refractive index. X-rays have energies of the order of 10 keV and hence are able to probe atomic length scales, and are used to measure variations in electron charge density.
Neutrons can also probe atomic length scales and are used to study scattering off nuclei and electron spins and magnetization (as neutrons themselves have spin but no charge). Coulomb and Mott scattering measurements can be made by using electron beams as scattering probes. Similarly, positron annihilation can be used as an indirect measurement of local electron density. Laser spectroscopy is an excellent tool for studying the microscopic properties of a medium, for example, to study forbidden transitions in media with nonlinear optical spectroscopy. 
External magnetic fields.
In experimental condensed matter physics, external magnetic fields act as thermodynamic variables that control the state, phase transitions and properties of material systems. Nuclear magnetic resonance (NMR) is a technique by which external magnetic fields can be used to find resonance modes of individual electrons, thus giving information about the atomic, molecular and bond structure of their neighborhood. NMR experiments can be made in magnetic fields with strengths up to 60 Tesla. Higher magnetic fields can improve the quality of NMR measurement data. Quantum oscillations is another experimental technique where high magnetic fields are used to study material properties such as the geometry of the Fermi surface. High magnetic fields will be useful in experimentally testing of the various theoretical predictions such as the quantized magnetoelectric effect, image magnetic monopole, and the half-integer quantum Hall effect.
Cold atomic gases.
Cold atom trapping in optical lattices is an experimental tool commonly used in condensed matter as well as atomic, molecular, and optical physics. The technique involves using optical lasers to create an interference pattern, which acts as a "lattice", in which ions or atoms can be placed at very low temperatures. Cold atoms in optical lattices are used as "quantum simulators", that is, they act as controllable systems that can model behavior of more complicated systems, such as frustrated magnets. In particular, they are used to engineer one-, two- and three-dimensional lattices for a Hubbard model with pre-specified parameters, and to study phase transitions for antiferromagnetic and spin liquid ordering.
In 1995, a gas of rubidium atoms cooled down to a temperature of 170 nK was used to experimentally realize the Bose–Einstein condensate, a novel state of matter originally predicted by S. N. Bose and Albert Einstein, wherein a large number of atoms occupy a single quantum state.
Applications.
Research in condensed matter physics has given rise to several device applications, such as the development of the semiconductor transistor, and laser technology. Several phenomena studied in the context of nanotechnology come under the purview of condensed matter physics. Techniques such as scanning-tunneling microscopy can be used to control processes at the nanometer scale, and have given rise to the study of nanofabrication.
In quantum computation, information is represented by quantum bits, or qubits. The qubits may decohere quickly before useful computation is completed. This serious problem must be solved before quantum computation may be realized. The superconducting Josephson junction qubits, the spintronic qubits using the spin orientation of magnetic materials, or the topological non-Abelian anyons from fractional quantum Hall states are a few of the promising approaches proposed in condensed matter physics to solve this problem.
Condensed matter physics also has important applications to biophysics, for example, the experimental technique of magnetic resonance imaging, which is widely used in medical diagnosis.

</doc>
<doc id="5388" url="https://en.wikipedia.org/wiki?curid=5388" title="Cultural anthropology">
Cultural anthropology

<onlyinclude>
Cultural anthropology is a branch of anthropology focused on the study of cultural variation among humans and is in contrast to social anthropology which perceives cultural variation as a subset of the anthropological constant. 
A variety of methods are part of anthropological methodology, including participant observation (often called fieldwork because it involves the anthropologist spending an extended period of time at the research location), interviews, and surveys.
One of the earliest articulations of the anthropological meaning of the term "culture" came from Sir Edward Tylor who writes on the first page of his 1897 book: "Culture, or civilization, taken in its broad, ethnographic sense, is that complex whole which includes knowledge, belief, art, morals, law, custom, and any other capabilities and habits acquired by man as a member of society." The term "civilization" later gave way to definitions by V. Gordon Childe, with culture forming an umbrella term and civilization becoming a particular kind of culture.
The anthropological concept of "culture" reflects in part a reaction against earlier Western discourses based on an opposition between "culture" and "nature", according to which some human beings lived in a "state of nature". Anthropologists have argued that culture "is" "human nature", and that all people have a capacity to classify experiences, encode classifications symbolically (i.e. in language), and teach such abstractions to others.
Since humans acquire culture through the learning processes of enculturation and socialization, people living in different places or different circumstances develop different cultures. Anthropologists have also pointed out that through culture people can adapt to their environment in non-genetic ways, so people living in different environments will often have different cultures. Much of anthropological theory has originated in an appreciation of and interest in the tension between the local (particular cultures) and the global (a universal human nature, or the web of connections between people in distinct places/circumstances).
The rise of cultural anthropology occurred within the context of the late 19th century, when questions regarding which cultures were "primitive" and which were "civilized" occupied the minds of not only Marx and Freud, but many others. Colonialism and its processes increasingly brought European thinkers in contact, directly or indirectly with "primitive others." The relative status of various humans, some of whom had modern advanced technologies that included engines and telegraphs, while others lacked anything but face-to-face communication techniques and still lived a Paleolithic lifestyle, was of interest to the first generation of cultural anthropologists.
Parallel with the rise of cultural anthropology in the United States, social anthropology, in which "sociality" is the central concept and which focuses on the study of social statuses and roles, groups, institutions, and the relations among them—developed as an academic discipline in Britain and in France. An umbrella term socio-cultural anthropology makes reference to both cultural and social anthropology traditions.
</onlyinclude>
Theoretical foundations.
The critique of evolutionism.
Anthropology is concerned with the lives of people within different parts of the world, particularly in relation to the discourse of beliefs and practices. In addressing this question, ethnologists in the 19th century divided into two schools of thought. Some, like Grafton Elliot Smith, argued that different groups must somehow have learned from one another, however indirectly; in other words, they argued that cultural traits spread from one place to another, or "diffused".
Other ethnologists argued that different groups had the capability of creating similar beliefs and practices independently. Some of those who advocated "independent invention", like Lewis Henry Morgan, additionally supposed that similarities meant that different groups had passed through the same stages of cultural evolution (See also classical social evolutionism). Morgan, in particular, acknowledged that certain forms of society and culture could not possibly have arisen before others. For example, industrial farming could not have been invented before simple farming, and metallurgy could not have developed without previous non-smelting processes involving metals (such as simple ground collection or mining). Morgan, like other 19th century social evolutionists, believed there was a more or less orderly progression from the primitive to the civilized.
20th-century anthropologists largely reject the notion that all human societies must pass through the same stages in the same order, on the grounds that such a notion does not fit the empirical facts. Some 20th-century ethnologists, like Julian Steward, have instead argued that such similarities reflected similar adaptations to similar environments. Although 19th-century ethnologists saw "diffusion" and "independent invention" as mutually exclusive and competing theories, most ethnographers quickly reached a consensus that both processes occur, and that both can plausibly account for cross-cultural similarities. But these ethnographers also pointed out the superficiality of many such similarities. They noted that even traits that spread through diffusion often were given different meanings and function from one society to another. Analyses of large human concentrations in big cities, in multidisciplinary studies by Ronald Daus, show how new methods may be applied to the understanding of man living in a global world and how it was caused by the action of extra-European nations, so high-lighting the role of Ethics in modern anthropology.
Accordingly, most of these anthropologists showed less interest in comparing cultures, generalizing about human nature, or discovering universal laws of cultural development, than in understanding particular cultures in those cultures' own terms. Such ethnographers and their students promoted the idea of "cultural relativism", the view that one can only understand another person's beliefs and behaviors in the context of the culture in which he or she lived or lives.
Others, such as Claude Lévi-Strauss (who was influenced both by American cultural anthropology and by French Durkheimian sociology), have argued that apparently similar patterns of development reflect fundamental similarities in the structure of human thought (see structuralism). By the mid-20th century, the number of examples of people skipping stages, such as going from hunter-gatherers to post-industrial service occupations in one generation, were so numerous that 19th-century evolutionism was effectively disproved.
Cultural relativism.
Cultural relativism is a principle that was established as axiomatic in anthropological research by Franz Boas and later popularized by his students. Boas first articulated the idea in 1887: "...civilization is not something absolute, but ... is relative, and ... our ideas and conceptions are true only so far as our civilization goes." Although, Boas did not coin the term, it became common among anthropologists after Boas' death in 1942, to express their synthesis of a number of ideas Boas had developed. Boas believed that the sweep of cultures, to be found in connection with any sub-species, is so vast and pervasive that there cannot be a relationship between culture and race. Cultural relativism involves specific epistemological and methodological claims. Whether or not these claims require a specific ethical stance is a matter of debate. This principle should not be confused with moral relativism.
Cultural relativism was in part a response to Western ethnocentrism. Ethnocentrism may take obvious forms, in which one consciously believes that one's people's arts are the most beautiful, values the most virtuous, and beliefs the most truthful. Franz Boas, originally trained in physics and geography, and heavily influenced by the thought of Kant, Herder, and von Humboldt, argued that one's culture may mediate and thus limit one's perceptions in less obvious ways. This understanding of culture confronts anthropologists with two problems: first, how to escape the unconscious bonds of one's own culture, which inevitably bias our perceptions of and reactions to the world, and second, how to make sense of an unfamiliar culture. The principle of cultural relativism thus forced anthropologists to develop innovative methods and heuristic strategies.
Foundational thinkers.
Boas and his students realized that if they were to conduct scientific research in other cultures, they would need to employ methods that would help them escape the limits of their own ethnocentrism. One such method is that of ethnography: basically, they advocated living with people of another culture for an extended period of time, so that they could learn the local language and be enculturated, at least partially, into that culture.
In this context, cultural relativism is of fundamental methodological importance, because it calls attention to the importance of the local context in understanding the meaning of particular human beliefs and activities. Thus, in 1948 Virginia Heyer wrote, "Cultural relativity, to phrase it in starkest abstraction, states the relativity of the part to the whole. The part gains its cultural significance by its place in the whole, and cannot retain its integrity in a different situation."
Lewis Henry Morgan.
Lewis Henry Morgan (1818–1881), a lawyer from Rochester, New York, became an advocate for and ethnological scholar of the Iroquois. His comparative analyses of religion, government, material culture, and especially kinship patterns proved to be influential contributions to the field of anthropology. Like other scholars of his day (such as Edward Tylor), Morgan argued that human societies could be classified into categories of cultural evolution on a scale of progression that ranged from "savagery", to "barbarism", to "civilization". Generally, Morgan used technology (such as bowmaking or pottery) as an indicator of position on this scale.
Franz Boas, founder of the modern discipline.
Franz Boas established academic anthropology in the United States in opposition to this sort of evolutionary perspective. His approach was empirical, skeptical of overgeneralizations, and eschewed attempts to establish universal laws. For example, Boas studied immigrant children to demonstrate that biological race was not immutable, and that human conduct and behavior resulted from nurture, rather than nature.
Influenced by the German tradition, Boas argued that the world was full of distinct "cultures," rather than societies whose evolution could be measured by how much or how little "civilization" they had. He believed that each culture has to be studied in its particularity, and argued that cross-cultural generalizations, like those made in the natural sciences, were not possible.
In doing so, he fought discrimination against immigrants, blacks, and indigenous peoples of the Americas. Many American anthropologists adopted his agenda for social reform, and theories of race continue to be popular subjects for anthropologists today. The so-called "Four Field Approach" has its origins in Boasian Anthropology, dividing the discipline in the four crucial and interrelated fields of sociocultural, biological, linguistic, and archaic anthropology (e.g. archaeology). Anthropology in the United States continues to be deeply influenced by the Boasian tradition, especially its emphasis on culture.
Kroeber, Mead and Benedict.
Boas used his positions at Columbia University and the American Museum of Natural History to train and develop multiple generations of students. His first generation of students included Alfred Kroeber, Robert Lowie, Edward Sapir and Ruth Benedict, who each produced richly detailed studies of indigenous North American cultures. They provided a wealth of details used to attack the theory of a single evolutionary process. Kroeber and Sapir's focus on Native American languages helped establish linguistics as a truly general science and free it from its historical focus on Indo-European languages.
The publication of Alfred Kroeber's textbook, "Anthropology," marked a turning point in American anthropology. After three decades of amassing material, Boasians felt a growing urge to generalize. This was most obvious in the 'Culture and Personality' studies carried out by younger Boasians such as Margaret Mead and Ruth Benedict. Influenced by psychoanalytic psychologists including Sigmund Freud and Carl Jung, these authors sought to understand the way that individual personalities were shaped by the wider cultural and social forces in which they grew up.
Though such works as "Coming of Age in Samoa" and "The Chrysanthemum and the Sword" remain popular with the American public, Mead and Benedict never had the impact on the discipline of anthropology that some expected. Boas had planned for Ruth Benedict to succeed him as chair of Columbia's anthropology department, but she was sidelined by Ralph Linton, and Mead was limited to her offices at the AMNH.
Wolf, Sahlins, Mintz and political economy.
In the 1950s and mid-1960s anthropology tended increasingly to model itself after the natural sciences. Some anthropologists, such as Lloyd Fallers and Clifford Geertz, focused on processes of modernization by which newly independent states could develop. Others, such as Julian Steward and Leslie White, focused on how societies evolve and fit their ecological niche—an approach popularized by Marvin Harris.
Economic anthropology as influenced by Karl Polanyi and practiced by Marshall Sahlins and George Dalton challenged standard neoclassical economics to take account of cultural and social factors, and employed Marxian analysis into anthropological study. In England, British Social Anthropology's paradigm began to fragment as Max Gluckman and Peter Worsley experimented with Marxism and authors such as Rodney Needham and Edmund Leach incorporated Lévi-Strauss's structuralism into their work. Structuralism also influenced a number of developments in 1960s and 1970s, including cognitive anthropology and componential analysis.
In keeping with the times, much of anthropology became politicized through the Algerian War of Independence and opposition to the Vietnam War; Marxism became an increasingly popular theoretical approach in the discipline. By the 1970s the authors of volumes such as "Reinventing Anthropology" worried about anthropology's relevance.
Since the 1980s issues of power, such as those examined in Eric Wolf's "Europe and the People Without History", have been central to the discipline. In the 1980s books like "Anthropology and the Colonial Encounter" pondered anthropology's ties to colonial inequality, while the immense popularity of theorists such as Antonio Gramsci and Michel Foucault moved issues of power and hegemony into the spotlight. Gender and sexuality became popular topics, as did the relationship between history and anthropology, influenced by Marshall Sahlins (again), who drew on Lévi-Strauss and Fernand Braudel to examine the relationship between symbolic meaning, sociocultural structure, and individual agency in the processes of historical transformation. Jean and John Comaroff produced a whole generation of anthropologists at the University of Chicago that focused on these themes. Also influential in these issues were Nietzsche, Heidegger, the critical theory of the Frankfurt School, Derrida and Lacan.
Geertz, Schneider and interpretive anthropology.
Many anthropologists reacted against the renewed emphasis on materialism and scientific modelling derived from Marx by emphasizing the importance of the concept of culture. Authors such as David Schneider, Clifford Geertz, and Marshall Sahlins developed a more fleshed-out concept of culture as a web of meaning or signification, which proved very popular within and beyond the discipline. Geertz was to state:
Geertz's interpretive method involved what he called "thick description." The cultural symbols of rituals, political and economic action, and of kinship, are "read" by the anthropologist as if they are a document in a foreign language. The interpretation of those symbols must be re-framed for their anthropological audience, i.e. transformed from the "experience-near" but foreign concepts of the other culture, into the "experience-distant" theoretical concepts of the anthropologist. These interpretations must then be reflected back to its originators, and its adequacy as a translation fine-tuned in a repeated way, a process called the hermeneutic circle. Geertz applied his method in a number of areas, creating programs of study that were very productive. His analysis of "religion as a cultural system" was particularly influential outside of anthropology. David Schnieder's cultural analysis of American kinship has proven equally influential. Schneider demonstrated that the American folk-cultural emphasis on "blood connections" had an undue influence on anthropological kinship theories, and that kinship is not a biological characteristic but a cultural relationship established on very different terms in different societies.
Prominent British symbolic anthropologists include Victor Turner and Mary Douglas.
The post-modern turn.
In the late 1980s and 1990s authors such as George Marcus and James Clifford pondered ethnographic authority, in particular how and why anthropological knowledge was possible and authoritative. They were reflecting trends in research and discourse initiated by Feminists in the academy, although they excused themselves from commenting specifically on those pioneering critics. Nevertheless, key aspects of feminist theorizing and methods became "de rigueur" as part of the 'post-modern moment' in anthropology: Ethnographies became more interpretative and reflexive, explicitly addressing the author's methodology, cultural, gender and racial positioning, and their influence on his or her ethnographic analysis. This was part of a more general trend of postmodernism that was popular contemporaneously. Currently anthropologists pay attention to a wide variety of issues pertaining to the contemporary world, including globalization, medicine and biotechnology, indigenous rights, virtual communities, and the anthropology of industrialized societies.
Methods.
Modern cultural anthropology has its origins in, and developed in reaction to, 19th century "ethnology", which involves the organized comparison of human societies. Scholars like E.B. Tylor and J.G. Frazer in England worked mostly with materials collected by others – usually missionaries, traders, explorers, or colonial officials – earning them the moniker of "arm-chair anthropologists".
Participant observation.
Participant observation is a widely used methodology in many disciplines, particularly cultural anthropology, less so in sociology, communication studies, and social psychology. Its aim is to gain a close and intimate familiarity with a given group of individuals (such as a religious, occupational, sub cultural group, or a particular community) and their practices through an intensive involvement with people in their cultural environment, usually over an extended period of time. The method originated in the field research of social anthropologists, especially Bronislaw Malinowski in Britain, the students of Franz Boas in the United States, and in the later urban research of the Chicago School of sociology.
Such research involves a range of well-defined, though variable methods: informal interviews, direct observation, participation in the life of the group, collective discussions, analyses of personal documents produced within the group, self-analysis, results from activities undertaken off or online, and life-histories. Although the method is generally characterized as qualitative research, it can (and often does) include quantitative dimensions. Traditional participant observation is usually undertaken over an extended period of time, ranging from several months to many years, and even generations. An extended research time period means that the researcher is able to obtain more detailed and accurate information about the individuals, community, and/or population under study. Observable details (like daily time allotment) and more hidden details (like taboo behavior) are more easily observed and interpreted over a longer period of time. A strength of observation and interaction over extended periods of time is that researchers can discover discrepancies between what participants say—and often believe—should happen (the formal system) and what actually does happen, or between different aspects of the formal system; in contrast, a one-time survey of people's answers to a set of questions might be quite consistent, but is less likely to show conflicts between different aspects of the social system or between conscious representations and behavior.
Ethnography.
In the 20th century, most cultural and social anthropologists turned to the crafting of ethnographies. An ethnography is a piece of writing about a people, at a particular place and time. Typically, the anthropologist lives among people in another society for a period of time, simultaneously participating in and observing the social and cultural life of the group.
Numerous other ethnographic techniques have resulted in ethnographic writing or details being preserved, as cultural anthropologists also curate materials, spend long hours in libraries, churches and schools poring over records, investigate graveyards, and decipher ancient scripts. A typical ethnography will also include information about physical geography, climate and habitat. It is meant to be a holistic piece of writing about the people in question, and today often includes the longest possible timeline of past events that the ethnographer can obtain through primary and secondary research.
Bronisław Malinowski developed the ethnographic method, and Franz Boas taught it in the United States. Boas' students such as Alfred L. Kroeber, Ruth Benedict and Margaret Mead drew on his conception of culture and cultural relativism to develop cultural anthropology in the United States. Simultaneously, Malinowski and A.R. Radcliffe Brown´s students were developing social anthropology in the United Kingdom. Whereas cultural anthropology focused on symbols and values, social anthropology focused on social groups and institutions. Today socio-cultural anthropologists attend to all these elements.
In the early 20th century, socio-cultural anthropology developed in different forms in Europe and in the United States. European "social anthropologists" focused on observed social behaviors and on "social structure", that is, on relationships among social roles (for example, husband and wife, or parent and child) and social institutions (for example, religion, economy, and politics).
American "cultural anthropologists" focused on the ways people expressed their view of themselves and their world, especially in symbolic forms, such as art and myths. These two approaches frequently converged and generally complemented one another. For example, kinship and leadership function both as symbolic systems and as social institutions. Today almost all socio-cultural anthropologists refer to the work of both sets of predecessors, and have an equal interest in what people do and in what people say.
Cross-cultural comparison.
One means by which anthropologists combat ethnocentrism is to engage in the process of cross-cultural comparison. It is important to test so-called "human universals" against the ethnographic record. Monogamy, for example, is frequently touted as a universal human trait, yet comparative study shows that it is not. 
The Human Relations Area Files, Inc. (HRAF) is a research agency based at Yale University. Since 1949, its mission has been to encourage and facilitate worldwide comparative studies of human culture, society, and behavior in the past and present. The name came from the Institute of Human Relations, an interdisciplinary program/building at Yale at the time. The Institute of Human Relations had sponsored HRAF’s precursor, the "Cross-Cultural Survey" (see George Peter Murdock), as part of an effort to develop an integrated science of human behavior and culture. The two eHRAF databases on the Web are expanded and updated annually. "eHRAF World Cultures" includes materials on cultures, past and present, and covers nearly 400 cultures. The second database, "eHRAF Archaeology", covers major archaeological traditions and many more sub-traditions and sites around the world.
Comparison across cultures includies the industrialized (or de-industrialized) West. Cultures in the more traditional standard cross-cultural sample of small scale societies are:
Multi-sited ethnography.
Ethnography dominates socio-cultural anthropology. Nevertheless, many contemporary socio-cultural anthropologists have rejected earlier models of ethnography as treating local cultures as bounded and isolated. These anthropologists continue to concern themselves with the distinct ways people in different locales experience and understand their lives, but they often argue that one cannot understand these particular ways of life solely from a local perspective; they instead combine a focus on the local with an effort to grasp larger political, economic, and cultural frameworks that impact local lived realities. Notable proponents of this approach include Arjun Appadurai, James Clifford, George Marcus, Sidney Mintz, Michael Taussig, Eric Wolf and Ronald Daus.
A growing trend in anthropological research and analysis is the use of multi-sited ethnography, discussed in George Marcus' article, "Ethnography In/Of the World System: the Emergence of Multi-Sited Ethnography". Looking at culture as embedded in macro-constructions of a global social order, multi-sited ethnography uses traditional methodology in various locations both spatially and temporally. Through this methodology, greater insight can be gained when examining the impact of world-systems on local and global communities.
Also emerging in multi-sited ethnography are greater interdisciplinary approaches to fieldwork, bringing in methods from cultural studies, media studies, science and technology studies, and others. In multi-sited ethnography, research tracks a subject across spatial and temporal boundaries. For example, a multi-sited ethnography may follow a "thing," such as a particular commodity, as it is transported through the networks of global capitalism.
Multi-sited ethnography may also follow ethnic groups in diaspora, stories or rumours that appear in multiple locations and in multiple time periods, metaphors that appear in multiple ethnographic locations, or the biographies of individual people or groups as they move through space and time. It may also follow conflicts that transcend boundaries. An example of multi-sited ethnography is Nancy Scheper-Hughes' work on the international black market for the trade of human organs. In this research, she follows organs as they are transferred through various legal and illegal networks of capitalism, as well as the rumours and urban legends that circulate in impoverished communities about child kidnapping and organ theft.
Sociocultural anthropologists have increasingly turned their investigative eye on to "Western" culture. For example, Philippe Bourgois won the Margaret Mead Award in 1997 for "In Search of Respect", a study of the entrepreneurs in a Harlem crack-den. Also growing more popular are ethnographies of professional communities, such as laboratory researchers, Wall Street investors, law firms, or information technology (IT) computer employees.

</doc>
<doc id="5390" url="https://en.wikipedia.org/wiki?curid=5390" title="Conversion of units">
Conversion of units

Conversion of units is the conversion between different units of measurement for the same quantity, typically through multiplicative conversion factors.
Techniques.
Process.
The process of conversion depends on the specific situation and the intended purpose. This may be governed by regulation, contract, technical specifications or other published standards. Engineering judgment may include such factors as:
Some conversions from one system of units to another need to be exact, without increasing or decreasing the precision of the first measurement. This is sometimes called "soft conversion". It does not involve changing the physical configuration of the item being measured.
By contrast, a "hard conversion" or an "adaptive conversion" may not be exactly equivalent. It changes the measurement to convenient and workable numbers and units in the new system. It sometimes involves a slightly different configuration, or size substitution, of the item. Nominal values are sometimes allowed and used.
Multiplication factors.
Conversion between units in the metric system can be discerned by their prefixes (for example, 1 kilogram = 1000 grams, 1 milligram = 0.001 grams) and are thus not listed in this article. Exceptions are made if the unit is commonly known by another name (for example, 1 micron = 10−6 metre).
Table ordering.
Within each table, the units are listed alphabetically, and the SI units (base or derived) are highlighted.
Tables of conversion factors.
This article gives lists of conversion factors for each of a number of physical quantities, which are listed in the index. For each physical quantity, a number of different units (some only of historical interest) are shown and expressed in terms of the corresponding SI unit.
Mass.
Notes:
Speed or velocity.
A velocity consists of a speed combined with a direction; the speed part of the velocity takes units of speed.
Force.
"See also:" Conversion between weight (force) and mass
Information entropy.
Often, information entropy is measured in shannons, whereas the (discrete) storage space of digital devices is measured in bits. Thus, uncompressed redundant data occupy more than one bit of storage per shannon of information entropy. The multiples of a bit listed above are usually used with this meaning. Other times the bit is used as a measure of information entropy and is thus a synonym of shannon.
Luminous intensity.
The candela is the preferred nomenclature for the SI unit.
Radiation - source activity.
Please note that although becquerel (Bq) and hertz (Hz) both ultimately refer to the same SI base unit (s−1), Hz is used only for periodic phenomena, and Bq is only used for stochastic processes associated with radioactivity.
Radiation - exposure.
The roentgen is not an SI unit and the NIST strongly discourages its continued use.
Radiation - equivalent dose.
Although the definitions for sievert (Sv) and gray (Gy) would seem to indicate that they measure the same quantities, this is not the case. The effect of receiving a certain dose of radiation (given as Gy) is variable and depends on many factors, thus a new unit was needed to denote the biological effectiveness of that dose on the body; this is known as the equivalent dose and is shown in Sv. The general relationship between absorbed dose and equivalent dose can be represented as
where "H" is the equivalent dose, "D" is the absorbed dose, and "Q" is a dimensionless quality factor. Thus, for any quantity of "D" measured in Gy, the numerical value for "H" measured in Sv may be different.
Software tools.
There are many conversion tools. They are found in the function libraries of applications such as spreadsheets databases, in calculators, and in macro packages and plugins for many other applications such as the mathematical, scientific and technical applications.
There are many standalone applications that offer the thousands of the various units with conversions. For example, the free software movement offers a command line utility GNU units for Linux and Windows.

</doc>
<doc id="5391" url="https://en.wikipedia.org/wiki?curid=5391" title="City">
City

A city is a large and permanent human settlement. Although there is no agreement on how a city is distinguished from a town in general English language meanings, many cities have a particular administrative, legal, or historical status based on local law.
Cities generally have complex systems for sanitation, utilities, land usage, housing, and transportation. The concentration of development greatly facilitates interaction between people and businesses, sometimes benefiting both parties in the process, but it also presents challenges to managing urban growth. 
A big city or metropolis usually has associated suburbs and exurbs. Such cities are usually associated with metropolitan areas and urban areas, creating numerous business commuters traveling to urban centers for employment. Once a city expands far enough to reach another city, this region can be deemed a conurbation or megalopolis. Damascus is arguably the oldest city in the world. In terms of population, the largest city proper is Shanghai, while the fastest-growing is Dubai.
Origins.
There is not enough evidence to assert what conditions gave rise to the first cities. Some theorists have speculated on what they consider suitable pre-conditions and basic mechanisms that might have been important driving forces.
The conventional view holds that cities first formed after the Neolithic revolution. The Neolithic revolution brought agriculture, which made denser human populations possible, thereby supporting city development. The advent of farming encouraged hunter-gatherers to abandon nomadic lifestyles and to settle near others who lived by agricultural production. The increased population-density encouraged by farming and the increased output of food per unit of land created conditions that seem more suitable for city-like activities. In his book, "Cities and Economic Development", Paul Bairoch takes up this position in his argument that agricultural activity appears necessary before true cities can form.
According to Vere Gordon Childe, for a settlement to qualify as a city, it must have enough surplus of raw materials to support trade and a relatively large population. Bairoch points out that, due to sparse population densities that would have persisted in pre-Neolithic, hunter-gatherer societies, the amount of land that would be required to produce enough food for subsistence and trade for a large population would make it impossible to control the flow of trade. To illustrate this point, Bairoch offers an example: "Western Europe during the pre-Neolithic, the density must have been less than 0.1 person per square kilometre". Using this population density as a base for calculation, and allotting 10% of food towards surplus for trade and assuming that city dwellers do no farming, he calculates that "...to maintain a city with a population of 1,000, and without taking the cost of transport into account, an area of 100,000 square kilometres would have been required. When the cost of transport is taken into account, the figure rises to 200,000 square kilometres ...". Bairoch noted that this is roughly the size of Great Britain.
The urban theorist Jane Jacobs suggests that city-formation preceded the birth of agriculture, but this view is not widely accepted.
In his book "City Economics", Brendan O'Flaherty asserts "Cities could persist—as they have for thousands of years—only if their advantages offset the disadvantages" . O'Flaherty illustrates two similar attracting advantages known as increasing returns to scale and economies of scale, which are concepts usually associated with firms. Their applications are seen in more basic economic systems as well. Increasing returns to scale occurs when "doubling all inputs more than doubles the output an activity has economies of scale if doubling output less than doubles cost" . To offer an example of these concepts, O'Flaherty makes use of "one of the oldest reasons why cities were built: military protection" . In this example, the inputs are anything that would be used for protection (e.g., a wall) and the output is the area protected and everything of value contained in it. O'Flaherty then asks that we suppose the protected area is square, and each hectare inside it has the same value of protection. The advantage is expressed as: 
The inputs depend on the length of the perimeter:
So there are increasing returns to scale:
Also, economies of scale:
"Cities, then, economize on protection, and so protection against marauding barbarian armies is one reason why people have come together to live in cities ..." .
Similarly, "Are Cities Dying?", a paper by Harvard economist Edward L. Glaeser, delves into similar reasons for city formation: reduced transport costs for goods, people and ideas. Discussing the benefits of proximity, Glaeser claims that if a city is doubled in size, workers get a ten percent increase in earnings. Glaeser furthers his argument by stating that bigger cities do not pay more for equal productivity than in a smaller city, so it is reasonable to assume that workers become more productive if they move to a city twice the size as they initially worked in. The workers do not benefit much from the ten percent wage increase, because it is recycled back into the higher cost of living in a larger city. They do gain other benefits from living in cities, though.
Geography.
City planning has seen many different schemes for how a city should look. The most commonly seen pattern is the grid, used for thousands of years in China, independently invented by Alexander the Great's city planner Dinocrates of Rhodes and favoured by the Romans, while almost a rule in parts of pre-Columbian America. Derry, begun in 1613, was the first planned city in Ireland, with the walls being completed five years later. The central diamond within a walled city with four gates was considered a good design for defence. The grid pattern was widely copied in the colonies of British North America.
The ancient Greeks often gave their colonies around the Mediterranean a grid plan. One of the best examples is the city of Priene. This city had different specialised districts, much as is seen in modern city planning today. Fifteen centuries earlier, the Indus Valley Civilisation was using grids in such cities as Mohenjo-Daro. In medieval times there was evidence of a preference for linear planning. Good examples are the cities established by various rulers in the south of France and city expansions in old Dutch and Flemish cities.
Grid plans were popular among planners in the 19th century, particularly after the redesign of Paris. They cut through the meandering, organic streets that followed old paths. The United States imposed grid plans in new territories and towns, as the American West was rapidly established, in places such as Salt Lake City and San Francisco.
Other forms may include a radial structure, in which main roads converge on a central point. This was often a historic form, the effect of successive growth over long time with concentric traces of town walls and citadels. In more recent history, such forms were supplemented by ring-roads that take traffic around the outskirts of a town. Many Dutch cities are structured this way: a central square surrounded by concentric canals. Every city expansion would imply a new circle (canals together with town walls). In cities such as Amsterdam, Haarlem and also Moscow, this pattern is still clearly visible.
History.
Towns and cities have a long history, although opinions vary on whether any particular ancient settlement can be considered a city. A city formed as central places of trade for the benefit of the members living in close proximity to others facilitates interaction of all kinds. These interactions generate both positive and negative externalities between others' actions. Benefits include reduced transport costs, exchange of ideas, sharing of natural resources, large local markets, and later in their development, amenities such as running water and sewage disposal. Possible costs would include higher rate of crime, higher mortality rates, higher cost of living, worse pollution, traffic and high commuting times. Cities grow when the benefits of proximity between people and firms are higher than the cost.
The first true towns are sometimes considered large settlements where the inhabitants were no longer simply farmers of the surrounding area, but began to take on specialized occupations, and where trade, food storage and power were centralized. In 1950 Gordon Childe attempted to define a historic city with 10 general metrics. These are:
This categorisation is descriptive, and it is used as a general touchstone when considering ancient cities, although not all have each of its characteristics.
One characteristic that can be used to distinguish a small city from a large town is organized government. A town accomplishes common goals through informal agreements between neighbors or the leadership of a chief. A city has professional administrators, regulations, and some form of taxation (food and other necessities or means to trade for them) to feed the government workers. The governments may be based on heredity, religion, military power, work projects (such as canal building), food distribution, land ownership, agriculture, commerce, manufacturing, finance, or a combination of those. Societies that live in cities are often called civilizations.
Ancient times.
The more complex human societies, called the first civilisations emerged around 3000 BC in the river valleys of Mesopotamia, India, China, and Egypt. An increase in food production led to the significant growth in human population and the rise of cities. The peoples of Southwest Asia and Egypt laid the foundations of Western civilization, they developed cities and struggled with the problems of organised states as they moved from individual communities to larger territorial units and eventually to empires.
The Indus Valley Civilization and ancient China are two other areas with major indigenous urban traditions. Among the early Old World cities, Mohenjo-daro of the Indus Valley Civilization in present-day Pakistan, existing from about 2600 BC, was one of the largest, with a population of 50,000 or more.
In ancient Greece, beginning in the early 1st millennium BC, there emerged independent city-states that evolved for the first time the notion of citizenship, becoming in the process the archetype of the free city, the polis. The Agora, meaning "gathering place" or "assembly", was the center of athletic, artistic, spiritual and political life of the polis. These Greek city-states reached great levels of prosperity that resulted in an unprecedented cultural boom, that of classical Greece, expressed in architecture, drama, science, mathematics and philosophy, and nurtured in Athens under a democratic government. The Greek Hippodamus of Miletus (c. 407 BC) has been dubbed the "Father of City Planning" for his design of Miletus; the Hippodamian, or grid plan, was the basis for subsequent Greek and Roman cities. In the 4th century BC, Alexander the Great commissioned Dinocrates of Rhodes to lay out his new city of Alexandria, the grandest example of idealized urban planning of the ancient Mediterranean world, where the city's regularity was facilitated by its level site near a mouth of the Nile.
This roster of early urban traditions is notable for its diversity. Excavations at early urban sites show that some cities were sparsely populated political capitals, others were trade centers, and still other cities had a primarily religious focus. Some cities had large dense populations, whereas others carried out urban activities in the realms of politics or religion without having large associated populations. Theories that attempt to explain ancient urbanism by a single factor, such as economic benefit, fail to capture the range of variation documented by archaeologists.
The growth of the population of ancient civilizations, the formation of ancient empires concentrating political power, and the growth in commerce and manufacturing led to ever greater capital cities and centres of commerce and industry, with Alexandria, Antioch and Seleucia of the Hellenistic civilization, Pataliputra (now Patna) in India, Chang'an (now Xi'an) in China, Carthage, ancient Rome, its eastern successor Constantinople (later Istanbul).
Keith Hopkins estimates that ancient Rome had a population of about a million people by the end of the 1st century BC, after growing continually during the 3rd, 2nd, and 1st centuries BC, making it the largest city in the world at the time. Alexandria's population was also close to Rome's population at around the same time, the historian Rostovtzeff estimates a total population close to a million based on a census dated from 32 AD that counted 180,000 adult male citizens in Alexandria.
Cities of Late Antiquity underwent transformations as the urban power base shrank and was transferred to the local bishop (see Late Roman Empire). Cities essentially disappeared, earliest in Roman Britain and Germania and latest in the Eastern Roman Empire and Visigothic Spain.
In the ancient Americas, early urban traditions developed in the Andes and Mesoamerica. In the Andes, the first urban centers developed in the Norte Chico civilization (also Caral or Caral-Supe civilization), Chavin and Moche cultures, followed by major cities in the Huari, Chimu and Inca cultures. The Norte Chico civilization included as many as 30 major population centers in what is now the Norte Chico region of north-central coastal Peru. It is the oldest known civilization in the Americas, flourishing between the 30th century BC and the 18th century BC. Mesoamerica saw the rise of early urbanism in several cultural regions, including the Preclassic Maya, the Zapotec of Oaxaca, and Teotihuacan in central Mexico. Later cultures such as the Aztec drew on these earlier urban traditions.
In the first millennium AD, an urban tradition developed in the Khmer region of Cambodia, where Angkor grew into one of the largest cities (in area) of the world. The closest rival to Angkor, the Mayan city of Tikal in Guatemala, was between in total size. Although its population remains a topic of research and debate, newly identified agricultural systems in the Angkor area may have supported up to one million people.
Agriculture was practiced in sub-Saharan Africa since the third millennium BC. Because of this, cities could develop as centers of non-agricultural activity. Exactly when this first happened is still a topic of archeological and historical investigation. Western scholarship has tended to focus on cities in Europe and Mesopotamia, but emerging archeological evidence indicates that urbanization occurred south of the Sahara well before the influence of Arab urban culture. One of the oldest sites documented thus far, Jenné-Jeno in what is today Mali, has in fact been dated back to the third century BC. According to Roderick and Susan McIntosh, Jenné-Jeno did not fit into traditional Western conceptions of urbanity as it lacked monumental architecture and a distinctive elite social class, but it should indeed be considered a city based on more a more functional redefinition of urban development. In particular, Jenné-Jeno featured settlement mounds arranged according to a horizontal, rather than vertical, power hierarchy, and served as a center of specialized production and exhibited functional interdependence with the surrounding hinterland. Archaeological evidence from Jenné-Jeno, specifically the presence of non-West African glass beads dated from the third century BC to the fourth century AD, indicates that pre-Arabic trade contacts probably existed between Jenné-Jeno and North Africa. Additionally, other early urban centers in sub-Saharan Africa, dated to around 500 AD, include Awdaghust, Kumbi-Saleh the ancient capital of Ghana, and Maranda a center located on a trade rout between Egypt and Gao.
Middle Ages.
While David Kessler and Peter Temin consider ancient Rome the largest city before the 19th century, London was the first to exceed a population of 1 million. George Modelski considers medieval Baghdad, with an estimated population of 1.2 million at its peak, the largest city before 19th century London and the first with a population of over one million. Others estimate that Baghdad's population may have been as large as 2 million in the 9th century.
From the 9th through the end of the 12th century, the city of Constantinople, capital of the Byzantine Empire, was the largest and wealthiest city in Europe, with a population approaching 1 million.
During the European Middle Ages, a town was as much a political entity as a collection of houses. City residence brought freedom from customary rural obligations to lord and community: ""Stadtluft macht frei"" ("City air makes you free") was a saying in Germany. In Continental Europe cities with a legislature of their own were not unheard of, the laws for towns as a rule other than for the countryside, the lord of a town often being another than for surrounding land. In the Holy Roman Empire, some cities had no other lord than the emperor. In Italy medieval communes had quite a statelike power. In exceptional cases like Venice, Genoa or Lübeck, cities themselves became powerful states, sometimes taking surrounding areas under their control or establishing extensive maritime empires. Similar phenomena existed elsewhere, as in the case of Sakai, which enjoyed a considerable autonomy in late medieval Japan.
Early modern.
While the city-states, or poleis, of the Mediterranean and Baltic Sea languished from the 16th century, Europe's larger capitals benefited from the growth of commerce following the emergence of an Atlantic trade. By the early 19th century, London had become the largest city in the world with a population of over a million, while Paris rivaled the well-developed regionally traditional capital cities of Baghdad, Beijing, Istanbul and Kyoto.
During the Spanish colonization of the Americas the old Roman city concept was extensively used. Cities were founded in the middle of the newly conquered territories, and were bound to several laws about administration, finances and urbanism.
Most towns remained far smaller, so that in 1500 only some two dozen places in the world contained more than 100,000 inhabitants. As late as 1700, there were fewer than forty, a figure that rose to 300 in 1900.
Industrial age.
The growth of modern industry from the late 18th century onward led to massive urbanization and the rise of new great cities, first in Europe and then in other regions, as new opportunities brought huge numbers of migrants from rural communities into urban areas. In the United States from 1860 to 1910, the introduction of railroads reduced transportation costs, and large manufacturing centers began to emerge, thus allowing migration from rural to city areas. Cities during this period were deadly places to live in, due to health problems resulting from contaminated water and air, and communicable diseases. In the Great Depression of the 1930s cities were hard hit by unemployment, especially those with a base in heavy industry. In the U.S. urbanization rate increased forty to eighty percent during 1900–1990. Today the world's population is slightly over half urban, and continues to urbanize, with roughly a million people moving into cities every 24 hours worldwide.
External effects.
Modern cities are known for creating their own microclimates. This is due to the large clustering of heat absorbent surfaces that heat up in sunlight and that channel rainwater into underground ducts.
Waste and sewage are two major problems for cities, as is air pollution from various forms of combustion, including fireplaces, wood or coal-burning stoves, other heating systems, and internal combustion engines. The impact of cities on places elsewhere, be it hinterlands or places far away, is considered in the notion of city footprinting ("ecological footprint").
Other negative external effects include health consequences such as communicable diseases, crime, and high traffic and commuting times. Cities cause more interaction with more people than rural areas, thus a higher probability to contracting contagious diseases. However, many inventions such as inoculations, vaccines, and water filtration systems have also lowered health concerns. Crime is also a concern in the cities. Studies have shown that crime rates in cities are higher and the chance of punishment after getting caught is lower. In cases such as burglary, the higher concentration of people in cities create more items of higher value worth the risk of crime. The high concentration of people also makes using auto mobiles inconvenient and pedestrian traffic is more prominent in metropolitan areas than a rural or suburban one.
Cities also generate positive external effects. The close physical proximity facilitates knowledge spillovers, helping people and firms exchange information and generate new ideas. A thicker labor market allows for better skill matching between firms and individuals. Population density enables also sharing of common infrastructure and production facilities, however in very dense cities, increased crowding and waiting times may lead to some negative effects.
Cities may have a positive influence on the environment. UN-HABITAT stated in its reports that city living can be the best solution for dealing with the rising population numbers (and thus still be a good approach on dealing with overpopulation) This is because cities concentrate human activity into one place, making the environmental damage on other places smaller. However, this can be achieved only if urban planning is improved and if city services are properly maintained.
Distinction between cities and towns.
The difference between "towns" and "cities" is differently understood in different parts of the world.
Even within the English-speaking world there is no one standard definition of a city: the term may be used either for a town possessing city status; for an urban locality exceeding an arbitrary population size; for a town dominating other towns with particular regional economic or administrative significance. British city status was historically conferred on settlements with a diocesan cathedral; in more recent times towns apply to receive city status at times of national celebration. Larger settlements which are not designated as cities are towns, smaller settlements are villages and very small settlements are called hamlets. In the US "city" is used for much smaller settlements.
Historically, city status was a privilege granted by royal letters of patent. The status would allow markets and/or foreign trade, in contrast to towns. Sovereigns could establish cities by decree, e.g. Helsinki, regardless of what was in the location beforehand. Also, with the establishment of federal governments, the new capital could be established from scratch, e.g. Brasília, without going through organic growth from a village to a town.
Although "city" can refer to an agglomeration including suburban and satellite areas, the term is not usually applied to a conurbation (cluster) of distinct urban places, nor for a wider metropolitan area including more than one city, each acting as a focus for parts of the area. And the word "town" (also "downtown") may mean the center of the city.
Global cities.
A global city, also known as a world city, is a prominent centre of trade, banking, finance, innovation, and markets. The term "global city", as opposed to megacity, was coined by Saskia Sassen in a seminal 1991 work. Whereas "megacity" refers to any city of enormous size, a global city is one of enormous power or influence. Global cities, according to Sassen, have more in common with each other than with other cities in their host nations.
The notion of global cities is rooted in the concentration of power and capabilities within all cities. The city is seen as a container where skills and resources are concentrated: the better able a city is to concentrate its skills and resources, the more successful and powerful the city. This makes the city itself more powerful in the sense that it can influence what is happening around the world. Following this view of cities, it is possible to rank the world's cities hierarchically.
Critics of the notion point to the different realms of power and interchange. The term "global city" is heavily influenced by economic factors and, thus, may not account for places that are otherwise significant. One writer, for example argues that the term is 'reductive and skewed':
In 1995, Kanter argued that successful cities can be identified by three elements: good thinkers (concepts), good makers (competence) or good traders (connections). The interplay of these three elements, Kanter argued, means that good cities are not planned but managed.
Inner city.
In Paris, the inner city is the richest part of the metropolitan area, where housing is the most expensive, and where elites and high-income individuals dwell. In the developing world, economic modernization brings poor newcomers from the countryside to build haphazardly at the edge of current settlement (see favelas, shacks and shanty towns).
The United States, in particular, has a culture of anti-urbanism that dates back to colonial times. The American City Beautiful architecture movement of the late 19th century was a reaction to perceived urban decay and sought to provide stately civic buildings and boulevards to inspire civic pride in the motley residents of the urban core. Modern anti-urban attitudes are found in the United States in the form of a planning profession that continues to develop land on a low-density suburban basis, where access to amenities, work and shopping is provided almost exclusively by car rather than by foot or transit.
There is a growing movement in North America called "New Urbanism" that calls for a return to traditional city planning methods where mixed-use zoning allows people to walk from one type of land-use to another. The idea is that housing, shopping, office space, and leisure facilities are all provided within walking distance of each other, thus reducing the demand for road-space and also improving the efficiency and effectiveness of mass transit.
21st century.
There is a debate about whether technology and instantaneous communications are making cities obsolete, or reinforcing the importance of big cities as centres of the knowledge economy. Knowledge-based development of cities, globalization of innovation networks, and broadband services are driving forces of a new city planning paradigm towards smart cities that use technology and communication to create more efficient agglomerations in terms of competitiveness, innovation, environment, energy, utilities, governance, and delivery of services to the citizen. Some companies are building brand new masterplanned cities from scratch on greenfield sites.

</doc>
<doc id="5394" url="https://en.wikipedia.org/wiki?curid=5394" title="Chervil">
Chervil

Chervil (; Anthriscus cerefolium), sometimes called garden chervil to distinguish it from similar plants also called chervil, or French parsley, is a delicate annual herb related to parsley. It is commonly used to season mild-flavoured dishes and is a constituent of the French herb mixture fines herbes.
Biology.
A member of the Apiaceae, chervil is native to the Caucasus but was spread by the Romans through most of Europe, where it is now naturalised.
The plants grow to , with tripinnate leaves that may be curly. The small white flowers form small umbels, across. The fruit is about 1 cm long, oblong-ovoid with a slender, ridged beak.
Uses and impact.
Culinary arts.
Chervil is used, particularly in France, to season poultry, seafood, young spring vegetables (such as carrots), soups, and sauces. More delicate than parsley, it has a faint taste of liquorice or aniseed.
Chervil is one of the four traditional French "fines herbes", along with tarragon, chives, and parsley, which are essential to French cooking. Unlike the more pungent, robust herbs, thyme, rosemary, etc., which can take prolonged cooking, the "fines herbes" are added at the last minute, to salads, omelettes, and soups. 
Horticulture.
According to some, slugs are attracted to chervil and the plant is sometimes used to bait them.
Health.
Chervil has had various uses in folk medicine. It was claimed to be useful as a digestive aid, for lowering high blood pressure, and, infused with vinegar, for curing hiccups. Besides its digestive properties, it is used as a mild stimulant.
Chervil has also been implicated in "strimmer dermatitis", or phytophotodermatitis, due to spray from weed trimmers and other forms of contact. Other plants in the family Apiaceae can have similar effects.
Cultivation.
Transplanting chervil can be difficult, due to the long taproot. It prefers a cool and moist location; otherwise, it rapidly goes to seed (also known as bolting). It is usually grown as a cool-season crop, like lettuce, and should be planted in early spring and late fall or in a winter greenhouse. Regular harvesting of leaves also helps to prevent bolting. If plants bolt despite precautions, the plant can be periodically re-sown throughout the growing season, thus producing fresh plants as older plants bolt and go out of production.
Chervil grows to a height of , and a width of .

</doc>
<doc id="5395" url="https://en.wikipedia.org/wiki?curid=5395" title="Chives">
Chives

Chives is the common name of Allium schoenoprasum, an edible species of the "Allium" genus.
A perennial plant, it is widespread in nature across much of Europe, Asia and North America.
"A. schoenoprasum" is the only species of "Allium" native to both the New and the Old Worlds.
The name of the species derives from the Greek σχοίνος, "skhoínos" (sedge) and πράσον, "práson" (leek). Its English name, "chives", derives from the French word "cive", from "cepa", the Latin word for onion.
Chives are a commonly used herb and can be found in grocery stores or grown in home gardens. In culinary use, the scapes and the unopened, immature flower buds are diced and used as an ingredient for fish, potatoes, soups, and other dishes. Chives have insect-repelling properties that can be used in gardens to control pests.
Biology.
Chives are a bulb-forming herbaceous perennial plant, growing to tall. The bulbs are slender, conical, long and broad, and grow in dense clusters from the roots. The scapes (or stems) are hollow and tubular, up to long and across, with a soft texture, although, prior to the emergence of a flower, they may appear stiffer than usual. The leaves, which are shorter than the scapes, are also hollow and tubular, or terete, (round in cross-section) which distinguishes it at a glance from Garlic Chives. The flowers are pale purple, and star-shaped with six petals, wide, and produced in a dense inflorescence of 10-30 together; before opening, the inflorescence is surrounded by a papery bract. The seeds are produced in a small three-valved capsule, maturing in summer. The herb flowers from April to May in the southern parts of its habitat zones and in June in the northern parts.
Chives are the only species of "Allium" native to both the Old World and the New World. Sometimes, the plants found in North America are classified as "A. schoenoprasum" var. "sibiricum", although this is disputed. Differences among specimens are significant. One example was found in northern Maine growing solitary, instead of in clumps, also exhibiting dingy grey flowers.
Although chives are repulsive to insects in general, due to their sulfur compounds, their flowers attract bees, and they are at times kept to increase desired insect life.
Uses.
Culinary arts.
Chives are grown for their scapes, which are used for culinary purposes as a flavoring herb, and provide a somewhat milder flavor than those of other "Allium" species.
Chives have a wide variety of culinary uses, such as in traditional dishes in France, Sweden and elsewhere. In his 1806 book "Attempt at a Flora" ("Försök til en flora"), Retzius describes how chives are used with pancakes, soups, fish and sandwiches. They are also an ingredient of the "gräddfil" sauce with the traditional herring dish served at Swedish midsummer celebrations. The flowers may also be used to garnish dishes. In Poland and Germany, chives are served with quark cheese. Chives are one of the "fines herbes" of French cuisine, which also include tarragon, chervil or parsley. Chives can be found fresh at most markets year-round, making them readily available; they can also be dry-frozen without much impairment to the taste, giving home growers the opportunity to store large quantities harvested from their own gardens.
Uses in plant cultivation.
Retzius also describes how farmers would plant chives between the rocks making up the borders of their flowerbeds, to keep the plants free from pests (such as Japanese beetles). The growing plant repels unwanted insect life, and the juice of the leaves can be used for the same purpose, as well as fighting fungal infections, mildew and scab.
Its flowers are attractive to bees, which are important for gardens with an abundance of plants in need of pollination.
Medicine.
The medicinal properties of chives are similar to those of garlic, but weaker; the faint effects in comparison with garlic are probably the main reason for their limited use as a medicinal herb. Containing numerous organosulfur compounds such as allyl sulfides and alkyl sulfoxides, chives are reported to have a beneficial effect on the circulatory system. They also have mild stimulant, diuretic, and antiseptic properties. As chives are usually served in small amounts and never as the main dish, negative effects are rarely encountered, although digestive problems may occur following overconsumption.
Chives are also rich in vitamins A and C, contain trace amounts of sulfur, and are rich in calcium and iron.
Cultivation.
Chives are cultivated both for their culinary uses and their ornamental value; the violet flowers are often used in ornamental dry bouquets.
Chives thrive in well-drained soil, rich in organic matter, with a pH of 6-7 and full sun. They can be grown from seed and mature in summer, or early the following spring. Typically, chives need to be germinated at a temperature of 15 to 20 °C (60-70 °F) and kept moist. They can also be planted under a cloche or germinated indoors in cooler climates, then planted out later. After at least four weeks, the young shoots should be ready to be planted out. They are also easily propagated by division.
In cold regions, chives die back to the underground bulbs in winter, with the new leaves appearing in early spring. Chives starting to look old can be cut back to about 2–5 cm. When harvesting, the needed number of stalks should be cut to the base. During the growing season, the plant will continually regrow leaves, allowing for a continuous harvest.
History and cultural importance.
Chives have been cultivated in Europe since the Middle Ages (5th until the 15th centuries), although their usage dates back 5000 years. They were sometimes referred to as "rush leeks" (from the Greek "schoinos" meaning rush and "prason" meaning leek).
The Romans believed chives could relieve the pain from sunburn or a sore throat. They believed eating chives could increase blood pressure and act as a diuretic.
Romanian Gypsies have used chives in fortune telling. It was believed that bunches of dried chives hung around a house would ward off disease and evil.

</doc>
<doc id="5397" url="https://en.wikipedia.org/wiki?curid=5397" title="Chris Morris (satirist)">
Chris Morris (satirist)

Christopher J "Chris" Morris (born 15 June 1962) is an English comedian, writer, director, actor, voice actor, and producer.
In the early 1990s, Morris teamed up with his radio producer, Armando Iannucci, to create "On The Hour", a satire of news programmes. This was expanded into a television spin off, "The Day Today", which launched the career of Steve Coogan, and has since been hailed as one of the most important satirical shows of the 1990s. Morris further developed the satirical news format with "Brass Eye", which lampooned celebrities whilst focusing on themes such as crime and drugs. For many, the apotheosis of Morris's career was a "Brass Eye" special, which dealt with the moral panic surrounding paedophilia. It quickly became one of the most complained about programmes in British television history, leading the "Daily Mail" to describe him as "the most loathed man on TV".
Meanwhile, Morris's postmodern sketch and ambient music radio show "Blue Jam" helped him to gain a cult following. He went on to win a BAFTA for Best Short Film after expanding a "Blue Jam" sketch into "My Wrongs 8245–8249 & 117", which starred Paddy Considine. This was followed by "Nathan Barley", a sitcom written in collaboration with a then little-known Charlie Brooker that satirised hipsters, which had low ratings but found success upon its DVD release. Morris followed this by joining the cast of the Graham Linehan sitcom "The IT Crowd", his first project in which he did not have writing or producing input.
In 2010, Morris directed his first feature-length film, "Four Lions", which satirised Islamic terrorism through a group of inept British Pakistanis. Reception of the film was largely positive, earning Morris his second BAFTA, for "Outstanding Debut". Since 2012, he has directed four episodes of Iannucci's political comedy "Veep" and appeared onscreen in "The Double" and "Stewart Lee's Comedy Vehicle". He is known for his black humour, surrealism, and controversial subject matter, and has been hailed for his "uncompromising, moralistic drive" by the British Film Institute. His tendency to avoid the media spotlight has seen him become one of the more enigmatic figures in British comedy.
Biography.
Early life.
Morris was born in Colchester, Essex, to father Paul Michael Morris, a GP, and mother Rosemary Parrington and grew up in a Victorian farmhouse in the village Buckden, Huntingdonshire, which he describes as "very dull".
He has two younger brothers, including theatre director Tom Morris. From an early age he was a prankster, and also had a passion for radio. From the age of 10 he was educated at Stonyhurst College, an independent Jesuit boarding school in Lancashire. He went to study zoology at the University of Bristol, where he gained a 2:1.
Radio career.
On graduating, Morris pursued a career as a musician in various bands, for which he played the bass guitar. He then went to work for Radio West, a local radio station in Bristol. He then took up a news traineeship with BBC Radio Cambridgeshire, where he took advantage of access to editing and recording equipment to create elaborate spoofs and parodies. He also spent time in early 1987 hosting a 2–4pm afternoon show and finally ended up presenting Saturday morning show "I.T."
In July 1987, he moved on to BBC Radio Bristol to present his own show "No Known Cure", broadcast on Saturday and Sunday mornings. The show was surreal and satirical, with odd interviews conducted with unsuspecting members of the public. He was fired from Bristol in 1990 after "talking over the news bulletins and making silly noises". In 1988 he also joined, from its launch, Greater London Radio (GLR). He presented "The Chris Morris Show" on GLR until 1993, when one show got suspended after a sketch was broadcast involving a child "outing" celebrities.
In 1991, Morris joined Armando Iannucci's spoof news project "On the Hour". Broadcast on BBC Radio 4, it saw him work alongside Iannucci, Steve Coogan, Stewart Lee, Richard Herring and Rebecca Front. In 1992, Morris hosted Danny Baker's Radio 5 Morning Edition show for a week whilst Baker was on holiday. In 1994, Morris began a weekly evening show, the "Chris Morris Music Show", on BBC Radio 1 alongside Peter Baynham and 'man with a mobile phone' Paul Garner. In the shows, Morris perfected the spoof interview style that would become a central component of his "Brass Eye" programme. In the same year, Morris teamed up with Peter Cook, as Sir Arthur Streeb-Greebling, in a series of improvised conversations for BBC Radio 3, entitled "Why Bother?".
Move into television and film.
In 1994, a BBC 2 television series based on "On the Hour" was broadcast under the name "The Day Today". "The Day Today" made a star of Morris, and marked the television debut of Steve Coogan's Alan Partridge character. The programme ended on a high after just one series, with Morris winning the 1994 British Comedy Award for Best Newcomer for his lead role as the Paxmanesque news anchor.
In 1997, the black humour which had featured in "On the Hour" and "The Day Today" became more prominent in "Brass Eye", another spoof current affairs television documentary, shown on Channel 4. "Brass Eye" became known for tricking celebrities and politicians into throwing support behind public awareness campaigns for made-up issues that were often absurd or surreal (such as a drug called "cake" and an elephant with its trunk stuck up its anus).
From 1997 to 1999 Morris created "Blue Jam" for BBC Radio 1, a surreal taboo-breaking radio show set to an ambient soundtrack. In 2000 this was followed by "Jam", a television reworking. Morris released a 'remix' version of this, entitled "Jaaaaam".
In 2001, a special episode of "Brass Eye" on the moral panic that surrounds paedophilia led to a record-breaking number of complaints – it still remains the third highest on UK television after "Celebrity Big Brother 2007" and "" – as well as heated discussion in the press. Many complainants, some of whom later admitted to not having seen the programme (notably Beverley Hughes, a government minister), felt the satire was directed at the victims of paedophilia, which Morris denies. Channel 4 defended the show, insisting the target was the media and its hysterical treatment of paedophilia, and not victims of crime.
In 2002, Morris ventured into film, directing the short "My Wrongs#8245–8249 & 117", adapted from a "Blue Jam" monologue about a man led astray by a sinister talking dog. It was the first film project of Warp Films, a branch of Warp Records. In 2002 this won the BAFTA for best short film. In 2005 Morris worked on a sitcom entitled "Nathan Barley", based on the character created by Charlie Brooker for his website TVGoHome (Morris had contributed to TVGoHome on occasion, under the pseudonym 'Sid Peach'). Co-written by Brooker and Morris, the series was broadcast on Channel 4 in early 2005.
Post 2005.
Morris was a cast member in "The IT Crowd", a Channel 4 sitcom which focused on the information technology department of the fictional company Reynholm Industries. The series was written and directed by Graham Linehan (writer of "Father Ted" and "Black Books", with whom Morris collaborated on "The Day Today", "Brass Eye" and "Jam") and produced by Ash Atalla ("The Office"). Morris played Denholm Reynholm, the eccentric managing director of the company. This marked the first time Morris has acted in a substantial role in a project which he has not developed himself. Morris's character appeared to leave the series during episode two of the second series. His character made a brief return in the first episode of the third series.
In November 2007, Morris wrote an article for "The Observer" in response to Ronan Bennett's article published six days earlier in "The Guardian". Bennett's article, "Shame on us", accused the novelist Martin Amis of racism. Morris's response, "The absurd world of Martin Amis", was also highly critical of Amis; although he did not accede to Bennett's accusation of racism, Morris likened Amis to the Muslim cleric Abu Hamza (who was jailed for inciting racial hatred in 2006), suggesting that both men employ "mock erudition, vitriol and decontextualised quotes from the Qu'ran" to incite hatred.
Morris served as script editor for the 2009 series "Stewart Lee's Comedy Vehicle", working with former colleagues Stewart Lee, Kevin Eldon and Armando Iannucci. He maintained this role for the second (2011) and third series (2014).
Morris completed his debut feature film "Four Lions" in late 2009, a satire based on a group of Islamist terrorists in Sheffield.
It premiered at the Sundance Film Festival in January 2010 and was short-listed for the festival's World Cinema Narrative prize. The film (working title "Boilerhouse") was picked up by Film Four. Morris told "The Sunday Times" that the film sought to do for Islamic terrorism what "Dad's Army", the classic BBC comedy, did for the Nazis by showing them as "scary but also ridiculous".
In 2012, Morris directed the seventh and penultimate episode of the first season of "Veep", an Armando Iannucci-devised American version of "The Thick of It". In 2013, he returned to direct two episodes for the second season of "Veep", and a further episode for season three in 2014.
In 2013, Morris appeared briefly in Richard Ayoade's "The Double", a black comedy film based on the Fyodor Dostoyevsky novella of the same name. Morris had previously worked with Ayoade on "Nathan Barley" and "The IT Crowd".
In February 2014, Morris made a surprise appearance at the beginning of a Stewart Lee live show, introducing the comedian with fictional anecdotes about their work together. The following month, Morris appeared in the third series of "Stewart Lee's Comedy Vehicle" as a "hostile interrogator", a role previously occupied by Armando Iannucci.
In December 2014, it was announced that a short radio collaboration with Noel Fielding and Richard Ayoade would be broadcast on BBC Radio 6. According to an interview with Fielding in 2011, the work had already been in progress for five years by that point. However, it was decided, 'in consultation with ', that the project was not yet complete enough, and so the intended broadcast did not go ahead.
Music.
Morris often co-writes and performs incidental music for his television shows, notably with "Jam" and the 'extended remix' version, "Jaaaaam". In the early 1990s Morris contributed a Pixies parody track entitled "Motherbanger" to a flexi-disc given away with an edition of Select music magazine. Morris supplied sketches for British band Saint Etienne's 1993 single "You're in a Bad Way" (the sketch 'Spongbake' appears at the end of the 4th track on the CD single).
In 2000, he collaborated by mail with Amon Tobin to create the track "Bad Sex", which was released as a B-side on the Tobin single "Slowly".
British band Stereolab's song "Nothing to Do with Me" from their 2001 album "Sound-Dust" featured various lines from Chris Morris sketches as lyrics.
Recognition.
In 2003, Morris was listed in "The Observer" as one of the 50 funniest acts in British comedy. In 2005, Channel 4 aired a show called "The Comedian's Comedian" in which foremost writers and performers of comedy ranked their 50 favourite acts. Morris was at number eleven. Morris won the BAFTA for outstanding debut with his film "Four Lions". Adeel Akhtar and Nigel Lindsay collected the award in his absence. Lindsay stated that Morris had sent him a text message before they collected the award reading, 'Doused in petrol, Zippo at the ready'. In June 2012 Morris was placed at number 16 in the Top 100 People in UK Comedy.
In 2010, a biography, "Disgusting Bliss: The Brass Eye of Chris Morris", was published. Written by Lucian Randall, the book depicted Morris as "brilliant but uncompromising", and a "frantic-minded perfectionist".
In November 2014, a three-hour retrospective of Morris' radio career was broadcast on BBC Radio 4 Extra under the title 'Raw Meat Radio', presented by Mary Anne Hobbs and featuring interviews with Armando Iannucci, Peter Baynham, Paul Garner, and others.
Awards.
Morris won the Best TV Comedy Newcomer award from the British Comedy Awards in 1998 for his performance in "The Day Today". He has won two BAFTA awards: the BAFTA Award for Best Short Film in 2002 for "My Wrongs#8245–8249 & 117", and the BAFTA Award for Outstanding Debut by a British director, writer or producer in 2011 for "Four Lions".
Personal life.
Morris lives in Brixton, with his wife, actress turned literary agent Jo Unwin. The pair met in 1984 at the Edinburgh Festival, when he was playing bass guitar for the Cambridge Footlights Revue and she was in a comedy troupe called the Millies. They have two sons, Charles and Frederick, both of whom were born in Lambeth. Until the release of "Four Lions" he gave very few interviews and little had been published about Morris's personal life. Since 2009 he has made numerous media appearances to promote and support the film, both in the UK and US, at one point appearing as a guest on "Late Night with Jimmy Fallon".
Morris can be heard as himself in a 2008 podcast for CERN. He has a large birthmark on his face, which he typically covers with makeup when acting.

</doc>
