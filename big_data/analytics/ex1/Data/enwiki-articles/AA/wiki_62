<doc id="4477" url="https://en.wikipedia.org/wiki?curid=4477" title="The Beach Boys">
The Beach Boys

The Beach Boys are an American rock band formed in Hawthorne, California in 1961. The group's original lineup consisted of brothers Brian, Dennis, and Carl Wilson, their cousin Mike Love, and their friend Al Jardine. They emerged at the vanguard of the "California Sound", first performing original surf songs that gained international popularity for their distinct vocal harmonies and lyrics reflecting a southern California youth culture of surfing, cars, and romance. Rooted in jazz-based vocal groups, 1950s rock and roll, and doo-wop, Brian led the band in devising novel approaches to music production, arranging his compositions for studio orchestras, and experimenting with several genres ranging from pop ballads to psychedelic and baroque.
The group began as a garage band managed by the Wilsons' father Murry, with Brian's creative ambitions and sophisticated songwriting abilities dominating the group's musical direction. After 1964, their albums took a different stylistic path that featured more personal lyrics, multi-layered sounds, and recording experiments. In 1966, the "Pet Sounds" album and "Good Vibrations" single vaulted the group to the top level of rock innovators and established the band as symbols of the nascent counterculture era. Following "Smile"s dissolution, Brian gradually ceded control to the rest of the band, reducing his input because of mental health and substance abuse issues. Though the more democratic incarnation of the Beach Boys recorded a string of albums in various music styles that garnered international critical success, the group struggled to reclaim their commercial momentum in America. Since the 1980s, much-publicized legal wrangling over royalties, songwriting credits and use of the band's name transpired.
Dennis drowned in 1983 and Carl died of lung cancer in 1998. After Carl's death, many live configurations of the band fronted by Mike Love and Bruce Johnston continued to tour into the 2000s while other members pursued solo projects. For the band's 50th anniversary, the surviving co-founders briefly reunited for a new studio album and world tour.
The Beach Boys are regarded as the most iconic American band and one of the most critically acclaimed, commercially successful, and widely influential bands of all time, while AllMusic stated that their "unerring ability... made them America's first, best rock band." The group had over eighty songs chart worldwide, thirty-six of them US Top 40 hits (the most by an American rock band), four reaching number-one on the "Billboard" Hot 100 chart. The Beach Boys have sold in excess of 100 million records worldwide, making them one of the world's best-selling bands of all time and are listed at number 12 on "Rolling Stone" magazine's 2004 list of the "100 Greatest Artists of All Time".</ref> They have received one Grammy Award for "The Smile Sessions" (2011). The core quintet of the three Wilsons, Love and Jardine were inducted into the Rock and Roll Hall of Fame in 1988.
1958–66: Brian Wilson era.
Formation.
At the time of his sixteenth birthday on June 20, 1958, Brian Wilson shared a bedroom with his brothers, Dennis and Carl – aged thirteen and eleven, respectively – in their family home in Hawthorne. He had watched his father, Murry Wilson, play piano, and had listened intently to the harmonies of vocal groups such as the Four Freshmen. After dissecting songs such as "Ivory Tower" and "Good News", Brian would teach family members how to sing the background harmonies. For his birthday that year, Brian received a reel-to-reel tape recorder. He learned how to overdub, using his vocals and those of Carl and their mother. Brian played piano with Carl and David Marks, an eleven-year-old longtime neighbor, playing guitars they had each received as Christmas presents.
Soon Brian and Carl were avidly listening to Johnny Otis' KFOX radio show. Inspired by the simple structure and vocals of the rhythm and blues songs he heard, Brian changed his piano-playing style and started writing songs. His enthusiasm interfered with his music studies at school. Family gatherings brought the Wilsons in contact with cousin Mike Love. Brian taught Love's sister Maureen and a friend harmonies. Later, Brian, Mike Love and two friends performed at Hawthorne High School. Brian also knew Al Jardine, a high school classmate who had already played guitar in a folk group called the Islanders. Brian suggested to Jardine that they team up with his cousin and brother Carl. Love gave the fledgling band its name: "The Pendletones", a portmanteau of "Pendleton", a style of woolen shirt popular at the time and "tone", the musical term. Though surfing motifs were prominent in their early songs, Dennis was the only avid surfer in the group. He suggested that the group compose songs that celebrated the sport and the lifestyle that had developed around it within Southern California.
Jardine and a singer friend, Gary Winfrey, went to Brian to see if he could help out with a version of a folk song they wanted to record—"Sloop John B". In Brian's absence, the two spoke with their father, a music industry veteran of modest success. Murry arranged for the Pendletones to meet his publisher, Hite Morgan. The group performed a slower ballad, "Their Hearts Were Full of Spring", but failed to impress Morgan. After an awkward pause, Dennis mentioned they had an original song, "Surfin'". Brian finished the song, and together with Mike Love, wrote "Surfin' Safari". The group rented guitars, drums, amplifiers and microphones, and practiced for three days while the Wilsons' parents were on a short vacation.
In October 1961, the Pendletones recorded the two surfing song demos in twelve takes at Keen Recording Studio. Murry brought the demos to Herb Newman, owner of Candix Records and Era Records, and he signed the group on December 8, 1961. When the boys eagerly unpacked the first box of singles – released both under the Candix label, and also as a promo issue under X Records (Morgan's label) – they were shocked to see their band had been renamed as the Beach Boys. Murry Wilson called Morgan and learned that Candix wanted to name the group the Surfers to directly associate them with the increasingly popular teen sport. But Russ Regan, a young promoter with Era Records – who later became president of 20th Century Fox Records – noted that there already existed a group by that name, and he suggested calling them the Beach Boys.
Beach-themed period.
Released in December 1961, "Surfin'" soon aired on KFWB and KRLA, two of Los Angeles' most influential teen radio stations. It was a hit on the West Coast, going to number three in Southern California, and peaked at number 75 on the national pop charts. By the final weeks of 1961 "Surfin'" had sold more than 40,000 copies. By this time, the de facto manager of the Beach Boys, Murry Wilson landed the group's first paying gig (for which they earned $300) on New Year's Eve, 1961, at the Ritchie Valens Memorial Dance in Long Beach, headlined by Ike & Tina Turner. In their earliest public appearances, the band wore heavy wool jacket-like shirts that local surfers favored before switching to their trademark striped shirts and white pants. Murry effectively seized managerial control of the band, and Brian acknowledged that he "deserves credit for getting us off the ground ... he hounded us mercilessly ... also worked hard himself."
In the first half of February 1962, Jardine left the band and was replaced by Marks. The band recorded two more originals on April 19 at Western Studios, Los Angeles: "Lonely Sea" and "409". They also re-recorded "Surfin' Safari". During early 1962, Morgan requested that some of the members add vocals to a couple of instrumental tracks that he had recorded with other musicians. This led to the creation of the short-lived group Kenny & the Cadets, which Brian led under the pseudonym "Kenny". The other members were Carl, Jardine, and the Wilsons' mother Audree. 
On June 4, the Beach Boys released their second single "Surfin' Safari" backed with "409". The release prompted national coverage in the June 9 issue of "Billboard". The magazine praised Love's lead vocal and said the song had strong hit potential. On July 16, 1962—after being turned down by Dot and Liberty—the Beach Boys signed a seven-year contract with Capitol Records, based on the strength of the June demo session. This was at the urging of Capitol exec Nick Venet who signed the group, seeing them as the "teenage gold" he had been scouting for. By November, their first album was ready—"Surfin' Safari", which reached 32 on the US Billboard charts. Their song output continued along the same commercial line, focusing on California youth lifestyle.
In January 1963, three months after the release of their debut album, the band began recording their sophomore effort, "Surfin' U.S.A.", a breakthrough for Brian, who began asserting himself as songwriter and arranger. The LP was the start of Brian's penchant for doubletracking vocals, a pioneering innovation. Released on March 25, 1963, "Surfin' U.S.A.", met a more enthusiastic reception, reaching number two on the Billboard charts. This propelled the band into a nationwide spotlight, and was vital to launching surf music as a national craze. Five days prior to the release of "Surfin' U.S.A.", Brian produced "Surf City", a song he had written for Jan and Dean. "Surf City" hit number one on the "Billboard" charts in July 1963, a development that pleased Brian but angered Murry, who felt his son had "given away" what should have been the Beach Boys' first chart-topper.
At the beginning of a tour of the Mid-West in April 1963, Jardine rejoined the Beach Boys at Brian's request. As he began playing live gigs again, Brian left the road to focus on writing and recording. The result of this arrangement produced the albums "Surfer Girl", released on September 16, 1963 and "Little Deuce Coupe", released less than a month later on October 7, 1963. This sextet incarnation of the Beach Boys did not extend beyond these two albums, as Marks officially left the band in early October because of conflict with manager Murry, pulling Brian back into touring.
Around this time, Brian began using members of the Wrecking Crew to augment his increasingly demanding studio arrangements. Session musicians that participated on Wilson's productions were said to have been awestruck by his musical abilities. For composer Frank Zappa, the most exciting thing to him in "white-person-music" was when the Beach Boys used the progression V–II on "Little Deuce Coupe", calling it "an important step forward by going backward." The band released a standalone Christmas-themed single, "Little Saint Nick", in December 1963, backed with an a capella rendition of the scriptural song "The Lord's Prayer". The A-side peaked at number 3 on the US "Billboard" Christmas chart.
Following a successful Australasian tour in January and February 1964, the band returned home to face the British Invasion through the Beatles appearances on "The Ed Sullivan Show". Also representing the Beatles, Capitol support for the Beach Boys immediately began waning. This caused Murry to fight for the band at the label more than before, often visiting their offices without warning to "twist executive arms." Brian reacted to the Beatles bemusedly: "I was flipping out. I couldn't understand how a group could be just yelled and screamed at. The music they made, 'I Want to Hold Your Hand' for example, wasn't even that great a record, but they just screamed at it. ... It got us off our asses in the studio. We started cutting – we said 'look, don't worry about the Beatles, we'll cut our own stuff." Reportedly, Brian wanted more time to complete their next album, yet Capitol insisted they finish recording swiftly to avoid being forgotten in the throes of the impending invasion. Satisfying these demands, the band hastily finished the sessions on February 20, 1964 and titled the album "Shut Down Volume 2". "Fun, Fun, Fun" was released as a single from the album (backed with "Why Do Fools Fall in Love") and was a major hit. The LP, while containing several filler tracks, was propelled by other songs such as the melancholic "The Warmth of the Sun" and the advanced production style of "Don't Worry Baby".
Brian soon wrote his last surf song in April 1964. That month, during recording of the single "I Get Around", Murry was relieved of his duties as manager. Brian reflected, "We love the family thing – y'know: three brothers, a cousin and a friend is a really beautiful way to have a group – but the extra generation can become a hang-up." When the single was released in May of that year, it would climb to number one, their first single to do so. Two months later, the album that the song later appeared on, "All Summer Long", reached number four on the Billboard 200 charts. "All Summer Long" introduced exotic textures to the Beach Boys' sound exemplified by the piccolos and xylophones of its title track. The album was a swan-song to the surf and car music the Beach Boys built their commercial standing upon. Later albums took a different stylistic and lyrical path. Before this, a live album, "Beach Boys Concert", was released in October to a four-week chart stay at number one, containing a setlist of previously recorded hits and covers that they had not yet recorded.
"Today!" and "Summer Days".
In June 1964, Brian began recording the bulk of "The Beach Boys' Christmas Album" with a forty-one-piece studio orchestra in collaboration with Four Freshmen arranger Dick Reynolds. Released in December, it was divided between five new, original Christmas-themed songs, and seven reinterpretations of traditional Christmas songs. One single from the album, "The Man with All the Toys", was released, peaking at number 6 on the US "Billboard" Christmas chart. On October 29, the Beach Boys performed for "The T.A.M.I. Show", a concert film intended to bring together a wide range of hit-making musicians for a one-off performance. The result was released to movie theaters one month later.
By the end of the year, the stress of road travel, composing, producing and maintaining a high level of creativity became too much for Brian. On December 23, while on a flight from Los Angeles to Houston, he suffered a panic attack only hours after performing with the Beach Boys on the musical variety series "Shindig!". In January 1965, he announced his withdrawal from touring to concentrate entirely on songwriting and record production. For the rest of 1964 and into 1965, Glen Campbell served as Wilson's temporary replacement in concert, until his own career success pulled him from the group in April 1965. Bruce Johnston was asked to locate a replacement for Campbell; having failed to find one, Johnston himself became a full-time member of the band on May 19, 1965, first replacing Brian on the road and later contributing in the studio, beginning with the vocal sessions for "California Girls" on June 4, 1965.
After Brian stopped touring in 1965, he became a full-time studio artist, showcasing a great leap forward with "The Beach Boys Today!", an album containing a suite-like structure divided by songs and ballads, and portended the Album Era with its cohesive artistic statement. During the recording sessions for "Today!", Love told Melody Maker that he and the band wanted to look beyond surf rock and to avoid living in the past or resting on their laurels. The resulting LP had largely guitar-oriented pop songs such as "Dance, Dance, Dance" and "Good to My Baby" on side A with B-side ballads such as "Please Let Me Wonder" and "She Knows Me Too Well".
"Today!" marked a maturation in the Beach Boys' lyric content by abandoning themes related to surfing, cars, or teenage love. Some love songs remained, but with a marked increase in depth, along with introspective tracks accompanied by adventurous and distinct arrangements. While the band's contemporaries grew more intellectually aware, Capitol continued to bill them as "America’s Top Surfin' Group!" expecting Brian to write more surfing material for the yearly summer markets despite his disinterest.
In June 1965, the band released "Summer Days (And Summer Nights!!)". The album included a reworked arrangement of "Help Me, Rhonda" which became the band's second number one single in the spring of 1965, displacing the Beatles' "Ticket to Ride". "Let Him Run Wild" tapped into the youthful angst that later pervaded their music. In November 1965, the group followed their US number-three-charting "California Girls" from "Summer Days (And Summer Nights!!)" with another top-twenty single, "The Little Girl I Once Knew". It was considered the band's most experimental statement thus far, using silence as a pre-chorus, clashing keyboards, moody brass and vocal tics. The single continued Brian's ambitions for daring arrangements, featuring unexpected tempo changes and numerous false endings. Perhaps too extreme an arrangement to go much higher than its number 20 peak, it was the band's second single not to reach the top ten since their 1962 breakthrough.
Capitol demanded a Beach Boys LP for the 1965 Christmas season, and to appease them, Brian conceived "Beach Boys' Party!", a live-in-the-studio album consisting mostly of acoustic covers of 1950s rock and R&B songs, in addition to covers of three Beatles songs, Bob Dylan's "The Times They Are a-Changin'", and idiosyncratic rerecordings of the group's earlier hits. In December they scored an unexpected number two hit (number three in the UK) with "Barbara Ann", which Capitol released as a single with no band input. Originally by the Regents, it became one of the Beach Boys' most recognized hits.
"Pet Sounds".
In 1966, the Beach Boys formally established their use of unconventional instruments and elaborate layers of vocal harmonies on their groundbreaking record "Pet Sounds". It is considered Brian's most concise demonstration of his production and songwriting expertise. With songs such as "Wouldn't It Be Nice" and "Sloop John B", the album's innovative soundscape incorporates elements of jazz, classical, pop, exotica, and the avant-garde. The instrumentation combines found sounds such as bicycle bells and dog whistles with classically inspired orchestrations and the usual rock set-up of drums and guitars; among others, silverware, accordions, plucked piano strings, barking dogs, and plastic water jugs. For the basic rhythmic feel for "God Only Knows", harpsichord, piano with slapback echo, sleigh bells, and strings spilled into each other to create a rich blanket of sound.
Released in May, "Pet Sounds" eventually peaked at number eleven in the US and number two in the UK. This helped the Beach Boys become the strongest selling album act in the UK for the final quarter of 1966, dethroning the three-year reign of native bands such as the Beatles. Met with a lukewarm critical reception in the US, "Pet Sounds" was indifferently promoted by Capitol and failed to become the major hit Wilson had hoped it would be. Its failure to gain a wider recognition in the US combined with Capitol's decision to issue "Best of The Beach Boys" in July dispirited Brian, who considered "Pet Sounds" an extremely personal work. Some assumed that the label considered the album a risk, appealing more to an older demographic than the younger, female audience the Beach Boys built their commercial standing on. "Pet Sounds" sales numbered approximately 500,000 units, a significant drop-off from the chain of million-selling albums that immediately preceded it. "Best of The Beach Boys" was quickly certified Gold by the RIAA.
"Pet Sounds" is considered by some as a Brian Wilson solo album in all but name, as other members contributed relatively little to the compositions or recordings. Influenced by psychedelic drugs, Brian turned inward and probed his deep-seated self-doubts and emotional longings; the piece did not address the problems in the world around them, unlike other psychedelic rock groups. As Jim Miller wrote of the album's tone, " vented Wilson's obsession with isolation cataloging a forlorn quest for security. The whole enterprise, which smacked of song cycle pretensions, was streaked with regret and romantic langour." According to Brian, the album was designed as a collection of art pieces that belong together yet could stand alone. In a 1972 retrospective review of the album, music journalist Stephen Davis wrote: "From first cut to last we were treated to an intense, linear personal vision of the vagaries of a love affair and the painful, introverted anxieties that are the wrenching precipitates of the unstable chemistry of any love relationship. This trenchant cycle of love songs has the emotional impact of a shatteringly evocative novel ... nobody was prepared for anything so soulful, so lovely, something one had to think about so much."
"Pet Sounds" was massively influential upon its release, vaunting the band to the top level of rock innovators. It is one of the earliest rock concept albums, one of the earliest concept albums of the counterculture era, and an early album in the emerging psychedelic rock style, signaling a turning point wherein rock, which previously had been considered dance music, became music that was made for listening to. In 2016, "The Guardian"s Barbara Ellen reflected that the album was "Hailed as a revolution in harmonies and production techniques .. Wilson single-handedly reinvented the album as the in-depth illumination of an artist’s soul, kicking open a creative fire-door, liberating the album to exist as a self-contained art form on a par with literature, theatre, art, cinema, dance… anything the artist desired." Composer Phillip Glass observed:
In "The Album: A Guide to Pop Music's Most Provocative, Influential, and Important Creations", author James Perone championed the album for its complex orchestrations, sophisticated compositions, and varied tone colors, calling it a remove from "just about anything else that was going on in 1966 pop music." In 1976, journalist Robin Denselow wrote: "With the 1966 "Pet Sounds" album ... Wilson had become America's equivalent of the Beatles with his ability to expand the limits of popular taste." Paul McCartney named it one of his favorite albums of all time on multiple occasions, calling it the primary impetus for the Beatles' album "Sgt. Pepper's Lonely Hearts Club Band" (1967). In 2003, "Pet Sounds" was ranked second in "The 500 Greatest Albums of All Time" list selected by "Rolling Stone", behind only "Sgt. Pepper". In 2004, the album was acknowledged as an important historical and cultural work by the Library of Congress.
"Good Vibrations" and "Smile".
Seeking to expand on "Pet Sounds'" advances, Wilson began an even more ambitious project: "Good Vibrations". Like "Pet Sounds", Brian opted for an eclectic array of instruments rarely heard in pop music. Described by Brian as a "pocket symphony", it contains a mixture of classical, rock, and exotic instruments structured around a cut-up mosaic of musical sections represented by several discordant key and modal shifts. It became the Beach Boys' biggest hit to date, and a US and UK number one single in 1966. Coming at a time when pop singles were usually made in under two hours, it was one of the most complex pop productions ever undertaken, and the most expensive single ever recorded to that point. The production costs were estimated between $50,000 and $75,000 ($ and $ today) with sessions for the song stretching over several months in at least four major studios. According to Domenic Priore, the making of "Good Vibrations" was unlike anything previous in the realms of classical, jazz, international, soundtrack, or any other kind of recording.
The single was an unequivocal milestone in studio productions, and continued in establishing Brian as an extender of popular tastes. To the counterculture of the 1960s, "Good Vibrations" served as an anthem. Rock critic Gene Sculatti prophesied in 1968, " may yet prove to be the most significantly revolutionary piece of the current rock renaissance." Its instrumentation included Paul Tanner's Electro-Theremin, a manually-operated oscillator with a sound similar to a theremin, which helped the Beach Boys claim a new hippie audience. Upon release, the single prompted an unexpected revival in theremins while increasing awareness of analog synthesizers, leading Moog Music to produce their own brand of ribbon-controlled instruments. Reflecting on this period in 1971, "Cue" magazine wrote: "In the year and a half that followed "Pet Sounds", the Beach Boys were among the vanguard in practically every aspect of the counter culture – psychedelia, art rock, a return to roots, ecology, organic food, the cooled-out sound – anticipating changes that rock didn't accomplish until 1969–1970."
The group established a short-lived film production company, called Home Movies, during this time. It was supposed to have created live action film and television properties starring the Beach Boys. However, the company completed only one music video, for "Good Vibrations", though various other psychedelic sequences and segments exist.
Brian met lyricist and musician Van Dyke Parks while working on "Pet Sounds". A year later, while in the midst of recording "Good Vibrations", the duo began an intense collaboration that resulted in a suite of challenging new songs for the Beach Boys forthcoming album "Smile", intended to surpass "Pet Sounds". Recording for the album spanned about a year, from 1966 to 1967. Wilson and Parks intended "Smile" to be a continuous suite of songs that were linked both thematically and musically, with the main songs being linked together by small vocal pieces and instrumental segments that elaborated upon the musical themes of the major songs. Surviving recordings have shown that the music incorporated chanting, cowboy songs, explorations in Indian and Hawaiian music, jazz, tone poems with classical elements, cartoon sound effects, "musique concrète", and yodeling.
In October 1966 interviews, Brian touted the album "a teenage symphony to God". His spiritual aims were made explicit in the album's musical contents and lyrics, which also included existential angst, the exploration of human innocence, and the philosophy of childlikeness. Parks has stated: he and Brian were conscious of the counterculture, and the two had felt estranged from it, but it was necessary to adhere to because of a willingness to "get out of the Eisenhower mindset." Parks stresses, "At the same time, he didn't want to lose that kind of gauche sensibility that he had. He was doing stuff that nobody would dream of doing," citing an instance when Brian instructed a banjo player to play only one string, a "gauche" style of playing that "just wasn't done."
"Smile" would go on to become the most legendary unreleased album in the history of popular music. In the decades following its non-release, it became the subject of intense speculation and mystique. Many believe that, had the album been released, it would have substantially altered the group's direction and established them at the vanguard of rock innovators. Writing about the album for the 33⅓ book series, Luis Sanchez stated: "If Brian Wilson and The Beach Boys were going to survive as the defining force of American pop music they were, "Smile" was a conscious attempt to rediscover the impulses and ideas that power American consciousness from the inside out. It was a collaboration that led to some incredible music, which, if it had been completed as an album and delivered to the public in 1966, might have had an incredible impact."
If released when intended, composer Frank Oteri believes the album would have been the first piece of album-oriented rock. Its cover artwork, now considered iconic, depicted an illustration of a store selling smiles, also would have been among the earliest covers by a popular music group to feature original, specifically commissioned artwork rather than a photograph of the performers. Throughout the 1980s and 1990s, bootlegs from its recording sessions leaked, inspiring many attempts to reassemble the album, and ultimately becoming a progenitor for indie rock.
Many factors combined to put intense pressure on Brian Wilson as "Smile" neared completion: his mental instability, the pressure to create despite fierce internal opposition to his new music, the relatively unenthusiastic response to "Pet Sounds" in the United States, Carl Wilson's draft resistance, and a major dispute with Capitol Records. Furthermore, Wilson's reliance on both prescription drugs and amphetamines exacerbated his underlying mental health problems. Comparable to Brian Jones and Syd Barrett, Brian Wilson's use of psychedelic drugs—especially LSD—led to a nervous breakdown in the late-1960s. As his legend grew, the "Smile" period came to be seen as the pivotal episode in his decline, and he became tagged as a drug casualty.
1967–77: fluctuating leadership.
"Smiley Smile" and "Wild Honey".
Some "Smile" tracks were salvaged and re-recorded in scaled-down versions at Brian's new home studio. Along with the single version of "Good Vibrations", these tracks were released on the album "Smiley Smile", which elicited positive critical and commercial response abroad, but was the first real commercial failure for the group in the United States. In 1969 the group filed a lawsuit in the Los Angeles Superior Court against Capitol Records for over two million dollars and at the same time, severed their relationship with the label. At this time the Beach Boys' management (Nick Grillo and David Anderle) created the band's own record label, Brother. One of the first labels owned by a rock group Brother Records was intended for releases of Beach Boys side projects, and as an invitation to new talent. The initial output of the label, however, was limited to "Smiley Smile" and two resulting singles from the album. The failure of "Gettin' Hungry" caused the band to shelve Brother until 1970. Despite the cancellation of "Smile", several tracks—including "Our Prayer", "Cabin Essence" and "Surf's Up"—continued to trickle out in later albums often as filler songs to offset Brian's unwillingness to contribute. The band was still expecting to complete and release "Smile" as late as 1973 before it became clear that only Brian could comprehend the endless fragments that had been recorded. In 1967, "Smiley Smile" was followed up three months later in December with "Wild Honey" being released, featuring songs written by Wilson and Love, including the hit "Darlin'" and a rendition of Stevie Wonder's "I Was Made to Love Her". The album fared better than its predecessor, reaching number 24 in the US.
Compounding the group's recent setbacks, their public image took a cataclysmic hit following their withdrawal from the 1967 Monterey Pop Festival for the reason that they had no new material to play while their forthcoming single and album lay in limbo. Their cancellation was seen as "...a damning admission that they were washed up unable to compete with the new music." This notion was exacerbated by "Rolling Stone" writer Jann Wenner, who in contemporary publications criticized Brian Wilson for his oft-repeated "genius" label, which he called a "promotional shuck" and an attempt to compare him with the Beatles. However, Wenner later responded to their "Wild Honey" album with more optimism, remarking two months later that "[in any case it's good to see that the Beach Boys are getting their heads straight once again".
While being interviewed in August 1967 for the failed live album "Lei'd in Hawaii", Brian admitted: "I think rock n' roll–the pop scene–is happening. It’s great. But I think basically, the Beach Boys are squares. We’re not happening." Former band publicist Derek Taylor later recalled a conversation with Brian and Dennis where they denied that the group had ever written surf music or songs about cars, and that the Beach Boys had never been involved with the surf and hot rod fads, as Taylor claimed, "...they would not concede." As a result of their initial target demographic and subsequent failures to blend with the hippie movement, the group was viewed as unhip relics, even though they had once been, as biographer Peter Ames Carlin wrote, "the absolute center of the American rock ’n’ roll scene," a time when they had ushered the psychedelic era. In early 1969, Brian proposed that the group change their name from "the Beach Boys" to "the Beach", reasoning that the band members were now grown men. Going to the effort of acquiring a contract that would declare a five-way agreement to officially rename the group, Stephen Desper reported, "They all just kind of shrugged and said, 'Aw, come on, Brian, we don't wanna do that. That's how the public knows us, man. And that was it. He put the paper on the piano and it stayed there until I picked it up and took it away."
"Friends" and "20/20".
After meeting Maharishi Mahesh Yogi at a UNICEF Variety Gala in Paris, France on December 15, 1967, Love, along with other high-profile celebrities such as Donovan and the Beatles traveled to Rishikesh in India during February and March 1968. The following Beach Boys album "Friends" (1968) had songs influenced by the Transcendental Meditation taught by the Maharishi. The album reached number 13 in the UK and 126 in the US, the title track placing at number 25 in the UK and number 47 in the US, the band's lowest singles peak since 1962. In support of the "Friends" album, Love had arranged for the Beach Boys to tour with the Maharishi in the US, which has been called "one of the more bizarre entertainments of the era." Starting on May 3, 1968, the tour lasted five shows and was cancelled when the Maharishi had to withdraw to fulfill film contracts. Because of disappointing audience numbers and the Maharishi's withdrawal, twenty-four tour dates were subsequently cancelled at a cost estimated at US$250,000 (approximately US$ today) for the band. This tour was followed by the release of "Do It Again", a single that critics described as an update of the Beach Boys' surf rock past in a late 1960s style. The single went to the top of the Australian and UK single charts in 1968 and was moderately successful in the US, peaking at number 20.
For a short time in mid-1968, Brian Wilson sought psychological treatment in hospital. During his absence, other members began writing and producing material themselves. To complete their contract with Capitol, they produced one more album. "20/20" (1969) was one of the group's most stylistically diverse albums, including hard rock songs such as "All I Want to Do", the waltz-based "Time to Get Alone", and a cover of the Ronettes' "I Can Hear Music". The diversity of genres have been described as an indicator that the group was trying to establish an updated identity. The album performed strongly in the UK, reaching number three on the charts. In the US, the album reached a modest 68.
In spring 1968, Dennis began a strained relationship with musician Charles Manson, which persisted for several months afterward. Dennis bought him time at Brian's home studio where recording sessions were attempted while Brian stayed in his room. Dennis then proposed that Manson be signed to Brother Records. Brian reportedly disliked Charlie, and so a deal was never made. Without Manson's involvement, the Beach Boys did record one song penned by Manson: "Cease to Exist", rewritten as "Never Learn Not to Love". The idea of the Beach Boys recording one of his songs reportedly thrilled Manson, and it was released as a Beach Boys single. After accruing a large monetary debt to the group, Dennis deliberately omitted Manson's credit on its release while also altering the song's arrangement and lyrics. This greatly angered Manson. Growing fearful, Dennis gradually distanced himself from Manson, whose family had taken over his home. He was eventually convicted for murder conspiracy; from there on, Dennis was too afraid of the Manson family to ever speak publicly on his relationship, let alone testify against him.
On April 12, 1969, the band revisited their 1967 lawsuit against Capitol Records after they alleged an audit undertaken revealed the band were owed over US$2,000,000 (US$ today) for unpaid royalties and production duties. The band's contract with Capitol Records expired on June 30, 1969, after which Capitol Records deleted the Beach Boys' catalog from print, effectively cutting off their royalty flow. In November 1969, Murry Wilson sold Sea of Tunes, the Beach Boys' catalog, to Irving Almo Music, a decision that, according to Marilyn Wilson, devastated Brian. In late 1969, the Beach Boys reactivated their Brother label and signed with Reprise. Around this time, the band commenced recording a new album. By the time the Beach Boys tenure ended with Capitol in 1969, they had sold 65 million records worldwide, closing the decade as the most commercially successful American group in popular music.
"Sunflower", "Surf's Up", "So Tough", and "Holland".
In 1970, armed with the new Reprise contract, the band appeared rejuvenated, releasing the album "Sunflower" to critical acclaim in the UK but indifference in the US. The album features a strong group presence with significant writing contributions from all band members. Brian was active during this period, writing or co-writing seven of the twelve songs on "Sunflower" and performing at half of the band's domestic concerts in 1970. "Sunflower" reached number 29 in the UK and number 151 in the US, the band's lowest domestic chart showing to that point. A version of "Cottonfields" arranged by Al Jardine appeared on European releases of "Sunflower" and as a single, reached number one in Australia, Norway, South Africa and Sweden and the top-five in six other countries, including the UK.
After "Sunflower", the band hired Jack Rieley as their manager. Under Rieley's management, the group's music began emphasizing political and social awareness. During this time, Carl Wilson gradually assumed leadership of the band and Rieley contributed lyrics. On August 30, 1971 the band released "Surf's Up", named after the Brian Wilson/Van Dyke Parks composition "Surf's Up". The album was moderately successful, reaching the US top 30, a marked improvement over their recent releases. While the record charted, the Beach Boys added to their renewed fame by performing a near-sellout set at Carnegie Hall, followed by an appearance with the Grateful Dead at Fillmore East on April 27, 1971. The live shows during this era included reworked arrangements of many of the band's previous songs. A large portion of their set lists culled from "Pet Sounds" and "Smile", as author Domenic Priore observes, "They basically played what they could have played at the Monterey Pop Festival in the summer of 1967."
Johnston ended his first stint with the band shortly after "Surf's Ups release, reportedly because of friction with Rieley. At Carl's suggestion, the addition of Ricky Fataar and Blondie Chaplin in February 1972 led to a dramatic restructuring in the band's sound. The album "Carl and the Passions – "So Tough"" was an uncharacteristic mix that included two songs written by Fataar and Chaplin.
For their next project the band, their families, assorted associates and technicians moved to the Netherlands for the summer of 1972. They rented a farmhouse to convert into a makeshift studio where recording sessions for the new project would take place. By the end of their sessions, the band felt they had produced one of their strongest efforts yet. Reprise, however, felt that the album required a strong single. This resulted in the song "Sail On, Sailor", a collaboration between Brian Wilson, Tandyn Almer, Ray Kennedy, Jack Rieley and Van Dyke Parks featuring a soulful lead vocal by Chaplin. Reprise subsequently approved and the resulting album, "Holland", was released early in 1973, peaking at number 37. Brian's musical children story, "Mount Vernon and Fairway (A Fairy Tale)", narrated by Rieley and strongly influenced by Randy Newman's "Sail Away" (1972), was included as a bonus EP. Despite indifference from Reprise, the band's concert audience started to grow.
"The Beach Boys in Concert", a double album documenting the 1972 and 1973 US tours, was another top-30 album and became the band's first gold record under Reprise. During this period the band established itself as one of America's most popular live acts. Chaplin and Fataar helped organize the concerts to obtain a high quality live performance, playing material off "Surf's Up", "Carl and the Passions" and "Holland" and adding songs from their older catalog. This concert arrangement lifted them back into American public prominence. In late 1973, the 41-song soundtrack to "American Graffiti" was released including the band's early songs "Surfin' Safari" and "All Summer Long". The album was a catalyst in creating a wave of nostalgia that reintroduced the Beach Boys into contemporary American consciousness. In 1974, Capitol Records issued "Endless Summer", the band's first major pre-"Pet Sounds" greatest hits package. The compilation surged to the top of the "Billboard" album charts and was the group's first multi-million selling record since "Good Vibrations". It remained on the charts for two years. Capitol followed with a second compilation, "Spirit of America", which also sold well. With these compilations, the Beach Boys became one of the most popular acts in rock, propelling themselves from opening for Crosby, Stills, Nash and Young to headliners selling out basketball arenas in a matter of weeks. "Rolling Stone" named the Beach Boys the "Band of the Year" for 1974, solely on the basis of their juggernaut touring schedule and material written over a decade earlier.
Rieley, who remained in the Netherlands after "Holland"s release, was relieved of his managerial duties in late 1973. Chaplin also left in late 1973 after an argument with Steve Love, the band's business manager (and Mike's brother). Fataar remained until 1974, when he was offered a chance to join a new group led by future Eagles member Joe Walsh. Chaplin's replacement, James William Guercio, started offering the group career advice that resulted in his becoming their new manager. Under Guercio, the Beach Boys staged a highly successful 1975 joint concert tour with Chicago, with each group performing some of the other's songs, including their previous year's collaboration on Chicago's hit "Wishing You Were Here". Beach Boys vocals were also heard on Elton John's 1974 hit "Don't Let the Sun Go Down on Me". Nostalgia had settled into the Beach Boys' hype; the group had not officially released any new material since 1973's "Holland". While their concerts continuously sold out, the stage act slowly changed from a contemporary presentation followed by oldies encores to an entire show made up of mostly pre-1967 music.
"15 Big Ones" and "Love You".
Recorded in the wake of California Music's demise, a supergroup that would have involved Brian Wilson, Bruce Johnston, and record producer Terry Melcher, "15 Big Ones" (1976) marked Brian's return as a major force in the group. The album included new songs by Brian, as well as cover versions of oldies such as "Rock and Roll Music", "Blueberry Hill", and "In the Still of the Night". "Rock and Roll Music" peaked at number 5 in the US. Brian and Love's "It's O.K." was in the vein of their early sixties style, and was a moderate hit. The album was publicized by an August 1976 NBC-TV special, simply titled "The Beach Boys". The special, produced by "Saturday Night Live" (SNL) creator Lorne Michaels, featured appearances by "SNL" cast members John Belushi and Dan Aykroyd.
The album was generally disliked by fans and critics upon release. During its sessions, Brian's production role was belittled as group members overdubbed and remixed tracks to fight against his desire for a rough, unfinished sound. Carl and Dennis disparaged the album to the press while Brian admitted, " the new album is nothing too deep", but remained hopeful that their next release would be on par with the group's "Good Vibrations".
For the remainder of 1976 to early 1977, Brian spent his time making sporadic public appearances and producing the band's next album "Love You" (1977), a quirky collection of 14 songs mostly written, arranged and produced by Brian. Brian revealed to Peter Ames Carlin that "Love You" is one of his favorite Beach Boys releases, telling him "That's when it all happened for me. That's where my heart lies." "Love You" peaked at number 28 in the UK and number 53 in the US and developed a cult following; regarded as one of the band's best albums by fans and critics alike, and an early work of synthpop.
Referring to "naysayers" of the album, the underground fanzine "Scram" wrote, "Fuck ... [the album showcases a truly original mix of humor and sadness. The original numbers always dance just a step away from the cliché, dealing with simple lyrical themes that make you wonder why they had never been explored before." "The A.V. Club" – considering the album in "the same vein" as "Tonight's the Night" (1975), "Pussy Cats" (1974), "The Madcap Laughs" (1970), and "Barrett" (1970) – described "Love You" as: "something almost desperately optimistic ... Wilson sings frayed songs about roller-skating, road-tripping, and Johnny Carson—like a frazzled man sitting in a corner chanting 'calm blue ocean' over and over. It’s a beautiful, noisy, funny, heartbreaking work of art—one not for everybody, yet vital for anyone who wants to understand Wilson’s overall worldview."
After "Love You" was released, Brian began to record and assemble "Adult/Child", an unreleased effort largely consisting of songs written by Wilson from 1976 and 1977 with select big band arrangements by Dick Reynolds. Though publicized as the Beach Boys' next release, "Adult/Child" caused tension within the group and was ultimately shelved. Following this period, his concert appearances with the band gradually diminished and their performances were occasionally erratic.
1978–present.
Internal divisions and personal struggles.
The internal wrangling came to a head after a show at Central Park on September 1, 1977, when the band effectively split into two camps; Dennis and Carl Wilson on one side, Mike Love and Al Jardine on the other with Brian remaining neutral. Following a confrontation on an airport tarmac, Dennis declared to "Rolling Stone" on September 3 that he had left the band: "It was Al Jardine who really knifed me in the heart when he said they didn't need me. That was the clincher. And all I told him was that he couldn't play more than four chords. They kept telling me I had my solo album now ["Pacific Ocean Blue"], like I should go off in a corner and leave the Beach Boys to them. The album really bothers them. They don't like to admit it's doing so well; they never even acknowledge it in interviews."
The band broke up for two and a half weeks, until a meeting on September 17 at Brian's house. In light of a potential new Caribou Records contract the parties negotiated a settlement resulting in Love gaining control of Brian's vote in the group, allowing Love and Jardine to outvote Carl and Dennis Wilson on any matter.
Dennis withdrew from the group to focus on his second solo album entitled "Bambu". The album was shelved just as alcoholism and marital problems overcame all three Wilson brothers. Carl appeared intoxicated during concerts (especially at appearances for their 1978 Australia tour) and Brian gradually slid back into addiction and an unhealthy lifestyle. Love remembered: "We were in Australia, and the Wilsons were upset that some of us were not trying heroin with them. That was a division. ... Brian, Carl and Dennis were into one lifestyle, whereas myself and Al Jardine and Bruce Johnston were not."
The Beach Boys' last album for Reprise, "M.I.U. Album" (1978), was recorded at Maharishi International University in Iowa at the suggestion of Love. Dennis and Carl made limited contributions; the album was mostly produced by Jardine and Ron Altbach, with Brian credited as "executive producer". "M.I.U." was largely a contractual obligation to finish out their association with Reprise, who likewise did not promote the result. The record cemented the divisions in the group. Love and Jardine focused on rock and roll-oriented material while Carl and Dennis chose the progressive focus they had established with the albums "Carl and the Passions" and "Holland".
After departing Reprise, the Beach Boys signed with CBS Records. They received a substantial advance and were paid $1 million per album even as CBS deemed their preliminary review of the band's first product, "L.A. (Light Album)" as unsatisfactory. Faced with the realization that Brian was unable to contribute, the band recruited Johnston as producer. The result paid off, as "Good Timin'" became a top 40 single. The group enjoyed moderate success with a disco reworking of the "Wild Honey" song "Here Comes the Night", followed by their highest charting UK single in nine years: Jardine's "Lady Lynda" peaked at number 6 in the UK Singles Chart. The album was followed in 1980 by "Keepin' the Summer Alive", with Johnston once again producing. Barring an appearance on percussion on the closing track, "Endless Harmony", Dennis was absent from this album.
In 1981, Carl quit the group because of unhappiness with the band's nostalgia format and lackluster live performances, subsequently pursuing a solo career. He returned in May 1982 – after approximately 14 months of being away—on the condition that the group reconsider their rehearsal and touring policies and refrain from "Las Vegas-type" engagements.
From 1980 through 1982, the Beach Boys and The Grass Roots performed Independence Day concerts at the National Mall in Washington, D.C., attracting large crowds.Phil McCombs and Richard Harrington, "Watt Sets Off Uproar with Music Ban", "The Washington Post", Washington, D.C., April 7, 1983, pp. A1, A17.</ref> However, in April 1983, James G. Watt, President Ronald Reagan's Secretary of the Interior, banned Independence Day concerts on the Mall by such groups. Watt said that "rock bands" that had performed on the Mall on Independence Day in 1981 and 1982 had encouraged drug use and alcoholism and had attracted "the wrong element", who would steal from attendees. During the ensuing uproar, which included over 40,000 complaints to the Department of the Interior, the Beach Boys stated that the Soviet Union, which had invited them to perform in Leningrad in 1978, "...obviously ... did not feel that the group attracted the wrong element." Vice President George H. W. Bush said of the Beach Boys, "They're my friends and I like their music". Watt later apologized to the band after learning that President Reagan and First Lady Nancy Reagan were fans. White House staff presented Watt with a plaster foot with a hole in it, showing that he had "shot himself in the foot". The band returned to D.C. for Independence Day in 1984 and performed to a crowd of 750,000 people.
Deaths of Dennis and Carl.
Founding drummer Dennis Wilson's alcoholism continued to escalate, and on December 28, 1983, he drowned in Marina del Rey while diving from a friend's boat trying to recover items he had previously thrown overboard in fits of rage. Despite his death, the Beach Boys continued as a successful touring act.
On July 4, 1985, the Beach Boys played to an afternoon crowd of one million in Philadelphia and the same evening they performed for over 750,000 people on the Mall in Washington. They also appeared nine days later at the Live Aid concert. That year, they released the eponymous album "The Beach Boys" and enjoyed a resurgence of interest later in the 1980s, assisted by tributes such as David Lee Roth's hit version of "California Girls". "Getcha Back", released from the album, gave the group a number 26 single in the US. Following this, the group put out "Rock 'n' Roll to the Rescue" (US, number 68) and a cover of the Mamas & the Papas' "California Dreamin'" (US, number 57). In 1987, they played with the rap group The Fat Boys, performing the song "Wipe Out" and filming a music video. It was a number 12 single in the US. and a number two rank in the UK.
By 1988, Brian had drifted from the Beach Boys and released his first solo album, "Brian Wilson", which received critical acclaim. During this period the band unexpectedly claimed their first US number one hit single in 22 years with "Kokomo", which had appeared in the movie "Cocktail", and soon became the band's largest selling single of all time. Inducted into the Rock and Roll Hall of Fame earlier in the year, the group became the second artist after Aretha Franklin to hit number one in the US after their induction. They released the album "Still Cruisin"', which went gold in the US and gave them their best chart showing since 1976. In 1990, the band gathered several studio musicians and recorded the Melcher-produced title track of the comedy "Problem Child". The album "Summer in Paradise", having no new contributions from Brian because of interference from caretaker Eugene Landy, was released two years later to a poor critical reception, and was a commercial disaster.
A lawsuit was filed by Brian in 1989 to reclaim the rights to his songs and the group's publishing company, Sea of Tunes, which he had supposedly signed away to his father Murry in 1969. He successfully argued that he had not been mentally fit to make an informed decision and that his father had potentially forged his signature. While Wilson failed to regain his copyrights, he was awarded $25 million for unpaid royalties. Soon after Wilson won his case, Love discovered that Murry Wilson had not properly credited him as co-writer on 79 Beach Boys songs. With Love and Brian unable to determine exactly what Love was properly owed, Love sued Brian in 1992, winning $13 million in 1994 for lost royalties. 35 of the group's songs were then amended to credit Love.
In 1993, the band appeared in Michael Feeney Callan's film "The Beach Boys Today", which included in-depth interviews with all members except Brian. Carl confided to Callan that Brian would record again with the band at some point in the near future. A few Beach Boys sessions devoted to new Brian Wilson compositions occurred during the mid-1990s, but they remain largely unreleased, and the album was quickly cancelled because of personal conflicts. In February 1996, the Beach Boys guested with Status Quo on a re-recording of "Fun, Fun, Fun", which became a British Top-30 hit. In June, the group worked with comedian Jeff Foxworthy on the recording "Howdy From Maui", and eventually released "Stars and Stripes Vol. 1" in August 1996. The album consisted of country renditions of several Beach Boys hits, performed by popular country artists such as Toby Keith and Willie Nelson. Brian, who was in a better mental state at the time, acted as co-producer.
In early 1997, Carl was diagnosed with lung cancer and brain cancer after years of heavy smoking. Despite his terminal condition, Carl continued to perform with the band on its 1997 summer tour while undergoing chemotherapy. During performances, he sat on a stool and needed oxygen after every song. However, Carl was able to stand when he played on "God Only Knows". Carl died on February 6, 1998, two months after the death of the Wilsons' mother, Audree.
Band split and name conflicts.
Following Carl's death, the remaining members splintered. Love, Johnston and former guitarist Marks continued to tour without Jardine, initially as "America's Band", but following several cancelled bookings under that name, they sought authorization through Brother Records Inc. (BRI) to tour as "The Beach Boys" and secured the necessary license. In turn, Jardine began to tour regularly with his band dubbed "Beach Boys: Family & Friends" until he ran into legal issues for using the name without license. Meanwhile, Jardine sued Love and Brian, claiming that he had been excluded from their concerts. BRI, through its longtime attorney, Ed McPherson, sued Jardine in Federal Court. Jardine, in turn, counter-claimed against BRI for wrongful termination. BRI ultimately prevailed after several years. Love was allowed to continue to tour as The Beach Boys, while Jardine was prohibited from touring using any form of the name.
Released from Landy's control, Brian Wilson sought different treatments for his illnesses that aided him in his solo career. He toured regularly with his backing band consisting of members of Wondermints and other LA/Chicago musicians. Marks also maintained a solo career. Their tours remained reliable draws, with Wilson and Jardine both remaining legal members of the Beach Boys organization and BRI. The surviving group members appeared as themselves for the 1998 documentary film "", directed by Alan Boyd. Following the success of 1997's "The Pet Sounds Sessions", many compilations were then issued by Capitol containing new archival material: "Endless Harmony Soundtrack" (1998), "Ultimate Christmas" (1998), and "Hawthorne, CA" (2001).
In 2004, Wilson recorded and released his solo album "Brian Wilson Presents Smile", a reinterpretation of the "Smile" project that he initiated with the Beach Boys thirty-six years earlier. That September, Wilson issued a free CD through the "Mail On Sunday" that included Beach Boys songs he had recently rerecorded, five of which he co-authored with Love. The 10 track compilation had 2.6 million copies distributed and prompted Love to file a lawsuit in November 2005; he claimed the promotion hurt the sales of the original recordings. Love's suit was dismissed in 2007 when a judge determined that there were no triable issues.
On June 13, 2006, the five surviving Beach Boys (Wilson, Love, Jardine, Johnston, and Marks) appeared together for the celebration of the 40th anniversary of "Pet Sounds" and the double-platinum certification of their greatest hits compilation, "", in a ceremony atop the Capitol Records building in Hollywood. Plaques were awarded for their efforts, with Wilson accepting on behalf of Dennis and Carl. Throughout the year, it was rumored that the band would reform to perform the "Pet Sounds" album live in its entirety in November. Ultimately, Wilson began a brief "Pet Sounds" tour with Jardine and no other group members.
50th year reunion celebration.
On October 31, 2011, the Beach Boys released surviving 1960s recordings from "Smile" in the form of "The Smile Sessions". The album—even in its incomplete form—garnered universal critical acclaim and experienced popular success, charting in both the Billboard US and UK Top 30. The band was rewarded with glowing reviews, including inclusion in Rolling Stone's Top 500 album list at number 381. "The Smile Sessions" went on to win Best Historical Album at the 2013 Grammy Awards.
In February 2011, the Beach Boys released "Don't Fight the Sea", a charity single to aid the victims of the 2011 Japan earthquake. The single, released on Jardine's 2011 album "A Postcard From California" featured Jardine, Wilson, Love and Johnston, with prerecorded vocals by Carl Wilson. Rumors then circulated regarding a potential 50th anniversary band reunion.
On December 16, 2011, it was announced that Wilson, Love, Jardine, Johnston and Marks would reunite for a new album and 50th anniversary tour in 2012 to include a performance at the New Orleans Jazz Festival in April 2012. On February 12, 2012, the Beach Boys performed at the 2012 Grammy Awards, in what was billed as a "special performance" by organizers. It marked the group's first live performance to include Brian since 1996. The Beach Boys then appeared at the April 10, 2012, season opener for the Los Angeles Dodgers and performed "Surfer Girl" and "The Star-Spangled Banner". In April, the new album's title was revealed as "That's Why God Made the Radio".</ref> The first single from the album, the title track, made its national radio debut April 25, 2012, on ESPN's "Mike and Mike in the Morning"</ref> and was released on iTunes and other digital platforms on April 26. "That's Why God Made the Radio" debuted at number three on US charts, making US chart history by expanding the group's span of "Billboard" 200 top ten albums across 49 years and one week, passing the Beatles with 47 years of top ten albums.
Later in 2012, the group released the "Fifty Big Ones" and "Greatest Hits" compilations along with reissues of 12 of their albums. The next year, the group released "Live – The 50th Anniversary Tour" a 41 song, 2-CD set documenting their "50th Anniversary Tour". While there were no definite plans, Brian stated that he would like to make another Beach Boys album following the world tour. In August 2013, the group released "Made in California", a six disc collection featuring more than seven and a half hours of music, including more than 60 previously unreleased tracks, and concluding the Beach Boys' 50th anniversary campaign.
Resumed band split.
In June 2012, Love announced additional touring dates that would not feature Wilson. Wilson then denied knowledge of these new dates.
On October 5, Love announced in a self-written press release to the "LA Times" that the band would return to its pre-50th Reunion Tour lineup with him and Johnston touring as the Beach Boys without Wilson, Jardine, and Marks:
Four days later, Wilson and Jardine submitted a written response to the rumors stating: "After Mike booked a couple of shows with Bruce, Al and I were, of course, disappointed. Then there was confusion in some markets when photos of me, Al and David and the 50th reunion band appeared on websites advertising his shows ... I was completely blindsided by his press release ... We hadn't even discussed as a band what we were going to do with all the offers that were coming in for more 50th shows." Love accused Wilson's statements in this press release to be falsified by his agents, again affirming that the presupposed agreements were "well-documented", and that Wilson had halted further touring dates. On December 13, Wilson and Jardine played a Christmas show at which they performed the Beach Boys Christmas songs. Following this appearance, Wilson announced concert dates featuring himself, Jardine and Marks. Love and Johnston continued to perform under the Beach Boys name, while Wilson, Jardine, and Marks continued to tour as a trio, and a subsequent tour with guitarist Jeff Beck also included former Beach Boy Blondie Chaplin at select dates. Reflecting upon the band's reunion in 2013, Love stated: "I had a wonderful experience being in the studio together. Brian has lost none of his ability to structure those melodies and chord progressions, and when we heard us singing together coming back over the speakers it sounded like 1965 again. Touring was more for the fans. ... It was a great experience, it had a term to it, and now everyone's going on with their ways of doing things."
Jardine, Marks, Johnston and Love appeared together at the 2014 Ella Awards Ceremony, where Love was honored for his work as a singer. Marks sang "409" in honor of Love, and Jardine performed "Help Me Rhonda". They closed the show with "Fun, Fun, Fun". Wilson's long time band associate Jeff Foskett also appeared, but not Wilson. On May 15, 2014 the touring Beach Boys (Love and Johnston) announced a tour celebrating "50 Years of 'Fun Fun Fun, named for their 1964 single. The tour featured the addition of Foskett, who replaced Mike's son Christian. Foskett left Wilson's band because of encumbering responsibilities, and hopes that Wilson and Love's band would someday converge, believing that the two Beach Boys do not "personally have a problem with each other." As of September 2014, Jardine has maintained that a continued reunion with the Beach Boys is "really up to him ... He claims he didn't, that he fired us after the reunion ... He’s a brilliant songwriter, and unfortunately he has brilliant lawyers. We wish him all the best, but doggonit, you know, we’d like to be Beach Boys, too. There you go." As Jardine restates "[Love doesn’t really want to work with us", biographer Jon Stebbins speculated that Love declined to continue working with the group because of the lesser control he had over the touring process, coupled with the lower financial gain, noting: "Night after night after night after night, Mike is making less money getting reminded that Brian is more popular than him. And he has to answer to people instead of calling all the shots himself."
In 2015, "Soundstage" aired an episode featuring Wilson performing with Jardine and former Beach Boys Blondie Chaplin and Ricky Fataar at The Venetian in Las Vegas. In April 2015, when asked if he was interested in making music with Love again, Wilson replied: "I don’t think so, no," later adding in July that he "doesn't talk to the Beach Boys Mike Love." On July 25, Love said: "If you get Brian and I, we might go to the piano. But with every band there are cliques that are formed with management, wives, agents, publicists — and the tendency is with some people is they tend to lionize or make one person more important than the others. ... The Beach Boys and all these bands that ever existed are a team. I learned as captain of my cross country team that you don't put a person down to get their best efforts, you encourage them."
Musical style and development.
In "Understanding Rock: Essays in Musical Analysis", music theorist Daniel Harrison summarizes:
The Beach Boys began as a garage band playing 1950s style rock and roll, reassembling styles of music such as surf to include vocal jazz harmony, which created their unique sound. In addition, they introduced their signature approach to common genres such as the pop ballad by applying harmonic or formal twists not native to rock and roll. Early on, Love sang lead vocals in the rock-oriented songs, while Carl contributed crisp guitar lines on the group's ballads. Miller observed, "On straight rockers they sang tight harmonies behind Love's lead ... on ballads, Brian played his falsetto off against lush, jazz-tinged voicings, often using (for rock) unorthodox harmonic structures." Harrison adds, "But even the least distinguished of the Beach Boys' early uptempo rock 'n' roll songs show traces of structural complexity at some level; Brian was simply too curious and experimental to leave convention alone." This new sound was quickly associated with the Modernism movement blooming in the Los Angeles music scene. Among the distinct elements of the Beach Boys' style were the nasal quality of their singing voices, their use of a falsetto harmony over a driving, locomotive-like melody, and the sudden chiming in of the whole group on a key line.
During their early years, the Beach Boys released music that displayed an increasing level of sophistication, a period where Brian Wilson consistently acted as the group's primary bandleader, songwriter, producer, and arranger for the group's most commercially and critically successful work. Brian is quoted saying: "Everyone contributed something. Carl kept us hip to the latest tunes, Al taught us his repertoire of folk songs, and Dennis, though he didn't play anything, added a combustible spark just by his presence." In a 1966 article that asks "Do the Beach Boys rely too much on sound genius Brian?" Carl responded that every member of the group contributes ideas, but admitted that Brian was majorly responsible for their music. In 1967, Dennis was cited as "the closest to brother Brian's own musical ideals ... He always emphasises the fusion, in their work, of pop and classical music."
In early 1964, Brian began his breakaway from beach-themed music. Later in November of the same year, the group expressed desires to advance from the surf rock style for which they initially became known for. "New York" magazine would later refer to the albums "Pet Sounds" and "Smiley Smile" as their "experimental pop phase". The band went on to incorporate many more genres, from baroque pop to psychedelia and synthpop.
Influences.
The band's earliest influences came primarily from the work of Chuck Berry and the Four Freshmen. Performed by the Four Freshmen, "Their Hearts Were Full of Spring" (1961) was a particular favorite of the group. By deconstructing their arrangements of pop standards, Brian educated himself on jazz harmony. Taking this into mind, Philip Lambert noted, "If Bob Flanigan helped teach Brian how to sing, then Gershwin, Kern, Porter, and the other members of this pantheon helped him learn how to craft a song." Other general influences on the group included the Hi-Los, the Penguins, the Robins, Bill Haley & His Comets, Otis Williams, the Cadets, the Everly Brothers, the Shirelles, the Regents, and the Crystals.
Geoffrey Himes wrote: "Though the Beach Boys are often caricatured as the ultimate white, suburban act, black R&B was crucial to their sound." Carl remembers: "Most of [Mike's] classmates were black. He was the only white guy on his track team. He was really immersed in doo-wop and that music and I think he influenced Brian to listen to it. The black artists were so much better in terms of rock records in those days that the white records almost sounded like put-ons." Their eclectic mix of white and black vocal groups – ranging from the rock and roll of Berry, the jazz harmonies of the Four Freshmen, the pop of the Four Preps, the folk of the Kingston Trio, the R&B of groups like the Coasters and the Five Satins, and the doo wop of Dion and the Belmonts – helped contribute to the Beach Boys' uniqueness in American popular music.
While the Beach Boys are not often associated with blues, Brian has called this a misapprehension, citing Smokey Robinson and Stevie Wonder as influences. Regarding surf rock pioneer Dick Dale, Brian clarified that his influence on the group was limited to Carl and his style of guitar playing. Carl himself named Berry, the Ventures, and John Walker for shaping his guitar style, and that the Beach Boys had learned to play all of the Ventures' songs by ear early in their career. On Jimi Hendrix and "heavy" music, Brian said he felt no pressure to go in that direction: "We never got into the heavy musical level trip. We never needed to. It's already been done."
The influence of the Beach Boys' peers combined with Brian's competitive nature drove him to reach higher creative peaks. Sometime around late 1963, he heard the song "Be My Baby" (1963) by the Ronettes for the first time, revamping his creative interests and songwriting. "Be My Baby" is considered the epitome of Phil Spector's Wall of Sound production technique, a recording method that fascinated Wilson for the next several decades. Brian later reflected: "I was unable to really think as a producer up until the time where I really got familiar with Phil Spector's work. That was when I started to design the experience to be a record rather than just a song." Other prominent inspirations for Brian included Gershwin's "Rhapsody in Blue" (1924), the Beatles' "Rubber Soul" (1965), and composer Burt Bacharach. Brian is quoted: "Burt Bacharach and Hal David are more like me. They’re also the best pop team – per se – today. As a producer, Bacharach has a very fresh, new approach."
Experimentation with psychotropic substances proved pivotal to the group's development as artists. In December 1964, Brian was introduced to cannabis before quickly progressing to LSD in early 1965. Of his first acid trip, Brian recalled that the drug had subjected him to "a very religious experience" which enlightened him to indescribable philosophies. The music for "California Girls" (the first Beach Boys song Bruce Johnston participated in) came from this first LSD experience, as did much of the group's subsequent work where they would partake in drug use during recording sessions.
Spirituality.
The band members often reflected on the spiritual nature of their music (and music in general), particularly for the recording of "Pet Sounds" and "Smile". Even though the Wilson family did not grow up in a particularly religious household, Carl was described as "the most truly religious person I know" by Brian, and Carl was forthcoming about the group's spiritual beliefs stating: "We believe in God as a kind of universal consciousness. God is love. God is you. God is me. God is everything right here in this room. It's a spiritual concept which inspires a great deal of our music." Carl told "Rave" magazine in 1967 that the group's influences are of a "religious nature", but not any religion in specific, only "an idea based upon that of Universal Consciousness. ... The spiritual concept of happiness and doing good to others is extremely important to the lyric of our songs, and the religious element of some of the better church music is also contained within some of our new work."
Brian is quoted during the "Smile" era: "I'm very religious. Not in the sense of churches, going to church; but like the essence of "all" religion." During the recording of "Pet Sounds", Brian held prayer meetings, later reflecting that "God was with us the whole time we were doing album ... I could feel that feeling in my brain." In 1966, he explained that he wanted to move into a white spiritual sound, and predicted that the rest of the music industry would follow suit. In 2011, Brian maintained the spirituality was important to his music, and that he did not follow any particular religion.
The Beach Boys included an interpretation of "The Lord's Prayer" as the B-side to their 1963 "Little Saint Nick" single. Brian expressed apprehensiveness over naming his song "God Only Knows" because, in the 1960s, references to God in pop music were largely unheard of. Carl said that "Smile" was chosen as an album title because of its connection to the group's spiritual beliefs. Brian referred to "Smile" as his "teenage symphony to God", composing a hymn, "Our Prayer", as the album's opening spiritual invocation. He spoke of his LSD trips as a "religious experience", and during a session for "Our Prayer", Brian can be heard asking the other Beach Boys: "Do you guys feel any acid yet?". In 1968, Mike Love's interest in transcendental meditation led the Beach Boys to record the original song "Transcendental Meditation".
Vocal ability.
Brian identified each member individually for their vocal range, once detailing the ranges for Carl, Dennis, Jardine (" progress upwards through G, A, and B"), Love ("can go from bass to the E above middle C"), and himself ("I can take the second D in the treble clef"). He declared in 1966 that his greatest interest was to expand modern vocal harmony, owing his fascination with voice to the Four Freshmen, which he considered a "groovy sectional sound." He added, "The harmonies that we are able to produce give us a uniqueness which is really the only important thing you can put into records – some quality that no one else has got. I love peaks in a song – and enhancing them on the control panel. Most of all, I love the human voice for its own sake." For a period, Brian avoided singing falsetto for the group, saying "I thought people thought I was a fairy. ... The band told me, 'If that's the way you sing, don't worry about it.'"
Rock critic Erik Davis wrote, "The 'purity' of tone and genetic proximity that smoothed their voices was almost creepy, pseudo-castrato, a 'barbershop' sound." According to Brian: "Jack Good once told us, 'You sing like eunuchs in a Sistine Chapel,' which was a pretty good quote." Writer Richard Goldstein reported that, according to a fellow journalist who asked Brian about the black roots of his music, Brian's response was: "We're white and we sing white." Goldstein added that when he asked where his approach to vocal harmonies had derived from, Wilson answered: "Barbershop." On the group's blend, Carl said: "Michael has a beautifully rich, very full-sounding bass voice. Yet his lead singing is real nasal, real punk. Alan’s voice has a bright timbre to it; it really cuts. My voice has a kind of calm sound. We’re big oooh-ers; we love to oooh. It’s a big, full sound, that’s very pleasing to us; it opens up the heart."
From lowest intervals to highest, the group's vocal harmony stack usually began with Love or Dennis, followed by Jardine or Carl, and finally Brian on top, according to Jardine, while Carl said that the blend was Love on bottom, Carl above, followed by Dennis or Jardine, and then Brian on top. Jardine explains, "We always sang the same vocal intervals. ... As soon as we heard the chords on the piano we’d figure it out pretty easily. If there was a vocal move envisioned, he’d show that particular singer that move. We had somewhat photographic memory as far as the vocal parts were concerned so that [was never a problem for us."
Striving for absolute perfection, Brian's intricate vocal arrangements exercised the group's calculated blend of intonation, attack, phrasing, and expression. Sometimes, he would sing each vocal harmony part alone through multi-track tape. Jimmy Webb has said, "They used very little vibrato and sing in very straight tones. The voices all lie down beside each other very easily – there's no bumping between them because the pitch is very precise."
As instrumentalists.
The group's instrumental combo initially involved Brian on bass guitar and keyboards, Carl on guitar, and Dennis on drums. Nine months after forming, they acquired national success, and demand for their personal appearance skyrocketed. Biographer James Murphy said, "By most contemporary accounts, they were not a very good live band when they started. ... The Beach Boys learned to play as a band in front of live audiences", but asserted that they eventually became "one of the best and enduring live bands".
For the recording of the Beach Boys' instrumental tracks, Brian arranged many of his compositions for a conglomerate of session musicians informally known as the Wrecking Crew. Their assistance was needed because of the increasingly complicated nature of the material. As a result, a number of songs do not credit the Beach Boys as instrumentalists, but nearly invariably as lead, harmony, or backing vocalists. It's the belief of Richie Unterberger that, "Before session musicians took over most of the parts, the Beach Boys could play respectably gutsy surf rock as a self-contained unit."
Carl continued to play beside these musicians whenever he was available to attend sessions. In archivist Craig Slowinski's view, "One should not sell short Carl's own contributions; the youngest Wilson had developed as a musician sufficiently to play alongside the horde of high-dollar session pros that big brother was now bringing into the studio. Carl's guitar playing a key ingredient."
It is often erroneously stated that Dennis' drumming in the Beach Boys' recordings was filled in exclusively by studio musicians. His drumming is documented on a number of the group's singles, including "I Get Around", "Fun, Fun Fun", and "Don't Worry Baby".
Songwriting and production.
Brian's experiments with his Wollensak tape recorder provide early examples of his flair for exotica and unusual percussive patterns and arranging ideas that he would recycle in later prominent work. Through attending Phil Spector's sessions sporadically, Brian learned how to act as a producer for records while being educated on the Wall of Sound process. From then on, Brian received some production advice from Jan Berry. As they collaborated on several hit singles written and produced for other artists, they recorded what would later be regarded the California Sound. The positive commercial response to Brian's structurally irregular and harmonically varied pop compositions gave him the prestige, resources, and courage to further his creative aspirations. He proceeded to explore many unusual combinations of instruments while emphasizing inventive percussion and progressively ambitious lyricism.
Although he was often dubbed a perfectionist, Brian was an inexperienced musician, and his understanding was mostly self-taught. He handled most stages of the group's recording process from the beginning, despite Nik Venet being credited for producing their early recordings. At the lyric stage, Brian usually worked with bandmate Mike Love, whose assertive persona provided youthful swagger that contrasted Brian's explorations in romanticism and sensitivity. Luis Sanchez noted a pattern where Brian would spare surfing imagery when working with collaborators outside of his band's circle, in the examples "Lonely Sea" and "In My Room".
After 1967.
Foreshadowed by "Beach Boys' Party!" (1965), much of the group's recordings from 1967 to 1970 displayed sparse instrumentation, a more relaxed ensemble, and a seeming inattention to production quality. Brian briefly experimented with "musique concrete" and minimalist rock approaches to music before retreating to his home recording studio to record "manic" material in the 1970s, enacting syncopated exercises and counterpoints layered on jittery eighth note tone clusters and loping shuffle grooves. During the infancy of Brian's home studio, the group was forced to improvise many technical aspects of recording. In one instance, they used an empty swimming pool as an echo chamber.
When Brian abdicated from the group, the other members were forced to take a more active production role. This is believed to have faltered the quality of their music. Richie Unterberger believes that after the December 1967 release of "Wild Honey", "The Beach Boys were revealed as a group that, although capable of producing some fine and interesting music, were no longer innovators on the level of the Beatles and other figureheads." The album marked the beginning of Carl's increased role as producer, who described it as "music for Brian to cool out by", signaling a mellower approach that pervaded into the 1970s. In 1968, Dennis contributed original songs to "Friends", revealing himself as a broodingly soulful songwriter and singer, while Bruce Johnston devised a moody instrumental, "The Nearest Faraway Place", for "20/20" the following year.
"Sunflower" (1970) marked an end to the experimental songwriting and production phase initiated by "Smiley Smile" (1967). Of the albums between "Surf's Up" (1971) and "Holland" (1973), Daniel Harrison wrote that they "contain a mixture of middle-of-the-road music entirely consonant with pop style during the early 1970s with a few oddities that proved that the desire to push beyond conventional boundaries was not dead." While Harrison adamantly states "1974 is the year in which the Beach Boys ceased to be a rock 'n' roll act and became an oldies act," "Love You" (1977) is perceived by some as an oddity that sounds like no other record in their catalog with synthesizer-laden arrangements played almost entirely by Brian.
Legacy.
Cultural impact and influence.
Regarded by some critics as one of the greatest American rock groups and an important catalyst in the evolution of popular music, the Beach Boys are one of the most critically acclaimed, commercially successful, and widely influential bands of all time. The Beach Boys' sales estimates range from 100 to 350 million records worldwide, and have influenced artists spanning many genres and decades. The group's early songs made them major pop stars in the United States, the United Kingdom, Australia and other countries, having seven top 10 singles between April 1963 and November 1964. They were one of the few American bands formed prior to the 1964 British Invasion to continue their success. Among artists of the 1960s, they are one of few central figures in the histories of rock.
Awards and honors.
The group routinely appears in the upper reaches of ranked lists such as "The Top 1000 Albums of All Time." Many of the group's songs and albums including "The Beach Boys Today!" (1965), "Smiley Smile" (1967), "Sunflower" (1970), and "Surf's Up" (1971) are featured in several lists devoted to the greatest of all time. The 1966 releases "Pet Sounds" and "Good Vibrations" frequently rank among the top of critics' lists of the greatest albums and singles of all time. In 2004, "Pet Sounds" was preserved in the National Recording Registry by the Library of Congress for being "culturally, historically, and aesthetically significant." Their recordings of "In My Room", "Good Vibrations", "California Girls" and the entire "Pet Sounds" album have been inducted into the Grammy Hall of Fame. On Acclaimed Music, "Good Vibrations" is ranked the third best song of all time, while "God Only Knows" is ranked twenty-first; the group itself is ranked eleven in its 1000 most recommended artists of all time.
In 1966 and 1967, reader polls conducted by the UK magazine "NME" crowned the Beach Boys as the world's number one vocal group, ahead of the Beatles and the Rolling Stones. In 1974, the Beach Boys were awarded "Band of the Year" by Rolling Stone. On December 30, 1980, the Beach Boys were awarded a star on the Hollywood Walk of Fame, located at 1500 Vine Street. The group was inducted into the Rock and Roll Hall of Fame in 1988. Ten years later they were selected for the Vocal Group Hall of Fame. In 2001, the group received a Grammy Lifetime Achievement Award. In 2004, "Rolling Stone" ranked the Beach Boys number 12 on its list of the 100 Greatest Artists of All Time. Brian Wilson was inducted into the UK Rock and Roll Hall of Fame in November 2006.
The Wilsons' California house, where the Wilson brothers grew up and the group began, was demolished in 1986 to make way for Interstate 105, the Century Freeway. A Beach Boys Historic Landmark (California Landmark No. 1041 at 3701 West 119th Street), dedicated on May 20, 2005, marks the location.
Selected filmography.
The Beach Boys appear as performers in the beach party films "The Girls on the Beach" (1965) and "The Monkey's Uncle" (1965). They have also made cameo appearances in the television series "Full House" (1988–1992), "Home Improvement" (1993), and "Baywatch" (1995).
The life of the Beach Boys is the subject of two made-for-television films: ' (1990) and ' (2000). "Love & Mercy" is a 2014 biopic that dramatizes Brian Wilson during his time with the Beach Boys.

</doc>
<doc id="4479" url="https://en.wikipedia.org/wiki?curid=4479" title="BCE (disambiguation)">
BCE (disambiguation)

BCE or B.C.E. may stand for:

</doc>
<doc id="4480" url="https://en.wikipedia.org/wiki?curid=4480" title="BC">
BC

BC may refer to:

</doc>
<doc id="4481" url="https://en.wikipedia.org/wiki?curid=4481" title="Beatrix Potter">
Beatrix Potter

Helen Beatrix Potter (28 July 186622 December 1943) was an English author, illustrator, natural scientist, and conservationist best known for her children's books featuring animals, such as those in "The Tale of Peter Rabbit".
Born into a privileged household, Potter was educated by governesses and grew up isolated from other children. She had numerous pets and spent holidays in Scotland and the Lake District, developing a love of landscape, flora and fauna, all of which she closely observed and painted. Though Potter was typical of women of her generation in having limited opportunities for higher education, her study and watercolors of fungi led to her being widely respected in the field of mycology. In her thirties, Potter published the highly successful children's book, "The Tale of Peter Rabbit". Potter began writing and illustrating children's books full-time.
With the proceeds from the books and a legacy from an aunt, in 1905 Potter bought Hill Top Farm in Near Sawrey, a village in the Lake District, which at that time was in Lancashire. Over the following decades, she purchased additional farms to preserve the unique hill country landscape. In 1913, at the age of 47, she married William Heelis, a respected local solicitor from Hawkshead. Potter was also a prize-winning breeder of Herdwick sheep and a prosperous farmer keenly interested in land preservation. She continued to write and illustrate, and to design spin-off merchandise based on her children's books for British publisher Warne, until the duties of land management and her diminishing eyesight made it difficult to continue.
Potter wrote about 30 books; the best known being her 24 children's tales. She died of pneumonia and heart disease on 22 December 1943 at her home in Near Sawrey at age 77, leaving almost all her property to the National Trust. She is credited with preserving much of the land that now constitutes the Lake District National Park. Potter's books continue to sell throughout the world in many languages with her stories being retold in song, film, ballet, and animation, and her life depicted in a feature film and television film.
Biography.
Early life.
Potter's paternal grandfather, Edmund Potter, from Glossop in Derbyshire, owned what was then the largest calico printing works in England, and later served as a Member of Parliament.
Beatrix's father, Rupert William Potter (1832–1914), was educated at Manchester College by the Unitarian philosopher Dr. James Martineau, an ancestor of Catherine, Duchess of Cambridge. He then trained as a barrister in London. Rupert practised law, specialising in equity law and conveyancing. He married Helen Leech (1839–1932) on 8 August 1863 at Hyde Unitarian Chapel, Gee Cross. Helen was the daughter of Jane Ashton (1806-1884) and John Leech, a wealthy cotton merchant and shipbuilder from Stalybridge. Helen's first cousin was Harriet Lupton ("née" Ashton) - the sister of Thomas Ashton, 1st Baron Ashton of Hyde. It was reported in July 2014 that Beatrix had personally given a number of her own original hand-painted illustrations to the two daughters of Dr Arthur and Harriet Lupton, who were blood cousins to both Beatrix and the Duchess of Cambridge.
Beatrix's parents lived comfortably at 2 Bolton Gardens, West Brompton, where Helen Beatrix was born on 28 July 1866 and her brother Walter Bertram on 14 March 1872. Both parents were artistically talented, and Rupert was an adept amateur photographer. Rupert had invested in the stock market and by the early 1890s was extremely wealthy.
Potter's family on both sides were from the Manchester area. They were English Unitarians, a dissenting Protestant sect who rejected the doctrine of the Trinity.
Beatrix was educated by three able governesses, the last of whom was Annie Moore ("née" Carter), just three years older than Beatrix, who tutored Beatrix in German as well as acting as lady's companion. She and Beatrix remained friends throughout their lives and Annie's eight children were the recipients of many of Potter's delightful picture letters. It was Annie who later suggested that these letters might make good children’s books.
She and her younger brother Walter Bertram (1872–1918) grew up with few friends outside their large extended family. Her parents were artistic, interested in nature, and enjoyed the countryside. As children, Beatrix and Bertram had numerous small animals as pets which they observed closely and drew endlessly. In their school room, Beatrix and Bertram kept a variety of small pets, mice, rabbits, a hedgehog and some bats, along with collections of butterflies and other insects which they drew and studied. Beatrix was devoted to the care of her small animals, often taking them with her on long holidays. In most of the first fifteen years of her life, Beatrix spent summer holidays at Dalguise, an estate on the River Tay in Perthshire, Scotland. There she sketched and explored an area that nourished her imagination and her observation. Beatrix and her brother were allowed great freedom in the country and both children became adept students of natural history. In 1887, when Dalguise was no longer available, the Potters took their first summer holiday in the Lake District, at Wray Castle near Lake Windermere. Here Beatrix met Hardwicke Rawnsley, vicar of Wray and later the founding secretary of the National Trust, whose interest in the countryside and country life inspired the same in Beatrix and who was to have a lasting impact on her life.
At about the age of 14, Beatrix began to keep a diary. It was written in a code of her own devising which was a simple letter for letter substitution. Her "Journal" was important to the development of her creativity, serving as both sketchbook and literary experiment: in tiny handwriting she reported on society, recorded her impressions of art and artists, recounted stories and observed life around her. The "Journal", decoded and transcribed by Leslie Linder in 1958, does not provide an intimate record of her personal life, but it is an invaluable source for understanding a vibrant part of British society in the late 19th century. It describes Potter's maturing artistic and intellectual interests, her often amusing insights on the places she visited, and her unusual ability to observe nature and to describe it. Started in 1881, her journal ends in 1897 when her artistic and intellectual energies were absorbed in scientific study and in efforts to publish her drawings. Precocious but reserved and often bored, she was searching for more independent activities and wished to earn some money of her own whilst dutifully taking care of her parents, dealing with her especially demanding mother, and managing their various households.
Scientific illustrations and work in mycology.
Beatrix Potter's parents did not discourage higher education. As was common in the Victorian era, women of her class were privately educated and rarely went to university.
Beatrix Potter was interested in every branch of natural science save astronomy. Botany was a passion for most Victorians and nature study was a popular enthusiasm. Potter was eclectic in her tastes: collecting fossils, studying archeological artefacts from London excavations, and interested in entomology. In all these areas she drew and painted her specimens with increasing skill. By the 1890s her scientific interests centred on mycology. First drawn to fungi because of their colours and evanescence in nature and her delight in painting them, her interest deepened after meeting Charles McIntosh, a revered naturalist and amateur mycologist, during a summer holiday in Dunkeld in Perthshire in 1892. He helped improve the accuracy of her illustrations, taught her taxonomy, and supplied her with live specimens to paint during the winter. Curious as to how fungi reproduced, Potter began microscopic drawings of fungus spores (the agarics) and in 1895 developed a theory of their germination. Through the connections of her uncle Sir Henry Enfield Roscoe, a chemist and vice-chancellor of the University of London, she consulted with botanists at Kew Gardens, convincing George Massee of her ability to germinate spores and her theory of hybridisation. She did not believe in the theory of symbiosis proposed by Simon Schwendener, the German mycologist, as previously thought; rather she proposed a more independent process of reproduction.
Rebuffed by William Thiselton-Dyer, the Director at Kew, because of her sex and her amateur status, Beatrix wrote up her conclusions and submitted a paper, "On the Germination of the Spores of the Agaricineae", to the Linnean Society in 1897. It was introduced by Massee because, as a female, Potter could not attend proceedings or read her paper. She subsequently withdrew it, realising that some of her samples were contaminated, but continued her microscopic studies for several more years. Her paper has only recently been rediscovered, along with the rich, artistic illustrations and drawings that accompanied it. Her work is only now being properly evaluated. Potter later gave her other mycological and scientific drawings to the Armitt Museum and Library in Ambleside, where mycologists still refer to them to identify fungi. There is also a collection of her fungus paintings at the Perth Museum and Art Gallery in Perth, Scotland, donated by Charles McIntosh. In 1967, the mycologist W.P.K. Findlay included many of Potter's beautifully accurate fungus drawings in his "Wayside & Woodland Fungi", thereby fulfilling her desire to one day have her fungus drawings published in a book. In 1997, the Linnean Society issued a posthumous apology to Potter for the sexism displayed in its handling of her research.
Artistic and literary career.
Potter’s artistic and literary interests were deeply influenced by fairies, fairy tales and fantasy. She was a student of the classic fairy tales of Western Europe. As well as stories from the Old Testament, John Bunyan's "The Pilgrim's Progress" and Harriet Beecher Stowe's "Uncle Tom's Cabin", she grew up with "Aesop's Fables", the fairy tales of the Brothers Grimm and Hans Christian Andersen, Charles Kingsley's "The Water Babies", the folk tales and mythology of Scotland, the German Romantics, Shakespeare, and the romances of Sir Walter Scott. As a young child, before the age of eight, Edward Lear's "Book of Nonsense", including the much loved "The Owl and the Pussycat", and Lewis Carroll's "Alice in Wonderland" had made their impression, although she later said of "Alice" that she was more interested in Tenniel's illustrations than what they were about. The "Brer Rabbit" stories of Joel Chandler Harris had been family favourites, and she later studied his "Uncle Remus" stories and illustrated them. She studied book illustration from a young age and developed her own tastes, but the work of the picture book triumvirate Walter Crane, Kate Greenaway and Randolph Caldecott, the last an illustrator whose work was later collected by her father, was a great influence. When she started to illustrate, she chose first the traditional rhymes and stories, "Cinderella", "Sleeping Beauty", "Ali Baba and the Forty Thieves", "Puss-in-boots", and "Red Riding Hood". But most often her illustrations were fantasies featuring her own pets: mice, rabbits, kittens, and guinea pigs.
In her teenage years, Potter was a regular visitor to the art galleries of London, particularly enjoying the summer and winter exhibitions at the Royal Academy in London. Her "Journal" reveals her growing sophistication as a critic as well as the influence of her father's friend, the artist Sir John Everett Millais, who recognised Beatrix's talent of observation. Although Potter was aware of art and artistic trends, her drawing and her prose style were uniquely her own.
As a way to earn money in the 1890s, Beatrix and her brother began to print Christmas cards of their own design, as well as cards for special occasions. Mice and rabbits were the most frequent subject of her fantasy paintings. In 1890, the firm of Hildesheimer and Faulkner bought several of her drawings of her rabbit Benjamin Bunny to illustrate verses by Frederic Weatherly titled "A Happy Pair". In 1893, the same printer bought several more drawings for Weatherly's "Our Dear Relations", another book of rhymes, and the following year Potter sold a series of frog illustrations and verses for "Changing Pictures", a popular annual offered by the art publisher Ernest Nister. Potter was pleased by this success and determined to publish her own illustrated stories.
Whenever Potter went on holiday to the Lake District or Scotland, she sent letters to young friends, illustrating them with quick sketches. Many of these letters were written to the children of her former governess Annie Carter Moore, particularly to her eldest son Noel who was often ill. In September 1893,Potter was on holiday at Eastwood in Dunkeld, Perthshire. She had run out of things to say to Noel and so she told him a story about "four little rabbits whose names were Flopsy, Mopsy, Cottontail and Peter". It became one of the most famous children's letters ever written and the basis of Potter's future career as a writer-artist-storyteller.
In 1900, Potter revised her tale about the four little rabbits, and fashioned a dummy book of it - it has been suggested, in imitation of Helen Bannerman's 1899 bestseller "The Story of Little Black Sambo". Unable to find a buyer for the work, she published it for family and friends at her own expense in December 1901. It was drawn in black and white with a coloured frontispiece. Family friend Canon Hardwicke Rawnsley had great faith in Potter's tale, recast it in didactic verse, and made the rounds of the London publishing houses. Frederick Warne & Co had previously rejected the tale but, eager to compete in the booming small format children's book market, reconsidered and accepted the "bunny book" (as the firm called it) following the recommendation of their prominent children's book artist L. Leslie Brooke. The firm declined Rawnsley's verse in favour of Potter's original prose, and Potter agreed to colour her pen and ink illustrations, choosing the then new Hentschel three-colour process to reproduce her watercolours.
On 2 October 1902, "The Tale of Peter Rabbit" was published, and was an immediate success. It was followed the next year by "The Tale of Squirrel Nutkin" and "The Tailor of Gloucester", which had also first been written as picture letters to the Moore children. Working with Norman Warne as her editor, Potter published two or three little books each year: 23 books in all. The last book in this format was "Cecily Parsley's Nursery Rhymes" in 1922, a collection of favourite rhymes. Although "The Tale of Little Pig Robinson" was not published until 1930, it had been written much earlier. Potter continued creating her little books until after the First World War, when her energies were increasingly directed toward her farming, sheep-breeding and land conservation.
The immense popularity of Potter's books was based on the lively quality of her illustrations, the non-didactic nature of her stories, the depiction of the rural countryside, and the imaginative qualities she lent to her animal characters.
Potter was also a canny businesswoman. As early as 1903, she made and patented a Peter Rabbit doll. It was followed by other "spin-off" merchandise over the years, including painting books, board games, wall-paper, figurines, baby blankets and china tea-sets. All were licensed by Frederick Warne & Co and earned Potter an independent income, as well as immense profits for her publisher.
In 1905, Potter and Norman Warne became unofficially engaged. Potter's parents objected to the match because Warne was "in trade" and thus not socially suitable. The engagement lasted only one month until Warne died of leukemia at age 37. That same year, Potter used some of her income and a small inheritance from an aunt to buy Hill Top Farm in Near Sawrey in the English Lake District near Windermere. Potter and Warne may have hoped that Hill Top Farm would be their holiday home, but after Warne's death, Potter went ahead with its purchase as she had always wanted to own that farm, and live in "that charming village".
Country life.
The tenant farmer John Cannon and his family agreed to stay on to manage the farm for her while she made physical improvements and learned the techniques of fell farming and of raising livestock, including pigs, cows and chickens; the following year she added sheep. Realising she needed to protect her boundaries, she sought advice from W.H. Heelis & Son, a local firm of solicitors with offices in nearby Hawkshead. With William Heelis acting for her she bought contiguous pasture, and in 1909 the Castle Farm across the road from Hill Top Farm. She visited Hill Top at every opportunity, and her books written during this period (such as "The Tale of Ginger and Pickles", about the local shop in Near Sawrey and "The Tale of Mrs. Tittlemouse", a wood mouse) reflect her increasing participation in village life and her delight in country living.
Owning and managing these working farms required routine collaboration with the widely respected William Heelis. By the summer of 1912 Heelis had proposed marriage and Beatrix had accepted; although she did not immediately tell her parents, who once again disapproved because Heelis was only a country solicitor. Potter and Heelis were married on 15 October 1913 in London at St Mary Abbots in Kensington. The couple moved immediately to Near Sawrey, residing at Castle Cottage, the renovated farm house on Castle Farm, which was 34 acres large. Hill Top remained a working farm but was now remodelled to allow for the tenant family and Potter's private studio and workshop. At last her own woman, Potter settled into the partnerships that shaped the rest of her life: her country solicitor husband and his large family, her farms, the Sawrey community and the predictable rounds of country life. "The Tale of Jemima Puddle-Duck" and "The Tale of Tom Kitten" are representative of Hill Top Farm and of her farming life, and reflect her happiness with her country life.
Rupert Potter died in 1914 and, with the outbreak of WW1, Potter, now a wealthy woman, persuaded her mother to move to the Lake District and found a property for her to rent in Sawrey. Finding life in Sawrey dull, Helen Potter soon moved to Lindeth Howe (now a 34 bedroomed country house hotel) a large house the Potters had previously rented for the summer in Bowness, on the other side of Lake Windermere, Potter continued to write stories for Frederick Warne & Co and fully participated in country life. She established a Nursing Trust for local villages, and served on various committees and councils responsible for footpaths and other rural issues.
Sheep farming.
Beatrix Potter Heelis became keenly interested in the breeding and raising of Herdwick sheep, the indigenous fell sheep, soon after acquiring Hill Top Farm. In 1923 she bought a former deer park and vast sheep farm in the Troutbeck Valley called Troutbeck Park Farm, restoring its land with thousands of Herdwick sheep. This established her as one of the major Herdwick sheep farmers in the area. She was admired by her shepherds and farm managers for her willingness to experiment with the latest biological remedies for the common diseases of sheep, and for her employment of the best shepherds, sheep breeders, and farm managers.
By the late 1920s Potter and her Hill Top farm manager Tom Storey had made a name for their prize-winning Herdwick flock, for which she won many prizes at the local agricultural shows, where she was also often asked to serve as a judge. In 1942 she was named President-elect of The Herdwick Sheepbreeders’ Association, the first time a woman had ever been elected to that office, but died before taking office.
Lake District conservation.
Potter had been a disciple of the land conservation and preservation ideals of her long-time friend and mentor, Canon Hardwicke Rawnsley, the first secretary and founding member of the National Trust for Places of Historic Interest or Natural Beauty. She supported the efforts of the National Trust to preserve not just the places of extraordinary beauty but also those heads of valleys and low grazing lands that would be irreparably ruined by development. She was also an authority on the traditional Lakeland crafts, period furniture and stonework. She restored and preserved the farms that she bought or managed, making sure that each farm house had in it a piece of antique Lakeland furniture. Potter was interested in preserving not only the Herdwick sheep, but also the way of life of fell farming. In 1930 the Heelises became partners with the National Trust in buying and managing the fell farms included in the large Monk Coniston Estate. The estate was composed of many farms spread over a wide area of north-western Lancashire, including the Tarn Hows. Potter was the "de facto" estate manager for the Trust for seven years until the National Trust could afford to buy most of the property back from her. Her stewardship of these farms earned her wide regard, but she was not without her critics, not the least of which were her contemporaries who felt she used her wealth and the position of her husband to acquire properties in advance of their being made public. She was notable in observing the problems of afforestation, preserving the intake grazing lands, and husbanding the quarries and timber on these farms. All her farms were stocked with Herdwick sheep and frequently with Galloway cattle.
Later life.
Potter continued to write stories and to draw, although mostly for her own pleasure. Her books in the late 1920s included the semi-autobiographical "The Fairy Caravan", a fanciful tale set in her beloved Troutbeck fells. It was published only in the US during Potter's lifetime, and not until 1952 in the UK. "Sister Anne", Potter's version of the story of Bluebeard, was written especially for her American readers, but illustrated by Katharine Sturges. A final folktale, "Wag by Wall", was published posthumously by "The Horn Book Magazine" in 1944. Potter was a generous patron of the Girl Guides, whose troupes she allowed to make their summer encampments on her land, and whose company she enjoyed as an older woman.
Potter and William Heelis enjoyed a happy marriage of thirty years, continuing their farming and preservation efforts throughout the hard days of the Second World War. Although they were childless, Potter played an important role in William’s large family, particularly enjoying her relationship with several nieces whom she helped educate, and giving comfort and aid to her husband’s brothers and sisters.
Potter died of complications from pneumonia and heart disease on 22 December 1943 at Castle Cottage, and her remains were cremated at Carleton Crematorium. She left nearly all her property to the National Trust, including over of land, sixteen farms, cottages and herds of cattle and Herdwick sheep. Hers was the largest gift at that time to the National Trust, and it enabled the preservation of the land now included in the Lake District National Park and the continuation of fell farming. The central office of the National Trust in Swindon was named "Heelis" in 2005 in her memory. William Heelis continued his stewardship of their properties and of her literary and artistic work for the eighteen months he survived her. When he died in August 1945 he left the remainder to the National Trust.
Legacies.
Potter left almost all the original illustrations for her books to the National Trust. The copyright to her stories and merchandise was then given to her publisher Frederick Warne & Co, now a division of the Penguin Group. On 1 January 2014, the copyright expired in the UK and other countries with a 70-years-after-death limit. Hill Top Farm was opened to the public by the National Trust in 1946; her artwork was displayed there until 1985 when it was moved to William Heelis’s former law offices in Hawkshead, also owned by the National Trust as the Beatrix Potter Gallery.
Potter gave her folios of mycological drawings to the Armitt Library and Museum in Ambleside before her death. "The Tale of Peter Rabbit" is owned by Frederick Warne and Company, "The Tailor of Gloucester" by the Tate Gallery and "The Tale of the Flopsy Bunnies" by the British Museum.
The largest public collection of her letters and drawings is the Leslie Linder Bequest and Leslie Linder Collection at the Victoria and Albert Museum in London. In the United States, the largest public collections are those in the Special Collections of the Free Library of Philadelphia, and the Lloyd Cotsen Children’s Library at Princeton University.
In 2015 a manuscript for an unpublished book was discovered by Jo Hanks, a publisher at Penguin Random House Children's Books, in the Victoria and Albert Museum archive. The book "The Tale of Kitty-in-Boots", with illustrations by Quentin Blake, is to be published September 1, 2016, to mark the 150th anniversary of Potter's birth.
Themes.
There are many interpretations of Potter’s literary work, the sources of her art, and her life and times. These include critical evaluations of her corpus of children's literature, and Modernist interpretations of Humphrey Carpenter and Katherine Chandler. Judy Taylor, "That Naughty Rabbit: Beatrix Potter and Peter Rabbit" (rev. 2002) tells the story of the first publication and many editions.
Potter’s country life and her farming has also been widely discussed in the work of Susan Denyer and by other authors in the publications of The National Trust.
Potter's work as a scientific illustrator and her work in mycology is highlighted in several chapters in Linda Lear, "Beatrix Potter: A Life in Nature", 2007; "Beatrix Potter: The Extraordinary Life of a Victorian Genius". 2008, UK.
Adaptations and fictionalisations.
In 1971, a ballet film was released, "The Tales of Beatrix Potter", directed by Reginald Mills, set to music by John Lanchbery with choreography by Frederick Ashton, and performed in character costume by members of the Royal Ballet and the Royal Opera House orchestra. The ballet of the same name has been performed by other dance companies around the world.
In 1982, the BBC produced "The Tale of Beatrix Potter". This dramatisation of her life was written by John Hawkesworth, directed by Bill Hayes, and starred Holly Aird and Penelope Wilton as the young and adult Beatrix, respectively. "The World of Peter Rabbit and Friends", a TV series based on her stories, has been released on VHS by Pickwick Video and later Carlton Video.
In 2006, Chris Noonan directed "Miss Potter", a biopic of Potter’s life focusing on her early career and romance with her editor Norman Warne. Renée Zellweger and Ewan McGregor play the lead roles.
Potter is also featured in Susan Wittig Albert's series of light mysteries called The Cottage Tales of Beatrix Potter. The first of the eight-book series is "Tale of Hill Top Farm" (2004), which deals with Potter's life in the Lake District and the village of Near Sawrey between 1905 and 1913.
Publications.
The 24 Tales
Other books
Original manuscript from 1914, rediscovered in 2015.
External links.
→≠

</doc>
<doc id="4482" url="https://en.wikipedia.org/wiki?curid=4482" title="Liberal Party (UK)">
Liberal Party (UK)

The Liberal Party was a liberal political party which was one of the two major parties in the United Kingdom in the 19th and early 20th century.
The party arose from an alliance of Whigs and free-trade Peelites and Radicals in the 1850s. By the end of the nineteenth century, it had formed four governments under William Gladstone. Despite splitting over the issue of Irish Home Rule, the party returned to power in 1906 with a landslide victory and introduced the welfare reforms that created a basic British welfare state. H. H. Asquith was Liberal Prime Minister between 1908 and 1916, followed by David Lloyd George whose premiership lasted until 1922 when the coalition the party had formed with the Conservative Party in World War I came to an end.
By the end of the 1920s, the Labour Party had replaced the Liberals as the Tories' main rival. The party went into decline and by the 1950s won no more than six seats at general elections. Apart from notable by-election victories, the party's fortunes did not improve significantly until it formed the SDP–Liberal Alliance with the newly formed Social Democratic Party (SDP) in 1981. At the 1983 General Election, the Alliance won over a quarter of the vote, but only 23 of the 650 seats it contested. At the 1987 General Election, its vote fell below 23% and the Liberal and Social Democratic parties merged in 1988 to form the Liberal Democrats. A small Liberal Party was formed in 1989 by party members opposed to the merger.
Prominent intellectuals associated with the Liberal Party include the philosopher John Stuart Mill, the economist John Maynard Keynes and social planner William Beveridge.
History.
Origins.
The Liberal Party grew out of the Whigs, who had their origins in an aristocratic faction in the reign of Charles II, and the early 19th century Radicals. The Whigs were in favour of reducing the power of the Crown and increasing the power of Parliament. Although their motives in this were originally to gain more power for themselves, the more idealistic Whigs gradually came to support an expansion of democracy for its own sake. The great figures of reformist Whiggery were Charles James Fox (died 1806) and his disciple and successor Earl Grey. After decades in opposition, the Whigs returned to power under Grey in 1830 and carried the First Reform Act in 1832.
The Reform Act was the climax of Whiggism, but it also brought about the Whigs' demise. The admission of the middle classes to the franchise and to the House of Commons led eventually to the development of a systematic middle class liberalism and the end of Whiggery, although for many years reforming aristocrats held senior positions in the party. In the years after Grey's retirement, the party was led first by Lord Melbourne, a fairly traditional Whig, and then by Lord John Russell, the son of a Duke but a crusading radical, and by Lord Palmerston, a renegade Irish Tory and essentially a conservative, although capable of radical gestures.
As early as 1839 Russell had adopted the name of "Liberals", but in reality his party was a loose coalition of Whigs in the House of Lords and Radicals in the Commons. The leading Radicals were John Bright and Richard Cobden, who represented the manufacturing towns which had gained representation under the Reform Act. They favoured social reform, personal liberty, reducing the powers of the Crown and the Church of England (many of them were Nonconformists), avoidance of war and foreign alliances (which were bad for business), and above all free trade. For a century, free trade remained the one cause which could unite all Liberals.
In 1841 the Liberals lost office to the Conservatives under Sir Robert Peel, but their period in opposition was short, because the Conservatives split over the repeal of the Corn Laws, a free trade issue, and a faction known as the Peelites (but not Peel himself, who died soon after) defected to the Liberal side. This allowed ministries led by Russell, Palmerston, and the Peelite Lord Aberdeen to hold office for most of the 1850s and 1860s. A leading Peelite was William Ewart Gladstone, who was a reforming Chancellor of the Exchequer in most of these governments. The formal foundation of the Liberal Party is traditionally traced to 1859 and the formation of Palmerston's second government.
The Whig-Radical amalgam could not become a true modern political party, however, while it was dominated by aristocrats, and it was not until the departure of the "Two Terrible Old Men", Russell and Palmerston, that Gladstone could become the first leader of the modern Liberal Party. This was brought about by Palmerston's death in 1865 and Russell's retirement in 1868. After a brief Conservative government (during which the Second Reform Act was passed by agreement between the parties) Gladstone won a huge victory at the 1868 election and formed the first Liberal government. The establishment of the party as a national membership organisation came with the foundation of the National Liberal Federation in 1877.
John Stuart Mill was a Liberal MP from 1865 to 1868.
The Gladstonian era.
For the next thirty years Gladstone and Liberalism were synonymous. William Ewart Gladstone served as prime minister four times (1868–74, 1880–85, 1886, and 1892–94). His financial policies, based on the notion of balanced budgets, low taxes, and laissez-faire, were suited to a developing capitalist society, but they could not respond effectively as economic and social conditions changed. Called the "Grand Old Man" later in life, Gladstone was always a dynamic popular orator who appealed strongly to the working class and to the lower middle class. Deeply religious, Gladstone brought a new moral tone to politics, with his evangelical sensibility and his opposition to aristocracy. His moralism often angered his upper-class opponents (including Queen Victoria), and his heavy-handed control split the Liberal Party.
In foreign policy, Gladstone was in general against foreign entanglements, but he did not resist the realities of imperialism. For example, he approved of the occupation of Egypt by British forces in 1882. His goal was to create a European order based on co-operation rather than conflict and on mutual trust instead of rivalry and suspicion; the rule of law was to supplant the reign of force and self-interest. This Gladstonian concept of a harmonious Concert of Europe was opposed to and ultimately defeated by a Bismarckian system of manipulated alliances and antagonisms.
As prime minister 1868 to 1874, Gladstone headed a Liberal Party which was a coalition of Peelites like himself, Whigs, and Radicals; he was now a spokesman for "peace, economy and reform." One major achievement was the Elementary Education Act of 1870, which provided England with an adequate system of elementary schools for the first time. He also secured the abolition of the purchase of commissions in the army and of religious tests for admission to Oxford and Cambridge; the introduction of the secret ballot in elections; the legalization of trade unions; and the reorganization of the judiciary in the Judicature Act.
Regarding Ireland, the major Liberal achievements were land reform, where he ended centuries of landlord oppression, and the disestablishment of the (Anglican) Church of Ireland through the Irish Church Act 1869.
In the 1874 general election Gladstone was defeated by the Conservatives under Disraeli during a sharp economic recession. He formally resigned as Liberal leader and was succeeded by the Marquess of Hartington, but he soon changed his mind and returned to active politics. He strongly disagreed with Disraeli's pro-Ottoman foreign policy and in 1880 he conducted the first outdoor mass-election campaign in Britain, known as the Midlothian campaign. The Liberals won a large majority in the 1880 election. Hartington ceded his place and Gladstone resumed office.
Among the consequences of the Third Reform Act (1884–85) was the giving of the vote to the Catholic peasants in Ireland, and the consequent creation of an Irish Parliamentary Party led by Charles Stewart Parnell. In the 1885 general election this party won the balance of power in the House of Commons, and demanded Irish Home Rule as the price of support for a continued Gladstone ministry. Gladstone personally supported Home Rule, but a strong Liberal Unionist faction led by Joseph Chamberlain, along with the last of the Whigs, Hartington, opposed it. The Irish Home Rule bill gave all owners of Irish land a chance to sell to the state at a price equal to 20 years' purchase of the rents and allowing tenants to purchase the land. Irish nationalist reaction was mixed, Unionist opinion was hostile, and the election addresses during the 1886 election revealed English radicals to be against the bill also. Among the Liberal rank and file, several Gladstonian candidates disowned the bill, reflecting fears at the constituency level that the interests of the working people were being sacrificed to finance a rescue operation for the landed elite.
The result was a catastrophic split in the Liberal Party, and heavy defeat in the 1886 election at the hands of Lord Salisbury. There was a final weak Gladstone ministry in 1892, but it also was dependent on Irish support and failed to get Irish Home Rule through the House of Lords.
Gladstone finally retired in 1894, and his ineffectual successor, Lord Rosebery, led the party to another heavy defeat in the 1895 general election.
A major long-term consequence of the Third Reform Act was the rise of Lib-Lab candidates, in the absence of any committed Labour Party. The Act split all county constituencies (which were represented by multiple MPs) into single-member constituencies, roughly corresponding to population patterns. In areas with working class majorities, in particular coal-mining areas, Lib-Lab candidates were popular, and they received sponsorship and endorsement from trade unions. In the first election after the Act was passed (1885), thirteen were elected, up from two in 1874. The Third Reform Act also facilitated the demise of the Whig old guard: in two-member constituencies, it was common to pair a Whig and a radical under the Liberal banner. After the Third Reform Act, fewer Whigs were selected as candidates.
Historian Walter L. Arnstein, concludes:
The rise of New Liberalism.
The late nineteenth century saw the emergence of “New Liberalism” within the Liberal Party, which advocated state intervention as a means of guaranteeing freedom and removing obstacles to it such as poverty and unemployment. Contrasting Old Liberalism with New Liberalism, David Lloyd George noted in a 1908 speech that the old Liberals
“used the natural discontent of the people with the poverty and precariousness of the means of subsistence as a motive power to win for them a better, more influential, and more honourable status in the citizenship of their native land. The new Liberalism, while pursuing this great political ideal with unflinching energy, devotes a part of its endeavour also to the removing of the immediate causes of discontent. It is true that man cannot live by bread alone. It is equally true that a man cannot live without bread.”
The Liberal zenith.
The Liberals languished in opposition for a decade, while the coalition of Salisbury and Chamberlain held power. The 1890s were marred by infighting between the three principal successors to Gladstone, party leader William Harcourt, former Prime Minister Lord Rosebery, and Gladstone's personal secretary, John Morley. This intrigue finally led Harcourt and Morley to resign their positions in 1898 as they continued to be at loggerheads with Rosebery over Irish home rule and issues relating to imperialism. Replacing Harcourt as party leader was Sir Henry Campbell-Bannerman. Harcourt's resignation briefly muted the turmoil in the party, but the beginning of the Second Boer War soon nearly broke the party apart, with Rosebery and a circle of supporters including important future Liberal leaders H.H. Asquith, Edward Grey, and Richard Burdon Haldane forming a clique dubbed the "Liberal Imperialists" that supported the government in the prosecution of the war. On the other side, more radical members of the party formed a Pro-Boer faction that denounced the conflict and called for an immediate end to hostilities. Quickly rising to prominence among the Pro-Boers was David Lloyd George, a relatively new MP and a master of rhetoric, who took advantage of having a national stage to speak out on a controversial issue to make his name in the party. Harcourt and Morley also sided with this group, though with slightly different aims. Campbell-Bannerman tried to keep these forces together at the head of a moderate Liberal rump, but in 1901 he delivered a speech on the government's "methods of barbarism" in South Africa that pulled him further to the left and nearly tore the party in two. The party was saved after Salisbury's retirement in 1902 when his successor, Arthur Balfour, pushed a series of unpopular initiatives such as a new education bill and Joseph Chamberlain called for a new system of protectionist tariffs. Campbell-Bannerman was able to rally the party around the traditional liberal platform of free trade and land reform and led them to the greatest election victory in their history. This would prove the last time the Liberals won a majority in their own right.
Although he presided over a large majority, Sir Henry Campbell-Bannerman was overshadowed by his ministers, most notably H. H. Asquith at the Exchequer, Edward Grey at the Foreign Office, Richard Burdon Haldane at the War Office and David Lloyd George at the Board of Trade. An ill Campbell-Bannerman retired in 1908 and died later that year. He was succeeded by Asquith, who stepped up the government's radicalism. Lloyd George succeeded Asquith at the Exchequer, and was in turn succeeded at the Board of Trade by Winston Churchill, a recent defector from the Conservatives.
The General Election of 1906 also represented a shift to the Left by the Liberal Party. According to Rosemary Rees, almost half of the Liberal MPs elected in 1906 were supportive of the 'New Liberalism' (which advocated government action to improve people's lives),) while claims were made that “five-sixths of the Liberal party are left wing.” Other historians, however, have questioned the extent to which the Liberal Party experienced a leftward shift; according to Robert C. Self however, only between 50 and 60 Liberal MPs out of the 400 in the parliamentary party after 1906 were Social Radicals, with a core of 20 to 30. Nevertheless, important junior offices were held in the cabinet by what Duncan Tanner has termed "genuine New Liberals, Centrist reformers, and Fabian collectivists," and much legislation was pushed through by the Liberals in government. This included the regulation of working hours, National Insurance and welfare. It was at this time that a political battle over the so-called People's Budget resulted in the passage of an act ending the power of the House of Lords to block legislation. The cost was high, however, as the government was required by the king to call two general elections in 1910 to validate its position and ended up frittering away most of its large majority, being left once again dependent on the Irish Nationalists.
As a result, Asquith was forced to introduce a new third Home Rule bill in 1912. Since the House of Lords no longer had the power to block the bill, the Unionist's Ulster Volunteers led by Sir Edward Carson, launched a campaign of opposition that included the threat of armed resistance in Ulster and the threat of mass resignation of their commissions by army officers in Ireland in 1914 ("see Curragh Incident"). In their resistance to Home Rule the Ulster Protestants had the full support of the Conservatives, whose leader, Andrew Bonar Law, was of Ulster-Scots descent. The country seemed to be on the brink of civil war when the First World War broke out in August 1914. Historian George Dangerfield has argued that the multiplicity of crises in 1910 to 1914, before the war broke out, so weakened the Liberal coalition that it marked the "Strange Death of Liberal England." However, most historians date the collapse to the crisis of the First World War.
Decline.
The war struck at the heart of everything British Liberals believed in. The party divided over the distinctly illiberal policies that were introduced under its auspices, including conscription and the Defence of the Realm Act. Several Cabinet ministers resigned, and Asquith, the master of domestic politics, proved a poor war leader. Lloyd George and Churchill, however, were zealous supporters of the war, and gradually forced the old peace-orientated Liberals out. The poor British performance in the early months of the war forced Asquith to invite the Conservatives into a coalition (on 17 May 1915). This marked the end of the last all-Liberal government. This coalition fell apart at the end of 1916, when the Conservatives withdrew their support from Asquith and gave it instead to Lloyd George, who became Prime Minister at the head of a coalition government largely made up of Conservatives. Asquith and his followers moved to the opposition benches in Parliament and the Liberal Party was split once again.
Wilson argues that Lloyd George abandoned many liberal principles in his single-minded crusade to win the war at all costs. That brought him and like-minded Liberals into a coalition with the Conservatives, largely on the ground long occupied by Conservatives: they were not oriented toward world peace or liberal treatment of Germany, nor discomfited by aggressive and authoritarian measures of state power. More deadly to the future of the Party, says Wilson, was its repudiation by ideological Liberals, who decided sadly that it no longer represented their principles. Finally the presence of the vigorous new Labour Party on the left gave a new home to voters disenchanted with the Liberal Party.
In the 1918 general election Lloyd George, "the Man Who Won the War", led his coalition into another "khaki election", and won a sweeping victory over the Asquithian Liberals and the newly emerging Labour Party. Lloyd George and the Conservative leader Andrew Bonar Law wrote a joint letter of support to candidates to indicate they were considered the official Coalition candidates – this "coupon", as it became known, was issued against many sitting Liberal MPs, often to devastating effect, though not against Asquith himself. Asquith and most of his colleagues lost their seats. Lloyd George still claimed to be leading a Liberal government, but he was increasingly under the influence of the rejuvenated Conservative party. In 1922 the Conservative backbenchers rebelled against the continuation of the coalition, citing in particular the Chanak Crisis over Turkey and Lloyd George's corrupt sale of honours, amongst other grievances, and Lloyd George was forced to resign. The Conservatives came back to power under Bonar Law and then Stanley Baldwin.
At the 1922 and 1923 elections the Liberals won barely a third of the vote and only a quarter of the seats in the House of Commons, as many radical voters abandoned the divided Liberals and went over to Labour. In 1922 Labour became the official opposition. A reunion of the two warring factions took place in 1923 when the new Conservative Prime Minister Stanley Baldwin committed his party to protective tariffs, causing the Liberals to reunite in support of free trade. The party gained ground in the 1923 general election but ominously made most of its gains from Conservatives whilst losing ground to Labour – a sign of the party's direction for many years to come. The party remained the third largest in the House of Commons, but the Conservatives had lost their majority. There was much speculation and fear about the prospect of a Labour government, and comparatively little about a Liberal government, even though it could have plausibly presented an experienced team of ministers compared to Labour's almost complete lack of experience, as well as offering a middle ground that could obtain support from both Conservatives and Labour in crucial Commons divisions. But instead of trying to force the opportunity to form a Liberal government, Asquith decided instead to allow Labour the chance of office, in the belief that they would prove incompetent and this would set the stage for a revival of Liberal fortunes at Labour's expense. It was a fatal error.
Labour was determined to destroy the Liberals and become the sole party of the left. Ramsay MacDonald was forced into a snap election in 1924, and although his government was defeated, he achieved his objective of virtually wiping the Liberals out as many more radical voters now moved to Labour, whilst moderate middle-class Liberal voters concerned about socialism moved to the Conservatives. The Liberals were reduced to a mere forty seats in Parliament, only seven of which had been won against candidates from both parties; and none of these formed a coherent area of Liberal survival. The party seemed finished, and during this period some Liberals, such as Churchill, went over to the Conservatives, while others went over to Labour. (Several Labour ministers of later generations, such as Michael Foot and Tony Benn, were the sons of Liberal MPs.)
Asquith died in 1928 and the enigmatic figure of Lloyd George returned to the leadership and began a drive to produce coherent policies on many key issues of the day. In the 1929 general election he made a final bid to return the Liberals to the political mainstream, with an ambitious programme of state stimulation of the economy called "We Can Conquer Unemployment!", largely written for him by the Liberal economist John Maynard Keynes. The Liberals gained ground, but once again it was at the Conservatives' expense whilst also losing seats to Labour. Indeed, the urban areas of the country suffering heavily from unemployment, which might have been expected to respond the most to the radical economic policies of the Liberals, instead gave the party its worst results. By contrast most of the party's seats were won either due to the absence of a candidate from one of the other parties or in rural areas on the "Celtic fringe", where local evidence suggests that economic ideas were at best peripheral to the electorate's concerns. The Liberals now found themselves with 59 members, holding the balance of power in a Parliament where Labour was the largest party but lacked an overall majority. Lloyd George offered a degree of support to the Labour government in the hope of winning concessions, including a degree of electoral reform to introduce the alternative vote, but this support was to prove bitterly divisive as the Liberals increasingly divided between those seeking to gain what Liberal goals they could achieve, those who preferred a Conservative government to a Labour one and vice versa.
The last majority Liberal Government in Britain was elected in 1906.
The years preceding the First World War were marked by worker strikes and civil unrest and saw many violent confrontations between civilians and the police and armed forces. Other issues of the period included women's suffrage and the Irish Home Rule movement.
After the carnage of 1914–1918, the democratic reforms of the Representation of the People Act 1918 instantly tripled the number of people entitled to vote in Britain from seven to twenty-one million. The Labour Party benefited most from this huge change in the electorate, forming its first minority government in 1924.
The splits over the National Government.
In 1931 MacDonald's government fell apart under the Great Depression, and the Liberals agreed to join his National Government, dominated by the Conservatives. Lloyd George himself was ill and did not actually join. Soon, however, the Liberals faced another divisive crisis when a National Government was proposed to fight the 1931 general election with a mandate for tariffs. From the outside, Lloyd George called for the party to abandon the government completely in defence of free trade, but only a few MPs and candidates followed. Another group under Sir John Simon then emerged, who were prepared to continue their support for the government and take the Liberal places in the Cabinet if there were resignations. The third group under Sir Herbert Samuel pressed for the parties in government to fight the election on separate platforms. In doing so the bulk of Liberals remained supporting the government, but two distinct Liberal groups had emerged within this bulk – the Liberal Nationals (officially the "National Liberals" after 1947) led by Simon, also known as "Simonites", and the "Samuelites" or "official Liberals", led by Samuel who remained as the official party. Both groups secured about 34 MPs but proceeded to diverge even further after the election, with the Liberal Nationals remaining supporters of the government throughout its life. There were to be a succession of discussions about them rejoining the Liberals, but these usually foundered on the issues of free trade and continued support for the National Government. The one significant reunification came in 1946 when the Liberal and Liberal National party organisations in London merged.
The official Liberals found themselves a tiny minority within a government committed to protectionism. Slowly they found this issue to be one they could not support. In early 1932 it was agreed to suspend the principle of collective responsibility to allow the Liberals to oppose the introduction of tariffs. Later in 1932 the Liberals resigned their ministerial posts over the introduction of the Ottawa Agreement on Imperial Preference. However, they remained sitting on the government benches supporting it in Parliament, though in the country local Liberal activists bitterly opposed the government. Finally in late 1933 the Liberals crossed the floor of the House of Commons and went into complete opposition. By this point their number of MPs was severely depleted. In the 1935 general election, just 17 Liberal MPs were elected, along with Lloyd George and three followers as "independent Liberals". Immediately after the election the two groups reunited, though Lloyd George declined to play much of a formal role in his old party. Over the next ten years there would be further defections as MPs deserted to either the Liberal Nationals or Labour. Yet there were a few recruits, such as Clement Davies, who had deserted to the National Liberals in 1931 but now returned to the party during the Second World War and who would lead it after the war.
Near extinction.
Samuel had lost his seat in the 1935 election and the leadership of the party fell to Sir Archibald Sinclair. With many traditional domestic Liberal policies now regarded as irrelevant, he focused the party on opposition to both the rise of Fascism in Europe and the appeasement foreign policy of the British government, arguing that intervention was needed, in contrast to the Labour calls for pacifism. Despite the party's weaknesses, Sinclair gained a high profile as he sought to recall the Midlothian Campaign and once more revitalise the Liberals as the party of a strong foreign policy.
In 1940 they joined Churchill's wartime coalition government, with Sinclair serving as Secretary of State for Air, the last British Liberal to hold Cabinet rank office for seventy years. However, it was a sign of the party's lack of importance that they were not included in the War Cabinet; some leading party members founded Radical Action, a group which called for liberal candidates to break the war-time electoral pact. At the 1945 general election, Sinclair and many of his colleagues lost their seats to both Conservatives and Labour, and the party returned just 12 MPs to Westminster. But this was just the beginning of the decline. In 1950, the general election saw the Liberals return just nine MPs. Another general election was called in 1951, and the Liberals were left with just six MPs; all but one of them were aided by the fact that the Conservatives refrained from fielding candidates in those constituencies.
In 1957 this total fell to five when one of the Liberal MPs died and the subsequent by-election was lost to the Labour Party, which selected the former Liberal Deputy Leader Lady Megan Lloyd George as its own candidate. The Liberal Party seemed close to extinction. During this low period, it was often joked that Liberal MPs could hold meetings in the back of one taxi.
Liberal revival.
Through the 1950s and into the 1960s the Liberals survived only because a handful of constituencies in rural Scotland and Wales clung to their Liberal traditions, whilst in two English towns, Bolton and Huddersfield, local Liberals and Conservatives agreed to each contest only one of the town's two seats. Jo Grimond, for example, who became Liberal leader in 1956, was MP for the remote Orkney and Shetland islands. Under his leadership a Liberal revival began, marked by the Orpington by-election of March 1962 which was won by Eric Lubbock. There, the Liberals won a seat in the London suburbs for the first time since 1935.
The Liberals became the first of the major British political parties to advocate British membership of the European Economic Community. Grimond also sought an intellectual revival of the party, seeking to position it as a non-socialist radical alternative to the Conservative government of the day. In particular he canvassed the support of the young post-war university students and recent graduates, appealing to younger voters in a way that many of his recent predecessors had not, and asserting a new strand of Liberalism for the post-war world.
The new middle-class suburban generation began to find the Liberals' policies attractive again. Under Grimond (who retired in 1967) and his successor, Jeremy Thorpe, the Liberals regained the status of a serious third force in British politics, polling up to 20% of the vote but unable to break the duopoly of Labour and Conservative and win more than fourteen seats in the Commons. An additional problem was competition in the Liberal heartlands in Scotland and Wales from the Scottish National Party and Plaid Cymru who both grew as electoral forces from the 1960s onwards. Although Emlyn Hooson held on to the seat of Montgomeryshire, upon Clement Davies death in 1962, the party lost five Welsh seats between 1950 and 1966. In September 1966 the Welsh Liberal Party formed their own state party, moving the Liberal Party into a fully federal structure.
In local elections Liverpool remained a Liberal stronghold, with the party taking the plurality of seats on the elections to the new Liverpool Metropolitan Borough Council in 1973. In the February 1974 general election the Conservative government of Edward Heath won a plurality of votes cast, but the Labour Party gained a plurality of seats due to the Ulster Unionist MPs refusing to support the Conservatives after the Northern Ireland Sunningdale Agreement. The Liberals now held the balance of power in the Commons. Conservatives offered Thorpe the Home Office if he would join a coalition government with Heath. Thorpe was personally in favour of it, but the party insisted on a clear government commitment to introducing proportional representation and a change of Prime Minister. The former was unacceptable to Heath's Cabinet and the latter to Heath personally, so the talks collapsed. Instead a minority Labour government was formed under Harold Wilson but with no formal support from Thorpe. In the October 1974 general election the Liberals slipped back slightly and the Labour government won a wafer-thin majority.
Thorpe was subsequently forced to resign after allegations about his private life. The party's new leader, David Steel, negotiated the Lib-Lab pact with Wilson's successor as Prime Minister, James Callaghan. According to this pact, the Liberals would support the government in crucial votes in exchange for some influence over policy. The agreement lasted from 1977 to 1978, but proved mostly fruitless, for two reasons: the Liberals' key demand of proportional representation was rejected by most Labour MPs, whilst the contacts between Liberal spokespersons and Labour ministers often proved detrimental, such as between finance spokesperson John Pardoe and Chancellor of the Exchequer Denis Healey, who were mutually antagonistic.
Alliance and Liberal Democrats.
The Conservative Party under the leadership of Margaret Thatcher won the 1979 general election, placing the Labour Party back in opposition, which served to push the Liberals back into the margins.
In 1981, defectors from a moderate faction of the Labour Party, led by former Cabinet ministers Roy Jenkins, David Owen and Shirley Williams, founded the Social Democratic Party (SDP). The new party and the Liberals quickly formed the SDP-Liberal Alliance, which for a while polled as high as 50% in the opinion polls and appeared capable of winning the next general election. Indeed, Steel was so confident of an Alliance victory that he told the 1981 Liberal conference, "Go back to your constituencies, and prepare for government!"
However, the Alliance was overtaken in the polls by the Tories in the aftermath of the Falkland Islands War and at the 1983 general election the Conservatives were re-elected by a landslide, with Labour once again forming the opposition. While the SDP-Liberal Alliance came close to Labour in terms of votes (a share of more than 25%), it only had 23 MPs compared to Labour's 209.
In the 1987 general election, the Alliance's share of the votes fell slightly and it now had 22 MPs. In the election's aftermath Steel proposed a merger of the two parties. Most SDP members voted in favour of the merger, but SDP leader David Owen objected and continued to lead a "rump" SDP.
In March 1988 the Liberal Party and Social Democratic Party merged to create the Social and Liberal Democrats, renamed the Liberal Democrats in October 1989. Over two-thirds of the members, and all the serving MPs, of the Liberal Party joined this party, led first jointly by Steel and the SDP leader Robert Maclennan.
A group of Liberal opponents of the merger with the Social Democrats, including Michael Meadowcroft (the former Liberal MP for Leeds West) and Paul Wiggin (who served on Peterborough City Council as a Liberal), continued with a new party organisation under the old name of "the Liberal Party". Meadowcroft joined the Liberal Democrats in 2007.
Ideology.
During the 19th century, the Liberal Party was broadly in favour of what would today be called classical liberalism: supporting "laissez-faire" economic policies such as free trade and minimal government interference in the economy (this doctrine was usually termed 'Gladstonian Liberalism' after the Victorian era Liberal Prime Minister William Ewart Gladstone). The Liberal Party favoured social reform, personal liberty, reducing the powers of the Crown and the Church of England (many of them were Nonconformists) and an extension of the electoral franchise. Sir William Harcourt, a prominent Liberal politician in the Victorian era, said this about liberalism in 1872:
If there be any party which is more pledged than another to resist a policy of restrictive legislation, having for its object social coercion, that party is the Liberal party. (Cheers.) But liberty does not consist in making others do what you think right, (Hear, hear.) The difference between a free Government and a Government which is not free is principally this—that a Government which is not free interferes with everything it can, and a free Government interferes with nothing except what it must. A despotic Government tries to make everybody do what it wishes; a Liberal Government tries, as far as the safety of society will permit, to allow everybody to do as he wishes. It has been the tradition of the Liberal party consistently to maintain the doctrine of individual liberty. It is because they have done so that England is the place where people can do more what they please than in any other country in the world...It is this practice of allowing one set of people to dictate to another set of people what they shall do, what they shall think, what they shall drink, when they shall go to bed, what they shall buy, and where they shall buy it, what wages they shall get and how they shall spend them, against which the Liberal party have always protested.
The political terms of "modern", "progressive" or "new" Liberalism began to appear in the mid to late 1880s and became increasingly common to denote the tendency in the Liberal Party to favour an increased role for the state as more important than the classical liberal stress on self-help and freedom of choice.
By the early 20th century the Liberals stance began to shift towards "New Liberalism", what would today be called social liberalism: a belief in personal liberty with a support for government intervention to provide minimum levels of welfare. This shift was best exemplified by the Liberal government of H. H. Asquith and his Chancellor David Lloyd George, whose Liberal reforms in the early 1900s created a basic welfare state.
David Lloyd George adopted a programme at the 1929 general election entitled "We Can Conquer Unemployment!", although by this stage the Liberals had declined to third-party status. The Liberals now (as expressed in the Liberal Yellow Book) regarded opposition to state intervention as being a characteristic of right-wing extremists.
After nearly becoming extinct in the 1940s and 50s, the Liberal Party revived its fortunes somewhat under the leadership of Jo Grimond in the 1960s, by positioning itself as a radical centrist non-socialist alternative to the Conservative and Labour Party governments of the time.
Religious alignment.
Since 1660, Nonconformist Protestants have played a major role in English politics. Relatively few MPs were dissenters. However the Dissenters were major voting bloc in many areas, such as East Midlands. They were very well organised and highly motivated and largely won over the Whigs and Liberals to their cause. Down to the 1830s, Dissenters demanded removal of political and civil disabilities that applied to them (especially those in the Test and Corporation Acts). The Anglican establishment strongly resisted until 1828. Numerous reforms of voting rights, especially that of 1832, increased the political power of Dissenters. They demanded an end to compulsory church rates, in which local taxes when only to Anglican churches. They finally achieve the end of religious tests for university degrees in 1905. Gladstone brought the majority of dissenters around to support for Home Rule for Ireland, putting the dissenting Protestants in league with the Irish Roman Catholics in an otherwise unlikely alliance. The dissenters gave significant support to moralistic issues, such as temperance and sabbath enforcement. The nonconformist conscience, as it was called, was repeatedly called upon by Gladstone for support for his moralistic foreign policy. In election after election, Protestant ministers rallied their congregations to the Liberal ticket. In Scotland, the Presbyterians played a similar role to the Nonconformist Methodists, Baptists and other groups in England and Wales.
The political strength of Dissent faded sharply after 1920 with the secularisation of British society in the 20th century. The rise of the Labour Party reduce the Liberal Party strongholds into the nonconformist and remote "Celtic Fringe," where the party survived by an emphasis on localism and historic religious identity, thereby neutralising much of the class pressure on behalf of the Labour movement. Meanwhile, the Anglican church was a bastion of strength for the Conservative party. On the Irish issue, the Anglicans strongly supported unionism. Increasingly after 1850, the Roman Catholic element in England and Scotland was composed of recent immigrants from Ireland. They voted largely for the Irish Parliamentary Party, until its collapse in 1918.
The different Nonconformists campaigned together against the Test and Corporation Acts that had been passed by Parliament in the 17th century. These Acts excluded Nonconformists from holding civil or military office. Attendance at an English university had required conformity to the Church of England before University College London (UCL) was founded, compelling Nonconformists to fund their own Dissenting Academies privately.
The Tories in the House of Commons tended to be in favour of these Acts and so the Nonconformist cause was linked closely to the Whigs, who advocated civil and religious liberty. After the Test and Corporation Acts were repealed in 1828, all the Nonconformists elected to Parliament were Liberals.
Nonconformists were angered by the Education Act 1902, which integrated denominational schools into the state system and provided for their support from taxes. John Clifford formed the National Passive Resistance Committee and by 1906 over 170 Nonconformists had gone to prison for refusing to pay school taxes. They included 60 Primitive Methodists, 48 Baptists, 40 Congregationalists, and 15 Wesleyan Methodists.

</doc>
<doc id="4484" url="https://en.wikipedia.org/wiki?curid=4484" title="Bank of England">
Bank of England

The Bank of England, formally the Governor and Company of the Bank of England, is the central bank of the United Kingdom and the model on which most modern central banks have been based. Established in 1694, it is the second oldest central bank in the world, after the Sveriges Riksbank, and the world's 8th oldest bank. It was established to act as the English Government's banker and is still the banker for the Government of the United Kingdom. The Bank was privately owned by stockholders from its foundation in 1694 until nationalised in 1946.
In 1998, it became an independent public organisation, wholly owned by the Treasury Solicitor on behalf of the government, with independence in setting monetary policy.
The Bank is one of eight banks authorised to issue banknotes in the United Kingdom, but has a monopoly on the issue of banknotes in England and Wales and regulates the issue of banknotes by commercial banks in Scotland and Northern Ireland.
The Bank's Monetary Policy Committee has devolved responsibility for managing monetary policy. The Treasury has reserve powers to give orders to the committee "if they are required in the public interest and by extreme economic circumstances" but such orders must be endorsed by Parliament within 28 days. The Bank's Financial Policy Committee held its first meeting in June 2011 as a macro prudential regulator to oversee regulation of the UK's financial sector.
The Bank's headquarters have been in London's main financial district, the City of London, on Threadneedle Street, since 1734. It is sometimes known by the metonym "The Old Lady of Threadneedle Street" or "The Old Lady", a name taken from the legend of Sarah Whitehead, whose ghost is said to haunt the Bank's garden. The busy road junction outside is known as Bank junction.
Mark Carney assumed the post of Governor of the Bank of England on 1 July 2013. He succeeded Mervyn King, who took over on 30 June 2003. Carney, a Canadian, will serve an initial five-year term rather than the typical eight, and will seek UK citizenship. He is the first non-British citizen to hold the post. As of January 2014, the Bank also has four Deputy Governors.
History.
Founding.
England's crushing defeat by France, the dominant naval power, in naval engagements culminating in the 1690 Battle of Beachy Head, became the catalyst for England's rebuilding itself as a global power. England had no choice but to build a powerful navy. No public funds were available, and the credit of William III's government was so low in London that it was impossible for it to borrow the £1,200,000 (at 8% p.a.) that the government wanted.
To induce subscription to the loan, the subscribers were to be incorporated by the name of the Governor and Company of the Bank of England. The Bank was given exclusive possession of the government's balances, and was the only limited-liability corporation allowed to issue bank notes. The lenders would give the government cash (bullion) and issue notes against the government bonds, which can be lent again. The £1.2m was raised in 12 days; half of this was used to rebuild the navy.
As a side effect, the huge industrial effort needed, including establishing ironworks to make more nails and advances in agriculture feeding the quadrupled strength of the navy, started to transform the economy. This helped the new Kingdom of Great Britain – England and Scotland were formally united in 1707 – to become powerful. The power of the navy made Britain the dominant world power in the late 18th and early 19th centuries.
The establishment of the bank was devised by Charles Montagu, 1st Earl of Halifax, in 1694, to the plan which had been proposed by William Paterson three years before, but not acted upon. He proposed a loan of £1.2m to the government; in return the subscribers would be incorporated as The Governor and Company of the Bank of England with long-term banking privileges including the issue of notes. The Royal Charter was granted on 27 July through the passage of the Tonnage Act 1694. Public finances were in so dire a condition at the time that the terms of the loan were that it was to be serviced at a rate of 8% per annum, and there was also a service charge of £4,000 per annum for the management of the loan. The first governor was Sir John Houblon, who is depicted in the £50 note issued in 1994. The charter was renewed in 1742, 1764, and 1781.
18th century.
The Bank's original home was in Walbrook, a street in the City of London, where during reconstruction in 1954 archaeologists found the remains of a Roman temple of Mithras (Mithras is – rather fittingly – said to have been worshipped "inter alia" as the God of Contracts); the Mithraeum ruins are perhaps the most famous of all 20th-century Roman discoveries in the City of London and can be viewed by the public.
The Bank moved to its current location in Threadneedle Street in 1734, and thereafter slowly acquired neighbouring land to create the edifice seen today. Sir Herbert Baker's rebuilding of the Bank, demolishing most of Sir John Soane's earlier building, was described by architectural historian Nikolaus Pevsner as "the greatest architectural crime, in the City of London, of the twentieth century".
When the idea and reality of the National Debt came about during the 18th century, this was also managed by the Bank. By the charter renewal in 1781 it was also the bankers' bank – keeping enough gold to pay its notes on demand until 26 February 1797 when war had so diminished gold reserves that - following an invasion scare caused by the Battle of Fishguard days earlier - the government prohibited the Bank from paying out in gold by the passing of the Bank Restriction Act 1797. This prohibition lasted until 1821.
19th century.
The 1844 Bank Charter Act tied the issue of notes to the gold reserves and gave the Bank sole rights with regard to the issue of banknotes. Private banks that had previously had that right retained it, provided that their headquarters were outside London and that they deposited security against the notes that they issued. A few English banks continued to issue their own notes until the last of them was taken over in the 1930s. Scottish and Northern Irish private banks still have that right.
The bank acted as lender of last resort for the first time in the panic of 1866.
The last private bank in England to issue its own notes was Thomas Fox's Fox, Fowler and Company bank in Wellington, which rapidly expanded, until it merged with Lloyds Bank in 1927. They were legal tender until 1964. There are nine notes left in circulation; one is housed at Tone Dale House Wellington.
20th century.
Britain remained on the gold standard until 1931 when the gold and foreign exchange reserves were transferred to the Treasury, but they continued to be managed by the Bank.
During the governorship of Montagu Norman, from 1920–44, the Bank made deliberate efforts to move away from commercial banking and become a central bank. In 1946, shortly after the end of Norman's tenure, the bank was nationalised by the Labour government.
After 1945 the Bank pursued the multiple goals of Keynesian economics, especially "easy money" and low interest rates to support aggregate demand. It tried to keep a fixed exchange rate, and attempted to deal with inflation and sterling weakness by credit and exchange controls.
In 1977, the Bank set up a wholly owned subsidiary called Bank of England Nominees Limited (BOEN), a private limited company, with two of its hundred £1 shares issued. According to its Memorandum & Articles of Association, its objectives are: "To act as Nominee or agent or attorney either solely or jointly with others, for any person or persons, partnership, company, corporation, government, state, organisation, sovereign, province, authority, or public body, or any group or association of them..." Bank of England Nominees Limited was granted an exemption by Edmund Dell, Secretary of State for Trade, from the disclosure requirements under Section 27(9) of the Companies Act 1976, because "it was considered undesirable that the disclosure requirements should apply to certain categories of shareholders." The Bank of England is also protected by its Royal Charter status, and the Official Secrets Act. BOEN is a vehicle for governments and heads of state to invest in UK companies (subject to approval from the Secretary of State), providing they undertake "not to influence the affairs of the company". BOEN is no longer exempt from company law disclosure requirements. Although a dormant company, dormancy does not preclude a company actively operating as a nominee shareholder. BOEN has two shareholders: the Bank of England, and the Secretary of the Bank of England.
In 1981 the reserve requirement for banks to hold a minimum fixed proportion of their deposits as reserves at the Bank of England was abolished: see reserve requirement#United Kingdom for more details.
On 6 May 1997, following the 1997 general election which brought a Labour government to power for the first time since 1979, it was announced by the Chancellor of the Exchequer, Gordon Brown, that the Bank would be granted operational independence over monetary policy. Under the terms of the Bank of England Act 1998 (which came into force on 1 June 1998), the Bank's Monetary Policy Committee was given sole responsibility for setting interest rates to meet the Government's Retail Prices Index (RPI) inflation target of 2.5%. The target has changed to 2% since the Consumer Price Index (CPI) replaced the Retail Prices Index as the Treasury's inflation index. If inflation overshoots or undershoots the target by more than 1%, the Governor has to write a letter to the Chancellor of the Exchequer explaining why, and how he will remedy the situation.
The handing over of monetary policy to the Bank had been a key plank of the Liberal Democrats' economic policy since the 1992 general election. Conservative MP Nicholas Budgen had also proposed this as a private member's bill in 1996, but the bill failed as it had the support of neither the government nor the opposition.
Functions of the Bank.
The Bank performs all the functions of a central bank. The most important of these are maintaining price stability, and supporting the economic policies of Her Majesty's Government, thus promoting economic growth. There are two main areas which are tackled by the Bank to ensure it carries out these functions efficiently:
Financial stability.
The Bank works together with other institutions to secure both monetary and financial stability, including:
The 1997 Memorandum of Understanding describes the terms under which the Bank, the Treasury and the FSA work toward the common aim of increased financial stability. In 2010 the incoming Chancellor announced his intention to merge the FSA back into the Bank. As of 2012, the current director for financial stability is Andy Haldane.
The Bank acts as the government's banker, and it maintains the government's Consolidated Fund account. It also manages the country's foreign exchange and gold reserves. The Bank also acts as the bankers' bank, especially in its capacity as a lender of last resort.
The Bank has a monopoly on the issue of banknotes in England and Wales. Scottish and Northern Irish banks retain the right to issue their own banknotes, but they must be backed one for one with deposits at the Bank, excepting a few million pounds representing the value of notes they had in circulation in 1845. The Bank decided to sell its banknote printing operations to De La Rue in December 2002, under the advice of Close Brothers Corporate Finance Ltd.
Since 1998, the Monetary Policy Committee (MPC) has had the responsibility for setting the official interest rate. However, with the decision to grant the Bank operational independence, responsibility for government debt management was transferred in 1998 to the new Debt Management Office, which also took over government cash management in 2000. Computershare took over as the registrar for UK Government bonds (gilt-edged securities or "gilts") from the Bank at the end of 2004.
The Bank used to be responsible for the regulation and supervision of the banking and insurance industries. This responsibility was transferred to the Financial Services Authority in June 1998, but after the financial crises in 2008 new banking legislation transferred the responsibility for regulation and supervision of the banking and insurance industries back to the Bank.
In 2011 the interim Financial Policy Committee (FPC) was created as a mirror committee to the MPC to spearhead the Bank's new mandate on financial stability. The FPC is responsible for macro prudential regulation of all UK banks and insurance companies.
To help maintain economic stability, the Bank attempts to broaden understanding of its role, both through regular speeches and publications by senior Bank figures, a semiannual Financial Stability Report, and through a wider education strategy aimed at the general public. It maintains a free museum and runs the Target Two Point Zero competition for A-level students.
Asset purchase facility.
The Bank has operated, since January 2009, an Asset Purchase Facility (APF) to buy "high-quality assets financed by the issue of Treasury bills and the DMO's cash management operations" and thereby improve liquidity in the credit markets. It has, since March 2009, also provided the mechanism by which the Bank's policy of quantitative easing (QE) is achieved, under the auspices of the MPC. Along with the managing the £200 billion of QE funds, the APF continues to operate its corporate facilities. Both are undertaken by a subsidiary company of the Bank of England, the Bank of England Asset Purchase Facility Fund Limited (BEAPFF).
Banknote issues.
The Bank has issued banknotes since 1694. Notes were originally hand-written; although they were partially printed from 1725 onwards, cashiers still had to sign each note and make them payable to someone. Notes were fully printed from 1855. Until 1928 all notes were "White Notes", printed in black and with a blank reverse. In the 18th and 19th centuries White Notes were issued in £1 and £2 denominations. During the 20th century White Notes were issued in denominations between £5 and £1000.
Until the mid-19th century, commercial banks were allowed to issue their own banknotes, and notes issued by provincial banking companies were commonly in circulation. The Bank Charter Act 1844 began the process of restricting note issue to the Bank; new banks were prohibited from issuing their own banknotes and existing note-issuing banks were not permitted to expand their issue. As provincial banking companies merged to form larger banks, they lost their right to issue notes, and the English private banknote eventually disappeared, leaving the Bank with a monopoly of note issue in England and Wales. The last private bank to issue its own banknotes in England and Wales was Fox, Fowler and Company in 1921. However, the limitations of the 1844 Act only affected banks in England and Wales, and today three commercial banks in Scotland and four in Northern Ireland continue to issue their own banknotes, regulated by the Bank.
At the start of the First World War, the Currency and Bank Notes Act 1914 was passed, which granted temporary powers to HM Treasury for issuing banknotes to the values of £1 and 10/- (ten shillings). Treasury notes had full legal tender status and were not convertible into gold through the Bank; they replaced the gold coin in circulation to prevent a run on sterling and to enable raw material purchases for armament production. These notes featured an image of King George V (Bank of England notes did not begin to display an image of the monarch until 1960). The wording on each note was:
Treasury notes were issued until 1928, when the Currency and Bank Notes Act 1928 returned note-issuing powers to the banks. The Bank of England issued notes for ten shillings and one pound for the first time on 22 November 1928.
During the Second World War the German Operation Bernhard attempted to counterfeit denominations between £5 and £50, producing 500,000 notes each month in 1943. The original plan was to parachute the money into the UK in an attempt to destabilise the British economy, but it was found more useful to use the notes to pay German agents operating throughout Europe. Although most fell into Allied hands at the end of the war, forgeries frequently appeared for years afterwards, which led banknote denominations above £5 to be removed from circulation.
In 2006, over £53 million in banknotes belonging to the Bank was stolen from a depot in Tonbridge, Kent.
Modern banknotes are printed by contract with De La Rue Currency in Loughton, Essex.
The Vault.
The Bank is custodian to the official gold reserves of the United Kingdom and many other countries. The vault, beneath the City of London, covers a floor space greater than that of the third-tallest building in the City, Tower 42, and needs keys that are three feet (90 cm) long to open. As at around 2011, the Bank was the 15th-largest custodian of gold reserves, holding around 4600 tonnes. These gold deposits were estimated in February 2012 to have a current market value of £156,000,000,000.
Governance of the Bank of England.
Governors.
Following is a list of the Governors of the Bank of England since the beginning of the 20th century:
Other staff.
Since 2013, the Bank has had a Chief Operating Officer (COO). , the Bank's COO has been Charlotte Hogg.
, the Bank's chief economist is Andrew Haldane.

</doc>
<doc id="4485" url="https://en.wikipedia.org/wiki?curid=4485" title="Bakelite">
Bakelite

Bakelite ( , sometimes spelled Baekelite), or polyoxybenzylmethylenglycolanhydride, is an early plastic. It is a thermosetting phenol formaldehyde resin, formed from a condensation reaction of phenol with formaldehyde. It was developed by the Belgian-American chemist Leo Baekeland in New York in 1907.
One of the first plastics made from synthetic components, Bakelite was used for its electrical nonconductivity and heat-resistant properties in electrical insulators, radio and telephone casings, and such diverse products as kitchenware, jewelry, pipe stems, children's toys, and firearms. The "retro" appeal of old Bakelite products has made them collectible.
Bakelite was designated a National Historic Chemical Landmark on November 9, 1993 by the American Chemical Society in recognition of its significance as the world's first synthetic plastic.
History.
Baekeland was already wealthy, due to his invention of Velox photographic paper, when he began to investigate the reactions of phenol and formaldehyde in his home laboratory. Chemists had begun to recognize that many natural resins and fibres were polymers. Baekeland's initial intent was to find a replacement for shellac, a material that was in limited supply because it was made naturally from the excretion of lac insects. Baekeland produced a soluble phenol-formaldehyde shellac called "Novolak", but it was not a market success.
Baekeland then began trying to strengthen wood by impregnating it with a synthetic resin, rather than coating it. By controlling the pressure and temperature applied to phenol and formaldehyde, Baekeland produced a hard moldable material which he named "Bakelite". The first synthetic thermosetting plastic ever made, Baekeland speculated on "the thousand and one... articles" that it could be used to make. Baekeland considered the possibilities of using a wide variety of filling materials, including cotton, powdered bronze, and slate dust, but was most successful with wood and asbestos fibers.
Baekeland filed a substantial number of patents in the area. His "Method of making insoluble products of phenol and formaldehyde" was filed on July 13, 1907 and granted on December 7, 1909. Baekeland also filed for patent protection in other countries, including Belgium, Canada, Denmark, Hungary, Japan, Mexico, Russia, and Spain. He announced his invention at a meeting of the American Chemical Society on February 5, 1909.
Baekeland started semi-commercial production of his new material in his home laboratory, marketing it as a material for electrical insulators. By 1910, he was producing enough material to justify expansion. He formed the General Bakelite Company as a U.S. company to manufacture and market his new industrial material. He also made overseas connections to produce materials in other countries.
Bijker gives a detailed discussion of the development of Bakelite and the Bakelite Company's production of various applications of materials. As of 1911, the company's main focus was laminating varnish, whose sales volume vastly outperformed both molding material and cast resin. By 1912, molding material was gaining ground, but its sales volume for the company did not exceed that of laminating varnish until the 1930s.
As the sales figures also show, the Bakelite Company produced "Transparent" cast resin (which did not include filler) for a small ongoing market during the 1910s and 1920s. Blocks or rods of cast resin, also known as "artificial amber", were machined and carved to create items such as pipe stems, cigarette holders and jewelry. However, the demand for molded plastics led the Bakelite Company to concentrate on molding, rather than developing market opportunities for cast resins.
The Bakelite Corporation was formed in 1922 (after patent litigation favorable to Baekeland) from a merger of three companies: Baekeland's General Bakelite Company; the Condensite Company, founded by J.W. Aylesworth; and the Redmanol Chemical Products Company, founded by L.V. Redman. Under director of advertising and public relations Allan Brown, who came to Bakelite from Condensite, Bakelite was aggressively marketed as "The Material of A Thousand Uses." A filing for a trademark featuring the letter B above the mathematical symbol for infinity was made August 25, 1925, and claimed the mark was in use as of December 1, 1924. A wide variety of uses were listed in their trademark applications.
The first issue of "Plastics" magazine, October 1925, featured Bakelite on its cover, and included the article "Bakelite – What It Is" by Allan Brown. The range of colors available included "black, brown, red, yellow, green, gray, blue, and blends of two or more of these". The article emphasized that Bakelite came in various forms. "Bakelite is manufactured in several forms to suit varying requirements. In all these forms the fundamental basis is the initial Bakelite resin. This variety includes Clear Material, for jewelry, smokers' articles, etc.; cement, using in sealing electric light bulbs in metal bases; varnishes, for impregnating electric coils, etc.; lacquers, for protecting the surface of hardware; enamels, for giving resistive coating to industrial equipment; Laminated Bakelite, used for silent gears and insulation; and molding material, from which are formed innumerable articles of utility and beauty. The Molding Material is prepared ordinarily by the impregnation of cellulose substances with the initial 'uncured' resin." In a 1925 report, the United States Tariff Commission hailed the commercial manufacture of synthetic phenolic resin as "distinctly an American achievement", and noted that "the publication of figures, however, would be a virtual disclosure of the production of an individual company."
In England Bakelite Limited, a merger of three British phenol formaldehyde resin suppliers (Damard Lacquer Company Limited of Birmingham, Mouldensite Limited of Darley Dale and Redmanol Chemical Products Company of London) was formed in 1926. A new Bakelite factory opened in Tyseley, Birmingham, England around 1928. It was demolished in 1998.
A new factory was opened in Bound Brook, New Jersey, in 1931. In 1939, the companies were acquired by Union Carbide and Carbon Corporation. Union Carbide's phenolic resin business including the Bakelite and Bakelit registered trademarks are owned by Momentive Specialty Chemicals.
In addition to the original Bakelite material, these companies eventually made a wide range of other products, many of which were marketed under the brand name "Bakelite plastics". These included other types of cast phenolic resins similar to Catalin, and urea-formaldehyde resins, which could be made in brighter colors than polyoxybenzylmethylenglycolanhydride.
Once Baekeland's heat and pressure patents expired in 1927, Bakelite Corporation faced serious competition from other companies. Because molded Bakelite incorporated fillers to give it strength, it tended to be made in concealing dark colors. In 1927, beads, bangles and earrings were produced by the Catalin Company, through a different process which enabled them to introduce 15 new colors. Translucent jewelry, poker chips and other items made of phenolic resins were introduced in the 1930s or 1940s by the Catalin Company under the Prystal name. The creation of marbled phenolic resins may also be attributable to the Catalin Company.
Synthesis.
Making Bakelite was a multi-stage process. It began with the heating of phenol and formaldehyde in the presence of a catalyst such as hydrochloric acid, zinc chloride, or the base ammonia. This created a liquid condensation product, referred to as "Bakelite A", which was soluble in alcohol, acetone, or additional phenol. Heated further, the product became partially soluble and could still be softened by heat. Sustained heating resulted in an "insoluble hard gum". However, the high temperatures required to create this tended to cause violent foaming of the mixture, which resulted in the cooled material being porous and breakable. Baekeland's innovative step was to put his "last condensation product" into an egg-shaped "Bakelizer". By heating it under pressure, at about 150 °C, Baekeland was able to suppress the foaming that would otherwise occur. The resulting substance was extremely hard and both infusable and insoluble. 
Compression molding.
Molded Bakelite forms in a condensation reaction of phenol and formaldehyde, with wood flour or asbestos fiber as a filler, under high pressure and heat in a time frame of a few minutes of curing. The result is a hard plastic material.
Bakelite's molding process had a number of advantages. Bakelite resin could be provided either as powder, or as preformed partially cured slugs, increasing the speed of the casting. Thermosetting resins such as Bakelite required heat and pressure during the molding cycle, but could be removed from the molding process without being cooled, again making the molding process faster. Also, because of the smooth polished surface that resulted, Bakelite objects required less finishing. Millions of parts could be duplicated quickly and relatively cheaply.
Phenolic sheet.
Another market for Bakelite resin was the creation of phenolic sheet materials. Phenolic sheet is a hard, dense material made by applying heat and pressure to layers of paper or glass cloth impregnated with synthetic resin. Cellulose paper, cotton fabrics, synthetic yarn fabrics, glass fabrics and unwoven fabrics are all possible materials used in lamination. When heat and pressure are applied, polymerization transforms the layers into thermosetting industrial laminated plastic.
Bakelite phenolic sheet is produced in dozens of commercial grades and with various additives to meet diverse mechanical, electrical and thermal requirements. Some common types include:
Properties.
Bakelite has a number of important properties. It can be molded very quickly, allowing identical units to be mass-produced. Moldings are smooth, retain their shape and are resistant to heat, scratches, and destructive solvents. It is also resistant to electricity, and prized for its low conductivity. It is not flexible.
Phenolic resin products may swell slightly under conditions of extreme humidity or perpetual dampness. When rubbed or burnt, Bakelite has a distinctive, acrid, sickly-sweet or fishy odor.
Applications and usage.
These characteristics made Bakelite particularly suitable as a molding compound, an adhesive or binding agent, a varnish, and as a protective coating. Bakelite was particularly suitable for the emerging electrical and automobile industries because of its extraordinarily high resistance to electricity, heat and chemical action.
The earliest commercial use of Bakelite in the electrical industry was the molding of tiny insulating bushings the size of mustard seeds, made in 1908 for the Weston Electrical Instrument Corporation by Richard W. Seabury of the Boonton Rubber Company. Bakelite was soon used for non-conducting parts of telephones, radios and other electrical devices, including bases and sockets for light bulbs and electron tubes, supports for any type of electrical components, automobile distributor caps and other insulators. 
By 1912, it was being used to make billiard balls, since its elasticity and the sound it made were similar to ivory.
During World War I, Bakelite was used widely for machine parts, particularly electrical systems. Important projects included the Liberty Motor, the wireless telephone and radio phone and the use of micarta-bakelite propellors in the NBS-1 bomber and the DH-4B aeroplane.
Bakelite's availability and ease and speed of molding helped to lower the costs of production and increase product availability so that both telephones and radios became common household consumer goods. It was also very important to the developing automobile industry. It was soon found in myriad other consumer products ranging from pipe stems and buttons to saxophone mouthpieces, cameras, early machine guns, and appliance casings.
Beginning in the 1920s, it became a popular material for jewelry. Designer Coco Chanel included Bakelite bracelets in her costume jewelry collections. Designers such as Elsa Schiaparelli used it for jewelry and also for specially designed dress buttons. Later, Diana Vreeland, editor of Vogue, was enthusiastic about Bakelite. Bakelite was also used to make presentation boxes for Breitling watches. Jewelry designers such as Jorge Caicedo Montes De Oca still use vintage Bakelite materials to make designer jewelry.
By 1930, designer Paul T. Frankl considered Bakelite a "Materia Nova", "expressive of our own age". By the 1930s, Bakelite was used for game pieces like chessmen, poker chips, dominoes and Mah Jong sets. Kitchenware made with Bakelite, including canisters and tableware, was promoted for its resistance to heat and to chipping. In the mid-30s, Northland marketed a line of skis with a black 'Ebonite' base, a coating of Bakelite. By 1935, it was used in solid-body electric guitars. Performers such as Jerry Byrd loved the tone of Bakelite guitars but found them difficult to keep in tune.
The British children's construction toy Bayko, launched in 1933, originally used Bakelite for many of its parts, and took its name from the material.
During World War II, Bakelite was used in a variety of wartime equipment including pilot goggles and field telephones. It was also used for patriotic wartime jewelry. In 1943, the thermosetting phenolic resin was even considered for the manufacture of coins, due to a shortage of traditional material. Bakelite and other non-metal materials were tested for usage for the one cent coin in the US before the Mint settled on zinc-coated steel.
In 1947, Dutch art forger Han van Meegeren was convicted of forgery, after chemist and curator Paul B. Coremans proved that a purported Vermeer contained Bakelite, which van Meegeren had used as a paint hardener.
Bakelite was sometimes used as a substitute for metal in the magazine, pistol grip, fore grip, hand guard, and butt stock of firearms. Some early AK-74 rifles have been identified as using Bakelite, but most were made with AG-S4, commonly mistaken for Bakelite.
By the late 1940s, newer materials were superseding Bakelite in many areas. Phenolics are less frequently used in general consumer products today due to their cost and complexity of production and their brittle nature. They still appear in some applications where their specific properties are required, such as small precision-shaped components, molded disc brake cylinders, saucepan handles, electrical plugs, switches and parts for electrical irons., as well as in the area of inexpensive board and tabletop games produced in China, Hong Kong and India. Items such as billiard balls, dominoes and pieces for board games like chess, checkers, and backgammon are constructed of Bakelite for its look, durability, fine polish, weight, and sound. Common dice are sometimes made of Bakelite for weight and sound, but the majority are made of a thermoplastic polymer such as acrylonitrile butadiene styrene (ABS).
Bakelite continues to be used for wire insulation, brake pads and related automotive components, and industrial electrical-related applications. Bakelite stock is still manufactured and produced in sheet, rod and tube form for industrial applications in the electronics, power generation and aerospace industries, and under a variety of commercial brand names.
Phenolic resins have been commonly used in ablative heat shields. Soviet heatshields for ICBM warheads and spacecraft reentry consisted of asbestos textolite, impregnated with Bakelite. Bakelite is also used in the mounting of metal samples in metallography.
Collectable status.
Bakelite items, particularly jewelry and radios, have become a popular collectable. The term "Bakelite" is sometimes used in the resale market to indicate various types of early plastics, including Catalin which may be brightly colored, as well as items made of Bakelite material.
Patents.
The United States Patent and Trademark Office granted Baekeland a patent for a "Method of making insoluble products of phenol and formaldehyde" on December 7, 1909. Producing hard, compact, insoluble and infusable condensation products of phenols and formaldehyde marked the beginning of the modern plastics industry.

</doc>
<doc id="4487" url="https://en.wikipedia.org/wiki?curid=4487" title="Bean">
Bean

Bean is a common name for large plant seeds of several genera of the family Fabaceae (alternately Leguminosae) which are used for human or animal food.
Terminology.
The term "bean" originally referred to the seed of the broad or fava bean, but was later expanded to include members of the New World genus "Phaseolus", such as the common bean and the runner bean, and the related genus "Vigna". The term is now applied generally to many other related plants such as Old World soybeans, peas, chickpeas (garbanzo beans), vetches, and lupins.
"Bean" is sometimes used as a synonym of "pulse", an edible legume, though the term "pulses" is more correctly reserved for leguminous crops harvested for their dry grain. The term "bean" usually excludes crops used mainly for oil extraction (such as soy-beans and pea-nuts), as well as those used exclusively for sowing purposes (such as clover and alfalfa). Leguminous crops harvested green for food, such as snap peas, snow peas, and so on, are not considered beans, and are classified as vegetable crops. According to the United Nations Food and Agriculture Organization the term "bean" should include only species of "Phaseolus"; however, a strict consensus definition has proven difficult because in the past, several species such as "Vigna" "angularis" (azuki bean), "mungo" (black gram), "radiata" (green gram), "aconitifolia" (moth bean)) were classified as "Phaseolus" and later reclassified. The use of the term "bean" to refer to species other than "Phaseolus" thus remains. In some countries, the term "bean" can mean a host of different species.
In English usage, the word "bean" is also sometimes used to refer to the seeds or pods of plants that are not in the family leguminosae, but which bear a superficial resemblance to true beans—for example coffee beans, castor beans and cocoa beans (which resemble bean seeds), and vanilla beans, which superficially resemble bean pods.
Cultivation.
Unlike the closely related pea, beans are a summer crop that need warm temperatures to grow. Maturity is typically 55–60 days from planting to harvest. As the bean pods mature, they turn yellow and dry up, and the beans inside change from green to their mature colour. As a vine, bean plants need external support, which may be provided in the form of special "bean cages" or poles. Native Americans customarily grew them along with corn and squash (the so-called Three Sisters), with the tall cornstalks acting as support for the beans.
In more recent times, the so-called "bush bean" has been developed which does not require support and has all its pods develop simultaneously (as opposed to pole beans which develop gradually). This makes the bush bean more practical for commercial production.
History.
Beans are one of the longest-cultivated plants. Broad beans, also called fava beans, in their wild state the size of a small fingernail, were gathered in Afghanistan and the Himalayan foothills. In a form improved from naturally occurring types, they were grown in Thailand since the early seventh millennium BCE, predating ceramics. They were deposited with the dead in ancient Egypt. Not until the second millennium BCE did cultivated, large-seeded broad beans appear in the Aegean, Iberia and transalpine Europe. In the "Iliad" (8th century BCE) is a passing mention of beans and chickpeas cast on the threshing floor.
Beans were an important source of protein throughout Old and New World history, and still are today.
The oldest-known domesticated beans in the Americas were found in Guitarrero Cave, an archaeological site in Peru, and dated to around the second millennium BCE.
Most of the kinds commonly eaten fresh or dried, those of the genus "Phaseolus", come originally from the Americas, being first seen by a European when Christopher Columbus, during his exploration of what may have been the Bahamas, found them growing in fields. Five kinds of "Phaseolus" beans were domesticated by pre-Columbian peoples: common beans ("Phaseolus vulgaris") grown from Chile to the northern part of what is now the United States, and lima and sieva beans ("Phaseolus lunatus"), as well as the less widely distributed teparies ("Phaseolus acutifolius"), scarlet runner beans ("Phaseolus coccineus") and polyanthus beans ("Phaseolus polyanthus") One especially famous use of beans by pre-Columbian people as far north as the Atlantic seaboard is the "Three Sisters" method of companion plant cultivation:
Dry beans come from both Old World varieties of broad beans (fava beans) and New World varieties (kidney, black, cranberry, pinto, navy/haricot).
Beans are a heliotropic plant, meaning that the leaves tilt throughout the day to face the sun. At night, they go into a folded "sleep" position.
Types.
Currently, the world genebanks hold about 40,000 bean varieties, although only a fraction are mass-produced for regular consumption.
Some bean types include:
Health concerns.
Toxins.
Some kinds of raw beans contain a harmful tasteless toxin, lectin phytohaemagglutinin, that must be removed by cooking. Red and kidney beans are particularly toxic, but other types also pose risks of food poisoning. A recommended method is to boil the beans for at least ten minutes; undercooked beans may be more toxic than raw beans.
Cooking beans, without bringing them to the boil, in a slow cooker at a temperature well below boiling may not destroy toxins. A case of poisoning by butter beans used to make falafel was reported; the beans were used instead of traditional broad beans or chickpeas, soaked and ground without boiling, made into patties, and shallow fried.
Bean poisoning is not well known in the medical community, and many cases may be misdiagnosed or never reported; figures appear not to be available. In the case of the UK National Poisons Information Service, available only to health professionals, the dangers of beans other than red beans were not flagged .
Fermentation is used in some parts of Africa to improve the nutritional value of beans by removing toxins. Inexpensive fermentation improves the nutritional impact of flour from dry beans and improves digestibility, according to research co-authored by Emire Shimelis, from the Food Engineering Program at Addis Ababa University. Beans are a major source of dietary protein in Kenya, Malawi, Tanzania, Uganda and Zambia.
Bacterial infection from bean sprouts.
It is common to make beansprouts by letting some types of bean, often mung beans, germinate in moist and warm conditions; beansprouts may be used as ingredients in cooked dishes, or eaten raw or lightly cooked. There have been many outbreaks of disease from bacterial contamination, often by "salmonella", "listeria", and "Escherichia coli", of beansprouts not thoroughly cooked, some causing significant mortality.
Antinutrients.
Many types of bean contain significant amounts of antinutrients that inhibit some enzyme processes in the body. Phytic acid and phytates, present in grains, nuts, seeds and beans, interfere with bone growth and interrupt vitamin D metabolism. Pioneering work on the effect of phytic acid was done by Edward Mellanby from 1939.
Nutrition.
Beans have significant amounts of fiber and soluble fiber, with one cup of cooked beans providing between nine and 13 grams of fiber. Soluble fiber can help lower blood cholesterol. Beans are also high in protein, complex carbohydrates, folate, and iron.
Flatulence.
Many edible beans, including broad beans and soybeans, contain oligosaccharides (particularly raffinose and stachyose), a type of sugar molecule also found in cabbage. An anti-oligosaccharide enzyme is necessary to properly digest these sugar molecules. As a normal human digestive tract does not contain any anti-oligosaccharide enzymes, consumed oligosaccharides are typically digested by bacteria in the large intestine. This digestion process produces flatulence-causing gases as a byproduct. Since sugar dissolves in water, another method of reducing flatulence associated with eating beans is to drain the water in which the beans have been cooked.
Some species of mold produce alpha-galactosidase, an anti-oligosaccharide enzyme, which humans can take to facilitate digestion of oligosaccharides in the small intestine. This enzyme, currently sold in the United States under the brand-names Beano and Gas-X Prevention, can be added to food or consumed separately. In many cuisines beans are cooked along with natural carminatives such as anise seeds, coriander seeds and cumin .
One effective strategy is to soak beans in alkaline (baking soda) water overnight before rinsing thoroughly . Sometimes vinegar is added, but only after the beans are cooked as vinegar interferes with the beans' softening.
Fermented beans will usually not produce most of the intestinal problems that unfermented beans will, since yeast can consume the offending sugars.
Production.
The world leader in production of dry beans is Burma, followed by India and Brazil. In Africa, the most important producer is Tanzania.

</doc>
<doc id="4489" url="https://en.wikipedia.org/wiki?curid=4489" title="Breast">
Breast

The breast is one of two prominences found on the upper ventral region of the torso of female and male primates. In females, it serves as the mammary gland, which produces and secretes milk and feeds infants. Both females and males develop breasts from the same embryological tissues. At puberty, estrogens, in conjunction with growth hormone, cause breast development. Males do not develop pronounced or physiologically matured breasts because their bodies produce lower levels of estrogens and higher levels of androgens, namely testosterone, which suppress the effects of estrogens in developing breast tissue. The breasts of females are typically far more prominent than those of males.
Subcutaneous fat covers and envelops a network of ducts that converge to the nipple, and these tissues give the breast its size and shape. At the ends of the ducts are lobules, or clusters of alveoli, where milk is produced and stored in response to hormonal signals. During pregnancy, the breast responds to a complex interaction of hormones, including estrogens, progesterone, and prolactin, that mediate the completion of its development, namely lobuloalveolar maturation, in preparation of lactation and breastfeeding. Upon childbirth, the alveoli are stimulated to produce and secrete milk for infants.
Along with their function in feeding infants, female breasts have social and sexual characteristics. Breasts have been featured in notable ancient and modern sculpture, art, and photography. Female breasts can figure prominently in a woman's perception of her body image and sexual attractiveness. A number of Western cultures associate breasts with sexuality and tend to regard bare breasts as immodest or indecent. Breasts and especially the nipples are an erogenous zone on women. Given the emphasis of some cultures on breast size and attractiveness, some women seek breast augmentation or other kinds of surgery to enlarge or reduce their breast size or to reverse sagging breasts.
Etymology.
The English word "breast" derives from the Old English word "brēost" (breast, bosom) from Proto-Germanic "breustam" (breast), from the Proto-Indo-European base bhreus– (to swell, to sprout). The "breast" spelling conforms to the Scottish and North English dialectal pronunciations.
Anatomy.
In women, the breasts overlay the pectoralis major muscles and usually extend from the level of the second rib to the level of the sixth rib in the front of the human rib cage; thus, the breasts cover much of the chest area and the chest walls. At the front of the chest, the breast tissue can extend from the clavicle (collarbone) to the middle of the sternum (breastbone). At the sides of the chest, the breast tissue can extend into the axilla (armpit), and can reach as far to the back as the latissimus dorsi muscle, extending from the lower back to the humerus bone (the longest bone of the upper arm). As a mammary gland, the breast is composed of layers of different types of tissue, among which predominate two types, adipose tissue and glandular tissue, which effects the lactation functions of the breasts.
Morphologically, the breast is a cone with the base at the chest wall, and the apex is at the nipple, the center of the nipple-areola complex. The superﬁcial tissue layer (superficial fascia) is separated from the skin by 0.5–2.5 cm of subcutaneous fat (adipose tissue). The suspensory Cooper's ligaments are fibrous-tissue prolongations that radiate from the superficial fascia to the skin envelope. The female adult breast contains 14–18 irregular lactiferous lobes that converge at the nipple. Milk exits the breast through the nipple, which is surrounded by a pigmented area of skin called the areola. The size of the areola can vary widely among women. The areola contains modified sweat glands known as Montgomery's glands. These glands secrete oily fluid that lubricate and protect the nipple during breastfeeding. Volatile compounds in these secretions may also serve as an olfactory stimulus for newborn appetite.
The 2.0–4.5 mm milk ducts are immediately surrounded with dense connective tissue that support the glands. The glandular tissue of the breast is biochemically supported with estrogen. When a woman ceases menstruation and her body estrogen levels decrease, breast atrophy occurs – milk gland tissue atrophies, withers, and disappears, resulting in a breast composed of adipose tissue, superﬁcial fascia, suspensory ligaments, and the skin envelope.
The dimensions and weight of the breast vary widely among women, ranging from approximately 500 to 1,000 grams (1.1 to 2.2 pounds) each. A small-to-medium-sized breast weighs 500 grams (1.1 pounds) or less and a large breast can weighs approximately 750 to 1,000 grams (1.7 to 2.2 pounds) or more. The tissue composition ratios of the breast also vary among women. Some women's breasts have varying proportions of glandular tissue than of adipose or connective tissues. The fat-to-connective-tissue ratio determines the density or firmness of the breast. During a woman's life, her breasts change size, shape, and weight due to hormonal bodily changes during puberty, fertility, pregnancy, breastfeeding, and menopause.
Glandular structure.
The breast is an apocrine gland that produces milk to feed an infant child; for which the nipple of the breast is surrounded by the areola (nipple-areola complex), the skin color of which varies from pink to dark brown, and has many sebaceous glands. The basic units of the breast are the terminal duct lobular units (TDLUs), which produce the fatty breast milk. They give the breast its offspring-feeding functions as a mammary gland. They are distributed throughout the body of the breast; approximately two-thirds of the lactiferous tissue is within 30 mm of the base of the nipple. The terminal lactiferous ducts drain the milk from TDLUs into 4–18 lactiferous ducts, which drain to the nipple; the milk-glands-to-fat ratio is 2:1 in a lactating woman, and 1:1 in a non-lactating woman. In addition to the milk glands, the breast also is composed of connective tissues (collagen, elastin), white fat, and the suspensory Cooper's ligaments. Sensation in the breast is provided by the peripheral nervous system innervation, by means of the front (anterior) and side (lateral) cutaneous branches of the fourth-, the fifth-, and the sixth intercostal nerves, while the T-4 nerve (Thoracic spinal nerve 4), which innervates the dermatomic area, supplies sensation to the nipple-areola complex.
Lymphatic drainage.
Approximately 75% of the lymph from the breast travels to the axillary lymph nodes on the same side of the body, whilst 25% of the lymph travels to the parasternal nodes (beside the sternum bone). A small amount of remaining lymph travels to the other breast, and to the abdominal lymph nodes. The axillary lymph nodes include the pectoral (chest), subscapular (under the scapula), and humeral (humerus-bone area) lymph-node groups, which drain to the central axillary lymph nodes and to the apical axillary lymph nodes. The lymphatic drainage of the breasts is especially relevant to oncology, because breast cancer is a cancer common to the mammary gland, and cancer cells can metastasize (break away) from a tumour and be dispersed to other parts of the body by means of the lymphatic system.
Shape and support.
The morphologic variations in the size, shape, volume, tissue density, pectoral locale, and spacing of the breasts determine their natural shape, appearance, and position on a woman's chest. Breast size and other characteristics do not predict the fat-to-milk-gland ratio or the potential for the woman to nurse an infant child. The size and the shape of the breasts are influenced by normal-life hormonal changes (thelarche, menstruation, pregnancy, menopause) and medical conditions (e.g. virginal breast hypertrophy). The shape of the breasts is naturally determined by the support of the suspensory Cooper's ligaments, the underlying muscle and bone structures of the chest, and the skin envelope. The suspensory ligaments sustain the breast from the clavicle (collarbone) and the clavico-pectoral fascia (collarbone and chest), by traversing and encompassing the fat and milk-gland tissues, the breast is positioned, affixed to, and supported upon the chest wall, while its shape is established and maintained by the skin envelope. While it has been a common belief that breastfeeding causes breasts to sag, researchers have found that a woman's breasts sag due to four key factors: cigarette smoking, the number of pregnancies, gravity, and weight gain and loss.
The base of each breast is attached to the chest by the deep fascia over the pectoralis major muscles. The space between the breast and the pectoralis major muscle is called retromammary space and gives mobility to the breast. Some breasts are mounted high upon the chest wall, are of rounded shape, and project almost horizontally from the chest, which features are common to girls and women in the early stages of thelarchic development, the sprouting of the breasts. In the high-breast configuration, the dome-shaped and the cone-shaped breast is affixed to the chest at the base, and the weight is evenly distributed over the base area. In the low-breast configuration, a proportion of the breast weight is supported by the chest, against which rests the lower surface of the breast, thus is formed the inframammary fold (IMF). Because the base is deeply affixed to the chest, the weight of the breast is distributed over a greater area, and so reduces the weight-bearing strain upon the chest, shoulder, and back muscles that bear the weight of the bust.
The chest (thoracic cavity) progressively slopes outwards from the thoracic inlet (atop the breastbone) and above to the lowest ribs that support the breasts. The inframammary fold, where the lower portion of the breast meets the chest, is an anatomic feature created by the adherence of the breast skin and the underlying connective tissues of the chest; the IMF is the lower-most extent of the anatomic breast. In the course of thelarche, some girls develop breasts the lower skin-envelope of which touches the chest below the IMF, and some girls do not; both breast anatomies are statistically normal morphologic variations of the size and shape of women's breasts.
Asymmetry.
Up to 25% of women's breasts display a persistent, visible breast asymmetry, which is defined as differing in size by at least one cup size. For about 5% to 10% of women, their breasts are severely different, with the left breast being larger in 62% of cases. This is due to the left breast's proximity to the heart, a greater number of arteries and veins, and a protective layer of fat surrounding the heart located beneath it.
The most common cause for asymmetric breast density is the common normal variant of asymmetrically distributed breast tissue. Minor asymmetry may be resolved by wearing a padded bra, but in severe cases of developmental breast deformity—commonly called "Amazon's Syndrome" by physicians—may require corrective surgery due to morphological alterations caused by variations in shape, volume, position of the breasts relative to the inframammary fold, the nipple-areola complex on the chest, or both.
If a woman is uncomfortable with her breasts' asymmetry and the difference is relatively minor, she can minimize the difference with a corrective or padded bra or use gel bra inserts. If the difference is significant, this can indicate that the woman should consult a doctor to establish the cause. Breast asymmetry is related to several known risk factors for breast cancer, but only 3% of mammograms of women with asymmetrical breasts result in a cancer diagnosis. Types of breast asymmetry include bilateral asymmetric hypertrophy, unilateral hypertrophy with normal contralateral breast, unilateral hypertrophy with amastia or hypoplasia of the contralateral side, unilateral amastia or hypoplasia with normal contralateral breast, asymmetric bilateral hypoplasia, and unilateral mammary ptosis.
Most surgeons will only perform an augmentation procedure to treat asymmetry if the woman's breasts differ by at least one cup size. Options include a minimally invasive procedure known as platelet injection fat transfer, which transfers fat cells from the woman's thighs to her smaller breast. More invasive procedures include reduction or augmentation mammoplasty, such as mastopexy, breast reduction plasty, or breast augmentation. Depending on the nature of the asymmetry, it may be necessary to operate on one or both breasts.
Development.
The morphological structure of the human breast is identical in males and females until puberty. For a girl in puberty, during thelarche (the breast-development stage), the female sex hormones (principally estrogens), in conjunction with growth hormone, promote the sprouting, growth, and development of the breasts, in the course of which, as mammary glands, they grow in size and volume, and rest on her chest; these development stages of secondary sex characteristics (breasts, pubic hair, etc.) are illustrated in the five-stage Tanner Scale.
During thelarche, the developing breasts sometimes are of unequal size, and usually the left breast is slightly larger; said condition of asymmetry is transitory and statistically normal to female physical and sexual development. Moreover, breast development sometimes is abnormal, manifested either as overdevelopment (e.g., virginal breast hypertrophy, macromastia) or as underdevelopment (e.g., tuberous breast deformity, micromastia) in girls and women; and manifested in boys and men as gynecomastia (woman's breasts), the consequence of a biochemical imbalance between the normal levels of estrogen and testosterone in the male body.
Approximately two years after the onset of puberty (a girl's first menstrual cycle), the hormone estrogen, in conjunction with growth hormone, stimulates the development and growth of the glandular, fat, and suspensory tissues that compose the breast. This continues for approximately four years until establishing the final shape of the breast (size, volume, density) when she is a woman of approximately 21 years of age. Mammoplasia (breast enlargement) in girls begins at puberty, unlike all other primates in which breasts enlarge only during lactation.
The breasts are principally composed of adipose, glandular and connective tissues. Because these tissues have hormone receptors, their sizes and volumes fluctuate according to the hormonal changes particular to thelarche (sprouting of breasts), menstruation (egg production), pregnancy (reproduction), lactation (feeding of offspring), and menopause (end of menstruation).
During the menstrual cycle, the breasts are enlarged by premenstrual water retention and temporary growth. During pregnancy, the breasts become enlarged and denser (firmer) because of the prolactin-caused organ hypertrophy, which begins the production of breast milk, increases the size of the nipples, and darkens the skin color of the nipple-areola complex. These changes continue during the lactation and the breastfeeding periods. Afterwards, the breasts generally revert to their pre-pregnancy size, shape, and volume, yet might show stretch marks.
At menopause, breast atrophy occurs – the breasts can decrease in size when the levels of circulating estrogen decline, followed by the withering of the adipose tissue and the milk glands. Additional to such natural biochemical stimuli, the breasts can become enlarged consequent to an adverse side effect of combined oral contraceptive pills; and the size of the breasts can also increase and decrease in response to the body weight fluctuations of the woman. Moreover, the physical changes occurred to the breasts often are recorded in the stretch marks of the skin envelope; they can serve as historical indicators of the increments and the decrements of the size and the volume of a woman's breasts throughout the course of her life.
Physiology.
The primary function of the breasts, as mammary glands, is the feeding and the nourishing of an infant child with breast milk during the maternal lactation period. The round shape of the breast helps to limit the loss of maternal body heat, because milk production depends upon a higher-temperature environment for the proper, milk-production function of the mammary gland tissues, the lactiferous ducts. Regarding the shape of the breast, the study "The Evolution of the Human Breast" (2001) proposed that the rounded shape of a woman's breast evolved to prevent the sucking infant offspring from suffocating while feeding at the teat; that is, because of the human infant's small jaw, which did not project from the face to reach the nipple, he or she might block the nostrils against the mother's breast if it were of a flatter form (cf. chimpanzee); theoretically, as the human jaw receded into the face, the woman's body compensated with round breasts.
In a woman, the condition of lactation unrelated to pregnancy can occur as galactorrhea (spontaneous milk flow), and because of the adverse effects of drugs (e.g. antipsychotic medications), of extreme physical stress, and of endocrine disorders. In a newborn infant, the capability of lactation is consequence of the mother's circulating hormones (prolactin, oxytocin, etc.) in his or her blood stream, which were introduced by the shared circulatory system of the placenta. In men, the mammary glands are also present in the body, but normally remain undeveloped because of the hormone testosterone, however, when male lactation occurs, it is considered a pathological symptom of a disorder of the pituitary gland.
Aging.
Breast ptosis, or sagging of the breasts, is a normal consequence of aging where the breast tissue droops lower on the chest and the nipple points downward. Researchers have found that ptosis is influenced by several key factors: greater age, higher body mass index, larger bra cup size, history of significant weight loss (>50 lbs), number of pregnancies, and history of cigarette smoking. It is not caused, as commonly believed by many women and medical practitioners, by breastfeeding.
Plastic surgeons categorize ptosis by evaluating the position of the nipple relative to the inframammary crease (where the underside of the breast meets the chest wall). This is determined by measuring from the center of the nipple to the sternal notch (at the top of the breast bone) to gauge how far the nipple has fallen. The standard anthropometric measurement for young women is . This measurement is used to assess both breast ptosis and breast symmetry. The surgeon will assess the breast's angle of projection. The apex of the breast, which includes the nipple, can have a flat angle of projection (180 degrees) or acute angle of projection (greater than 180 degrees). The apex rarely has an angle greater than 60 degrees. The angle of the breast apex is partly determined by the tautness of the suspensory Cooper's ligaments. For example, when a woman lies on her back, the angle of the breast apex becomes a flat, obtuse angle (less than 180 degrees) while the base-to-length ratio of the breast ranges from 0.5 to 1.0.
Clinical significance.
The breast is susceptible to numerous benign and malignant conditions. The most frequent benign conditions are puerperal mastitis, fibrocystic breast changes and mastalgia. Breast cancer is one of the leading causes of death among women.
Society and culture.
Art history.
In European pre-historic societies, sculptures of female figures with pronounced or highly exaggerated breasts were common. A typical example is the so-called Venus of Willendorf, one of many Paleolithic Venus figurines with ample hips and bosom. Artifacts such as bowls, rock carvings and sacred statues with breasts have been recorded from 15,000 BC up to late antiquity all across Europe, North Africa and the Middle East.
Many female deities representing love and fertility were associated with breasts and breast milk. Figures of the Phoenician goddess Astarte were represented as pillars studded with breasts. Isis, an Egyptian goddess who represented, among many other things, ideal motherhood, was often portrayed as suckling pharaohs, thereby confirming their divine status as rulers. Even certain male deities representing regeneration and fertility were occasionally depicted with breast-like appendices, such as the river god Hapy who was considered to be responsible for the annual overflowing of the Nile.
Female breasts were also prominent in the Minoan civilization in the form of the famous Snake Goddess statuettes. In Ancient Greece there were several cults worshipping the "Kourotrophos", the suckling mother, represented by goddesses such as Gaia, Hera and Artemis. The worship of deities symbolized by the female breast in Greece became less common during the first millennium. The popular adoration of female goddesses decreased significantly during the rise of the Greek city states, a legacy which was passed on to the later Roman Empire.
During the middle of the first millennium BC, Greek culture experienced a gradual change in the perception of female breasts. Women in art were covered in clothing from the neck down, including female goddesses like Athena, the patron of Athens who represented heroic endeavor. There were exceptions: Aphrodite, the goddess of love, was more frequently portrayed fully nude, though in postures that were intended to portray shyness or modesty, a portrayal that has been compared to modern pin ups by historian Marilyn Yalom. Although nude men were depicted standing upright, most depictions of female nudity in Greek art occurred "usually with drapery near at hand and with a forward-bending, self-protecting posture". A popular legend at the time was of the Amazons, a tribe of fierce female warriors who socialized with men only for procreation and even removed one breast to become better warriors (the idea being that the right breast would interfere with the operation of a bow and arrow). The legend was a popular motif in art during Greek and Roman antiquity and served as an antithetical cautionary tale.
Body image.
Many women regard their breasts as important to their sexual attractiveness, as a sign of femininity that is important to their sense of self. Due to this, when a woman considers her breasts deficient in some respect, she might choose to undergo a plastic surgery procedure to enhance them, either to have them augmented or to have them reduced, or to have them reconstructed if she suffered a deformative disease, such as breast cancer. After mastectomy, the reconstruction of the breast or breasts is done with breast implants or autologous tissue transfer, using fat and tissues from the abdomen, which is performed with a TRAM flap or with a back (latissiumus muscle flap). Breast reduction surgery is a procedure that involves removing excess breast tissue, fat, and skin, and the repositioning of the nipple-areola complex.
Cosmetic improvement procedures include breast lift (mastopexy), breast augmentation with implants, and combination procedures; the two types of available breast implants are models filled with silicone gel, and models filled with saline solution. These types of breast surgery can also repair inverted nipples by releasing milk duct tissues that have become tethered. Furthermore, in the case of the obese woman, a breast lift (mastopexy) procedure, with or without a breast volume reduction, can be part of an upper-body lift and contouring for the woman who has undergone massive body weight loss.
Surgery of the breast presents the health risk of interfering with the ability to breast-feed an infant child, and might include consequences such as altered sensation in the nipple-areola complex, interference with mammography (breast x-rays images) when there are breast implants present in the breasts. Regarding breastfeeding capability after breast reduction surgery, studies reported that women who underwent breast reduction can retain the ability to nurse an infant child, when compared to women in a control group who underwent breast surgery using a modern pedicle surgical technique. Plastic surgery organizations generally discourage elective cosmetic breast augmentation surgery for teen-aged girls, because, at that age, the volume of the breast tissues (milk glands and fat) can continue to grow as the girl matures to womanhood. Breast reduction surgery for teen-aged girls, however, is a matter handled according to the particulars of the case of hypoplasia. (see: breast hypertrophy.)
Other potential means of breast enlargement also exist, such as hormonal breast enhancement.
Clothing.
Because breasts are mostly fatty tissue, their shape can within limits be molded by clothing, such as foundation garments. Bras are commonly worn by about 90% of Western women, and are often worn for support. The social norm in most Western cultures is to cover breasts in public, though the extent of coverage varies depending on the social context. Some religions ascribe a special status to the female breast, either in formal teachings or through symbolism. Islam forbids women from exposing their breasts in public.
Many cultures associate breasts with sexuality and tend to regard bare breasts as immodest or indecent. In some cultures, like the Himba in northern Namibia, bare-breasted women are normal, while a thigh is highly sexualised and not exposed in public. In a few Western countries female toplessness at a beach is acceptable, although it may not be acceptable in the town center. In some areas, exposing a woman's breasts applies only to the exposure of nipples.
In the United States, women who breast-feed in public can receive negative attention. There have been instances where women have been asked to leave public venues. In New York, the topfreedom equality movement helped to bring a case, "People v. Santorelli" (1992), to the New York State Court of Appeals. They ruled that New York's indecent exposure laws did not apply to a bare-breasted woman. Other (gender equality) efforts succeeded in most of Canada in the 1990s. Bare-breasted women are legal and culturally acceptable at public beaches in Australia and much of Europe.
Sexual characteristic.
In some cultures, breasts play a role in human sexual activity. Breasts and especially the nipples are among the various human erogenous zones. They are sensitive to the touch as they have many nerve endings; and it is common to press or massage them with hands or orally before or during sexual activity. Some women can achieve an orgasm from such activities. Research has suggested that the sensations are genital orgasms caused by nipple stimulation, and may also be directly linked to "the genital area of the brain". Sensation from the nipples travels to the same part of the brain as sensations from the vagina, clitoris and cervix. Nipple stimulation may trigger uterine contractions, which then produce a sensation in the genital area of the brain. In the ancient Indian work the "Kama Sutra", light scratching of the breasts with nails and biting with teeth are considered erotic. During sexual arousal, breast size increases, venous patterns across the breasts become more visible, and nipples harden. Compared to other primates, human breasts are proportionately large throughout adult females' lives. Some writers have suggested that they may have evolved as a visual signal of sexual maturity and fertility.
Many people regard the female human body, of which breasts are an important aspect, to be aesthetically pleasing, as well as erotic. Research conducted at the Victoria University of Wellington showed that breasts are often the first thing men look at, and for a longer time than other body parts. The writers of the study had initially speculated that the reason for this is due to endocrinology with larger breasts indicating higher levels of estrogen and a sign of greater fertility, but the researchers said that "Men may be looking more often at the breasts because they are simply aesthetically pleasing, regardless of the size."
Many people regard bare female breasts to be erotic, and they can elicit heightened sexual desires in men in many cultures. Some people show a sexual interest in female breasts distinct from that of the person, which may be regarded as a breast fetish. While U.S. culture prefers breasts that are youthful and upright, some cultures venerate women with drooping breasts, indicating mothering and the wisdom of experience.
Anthropomorphic geography.
There are many mountains named after the breast because they resemble it in appearance and so are objects of religious and ancestral veneration as a fertility symbol and of well-being. In Asia, there was "Breast Mountain", which had a cave where the Buddhist monk Bodhidharma (Da Mo) spent much time in meditation. Other such breast mountains are Mount Elgon on the Uganda-Kenya border, Beinn Chìochan and the Maiden Paps in Scotland, the "Bundok ng Susong Dalaga" (Maiden's breast mountains) in Talim Island, Philippines, the twin hills known as the Paps of Anu ("Dá Chích Anann" or "the breasts of Anu"), near Killarney in Ireland, the 2,086 m high "Tetica de Bacares" or "La Tetica" in the Sierra de Los Filabres, Spain, and Khao Nom Sao in Thailand, Cerro Las Tetas in Puerto Rico and the Breasts of Aphrodite in Mykonos, among many others. In the United States, the Teton Range is named after the French word for "breast".
Symbolism.
In Christian iconography, some works of art depict women with their breasts in their hands or on a platter, signifying that they died as a martyr by having their breasts severed; one example of this is Saint Agatha of Sicily.

</doc>
<doc id="4492" url="https://en.wikipedia.org/wiki?curid=4492" title="Baghdad">
Baghdad

Baghdad ( "", , Iraqi pronunciation: ) is the capital of the Republic of Iraq. The population of Baghdad, , is approximately 7,216,040, making it the largest city in Iraq, the second largest city in the Arab world (after Cairo, Egypt), and the second largest city in Western Asia (after Tehran, Iran). According to the government, the population of the country has reached 35 million, with 9 million in the capital.
Located along the Tigris River, the city was founded in the 8th century and became the capital of the Abbasid Caliphate. Within a short time of its inception, Baghdad evolved into a significant cultural, commercial, and intellectual center for the Islamic world. This, in addition to housing several key academic institutions (e.g. House of Wisdom), garnered the city a worldwide reputation as the "Center of Learning".
Throughout the High Middle Ages, Baghdad was considered to be the largest city in the world with an estimated population of 1,200,000 people. The city was largely destroyed at the hands of the Mongol Empire in 1258, resulting in a decline that would linger through many centuries due to frequent plagues and multiple successive empires. With the recognition of Iraq as an independent state (formerly the British Mandate of Mesopotamia) in 1938, Baghdad gradually regained some of its former prominence as a significant center of Arab culture.
In contemporary times, the city has often faced severe infrastructural damage, most recently due to the 2003 invasion of Iraq, and the subsequent Iraq War that lasted until December 2011. In recent years, the city has been frequently subjected to insurgency attacks. , Baghdad was listed as one of the least hospitable places in the world to live, and was ranked by Mercer as the worst of 221 major cities as measured by quality-of-life.
Etymology.
The name Baghdad is pre-Islamic. The site where the city of Baghdad came to stand has been populated for millennia and by the 8th century AD several (Assyrian Christian) villages had developed there, including a Persian hamlet called "Baghdad", the name which would come to be used for the Abbasid metropolis.
It has been proposed that the name is of Indo-European origin and a Middle Persian compound of "Bagh" () "god" and "dād" () "given by", translating to "Bestowed by God" or "God's gift". In Old Persian the first element can be traced to "boghu" and is related to Slavic "bog" "god", while the second can be traced to "dadāti". A similar term in Middle Persian is the name "Mithradāt" ("Mihrdād" in New Persian), known in English by its Hellenistic form Mithridates meaning "gift of Mithra" ("dāt" is the more archaic form of "dād", related to Latin "dat" and English "donor"). There are a number of other locations in the wider region whose names are compounds of the word "bagh", including Baghlan and Bagram in Afghanistan or a village called Bagh-šan in Iran.
When the Abbasid caliph, al-Mansur, founded a completely new city for his capital, he chose the name Madinat al-Salaam or "City of Peace". This was the official name on coins, weights, and other official usage, although the common people continued to use the old name. By the 11th century, "Baghdad" became almost the exclusive name for the world-renowned metropolis.
History.
Foundation.
After the fall of the Umayyads, the first Muslim dynasty, the victorious Abbasid rulers wanted their own capital to rule from. Choosing a site north of the Sassanid capital of Ctesiphon (and also just north of where ancient Babylon once stood), on 30 July 762, the caliph Al-Mansur commissioned the construction of the city, and it was built under the supervision of the Barmakids. Mansur believed that Baghdad was the perfect city to be the capital of the Islamic empire under the Abbasids. Mansur loved the site so much he is quoted saying, "This is indeed the city that I am to found, where I am to live, and where my descendants will reign afterward".
The city's growth was helped by its excellent location, based on at least two factors: it had control over strategic and trading routes along the Tigris; the abundance of water in a dry climate. Water exists on both the north and south ends of the city, allowing all households to have a plentiful supply, which was very uncommon during this time.
Baghdad eclipsed Ctesiphon, the capital of the Persian Empire, which was located some to the southeast. Today, all that remains of Ctesiphon is the shrine town of Salman Pak, just to the south of Greater Baghdad. Ctesiphon itself had replaced and absorbed Seleucia, the first capital of the Seleucid Empire. Seleucia had earlier replaced the city of Babylon.
In its early years, the city was known as a deliberate reminder of an expression in the Qur'an, when it refers to Paradise. It took four years to build (764-768). Mansur assembled engineers, surveyors, and art constructionists from around the world to come together and draw up plans for the city. Over 100,000 construction workers came to survey the plans; many were distributed salaries to start the building of the city. July was chosen as the starting time because two Astrologers, Naubakht Ahvazi and Mashallah, believed that the city should be built under the sign of the lion, Leo. Leo is associated with fire and symbolises productivity, pride, and expansion.
The bricks used to make the city were on all four sides. Abū Ḥanīfa was the counter of the bricks and he developed a canal, which brought water to the work site for the use of both human consumption and the manufacturing of the bricks. Marble was also used to make buildings throughout the city, and marble steps led down to the river's edge.
The basic framework of the city consists of two large semicircles about in diameter. The city was designed as a circle about in diameter, leading it to be known as the "Round City". The original design shows as single ring of residential and commercial structures along the inside of the city walls, but the final construction added another ring inside the first. Within the city there were many parks, gardens, villas, and promenades. In the center of the city lay the mosque, as well as headquarters for guards. The purpose or use of the remaining space in the center is unknown. The circular design of the city was a direct reflection of the traditional Persian Sasanian urban design. The Sasanian city of Gur in Fars, built 500 years before Baghdad, is nearly identical in its general circular design, radiating avenues, and the government buildings and temples at the centre of the city. This style of urban planning contrasted with Ancient Greek and Roman urban planning, in which cities are designed as squares or rectangles with streets intersecting each other at right angles.
Surrounding walls.
The four surrounding walls of Baghdad were named Kufa, Basra, Khurasan, and Syria; named because their gates pointed in the directions of these destinations. The distance between these gates was a little less than . Each gate had double doors that were made of iron; the doors were so heavy it took several men to open and close them. The wall itself was about 44 m thick at the base and about 12 m thick at the top. Also, the wall was 30 m high, which included merlons, a solid part of an embattled parapet usually pierced by embrasures. This wall was surrounded by another wall with a thickness of 50 m. The second wall had towers and rounded merlons, which surrounded the towers. This outer wall was protected by a solid glacis, which is made out of bricks and quicklime. Beyond the outer wall was a water-filled moat.
Golden Gate Palace.
In the middle of Baghdad, in the central square was the Golden Gate Palace. The Palace was the residence of the caliph and his family. In the central part of the building was a green dome that was 39 m high. Surrounding the palace was an esplanade, a waterside building, in which only the caliph could come riding on horseback. In addition, the palace was near other mansions and officer's residences. Near the Gate of Syria a building served as the home for the guards. It was made of brick and marble. The palace governor lived in the latter part of the building and the commander of the guards in the front. In 813, after the death of caliph Al-Amin the palace was no longer used as the home for the caliph and his family.
The roundness points to the fact that it was based on Arabic script. The two designers who were hired by Al-Mansur to plan the city's design were Naubakht, a Zoroastrian who also determined that the date of the foundation of the city would be astrologically auspicious, and Mashallah, a Jew from Khorasan, Iran.
Abbasids and the round city.
The Abbasid Caliphate was based on their being the descendants of the uncle of Muhammad and being part of the Quraysh tribe. They used Shi'a resentment, Khorasanian movement, and appeals to the ambitions and traditions of the newly conquered Persian aristocracy to overthrow the Umayyads.
The Abbasids sought to combine the hegemony of the Arab tribes with the imperial, court, ceremonial, and administrative structures of the Persians. The Abbasids considered themselves the inherittures and the need of Mansur to place the capital in a place that was representative of Arab-Islamic identity by building the House of Wisdom, where ancient texts were translated from their original language, such as Greek, to Arabic. Mansur is credited with the "Translation Movement" for this. Further, Baghdad is also near the ancient Sassanid imperial seat of Ctesiphon on the Tigris River.
Center of learning (8th to 13th centuries).
Within a generation of its founding, Baghdad became a hub of learning and commerce. The House of Wisdom was an establishment dedicated to the translation of Greek, Middle Persian and Syriac works. Scholars headed to Baghdad from all over the Abbasid Caliphate, facilitating the introduction of Persian, Greek and Indian science into the Arabic and Islamic world at that time. Baghdad was likely the largest city in the world from shortly after its foundation until the 930s, when it was tied by Córdoba.
Several estimates suggest that the city contained over a million inhabitants at its peak. Many of the "One Thousand and One Nights" tales are set in Baghdad during this period.
Among the notable features of Baghdad during this period were its exceptional libraries. Many of the Abbasid caliphs were patrons of learning and enjoyed collecting both ancient and contemporary literature. Although some of the princes of the previous Umayyad dynasty had begun to gather and translate Greek scientific literature, the Abbasids were the first to foster Greek learning on a large scale. Many of these libraries were private collections intended only for the use of the owners and their immediate friends, but the libraries of the caliphs and other officials soon took on a public or a semi-public character. Four great libraries were established in Baghdad during this period. The earliest was that of the famous Al Mamun, who was caliph from 813 to 833. Another was established by Sabur Ibn Ardashir in 991 or 993 for the literary men and scholars who frequented his academy. Unfortunately, this second library was plundered and burned by the Seljuks only seventy years after it was established. This was a good example of the sort of library built up out of the needs and interests of a literary society. The last two were examples of "madrasa" or theological college libraries. The Nizamiyah was founded by the Persian Nizam al Mulk, who was vizier of two early Seljuk sultans. It continued to operate even after the coming of the Mongols in 1258. The Mustansiriyah "madrasa", which owned an exceedingly rich library, was founded by Al Mustansir, the second last Abbasid caliph, who died in 1242. This would prove to be the last great library built by the caliphs of Baghdad.
End of the Abbasids in Baghdad.
By the 10th century, the city's population was between 1.2 million and 2 million. Baghdad's early meteoric growth eventually slowed due to troubles within the Caliphate, including relocations of the capital to Samarra (during 808–819 and 836–892), the loss of the western and easternmost provinces, and periods of political domination by the Iranian Buwayhids (945–1055) and Seljuk Turks (1055–1135).
The Seljuks were a clan of the Oghuz Turks from the Central Asia that converted to the Sunni branch of Islam. In 1040, they destroyed the Ghaznavids, taking over their land and in 1055, Tughril Beg, the leader of the Seljuks, took over Baghdad. The Seljuks expelled the Buyid dynasty of Shiites that ruled for some time and took over power and control of Baghdad. They ruled as Sultans in the name of the Abbasid caliphs (they saw themselves as being part of the Abbasid regime). Tughril Beg saw himself as the protector of the Abbasid Caliphs.
Wars in which Baghdad was involved are listed below:
In 1058, Baghdad was captured by the Fatimids under the Turkish general Abu'l-Ḥārith Arslān al-Basasiri, an adherent of the Ismailis along with the 'Uqaylid Quraysh. Not long before the arrival of the Saljuqs in Baghdad, al-Basasiri petitioned to the Fatimid Imam-Caliph al-Mustansir to support him in conquering Baghdad on the Ismaili Imam's behalf. It has recently come to light that the famed Fatimid "da'i", al-Mu'ayyad al-Shirazi, had a direct role in supporting al-Basasiri and helped the general to succeed in taking Mawṣil, Wāsit and Kufa. Soon after, by December 1058, a Shi'i "adhān" (call to prayer) was implemented in Baghdad and a "khutbah" (sermon) was delivered in the name of the Fatimid Imam-Caliph. Despite his Shi'i inclinations, Al-Basasiri received support from Sunnis and Shi'is alike, for whom opposition to the Saljuq power was a common factor.
On 10 February 1258, Baghdad was captured by the Mongols led by Hulegu, a grandson of Chingiz Khan (Genghis Khan), during the siege of Baghdad. Many quarters were ruined by fire, siege, or looting. The Mongols massacred most of the city's inhabitants, including the caliph Al-Musta'sim, and destroyed large sections of the city. The canals and dykes forming the city's irrigation system were also destroyed. During this time, in Baghdad, Christians and Shia were tolerated, while Sunnis were treated as enemies. The sack of Baghdad put an end to the Abbasid Caliphate, a blow from which the Islamic civilization never fully recovered. 
At this point, Baghdad was ruled by the Ilkhanate, a breakaway state of the Mongol Empire, ruling from Iran. In 1401, Baghdad was again sacked, by the Central Asian Turkic conqueror Timur ("Tamerlane"). When his forces took Baghdad, he spared almost no one, and ordered that each of his soldiers bring back two severed human heads. It became a provincial capital controlled by the Mongol Jalayirid (1400–1411), Turkic Kara Koyunlu (1411–1469), Turkic Ak Koyunlu (1469–1508), and the Iranian Safavid (1508–1534) dynasties.
Ottoman era (16th to 19th centuries).
In 1534, Baghdad was captured by the Ottoman Turks. Under the Ottomans, Baghdad continued into a period of decline, partially as a result of the enmity between its rulers and Iranian Safavids, which did not accept the Sunni control of the city. Between 1623 and 1638, it returned to Iranian rule before falling back into Ottoman hands.
Baghdad has suffered severely from visitations of the plague and cholera, and sometimes two-thirds of its population has been wiped out.
For a time, Baghdad had been the largest city in the Middle East. The city saw relative revival in the latter part of the 18th century under a Mamluk government. Direct Ottoman rule was reimposed by Ali Rıza Pasha in 1831. From 1851 to 1852 and from 1861 to 1867, Baghdad was governed, under the Ottoman Empire by Mehmed Namık Pasha. The Nuttall Encyclopedia reports the 1907 population of Baghdad as 185,000.
20th and 21st centuries.
Baghdad and southern Iraq remained under Ottoman rule until 1917, when captured by the British during World War I. In 1920, Baghdad became the capital of the British Mandate of Mesopotamia and after receiving independence in 1932, the capital of the Kingdom of Iraq. The city's population grew from an estimated 145,000 in 1900 to 580,000 in 1950. During the Mandate, Baghdad's substantial Jewish community comprised a quarter of the city's population.
On 1 April 1941, members of the "Golden Square" and Rashid Ali staged a coup in Baghdad. Rashid Ali installed a pro-German and pro-Italian government to replace the pro-British government of Regent Abdul Ilah. On 31 May, after the resulting Anglo-Iraqi War and after Rashid Ali and his government had fled, the Mayor of Baghdad surrendered to British and Commonwealth forces.
On 14 July 1958, members of the Iraqi Army, under Abd al-Karim Qasim, staged a coup to topple the Kingdom of Iraq. King Faisal II, former Prime Minister Nuri as-Said, former Regent Prince 'Abd al-Ilah, members of the royal family, and others were brutally killed during the coup. Many of the victim's bodies were then dragged through the streets of Baghdad.
During the 1970s, Baghdad experienced a period of prosperity and growth because of a sharp increase in the price of petroleum, Iraq's main export. New infrastructure including modern sewerage, water, and highway facilities were built during this period. The masterplans of the city (1967, 1973) were delivered by the Polish planning office Miastoprojekt-Kraków, mediated by Polservice. However, the Iran–Iraq War of the 1980s was a difficult time for the city, as money was diverted by Saddam Hussein to the army and thousands of residents were killed. Iran launched a number of missile attacks against Baghdad in retaliation for Saddam Hussein's continuous bombardments of Tehran's residential districts.
In 1991 and 2003, the Gulf War and the 2003 invasion of Iraq caused significant damage to Baghdad's transportation, power, and sanitary infrastructure as the US-led coalition forces launched massive aerial assaults in the city in the two wars. Also in 2003, the minor riot in the city (which took place on July 21) caused some disturbance in the population.
The historic "Assyrian Quarter" of the city, Dora, which boasted a population of 150,000 Assyrians in 2003, made up over 3% of the capital's Assyrian population then. The community has been subject to kidnappings, death threats, vandalism, and house burnings. As of the end of 2014, only 1,500 Assyrians remained in Dora.
Main sights.
Points of interest include the National Museum of Iraq whose priceless collection of artifacts was looted during the 2003 invasion, and the iconic Hands of Victory arches. Multiple Iraqi parties are in discussions as to whether the arches should remain as historical monuments or be dismantled. Thousands of ancient manuscripts in the National Library were destroyed under Saddam's command.
Mutanabbi Street.
Mutanabbi Street (Arabic: شارع المتنبي) is located near the old quarter of Baghdad; at Al Rasheed Street. It is the historic center of Baghdad bookselling, a street filled with bookstores and outdoor book stalls. It was named after the 10th-century classical Iraqi poet Al-Mutanabbi. This street is well established for bookselling and has often been referred to as the heart and soul of the Baghdad literacy and intellectual community.
Baghdad Zoo.
The Baghdad Zoo was the largest zoo in the Middle East. Within eight days following the 2003 invasion, however, only 35 of the 650 animals in the facility survived. This was a result of theft of some animals for human food, and starvation of caged animals that had no food. South African Lawrence Anthony and some of the zoo keepers cared for the animals and fed the carnivores with donkeys they had bought locally. Eventually, L. Paul Bremer, Director of the Coalition Provisional Authority in Iraq from May 11, 2003 to June 28, 2004 ordered protection of the zoo and U.S. engineers helped to reopen the facility.
Kadhimiya mosque.
The Al-Kādhimiya Mosque is a shrine that is located in the Kādhimayn suburb of Baghdad, Iraq. It contains the tombs of the seventh Twelver Shīa Imām Musa al-Kadhim and the ninth Twelver Shīa Imām Muhammad at-Taqī al-Jawād. Many Shias travel to the mosque from far away places to commemorate.
Geography.
The city is located on a vast plain bisected by the River Tigris. The Tigris splits Baghdad in half, with the eastern half being called 'Risafa' and the Western half known as 'Karkh'. The land on which the city is built is almost entirely flat and low-lying, being of alluvial origin due to the periodic large floods which have occurred on the river.
Climate.
Baghdad has a subtropical desert climate (Köppen climate classification "BWh") and is one of the hottest cities in the world. In the summer from June to August, the average maximum temperature is as high as accompanied by blazing sunshine: rainfall has in fact been recorded on fewer than half a dozen occasions at this time of year and has never exceeded . Even at night temperatures in summer are seldom below . Baghdad's record highest temperature of 124 degrees Fahrenheit (51 degrees Celsius) was reached in July 2015. The humidity is typically very low (under 10%) due to Baghdad's distance from the marshy Persian Gulf, and dust storms from the deserts to the west are a normal occurrence during the summer.
Winters boast mild days and chilly nights. From December to February, Baghdad has maximum temperatures averaging , though highs above are not unheard of. Morning temperatures can be chilly: the average January low is but lows below freezing only occur a couple of times per year.
Annual rainfall, almost entirely confined to the period from November to March, averages around , but has been as high as and as low as . On January 11, 2008, light snow fell across Baghdad for the first time in memory.
Administrative divisions.
Administratively, Baghdad Governorate is divided into districts which are further divided into sub-districts. Municipally, the governorate is divided into 9 municipalities, which have responsibility for local issues. Regional services, however, are coordinated and carried out by a mayor who oversees the municipalities. There is no single city council that singularly governs Baghdad at a municipal level. The governorate council is responsible for the governorate-wide policy.
These official subdivisions of the city served as administrative centres for the delivery of municipal services but until 2003 had no political function. Beginning in April 2003, the U.S. controlled Coalition Provisional Authority (CPA) began the process of creating new functions for these. The process initially focused on the election of neighbourhood councils in the official neighbourhoods, elected by neighbourhood caucuses.
The CPA convened a series of meetings in each neighbourhood to explain local government, to describe the caucus election process and to encourage participants to spread the word and bring friends, relatives and neighbours to subsequent meetings. Each neighbourhood process ultimately ended with a final meeting where candidates for the new neighbourhood councils identified themselves and asked their neighbours to vote for them.
Once all 88 (later increased to 89) neighbourhood councils were in place, each neighbourhood council elected representatives from among their members to serve on one of the city's nine district councils. The number of neighbourhood representatives on a district council is based upon the neighbourhood's population. The next step was to have each of the nine district councils elect representatives from their membership to serve on the 37 member Baghdad City Council. This three tier system of local government connected the people of Baghdad to the central government through their representatives from the neighbourhood, through the district, and up to the city council.
The same process was used to provide representative councils for the other communities in Baghdad Province outside of the city itself. There, local councils were elected from 20 neighbourhoods (Nahia) and these councils elected representatives from their members to serve on six district councils (Qada). As within the city, the district councils then elected representatives from among their members to serve on the 35 member Baghdad Regional Council.
The first step in the establishment of the system of local government for Baghdad Province was the election of the Baghdad Provincial Council. As before, the representatives to the Provincial Council were elected by their peers from the lower councils in numbers proportional to the population of the districts they represent. The 41 member Provincial Council took office in February, 2004 and served until national elections held in January 2005, when a new Provincial Council was elected.
This system of 127 separate councils may seem overly cumbersome; however, Baghdad Province is home to approximately seven million people. At the lowest level, the neighbourhood councils, each council represents an average of 75,000 people.
The nine District Advisory Councils (DAC) are as follows:
The nine districts are subdivided into 89 smaller neighborhoods which may make up sectors of any of the districts above. The following is a "selection" (rather than a complete list) of these neighborhoods:
Economy.
Baghdad accounts for 20 per cent of Iraq's population and 40 per cent of her gross domestic product (PPP). Iraqi Airways, the national airline of Iraq, has its headquarters on the grounds of Baghdad International Airport in Baghdad. Al-Naser Airlines has its head office in Karrada, Baghdad.
Reconstruction efforts.
Most Iraqi reconstruction efforts have been devoted to the restoration and repair of badly damaged urban infrastructure. More visible efforts at reconstruction through private development, like architect and urban designer Hisham N. Ashkouri's Baghdad Renaissance Plan and the Sindbad Hotel Complex and Conference Center have also been made.
The Baghdad Eye, a tall Ferris wheel, was proposed for Baghdad in August 2008. At that time, three possible locations had been identified, but no estimates of cost or completion date were given. In October 2008, it was reported that Al-Zawraa Park was expected to be the site, and a wheel was installed there in March 2011.
Iraq's Tourism Board is also seeking investors to develop a "romantic" island on the River Tigris in Baghdad that was once a popular honeymoon spot for newlywed Iraqis. The project would include a six-star hotel, spa, an 18-hole golf course and a country club. In addition, the go-ahead has been given to build numerous architecturally unique skyscrapers along the Tigris that would develop the city's financial centre in Kadhehemiah. This project not only addresses the urgent need for new residential units in Baghdad but also acts as a real symbol of progress in the war torn city, as Baghdad has not seen projects of this scale for decades.
Housing.
In 2012, the Central Bank of Iraq signed a deal with Zaha Hadid Architects to build a tower which will be used as the bank's new headquarters.
Education.
The Mustansiriya Madrasah was established in 1227 by the Abbasid Caliph al-Mustansir. The name was changed to Al-Mustansiriya University in 1963. The University of Baghdad is the largest university in Iraq and the second largest in the Arab world.
Prior to the Gulf War multiple international schools operated in Baghdad, including:
Culture.
Baghdad has always played a significant role in the broader Arab cultural sphere, contributing several significant writers, musicians and visual artists. Famous Arab poets and singers such as Nizar Qabbani, Umm Kulthum, Fairuz, Salah Al-Hamdani, Ilham al-Madfai and others have performed for the city.
The dialect of Arabic spoken in Baghdad today differs from that of other large urban centres in Iraq, having features more characteristic of nomadic Arabic dialects (Verseegh, "The Arabic Language"). It is possible that this was caused by the repopulating of the city with rural residents after the multiple sacks of the late Middle Ages.
For poetry written about Baghdad, see Reuven Snir (ed.), "Baghdad: The City in Verse" (Harvard, 2013)[http://www.hup.harvard.edu/features/baghdad/]
Institutions.
Some of the important cultural institutions in the city include:
The live theatre scene received a boost during the 1990s, when UN sanctions limited the import of foreign films. As many as 30 movie theatres were reported to have been converted to live stages, producing a wide range of comedies and dramatic productions.
Institutions offering cultural education in Baghdad include The Music and Ballet School of Baghdad and the Institute of Fine Arts Baghdad. Baghdad is also home to a number of museums which housed artifacts and relics of ancient civilization; many of these were stolen, and the museums looted, during the widespread chaos immediately after United States forces entered the city.
During the 2003 occupation of Iraq, AFN Iraq ("Freedom Radio") broadcast news and entertainment within Baghdad, among other locations. There is also a private radio station called "Dijlah" (named after the Arabic word for the Tigris River) that was created in 2004 as Iraq's first independent talk radio station. Radio Dijlah offices, in the Jamia neighborhood of Baghdad, have been attacked on several occasions.
Sport.
Baghdad is home to some of the most successful football (soccer) teams in Iraq, the biggest being Al-Shorta (Police), Al-Quwa Al-Jawiya (Airforce club), Al-Zawra'a, and Talaba (Students). The largest stadium in Baghdad is Al-Shaab Stadium, which was opened in 1966. Another, but much larger stadium, is still in the opening stages of construction.
The city has also had a strong tradition of horse racing ever since World War I, known to Baghdadis simply as 'Races'. There are reports of pressures by the Islamists to stop this tradition due to the associated gambling.
Further reading.
Books:

</doc>
<doc id="4493" url="https://en.wikipedia.org/wiki?curid=4493" title="Outline of biology">
Outline of biology

The following outline is provided as an overview of and topical guide to biology:
Biology – study of living organisms. It is concerned with the characteristics, classification, and behaviors of organisms, how species come into existence, and the interactions they have with each other and with the environment. Biology encompasses a broad spectrum of academic fields that are often viewed as independent disciplines. However, together they address phenomena related to living organisms (biological phenomena) over a wide range of scales, from biophysics to ecology. All concepts in biology are subject to the same laws that other branches of science obey, such as the laws of thermodynamics and conservation of energy.
Branches of biology.
Branch of biology – subdiscipline of biology, also referred to as a biological science. Note that biology and all of its branches are also life sciences.
Interdisciplines:
History of biology.
History of biology

</doc>
<doc id="4495" url="https://en.wikipedia.org/wiki?curid=4495" title="British thermal unit">
British thermal unit

The British thermal unit (BTU or Btu) is a traditional unit of work equal to about 1055 joules. It is the amount of work needed to raise the temperature of one pound of water by one degree Fahrenheit One four-inch wooden kitchen match consumed completely generates approximately 1 BTU. In science, the joule, the SI unit of energy, has largely replaced the BTU.
The BTU/h is most often used as a measure of power in the electric power, steam generation, heating, and air conditioning industries. It is still used in some metric English-speaking countries (such as Canada, but notably not the United Kingdom). In North America, the heat value (energy content) of fuels is often expressed in BTUs.
The notation kBtu or KBTU is often used for thousand BTU, in sizing of heating systems and in the Energy Use Index (EUI) expressed as thousand BTU annual energy use per square foot of building. MBTU represents one million Btu, although the atypical notation MMBtu or mmBtu is sometimes used to represent one million BTU.(see definitions below)
Definitions.
A BTU is the amount of heat required to raise the temperature of 1 avoirdupois pound of liquid water by 1 degree Fahrenheit at a constant pressure of one atmosphere. As with the calorie, several definitions of the BTU exist, because the temperature response of water to heat energy is non-linear. This means that the change in temperature of a water mass caused by adding a certain amount of heat to it will be a function of the water's initial temperature. Definitions of the BTU based on different water temperatures can therefore vary by up to 0.5%.
The unit MBtu or mBtu was defined as one thousand BTU, presumably from the Roman numeral system where "M" or "m" stands for one thousand (1,000). This notation is easily confused with the SI "mega- (M)" prefix, which denotes multiplication by a factor of one million (×106), or with the SI "milli- (m)" prefix, which denotes division by a factor of one thousand (×10−3).
To avoid confusion, some companies and engineers use the notation "MMBtu" or "mmBtu" to represent one million BTU (although, confusingly, MM in Roman numerals would traditionally represent 2,000) and in many contexts this form of notation is deprecated and discouraged in favour of the more modern SI prefixes. Alternatively, the term "therm" may be used to represent 100,000 (or 105) BTU, and "quad" for 1015 BTU. Some companies also use 'BtuE6' in order to reduce confusion between 103 BTU and 106 BTU.
Conversions.
One BTU is approximately:
A BTU can be approximated as the heat produced by burning a single wooden kitchen match or as the amount of energy it takes to lift a one-pound weight .
As a unit of power.
When used as a unit of power for heating and cooling systems, BTU "per hour" (BTU/h) is the correct unit, though this is often abbreviated to just "BTU".
Associated units.
The BTU should not be confused with the Board of Trade Unit (B.O.T.U.), which is a much larger quantity of energy ().
The BTU is often used to express the conversion-efficiency of heat into electrical energy in power plants. Figures are quoted in terms of the quantity of heat in BTU required to generate 1 kW·h of electrical energy. A typical coal-fired power plant works at 10,500 BTU/kW·h, an efficiency of 32–33%.

</doc>
<doc id="4497" url="https://en.wikipedia.org/wiki?curid=4497" title="Bugatti">
Bugatti

Automobiles Ettore Bugatti was a French car manufacturer of high-performance automobiles, founded in 1909 in the then German city of Molsheim, Alsace by Italian-born Ettore Bugatti. Bugatti cars were known for their design beauty (Ettore Bugatti was from a family of artists and considered himself to be both an artist and constructor) and for their many race victories. Famous Bugattis include the Type 35 Grand Prix cars, the Type 41 "Royale", the Type 57 "Atlantic" and the Type 55 sports car.
The death of Ettore Bugatti in 1947 proved to be the end for the marque, and the death of his son Jean Bugatti in 1939 ensured there was not a successor to lead the factory. No more than about 8,000 cars were made. The company struggled financially, and released one last model in the 1950s, before eventually being purchased for its airplane parts business in the 1960s. In the 1990s, an Italian entrepreneur revived it as a builder of limited production exclusive sports cars. Today, the name is owned by German automobile manufacturing group Volkswagen.
Under Ettore Bugatti.
Founder Ettore Bugatti was born in Milan, Italy, and the automobile company that bears his name was founded in 1909 in Molsheim located in the Alsace region which was part of the German Empire from 1871 to 1919. The company was known both for the level of detail of its engineering in its automobiles, and for the artistic way in which the designs were executed, given the artistic nature of Ettore's family (his father, Carlo Bugatti (1856–1940), was an important Art Nouveau furniture and jewelry designer).
World War I and its aftermath.
During the war Ettore Bugatti was sent away, initially to Milan and later to Paris, but as soon as hostilities had been concluded he returned to his factory at Molsheim. Less than four months after the Versailles Treaty formalised the transfer of Alsace from Germany to France, Bugatti was able to obtain, at the last minute, a stand at the 15th Paris motor show in October 1919. He exhibited three light cars, all of them closely based on their pre-war equivalents, and each fitted with the same overhead camshaft 4-cylinder 1,368cc engine with four valves per cylinder. Smallest of the three was a "Type 13" with a racing body (constructed by Bugatti themselves) and using a chassis with a wheelbase. The others were a "Type 22" and a "Type 23" with wheelbases of respectively.
Racing successes.
The company also enjoyed great success in early Grand Prix motor racing: in 1929 a privately entered Bugatti won the first ever Monaco Grand Prix. Racing success culminated with driver Jean-Pierre Wimille winning the 24 hours of Le Mans twice (in 1937 with Robert Benoist and 1939 with Pierre Veyron).
Bugatti cars were extremely successful in racing. The little Bugatti Type 10 swept the top four positions at its first race. The 1924 Bugatti Type 35 is probably the most successful racing car of all time, with over 2,000 wins. The Type 35 was developed by Bugatti with master engineer and racing driver Jean Chassagne who also drove it in the car’s first ever Grand Prix in 1924 Lyon. Bugattis swept to victory in the Targa Florio for five years straight from 1925 through 1929. Louis Chiron held the most podiums in Bugatti cars, and the modern marque revival Bugatti Automobiles S.A.S. named the 1999 Bugatti 18/3 Chiron concept car in his honour. But it was the final racing success at Le Mans that is most remembered—Jean-Pierre Wimille and Pierre Veyron won the 1939 race with just one car and meagre resources.
Aeroplane racing.
In the 1930s, Ettore Bugatti got involved in the creation of a racer airplane, hoping to beat the Germans in the Deutsch de la Meurthe prize. This would be the Bugatti 100P, which never flew. It was designed by Belgian engineer Louis de Monge who had already applied Bugatti Brescia engines in his "Type 7.5" lifting body.
Railcar.
Ettore Bugatti also designed a successful motorised railcar, the "Autorail" (Autorail Bugatti).
Family tragedy.
The death of Ettore Bugatti's son, Jean Bugatti, on 11 August 1939 marked a turning point in the company's fortunes. Jean died while testing a Type 57 tank-bodied race car near the Molsheim factory.
After World War II.
World War II left the Molsheim factory in ruins and the company lost control of the property. During the war, Bugatti planned a new factory at Levallois, a northwestern suburb of Paris. After the war, Bugatti designed and planned to build a series of new cars, including the Type 73 road car and Type 73C single seat racing car, but in all Bugatti built only five Type 73 cars.
Development of a 375 cc supercharged car was stopped when Ettore Bugatti died on 21 August 1947. Following Ettore Bugatti's death, the business declined further and made its last appearance as a business in its own right at a Paris Motor Show in October 1952.
After a long decline, the original incarnation of Bugatti ceased operations in 1952.
Design.
Bugattis are noticeably focused on design. Engine blocks were hand scraped to ensure that the surfaces were so flat that gaskets were not required for sealing, many of the exposed surfaces of the engine compartment featured "guilloché" (engine turned) finishes on them, and safety wires had been threaded through almost every fastener in intricately laced patterns. Rather than bolt the springs to the axles as most manufacturers did, Bugatti's axles were forged such that the spring passed though a carefully sized opening in the axle, a much more elegant solution requiring fewer parts. He famously described his arch competitor Bentley's cars as "the world's fastest lorries" for focusing on durability. According to Bugatti, "weight was the enemy".
Gallery.
Notable finds in the modern era.
Relatives of Harold Carr found a rare 1937 Bugatti Type 57S Atalante when cataloguing the doctor's belongings after his death in 2009. Carr's Type 57S is notable because it was originally owned by British race car driver Earl Howe. Because much of the car's original equipment is intact, it can be restored without relying on replacement parts.
On 10 July 2009, a 1925 Bugatti Brescia Type 22 which had lain at the bottom of Lake Maggiore on the border of Switzerland and Italy for 75 years was recovered from the lake. The Mullin Museum in Oxnard, California bought it at auction for $351,343 at Bonham's Rétromobile sale in Paris in 2010.
Attempts at revival.
The company attempted a comeback under Roland Bugatti in the mid-1950s with the mid-engined Type 251 race car. Designed with help from Gioacchino Colombo, the car failed to perform to expectations and the company's attempts at automobile production were halted.
In the 1960s, Virgil Exner designed a Bugatti as part of his "Revival Cars" project. A show version of this car was actually built by Ghia using the last Bugatti Type 101 chassis, and was shown at the 1965 Turin Motor Show. Finance was not forthcoming, and Exner then turned his attention to a revival of Stutz.
Bugatti continued manufacturing airplane parts and was sold to Hispano-Suiza, also a former auto maker turned aircraft supplier, in 1963. Snecma took over Hispano-Suiza in 1968. After acquiring Messier, Snecma merged Messier and Bugatti into Messier-Bugatti in 1977.
Modern revivals.
Bugatti Automobili S.p.A. (1987–1995).
Italian entrepreneur Romano Artioli acquired the Bugatti brand in 1987, and established Bugatti Automobili S.p.A.. Artioli commissioned architect Giampaolo Benedini to design the factory which was built in Campogalliano, Modena, Italy. Construction of the plant began in 1988, alongside the development of the first model, and it was inaugurated two years later—in 1990.
By 1989 the plans for the new Bugatti revival were presented by Paolo Stanzani and Marcello Gandini, designers of the Lamborghini Miura and Lamborghini Countach. The first production vehicle was the Bugatti EB110 GT. It used a carbon-fibre-reinforced polymer chassis, a 3.5-litre, 5-valve per cylinder, quad-turbocharged 60° V12 engine, a six-speed gearbox, and four-wheel drive.
Famed racing car designer Mauro Forghieri served as Bugatti's technical director from 1992 through 1994.
On 27 August 1993, through his holding company, ACBN Holdings S.A. of Luxembourg, Romano Artioli purchased Lotus Cars from General Motors. Plans were made to list Bugatti shares on international stock exchanges.
Bugatti presented a prototype large saloon called the EB112 in 1993.
Perhaps the most famous Bugatti EB110 owner was seven-time Formula One World Champion racing driver Michael Schumacher who purchased an EB110 in 1994. Schumacher sold his EB110, which had been repaired after a severe 1994 crash, to Modena Motorsport, a Ferrari service and race preparation garage in Germany.
By the time the EB110 came to market, the North American and European economies were in recession. Poor economic conditions forced the company to fail and operations ceased in September 1995. A model specific to the US market called the "Bugatti America" was in the preparatory stages when the company ceased operations.
Bugatti's liquidators sold Lotus Cars to Proton of Malaysia. German firm Dauer Racing purchased the EB110 licence and remaining parts stock in 1997 in order to produce five more EB110 SS vehicles. These five SS versions of the EB110 were greatly refined by Dauer. The Campogalliano factory was sold to a furniture-making company, which subsequently collapsed before moving in, leaving the building unoccupied. After Dauer stopped producing cars in 2011, Toscana-Motors GmbH of Germany purchased the remaining parts stock from Dauer.
Bugatti Automobiles S.A.S. (1998–present).
Volkswagen AG acquired the Bugatti brand in 1998.
Bugatti Automobiles S.A.S. commissioned Giorgetto Giugiaro of ItalDesign to produce Bugatti Automobiles's first concept vehicle, the EB118, a coupé that debuted at the 1998 Paris Auto Show. The EB118 concept featured a , W-18 engine. After its Paris debut, the EB118 concept was shown again in 1999 at the Geneva Auto Show and the Tokyo Motor Show.
Bugatti introduced its next concepts, the EB 218 at the 1999 Geneva Motor Show and the 18/3 Chiron at the 1999 Frankfurt Motor Show (IAA).
Bugatti Automobiles S.A.S. began assembling its first regular-production vehicle, the Bugatti Veyron 16.4 (the 1001 BHP super car with an 8-litre W-16 engine with four turbochargers) in September 2005 at the Bugatti Molsheim, France assembly "studio". On 23 February 2015, Bugatti sold its last Veyron Grand Sport Vitesse, which was named La Finale.

</doc>
<doc id="4498" url="https://en.wikipedia.org/wiki?curid=4498" title="Benchmark">
Benchmark

Benchmark may refer to:

</doc>
<doc id="4499" url="https://en.wikipedia.org/wiki?curid=4499" title="Band">
Band

Band or BAND may refer to:

</doc>
<doc id="4501" url="https://en.wikipedia.org/wiki?curid=4501" title="Black Death">
Black Death

The Black Death was one of the most devastating pandemics in human history, resulting in the deaths of an estimated people and peaking in Europe in the years 1346–53. Although there were several competing theories as to the etiology of the Black Death, analysis of DNA from victims in northern and southern Europe published in 2010 and 2011 indicates that the pathogen responsible was the "Yersinia pestis" bacterium, probably causing several forms of plague.
The Black Death is thought to have originated in the arid plains of Central Asia, where it then travelled along the Silk Road, reaching Crimea by 1343. From there, it was most likely carried by Oriental rat fleas living on the black rats that were regular passengers on merchant ships. Spreading throughout the Mediterranean and Europe, the Black Death is estimated to have killed 30–60% of Europe's total population. In total, the plague reduced the world population from an estimated 450 million down to 350–375 million in the 14th century. The world population as a whole did not recover to pre-plague levels until the 17th century. The plague recurred occasionally in Europe until the 19th century.
The plague created a series of religious, social, and economic upheavals, which had profound effects on the course of European history.
Chronology.
Origins of the disease.
The plague disease, caused by "Yersinia pestis", is enzootic (commonly present) in populations of fleas carried by ground rodents, including marmots, in various areas including Central Asia, Kurdistan, Western Asia, Northern India and Uganda. Nestorian graves dating to 1338–39 near Lake Issyk Kul in Kyrgyzstan have inscriptions referring to plague and are thought by many epidemiologists to mark the outbreak of the epidemic, from which it could easily have spread to China and India. In October 2010, medical geneticists suggested that all three of the great waves of the plague originated in China. In China, the 13th century Mongol conquest caused a decline in farming and trading. However, economic recovery had been observed at the beginning of the 14th century. In the 1330s a large number of natural disasters and plagues led to widespread famine, starting in 1331, with a deadly plague arriving soon after. Epidemics that may have included plague killed an estimated 25 million Chinese and other Asians during the 15 years before it reached Constantinople in 1347.
The disease may have travelled along the Silk Road with Mongol armies and traders or it could have come via ship. By the end of 1346, reports of plague had reached the seaports of Europe: "India was depopulated, Tartary, Mesopotamia, Syria, Armenia were covered with dead bodies".
Plague was reportedly first introduced to Europe via Genoese traders at the port city of Kaffa in the Crimea in 1347. After a protracted siege, during which the Mongol army under Jani Beg was suffering from the disease, the army catapulted the infected corpses over the city walls of Kaffa to infect the inhabitants. The Genoese traders fled, taking the plague by ship into Sicily and the south of Europe, whence it spread north. Whether or not this hypothesis is accurate, it is clear that several existing conditions such as war, famine, and weather contributed to the severity of the Black Death.
European outbreak.
There appear to have been several introductions into Europe. The plague reached Sicily in October 1347, carried by twelve Genoese galleys, and rapidly spread all over the island. Galleys from Kaffa reached Genoa and Venice in January 1348, but it was the outbreak in Pisa a few weeks later that was the entry point to northern Italy. Towards the end of January, one of the galleys expelled from Italy arrived in Marseille.
From Italy, the disease spread northwest across Europe, striking France, Spain, Portugal and England by June 1348, then turned and spread east through Germany and Scandinavia from 1348 to 1350. It was introduced in Norway in 1349 when a ship landed at Askøy, then spread to Bjørgvin (modern Bergen) and Iceland. Finally it spread to northwestern Russia in 1351. The plague was somewhat less common in parts of Europe that had smaller trade relations with their neighbours, including the Kingdom of Poland, the majority of the Basque Country, isolated parts of Belgium and the Netherlands, and isolated alpine villages throughout the continent.
Modern researchers do not think that the plague ever became endemic in Europe or its rat population. The disease repeatedly wiped out the rodent carriers so that the fleas died out until a new outbreak from Central Asia repeated the process. The outbreaks have been shown to occur roughly 15 years after a warmer and wetter period in areas where plague is endemic in other species such as gerbils.
Middle Eastern outbreak.
The plague struck various countries in the Middle East during the pandemic, leading to serious depopulation and permanent change in both economic and social structures. As it spread to western Europe, the disease entered the region from southern Russia also. By autumn 1347, the plague reached Alexandria in Egypt, probably through the port's trade with Constantinople, and ports on the Black Sea. During 1347, the disease travelled eastward to Gaza, and north along the eastern coast to cities in Lebanon, Syria and Palestine, including Ashkelon, Acre, Jerusalem, Sidon, Damascus, Homs, and Aleppo. In 1348–49, the disease reached Antioch. The city's residents fled to the north, most of them dying during the journey, but the infection had been spread to the people of Asia Minor.
Mecca became infected in 1349. During the same year, records show the city of Mawsil (Mosul) suffered a massive epidemic, and the city of Baghdad experienced a second round of the disease. In 1351 Yemen experienced an outbreak of the plague, coinciding with the return of Sultan al-Mujahid Ali of Yemen from imprisonment in Cairo. His party may have brought the disease with them from Egypt.
Symptoms.
The only medical detail that is questionable is the infallibility of approaching death, as if the bubo discharges, recovery is possible.
This was followed by acute fever and vomiting of blood. Most victims died two to seven days after initial infection. Freckle-like spots and rashes, which could have been caused by flea-bites, were identified as another potential sign of the plague.
Some accounts, like that of Louis Heyligen, a musician in Avignon who died of the plague in 1348, noted a distinct form of the disease that infected the lungs and led to respiratory problems and is identified with pneumonic plague.
Naming.
The 12th-century French physician Gilles de Corbeil in his "De signis et sinthomatibus egritudinum" (On the signs and symptoms of diseases) used the phrase "atra mors" to refer to a pestilential fever ("febris pestilentialis") 
Writers contemporary to the plague referred to the event as the "Great Mortality", or the "Great Plague". The phrase "Black Death" ("mors nigra") was used in 1350 by Simon de Covino (or Couvin), a Belgian astronomer, who wrote the poem ""De judicio Solis in convivio Saturni"" (On the judgment of the Sun at a feast of Saturn) in which he attributed the plague to a conjunction of Jupiter and Saturn.
Gasquet (1908) claimed that the Latin name "atra mors" (Black Death) for the 14th-century epidemic first appeared in modern times in 1631 in a book on Danish history by J.I. Pontanus: ""Vulgo & ab effectu "atram mortem" vocatibant." ("Commonly and from its effects, they called it the black death"). The name spread through Scandinavia and then Germany, gradually becoming attached to the mid 14th-century epidemic as a proper name. In England, it was not until 1823 that the medieval epidemic was first called the Black Death.
Causes.
Medical knowledge had stagnated during the Middle Ages. The most authoritative account at the time came from the medical faculty in Paris in a report to the king of France that blamed the heavens, in the form of a conjunction of three planets in 1345 that caused a "great pestilence in the air". This report became the first and most widely circulated of a series of plague tracts that sought to give advice to sufferers. That the plague was caused by bad air became the most widely accepted theory. Today, this is known as the Miasma theory. The word 'plague' had no special significance at this time, and only the recurrence of outbreaks during the Middle Ages gave it the name that has become the medical term.
The importance of hygiene was recognised only in the nineteenth century; until then it was common that the streets were filthy, with live animals of all sorts around and human parasites abounding. A transmissible disease will spread easily in such conditions. One development as a result of the Black Death was the establishment of the idea of quarantine in Dubrovnik in 1377 after continuing outbreaks.
The dominant explanation for the Black Death is the plague theory, which attributes the outbreak to "Yersinia pestis", also responsible for an epidemic that began in southern China in 1865, eventually spreading to India. The investigation of the pathogen that caused the 19th-century plague was begun by teams of scientists who visited Hong Kong in 1894, among whom was the French-Swiss bacteriologist Alexandre Yersin, after whom the pathogen was named "Yersinia pestis". The mechanism by which "Y. pestis" was usually transmitted was established in 1898 by Paul-Louis Simond and was found to involve the bites of fleas whose midguts had become obstructed by replicating "Y. pestis" several days after feeding on an infected host. This blockage results in starvation and aggressive feeding behaviour by the fleas, which repeatedly attempt to clear their blockage by regurgitation, resulting in thousands of plague bacteria being flushed into the feeding site, infecting the host. The bubonic plague mechanism was also dependent on two populations of rodents: one resistant to the disease, which act as hosts, keeping the disease endemic, and a second that lack resistance. When the second population dies, the fleas move on to other hosts, including people, thus creating a human epidemic.
The historian Francis Aidan Gasquet wrote about the 'Great Pestilence' in 1893 and suggested that "it would appear to be some form of the ordinary Eastern or bubonic plague". He was able to adopt the epidemiology of the bubonic plague for the Black Death for the second edition in 1908, implicating rats and fleas in the process, and his interpretation was widely accepted for other ancient and medieval epidemics, such as the Justinian plague that was prevalent in the Eastern Roman Empire from 541 to 700 CE.
Other forms of plague have been implicated by modern scientists. The modern bubonic plague has a mortality rate of 30–75% and symptoms including fever of , headaches, painful aching joints, nausea and vomiting, and a general feeling of malaise. Left untreated, of those that contract the bubonic plague, 80 percent die within eight days. Pneumonic plague has a mortality rate of 90 to 95 percent. Symptoms include fever, cough, and blood-tinged sputum. As the disease progresses, sputum becomes free flowing and bright red. Septicemic plague is the least common of the three forms, with a mortality rate near 100%. Symptoms are high fevers and purple skin patches (purpura due to disseminated intravascular coagulation). In cases of pneumonic and particularly septicemic plague, the progress of the disease is so rapid that there would often be no time for the development of the enlarged lymph nodes that were noted as buboes.
DNA evidence.
In October 2010, the open-access scientific journal "PLoS Pathogens" published a paper by a multinational team who undertook a new investigation into the role of "Yersinia pestis" in the Black Death following the disputed identification by Drancourt and Raoult in 1998. They assessed the presence of DNA/RNA with Polymerase Chain Reaction (PCR) techniques for "Y. pestis" from the tooth sockets in human skeletons from mass graves in northern, central and southern Europe that were associated archaeologically with the Black Death and subsequent resurgences. The authors concluded that this new research, together with prior analyses from the south of France and Germany, ". . . ends the debate about the etiology of the Black Death, and unambiguously demonstrates that "Y. pestis" was the causative agent of the epidemic plague that devastated Europe during the Middle Ages".
The study also found that there were two previously unknown but related clades (genetic branches) of the "Y. pestis" genome associated with medieval mass graves. These clades (which are thought to be extinct) were found to be ancestral to modern isolates of the modern "Y. pestis" strains "Y. p. orientalis" and "Y. p. medievalis", suggesting the plague may have entered Europe in two waves. Surveys of plague pit remains in France and England indicate the first variant entered Europe through the port of Marseille around November 1347 and spread through France over the next two years, eventually reaching England in the spring of 1349, where it spread through the country in three epidemics. Surveys of plague pit remains from the Dutch town of Bergen op Zoom showed the "Y. pestis" genotype responsible for the pandemic that spread through the Low Countries from 1350 differed from that found in Britain and France, implying Bergen op Zoom (and possibly other parts of the southern Netherlands) was not directly infected from England or France in 1349 and suggesting a second wave of plague, different from those in Britain and France, may have been carried to the Low Countries from Norway, the Hanseatic cities or another site.
The results of the Haensch study have since been confirmed and amended. Based on genetic evidence derived from Black Death victims in the East Smithfield burial site in England, Schuenemann et al. concluded in 2011 "that the Black Death in medieval Europe was caused by a variant of "Y. pestis" that may no longer exist." A study published in "Nature" in October 2011 sequenced the genome of "Y. pestis" from plague victims and indicated that the strain that caused the Black Death is ancestral to most modern strains of the disease.
DNA taken from 25 skeletons from the 14th century found in London have shown the plague is a strain of "Y. pestis" that is almost identical to that which hit Madagascar in 2013.
Alternative explanations.
The plague theory was first significantly challenged by the work of British bacteriologist J. F. D. Shrewsbury in 1970, who noted that the reported rates of mortality in rural areas during the 14th-century pandemic were inconsistent with the modern bubonic plague, leading him to conclude that contemporary accounts were exaggerations. In 1984 zoologist Graham Twigg produced the first major work to challenge the bubonic plague theory directly, and his doubts about the identity of the Black Death have been taken up by a number of authors, including Samuel K. Cohn, Jr. (2002 and 2013), David Herlihy (1997), and Susan Scott and Christopher Duncan (2001).
It is recognised that an epidemiological account of the plague is as important as an identification of symptoms, but researchers are hampered by the lack of reliable statistics from this period. Most work has been done on the spread of the plague in England, and even estimates of overall population at the start vary by over 100% as no census was undertaken between the time of publication of the Domesday Book and the year 1377. Estimates of plague victims are usually extrapolated from figures from the clergy.
In addition to arguing that the rat population was insufficient to account for a bubonic plague pandemic, sceptics of the bubonic plague theory point out that the symptoms of the Black Death are not unique (and arguably in some accounts may differ from bubonic plague); that transference via fleas in goods was likely to be of marginal significance; and that the DNA results may be flawed and might not have been repeated elsewhere, despite extensive samples from other mass graves. Other arguments include the lack of accounts of the death of rats before outbreaks of plague between the 14th and 17th centuries; temperatures that are too cold in northern Europe for the survival of fleas; that, despite primitive transport systems, the spread of the Black Death was much faster than that of modern bubonic plague; that mortality rates of the Black Death appear to be very high; that, while modern bubonic plague is largely endemic as a rural disease, the Black Death indiscriminately struck urban and rural areas; and that the pattern of the Black Death, with major outbreaks in the same areas separated by 5 to 15 years, differs from modern bubonic plague—which often becomes endemic for decades with annual flare-ups.
McCormick has suggested that earlier archaeologists were simply not interested in the "laborious" processes needed to discover rat remains. Walløe complains that all of these authors "take it for granted that Simond's infection model, black rat → rat flea → human, which was developed to explain the spread of plague in India, is the only way an epidemic of "Yersinia pestis" infection could spread", whilst pointing to several other possibilities. Similarly, Green has argued that greater attention is needed to the range of (especially non-commensal) animals that might be involved in the transmission of plague.
A variety of alternatives to the "Y. pestis" have been put forward. Twigg suggested that the cause was a form of anthrax, and Norman Cantor (2001) thought it may have been a combination of anthrax and other pandemics. Scott and Duncan have argued that the pandemic was a form of infectious disease that characterise as "hemorrhagic" plague similar to Ebola. Archaeologist Barney Sloane has argued that there is insufficient evidence of the extinction of a large number of rats in the archaeological record of the medieval waterfront in London and that the plague spread too quickly to support the thesis that the "Y. pestis" was spread from fleas on rats; he argues that transmission must have been person to person. However, no single alternative solution has achieved widespread acceptance. Many scholars arguing for the "Y. pestis" as the major agent of the pandemic suggest that its extent and symptoms can be explained by a combination of bubonic plague with other diseases, including typhus, smallpox and respiratory infections. In addition to the bubonic infection, others point to additional septicemic (a type of "blood poisoning") and pneumonic (an airborne plague that attacks the lungs before the rest of the body) forms of the plague, which lengthen the duration of outbreaks throughout the seasons and help account for its high mortality rate and additional recorded symptoms. In 2014, scientists with Public Health England announced the results of an examination of 25 bodies exhumed from the Clerkenwell area of London, as well as of wills registered in London during the period, which supported the pneumonic hypothesis.
Consequences.
Death toll.
There are no exact figures for the death toll; the rate varied widely by locality. In urban centers, the greater the population before the outbreak, the longer the duration of the period of abnormal mortality. It killed some people in Eurasia.
The most widely accepted estimate for the Middle East, including Iraq, Iran and Syria, during this time, is for a death rate of about a third. The Black Death killed about 40% of Egypt's population. Half of Paris's population of 100,000 people died. In Italy, the population of Florence was reduced from 110–120 thousand inhabitants in 1338 down to 50 thousand in 1351. At least 60% of the population of Hamburg and Bremen perished, and a similar percentage of Londoners may have died from the disease as well. Interestingly while contemporary reports account of mass burial pits being created in response to the large numbers of dead, recent scientific investigations of a burial pit in Central London found well-preserved individuals to be buried in isolated, evenly spaced graves, suggesting at least some pre-planning and Christian burials at this time. Before 1350, there were about 170,000 settlements in Germany, and this was reduced by nearly 40,000 by 1450. In 1348, the plague spread so rapidly that before any physicians or government authorities had time to reflect upon its origins, about a third of the European population had already perished. In crowded cities, it was not uncommon for as much as 50% of the population to die. The disease bypassed some areas, and the most isolated areas were less vulnerable to contagion. Monks and priests were especially hard hit since they cared for victims of the Black Death.
Persecutions.
Renewed religious fervor and fanaticism bloomed in the wake of the Black Death. Some Europeans targeted "various groups such as Jews, friars, foreigners, beggars, pilgrims", lepers, and Romani, thinking that they were to blame for the crisis. Lepers, and other individuals with skin diseases such as acne or psoriasis, were singled out and exterminated throughout Europe.
Because 14th-century healers were at a loss to explain the cause, Europeans turned to astrological forces, earthquakes, and the poisoning of wells by Jews as possible reasons for the plague's emergence. The governments of Europe had no apparent response to the crisis because no one knew its cause or how it spread. The mechanism of infection and transmission of diseases was little understood in the 14th century; many people believed only God's anger could produce such horrific displays.
There were many attacks against Jewish communities. In February 1349, the citizens of Strasbourg murdered 2,000 Jews. In August 1349, the Jewish communities in Mainz and Cologne were exterminated. By 1351, 60 major and 150 smaller Jewish communities had been destroyed.
Recurrence.
The plague repeatedly returned to haunt Europe and the Mediterranean throughout the 14th to 17th centuries. According to Biraben, the plague was present somewhere in Europe in every year between 1346 and 1671. The was particularly widespread in the following years: 1360–63; 1374; 1400; 1438–39; 1456–57; 1464–66; 1481–85; 1500–03; 1518–31; 1544–48; 1563–66; 1573–88; 1596–99; 1602–11; 1623–40; 1644–54; and 1664–67. Subsequent outbreaks, though severe, marked the retreat from most of Europe (18th century) and northern Africa (19th century). According to Geoffrey Parker, "France alone lost almost a million people to the plague in the epidemic of 1628–31."
In England, in the absence of census figures, historians propose a range of preincident population figures from as high as 7 million to as low as 4 million in 1300, and a postincident population figure as low as 2 million. By the end of 1350, the Black Death subsided, but it never really died out in England. Over the next few hundred years, further outbreaks occurred in 1361–62, 1369, 1379–83, 1389–93, and throughout the first half of the 15th century. An outbreak in 1471 took as much as 10–15% of the population, while the death rate of the plague of 1479–80 could have been as high as 20%. The most general outbreaks in Tudor and Stuart England seem to have begun in 1498, 1535, 1543, 1563, 1589, 1603, 1625, and 1636, and ended with the Great Plague of London in 1665.
In 1466, perhaps 40,000 people died of the plague in Paris. During the 16th and 17th centuries, the plague was present in Paris around 30 per cent of the time. The Black Death ravaged Europe for three years before it continued on into Russia, where the disease was present somewhere in the country 25 times between 1350 and 1490. Plague epidemics ravaged London in 1563, 1593, 1603, 1625, 1636, and 1665, reducing its population by 10 to 30% during those years. Over 10% of Amsterdam's population died in 1623–25, and again in 1635–36, 1655, and 1664. Plague occurred in Venice 22 times between 1361 and 1528. The plague of 1576–77 killed 50,000 in Venice, almost a third of the population. Late outbreaks in central Europe included the Italian Plague of 1629–1631, which is associated with troop movements during the Thirty Years' War, and the Great Plague of Vienna in 1679. Over 60% of Norway's population died in 1348–50. The last plague outbreak ravaged Oslo in 1654.
In the first half of the 17th century, a plague claimed some 1.7 million victims in Italy, or about 14% of the population. In 1656, the plague killed about half of Naples' 300,000 inhabitants. More than 1.25 million deaths resulted from the extreme incidence of plague in 17th-century Spain. The plague of 1649 probably reduced the population of Seville by half. In 1709–13, a plague epidemic that followed the Great Northern War (1700–21, Sweden v. Russia and allies) killed about 100,000 in Sweden, and 300,000 in Prussia. The plague killed two-thirds of the inhabitants of Helsinki, and claimed a third of Stockholm's population. Europe's last major epidemic occurred in 1720 in Marseille.
The Black Death ravaged much of the Islamic world. Plague was present in at least one location in the Islamic world virtually every year between 1500 and 1850. Plague repeatedly struck the cities of North Africa. Algiers lost 30 to 50 thousand inhabitants to it in 1620–21, and again in 1654–57, 1665, 1691, and 1740–42. Plague remained a major event in Ottoman society until the second quarter of the 19th century. Between 1701 and 1750, thirty-seven larger and smaller epidemics were recorded in Constantinople, and an additional thirty-one between 1751 and 1800. Baghdad has suffered severely from visitations of the plague, and sometimes two-thirds of its population has been wiped out.
Third plague pandemic.
The Third plague pandemic (1855–1859) started in China in the middle of the 19th century, spreading to all inhabited continents and killing 10 million people in India alone. Twelve plague outbreaks in Australia in 1900–25 resulted in well over 1,000 deaths, chiefly in Sydney. This led to the establishment of a Public Health Department there which undertook some leading-edge research on plague transmission from rat fleas to humans via the bacillus "Yersinia pestis".
The first North American plague epidemic was the San Francisco plague of 1900–04, followed by another outbreak in 1907–08.
</ref> From 1944 through 1993, 362 cases of human plague were reported in the United States; approximately 90% occurred in four western states: Arizona, California, Colorado, and New Mexico.
Modern treatment methods include insecticides, the use of antibiotics, and a plague vaccine. The plague bacterium could develop drug-resistance and again become a major health threat. One case of a drug-resistant form of the bacterium was found in Madagascar in 1995. A further outbreak in Madagascar was reported in November 2014.

</doc>
<doc id="4502" url="https://en.wikipedia.org/wiki?curid=4502" title="Biotechnology">
Biotechnology

Biotechnology is the use of living systems and organisms to develop or make products, or "any technological application that uses biological systems, living organisms or derivatives thereof, to make or modify products or processes for specific use" (UN Convention on Biological Diversity, Art. 2). Depending on the tools and applications, it often overlaps with the (related) fields of bioengineering, biomedical engineering, biomanufacturing, etc.
For thousands of years, humankind has used biotechnology in agriculture, food production, and medicine. The term is largely believed to have been coined in 1919 by Hungarian engineer Károly Ereky. In the late 20th and early 21st century, biotechnology has expanded to include new and diverse sciences such as genomics, recombinant gene techniques, applied immunology, and development of pharmaceutical therapies and diagnostic tests.
Definitions.
The wide concept of "biotech" or "biotechnology" encompasses a wide range of procedures for modifying living organisms according to human purposes, going back to domestication of animals, cultivation of plants, and "improvements" to these through breeding programs that employ artificial selection and hybridization. Modern usage also includes genetic engineering as well as cell and tissue culture technologies. The American Chemical Society defines biotechnology as the application of biological organisms, systems, or processes by various industries to learning about the science of life and the improvement of the value of materials and organisms such as pharmaceuticals, crops, and livestock. 
As per European Federation of Biotechnology, Biotechnology is the integration of natural science and organisms, cells, parts thereof, and molecular analogues for products and services.
Biotechnology also writes on the pure biological sciences (animal cell culture, biochemistry, cell biology, embryology, genetics, microbiology, and molecular biology). In many instances, it is also dependent on knowledge and methods from outside the sphere of biology including:
Conversely, modern biological sciences (including even concepts such as molecular ecology) are intimately entwined and heavily dependent on the methods developed through biotechnology and what is commonly thought of as the life sciences industry. Biotechnology is the research and development in the laboratory using bioinformatics for exploration, extraction, exploitation and production from any living organisms and any source of biomass by means of biochemical engineering where high value-added products could be planned (reproduced by biosynthesis, for example), forecasted, formulated, developed, manufactured and marketed for the purpose of sustainable operations (for the return from bottomless initial investment on R & D) and gaining durable patents rights (for exclusives rights for sales, and prior to this to receive national and international approval from the results on animal experiment and human experiment, especially on the pharmaceutical branch of biotechnology to prevent any undetected side-effects or safety concerns by using the products).
By contrast, bioengineering is generally thought of as a related field that more heavily emphasizes higher systems approaches (not necessarily the altering or using of biological materials "directly") for interfacing with and utilizing living things. Bioengineering is the application of the principles of engineering and natural sciences to tissues, cells and molecules. This can be considered as the use of knowledge from working with and manipulating biology to achieve a result that can improve functions in plants and animals. Relatedly, biomedical engineering is an overlapping field that often draws upon and applies "biotechnology" (by various definitions), especially in certain sub-fields of biomedical and/or chemical engineering such as tissue engineering, biopharmaceutical engineering, and genetic engineering.
History.
Although not normally what first comes to mind, many forms of human-derived agriculture clearly fit the broad definition of "'utilizing a biotechnological system to make products". Indeed, the cultivation of plants may be viewed as the earliest biotechnological enterprise.
Agriculture has been theorized to have become the dominant way of producing food since the Neolithic Revolution. Through early biotechnology, the earliest farmers selected and bred the best suited crops, having the highest yields, to produce enough food to support a growing population. As crops and fields became increasingly large and difficult to maintain, it was discovered that specific organisms and their by-products could effectively fertilize, restore nitrogen, and control pests. Throughout the history of agriculture, farmers have inadvertently altered the genetics of their crops through introducing them to new environments and breeding them with other plants — one of the first forms of biotechnology.
These processes also were included in early fermentation of beer. These processes were introduced in early Mesopotamia, Egypt, China and India, and still use the same basic biological methods. In brewing, malted grains (containing enzymes) convert starch from grains into sugar and then adding specific yeasts to produce beer. In this process, carbohydrates in the grains were broken down into alcohols such as ethanol. Later other cultures produced the process of lactic acid fermentation which allowed the fermentation and preservation of other forms of food, such as soy sauce. Fermentation was also used in this time period to produce leavened bread. Although the process of fermentation was not fully understood until Louis Pasteur's work in 1857, it is still the first use of biotechnology to convert a food source into another form.
Before the time of Charles Darwin's work and life, animal and plant scientists had already used selective breeding. Darwin added to that body of work with his scientific observations about the ability of science to change species. These accounts contributed to Darwin's theory of natural selection.
For thousands of years, humans have used selective breeding to improve production of crops and livestock to use them for food. In selective breeding, organisms with desirable characteristics are mated to produce offspring with the same characteristics. For example, this technique was used with corn to produce the largest and sweetest crops.
In the early twentieth century scientists gained a greater understanding of microbiology and explored ways of manufacturing specific products. In 1917, Chaim Weizmann first used a pure microbiological culture in an industrial process, that of manufacturing corn starch using "Clostridium acetobutylicum," to produce acetone, which the United Kingdom desperately needed to manufacture explosives during World War I.
Biotechnology has also led to the development of antibiotics. In 1928, Alexander Fleming discovered the mold "Penicillium". His work led to the purification of the antibiotic compound formed by the mold by Howard Florey, Ernst Boris Chain and Norman Heatley - to form what we today know as penicillin. In 1940, penicillin became available for medicinal use to treat bacterial infections in humans.
The field of modern biotechnology is generally thought of as having been born in 1971 when Paul Berg's (Stanford) experiments in gene splicing had early success. Herbert W. Boyer (Univ. Calif. at San Francisco) and Stanley N. Cohen (Stanford) significantly advanced the new technology in 1972 by transferring genetic material into a bacterium, such that the imported material would be reproduced. The commercial viability of a biotechnology industry was significantly expanded on June 16, 1980, when the United States Supreme Court ruled that a genetically modified microorganism could be patented in the case of "Diamond v. Chakrabarty". Indian-born Ananda Chakrabarty, working for General Electric, had modified a bacterium (of the "Pseudomonas" genus) capable of breaking down crude oil, which he proposed to use in treating oil spills. (Chakrabarty's work did not involve gene manipulation but rather the transfer of entire organelles between strains of the "Pseudomonas" bacterium.
Revenue in the industry is expected to grow by 12.9% in 2008. Another factor influencing the biotechnology sector's success is improved intellectual property rights legislation—and enforcement—worldwide, as well as strengthened demand for medical and pharmaceutical products to cope with an ageing, and ailing, U.S. population.
Rising demand for biofuels is expected to be good news for the biotechnology sector, with the Department of Energy estimating ethanol usage could reduce U.S. petroleum-derived fuel consumption by up to 30% by 2030. The biotechnology sector has allowed the U.S. farming industry to rapidly increase its supply of corn and soybeans—the main inputs into biofuels—by developing genetically modified seeds which are resistant to pests and drought. By boosting farm productivity, biotechnology plays a crucial role in ensuring that biofuel production targets are met.
Examples.
Biotechnology has applications in four major industrial areas, including health care (medical), crop production and agriculture, non food (industrial) uses of crops and other products (e.g. biodegradable plastics, vegetable oil, biofuels), and environmental uses.
For example, one application of biotechnology is the directed use of organisms for the manufacture of organic products (examples include beer and milk products). Another example is using naturally present bacteria by the mining industry in bioleaching. Biotechnology is also used to recycle, treat waste, clean up sites contaminated by industrial activities (bioremediation), and also to produce biological weapons.
A series of derived terms have been coined to identify several branches of biotechnology; for example:
The investment and economic output of all of these types of applied biotechnologies is termed as "bioeconomy".
Medicine.
In medicine, modern biotechnology finds applications in areas such as pharmaceutical drug discovery and production, pharmacogenomics, and genetic testing (or genetic screening).
Pharmacogenomics (a combination of pharmacology and genomics) is the technology that analyses how genetic makeup affects an individual's response to drugs. It deals with the influence of genetic variation on drug response in patients by correlating gene expression or single-nucleotide polymorphisms with a drug's efficacy or toxicity. By doing so, pharmacogenomics aims to develop rational means to optimize drug therapy, with respect to the patients' genotype, to ensure maximum efficacy with minimal adverse effects. Such approaches promise the advent of "personalized medicine"; in which drugs and drug combinations are optimized for each individual's unique genetic makeup.
Biotechnology has contributed to the discovery and manufacturing of traditional small molecule pharmaceutical drugs as well as drugs that are the product of biotechnology - biopharmaceutics. Modern biotechnology can be used to manufacture existing medicines relatively easily and cheaply. The first genetically engineered products were medicines designed to treat human diseases. To cite one example, in 1978 Genentech developed synthetic humanized insulin by joining its gene with a plasmid vector inserted into the bacterium "Escherichia coli". Insulin, widely used for the treatment of diabetes, was previously extracted from the pancreas of abattoir animals (cattle and/or pigs). The resulting genetically engineered bacterium enabled the production of vast quantities of synthetic human insulin at relatively low cost. Biotechnology has also enabled emerging therapeutics like gene therapy. The application of biotechnology to basic science (for example through the Human Genome Project) has also dramatically improved our understanding of biology and as our scientific knowledge of normal and disease biology has increased, our ability to develop new medicines to treat previously untreatable diseases has increased as well.
Genetic testing allows the genetic diagnosis of vulnerabilities to inherited diseases, and can also be used to determine a child's parentage (genetic mother and father) or in general a person's ancestry. In addition to studying chromosomes to the level of individual genes, genetic testing in a broader sense includes biochemical tests for the possible presence of genetic diseases, or mutant forms of genes associated with increased risk of developing genetic disorders. Genetic testing identifies changes in chromosomes, genes, or proteins. Most of the time, testing is used to find changes that are associated with inherited disorders. The results of a genetic test can confirm or rule out a suspected genetic condition or help determine a person's chance of developing or passing on a genetic disorder. As of 2011 several hundred genetic tests were in use. Since genetic testing may open up ethical or psychological problems, genetic testing is often accompanied by genetic counseling.
Agriculture.
Genetically modified crops ("GM crops", or "biotech crops") are plants used in agriculture, the DNA of which has been modified with genetic engineering techniques. In most cases the aim is to introduce a new trait to the plant which does not occur naturally in the species.
Examples in food crops include resistance to certain pests, diseases, stressful environmental conditions, resistance to chemical treatments (e.g. resistance to a herbicide), reduction of spoilage, or improving the nutrient profile of the crop. Examples in non-food crops include production of pharmaceutical agents, biofuels, and other industrially useful goods, as well as for bioremediation.
Farmers have widely adopted GM technology. Between 1996 and 2011, the total surface area of land cultivated with GM crops had increased by a factor of 94, from to 1,600,000 km2 (395 million acres). 10% of the world's crop lands were planted with GM crops in 2010. As of 2011, 11 different transgenic crops were grown commercially on 395 million acres (160 million hectares) in 29 countries such as the USA, Brazil, Argentina, India, Canada, China, Paraguay, Pakistan, South Africa, Uruguay, Bolivia, Australia, Philippines, Myanmar, Burkina Faso, Mexico and Spain.
Genetically modified foods are foods produced from organisms that have had specific changes introduced into their DNA with the methods of genetic engineering. These techniques have allowed for the introduction of new crop traits as well as a far greater control over a food's genetic structure than previously afforded by methods such as selective breeding and mutation breeding. Commercial sale of genetically modified foods began in 1994, when Calgene first marketed its Flavr Savr delayed ripening tomato. To date most genetic modification of foods have primarily focused on cash crops in high demand by farmers such as soybean, corn, canola, and cotton seed oil. These have been engineered for resistance to pathogens and herbicides and better nutrient profiles. GM livestock have also been experimentally developed, although as of November 2013 none are currently on the market.
There is a general scientific agreement that food from genetically modified crops is not inherently riskier to human health than conventional food, but should be tested on a case-by-case basis. GM crops also provide a number of ecological benefits, if not used in excess. However, opponents have objected to GM crops per se on several grounds, including environmental concerns, whether food produced from GM crops is safe, whether GM crops are needed to address the world's food needs, and economic concerns raised by the fact these organisms are subject to intellectual property law.
Industrial biotechnology.
Industrial biotechnology (known mainly in Europe as white biotechnology) is the application of biotechnology for industrial purposes, including industrial fermentation. It includes the practice of using cells such as micro-organisms, or components of cells like enzymes, to generate industrially useful products in sectors such as chemicals, food and feed, detergents, paper and pulp, textiles and biofuels. In doing so, biotechnology uses renewable raw materials and may contribute to lowering greenhouse gas emissions and moving away from a petrochemical-based economy.
Environmental biotechnology.
The environment can be affected by biotechnologies, both positively and adversely. Vallero and others have argued that the difference between beneficial biotechnology (e.g. bioremediation to clean up an oil spill or hazard chemical leak) versus the adverse effects stemming from biotechnological enterprises (e.g. flow of genetic material from transgenic organisms into wild strains) can be seen as applications and implications, respectively. Cleaning up environmental wastes is an example of an application of environmental biotechnology; whereas loss of biodiversity or loss of containment of a harmful microbe are examples of environmental implications of biotechnology.
Regulation.
The regulation of genetic engineering concerns approaches taken by governments to assess and manage the risks associated with the use of genetic engineering technology, and the development and release of genetically modified organisms (GMO), including genetically modified crops and genetically modified fish. There are differences in the regulation of GMOs between countries, with some of the most marked differences occurring between the USA and Europe. Regulation varies in a given country depending on the intended use of the products of the genetic engineering. For example, a crop not intended for food use is generally not reviewed by authorities responsible for food safety. The European Union differentiates between approval for cultivation within the EU and approval for import and processing. While only a few GMOs have been approved for cultivation in the EU a number of GMOs have been approved for import and processing. The cultivation of GMOs has triggered a debate about coexistence of GM and non GM crops. Depending on the coexistence regulations incentives for cultivation of GM crops differ.
Learning.
In 1988, after prompting from the United States Congress, the National Institute of General Medical Sciences (National Institutes of Health) (NIGMS) instituted a funding mechanism for biotechnology training. Universities nationwide compete for these funds to establish Biotechnology Training Programs (BTPs). Each successful application is generally funded for five years then must be competitively renewed. Graduate students in turn compete for acceptance into a BTP; if accepted, then stipend, tuition and health insurance support is provided for two or three years during the course of their Ph.D. thesis work. Nineteen institutions offer NIGMS supported BTPs. Biotechnology training is also offered at the undergraduate level and in community colleges.

</doc>
<doc id="4503" url="https://en.wikipedia.org/wiki?curid=4503" title="Battle of Poitiers">
Battle of Poitiers

The Battle of Poitiers was a major battle of the Hundred Years' War between England and France. The battle occurred on 19 September 1356 near Poitiers, France. Preceded by the Battle of Crécy in 1346, and followed by the Battle of Agincourt in 1415, it was the second of the three great English victories of the war.
Background.
Edward, Prince of Wales (later known as the Black Prince), the eldest son of King Edward III, began a great "chevauchée" on 8 August 1356. He conducted many scorched earth raids northwards from the English base in Aquitaine, in an effort to bolster his troops in central France, as well as to raid and ravage the countryside. His forces met little resistance, burning numerous towns to the ground and living off the land, until they reached the River Loire at Tours. They were unable to take the castle or burn the town due to a heavy downpour. This delay allowed John II, King of France, to attempt to catch Edward's army. The King, who had been besieging Breteuil in Normandy, arranged the bulk of his army at Chartres to the north of the besieged Tours, dismissing approximately 15,000–20,000 of his lower-quality infantry to increase the speed of his forces.
Negotiations prior to the Battle of Poitiers.
There were negotiations before the battle of Poitiers that are recorded in the writings of the life of Sir John Chandos. He records the final moments of a meeting of both sides in an effort to avoid the bloody conflict at Poitiers. The extraordinary narrative occurred just before that battle and reads as follows:
Nobles and men-at-arms who fought with the Black Prince.
Jean Froissart states these men fought at Poitiers: Thomas de Beauchamp, 11th Earl of Warwick, William de Ufford, 2nd Earl of Suffolk; William de Montacute, 2nd Earl of Salisbury, John de Vere, 7th Earl of Oxford, Reginald de Cobham, 1st Baron Cobham, Edward le Despencer, 1st Baron le Despencer, Lord James Audley, Lord Peter Audley (his brother), Lord Berkeley, Lord Basset, Lord Warin, Lord Delaware, Lord Manne, Lord Willoughby, Lord Bartholomew de Burghersh, Lord of Felton, Lord Richard of Pembroke, Lord Stephen of Cosington; Lord Bradetane and other Englishmen; Lord of Pommiers from Gascon, Lord of Languiran, the captal of Buch, Lord John the 69th of Caumont, Lord de Lesparre, Lord of Rauzan, Lord of Condon, Lord of Montferrand, the Lord of Landiras, Lord Soudic of Latrau and other (men-at-arms); from Hainowes, Lord Eustace d'Aubrecicourt; Lord John of Ghistelles, and two other strangers, Lord Daniel Pasele and Lord Denis of Amposta, a fortress in Catalonia.Edward le Despencer, 1st Baron le Despencer also fought at Poitiers under The Black Prince. Sir Thomas Felton fought not only at Poitiers but also at the Battle of Crécy.
One of the chief commanders at both Crécy and Poitiers was John de Vere, Earl of Oxford, mentioned above.
Another account states that John of Ghistelles perished at the Battle of Crécy so there is some ambiguity as to this man.
Nobles and men-at-arms who fought with King John II at, or just prior to, the battle.
Froissart describes, with less specificity in this passage, some of the nobles that were assembled at, or just prior to the Battle: The Englishmen were shadowed by some expert French knights, who reported back to the king what the Englishmen did. The king came to the Haye in Touraine and his men had passed the river Loire, some at the bridge of Orléans and some at Meung, at Saumur, at Blois, and at Tours and other places: there were twenty thousand men of arms and other soldiers. He estimates there were twenty-six dukes and earls (Counts), more than 120 banners, and the four sons of the king, Duke Charles of Normandy, the Duke Louis, the Duke of Anjou, and John, Duke of Berry, and Lord Philip.
The French army also included a contingent of Scots commanded by Sir William Douglas. Douglas fought in the King's own Battle, but when the fight seemed over Douglas was dragged by his men from the melee. Froissart states that "... the Earl Douglas of Scotland, who fought a season valiantly, but when he saw the discomfiture he departed and saved himself; for in no wise would he be taken by the Englishmen, he would rather there be slain".
Others who were either killed or captured in the battle were: King John II; Prince Philip (youngest son and progenitor of the House of Valois-Burgundy), Geoffroi de Charny, carrier of the Oriflamme, Peter I, Duke of Bourbon, Walter VI, Count of Brienne and Constable of France, Jean de Clermont, Marshal of France, Arnoul d'Audrehem, the Count of Eu, the Count of Marche and Ponthieu Jacques de Bourbon taken prisoner at the battle and died 1361, the Count of Étampes, the Count of Tancarville, the Count of Dammartin, the Count of Joinville, Guillaume de Melun, Archbishop of Sens.
The battle.
At the beginning of the battle, the English removed their baggage train leading the French to think they were about to retreat which provoked a hasty charge by the French knights against the archers. According to Froissart, the English attacked the enemy, especially the horses, with a shower of arrows. Geoffrey the Baker writes that the French armour was invulnerable to the English arrows, that the arrowheads either skidded off the armour or shattered on impact. Given the following actions of the archers, it seems likely Baker was correct. The armour on the horses was weaker on the sides and back, so the archers moved to the sides of the cavalry and shot the horses in the flanks. This was a popular method of stopping a cavalry charge, as a falling horse often destroyed the cohesion of the enemy's line. The results were devastating. The Dauphin attacked Salisbury and pressed his advance in spite of heavy shot by the English archers and complications of running into the retreating vanguard of Clermont's force. Green suggests that the Dauphin had thousands of troops with him in this phase of the attack. He advanced to the English lines but ultimately fell back. The French were unable to penetrate the protective hedge the English were using. This phase of the attack lasted about two hours.
This cavalry attack was followed by infantry attack. The Dauphin's infantry engaged in heavy fighting, but withdrew to regroup. The next wave of infantry under Orléans, seeing that the Dauphin's men were not attacking, turned back and panicked. This stranded the forces led by the King himself. This was a formidable fighting force, and the English archers were running very low on arrows; the archers joined the infantry in the fight and some of both groups mounted horses to form an improvised cavalry.
At about this time, King John sent two sons from the battlefield. His youngest son, Philip, stayed with him and fought at his side in the final phase of the battle. When the Dauphin and other sons withdrew, the duke of Orléans also withdrew. Combat was hard, but the Black Prince still had a mobile reserve hidden in the woods, commanded by Jean de Grailly, the Captal de Buch; which was able to circle around and attack the French in the flank and rear. The French were fearful of encirclement and attempted to flee. King John was captured with his immediate entourage only after a memorable resistance.
Amongst the notable captured or killed according to Froissart were:
The capture of the French king.
Froissart again gives us a vivid description of the capture of King Jean II and his youngest son in this passage:
Aftermath of the battle.
As Edward, the Black Prince, wrote shortly afterward in a letter to the people of London:
See also Ransom of King John II of France.
Aftermath in France.
Jean de Venette, a Carmelite friar and medieval chronicler vividly describes the chaos in France which he states he himself witnessed, after the time of this Battle. He states:

</doc>
<doc id="4505" url="https://en.wikipedia.org/wiki?curid=4505" title="Backbone cabal">
Backbone cabal

The backbone cabal was an informal organization of large-site administrators of the worldwide distributed newsgroup-based discussion system Usenet. It existed from about 1983 at least into the 2000s.
The cabal was created in an effort to facilitate reliable propagation of new Usenet posts: While in the 1970s and 1980s many news servers only operated during night time to save on the cost of long distance communication, servers of the backbone cabal were available 24 hours a day. The administrators of these servers gained sufficient influence in the otherwise anarchic Usenet community to be able to push through controversial changes, for instance the Great Renaming of Usenet newsgroups during 1987.
History.
As Usenet has few technologically or legally enforced hierarchies, just about the only ones that formed were social. People acquired power through persuasion (both publicly and privately), public debate, force of will (often via aggressive flames), garnering authority and respect by spending much time and effort contributing to the community (by being a maintainer of a FAQ, for example; see also Kibo, etc.).
Credit for organizing the backbone about 1983 is commonly attributed to Gene "Spaf" Spafford, although it is also claimed by Mary Ann Horton. Other prominent members of the cabal were Brian Reid, Richard Sexton, Chuq von Rospach, Neil Crellin and Rick Adams.
In Internet culture.
During most of its existence, the cabal (sometimes capitalized) steadfastly denied its own existence; those involved would often respond "There is no Cabal" (sometimes abbreviated as "TINC"), whenever the existence or activities of the group were speculated on in public. It is sometimes used humorously to dispel cabal-like organizational conspiracy theories, or as an ironic statement, indicating one who knows the existence of "the cabal" will invariably deny there is one.
This belief became a model for various conspiracy theories about various Cabals with dark nefarious objectives beginning with taking over Usenet or the Internet. Spoofs include the "Eric Conspiracy" of moustachioed hackers named "Eric"; ex-members of the P.H.I.R.M.; and the Lumber Cartel putatively funding anti-spam efforts to support the paper industry.
The result of this policy was an aura of mystery, even a decade after the cabal mailing list disbanded in late 1988 following an internal fight.

</doc>
<doc id="4506" url="https://en.wikipedia.org/wiki?curid=4506" title="Bongo (antelope)">
Bongo (antelope)

The bongo ("Tragelaphus eurycerus") is a herbivorous, mostly nocturnal forest ungulate. It is among the largest of the African forest antelope species.
Bongos are characterised by a striking reddish-brown coat, black and white markings, white-yellow stripes and long slightly spiralled horns. Indeed, bongos are the only tragelaphid in which both sexes have horns. They have a complex social interaction and are found in African dense forest mosaics.
The western or lowland bongo, "T. e. eurycerus", faces an ongoing population decline, and the IUCN Antelope Specialist Group considers it to be Near Threatened on the conservation status scale.
The eastern or mountain bongo, "T. e. isaaci", of Kenya, has a coat even more vibrant than that of "T. e. eurycerus". The mountain bongo is only found in the wild in one remote region of central Kenya. This bongo is classified by the IUCN Antelope Specialist Group as Critically Endangered, with more specimens in captivity than in the wild.
In 2000, the Association of Zoos and Aquariums in the USA (AZA) upgraded the bongo to a Species Survival Plan participant and in 2006 added the Bongo Restoration to Mount Kenya Project to its list of the Top Ten Wildlife Conservation Success Stories of the year. However, in 2013, it seems, these successes have been negated with reports of possibly only 100 mountain bongos left in the wild due to logging and poaching.
Taxonomy.
The scientific name of the bongo is "Tragelaphus eurycerus", and it belongs to the genus "Tragelaphus" and family Bovidae. It was first described by Irish naturalist William Ogilby in 1837. The generic name "Tragelaphus" is composed of two Greek words: "tragos", meaning a male goat; and "elaphos", meaning deer. The specific name "eurycerus" originated from the fusion of "eurus" (broad, widespread) and "keras" (an animal's horn). The common name "bongo" originated probably from the Kele language of Gabon. The first known use of the name "bongo" dates back to 1861.
Bongos are further classified into two subspecies: "T. e. eurycerus", the lowland or western bongo, and the far rarer "T. e. isaaci", the mountain or eastern bongo, restricted to the mountains of Kenya only. The eastern bongo is larger and heavier than the western bongo. Two other subspecies are described from West and Central Africa, but taxonomic clarification is required. They have been observed to live up to 19 years.
Distribution and habitat.
Bongos are found in tropical jungles with dense undergrowth up to an altitude of in Central Africa, with isolated populations in Kenya, and these West African countries: Cameroon, the Central African Republic, the Republic of the Congo, the Democratic Republic of Congo, the Ivory Coast, Equatorial Guinea, Gabon, Ghana, Guinea, Liberia, Sierra Leone, South Sudan.
Historically, bongos are found in three disjunct parts of Africa: East, Central and West. Today, all three populations’ ranges have shrunk in size due to habitat loss for agriculture and uncontrolled timber cutting, as well as hunting for meat.
Bongos favour disturbed forest mosaics that provide fresh, low-level green vegetation. Such habitats may be promoted by heavy browsing by elephants, fires, flooding, tree-felling (natural or by logging), and fallowing. Mass bamboo die-off provides ideal habitat in East Africa. They can live in bamboo forests.
Appearance.
Bongos are one of the largest of the forest antelopes. In addition to the deep chestnut colour of their coats, they have bright white stripes on their sides to help with camouflage.
Adults of both sexes are similar in size. Adult height is about at the shoulder and length is , including a tail of . Females weigh around , while males weigh about . Its large size puts it as the third-largest in the Bovidae tribe of Strepsicerotini, behind both the common and greater elands by about , and above the greater kudu by about .
Both sexes have heavy spiral horns; those of the male are longer and more massive. All bongos in captivity are from the isolated Aberdare Mountains of central Kenya.
Coat and body.
The bongo sports a bright auburn or chestnut coat, with the neck, chest, and legs generally darker than the rest of the body. Coats of male bongos become darker as they age until they reach a dark mahogany-brown colour. Coats of female bongos are usually more brightly coloured than those of males. The eastern bongo is darker in color than the western and this is especially pronounced in older makes which tend to be chestnut brown especially on the front part of their bodies.
The pigmentation in the coat rubs off quite easily; anecdotal reports suggest rain running off a bongo may be tinted red with pigment. The smooth coat is marked with 10–15 vertical white-yellow stripes, spread along the back from the base of the neck to the rump. The number of stripes on each side is rarely the same. It also has a short, bristly, and vertical brown ridge of hair along the spine from the shoulder to the rump; the white stripes run into this ridge.
A white chevron appears between the eyes, and two large white spots grace each cheek. Another white chevron occurs where the neck meets the chest. The large ears are to sharpen hearing, and the distinctive coloration may help bongos identify one another in their dark forest habitats. Bongos have no special secretion glands, so rely less on scent to find one another than do other similar antelopes. The lips of a bongo are white, topped with a black muzzle.
Horns.
Bongos have two heavy and slightly spiralled horns that slope over their backs, and like many other antelope species, both male and female bongos have horns. Bongos are the only tragelaphids in which both sexes have horns. The horns of bongos are in the form of a lyre and bear a resemblance to those of the related antelope species of nyalas, sitatungas, bushbucks, kudus and elands.
Unlike deer, which have branched antlers shed annually, bongos and other antelopes have pointed horns they keep throughout their lives. Males have massive backswept horns, while females have smaller, thinner, and more parallel horns. The size of the horns range between . The horns twist once.
Like all other horns of antelopes, the core of a bongo's horn is hollow and the outer layer of the horn is made of keratin, the same material that makes up human fingernails, toenails. and hair. The bongo runs gracefully and at full speed through even the thickest tangles of lianas, laying its heavy spiralled horns on its back so the brush cannot impede its flight. Bongos are hunted for their horns by humans.
Social organization and behavior.
Like other forest ungulates, bongos are seldom seen in large groups. Males, called bulls, tend to be solitary, while females with young live in groups of six to eight. Bongos have seldom been seen in herds of more than 20. Gestation is about 285 days (9.5 months), with one young per birth, and weaning occurs at six months. Sexual maturity is reached at 24–27 months. The preferred habitat of this species is so dense and difficult to operate in, that few Europeans or Americans observed this species until the 1960s.
As young males mature and leave their maternal groups, they most often remain solitary, although rarely they join an older male. Adult males of similar size/age tend to avoid one another. Occasionally, they meet and spar with their horns in a ritualised manner and rarely serious fights take place. However, such fights are usually discouraged by visual displays, in which the males bulge their necks, roll their eyes, and hold their horns in a vertical position while slowly pacing back and forth in front of the other male. They seek out females only during mating time. When they are with a herd of females, males do not coerce them or try to restrict their movements as do some other antelopes.
Although mostly nocturnal, they are occasionally active during the day. However, like deer, bongos may exhibit crepuscular behaviour. Bongos are both timid and easily frightened; after a scare, a bongo moves away at considerable speed, even through dense undergrowth. Once they find cover, they stay alert and face away from the disturbance, but peek every now and then to check the situation. The bongo's hindquarters are less conspicuous than the forequarters, and from this position the animal can quickly flee.
When in distress, the bongo emits a bleat. It uses a limited number of vocalisations, mostly grunts and snorts; females have a weak mooing contact-call for their young. Females prefer to use traditional calving grounds restricted to certain areas, while newborn calves lie in hiding for a week or more, receiving short visits by the mother to suckle.
The calves grow rapidly and can soon accompany their mothers in the nursery herds. Their horns grow rapidly and begin to show in 3.5 months. They are weaned after six months and reach sexual maturity at about 20 months.
Diet.
Like many forest ungulates, bongos are herbivorous browsers and feed on tree/bush leaves, bushes, vines, bark and pith of rotting trees, grasses/herbs, roots, cereals, shrubs, and fruits.
Bongos require salt in their diets, and are known to regularly visit natural salt licks. Examination of bongo feces revealed the charcoal from trees burnt by lightning is consumed. This behavior is believed to be a means of getting salts and minerals into their diets. This behavior has also been reported in the okapi. Another similarity to the okapi, though the bongo is unrelated, is that the bongo has a long prehensile tongue which it uses to grasp grasses and leaves.
Suitable habitats for bongos must have permanent water available. As a large animal, the bongo requires an ample amount of food, and is restricted to areas with abundant year-round growth of herbs and low shrubs.
Population and conservation.
Few estimates of population density are available. Assuming average population densities of 0.25 animals per km2 in regions where it is known to be common or abundant, and 0.02 per km2 elsewhere, and with a total area of occupancy of 327,000 km2, a total population estimate of around 28,000 is suggested. Only about 60% are in protected areas, suggesting the actual numbers of the lowland subspecies may only be in the low tens of thousands. In Kenya, their numbers have declined significantly and on Mt. Kenya, they were within the last decade due to illegal hunting with dogs. Although information on their status in the wild is lacking, lowland bongos are not presently considered endangered.
Bongos are susceptible to diseases such as rinderpest, which almost exterminated the species during the 1890s. "Tragelaphus eurycerus" may suffer from goitre. Over the course of the disease, the thyroid glands greatly enlarge (up to 10 x 20 cm) and may become polycystic. Pathogenesis of goiter in the bongo may reflect a mixture of genetic predisposition coupled with environmental factors, including a period of exposure to a goitrogen. Leopards and spotted hyenas are the primary natural predators (lions are seldom encountered due to differing habitat preferences); pythons sometimes eat bongo calves. Humans prey on them for their pelts, horns, and meat, with the species being a common local source for "bush meat". Bongo populations have been greatly reduced by hunting, poaching, and animal trapping, although some bongo refuges exist.
Although bongos are quite easy for humans to catch using snares, many people native to the bongos' habitat believed that if they ate or touched bongo, they would have spasms similar to epileptic seizures. Because of this superstition, bongos were less harmed in their native ranges than expected. However, these taboos are said no longer to exist, which may account for increased hunting by humans in recent times.
Zoo programmes.
An international studbook is maintained to help manage animals held in captivity. Because of its bright colour, it is very popular in zoos and private collections. In North America, over 400 individuals are thought to be held, a population that probably exceeds that of the mountain bongo in the wild.
In 2000, the Association of Zoos and Aquariums (AZA) upgraded the bongo to a Species Survival Plan participant, which works to improve the genetic diversity of managed animal populations. The target population for participating zoos and private collections in North America is 250 animals. Through the efforts of zoos in North America, a reintroduction to the population in Kenya is being developed.
At least one collaborative effort for reintroduction between North American wildlife facilities has already been carried out. In 2004, 18 eastern bongos born in North American zoos gathered at White Oak Conservation in Yulee, Florida for release in Kenya. White Oak staff members traveled with the bongos to a Mt. Kenya holding facility, where they stayed until being reintroduced.
Conservation.
In the last few decades, a rapid decline in the numbers of wild mountain bongo has occurred due to poaching and human pressure on their habitat, with local extinctions reported in Cherangani and Chepalungu hills, Kenya.
The Bongo Surveillance Programme, working alongside the Kenya Wildlife Service, have recorded photos of bongos at remote salt licks in the Aberdare Forests using camera traps, and, by analyzing DNA extracted from dung, have confirmed the presence of bongo in Mount Kenya, Eburru and Mau forests. The programme estimate as few as 140 animals left in the wild – spread across four isolated populations. Whilst captive breeding programmes can be viewed as having been successful in ensuring survival of this species in Europe and North America, the situation in the wild has been less promising. Evidence exists of bongo surviving in Kenya. However, these populations are believed to be small, fragmented, and vulnerable to extinction.
Animal populations with impoverished genetic diversity are inherently less able to adapt to changes in their environments (such as climate change, disease outbreaks, habitat change, etc.). The isolation of the four remaining small bongo populations, which themselves would appear to be in decline, means a substantial amount of genetic material is lost each generation. Whilst the population remains small, the impact of transfers will be greater, so the establishment of a "metapopulation management plan" occurs concurrently with conservation initiatives to enhance "in situ" population growth, and this initiative is both urgent and fundamental to the future survival of mountain bongo in the wild.
The western/lowland bongo faces an ongoing population decline as habitat destruction and hunting pressures increase with the relentless expansion of human settlement. Its long-term survival will only be assured in areas which receive active protection and management. At present, such areas comprise about 30,000 km2, and several are in countries where political stability is fragile. So, a realistic possibility exists whereby its status could decline to Threatened in the near future.
As the largest and most spectacular forest antelope, the western/lowland bongo is both an important flagship species for protected areas such as national parks, and a major trophy species which has been taken in increasing numbers in Central Africa by sport hunters during the 1990s. Both of these factors are strong incentives to provide effective protection and management of populations.
Trophy hunting has the potential to provide economic justification for the preservation of larger areas of bongo habitat than national parks, especially in remote regions of Central Africa, where possibilities for commercially successful tourism are very limited.
The eastern/mountain bongo’s survival in the wild is dependent on more effective protection of the surviving remnant populations in Kenya. If this does not occur, it will eventually become extinct in the wild. The existence of a healthy captive population of this subspecies offers the potential for its reintroduction.
Groups supporting bongo conservation in Kenya.
In 2004, Dr. Jake Veasey, the head of the Department of Animal Management and Conservation at Woburn Safari Park and a member of the European Association of Zoos and Aquariums Population Management Advisory Group, with the assistance of Lindsay Banks, took over responsibility for the management and coordination of the European Endangered Species Programme for the eastern bongo. This includes some 250 animals across Europe and the Middle East.
Along with the Rothschild giraffe, the eastern bongo is arguably one of the most threatened large mammals in Africa, with recent estimates numbering less than 140 animals, below a minimum sustainable viable population. The situation is exacerbated because these animals are spread across four isolated populations. Whilst the bongo endangered species program can be viewed as having been successful in ensuring survival of this species in Europe, it has not yet become actively involved in the conservation of this species in the wild in a coordinated fashion. The plan is to engage in conservation activities in Kenya to assist in reversing the decline of the eastern bongo populations and genetic diversity in Africa, and in particular, applying population management expertise to help ensure the persistence of genetic diversity in the free ranging wild populations.
To illustrate significance of genetic diversity loss, assume the average metapopulation size is 35 animals based on 140 animals spread across four populations (140/4=35). Assuming stable populations, these populations will lose 8% of their genetic diversity every decade. By managing all four populations as one, through strategic transfers, gene loss is reduced from 8% to 2% per decade, without any increase in bongo numbers in Kenya. By managing the European and African populations as one – by strategic exports from Europe combined with "in situ" transfers, gene loss is reduced to 0.72% every 100 years, with both populations remaining stable. If populations in Kenya are allowed to grow through the implementation of effective conservation, including strategic transfers, gene loss can be effectively halted in this species and its future secured in the wild.
The initial aims of the project are: 
If effective protection were implemented immediately and bongo populations allowed to expand without transfers, then this would create a bigger population of genetically impoverished bongos. These animals would be less able to adapt to a dynamic environment. Whilst the population remains small, the impact of transfers will be greater. For this reason, the 'metapopulation management plan' must occur concurrently with conservation strategies to enhance "in situ" population growth. This initiative is both urgent and fundamental to the future survival of the mountain bongo in the wild.
In 2013, SafariCom telecommunications donated money to the Bongo Surveillance Programme to try and keep tabs on what is thought to be the last 100 eastern bongos left in the wild in the Mau Eburu Forest in central Kenya, whose numbers are still declining due to logging of their habitat and illegal poaching.
Status.
In 2002 the IUCN listed the western/lowland species as Near Threatened. These bongos may be endangered due to human environmental interaction, as well as hunting and illegal actions towards wildlife. CITES lists bongos as an Appendix III species, only regulating their exportation from a single country, Ghana. It is not protected by the US Endangered Species Act and is not listed by the USFWS.
The IUCN Antelope Specialist Group considers the western or lowland bongo, "T. e. eurycerus", to be Lower Risk (Near Threatened), and the eastern or mountain bongo, "T. e. isaaci", of Kenya, to be Critically Endangered. Other subspecific names have been used, but their validity has not been tested.

</doc>
<doc id="4507" url="https://en.wikipedia.org/wiki?curid=4507" title="Bunyip">
Bunyip

The bunyip is a large mythical creature from Australian Aboriginal mythology, said to lurk in swamps, billabongs, creeks, riverbeds, and waterholes.
The origin of the word "bunyip" has been traced to the Wemba-Wemba or Wergaia language of Aboriginal people of South-Eastern Australia. However, the bunyip appears to have formed part of traditional Aboriginal beliefs and stories throughout Australia, although its name varied according to tribal nomenclature. In his 2001 book, writer Robert Holden identified at least nine regional variations for the creature known as the bunyip across Aboriginal Australia. Various written accounts of bunyips were made by Europeans in the early and mid-19th century, as settlement spread across the country.
Meaning.
The word "bunyip" is usually translated by Aboriginal Australians today as "devil" or "evil spirit". The word "bunyip" may not have appeared in print in English until the mid-1840s.
By the 1850s, "bunyip" had also become a "synonym for impostor, pretender, humbug and the like" in the broader Australian community. The term "bunyip aristocracy" was first coined in 1853 to describe Australians aspiring to be aristocrats. In the early 1990s, it was famously used by Prime Minister Paul Keating to describe members of the conservative Liberal Party of Australia opposition.
The word "bunyip" can still be found in a number of Australian contexts, including place names such as the Bunyip River (which flows into Westernport Bay in southern Victoria) and the town of Bunyip, Victoria.
Characteristics.
Descriptions of bunyips vary widely. George French Angus may have collected a description of a bunyip in his account of a "water spirit" from the Moorundi people of the Murray River before 1847, stating it is "much dreaded by them ... It inhabits the Murray; but ... they have some difficulty describing it. Its most usual form ... is said to be that of an enormous starfish." Robert Brough Smyth's "Aborigines of Victoria" of 1878 devoted ten pages to the bunyip, but concluded "in truth little is known among the blacks respecting its form, covering or habits; they appear to have been in such dread of it as to have been unable to take note of its characteristics." However, common features in many 19th-century newspaper accounts include a dog-like face, a crocodile like head, dark fur, a horse-like tail, flippers, and walrus-like tusks or horns or a duck-like bill.
The Challicum bunyip, an outline image of a bunyip carved by Aborigines into the bank of Fiery Creek, near Ararat, Victoria, was first recorded by "The Australasian" newspaper in 1851. According to the report, the bunyip had been speared after killing an Aboriginal man. Antiquarian Reynell Johns claimed that until the mid-1850s, Aboriginal people made a "habit of visiting the place annually and retracing the outlines of the figure the bunyip which is about 11 paces long and 4 paces in extreme breadth." The outline image no longer exists.
Debate over origins of the bunyip.
Non-Aboriginal Australians have made various attempts to understand and explain the origins of the bunyip as a physical entity over the past 150 years.
Writing in 1933, Charles Fenner suggested that it was likely that the "actual origin of the bunyip myth lies in the fact that from time to time seals have made their way up the ... Murray and Darling (Rivers)". He provided examples of seals found as far inland as Overland Corner, Loxton, and Conargo and reminded readers that "the smooth fur, prominent 'apricot' eyes and the bellowing cry are characteristic of the seal."
Another suggestion is that the bunyip may be a cultural memory of extinct Australian marsupials such as the "Diprotodon", "Zygomaturus", "Nototherium" or "Palorchestes". This connection was first formally made by Dr George Bennett of the Australian Museum in 1871, but in the early 1990s, palaeontologist Pat Vickers-Rich and geologist Neil Archbold also cautiously suggested that Aboriginal legends "perhaps had stemmed from an acquaintance with prehistoric bones or even living prehistoric animals themselves ... When confronted with the remains of some of the now extinct Australian marsupials, Aborigines would often identify them as the bunyip." They also note that "legends about the" mihirung paringmal" of western Victorian Aborigines ... may allude to the ... extinct giant birds the Dromornithidae."
Another connection to the bunyip is the shy Australasian bittern ("Botaurus poiciloptilus"). During the breeding season, the male call of this marsh-dwelling bird is a "low pitched boom"; hence, it is occasionally called the "bunyip bird".
Early accounts of settlers.
During the early settlement of Australia by Europeans, the notion that the bunyip was an actual unknown animal that awaited discovery became common. Early European settlers, unfamiliar with the sights and sounds of the island continent's peculiar fauna, regarded the bunyip as one more strange Australian animal and sometimes attributed unfamiliar animal calls or cries to it. It has also been suggested that 19th-century bunyip lore was reinforced by imported European memories, such as that of the Irish Púca.
A large number of bunyip sightings occurred during the 1840s and 1850s, particularly in the southeastern colonies of Victoria, New South Wales and South Australia, as European settlers extended their reach. The following is not an exhaustive list of accounts:
Hume find of 1818.
One of the earliest accounts relating to a large unknown freshwater animal was in 1818, when Hamilton Hume and James Meehan found some large bones at Lake Bathurst in New South Wales. They did not call the animal a bunyip, but described the remains indicating the creature as very much like a hippopotamus or manatee. The Philosophical Society of Australasia later offered to reimburse Hume for any costs incurred in recovering a specimen of the unknown animal, but for various reasons, Hume did not return to the lake. It might be noted that Diprotodon skeletons have sometimes been compared to the hippopotamus; they are a land animal, but have sometimes been found in a lake or water course.
Wellington Caves fossils, 1830.
More significant was the discovery of fossilised bones of "some quadruped much larger than the ox or buffalo" in the Wellington Caves in mid-1830 by bushman George Rankin and later by Thomas Mitchell. Sydney's Reverend John Dunmore Lang announced the find as "convincing proof of the deluge". However, it was British anatomist Sir Richard Owen who identified the fossils as the gigantic marsupials "Nototherium" and "Diprotodon". At the same time, some settlers observed "all natives throughout these ... districts have a tradition (of) a very large animal having at one time existed in the large creeks and rivers and by many it is said that such animals now exist."
First written use of the word "bunyip", 1845.
In July 1845, "The Geelong Advertiser" announced the discovery of fossils found near Geelong, under the headline "Wonderful Discovery of a new Animal". This was a continuation of a story on 'fossil remains' from the previous issue. The newspaper continued, "On the bone being shown to an intelligent black (sic), he at once recognised it as belonging to the bunyip, which he declared he had seen. On being requested to make a drawing of it, he did so without hesitation." The account noted a story of an Aboriginal woman being killed by a bunyip and the "most direct evidence of all" – that of a man named Mumbowran "who showed several deep wounds on his breast made by the claws of the animal". The account provided this description of the creature:
Shortly after this account appeared, it was repeated in other Australian newspapers. However, it appears to be the first use of the word "bunyip" in a written publication.
Australian Museum's bunyip of 1847.
In January 1846, a peculiar skull was taken from the banks of Murrumbidgee River near Balranald, New South Wales. Initial reports suggested that it was the skull of something unknown to science. The squatter who found it remarked, "all the natives to whom it was shown called a bunyip". By July 1847, several experts, including W. S. Macleay and Professor Owen, had identified the skull as the deformed foetal skull of a foal or calf. At the same time, however, the purported bunyip skull was put on display in the Australian Museum (Sydney) for two days. Visitors flocked to see it, and "The Sydney Morning Herald" said that it prompted many people to speak out about their "bunyip sightings". Reports of this discovery used the phrase 'Kine Pratie' as well as Bunyip and explorer William Hovell, who examined the skull, also called it a 'katen-pai'.
In March of that year "a bunyip or an immense Platibus" (Platypus) was sighted "sunning himself on the placid bosom of the Yarra, just opposite the Custom House" in Melbourne. "Immediately a crowd gathered" and three men set off by boat "to secure the stranger" who "disappeared" when they were "about a yard from him".
William Buckley's account of bunyips, 1852.
Another early written account is attributed to escaped convict William Buckley in his 1852 biography of thirty years living with the Wathaurong people. His 1852 account records "in ... Lake Moodewarri Lake Modewarre as well as in most of the others inland ... is a ... very extraordinary amphibious animal, which the natives call Bunyip." Buckley's account suggests he saw such a creature on several occasions. He adds, "I could never see any part, except the back, which appeared to be covered with feathers of a dusky grey colour. It seemed to be about the size of a full grown calf ... I could never learn from any of the natives that they had seen either the head or tail." Buckley also claimed the creature was common in the Barwon River and cites an example he heard of an Aboriginal woman being killed by one. He emphasized the bunyip was believed to have supernatural powers.
In popular culture and fiction.
The word "bunyip" has been used in other Australian contexts, including "The Bunyip" newspaper as the banner of a local weekly newspaper published in the town of Gawler, South Australia. First published as a pamphlet by the Gawler Humbug Society in 1863, the name was chosen because "the Bunyip is the true type of Australian Humbug!" The word is also used in numerous other Australian contexts, including the House of the Gentle Bunyip in Clifton Hill, Victoria.
Numerous tales of the bunyip in written literature appeared in the 19th and early 20th centuries. One of the earliest known is a story in Andrew Lang's "The Brown Fairy Book" (1904).
Alexander Bunyip, created by children's author and illustrator Michael Salmon, first appeared in print in "The Monster That Ate Canberra" in 1972, Alexander Bunyip went on to appear in many other books and a live-action television series, "Alexander Bunyip's Billabong". A statue of Alexander was opened in front of the Gungahlin Library in 2011. The artwork by Anne Ross, called 'A is for Alexander, B is for Bunyip, C is for Canberra' was commissioned by the ACT Government for Gungahlin's $3.8 million town park.
The Australian tourism boom of the 1970s brought a renewed interest in bunyip mythology.
Bunyip stories have also appeared outside of Australia.
In the 21st century the bunyip can be considered part of the international consciousness.

</doc>
<doc id="4508" url="https://en.wikipedia.org/wiki?curid=4508" title="Brabant">
Brabant

Brabant may refer to:
__TOC__
Place names in Europe.
In the Netherlands:
In Belgium:
In France:

</doc>
<doc id="4512" url="https://en.wikipedia.org/wiki?curid=4512" title="Boone, North Carolina">
Boone, North Carolina

Boone is a town located in the Blue Ridge Mountains of western North Carolina, United States. Boone's population was 17,122 in 2010. Boone is the county seat of Watauga County and the home of Appalachian State University.
The town is named for famous American pioneer and explorer Daniel Boone, and every summer since 1952 has hosted an outdoor amphitheatre drama, Horn in the West, portraying the White settlement of the area during the American Revolutionary War and featuring the contributions of its namesake.
In 2012, Boone was listed among the 10 best places to retire in the U.S. by "U.S. News".
Boone.
Boone was served by the narrow gauge East Tennessee and Western North Carolina Railroad (nicknamed "Tweetsie") until the flood of 1940. The flood washed away much of the tracks and it was decided not to replace them.
Boone is the home of Appalachian State University, a constituent member of the University of North Carolina.
Appalachian State is the sixth largest university in the seventeen-campus system.
Caldwell Community College & Technical Institute also operates a satellite campus in Boone.
""Horn in the West"", a dramatization of the life and times of the early settlers of the mountain area, which features Daniel Boone as one of its characters, has been performed in an outdoor amphitheatre above the town every summer since 1952.
The original actor in the role of "Daniel Boone" was Ned Austin. His "Hollywood Star" stands on a pedestal on King Street in downtown Boone. He was followed in the role by Glenn Causey, who portrayed the rugged frontiersman for 41 years, and whose image is still seen in many of the depictions of Boone featured in the area today.
Boone is a center for bluegrass musicians and Appalachian storytellers. Notable artists associated with Boone include the late, Grammy Award-winning bluegrass guitar player Doc Watson and the late guitarist Michael Houser, founding member of and lead guitarist for the band Widespread Panic, both Boone natives, as well as Old Crow Medicine Show, The Blue Rags, and Eric Church. 
The Blair Farm, Daniel Boone Hotel, Jones House, John Smith Miller House, and US Post Office-Boone are listed on the National Register of Historic Places.
Geography and climate.
Boone is located at (36.211364, −81.668657) and has an elevation of 3,333 feet (1015.9 m) above sea level. An earlier survey gave the elevation as 3,332 ft and since then it has been published as having an elevation of 3,333 ft (1,016 m). Boone has the highest elevation of any town of its size (over 10,000 population) east of the Mississippi River. As such, Boone features, depending on the isotherm used, a subtropical highland climate (Köppen "Cfb"), or a humid continental climate (Köppen "Dfb"), a rarity for the Southeastern United States, and straddles the boundary between USDA Plant Hardiness Zones 6B and 7A; the elevation also results in enhanced precipitation, with of average annual precipitation. Compared to the lower elevations of the Carolinas, winters are long and cold, with frequent sleet and snowfall. The daily average temperature in January is , which gives Boone a winter climate more similar to coastal southern New England rather than the Southeast, where a humid subtropical climate dominates. Blizzard-like conditions are not unusual during many winters. Summers are warm, but far cooler and less humid than lower regions to the south and east, with a July daily average temperature of . Boone typically receives on average nearly of snowfall annually, far higher than the lowland areas in the rest of North Carolina.
Demographics.
As of the census of 2000, there were 13,472 people, 4,374 households, and 1,237 families residing in the town. The population density was 2,307.0 people per square mile (890.7/km²). There were 4,748 housing units at an average density of 813.0 per square mile (313.9/km²). The racial makeup of the town was 93.98% White, 3.42% Black or African American, 0.30% Native American, 1.19% Asian, 0.05% Pacific Islander, 0.46% from other races, and 0.60% from two or more races. 1.64% of the population were Hispanic or Latino of any race.
There were 4,374 households out of which 9.6% had children under the age of 18 living with them, 21.0% were married couples living together, 5.6% had a female householder with no husband present, and 71.7% were non-families. 38.4% of all households were made up of individuals and 7.3% had someone living alone who was 65 years of age or older. The average household size was 1.97 and the average family size was 2.63.
The age distribution is 5.8% under 18, 65.9% from 18 to 24, 12.1% from 25 to 44, 9.1% from 45 to 64, and 7.1% who were 65 or older. The median age was 21 years. Both the overall age distribution and the median age are driven by the presence of the local university, Appalachian State. For every 100 females there are 95.6 males. For every 100 females age 18 and over, there were 94.7 males.
The median household income is $20,541, and the median family income is $49,762. The per capita income is $12,256. 37.0% of the population and 9.2% of families were below the poverty line.
Men had a median income of $28,060 versus $20,000 for women. However, poverty statistics that are based on surveys of the entire population can be extremely misleading in communities dominated by students, such as Boone. Out of the total population, 6.3% of those under the age of 18 and 9.1% of those 65 and older were living below the poverty line.
Media.
Newspaper.
Boone is mainly served by three local newspapers:
A smaller newspaper, The Appalachian, is Appalachian State University's campus newspaper published twice a week on Tuesdays and Thursdays. In addition to the locally printed papers, a monthly entertainment pamphlet named Kraut Creek Revival has limited circulation and is funded by a Denver, NC-based newspaper.
Law and government.
Boone operates under a mayor-council government. The city council consists of five members. The mayor presides over the council and casts a vote on issues only in the event of a tie. As of July 2015, the Town Council members are: Rennie Brantz, Acting Mayor and Mayor Pro Tem, and Councilors: Lynne Mason, Fred Hay, and Quint David.
Development.
Industrial, commercial, and residential development in the town of Boone is a controversial issue due to its location in the mountains of Appalachia. On October 16, 2009, the town council accepted the "Boone 2030 Land Use Plan." While the document itself is not in any way actual law, it is used by the town council, board of adjustment, and other committees to guide decision making as to what types of development are appropriate.
In 2009, the North Carolina Department of Transportation began widening 1.1 miles of U.S. 421 (King Street) to a 4-to-6-lane divided highway with a raised concrete median from U.S. 321 (Hardin Street) to east of N.C. 194 (Jefferson Road), including a new entrance and exit to the new Watauga High School, at a cost of $16.2 million. The widening has displaced 25 businesses and 63 residences east of historic downtown King Street. The project was slated to be completed by December 31, 2011, but construction continued into the spring of 2012.
Economy.
Samaritan's Purse is based in Boone.
Largest Employers.
According to the Town's 2014 Comprehensive Annual Financial Report, the top employers in the city are:

</doc>
<doc id="4513" url="https://en.wikipedia.org/wiki?curid=4513" title="Banshee">
Banshee

A banshee ( , from , , "woman of the barrows") is a female spirit in Irish mythology, who heralds the death of a member of one of the prominent Gaelic families.
Description.
The banshee is often described in Gaelic lore as wearing red or green, usually with long, disheveled hair. She can appear in a variety of forms. Perhaps most often she is seen as an ugly, frightful hag, but she can also appear as young and beautiful if she chooses. In some tales, the figure who first appears to be a banshee or other "cailleach" (hag) is later revealed to be the Irish battle goddess, the Morrígan.
Keening.
In Ireland and parts of Scotland, a traditional part of mourning is the keening woman ("bean chaointe"), who wails a lament - a , or , "caoin" meaning "to weep, to wail". This keening woman may in some cases be a professional, and the best keeners would be in high demand. Legend has it that for great Gaelic families – the O'Gradys, the O'Neills, the Ó Longs, the McCnaimhíns, the O'Briens the Ó Conchobhairs, and the Caomhánachs – the lament would be sung by a fairy woman; having foresight, she would sing it when a family member died, even if the person had died far away and news of their death had not yet come, so that the wailing of the banshee was the first warning the household had of the death. In later versions, the banshee might appear before the death and warn the family by wailing.
When several banshees appear at once, it indicates the death of someone great or holy. The tales sometimes recounted that the woman, though called a fairy, was a ghost, often of a specific murdered woman, or a mother who died in childbirth.
Origin.
The Ua Briain banshee is thought to be named Aibell and the ruler of 25 other banshees who would always be at her attendance. It is possible that this particular story is the source of the idea that the wailing of numerous banshees signifies the death of a great person.
Most, though not all, surnames associated with banshees have the "Ó" or "Mc/Mac" prefix - that is, surnames of Goidelic origin, indicating a family native to the Insular Celtic lands rather than those of the Norse, English, or Norman invaders. Accounts reach as far back as 1380 to the publication of the "Cathreim Thoirdhealbhaigh" ("Triumphs of Torlough") by Sean mac Craith. Mentions of banshees can also be found in Norman literature of that time.
In some parts of Leinster, she is referred to as the "bean chaointe" (keening woman) whose wail can be so piercing that it shatters glass. In Scottish folklore, a similar creature is known as the bean nighe or "ban nigheachain" (little washerwoman) or "nigheag na h-àth" (little washer at the ford) and is seen washing the bloodstained clothes or armour of those who are about to die. In Welsh folklore, a similar creature is known as the Hag of the mist.

</doc>
<doc id="4514" url="https://en.wikipedia.org/wiki?curid=4514" title="Genetically modified maize">
Genetically modified maize

Genetically modified maize (corn) is a genetically modified crop. Specific maize strains have been genetically engineered to express agriculturally-desirable traits, including resistance to pests and to herbicides. Maize strains with both traits are now in use in multiple countries. GM maize has also caused controversy with respect to possible health effects, impact on other insects and impact on other plants via gene flow. One strain, called Starlink, was approved only for animal feed in the US, but was found in food, leading to a series of recalls starting in 2000.
Marketed products.
Herbicide resistant maize.
Corn varieties resistant to glyphosate herbicides were first commercialized in 1996 by Monsanto, and are known as "Roundup Ready Corn". They tolerate the use of Roundup. Bayer CropScience developed "Liberty Link Corn" that is resistant to glufosinate. Pioneer Hi-Bred has developed and markets corn hybrids with tolerance to imidazoline herbicides under the trademark "Clearfield" - though in these hybrids, the herbicide-tolerance trait was bred using tissue culture selection and the chemical mutagen ethyl methanesulfonate not genetic engineering. Consequently, the regulatory framework governing the approval of transgenic crops does not apply for Clearfield.
As of 2011, herbicide-resistant GM corn was grown in 14 countries. By 2012, 26 varieties herbicide-resistant GM maize were authorised for import into the European Union., but such imports remain controversial. Cultivation of herbicide-resistant corn in the EU provides substantial farm-level benefits.
Insecticide-producing corn.
Bt corn is a variant of maize that has been genetically altered to express one or more proteins from the bacterium "Bacillus thuringiensis". The protein is poisonous to certain insect pests and is widely used in organic gardening. The European corn borer causes about a billion dollars in damage to corn crops each year.
In recent years, traits have been added to ward off Corn ear worms and root worms, the latter of which annually causes about a billion dollars in damages.
The Bt protein is expressed throughout the plant. When a vulnerable insect eats the Bt-containing plant, the protein is activated in its gut, which is alkaline. In the alkaline environment the protein partially unfolds and is cut by other proteins, forming a toxin that paralyzes the insect's digestive system and forms holes in the gut wall. The insect stops eating within a few hours and eventually starves.
In 1996, the first GM maize producing a Bt Cry protein was approved, which killed the European corn borer and related species; subsequent Bt genes were introduced that killed corn rootworm larvae.
Approved Bt genes include single and stacked (event names bracketed) configurations of: Cry1A.105 (MON89034), CryIAb (MON810), CryIF (1507), Cry2Ab (MON89034), Cry3Bb1 (MON863 and MON88017), Cry34Ab1 (59122), Cry35Ab1 (59122), mCry3A (MIR604), and Vip3A (MIR162), in both corn and cotton. Corn genetically modified to produce VIP was first approved in the US in 2010.
Drought resistance.
In 2013 Monsanto launched the first transgenic drought tolerance trait in a line of corn hybrids called DroughtGard. The MON 87460 trait is provided by the insertion of the cspB gene from the soil microbe "Bacillus subtilis"; it was approved by the USDA in 2011 and by China in 2013.
Sweet corn.
GM sweet corn varieties include "Attribute", the brand name for insect-resistant sweet corn developed by Syngenta.
and Performance Series™ insect-resistant sweet corn developed by Monsanto.
Products in development.
In 2007, South African researchers announced the production of transgenic maize resistant to maize streak virus (MSV), although it has not been released as a product.
Refuges.
US Environmental Protection Agency (EPA) regulations require farmers who plant Bt corn to plant non-Bt corn nearby (called a refuge) to provide a location to harbor vulnerable pests. Typically, 20% of corn in a grower's fields must be refuge; refuge must be at least 0.5 miles from Bt corn for lepidopteran pests, and refuge for corn rootworm must at least be adjacent to a Bt field.
The theory behind these refuges is to slow the evolution of resistance to the pesticide. EPA regulations also require seed companies to train farmers how to maintain refuges, to collect data on the refuges and to report that data to the EPA. A study of these reports found that from 2003 to 2005 farmer compliance with keeping refuges was above 90%, but that by 2008 approximately 25% of Bt corn farmers did not keep refuges properly, raising concerns that resistance would develop.
Unmodified crops received most of the economic benefits of Bt corn in the US in 1996-2007, because of the overall reduction of pest populations. This reduction came because females laid eggs on modified and unmodified strains alike.
Seed bags containing both Bt and refuge seed have been approved by the EPA in the United States. These seed mixtures were marketed as "Refuge in a Bag" (RIB) to increase farmer compliance with refuge requirements and reduce additional work needed at planting from having separate Bt and refuge seed bags on hand. The EPA approved a lower percentage of refuge seed in these seed mixtures ranging from 5 to 10%. This strategy is likely to reduce the likelihood of Bt-resistance occurring for corn rootworm, but may increase the risk of resistance for lepidopteran pests, such as European corn borer. Increased concerns for resistance with seed mixtures include partially resistant larvae on a Bt plant being able to move to a susceptible plant to survive or cross pollination of refuge pollen on to Bt plants that can lower the amount of Bt expressed in kernels for ear feeding insects.
Resistance.
Resistant strains of the European corn borer have developed in areas with defective or absent refuge management.
In November 2009, Monsanto scientists found the pink bollworm had become resistant to first-generation Bt cotton in parts of Gujarat, India - that generation expresses one Bt gene, "Cry1Ac". This was the first instance of Bt resistance confirmed by Monsanto anywhere in the world. Bollworm resistance to first generation Bt cotton has been identified in the Australia, China, Spain and the United States. In 2012, a Florida field trial demonstrated that army worms were resistant to pesticide-containing GM corn produced by Dupont-Dow; armyworm resistance was first discovered in Puerto Rico in 2006, prompting Dow and DuPont to voluntarily stop selling the product on the island.
Regulation.
Regulation of GM crops varies between countries, with some of the most-marked differences occurring between the USA and Europe. Regulation varies in a given country depending on intended uses.
Controversy.
There is general scientific agreement that food from genetically modified crops is not inherently riskier to human health than conventional food, but should be tested on a case-by-case basis. The scientific rigor of the studies regarding human health has been disputed due to alleged lack of independence and due to conflicts of interest involving governing bodies and some of those who perform and evaluate the studies.
GM crops provide a number of ecological benefits, but there are also concerns for their overuse, stalled research outside of the Bt seed industry, proper management and issues with Bt resistance arising from their misuse.
Critics have objected to GM crops on ecological, economic and health grounds. The economic issues derive from those organisms that are subject to intellectual property law, mostly patents. The first generation of GM crops lose patent protection beginning in 2015. Monsanto has claimed it will not pursue farmers who retain seeds of off-patent varieties. These controversies have led to litigation, international trade disputes, protests and to restrictive legislation in most countries.
Effects on nontarget insects.
Critics claim that Bt proteins could target predatory and other beneficial or harmless insects as well as the targeted pest. These proteins have been used as organic sprays for insect control in France since 1938 and the USA since 1958 with no ill effects on the environment reported. While "cyt" proteins are toxic towards the insect orders Coleoptera (beetles) and Diptera (flies), "cry" proteins selectively target Lepidopterans (moths and butterflies). As a toxic mechanism, "cry" proteins bind to specific receptors on the membranes of mid-gut (epithelial) cells, resulting in rupture of those cells. Any organism that lacks the appropriate gut receptors cannot be affected by the "cry" protein, and therefore Bt. Regulatory agencies assess the potential for the transgenic plant to impact nontarget organisms before approving commercial release.
A 1999 study found that in a lab environment, pollen from Bt maize dusted onto milkweed could harm the monarch butterfly. Several groups later studied the phenomenon in both the field and the laboratory, resulting in a risk assessment that concluded that any risk posed by the corn to butterfly populations under real-world conditions was negligible. A 2002 review of the scientific literature concluded that "the commercial large-scale cultivation of current Bt–maize hybrids did not pose a significant risk to the monarch population". A 2007 review found that "nontarget invertebrates are generally more abundant in Bt cotton and Bt maize fields than in nontransgenic fields managed with insecticides. However, in comparison with insecticide-free control fields, certain nontarget taxa are less abundant in Bt fields."
Gene flow.
Gene flow is the transfer of genes and/or alleles from one species to another. Concerns focus on the interaction between GM and other maize varieties in Mexico, and of gene flow into refuges.
In 2009 the government of Mexico created a regulatory pathway for genetically modified maize, but because Mexico is the center of diversity for maize, gene flow could affect a large fraction of the world's maize strains. A 2001 report in "Nature" presented evidence that Bt maize was cross-breeding with unmodified maize in Mexico. The data in this paper was later described as originating from an artifact. "Nature" later stated, "the evidence available is not sufficient to justify the publication of the original paper". A 2005 large-scale study failed to find any evidence of contamination in Oaxaca. However, other authors also found evidence of cross-breeding between natural maize and transgenic maize.
A 2004 study found Bt protein in kernels of refuge corn.
Food.
The French High Council of Biotechnologies Scientific Committee reviewed the 2009 Vendômois "et al." study and concluded that it "..presents no admissible scientific element likely to ascribe any haematological, hepatic or renal toxicity to the three re-analysed GMOs." However, the French government applies the precautionary principle with respect to GMOs.
A review by Food Standards Australia New Zealand and others of the same study concluded that the results were due to chance alone.
A 2011 Canadian study looked at the presence of CryAb1 protein (BT toxin) in non-pregnant women, pregnant women and fetal blood. All groups had detectable levels of the protein, including 93% of pregnant women and 80% of fetuses at concentrations of 0.19 ± 0.30 and 0.04 ± 0.04 mean ± SD ng/ml, respectively. The paper did not discuss safety implications or find any health problems. The paper was found to be unconvincing by multiple authors and organizations. In a swine model, Cry1Ab-specific antibodies were not detected in pregnant sows or their offspring and no negative effects from feeding Bt maize to pregnant sows were observed.
In January 2013, the European Food Safety Authority released all data submitted by Monsanto in relation to the 2003 authorisation of maize genetically modified for glyphosate tolerance.
Starlink corn recalls.
StarLink contains Cry9C, which had not previously been used in a GM crop. Starlink's creator, Plant Genetic Systems had applied to the US Environmental Protection Agency (EPA) to market Starlink for use in animal feed and in human food. However, because the Cry9C protein lasts longer in the digestive system than other Bt proteins, the EPA had concerns about its allergenicity, and PGS did not provide sufficient data to prove that Cry9C was not allergenic. As a result, PGS split its application into separate permits for use in food and use in animal feed. Starlink was approved by the EPA for use in animal feed only in May 1998.
StarLink corn was subsequently found in food destined for consumption by humans in the US, Japan, and South Korea. This corn became the subject of the widely publicized Starlink corn recall, which started when Taco Bell-branded taco shells sold in supermarkets were found to contain the corn. Sales of StarLink seed were discontinued. The registration for Starlink varieties was voluntarily withdrawn by Aventis in October 2000. (Pioneer had been bought by AgrEvo which then became Aventis CropScience at the time of the incident, which was later bought by Bayer
Fifty-one people reported adverse effects to the FDA; US Centers for Disease Control (CDC), which determined that 28 of them were possibly related to Starlink. However, the CDC studied the blood of these 28 individuals and concluded there was no evidence of hypersensitivity to the Starlink Bt protein.
A subsequent review of these tests by the Federal Insecticide, Fungicide, and Rodenticide Act Scientific Advisory Panel points out that while "the negative results decrease the probability that the Cry9C protein is the cause of allergic symptoms in the individuals examined ... in the absence of a positive control and questions regarding the sensitivity and specificity of the assay, it is not possible to assign a negative predictive value to this."
The US corn supply has been monitored for the presence of the Starlink Bt proteins since 2001.
In 2005, aid sent by the UN and the US to Central American nations also contained some StarLink corn. The nations involved, Nicaragua, Honduras, El Salvador and Guatemala refused to accept the aid.
Corporate espionage.
On December 19, 2013 six Chinese citizens were indicted in Iowa on charges of plotting to steal genetically modified seeds worth tens of millions of dollars from Monsanto and DuPont. Mo Hailong, director of international business at the Beijing Dabeinong Technology Group Co., part of the Beijing-based DBN Group, was accused of stealing trade secrets after he was found digging in an Iowa cornfield.

</doc>
<doc id="4516" url="https://en.wikipedia.org/wiki?curid=4516" title="Body substance isolation">
Body substance isolation

Body substance isolation is a practice of isolating all body substances (blood, urine, feces, tears, etc.) of individuals undergoing medical treatment, particularly emergency medical treatment of those who might be infected with illnesses such as HIV, or hepatitis so as to reduce as much as possible the chances of transmitting these illnesses. BSI is similar in nature to universal precautions, but goes further in isolating workers from pathogens, including substances now known to carry HIV.
Place of body substance isolation practice in history.
Practice of Universal precautions was introduced in 1985–88. In 1987, the practice of Universal precautions was adjusted by a set of rules known as body substance isolation. In 1996, both practices were replaced by the latest approach known as standard precautions (health care). Nowadays and in isolation, practice of body substance isolation has just historical significance.
Body substance isolation went further than universal precautions in isolating workers from pathogens, including substances now currently known to carry HIV. These pathogens fall into two broad categories, bloodborne (carried in the body fluids) and airborne. The practice of BSI was common in Pre-Hospital care and Emergency Medical Services due to the often unknown nature of the patient and his/her disease or medical conditions. It was a part of the National Standards Curriculum for Prehospital Providers and Firefighters.
Types of body substance isolation included:
It was postulated that BSI precautions should be practiced in environment where treaters were exposed to bodily fluids, such as:
Such infection control techniques that were recommended following the AIDS outbreak in the 1980s. Every patient was treated as if infected and therefore precautions were taken to minimize risk. Other conditions which called for minimizing risks with BSI:
or any combination of the above.

</doc>
<doc id="4517" url="https://en.wikipedia.org/wiki?curid=4517" title="Boudica">
Boudica

Boudica (; alternative spellings: Boudicca, Boudicea, also known as Boadicea and in Welsh as ) (d. AD 60 or 61) was a queen of the British Celtic Iceni tribe who led an uprising against the occupying forces of the Roman Empire.
Boudica's husband Prasutagus ruled as a nominally independent ally of Rome and left his kingdom jointly to his daughters and the Roman emperor in his will. However, when he died his will was ignored, and the kingdom was annexed. Boudica was flogged, her daughters raped, and Roman financiers called in their loans.
In AD 60 or 61, when the Roman governor Gaius Suetonius Paulinus was campaigning on the island of Anglesey off the northwest coast of Wales, Boudica led the Iceni, the Trinovantes, and others in revolt. They destroyed (modern Colchester), earlier the capital of the Trinovantes but at that time a "", a settlement for discharged Roman soldiers and site of a temple to the former Emperor Claudius. Upon hearing of the revolt, Suetonius hurried to (modern London), the 20-year-old commercial settlement that was the rebels' next target.
The Romans, having concluded that they lacked sufficient numbers to defend the settlement, evacuated and abandoned . Boudica led 100,000 Iceni, Trinovantes, and others to fight , and burned and destroyed , and (modern-day St Albans). 
An estimated 70,000–80,000 Romans and British were killed in the three cities by those led by Boudica. Suetonius, meanwhile, regrouped his forces in the West Midlands, and despite being heavily outnumbered defeated the Britons in the Battle of Watling Street.
The crisis caused Nero to consider withdrawing all Roman forces from Britain, but Suetonius' eventual victory over Boudica confirmed Roman control of the province. Boudica then either killed herself, to avoid capture, or died of illness. The extant sources, Tacitus and Cassius Dio, differ.
Interest in these events revived in the English Renaissance and led to Boudica's fame in the Victorian era. Boudica has remained an important cultural symbol in the United Kingdom. However, the absence of native British literature during the early part of the first millennium means that knowledge of Boudica's rebellion comes solely from the writings of the Romans.
Name.
Boudica has been known by several versions of her name. Raphael Holinshed calls her Voadicia, while Edmund Spenser calls her Bunduca, a version of the name that was used in the popular Jacobean play "Bonduca", in 1612. William Cowper's poem, "Boadicea, an ode" (1782) popularised an alternative version of the name. From the 19th century and much of the late 20th century, Boadicea was the most common version of the name, which is probably derived from a mistranscription when a manuscript of Tacitus was copied in the Middle Ages.
Her name was clearly spelled Boudicca in the best manuscripts of Tacitus, but also , , and in the (later and probably secondary) epitome of Cassius Dio. The name is attested in inscriptions as Boudica in Lusitania, Boudiga in Bordeaux, and Bodicca in Algeria.
Kenneth Jackson concludes, based on later development of Welsh and Irish, that the name derives from the Proto-Celtic feminine adjective "*boudīkā", "victorious", that in turn is derived from the Celtic word "*boudā", "victory" (cf. Irish ' (Classical Irish '), "Buaidheach", Welsh ""), and that the correct spelling of the name in Common Brittonic (the British Celtic language) is "Boudica", pronounced .
The closest English equivalent to the vowel in the first syllable is the "ow" in "bow-and-arrow". The modern English pronunciation is , and it has been suggested that the most comparable English name, in meaning only, would be "Victoria".
History.
Background.
Tacitus and Cassius Dio agree that Boudica was of royal descent. Dio describes her as "possessed of greater intelligence than often belongs to women." He also describes her as tall, with tawny hair hanging down to below her waist, a harsh voice and a piercing glare. He notes that she habitually wore a large golden necklace (perhaps a torc), a colourful tunic, and a thick cloak fastened by a brooch.
Boudicca’s husband, Prasutagus, was the king of the Iceni, a people who inhabited roughly what is now Norfolk. The Iceni initially voluntarily allied with Rome following Claudius's conquest of Southern Britain AD 43. They were proud of their independence, and had revolted in AD 47 when the then Roman governor Publius Ostorius Scapula planned to disarm all the peoples in the area of Britain under Roman control following a number of local uprisings. Ostorius defeated them and went on to put down other uprisings around Britain. The Iceni remained independent. Tacitus first mentioned Prasutagus when he wrote about Boudica’s rebellion. We do not know whether he became the king after the mentioned defeat of the Iceni. We do not have any record as to whether the Iceni at that point were still Roman allies or had become a client kingdom.
Tacitus wrote "The Icenian king Prasutagus, celebrated for his long prosperity, had named the emperor his heir, together with his two daughters; an act of deference which he thought would place his kingdom and household beyond the risk of injury. The result was contrary — so much so that his kingdom was pillaged by centurions, his household by slaves; as though they had been prizes of war." He added that Boudica was lashed and her two daughters were raped and that the estates of the leading Iceni men were confiscated.
Cassius Dio wrote: "An excuse for the war was found in the confiscation of the sums of money that Claudius had given to the foremost Britons; for these sums, as Decianus Catus, the procurator of the island maintained, were to be paid back." He also said that another reasons was "the fact that Seneca, in the hope of receiving a good rate of interest, had lent to the islanders 40,000,000 sesterces that they did not want, and had afterwards called in this loan all at once and had resorted to severe measures in exacting it." 
Tacitus did not say why Prasutagus’ naming the emperor as his heir as well as his daughters was meant to avert the risk of injury. He did not explain why the Romans pillaged the kingdom, why they took the lands of the chiefs or why Boudica was flogged and her daughters were raped. Cassius Dio did not mention any of this. He said that the cause of the rebellion was the decision of the procurator of Britain (the chief financial officer) and Seneca (an advisor of the emperor Nero) to call in Prasutagus’ debts and the harsh measures which were taken to collect them. Tacitus does not mention these events. However, he wrote: "Alarmed by this disaster and by the fury of the province which he had goaded into war by his rapacity, the procurator Catus crossed over into Gaul." 
It has to be noted that this was happening while the governor of Britain, Gaius Suetonius Paulinus, was away fighting in North Wales. We do not know whether he approved of these actions. We do not know who the centurions who pillaged the kingdom were and who sent them. The text of Cassius Dio seems to suggest that Seneca, who was a private citizen, was responsible for the violence. It is unlikely that a legion was sent to land of the Iceni as two of them were fighting at the island of Anglesey and the other two were stationed at their garrisons. Tacitus said that "It was against the veterans that their hatred was most intense. For these new settlers in the colony of Camulodunum drove people out of their houses, ejected them from their farms, called them captives and slaves …" 
Boudica's uprising.
In AD 60 or 61, while the current governor, Gaius Suetonius Paulinus, was leading a campaign against the island of (modern Anglesey) in the north of Wales, which was a refuge for British rebels and a stronghold of the druids, the Iceni conspired with their neighbours the Trinovantes, amongst others, to revolt. Boudica was chosen as their leader. According to Tacitus, they drew inspiration from the example of Arminius, the prince of the Cherusci who had driven the Romans out of Germany in AD 9, and their own ancestors who had driven Julius Caesar from Britain. Dio says that at the outset Boudica employed a form of divination, releasing a hare from the folds of her dress and interpreting the direction in which it ran, and invoked Andraste, a British goddess of victory.
The rebels' first target was (Colchester), the former Trinovantian capital and, at that time, a Roman "". The Roman veterans who had been settled there mistreated the locals and a temple to the former emperor Claudius had been erected there at local expense, making the city a focus for resentment. The Roman inhabitants sought reinforcements from the procurator, Catus Decianus, but he sent only two hundred auxiliary troops. Boudica's army fell on the poorly defended city and destroyed it, besieging the last defenders in the temple for two days before it fell. Archaeologists have shown that the city was methodically demolished. The future governor Quintus Petillius Cerialis, then commanding the Legio IX "Hispana", attempted to relieve the city, but suffered an overwhelming defeat. His infantry was wiped out—only the commander and some of his cavalry escaped. 
The location of this famous destruction of the is now claimed by some to be the village of Great Wratting, in Suffolk, which lies in the Stour Valley on the Icknield Way West of Colchester, and by a village in Essex. After this defeat, Catus Decianus fled to Gaul.
When news of the rebellion reached him, Suetonius hurried along Watling Street through hostile territory to . was a relatively new settlement, founded after the conquest of AD 43, but it had grown to be a thriving commercial centre with a population of travellers, traders, and, probably, Roman officials. Suetonius considered giving battle there, but considering his lack of numbers and chastened by Petillius's defeat, decided to sacrifice the city to save the province.
Alarmed by this disaster and by the fury of the province which he had goaded into war by his rapacity, the procurator Catus crossed over into Gaul. Suetonius, however, with wonderful resolution, marched amidst a hostile population to Londinium, which, though undistinguished by the name of a colony, was much frequented by a number of merchants and trading vessels. Uncertain whether he should choose it as a seat of war, as he looked round on his scanty force of soldiers, and remembered with what a serious warning the rashness of Petilius had been punished, he resolved to save the province at the cost of a single town. Nor did the tears and weeping of the people, as they implored his aid, deter him from giving the signal of departure and receiving into his army all who would go with him. Those who were chained to the spot by the weakness of their sex, or the infirmity of age, or the attractions of the place, were cut off by the enemy.— Tacitus
Londinium was abandoned to the rebels who burnt it down, slaughtering anyone who had not evacuated with Suetonius. Archaeology shows a thick red layer of burnt debris covering coins and pottery dating before AD 60 within the bounds of Roman Londinium; while Roman-era skulls found in the Walbrook in 2013 were potentially linked to victims of the rebels. (St Albans) was next to be destroyed.
In the three settlements destroyed, between seventy and eighty thousand people are said to have been killed. Tacitus says that the Britons had no interest in taking or selling prisoners, only in slaughter by gibbet, fire, or cross. Dio's account gives more detail; that the noblest women were impaled on spikes and had their breasts cut off and sewn to their mouths, "to the accompaniment of sacrifices, banquets, and wanton behaviour" in sacred places, particularly the groves of Andraste.
Romans rally.
While Boudica's army continued their assault in Verulamium (St. Albans), Suetonius regrouped his forces. According to Tacitus, he amassed a force including his own Legio XIV "Gemina", some "vexillationes" (detachments) of the XX "Valeria Victrix", and any available auxiliaries. The prefect of , Poenius Postumus, stationed near Exeter, ignored the call, and a fourth legion, , had been routed trying to relieve , but nonetheless the governor was able to call on almost ten thousand men.
Suetonius took a stand at an unidentified location, probably in the West Midlands somewhere along the Roman road now known as Watling Street, in a defile with a wood behind him — but his men were heavily outnumbered. Dio says that, even if they were lined up one deep, they would not have extended the length of Boudica's line. By now the rebel forces were said to have numbered 230,000. However, this number should be treated with scepticism — Dio's account is known only from a late epitome, and ancient sources commonly exaggerate enemy numbers.
Boudica exhorted her troops from her chariot, her daughters beside her. Tacitus gives her a short speech in which she presents herself not as an aristocrat avenging her lost wealth, but as an ordinary person, avenging her lost freedom, her battered body, and the abused chastity of her daughters. She said their cause was just, and the deities were on their side; the one legion that had dared to face them had been destroyed. She, a woman, was resolved to win or die; if the men wanted to live in slavery, that was their choice.
However, the lack of manoeuvrability of the British forces, combined with lack of open-field tactics to command these numbers, put them at a disadvantage to the Romans, who were skilled at open combat due to their superior equipment and discipline. Also, the narrowness of the field meant that Boudica could put forth only as many troops as the Romans could at a given time.
First, the Romans stood their ground and used volleys of ' (heavy javelins) to kill thousands of Britons who were rushing toward the Roman lines. The Roman soldiers, who had now used up their ', were then able to engage Boudica's second wave in the open. As the Romans advanced in a wedge formation, the Britons attempted to flee, but were impeded by the presence of their own families, whom they had stationed in a ring of wagons at the edge of the battlefield, and were slaughtered. This is not the first instance of this tactic — the women of the Cimbri, in the Battle of Vercellae against Gaius Marius, were stationed in a line of wagons and acted as a last line of defence. Ariovistus of the Suebi is reported to have done the same thing in his battle against Julius Caesar. Tacitus reports that "according to one report almost eighty thousand Britons fell" compared with only four hundred Romans.
According to Tacitus in his "Annals", Boudica poisoned herself, though in the ' which was written almost twenty years prior he mentions nothing of suicide and attributes the end of the revolt to ' ("indolence"); Dio says she fell sick and died and then was given a lavish burial; though this may be a convenient way to remove her from the story. Considering Dio must have read Tacitus, it is worth noting he mentions nothing about suicide (which was also how Postumus and Nero ended their lives).
Postumus, on hearing of the Roman victory, fell on his sword. Catus Decianus, who had fled to Gaul, was replaced by Gaius Julius Alpinus Classicianus. Suetonius conducted punitive operations, but criticism by Classicianus led to an investigation headed by Nero's freedman Polyclitus. Fearing Suetonius's actions would provoke further rebellion, Nero replaced the governor with the more conciliatory Publius Petronius Turpilianus. The historian Gaius Suetonius Tranquillus tells us the crisis had almost persuaded Nero to abandon Britain.
Location of her defeat.
The location of Boudica's defeat is unknown. Most historians favour a site in the West Midlands, somewhere along the Roman road now known as Watling Street. Kevin K. Carroll suggests a site close to High Cross in Leicestershire, on the junction of Watling Street and the Fosse Way, which would have allowed the , based at Exeter, to rendezvous with the rest of Suetonius's forces, had they not failed to do so. Manduessedum (Mancetter), near the modern town of Atherstone in Warwickshire, has also been suggested, as has "The Rampart" near Messing in Essex, according to legend. More recently, a discovery of Roman artefacts in Kings Norton close to Metchley Camp has suggested another possibility, and a thorough examination of a stretch of Watling Street between St. Albans, Boudica's last known location, and the Fosse Way junction has suggested the Cuttle Mill area of Paulerspury in Northamptonshire, which has topography very closely matching that described by Tacitus of the scene of the battle.
In 2009 it was suggested that the Iceni were returning to East Anglia along the Icknield Way when they encountered the Roman army in the vicinity of Arbury Bank, Hertfordshire. In March 2010, evidence was published suggesting the site may be located at Church Stowe, Northamptonshire.
Historical sources.
Tacitus, the most important Roman historian of this period, took a particular interest in Britain as his father-in-law Gnaeus Julius Agricola served there three times (and was the subject of his first book). Agricola was a military tribune under Suetonius Paulinus, which almost certainly gave Tacitus an eyewitness source for Boudica's revolt. Cassius Dio's account is only known from an epitome, and his sources are uncertain. He is generally agreed to have based his account on that of Tacitus, but he simplifies the sequence of events and adds details, such as the calling in of loans, that Tacitus does not mention.
Gildas, in his 6th century "", may have been alluding to Boudica when he wrote "A treacherous lioness butchered the governors who had been left to give fuller voice and strength to the endeavours of Roman rule".
Cultural depictions.
Boudica and King's Cross.
The area of King's Cross, London was previously a village known as Battle Bridge which was an ancient crossing of the River Fleet. The original name of the bridge was Broad Ford Bridge.
The name "Battle Bridge" led to a tradition that this was the site of a major battle between the Romans and the Iceni tribe led by Boudica. The tradition is not supported by any historical evidence and is rejected by modern historians. However, Lewis Spence's 1937 book "Boadicea — warrior queen of the Britons" went so far as to include a map showing the positions of the opposing armies. There is a belief that she was buried between platforms 9 and 10 in King's Cross station in London, England. There is no evidence for this and it is probably a post-World War II invention.
History and literature.
By the Middle Ages Boudica was forgotten. She makes no appearance in Bede's work, the ', the ' or Geoffrey of Monmouth's "History of the Kings of Britain". But the rediscovery of the works of Tacitus during the Renaissance allowed Polydore Vergil to reintroduce her into British history as "Voadicea" in 1534. Raphael Holinshed also included her story in his "Chronicles" (1577), based on Tacitus and Dio, and inspired Shakespeare's younger contemporaries Francis Beaumont and John Fletcher to write a play, "Bonduca", in 1610. William Cowper wrote a popular poem, "Boadicea, an ode", in 1782.
It was in the Victorian era that Boudica's fame took on legendary proportions as Queen Victoria came to be seen as Boudica's "namesake", their names being identical in meaning. Victoria's Poet Laureate, Alfred, Lord Tennyson, wrote a poem, "Boadicea", and several ships were named after her.
Comics.
She has also appeared in several comic book series, including:
Films.
Boudica has been the subject of multiple films:
Novels.
Boudica's story is the subject of several novels, including books by J. F. Broxholme (a pseudonym of Duncan Kyle),Pauline Gedge, Alan Gold, Roxanne Gregory, Mary Mackie, Diana L. Paxson, Simon Scarrow, Manda Scott, George Shipway, Rosemary Sutcliff, and David Wishart. 
Statue.
A statue of Boudica with her daughters in her war chariot (ahistorically furnished with scythes after the Persian fashion) was executed by Thomas Thornycroft over the 1850s and 1860s with the encouragement of Prince Albert, who lent his horses for use as models. Thornycroft exhibited the head separately in 1864. It was cast in bronze in 1902, 17 years after Thornycroft's death, by his son Sir John, who presented it to the London County Council. They erected it on a plinth on the Victoria Embankment next to Westminster Bridge and the Houses of Parliament, inscribed with the following lines from Cowper's poem:
<poem>
Regions Caesar never knew
Thy posterity shall sway.
</poem>
Ironically, the great anti-imperialist rebel was now identified with the head of the British Empire, and her statue stood guard over the city she razed to the ground.

</doc>
<doc id="4518" url="https://en.wikipedia.org/wiki?curid=4518" title="Borneo">
Borneo

Borneo (; , ) is the third-largest island in the world and the largest island in Asia. At the geographic center of Maritime Southeast Asia, in relation to major Indonesian islands, it is located north of Java, west of Sulawesi, and east of Sumatra.
The island is divided among three countries: Malaysia and Brunei in the north, and Indonesia to the south. Approximately 73% of the island is Indonesian territory. In the north, the East Malaysian states of Sabah and Sarawak make up about 26% of the island. Additionally, the Malaysian federal territory of Labuan is situated on a small island just off the coast of Borneo. The sovereign state of Brunei, located on the north coast, comprises about 1% of Borneo's land area. Antipodal to an area of Amazon rainforest, Borneo is itself home to one of the oldest rainforests in the world.
Etymology.
The island is known by many names; internationally it is known as "Borneo", after Brunei, derived from European historical contact with the kingdom in the 16th century during the Age of Exploration. The name "Brunei" possibly was initially derived from the Sanskrit word """" (), meaning either "ocean" or the mythological Varuna, the Hindu god of the ocean. Indonesian natives called it "Kalimantan," which was derived from the Sanskrit word "Kalamanthana," meaning "burning weather island" (to describe its hot and humid tropical weather).
Prior to that the island was also known by other names. In 977 Chinese records began to use the term "Po-ni" to refer to Borneo. In 1225 it was also mentioned by the Chinese official Chau Ju-Kua (趙汝适). The Javanese manuscript Nagarakretagama, written by Majapahit court poet Mpu Prapanca in 1365, mentioned the island as "Nusa Tanjungnagara", which means the island of the Tanjungpura Kingdom.
Geography.
With an area of , it is the third-largest island in the world, and is the largest island of Asia (the largest continent). Its highest point is Mount Kinabalu in Sabah, Malaysia, with an elevation of .
The largest river system is the Kapuas in West Kalimantan, with a length of . Other major rivers include the Mahakam in East Kalimantan (), the Barito in South Kalimantan (), and Rajang in Sarawak (). 
Borneo has significant cave systems. Clearwater Cave, for example, has one of the world's longest underground rivers. Deer Cave is home to over three million bats, with guano accumulated to over deep.
Before sea levels rose at the end of the last Ice Age, Borneo was part of the mainland of Asia, forming, with Java and Sumatra, the upland regions of a peninsula that extended east from present day Indochina. The South China Sea and Gulf of Thailand now submerge the former low-lying areas of the peninsula. Deeper waters separating Borneo from neighbouring Sulawesi prevented a land connection to that island, creating the divide known as Wallace's Line between Asian and Australia-New Guinea biological regions.
Ecology.
The Borneo rainforest is 140 million years old, making it one of the oldest rainforests in the world. There are about 15,000 species of flowering plants with 3,000 species of trees (267 species are dipterocarps), 221 species of terrestrial mammals and 420 species of resident birds in Borneo. There are about 440 freshwater fish species in Borneo (about the same as Sumatra and Java combined). It is the centre of the evolution and distribution of many endemic species of plants and animals. The Borneo rainforest is one of the few remaining natural habitats for the endangered Bornean orangutan. It is an important refuge for many endemic forest species, including the Borneo elephant, the eastern Sumatran rhinoceros, the Bornean clouded leopard, the Hose's palm civet and the dayak fruit bat. 
In 2010 the World Wide Fund for Nature stated that 123 species have been discovered in Borneo since the "Heart of Borneo" agreement was signed in 2007.
The WWFN has classified the island into seven distinct ecoregions. Most are lowland regions: 
The island historically had extensive rainforest cover, but the area was reduced due to heavy logging for the Malaysian and Indonesian plywood industry. Half of the annual global tropical timber acquisition comes from Borneo. Palm oil plantations have been widely developed and are rapidly encroaching on the last remnants of primary rainforest. Forest fires of 1997 to 1998, started by the locals to clear the forests for plantations were exacerbated by an exceptionally dry El Niño season, worsening the annual shrinkage of the rainforest. During these fires, hotspots were visible on satellite images and the resulting haze affected four countries: Brunei, Malaysia, Indonesia, and Singapore. 
In 2010 Sarawak announced a plan for energy production, the Sarawak Corridor of Renewable Energy, to try to establish sustainability.
History.
Early history.
According to ancient Chinese (977), Indian and Javanese manuscripts, western coastal cities of Borneo had become trading ports by the first millennium. In Chinese manuscripts, gold, camphor, tortoise shells, hornbill ivory, rhinoceros horn, crane crest, beeswax, lakawood (a scented heartwood and root wood of a thick liana, "Dalbergia parviflora"), dragon's blood, rattan, edible bird's nests and various spices were described as among the most valuable items from Borneo. The Indians named Borneo "Suvarnabhumi" (the land of gold) and also "Karpuradvipa" (Camphor Island). The Javanese named Borneo "Puradvipa", or Diamond Island. Archaeological findings in the Sarawak river delta reveal that the area was a thriving trading centre between India and China from the 6th century until about 1300.
One of the earliest evidence of Hindu influence in Southeast Asia were stone pillars which bear inscriptions in the Pallava script, found in Kutai along the Mahakam River in East Kalimantan, dating to around the second half of the 4th century.
By the 14th century, Borneo was under the control of the Majapahit kingdom based in present-day Indonesia. Muslims entered the island and converted many of the indigenous peoples to Islam. 
During the 1450s, Shari'ful Hashem Syed Abu Bakr, an Arab born in Johor, arrived in Sulu from Malacca. In 1457, he founded the Sultanate of Sulu; he titled himself as "Paduka Maulana Mahasari Sharif Sultan Hashem Abu Bakr". The Sultanate of Brunei, during its golden age from the 15th century to the 17th century, ruled a large part of northern Borneo. In 1703 (other sources say 1658), the Sultanate of Sulu received the eastern part of North Borneo from the Sultan of Brunei, after Sulu sent aid against a rebellion in Brunei. 
Dutch and British control.
The Sultanate of Brunei granted large parts of land in Sarawak in 1842 to the English adventurer James Brooke, as reward for his having helped quell a local rebellion. Brooke established the Kingdom of Sarawak and was recognised as its rajah after paying a fee to the Sultanate. He established a monarchy, and the Brooke dynasty (through his nephew and great-nephew) ruled Sarawak for 100 years; the leaders were known as the White Rajahs.
In the early 19th century, British and Dutch governments signed the Anglo-Dutch Treaty of 1824 to exchange trading ports under their controls and assert spheres of influence. This resulted in indirectly establishing British- and Dutch-controlled areas in Borneo, in the north and south, respectively. The Malay and Sea Dayak pirates preyed on maritime shipping in the waters between Singapore and Hong Kong from their haven in Borneo.
The British North Borneo Company controlled the territory of North Borneo (present-day Sabah) from 1882 to 1941.
World War II.
During World War II, Japanese forces gained control and occupied Borneo (1941–45). They decimated many local populations and killed Malay intellectuals, executing all the Malay Sultans of Kalimantan in the Pontianak incidents. Sultan Muhammad Ibrahim Shafi ud-din II of Sambas in Kalimantan was executed in 1944. The Sultanate was thereafter suspended and replaced by a Japanese council. During the Japanese occupation, the Dayak played a role in guerrilla warfare against the occupying forces, particularly in the Kapit Division. They temporarily revived headhunting of Japanese toward the end of the war. Allied Z Special Unit provided assistance to them. After the Fall of Singapore, the Japanese sent several thousand British and Australian prisoners of war to camps in Borneo. At one of the worst sites, around Sandakan in Borneo, only six of some 2,500 prisoners survived. In 1945, the Japanese were defeated by the Allies.
Recent history.
Borneo was the main site of the confrontation between Indonesia and Malaysia between 1962 and about 1969. The British Army was deployed against the Indonesians and communist revolts to gain control of the whole area. Before the formation of Malaysian Federation, the Philippines claimed that the eastern part of the Malaysian state of Sabah was within their territory. They based this on the history of the Sultanate of Sulu's leasing agreement with the British North Borneo Company. 
In 1962 Brunei Revolt, Brunei People's Party wanted to reunify Brunei, Sarawak and Sabah into one federation known as North Borneo Federation or Kesatuan Negara Kalimantan Utara in Malay where the Sultan of Brunei would be the Head of State for the federations. This caused a civil war for few days in British Protectorate Stats of Brunei and Sarawak State.
Demographics.
The demonym for Borneo is Bornean or Bornese.
Borneo has 19.8 million inhabitants (in mid-2010), a population density of 26 inhabitants per square km. Most of the population lives in coastal cities, although the hinterland has small towns and villages along the rivers. The population consists mainly of Dayak ethnic groups, Malay, Banjar, Orang Ulu, Chinese and Kadazan-Dusun. The Chinese, who make up 29% of the population of Sarawak and 17% of total population in West Kalimantan, Indonesia are descendants of immigrants primarily from southeastern China.
In Kalimantan since the 1990s, the Indonesian government has undertaken an intense transmigration program; it financed the relocation to that area of poor, landless families from Java, Madura, and Bali. By 2001, transmigrants made up 21% of the population in Central Kalimantan. Since the 1990s, the indigenous Dayak and Malays have resisted encroachment by these migrants: violent conflict has occurred between some transmigrant and indigenous populations. In the 1999 Sambas riots, Dayaks and Malays joined together to massacre thousands of the Madurese migrants. In Kalimantan, thousands were killed in 2001 fighting between Madurese transmigrants and the Dayak people in the Sampit conflict.
Largest cities.
The following is a list of 20 largest cities in Borneo by population, based on 2010 census for Indonesia and 2010 census for Malaysia. Population data signifies number within official districts and does not include adjoining or nearby conurbation outside defined districts—such as, but not limited to, Kota Kinabalu and Banjarbaru. In other instances, the district area is much larger than the actual city it represents thereby "inflating" the population by including the rural population living further outside the actual city—such as, but not limited to, Tawau and Palangkaraya. 
Administration.
The island of Borneo is divided administratively by three countries.
1) Brunei: Census of Population 2001
2) islands administered as Borneo, geologically part of Borneo, on nearshore islands (2.5 km off the main island of Borneo)
3) Citypopulation.de reports on Official Decennial Censuses in 2010 for both Indonesia and Malaysia, independent estimate for Brunei.

</doc>
<doc id="4519" url="https://en.wikipedia.org/wiki?curid=4519" title="Ballpoint pen">
Ballpoint pen

A ballpoint pen, also known as a "biro", "ball pen", or "dot pen" (in Indian English) is a pen that dispenses ink over a metal ball at its point, i.e. over a "ball point". The metal commonly used is steel, brass, or tungsten carbide. It was conceived and developed as a cleaner and more reliable alternative to quill and fountain pens, and it is now the world's most-used writing instrument: millions are manufactured and sold daily. As a result, it has influenced art and graphic design and spawned an artwork genre.
Pen manufacturers produce designer ballpoint pens for the high-end and collectors' markets.
The Bic Cristal is a popular disposable type of ballpoint pen whose design is recognised by its place in the permanent collection of the Museum of Modern Art, New York.
History.
Origins.
The concept of using a "ball point" within a writing instrument as a method of applying ink to paper has existed since the late 19th century. In these inventions, the ink was placed in a thin tube whose end was blocked by a tiny ball, held so that it could not slip into the tube or fall out of the pen.
The first patent for a ballpoint pen was issued on 30 October 1888, to John J. Loud, who was attempting to make a writing instrument that would be able to write "on rough surfaces-such as wood, coarse wrapping-paper, and other articles" which then-common fountain pens could not. Loud's pen had a small rotating steel ball, held in place by a socket. Although it could be used to mark rough surfaces such as leather, as Loud intended, it proved to be too coarse for letter-writing. With no commercial viability, its potential went unexploited and the patent eventually lapsed.
The manufacture of economical, reliable ballpoint pens as we know them arose from experimentation, modern chemistry, and precision manufacturing capabilities of the early 20th century. Patents filed worldwide during early development are testaments to failed attempts at making the pens commercially viable and widely available. Early ballpoints did not deliver the ink evenly; overflow and clogging were among the obstacles inventors faced toward developing reliable ballpoint pens. If the ball socket were too tight, or the ink too thick, it would not reach the paper. If the socket were too loose, or the ink too thin, the pen would leak or the ink would smear. Ink reservoirs pressurized by piston, spring, capillary action, and gravity would all serve as solutions to ink-delivery and flow problems.
László Bíró, a Hungarian newspaper editor frustrated by the amount of time that he wasted filling up fountain pens and cleaning up smudged pages, noticed that inks used in newspaper printing dried quickly, leaving the paper dry and smudge free. He decided to create a pen using the same type of ink. Bíró enlisted the help of his brother György, a chemist, to develop viscous ink formulae for new ballpoint designs.
Bíró's innovation successfully coupled ink-viscosity with a ball-socket mechanism which act compatibly to prevent ink from drying inside the reservoir while allowing controlled flow. Bíró filed a British patent on 15 June 1938.
In 1941, the Bíró brothers and a friend, Juan Jorge Meyne, fled Germany and moved to Argentina, where they formed "Bíró Pens of Argentina" and filed a new patent in 1943. Their pen was sold in Argentina as the "Birome" (portmanteau of the names Bíró and Meyne), which is how ballpoint pens are still known in that country. This new design was licensed by the British, who produced ballpoint pens for RAF aircrew as the "Biro". Ballpoint pens were found to be more versatile than fountain pens, especially at high altitudes, where fountain pens were prone to ink-leakage.
Bíró's patent, and other early patents on ball-point pens often used the term "ball-point fountain pen".
Postwar proliferation.
Following World War II, many companies vied to commercially produce their own ballpoint pen design. In post-war Argentina, success of the Birome ballpoint was limited, but in mid-1945, the "Eversharp" Co., a maker of mechanical pencils, teamed up with Eberhard Faber Co. to license the rights from Birome for sales in the United States.
During the same period, American entrepreneur Milton Reynolds came across a Birome ballpoint pen during a business trip to Buenos Aires, Argentina. Recognizing commercial potential, he purchased several ballpoint samples, returned to the United States, and founded "Reynolds International Pen Company". Reynolds bypassed the Birome patent with sufficient design alterations to obtain an American patent, beating Eversharp and other competitors to introduce the pen to the U.S. market. Debuting at Gimbels department store in New York City on 29 October 1945, for US$12.50 each (1945 US dollar value), "Reynolds Rocket" became the first commercially successful ballpoint pen. Reynolds went to great extremes to market the pen, with great success; Gimbel's sold many thousands of pens within one week. In Britain, the Miles Martin pen company was producing the first commercially successful ballpoint pens there by the end of 1945.
Neither Reynolds' nor Eversharp's ballpoint lived up to consumer expectations in America. Ballpoint pen sales peaked in 1946, and consumer interest subsequently plunged due to market-saturation. By the early 1950s the ballpoint boom had subsided and Reynolds' company folded.
Paper Mate pens, among the emerging ballpoint brands of the 1950s, bought the rights to distribute their own ballpoint pens in Canada. Facing concerns about ink-reliability, Paper Mate pioneered new ink formulas and advertised them as "banker-approved". In 1954, Parker Pens released "The Jotter" — that company's first ballpoint — boasting additional features and technological advances which also included the use of tungsten-carbide textured ball-bearings in their pens. In less than a year, Parker sold several million pens at prices between three and nine dollars. In the 1960s, the failing Eversharp Co. sold its pen division to Parker and ultimately folded.
Marcel Bich also introduced a ballpoint pen to the American marketplace in the 1950s, licensed from Bíró and based on the Argentine designs. Bich shortened his name to Bic in 1953, becoming the ballpoint brand now recognised globally. Bic pens struggled until the company launched its "Writes The First Time, Every Time!" advertising campaign in the 1960s. Competition during this era forced unit prices to drop considerably.
Types of ballpoint pens.
Ballpoint pens are produced in both disposable and refillable models. Refills allow for the entire internal ink reservoir, including a ballpoint and socket, to be replaced. Such characteristics are usually associated with designer-type pens or those constructed of finer materials. The simplest types of ballpoint pens are disposable and have a cap to cover the tip when the pen is not in use, or a mechanism for retracting the tip, which varies between manufacturers but is usually a spring- or screw-mechanism.
Rollerball pens employ the same ballpoint mechanics, but with the use of water-based inks instead of oil-based inks. Compared to oil-based ballpoints, rollerball pens are said to provide more fluid ink-flow, but the water-based inks will blot if held stationary against the writing surface. Water-based inks also remain wet longer when freshly applied and are thus prone to "smearing"—posing problems to left-handed people (or right handed people writing right-to-left script)—and "running", should the writing surface become wet.
Because of a ballpoint pen's reliance on gravity to coat the ball with ink, most cannot be used to write upside-down. However, technology developed by Fisher pens in the United States resulted in the production of what came to be known as the "Fisher Space Pen". Space Pens combine a more viscous ink with a pressurised ink reservoir which forces the ink toward the point. Unlike standard ballpoints, the rear end of a Space Pen's pressurized reservoir is sealed, eliminating evaporation and leakage, thus allowing the pen to write upside-down, in zero-gravity environments, and reportedly underwater. Astronauts have made use of these pens in outer space.
Ballpoint pens with "erasable ink" were pioneered by the Paper Mate pen company. The ink formulas of erasable ballpoints have properties similar to rubber cement, allowing the ink to be literally rubbed clean from the writing surface before drying and eventually becoming permanent. Erasable ink is much thicker than standard ballpoint inks, requiring pressurised cartridges to facilitate inkflow—meaning they can also write upside-down. Though these pens are equipped with erasers, any eraser will suffice.
The inexpensive, disposable Bic Cristal (also simply "Bic pen" or "Biro") is reportedly the most widely sold pen in the world. It was the Bic company's first product and is still synonymous with the company name. The Bic Cristal is part of the permanent collection at the Museum of Modern Art in New York City, acknowledged for its industrial design. Its hexagonal barrel mimics that of a wooden pencil and is transparent, showing the ink level in the reservoir. The pen's streamlined cap has a small hole to prevent suffocation if children suck it into the throat.
Ballpoint pens are sometimes provided free by businesses, such as hotels, as a form of advertising—printed with a company's name; a ballpoint pen is a relatively low cost advertisement that is highly effective (customers will use, and therefore see, a pen daily). Businesses and charities include ballpoint pens in direct mail campaigns to increase a customer's interest in the mailing. Ballpoints have also been produced to commemorate events, such as a pen commemorating the 1963 assassination of President John F. Kennedy.
As art medium.
Ballpoint pens have proven to be a versatile art medium for professional artists as well as amateur doodlers. Low cost, availability, and portability are cited by practitioners as qualities which make this common writing tool a convenient, alternative art supply. Some artists use them within mixed-media works, while others use them solely as their medium-of-choice.
Effects not generally associated with ballpoint pens can be achieved. Traditional pen-and-ink techniques such as stippling and cross-hatching can be used to create half-tones or the illusion of form and volume. For artists whose interests necessitate precision line-work, ballpoints are an obvious attraction; ballpoint pens allow for sharp lines not as effectively executed using a brush. Finely applied, the resulting imagery has been mistaken for airbrushed artwork and photography, causing reactions of disbelief which ballpoint artist Lennie Mace refers to as the "Wow Factor".
Famous 20th-Century artists such as Andy Warhol, among others, have utilised ballpoint pens to some extent during their careers. Ballpoint pen artwork continues to attract interest in the 21st Century, with contemporary artists gaining recognition for their specific use of ballpoint pens; for their technical proficiency, imagination, and innovation. Korean-American artist Il Lee has been creating large-scale, ballpoint-only abstract artwork since the late 1970s. Since the 1980s, Lennie Mace creates imaginative, ballpoint-only artwork of varying content and complexity, applied to unconventional surfaces including wood and denim. The artist coined terms such as "PENtings" and "Media Graffiti" to describe his varied output. More recently, British artist James Mylne has been creating photo-realistic artwork using mostly black ballpoints, sometimes with minimal mixed-media color. In the mid-2000s (decade), Juan Francisco Casas generated Internet attention for a series of large-scale, photo-realistic ballpoint duplications of his own snapshots of friends, utilising only blue pens.
Using ballpoint pens to create artwork is not without limitations. Color availability and sensitivity of ink to light are among concerns of ballpoint pen artists. Mistakes pose greater risks to ballpoint artists; once a line is drawn, it generally cannot be erased. Additionally, "blobbing" of ink on the drawing surface and "skipping" of ink-flow require consideration when using ballpoint pens for artistic purposes. Although the mechanics of ballpoint pens remain relatively unchanged, ink composition has evolved to solve certain problems over the years, resulting in unpredictable sensitivity to light and some extent of fading.
Manufacturing.
Although designs and construction vary between brands, basic components of all ballpoint pens are universal. Standard components include the freely rotating "ball point" itself (distributing the ink), a "socket" holding the ball in place, and a self-contained "ink reservoir" supplying ink to the ball. In modern pens, narrow plastic tubes contain the ink, which is compelled downward to the ball by gravity. Brass, steel, or tungsten carbide are used to manufacture the ball bearing-like points, then housed in a brass socket.
The function of these components can be compared with the ball-applicator of roll-on antiperspirant; the same technology at a larger scale. The ball point delivers the ink to the writing surface while acting as a "buffer" between the ink in the reservoir and the air outside, preventing the quick-drying ink from drying inside the reservoir. Modern ballpoints are said to have a two-year shelf life, on average.
The common ballpoint pen is a product of mass production, with components produced separately on assembly-lines. Basic steps in the manufacturing process include production of ink formulas, moulding of metal and plastic components, and assembly. Marcel Bich was involved in developing the production of inexpensive ballpoint pens.
Standards.
The International Organization for Standardization has published standards for ball point and roller ball pens:

</doc>
<doc id="4524" url="https://en.wikipedia.org/wiki?curid=4524" title="Burroughs Corporation">
Burroughs Corporation

The Burroughs Corporation was a major American manufacturer of business equipment. The company was founded in 1886 as the American Arithmometer Company, and after the 1986 merger with Sperry Univac was renamed Unisys. The company's history paralleled many of the major developments in computing. At its start it produced mechanical adding machines, and later moved into programmable ledgers and then computers. It was one of the largest producers of mainframe computers in the world, also producing related equipment including typewriters and printers.
Early history.
In 1886, the American Arithmometer Company was established in St. Louis, Missouri to produce and sell an adding machine invented by William Seward Burroughs (grandfather of Beat Generation author William S. Burroughs). In 1904, six years after Burroughs' death, the company moved to Detroit and changed its name to the Burroughs Adding Machine Company. It was soon the biggest adding machine company in America.
Evolving product lines.
The adding machine range began with the basic, hand-cranked P100 which was only capable of adding. The design included some revolutionary features, foremost of which was the dashpot. The P200 offered a subtraction capability and the P300 provided a means of keeping 2 separate totals. The P400 provided a moveable carriage, and the P600 and top-of-the-range P612 offered some limited programmability based upon the position of the carriage. The range was further extended by the inclusion of the "J" series which provided a single finger calculation facility, and the "c" series of both manual and electrical assisted comptometers. In the late 1960s, the Burroughs sponsored "nixi-tube" provided an electronic display calculator. Burroughs developed a range of adding machines with different capabilities, gradually increasing in their capabilities. A revolutionary adding machine was the "Sensimatic", which was able to perform many business functions semi-automatically. It had a moving programmable carriage to maintain ledgers. It could store 9, 18 or 27 balances during the ledger posting operations and worked with a mechanical adder named a Crossfooter. The Sensimatic developed into the "Sensitronic" which could store balances on a magnetic stripe which was part of the ledger card. This balance was read into the accumulator when the card was inserted into the carriage. The Sensitronic was followed by the E1000, E2000, E3000, E4000, E6000 and the E8000, which were computer systems supporting magnetic tape, card reader/punches and a line printer.
Later, Burroughs was selling more than adding machines, including typewriters. But the biggest shift in company history came in 1953: the Burroughs Adding Machine Company was renamed the Burroughs Corporation and began moving into computer products, initially for banking institutions. This move began with Burroughs' purchase, in June 1956, of the ElectroData Corporation in Pasadena, California, a spinoff of the Consolidated Engineering Corporation which had designed test instruments and had a cooperative relationship with Caltech in Pasadena. ElectroData had built the Datatron 205 and was working on the Datatron 220. The first major computer product that came from this marriage was the B205 tube computer. In the late 1960s the L and TC series range was produced (e.g. the TC500—Terminal Computer 500) which had a golf ball printer and in the beginning a 1K (64 bit) disk memory. These were popular as branch terminals to the B5500/6500/6700 systems, and sold well in the banking sector, where they were often connected to non-Burroughs mainframes. In conjunction with these products, Burroughs also manufactured an extensive range of cheque processing equipment, normally attached as terminals to a larger system such as a B2700 or B1700.
A force in the computing industry.
Burroughs was one of the nine major United States computer companies in the 1960s, with IBM the largest, Honeywell, NCR Corporation, Control Data Corporation (CDC), General Electric (GE), Digital Equipment Corporation (DEC), RCA and Sperry Rand (UNIVAC line). In terms of sales, Burroughs was always a distant second to IBM. In fact, IBM's market share was so much larger than all of the others that this group was often referred to as "IBM and the Seven Dwarfs." By 1972 when GE and RCA were no longer in the mainframe business, the remaining five companies behind IBM became known as the BUNCH, an acronym based on their initials.
At the same time, Burroughs was very much a competitor. Like IBM, Burroughs tried to supply a complete line of products for its customers, including Burroughs-designed printers, disk drives, tape drives, computer printing paper, and even typewriter ribbons.
In the 1950s, Burroughs worked with the Federal Reserve Bank on the development and computer processing of magnetic ink character recognition (MICR) especially for the processing of bank cheques. Burroughs made special MICR/OCR sorter/readers which attached to their medium systems line of computers (2700/3700/4700) and this entrenched the company in the computer side of the banking industry.
Developments and innovations.
The Burroughs Corporation developed three highly innovative architectures, based on the design philosophy of "language-directed design". Their machine instruction sets favored one or many high level programming languages, such as ALGOL, COBOL or FORTRAN. All three architectures were considered mainframe class machines:
Merger.
In September 1986, Burroughs Corporation merged with Sperry Corporation to form Unisys. For a time, the combined company retained the Burroughs processors as the A- and V-systems lines. However, as the market for large systems shifted from proprietary architectures to common servers, the company eventually dropped the V-Series line, although customers continued to use V-series systems . Unisys continues to develop and market the A-Series, now known as ClearPath.
Re-emergence of the Burroughs name.
In 2010, UNISYS sold off its Payment Systems Division to Marlin Equity Partners, a California-based private investment firm, which incorporated it as Burroughs Payment Systems based in Plymouth, Michigan.
References in popular culture.
Burroughs B205 hardware has appeared as props in many Hollywood television and film productions from the late 1950s. For example, a B205 console was often shown in the television series "Batman" as the "Bat Computer"; also as the computer in "Lost in Space". B205 tape drives were often seen in series such as "The Time Tunnel" and "Voyage to the Bottom of the Sea".

</doc>
<doc id="4526" url="https://en.wikipedia.org/wiki?curid=4526" title="Brick">
Brick

A brick is building material used to make walls, pavements and other elements in masonry construction. Traditionally, the term brick referred to a unit composed of clay, but it is now used to denote any rectangular units laid in mortar. A brick can be composed of clay-bearing soil, sand and lime, or concrete materials. Bricks are produced in numerous classes, types, materials, and sizes which vary with region and time period, and are produced in bulk quantities. Two basic categories of bricks are "fired" and "non-fired" bricks.
Block is a similar term referring to a rectangular building unit composed of similar materials, but is usually larger than a brick. Lightweight bricks (also called "lightweight blocks") are made from expanded clay aggregate. 
Fired bricks are one of the longest-lasting and strongest building materials, sometimes referred to as artificial stone, and have been used since circa 5000 BC. Air-dried bricks, also known as mudbricks, have a history older than fired bricks, and have an additional ingredient of a mechanical binder such as straw. 
Bricks are laid in "courses" and numerous patterns known as "bonds", collectively known as brickwork, and may be laid in various kinds of mortar to hold the bricks together to make a durable structure.
History.
Middle East and South Asia.
The earliest bricks were "dried brick", meaning they were formed from clay-bearing earth or mud and dried (usually in the sun) until they were strong enough for use. The oldest discovered bricks, originally made from shaped mud and dating before 7500 BC, were found at Tell Aswad, in the upper Tigris region and in southeast Anatolia close to Diyarbakir. Other more recent findings, dated between 7,000 and 6,395 BC, come from Jericho, Catal Hüyük, the ancient Egyptian fortress of Buhen, and the ancient Indus Valley cities of Mohenjo-daro, Harappa, and Mehrgarh. Ceramic, or "fired brick" was used as early as 3000 BC in early Indus Valley cities.
China.
In pre-modern China, bricks were being used from the 2nd millennium BCE at a site near Xi'an. Bricks were produced on a larger scale under the Western Zhou dynasty about 3,000 years ago, and evidence for some of the first fired bricks ever produced has been discovered in ruins dating back to the Zhou. The carpenter's manual "Yingzao Fashi", published in 1103 at the time of the Song dynasty described the brick making process and glazing techniques then in use. Using the 17th century encyclopaedic text "Tiangong Kaiwu", historian Timothy Brook outlined the brick production process of Ming Dynasty China:
In the 21st century, the ILAB has recorded significant instances of child labor and forced labor in the bricks manufacture sector and classified China as one of the 76 countries mentioned in its 2014 "List of Goods Produced by Child Labor or Forced Labor".
Europe.
Early civilisations around the Mediterranean adopted the use of fired bricks, including the Ancient Greeks and Romans. The Roman legions operated mobile kilns, and built large brick structures throughout the Roman Empire, stamping the bricks with the seal of the legion.
During the Early Middle Ages the use of bricks in construction became popular in Northern Europe, after being introduced there from Northern-Western Italy. An independent style of brick architecture, known as brick Gothic (similar to Gothic architecture) flourished in places that lacked indigenous sources of rocks. Examples of this architectural style can be found in modern-day Denmark, Germany, Poland, and Russia.
This style evolved into Brick Renaissance as the stylistic changes associated with the Italian Renaissance spread to northern Europe, leading to the adoption of Renaissance elements into brick building. A clear distinction between the two styles only developed at the transition to Baroque architecture. In Lübeck, for example, Brick Renaissance is clearly recognisable in buildings equipped with terracotta reliefs by the artist Statius von Düren, who was also active at Schwerin (Schwerin Castle) and Wismar (Fürstenhof).
Long distance bulk transport of bricks and other construction equipment remained prohibitively expensive until the development of modern transportation infrastructure, with the construction of canal, roads and railways.
Industrial era.
Production of bricks increased massively with the onset of the Industrial Revolution and the rise in factory building in England. For reasons of speed and economy, bricks were increasingly preferred as building material to stone, even in areas where the stone was available. It was at this time in London, that bright red brick was chosen for construction to make the buildings more visible in the heavy fog and to prevent traffic accidents.
The transition from the traditional method of production known as 'hand-moulding' to a mechanised form of mass production slowly took place during the first half of the nineteenth century. Possibly the first successful brick-making machine was patented by a Mr Henry Clayton, employed at the Atlas Works in Middlesex, England, in 1855, and was capable of producing up to 25,000 bricks daily with minimal supervision. His mechanical apparatus soon achieved widespread attention after it was adopted for use by the South Eastern Railway Company for brick-making at their factory near Folkestone. The Bradley & Craven Ltd ‘Stiff-Plastic Brickmaking Machine’ was patented in 1853, apparently predating Clayton. Bradley & Craven went on to be a dominant manufacturer of brickmaking machinery. Predating both Clayton and Bradley & Craven Ltd. however was the brick making machine patented by Richard A. Ver Valen of Haverstraw, New York in 1852.
The demand for high office building construction at the turn of the 20th century, led to a much greater use of cast and wrought iron and later steel and concrete. The use of brick for skyscraper construction severely limited the size of the building – the Monadnock Building, built in 1896 in Chicago required exceptionally thick walls to maintain the structural integrity of its 17 storeys.
Following pioneering work in the 1950s at the Swiss Federal Institute of Technology and the Building Research Establishment in Watford, UK, the use of improved masonry for the construction of tall structures up to 18 storeys high was made viable. However, the use of brick has largely remained restricted to small to medium-sized buildings, as steel and concrete remain superior materials for high-rise construction.
Methods of manufacture.
Three basic types of brick are un-fired, fired, and chemically set bricks. Each type is manufactured differently.
Mudbrick.
Unfired bricks, also known as mudbricks, are made from a wet, clay-containing soil mixed with straw or similar binders. They are air-dried until ready for use.
Fired brick.
Fired bricks are burned in a kiln which makes them durable. Modern, fired, clay bricks are formed in one of three processes – soft mud, dry press, or extruded.
Normally, brick contains the following ingredients:
The soft mud method is the most common, as it is the most economical. It starts with the raw clay, preferably in a mix with 25–30% sand to reduce shrinkage. The clay is first ground and mixed with water to the desired consistency. The clay is then pressed into steel moulds with a hydraulic press. The shaped clay is then fired ("burned") at 900–1000 °C to achieve strength.
Rail kilns.
In modern brickworks, this is usually done in a continuously fired tunnel kiln, in which the bricks are fired as they move slowly through the kiln on conveyors, rails, or kiln cars, which achieves a more consistent brick product. The bricks often have lime, ash, and organic matter added, which accelerates the burning process.
Bull's Trench Kilns.
In India, brick making is typically a manual process. The most common type of brick kiln in use there is the Bull's Trench Kiln (BTK), based on a design developed by British engineer W. Bull in the late 19th century.
An oval or circular trench is dug, 6–9 metres wide, 2-2.5 metres deep, and 100–150 metres in circumference. A tall exhaust chimney is constructed in the centre. Half or more of the trench is filled with "green" (unfired) bricks which are stacked in an open lattice pattern to allow airflow. The lattice is capped with a roofing layer of finished brick.
In operation, new green bricks, along with roofing bricks, are stacked at one end of the brick pile; cooled finished bricks are removed from the other end for transport to their destinations. In the middle, the brick workers create a firing zone by dropping fuel (coal, wood, oil, debris, and so on) through access holes in the roof above the trench.
The advantage of the BTK design is a much greater energy efficiency compared with clamp or scove kilns. Sheet metal or boards are used to route the airflow through the brick lattice so that fresh air flows first through the recently burned bricks, heating the air, then through the active burning zone. The air continues through the green brick zone (pre-heating and drying the bricks), and finally out the chimney, where the rising gases create suction which pulls air through the system. The reuse of heated air yields savings in fuel cost.
As with the rail process above, the BTK process is continuous. A half dozen labourers working around the clock can fire approximately 15,000–25,000 bricks a day. Unlike the rail process, in the BTK process the bricks do not move. Instead, the locations at which the bricks are loaded, fired, and unloaded gradually rotate through the trench.
Dry pressed bricks.
The dry press method is similar to the soft mud method, but starts with a much thicker clay mix, so it forms more accurate, sharper-edged bricks. The greater force in pressing and the longer burn make this method more expensive.
Extruded bricks.
For extruded bricks the clay is mixed with 10–15% water (stiff extrusion) or 20–25% water (soft extrusion) in a pugmill. This mixture is forced through a die to create a long cable of material of the desired width and depth. This mass is then cut into bricks of the desired length by a wall of wires. Most structural bricks are made by this method as it produces hard, dense bricks, and suitable dies can produce perforations as well. The introduction of such holes reduces the volume of clay needed, and hence the cost. Hollow bricks are lighter and easier to handle, and have different thermal properties from solid bricks. The cut bricks are hardened by drying for 20 to 40 hours at 50 to 150 °C before being fired. The heat for drying is often waste heat from the kiln.
European-style extruded bricks or blocks are used in single-wall construction with finishes applied on the inside and outside. Their many voids comprise a greater proportion of the volume than the solid, thin walls of fired clay. Such bricks are made in 15-, 25-, 30-, 42- and 50-cm widths. Some models have very high thermal insulation properties, making them suitable for zero-energy buildings.
Influences on colour.
The fired colour of tired clay bricks is influenced by the chemical and mineral content of the raw materials, the firing temperature, and the atmosphere in the kiln. For example, pink coloured bricks are the result of a high iron content, white or yellow bricks have a higher lime content. Most bricks burn to various red hues; as the temperature is increased the colour moves through dark red, purple and then to brown or grey at around . The names of bricks may reflect their origin and colour, such as London stock brick and Cambridgeshire White. "Brick tinting" may be performed to change the color of bricks to blend-in areas of brickwork with the surrounding masonry.
An impervious and ornamental surface may be laid on brick either by salt glazing, in which salt is added during the burning process, or by the use of a "slip," which is a glaze material into which the bricks are dipped. Subsequent reheating in the kiln fuses the slip into a glazed surface integral with the brick base.
Types of fired clay bricks.
There are thousands of types of bricks that are named for their use, size, forming method, origin, quality, texture, and/or materials.
Categorized by manufacture method:
Categorized by use:
Specialized use bricks:
Bricks named for place of origin:
Chemically set bricks.
Chemically set bricks are not fired but may have the curing process accelerated by the application of heat and pressure in an autoclave.
Calcium-silicate bricks.
Calcium-silicate bricks are also called sandlime or flintlime bricks depending on their ingredients. Rather than being made with clay they are made with lime binding the silicate material. The raw materials for calcium-silicate bricks include lime mixed in a proportion of about 1 to 10 with sand, quartz, crushed flint or crushed siliceous rock together with mineral colourants. The materials are mixed and left until the lime is completely hydrated; the mixture is then pressed into moulds and cured in an autoclave for three to fourteen hours to speed the chemical hardening. The finished bricks are very accurate and uniform, although the sharp arrises need careful handling to avoid damage to brick and bricklayer. The bricks can be made in a variety of colours; white, black, buff and grey-blues are common and pastel shades can be achieved. This type of brick is common in Sweden, especially in houses built or renovated in the 1970s. In India these are known as fly ash bricks, manufactured using the FaL-G (fly ash, lime and gypsum) process. Calcium-silicate bricks are also manufactured in Canada and the United States, and meet the criteria set forth in ASTM C73 – 10 Standard Specification for Calcium Silicate Brick (Sand-Lime Brick).
Concrete bricks.
Bricks formed from concrete are usually termed blocks, and are typically pale grey in colour. They are made from a dry, small aggregate concrete which is formed in steel moulds by vibration and compaction in either an "egglayer" or static machine. The finished blocks are cured rather than fired using low-pressure steam. Concrete blocks are manufactured in a much wider range of shapes and sizes than clay bricks and are also available with a wider range of face treatments – a number of which simulate the appearance of clay bricks.
Concrete bricks are available in many colours and as an engineering brick made with sulfate-resisting Portland cement or equivalent. When made with adequate amount of cement they are suitable for harsh environments such as wet conditions and retaining walls. They are made to standards BS 6073, EN 771-3. Concrete bricks expand and contract more than clay or sandlime bricks so they need movement joints every 5 to 6 metres, but are similar to other bricks of similar density in thermal and sound resistance and fire resistance.
Compressed earth blocks.
Compressed earth blocks are made mostly from slightly moistened local soils compressed with a mechanical hydraulic press or manual lever press. A small amount of a cement binder may be added, resulting in a "stabalized compressed earth block".
Optimal dimensions, characteristics, and strength.
For efficient handling and laying, bricks must be small enough and light enough to be picked up by the bricklayer using one hand (leaving the other hand free for the trowel). Bricks are usually laid flat and as a result the effective limit on the width of a brick is set by the distance which can conveniently be spanned between the thumb and fingers of one hand, normally about four inches (about 100 mm). In most cases, the length of a brick is about twice its width, about eight inches (about 200 mm) or slightly more. This allows bricks to be laid "bonded" in a structure which increases stability and strength (for an example, see the illustration of bricks laid in "English bond", at the head of this article). The wall is built using alternating courses of "stretchers", bricks laid longways, and "headers", bricks laid crossways. The headers tie the wall together over its width. In fact, this wall is built in a variation of "English bond" called "English cross bond" where the successive layers of stretchers are displaced horizontally from each other by half a brick length. In true "English bond", the perpendicular lines of the stretcher courses are in line with each other.
A bigger brick makes for a thicker (and thus more insulating) wall. Historically, this meant that bigger bricks were necessary in colder climates (see for instance the slightly larger size of the Russian brick in table below), while a smaller brick was adequate, and more economical, in warmer regions. A notable illustration of this correlation is the Green Gate in Gdansk; built in 1571 of imported Dutch brick, too small for the colder climate of Gdansk, it was notorious for being a chilly and drafty residence. Nowadays this is no longer an issue, as modern walls typically incorporate specialised insulation materials.
The correct brick for a job can be selected from a choice of colour, surface texture, density, weight, absorption and pore structure, thermal characteristics, thermal and moisture movement, and fire resistance.
In England, the length and width of the common brick has remained fairly constant over the centuries (but see brick tax), but the depth has varied from about two inches (about 51 mm) or smaller in earlier times to about two and a half inches (about 64 mm) more recently. In the United Kingdom, the usual size of a modern brick is 215 × 102.5 × 65 mm (about × ×  inches), which, with a nominal 10 mm ( inch) mortar joint, forms a "unit size" of 225 × 112.5 × 75 mm (9 × × 3 inches), for a ratio of 6:3:2.
In the United States, modern standard bricks are (controlled by American Society for Testing and Materials ASTM ) about 8 ×   ×  inches (203 × 92 × 57 mm). The more commonly used is the modular brick   ×   ×  inches (194 × 92 × 57 mm). This modular brick of with a mortar joint eases the calculation of the number of bricks in a given run.
Some brickmakers create innovative sizes and shapes for bricks used for plastering (and therefore not visible) where their inherent mechanical properties are more important than their visual ones. These bricks are usually slightly larger, but not as large as blocks and offer the following advantages:
Blocks have a much greater range of sizes. Standard co-ordinating sizes in length and height (in mm) include 400×200, 450×150, 450×200, 450×225, 450×300, 600×150, 600×200, and 600×225; depths (work size, mm) include 60, 75, 90, 100, 115, 140, 150, 190, 200, 225, and 250. They are usable across this range as they are lighter than clay bricks. The density of solid clay bricks is around 2,000 kg/m³: this is reduced by frogging, hollow bricks, and so on, but aerated autoclaved concrete, even as a solid brick, can have densities in the range of 450–850 kg/m³.
Bricks may also be classified as "solid" (less than 25% perforations by volume, although the brick may be "frogged," having indentations on one of the longer faces), "perforated" (containing a pattern of small holes through the brick, removing no more than 25% of the volume), "cellular" (containing a pattern of holes removing more than 20% of the volume, but closed on one face), or "hollow" (containing a pattern of large holes removing more than 25% of the brick's volume). Blocks may be solid, cellular or hollow
The term "frog" can refer to the indentation or the implement used to make it. Modern brickmakers usually use plastic frogs but in the past they were made of wood.
The compressive strength of bricks produced in the United States ranges from about 1000 lbf/in² to 15,000 lbf/in² (7 to 105 MPa or N/mm² ), varying according to the use to which the brick are to be put. In England clay bricks can have strengths of up to 100 MPa, although a common house brick is likely to show a range of 20–40 MPa.
Use.
Bricks are used for homes, buildings, block paving and pavement. 
In the U.S., brick have been used for both buildings and pavements. Examples of brick use in buildings can be seen in colonial era buildings and other notable structures around the country. Brick have been used in pavements especially during the late 19th century and early 20th century. The introduction of asphalt and concrete reduced the use of brick pavements, but it is used as a method of traffic calming or as a decorative surface in pedestrian precincts. For example, in the early 1900s, most of the streets in the city of Grand Rapids, Michigan, were paved with brick. Today, there are only about 20 blocks of brick paved streets remaining (totalling less than 0.5 percent of all the streets in the city limits). This is true in many other cities around the U.S.
Bricks in the metallurgy and glass industries are often used for lining furnaces, in particular refractory bricks such as silica, magnesia, chamotte and neutral (chromomagnesite) refractory bricks. This type of brick must have good thermal shock resistance, refractoriness under load, high melting point, and satisfactory porosity. There is a large refractory brick industry, especially in the United Kingdom, Japan, the United States, Belgium and the Netherlands.
In Northwest Europe, bricks have been used in construction for centuries. Until recently, almost all houses were built almost entirely from bricks. Although many houses are now built using a mixture of concrete blocks and other materials, many houses are skinned with a layer of bricks on the outside for aesthetic appeal.
Engineering bricks are used where strength, low water porosity or acid (flue gas) resistance are needed.
In the UK a redbrick university is one founded and built in the Victorian era. The term is used to refer to such institutions collectively to distinguish them from the older Oxbridge institutions, the post-war 'plate glass' universities, and the 'new' universities of the 1990s.
Colombian architect Rogelio Salmona was noted for his extensive use of red brick in his buildings and for using natural shapes like spirals, radial geometry and curves in his designs. Most buildings in Colombia are made of brick, given the abundance of clay in equatorial countries like this one.
Limitations.
Starting in the 20th century, the use of brickwork declined in some areas due to concerns with earthquakes. Earthquakes such as the San Francisco earthquake of 1906 and the 1933 Long Beach earthquake revealed the weaknesses of unreinforced brick masonry in earthquake-prone areas. During seismic events, the mortar cracks and crumbles, and the bricks are no longer held together. Brick masonry with steel reinforcement, which helps hold the masonry together during earthquakes, was used to replace many of the unreinforced masonry buildings. Retrofitting older unreinforced masonry structures has been mandated in many jurisdictions.

</doc>
<doc id="4527" url="https://en.wikipedia.org/wiki?curid=4527" title="Béla Bartók">
Béla Bartók

Béla Viktor János Bartók (; ; 25 March 1881 – 26 September 1945) was a Hungarian composer and pianist. He is considered one of the most important composers of the 20th century; he and Liszt are regarded as Hungary's greatest composers . Through his collection and analytical study of folk music, he was one of the founders of comparative musicology, which later became ethnomusicology. 
Biography.
Childhood and early years (1881–98).
Béla Bartók was born in the small Banatian town of Nagyszentmiklós in the Kingdom of Hungary, Austria-Hungary (since 1920 Sânnicolau Mare, Romania) on 25 March 1881. Bartók had a diverse ancestry. On his father's side, the Bartók family was a Hungarian lower noble family, originating from Borsodszirák, Borsod county , although his father's mother was of a Roman Catholic Serbian family . Béla Bartók's mother, Paula (born Paula Voit), was an ethnic German, though she spoke Hungarian fluently .
Béla displayed notable musical talent very early in life: according to his mother, he could distinguish between different dance rhythms that she played on the piano before he learned to speak in complete sentences . By the age of four he was able to play 40 pieces on the piano and his mother began formally teaching him the next year.
Béla was a small and sickly child and suffered from severe eczema until the age of 5 . In 1888, when he was seven, his father (the director of an agricultural school) died suddenly. Béla's mother then took him and his sister, Erzsébet, to live in Nagyszőlős (today Vinogradiv, Ukraine) and then to Pozsony (German: Pressburg, today Bratislava, Slovakia). Béla gave his first public recital aged 11 in Nagyszőlős, to a warm critical reception . Among the pieces he played was his own first composition, written two years previously: a short piece called "The Course of the Danube" . Shortly thereafter László Erkel accepted him as a pupil.
Early musical career (1899–1908).
From 1899 to 1903, Bartók studied piano under István Thomán, a former student of Franz Liszt, and composition under János Koessler at the Royal Academy of Music in Budapest. There he met Zoltán Kodály, who influenced him greatly and became his lifelong friend and colleague. In 1903, Bartók wrote his first major orchestral work, "Kossuth", a symphonic poem which honored Lajos Kossuth, hero of the Hungarian Revolution of 1848.
The music of Richard Strauss, whom he met in 1902 at the Budapest premiere of "Also sprach Zarathustra," strongly influenced his early work. When visiting a holiday resort in the summer of 1904, Bartók overheard a young nanny, Lidi Dósa from Kibéd in Transylvania, sing folk songs to the children in her care. This sparked his lifelong dedication to folk music.
From 1907, he also began to be influenced by the French composer Claude Debussy, whose compositions Kodály had brought back from Paris. Bartók's large-scale orchestral works were still in the style of Johannes Brahms and Richard Strauss, but he wrote a number of small piano pieces which showed his growing interest in folk music. The first piece to show clear signs of this new interest is the String Quartet No. 1 in A minor (1908), which contains folk-like elements.
In 1907, Bartók began teaching as a piano professor at the Royal Academy. This position freed him from touring Europe as a pianist and enabled him to work in Hungary. Among his notable students were Fritz Reiner, Sir Georg Solti, György Sándor, Ernő Balogh, and Lili Kraus. After Bartók moved to the United States, he taught Jack Beeson and Violet Archer.
In 1908, he and Kodály traveled into the countryside to collect and research old Magyar folk melodies. Their growing interest in folk music coincided with a contemporary social interest in traditional national culture. They made some surprising discoveries. Magyar folk music had previously been categorised as Gypsy music. The classic example is Franz Liszt's famous "Hungarian Rhapsodies" for piano, which he based on popular art songs performed by Romani bands of the time. In contrast, Bartók and Kodály discovered that the old Magyar folk melodies were based on pentatonic scales, similar to those in Asian folk traditions, such as those of Central Asia, Anatolia and Siberia.
Bartók and Kodály quickly set about incorporating elements of such Magyar peasant music into their compositions. They both frequently quoted folk song melodies "verbatim" and wrote pieces derived entirely from authentic songs. An example is his two volumes entitled "For Children" for solo piano, containing 80 folk tunes to which he wrote accompaniment. Bartók's style in his art music compositions was a synthesis of folk music, classicism, and modernism. His melodic and harmonic sense was profoundly influenced by the folk music of Hungary, Romania, and other nations. He was especially fond of the asymmetrical dance rhythms and pungent harmonies found in Bulgarian music. Most of his early compositions offer a blend of nationalist and late Romanticism elements.
Middle years and career (1909–39).
Personal life.
In 1909 at the age of 28, Bartók married Márta Ziegler (1893–1967), aged 16. Their son, Béla III, was born on 22 August 1910. After nearly 15 years together, Bartók divorced Márta in June 1923.
Two months after his divorce, he married Ditta Pásztory (1903–1982), a piano student, ten days after proposing to her. She was aged 19, he 42. Their son, Péter, was born in 1924.
Opera.
In 1911, Bartók wrote what was to be his only opera, "Bluebeard's Castle", dedicated to Márta. He entered it for a prize by the Hungarian Fine Arts Commission, but they rejected his work as not fit for the stage . In 1917 Bartók revised the score for the 1918 première, and rewrote the ending. Following the 1919 revolution, he was pressured by the new Soviet government to remove the name of the librettist Béla Balázs from the opera , as he was blacklisted and had left the country for Vienna. "Bluebeard's Castle" received only one revival, in 1936, before Bartók emigrated. For the remainder of his life, although he was passionately devoted to Hungary, its people and its culture, he never felt much loyalty to the government or its official establishments.
Folk music and composition.
After his disappointment over the Fine Arts Commission competition, Bartók wrote little for two or three years, preferring to concentrate on collecting and arranging folk music. He collected first in the Carpathian Basin (then the Kingdom of Hungary), where he notated Hungarian, Slovakian, Romanian, and Bulgarian folk music. He also collected in Moldavia, Wallachia, and (in 1913) Algeria. The outbreak of World War I forced him to stop the expeditions; and he returned to composing, writing the ballet "The Wooden Prince" (1914–16) and the String Quartet No. 2 in (1915–17), both influenced by Debussy.
Raised as a Roman Catholic, by his early adulthood Bartók had become an atheist. He believed that the existence of God could not be determined and was unnecessary. He later became attracted to Unitarianism and publicly converted to the Unitarian faith in 1916. As an adult, his son later became president of the Hungarian Unitarian Church .
Bartók wrote another ballet, "The Miraculous Mandarin" influenced by Igor Stravinsky, Arnold Schoenberg, as well as Richard Strauss. A modern story of prostitution, robbery, and murder, it was started in 1918, but not performed until 1926 because of its sexual content. He next wrote his two violin sonatas (written in 1921 and 1922 respectively), which are harmonically and structurally some of his most complex pieces.
In 1927–28, Bartók wrote his Third and Fourth String Quartets, after which his compositions demonstrated his mature style. Notable examples of this period are "Music for Strings, Percussion and Celesta" (1936) and "Divertimento for String Orchestra BB 118" (1939). The Fifth String Quartet was composed in 1934, and the Sixth String Quartet (his last) in 1939.
In 1936 he travelled to Turkey to collect and study folk music. He worked in collaboration with Turkish composer Ahmet Adnan Saygun mostly around Adana (; ).
World War II and last years in America (1940–45).
In 1940, as the European political situation worsened after the outbreak of World War II, Bartók was increasingly tempted to flee Hungary. He was strongly opposed to the Nazis and Hungary's siding with Germany. After the Nazis came to power in the early 1930s, Bartók refused to give concerts in Germany and broke away from his publisher there. His anti-fascist political views caused him a great deal of trouble with the establishment in Hungary. Having first sent his manuscripts out of the country, Bartók reluctantly emigrated to the U.S. with his wife Ditta in October that year. They settled in New York City. After joining them in 1942, their son, Péter Bartók, enlisted in the United States Navy where he served in the Pacific during the remainder of the war and later settled in Florida where he became a recording and sound engineer. His oldest son, Béla Bartók III, remained in Hungary where he survived the war and later worked as a railroad official until his retirement in the early 1980s.
Although he became an American citizen in 1945, shortly before his death , Bartók never became fully at home in the USA. He initially found it difficult to compose. Although well known in America as a pianist, ethnomusicologist and teacher, he was not well known as a composer. There was little American interest in his music during his final years. He and his wife Ditta gave some concerts, although demand for them was low. Bartók, who had made some recordings in Hungary, also recorded for Columbia Records after he came to the US; many of these recordings (some with Bartók's own spoken introductions) were later issued on LP and CD .
Supported by a research fellowship from Columbia University, for several years, Bartók and Ditta worked on a large collection of Serbian and Croatian folk songs in Columbia's libraries. Bartók's economic difficulties during his first years in America were mitigated by publication royalties, teaching and performance tours. While his finances were always precarious, he did not live and die in poverty as was the common myth. He had enough friends and supporters to ensure that there was sufficient money and work available for him to live on. Bartók was a proud man and did not easily accept charity. Despite being short on cash at times, he often refused money that his friends offered him out of their own pockets. Although he was not a member of the ASCAP, the society paid for any medical care he needed during his last two years. Bartók reluctantly accepted this .
The first symptoms of his health problems began late in 1940, when his right shoulder began to show signs of stiffening. In 1942, symptoms increased and he started having bouts of fever, but no underlying disease was diagnosed, in spite of medical examinations. Finally, in April 1944, leukemia was diagnosed, but by this time, little could be done .
As his body slowly failed, Bartók found more creative energy, and he produced a final set of masterpieces, partly thanks to the violinist Joseph Szigeti and the conductor Fritz Reiner (Reiner had been Bartók's friend and champion since his days as Bartók's student at the Royal Academy). Bartók's last work might well have been the String Quartet No. 6 but for Serge Koussevitzky's commission for the Concerto for Orchestra. Koussevitsky's Boston Symphony Orchestra premièred the work in December 1944 to highly positive reviews. The Concerto for Orchestra quickly became Bartók's most popular work, although he did not live to see its full impact. In 1944, he was also commissioned by Yehudi Menuhin to write a Sonata for Solo Violin. In 1945, Bartók composed his Piano Concerto No. 3, a graceful and almost neo-classical work, as a surprise 42nd birthday present for Ditta, but he died just over a month before her birthday, with the scoring not quite finished. He had also sketched his Viola Concerto, but had barely started the scoring at his death, leaving completed only the viola part and sketches of the orchestral part.
Béla Bartók died at age 64 in a hospital in New York City from complications of leukemia (specifically, of secondary polycythemia) on 26 September 1945. His funeral was attended by only ten people. Among them were his wife Ditta, their son Péter, and his pianist friend György Sándor .
Bartók's body was initially interred in Ferncliff Cemetery in Hartsdale, New York. During the final year of communist Hungary in the late 1980s, the Hungarian government, along with his two sons, Béla III and Péter, requested that his remains be exhumed and transferred back to Budapest for burial, where Hungary arranged a state funeral for him on 7 July 1988. He was reinterred at Budapest's Farkasréti Cemetery, next to the remains of Ditta, who died in 1982, the year after his centenary .
The two unfinished works were later completed by his pupil Tibor Serly. György Sándor was the soloist in the first performance of the Third Piano Concerto on February 8, 1946. Ditta Pásztory-Bartók later played and recorded it. The Viola Concerto was revised and published in the 1990s by Bartók's son, Peter; this version may be closer to what Bartók intended .
Concurrently, Peter Bartók, in association with Argentinian musician Nelson Dellamaggiore, worked to reprint and revise past editions of the Third Piano Concerto .
Compositions.
Bartók's music reflects two trends that dramatically changed the sound of music in the 20th century: the breakdown of the diatonic system of harmony that had served composers for the previous two hundred years ; and the revival of nationalism as a source for musical inspiration, a trend that began with Mikhail Glinka and Antonín Dvořák in the last half of the 19th century . In his search for new forms of tonality, Bartók turned to Hungarian folk music, as well as to other folk music of the Carpathian Basin and even of Algeria and Turkey; in so doing he became influential in that stream of modernism which exploited indigenous music and techniques .
One characteristic style of music is his Night music, which he used mostly in slow movements of multi-movement ensemble or orchestral compositions in his mature period. It is characterised by "eerie dissonances providing a backdrop to sounds of nature and lonely melodies" . An example is the third movement (Adagio) of his "Music for Strings, Percussion and Celesta".
His music can be grouped roughly in accordance with the different periods in his life.
Youth: Late-Romanticism (1890–1902).
The works of his youth are of a late-Romantic style. Between 1890 and 1894 (nine to 13 years of age) he wrote 31 pieces with corresponding opus numbers. He started numbering his works anew with "opus 1" in 1894 with his first large scale work, a piano sonata. Up to 1902, Bartók wrote in total 74 works which can be considered in Romantic style. Most of these early compositions are either scored for piano solo or include a piano. Additionally, there is some chamber music for strings.
New influences (1903–11).
Under the influence of Richard Strauss—among other works "Also sprach Zarathustra" , Bartók composed in 1903 "Kossuth", a symphonic poem in ten tableaux. In 1904 followed his "Rhapsody for piano and orchestra" which he numbered opus 1 again, marking it himself as the start of a new era in his music. An even more important occurrence of this year was his overhearing the eighteen-year-old nanny Lidi Dósa from Transylvania sing folk songs, sparking Bartók's lifelong dedication to folk music . When criticised for not composing his own melodies Bartók pointed out that Molière and Shakespeare mostly based their plays on well-known stories too. Regarding the incorporation of folk music into art music he said:
The question is, what are the ways in which peasant music is taken over and becomes transmuted into modern music? We may, for instance, take over a peasant melody unchanged or only slightly varied, write an accompaniment to it and possibly some opening and concluding phrases. This kind of work would show a certain analogy with Bach's treatment of chorales. ... Another method ... is the following: the composer does not make use of a real peasant melody but invents his own imitation of such melodies. There is no true difference between this method and the one described above. ... There is yet a third way ... Neither peasant melodies nor imitations of peasant melodies can be found in his music, but it is pervaded by the atmosphere of peasant music. In this case we may say, he has completely absorbed the idiom of peasant music which has become his musical mother tongue. 
Bartók became first acquainted with Debussy's music in 1907 and regarded his music highly. In an interview in 1939 Bartók said
Debussy's great service to music was to reawaken among all musicians an awareness of harmony and its possibilities. In that, he was just as important as Beethoven, who revealed to us the possibilities of progressive form, or as Bach, who showed us the transcendent significance of counterpoint. Now, what I am always asking myself is this: is it possible to make a synthesis of these three great masters, a living synthesis that will be valid for our time? Debussy's influence is present in the Fourteen Bagatelles (1908). These made Ferruccio Busoni exclaim 'At last something truly new!' . Until 1911, Bartók composed widely differing works which ranged from adherence to romantic-style, to folk song arrangements and to his modernist opera "Bluebeard's Castle". The negative reception of his work led him to focus on folk music research after 1911 and abandon composition with the exception of folk music arrangements (; ).
New inspiration and experimentation (1916–21).
His pessimistic attitude towards composing was lifted by the stormy and inspiring contact with Klára Gombossy in the summer of 1915 . This interesting episode in Bartók's life remained hidden until it was researched by Denijs Dille between 1979 and 1989 . Bartók started composing again, including the Suite for piano opus 14 (1916), and "The Miraculous Mandarin" (1918) and he completed "The Wooden Prince" (1917).
Bartók felt the result of World War I as a personal tragedy . Many regions he loved were severed from Hungary: Transylvania, the Banat where he was born, and Pozsony where his mother lived. Additionally, the political relations between Hungary and the other successor states to the Austro-Hungarian empire prohibited his folk music research outside of Hungary . Bartók also wrote the noteworthy "Eight Improvisations on Hungarian Peasant Songs" in 1920, and the sunny "Dance Suite" in 1923, the year of his second marriage.
"Synthesis of East and West" (1926–45).
In 1926, Bartók needed a significant piece for piano and orchestra with which he could tour in Europe and America. In the preparation for writing his First Piano Concerto, he wrote his Sonata, "Out of Doors", and "Nine Little Pieces", all for solo piano . He increasingly found his own voice in his maturity. The style of his last period—named "Synthesis of East and West" —is hard to define let alone to put under one term. In his mature period, Bartók wrote relatively few works but most of them are large-scale compositions for large settings. Only his voice works have programmatic titles and his late works often adhere to classical forms.
Among his masterworks are all the six string quartets (1908, 1917, 1927, 1928, 1934, and 1939), the "Cantata Profana" (1930, Bartók declared that this was the work he felt and professed to be his most personal "credo" , the "Music for Strings, Percussion and Celesta" (1936), the Concerto for Orchestra (1943) and the Third Piano Concerto (1945).
Bartók also made a lasting contribution to the literature for younger students: for his son Péter's music lessons, he composed "Mikrokosmos", a six-volume collection of graded piano pieces.
Musical analysis.
Paul Wilson lists as the most prominent characteristics of Bartók's music from late 1920s onwards the influence of the Carpathian basin and European art music, and his changing attitude toward (and use of) tonality, but without the use of the traditional harmonic functions associated with major and minor scales .
Although Bartók claimed in his writings that his music was always tonal, he rarely uses the chords or scales of tonality, and so the descriptive resources of tonal theory are of limited use. George and Elliott focus on alternative methods of signaling tonal centers, via axes of inversional symmetry. Others view Bartók's axes of symmetry in terms of atonal analytic protocols. Richard argues that inversional symmetry is often a byproduct of another atonal procedure, the formation of chords from transpositionally related dyads. Atonal pitch-class theory also furnishes the resources for exploring polymodal chromaticism, projected sets, privileged patterns, and large set types used as source sets such as the equal tempered twelve tone aggregate, octatonic scale (and alpha chord), the diatonic and "heptatonia secunda" seven-note scales, and less often the whole tone scale and the primary pentatonic collection .
He rarely used the simple aggregate actively to shape musical structure, though there are notable examples such as the second theme from the first movement of his Second Violin Concerto, commenting that he "wanted to show Schoenberg that one can use all twelve tones and still remain tonal" . More thoroughly, in the first eight measures of the last movement of his Second Quartet, all notes gradually gather with the twelfth (G♭) sounding for the first time on the last beat of measure 8, marking the end of the first section. The aggregate is partitioned in the opening of the Third String Quartet with C♯–D–D♯–E in the accompaniment (strings) while the remaining pitch classes are used in the melody (violin 1) and more often as 7–35 (diatonic or "white-key" collection) and 5–35 (pentatonic or "black-key" collection) such as in no. 6 of the "Eight Improvisations". There, the primary theme is on the black keys in the left hand, while the right accompanies with triads from the white keys. In measures 50–51 in the third movement of the Fourth Quartet, the first violin and cello play black-key chords, while the second violin and viola play stepwise diatonic lines . On the other hand, from as early as the Suite for piano, Op. 14 (1914), he occasionally employed a form of serialism based on compound interval cycles, some of which are maximally distributed, multi-aggregate cycles (; ).
Ernő Lendvai (1971) analyses Bartók's works as being based on two opposing tonal systems, that of the acoustic scale and the axis system, as well as using the golden section as a structural principle.
Milton Babbitt, in his 1949 critique of Bartók's string quartets, criticized Bartók for using tonality and non tonal methods unique to each piece. Babbitt noted that "Bartók's solution was a specific one, it cannot be duplicated" . Bartók's use of "two organizational principles"—tonality for large scale relationships and the piece-specific method for moment to moment thematic elements—was a problem for Babbitt, who worried that the "highly attenuated tonality" requires extreme non-harmonic methods to create a feeling of closure .
Catalogues and opus numbers.
The cataloguing of Bartók's works is somewhat complex. Bartók assigned opus numbers to his works three times, the last of these series ending with the Sonata for Violin and Piano No. 1, Op. 21 in 1921. He ended this practice because of the difficulty of distinguishing between original works and ethnographic arrangements, and between major and minor works. Since his death, three attempts—two full and one partial—have been made at cataloguing. The first, and still most widely used, is András Szőllősy's chronological Sz. numbers, from 1 to 121. Denijs Dille subsequently reorganised the juvenilia (Sz. 1–25) thematically, as DD numbers 1 to 77. The most recent catalogue is that of László Somfai; this is a chronological index with works identified by BB numbers 1 to 129, incorporating corrections based on the Béla Bartók Thematic Catalogue.
On 1 January 2016 his work entered the public domain in the European Union.

</doc>
