<doc id="2144" url="https://en.wikipedia.org/wiki?curid=2144" title="Aaliyah">
Aaliyah

Aaliyah Dana Haughton (; January 16, 1979 – August 25, 2001) was an American singer, dancer, actress, and model.
She was born in Brooklyn, New York, and raised in Detroit, Michigan. At the age of 10, she appeared on the television show "Star Search" and performed in concert alongside Gladys Knight. At age 12, Aaliyah signed with Jive Records and her uncle Barry Hankerson's Blackground Records. Hankerson introduced her to R. Kelly, who became her mentor, as well as lead songwriter and producer of her debut album, "Age Ain't Nothing but a Number". The album sold three million copies in the United States and was certified double platinum by the Recording Industry Association of America (RIAA). After facing allegations of an illegal marriage with R. Kelly, Aaliyah ended her contract with Jive and signed with Atlantic Records.
Aaliyah worked with record producers Timbaland and Missy Elliott for her second album, "One in a Million"; it sold 3 million copies in the United States and over eight million copies worldwide. In 2000, Aaliyah appeared in her first film, "Romeo Must Die". She contributed to the film's soundtrack, which spawned the single "Try Again". The song topped the "Billboard" Hot 100 solely on airplay, making Aaliyah the first artist in "Billboard" history to achieve this goal. "Try Again" earned Aaliyah a Grammy Award nomination for Best Female R&B Vocalist. After completing "Romeo Must Die", Aaliyah filmed her role in "Queen of the Damned". She released her third and final album, "Aaliyah", in July 2001.
On August 25, 2001, Aaliyah and eight others were killed in a plane crash in the Bahamas after filming the music video for the single "Rock the Boat". The pilot, Luis Morales III, was unlicensed at the time of the accident and had traces of cocaine and alcohol in his system. Aaliyah's family later filed a wrongful death lawsuit against Blackhawk International Airways, which was settled out of court. Aaliyah's music has continued to achieve commercial success with several posthumous releases. Aaliyah has sold an estimate of 24 to 32 million albums worldwide. She has been credited for helping redefine contemporary R&B, pop and hip hop, earning her the nicknames "Princess of R&B" and "Queen of Urban Pop". She is listed by "Billboard" as the tenth most successful female R&B artist of the past 25 years and 27th most successful R&B artist in history.
Early life.
Aaliyah Dana Haughton was born on January 16, 1979, in Brooklyn, New York, and was the younger child of Diane and Michael Haughton. She was African American, and was said to have Native American (Oneida) heritage from a grandmother. Her name has been described as a female version of the Arabic "Ali"; however, the original Arabic and Jewish name "Aliya (Hebrew: אליה)" derived from the Hebrew word "aliyah (Hebrew: עלייה)", and meant "highest, most exalted one, the best." Regardless of origin, the singer was highly fond of her Semitic name, expressing support by calling it "beautiful" and asserting that she was "very proud of it," and she thus spent her entire life striving to live up to her name every day. At a young age, Aaliyah was enrolled in voice lessons by her mother. She started performing at weddings, church choir and charity events. When she was five years old, her family moved to Detroit, Michigan, where she was raised along with her older brother, Rashad. She attended a Catholic school, Gesu Elementary, where in first grade, she received a part in the stage play "Annie". From then on, she was determined to become an entertainer. In Detroit, her father began working in the warehouse business, one of his brother-in-law Barry Hankerson's widening interests. Her mother stayed home and raised Aaliyah and her brother.
Throughout her life, she had a good relationship with her brother, which traced back to their childhood as Rashad reflected that growing up with Aaliyah was "amazing". He recalled her running around their home singing and that never being annoying due to her having a "beautiful voice". She and her brother became close with their cousin Jomo Hankerson, since growing up, they lived "about five blocks apart". Jomo walked Aaliyah and Rashad to their home from school when their mother was not able to pick them up and recalled the Haughton household being filled with music. Aaliyah's family was very close due to the struggles of her grandparents and when the Haughtons moved to Detroit, the Hankersons were ready to take them in if necessary. These same bonds led to ties in the music industry, under the Blackground Records label.
Aaliyah's mother was a vocalist, and her uncle, Barry Hankerson, was an entertainment lawyer who had been married to Gladys Knight. As a child, Aaliyah traveled with Knight and worked with an agent in New York to audition for commercials and television programs, including "Family Matters"; she went on to appear on "Star Search" at the age of ten. Aaliyah chose to begin auditioning while her mother made the decision to have her surname dropped. She auditioned for several record labels and at age 11 appeared in concerts alongside Knight. She had several pet animals in during her childhood, which included ducks, snakes and iguanas. Her cousin Jomo had a pet alligator, which Aaliyah felt was too much, remarking, "that was something I wasn't going to stroke."
Her grandmother died in 1991. Years after her death, Aaliyah said her grandmother supported everyone in the family and always wanted to hear her sing, as well as admitting that she "spoiled" her and her brother Rashad "to death." She also enjoyed Aaliyah's singing and would have Aaliyah to sing for her. Aaliyah stated that she thought of her grandmother whenever she fell into depression. Aaliyah's hands reminded her of her aunt, who died when she was "very young" and Aaliyah referred to her as an "amazingly beautiful woman".
Education.
Aaliyah attended Detroit schools growing up and believed she was well-liked, but got teased for her short stature. She recalled coming into her own prior to age 15 and grew to love her height. Her mother would tell her to be happy that she was small and compliment her. Other children disliked Aaliyah, but she did not stay focused on them. "You always have to deal with people who are jealous, but there were so few it didn't even matter. The majority of kids supported me, which was wonderful. When it comes to dealing with negative people, I just let it in one ear and out the other. Those people were invisible to me." Even in her adult life, she considered herself small. She had "learned to accept and love" herself and added: "... the most important thing is to think highly of yourself because if you don't, no one else will".
During her audition for acceptance to the Detroit High School for the Fine and Performing Arts Aaliyah sung the song "Ave Maria" in its entirety in the Italian language.
Aaliyah, who maintained a perfect 4.0 grade point average when graduating from Detroit High School for the Fine and Performing Arts, felt education was important. She saw fit to keep her grades up despite the pressures and time constraints brought on her during the early parts of her career. She labeled herself as a perfectionist and recalled always being a good student. Aaliyah reflected: "I always wanted to maintain that, even in high school when I first started to travel. I wanted to keep that 4.0. Being in the industry, you know, I don't want kids to think, 'I can just sing and forget about school.' I think it's very important to have an education, and even more important to have something to fall back on." She did this in her own life, as she planned to "fall back on" another part of the entertainment industry. She believed that if she could teach music history or open her own school to teach that or drama if she did not make a living as a recording artist because, as she reasoned, "when you pick a career it has to be something you love".
Career.
1991–95: "Age Ain't Nothing but a Number".
After Hankerson signed a distribution deal with JIVE Records, he signed Aaliyah to his Blackground Records label at the age of 12. Hankerson later introduced her to recording artist and producer R. Kelly, who became Aaliyah's mentor, as well as lead songwriter and producer of the album, which was recorded when she was 14. Aaliyah's debut album, "Age Ain't Nothing but a Number", was released under JIVE and Blackground Records; the album debut at number 24 on the "Billboard" 200 chart, selling 74,000 copies in its first week. It ultimately peaked at number 18 on the "Billboard" 200 and sold over three million copies in the United States, where it was certified two times Platinum by the RIAA. In Canada, the album sold over 50,000 copies and was certified gold by the CRIA. Aaliyah's debut single, "Back & Forth", topped the "Billboard" Hot R&B/Hip-Hop Songs chart for three weeks and was certified Gold by the RIAA. The second single, a cover of The Isley Brothers' "At Your Best (You Are Love)", peaked at number six on the "Billboard" Hot 100 and was also certified Gold by the RIAA. The title track, "Age Ain't Nothing but a Number", peaked at number 75 on the Hot 100. Additionally, she released "The Thing I Like" as part of the soundtrack to the 1994 film "A Low Down Dirty Shame".
"Age Ain't Nothing But a Number" received generally favorable reviews from music critics. Some writers noted that Aaliyah's "silky vocals" and "sultry voice" blended with Kelly's new jack swing helped define R&B in the 1990s. Her sound was also compared to that of female quartet En Vogue. Christopher John Farley of "Time" magazine described the album as a "beautifully restrained work", noting that Aaliyah's "girlish, breathy vocals rode calmly on R. Kelly's rough beats". Stephen Thomas Erlewine of AllMusic felt that the album had its "share of filler", but described the singles as "slyly seductive". He also claimed that the songs on the album were "frequently better" than that of Kelly's second studio album, "12 Play". The single "At Your Best (You Are Love)" was criticized by "Billboard" for being out of place on the album and for its length.
1996–99: "One in a Million".
In 1996, Aaliyah left JIVE Records and signed with Atlantic Records. She worked with record producers Timbaland and Missy Elliott, who contributed to her second studio album, "One in a Million". Missy Elliott recalled Timbaland and herself being nervous to work with Aaliyah, since Aaliyah had already released her successful début album while Missy Elliott and Timbaland were just starting out. Missy Elliott also feared she would be a diva, but reflected that Aaliyah "came in and was so warming; she made us immediately feel like family."
The album yielded the single "If Your Girl Only Knew", which topped the "Billboard" Hot R&B/Hip-Hop Songs for two weeks. It also generated the singles "Hot Like Fire" and "4 Page Letter". The following year, Aaliyah was featured on Timbaland & Magoo's debut single, "Up Jumps da Boogie". "One in a Million" peaked at number 18 on the "Billboard" 200, selling 3 million copies in the United States and over eight million copies worldwide.
The album was certified double platinum by the RIAA on June 16, 1997, denoting shipments of two million copies. The month prior to "One in a Million"s release, on May 5, 1997, music publisher Windswept Pacific filed a lawsuit in U.S. District Court against Aaliyah claiming she had illegally copied Bobby Caldwell's "What You Won't Do for Love" for the single "Age Ain't Nothing but a Number".
Aaliyah attended the Detroit High School for the Fine and Performing Arts, where she majored in drama and graduated in 1997 with a 4.0 GPA. Aaliyah began her acting career that same year; she played herself in the police drama television series "New York Undercover". During this time, Aaliyah participated in the Children's Benefit Concert, a charity concert that took place at the Beacon Theatre in New York. Aaliyah also became the spokesperson for Tommy Hilfiger Corporation. In 1997 Aaliyah performed the Christmas carol What Child Is This at the annual holiday special Christmas in Washington. She contributed on the soundtrack album for the Fox Animation Studios animated feature "Anastasia", performing a cover version of "Journey to the Past" which earned songwriters Lynn Ahrens and Stephen Flaherty a nomination for the Academy Award for Best Original Song. Aaliyah performed the song at the 1998 Academy Awards ceremony and became the youngest singer to perform at the event. The song "Are You That Somebody?" was featured on the "Dr. Dolittle" soundtrack, which earned Aaliyah her first Grammy Award nomination. The song peaked at number 21 on the Hot 100.
2000: "Romeo Must Die".
In 1999, Aaliyah landed her first film role in "Romeo Must Die", released March 22, 2000. Aaliyah starred opposite martial artist Jet Li, playing a couple who fall in love amid their warring families. It grossed US$18.6 million in its first weekend, ranking number two at the box office. Aaliyah purposely stayed away from reviews of the film to "make it easier on" herself, but she heard "that people were able to get into me, which is what I wanted." In contrast, some critics felt there was no chemistry between her and Jet Li, as well as viewing the film was too simplistic. This was echoed by Elvis Mitchell of "The New York Times", who wrote that while Aaliyah was "a natural" and the film was conceived as a spotlight for both her and Li, "they have so little chemistry together you'd think they're putting out a fire instead of shooting off sparks. Her role was well received by Glen Oliver by IGN who liked that she did not portray her character "as a victimized female" but instead "as a strong female who does not come across as an over-the-top Women's Right Advocate."
In addition to acting, Aaliyah served as an executive producer of the film's soundtrack, where she contributed four songs. "Try Again" was released as a single from the soundtrack; the song topped the "Billboard" Hot 100, making Aaliyah the first artist to top the chart based solely on airplay; this led the song to be released in a 12" vinyl and 7" single. The music video won the Best Female Video and Best Video from a Film awards at the 2000 MTV Video Music Awards. It also earned her a Grammy Award nomination for Best Female R&B Vocalist. The soundtrack went on to sell 1.5 million copies in the United States.
2001: "Aaliyah".
After completing "Romeo Must Die", Aaliyah began to work on her second film, "Queen of the Damned". She played the role of an ancient vampire, Queen Akasha, which she described as a "manipulative, crazy, sexual being". Prior to her death, she expressed the possibility of recording songs for the film's soundtrack and welcomed the possibility of collaborating with Jonathan Davis. She was scheduled to film for the sequels of "The Matrix" as the character Zee.
In May 2001, Shaquille O'Neal admitted that his remarks where he claimed to have engaged in sexual intercourse with Aaliyah, Cindy Crawford and Venus Williams were false after making the allegations during an appearance on a radio station and apologized to the three. All three denied the claims. The following month, June 2001, Aaliyah posed for a photo shoot with Eric Johnson. Johnson kept the images in his "private personal archive" for thirteen years before providing digital copies of 13 Aaliyah photographs to an online photography magazine and authorizing the publication to use the photographs for a story they were doing on Aaliyah. Not long after, he filed a lawsuit claiming ABC had infringed his rights since the corporation authorized further reproduction by reproducing them online.
Aaliyah released her self-titled album, "Aaliyah", in July 2001. It debuted at number two on the "Billboard" 200, selling 187,000 copies in its first week. The first single from the album, "We Need a Resolution", peaked at number 59 on the "Billboard" Hot 100. She finished recording the album in March 2001 after a year of recording tracks that began in March of the previous year. At the time she started recording the album, Aaliyah's publicist disclosed the album's release date as most likely being in October 2000.
Filming for "Queen of the Damned" delayed the release of "Aaliyah". Aaliyah enjoyed balancing her singing and acting careers. Though she called music a "first" for her, she also had been acting since she was young and had wanted to begin acting "at some point in my career," but "wanted it to be the right time and the right vehicle" and felt "Romeo Must Die" "was it".
"Aaliyah" was released five years after "One in a Million". Aaliyah had not intended for the albums to have such a gap between them. "I wanted to take a break after "One in a Million" to just relax, think about how I wanted to approach the next album. Then, when I was ready to start back up, "Romeo" happened, and so I had to take another break and do that film and then do the soundtrack, then promote it. The break turned into a longer break than I anticipated." Connie Johnson of the "Los Angeles Times" argued that Aaliyah having to focus on her film career may have caused her to not give the album "the attention it merited." Collaborator Timbaland concurred, stating that he was briefly in Australia to work on the album while Aaliyah was filming and did not feel the same production had gone into "Aaliyah" as "One in a Million" had. He also said Virgin Records had rushed the album and Aaliyah had specifically requested Missy Elliott and Timbaland work on "Aaliyah" with her.
The week after Aaliyah's death, her third studio album, "Aaliyah", rose from number 19 to number one on the "Billboard" 200. "Rock the Boat" was released as a posthumous single. The music video premiered on BET's "Access Granted"; it became the most viewed and highest rated episode in the history of the show. The song peaked at number 14 on the "Billboard" Hot 100 and number two on the "Billboard" Hot R&B/Hip-Hop Songs chart. It was also included on the "Now That's What I Call Music! 8" compilation series; a portion of the album's profits was donated to the Aaliyah Memorial Fund. Promotional posters for "Aaliyah" that had been put up in major cities such as New York and Los Angeles became makeshift memorials for grieving fans.
"More than a Woman" and "I Care 4 U" were released as posthumous singles and peaked within the top 25 of the "Billboard" Hot 100. The album was certified double Platinum by the RIAA and sold 2.95 million copies in the United States. "More than a Woman" reached number one on the UK singles chart making Aaliyah the first deceased artist to reach number one on the UK singles chart. "More than a Woman" was replaced by George Harrison's "My Sweet Lord" which is the only time in the UK singles chart's history where a dead artist has replaced another dead artist at number one. In July 2001, she allowed MTV's show "Diary" behind-the-scenes access to her life and stated "I am truly blessed to wake up every morning to do something that I love; there is nothing better than that." She continued, "Everything is worth it – the hard work, the times when you're tired, the times when you are a bit sad. In the end, it's all worth it because it really makes me happy. I wouldn't trade it for anything else in the world. I've got good friends, a beautiful family and I've got a career. I thank God for his blessings every single chance I get."
Aaliyah was signed to appear in several future films, including "Honey", a romantic film titled "Some Kind of Blue", and a Whitney Houston-produced remake of the 1976 film "Sparkle". Whitney Houston recalled Aaliyah being "so enthusiastic" about the film and wanting to appear in the film "so badly". Houston also voiced her belief that Aaliyah was more than qualified for the role and the film was shelved after she died, since Aaliyah had "gone to a better place". Studio officials of Warner Brothers stated that Aaliyah and her mother had both read the script for "Sparkle". According to them, Aaliyah was passionate about playing the lead role of a young singer in a girl group.
The film was released in 2012, eleven years after Aaliyah's death. Before her death, Aaliyah had filmed part of her role in "The Matrix Reloaded" and was scheduled to appear in "The Matrix Revolutions" as Zee. Aaliyah told "Access Hollywood" that she was "beyond happy" to have landed the role.
The role was subsequently recast to Nona Gaye. Aaliyah's scenes were included in the tribute section of the "Matrix Ultimate Collection" series.
In November 2001, Ronald Isley stated that Aaliyah and the Isley Brothers had discussed a collaboration prior to her death. She had previously covered the Isley Brothers' single "At Your Best (You Are Love)". By 2001, Aaliyah had enjoyed her now seven-year career and felt a sense of accomplishment. "This is what I always wanted," she said of her career in "Vibe" magazine. "I breathe to perform, to entertain, I can't imagine myself doing anything else. I'm just a really happy girl right now. I honestly love every aspect of this business. I really do. I feel very fulfilled and complete."
Artistry.
Voice and style.
Aaliyah had the vocal range of a soprano. With the release of her debut single "Back & Forth", Dimitri Ehrlich of "Entertainment Weekly" expressed that Aaliyah's "silky vocals are more agile than those of self-proclaimed queen of hip-hop soul Mary J. Blige." In her review for Aaliyah's second studio album One in a Million "Vibe" magazine, music critic Dream Hampton said that Aaliyah's "deliciously feline" voice has the same "pop appeal" as Janet Jackson's. Aaliyah described her sound as "street but sweet", which featured her "gentle" vocals over a "hard" beat. Though Aaliyah did not write any of her own material, her lyrics were described as in-depth. She incorporated R&B, pop and hip hop into her music. Her songs were often uptempo and at the same time often dark, revolving around "matters of the heart". After her R. Kelly-produced debut album, Aaliyah worked with Timbaland and Missy Elliott, whose productions were more electronic. Sasha Frere-Jones of "The Wire" finds Aaliyah's "Are You That Somebody?" to be Timbaland's "masterpiece" and exemplary of his production's start-stop rhythms, with "big half-second pauses between beats and voices". Keith Harris of "Rolling Stone" cites "Are You That Somebody?" as "one of '90s R&B's most astounding moments".
Aaliyah's songs have been said to have "crisp production" and "staccato arrangements" that "extend genre boundaries" while containing "old-school" soul music. Kelefah Sanneh of "The New York Times" called Aaliyah "a digital diva who wove a spell with ones and zeroes", and writes that her songs comprised "simple vocal riffs, repeated and refracted to echo the manipulated loops that create digital rhythm", as Timbaland's "computer-programmed beats fitted perfectly with her cool, breathy voice to create a new kind of electronic music." When she experimented with other genres on "Aaliyah", such as Latin pop and heavy metal, "Entertainment Weekly"s Craig Seymour panned the attempt. While Analyzing her eponymous album British publication NME (New Musical Express) felt that Aaliyah's radical third album was intended to consolidate her position as U.S.R&B's most experimental artist. As her albums progressed, writers felt that Aaliyah matured, calling her progress a "declaration of strength and independence". ABC News noted that Aaliyah's music was evolving from the punchy pop influenced Hip hop and R&B to a more mature, introspective sound on her third album. Stephen Thomas Erlewine of AllMusic described her eponymous album, "Aaliyah", as "a statement of maturity and a stunning artistic leap forward" and called it one of the strongest urban soul records of its time. She portrayed "unfamiliar sounds, styles and emotions", but managed to please critics with the contemporary sound it contained. Ernest Hardy of "Rolling Stone" felt that Aaliyah reflected a stronger technique, where she gave her best vocal performance. Prior to her death, Aaliyah expressed a desire to learn about the burgeoning UK garage scene she had heard about at the time.
Influences.
As an artist, Aaliyah often voiced that she was inspired by a number of performers. These include Michael Jackson, Stevie Wonder, Sade, En Vogue, Nine Inch Nails, Korn, Prince, Naughty by Nature, Johnny Mathis, Janet Jackson and Barbra Streisand. Aaliyah expressed that Michael Jackson's "Thriller" was her "favorite album" and that "nothing will ever top "Thriller"." She stated that she admired Sade because "she stays true to her style no matter what ... she's an amazing artist, an amazing performer ... and I absolutely love her." Aaliyah expressed she had always desired to work with Janet Jackson, whom she had drawn frequent comparison to over the course of her career, stating "I admire her a great deal. She's a total performer ... I'd love to do a duet with Janet Jackson." Jackson reciprocated Aaliyah's affections, commenting "I've loved her from the beginning because she always comes out and does something different, musically." Jackson also stated she would have enjoyed collaborating with Aaliyah.
Image.
Aaliyah focused on her public image throughout her career. She often wore baggy clothes and sunglasses, stating that she wanted to be herself. She described her image as being "important ... to differentiate yourself from the rest of the pack". She often wore black clothing, starting a trend for similar fashion among women in United States and Japan. Aaliyah's fashionable style has been credited for being an influence on new fashion trend's called Health Goth and "Ghetto Goth" also known as GHE20 GOTH1K Aaliyah participated in fashion designer Tommy Hilfiger's All America Tour and was featured in Tommy Jean ads, which depicted her in boxer shorts, baggy jeans and a tube top. Hilfiger's brother, Andy, called it "a whole new look" that was "classy but sexy". Carson Daly A former VJ on MTV's "Total Request Live commented on Aaliyah's style by saying that she was "cutting edge" ,"always one step ahead of the curve" and that "the TRL audience looks to her to figure out what's hot and what's new".
When she changed her hairstyle, Aaliyah took her mother's advice to cover her left eye, much like Veronica Lake. The look has become known as her signature and been referred to as fusion of "unnerving emotional honesty" and "a sense of mystique". In 1998, she hired a personal trainer to keep in shape, and exercised five days a week and ate diet foods. Aaliyah was praised for her "clean-cut image" and "moral values". Robert Christgau of "The Village Voice" wrote of Aaliyah's artistry and image, "she was lithe and dulcet in a way that signified neither jailbait nor hottie—an ingenue whose selling point was sincerity, not innocence and the obverse it implies."
Aaliyah was viewed by others as a role model. Emil Wilbekin, described by CNN as "a friend of Aaliyah's" and follower of her career, explained: "Aaliyah is an excellent role model, because she started her career in the public eye at age 15 with a gold album entitled "Age Ain't Nothing but a Number". And then her second album, "One in a Million" went double platinum. She had the leading role in "Romeo Must Die", which was a box office success. She's won numerous awards, several MTV music video awards, and aside from her professional successes, many of her lyrics are very inspirational and uplifting. She also carried herself in a very professional manner. She was well spoken. She was beautiful, but she didn't use her beauty to sell her music. She used her talent. Many young hip-hop fans greatly admire her."
She was seen by yet others as a sex symbol. Aaliyah did not have problem with being considered one. "I know that people think I'm sexy and I am looked at as that, and it is cool with me," she stated. "It's wonderful to have sex appeal. If you embrace it, it can be a very beautiful thing. I am totally cool with that. Definitely. I see myself as sexy. If you are comfortable with it, it can be very classy and it can be very appealing." The single "We Need a Resolution" was argued to have transformed "the once tomboy into a sexy grown woman". Aaliyah mentioned that her mother, during her childhood, would take pictures of her and notice a sex appeal. She reinforced her mother's belief by saying that she did feel "sexy for sure" and that she embraced it and was comfortable with this view of her.
Personal life.
In her spare time, she was mostly a home person, which dated back to her younger years, but on occasion went out and played laser tag. She reasoned this was due to her liking "the simple things in life". Despite making a profit from her career that allowed her to purchase the vehicle she wanted, Aaliyah revealed during her final interview on August 21, 2001 on "106 & Park" that she had never owned a car due to living in New York City and hiring a car or driver on a regular basis.
Family.
Aaliyah's family played a major role in the course of her career. Aaliyah's father Michael Haughton, who died in 2012, served as her personal manager. Her mother assisted her in her career while brother Rashad Haughton and cousin Jomo Hankerson worked with her consistently. Her father's illness ended his co-management of Aaliyah with her mother Diane Haughton. She ran all of her decisions by Rashad.
Aaliyah was known to have usually been accompanied by members of her immediate family and the "Rock the Boat" filming was credited by Rashad Haughton as being the first and only time her family was not present. In October 2001, Rashad stated: "It really boggles everyone from Day One, every single video she ever shot there's always been myself or my mother or my father there. The circumstances surrounding this last video were really strange because my mother had eye surgery and couldn't fly. That really bothered her because she always traveled. My dad had to take care of my mom at that time. And I went to Australia to visit some friends. We really couldn't understand why we weren't there. You ask yourself maybe we could have stopped it. But you can't really answer the question. There's always gonna be that question of why." Her friend Kidada Jones said in the last year of her life her parents had given her more freedom and she had talked about wanting a family. "She wanted to have a family, and we talked about how we couldn't wait to kick back with our babies."
Gladys Knight, who married her uncle Barry Hankerson, was essential to the start of Aaliyah's career as she gave her many of her earlier performances. One of their last conversations concerned Aaliyah having difficulty with "another young artist" that she was trying to work with. Knight felt the argument was "petty" and insisted that she remain being who she was in spite of the conflict.
Illegal marriage.
With the release of "Age Ain't Nothing but a Number", rumors circulated of a relationship between Aaliyah and R. Kelly. Shortly after, there was speculation about a secret marriage with the release of "Age Ain't Nothing but a Number" and the adult content that Kelly had written for Aaliyah. "Vibe" magazine later revealed a marriage certificate that listed the couple married on August 31, 1994, in Sheraton Gateway Suites in Rosemont, Illinois. Aaliyah, who was 15 at the time, was listed as 18 on the certificate; the illegal marriage was annulled in February 1995 by her parents. The pair continued to deny marriage allegations, stating that neither was married. One particular allegation among the rumor was that Aaliyah wedded R. Kelly without her parents' knowledge.
Aaliyah reportedly developed a friendship with R. Kelly during the recording of her debut album. As she recalled to "Vibe" magazine in 1994, she and R. Kelly would "go watch a movie" and "go eat" when she got tired and would then "come back and work". She described the relationship between her and R. Kelly as being "rather close." In 2016, Kelly said that he was in love with Aaliyah as he was with "anybody else."
In December 1994, Aaliyah told the "Sun-Times" that whenever she was asked about being married to R. Kelly, she urged them not to believe "all that mess" and that she and R. Kelly were "close" and "people took it the wrong way."
In his 2011 book "The Man Behind the Man: Looking From the Inside Out", Demetrius Smith Sr., a former member of R. Kelly's entourage, wrote that R. Kelly told him "in a voice that sounded as if he wanted to burst into tears" that he thought Aaliyah was pregnant.
Jamie Foster Brown in the 1994 issue of "Sister 2 Sister" wrote that "R. Kelly told me that he and Aaliyah got together and it was just magic." Brown also reported hearing about a relationship between them. "I've been hearing about Robert and Aaliyah for a while—that she was pregnant. Or that she was coming and going in and out of his house. People would see her walking his dog, 12 Play, with her basketball cap and sunglasses on. Every time I asked the label, they said it was platonic. But I kept hearing complaints from people about her being in the studio with all those men. At 15," Brown said. "you have all those hormones and no brains attached to them."
Aaliyah admitted in court documents that she had lied about her age. In May 1997, she filed suit in Cook County seeking to have all records of the marriage expunged because she was not old enough under state law to get married without her parents' consent. It was reported that she cut off all professional and personal ties with R. Kelly after the marriage was annulled and ceased having contact with him. In 2014, Jomo Hankerson stated that Aaliyah "got villainized" over her relationship with R. Kelly and the scandal over the marriage made it difficult to find producers for her second album. "We were coming off of a multi-platinum debut album and except for a couple of relationships with Jermaine Dupri and Puffy, it was hard for us to get producers on the album." Hankerson also expressed confusion over why "they were upset" with Aaliyah given her age at the time.
Aaliyah was known to avoid answering questions regarding R. Kelly following the professional split. During an interview with Christopher John Farley, she was asked if she was still in contact with him and if she would ever work with him again. Farley said Aaliyah responded with a "firm, frosty" 'No' to both of the questions. "Vibe" magazine said Aaliyah changed the subject anytime "you bring up the marriage with her". A spokeswoman for Aaliyah told the "Chicago Sun-Times" in 2000 that when "R. Kelly comes up, she doesn't even speak his name, and nobody's allowed to ask about it at all". Kelly later commented that Aaliyah had opportunities to address the pair's relationship after they separated professionally but chose not to.
R. Kelly would have other allegations made about him regarding underage girls in the years following her death and his marriage to Aaliyah was used to evidence his involvement with them. He refused to discuss his relationship with her, citing that she was dead. "Out of respect for her, and her mom and her dad, I will not discuss Aaliyah. That was a whole other situation, a whole other time, it was a whole other thing, and I'm sure that people also know that." Aaliyah's mother Diane Haughton reflected that everything "that went wrong in her life" began with her relationship with R. Kelly. The allegations have been said to have done "little to taint Aaliyah's image or prevent her from becoming a reliable '90s hitmaker with viable sidelines in movies and modeling."
Relationships.
Quincy Jones said she was "like one of my daughters" and Aaliyah vacationed with him and his family in Fiji. She was close friends with his daughter Kidada Jones. By 2001, they had been best friends for five years and Jones described her as having a great sense of humor. Aaliyah and Jones would make prank phone calls to what Aaliyah referred to as "public establishments". At the time of her death, she and Jones were planning on starting a clothing line, benefited by Aaliyah's popularity as a "style-setter" and she sought to capitalize on her good taste. Her brother Rashad called her his best friend and stated that she "was my everything".
Beyoncé stated after Aaliyah's death that she was "the very first person to embrace Destiny's Child." Aaliyah met Beyoncé as well as the other members of Destiny's Child in 1998 in Los Angeles. They were intimidated about meeting her, since she had already established herself and they were just making their names at the time. Beyoncé observed Aaliyah as having "handled herself like a lady" and commented on the "aura" she had, which she believed was an indication of Aaliyah having "a great family."
In addition to working with them, Aaliyah had friendships with Missy Elliott and Timbaland. Aaliyah recalled her first time meeting Missy Elliott. "When we met, there was a bond that was established real quickly. A friendship formed and we built our studio relationship from that." Missy Elliott said in 2010 that "there's not a day that goes by that I don't think" of Aaliyah and that she misses her every day.
Missy Elliott said of Aaliyah during an appearance on RapFix Live in November 2012, "Aaliyah, she was like a comedian. She always wanted to laugh."
Timbaland admitted in 2011 that he was in love with her, but did not act out on his feelings due to their age difference and determined he would just "be her brother". Despite this declaration, he still struggled with keeping his feelings to himself. Timbaland said the pair had an argument before her death and he learned of her passing when attempting to reconcile. Immediately following her death, Timbaland told MTV he considered her as his "little sister" and that he and Aaliyah had a "chemistry" and that he had lost half of his creativity with her death. He also said that fans needed to know that beyond music "she was a brilliant person." Aaliyah's death had a large impact on both of them as they had been her closest collaborators in the last five years of her career and they owed their initial success to her.
In 2015, Damon Dash claimed that Jay Z tried to date Aaliyah, but she placed him in the "friend zone". In a New York City interview, Jay Z said her passing was a precursor to the September 11 attacks, which took place the month after Aaliyah's passing.
Engagement.
Aaliyah was dating co-founder of Roc-A-Fella Records Damon Dash at the time of her death and, though they were not formally engaged, in interviews given after Aaliyah's death Dash claimed the couple had planned to marry. Aaliyah and Dash met through his accountant and formed a friendship. Dash has said he is unsure of how he and Aaliyah started dating and that the two just understood each other. "I don't know we got involved, just spending time, you know, we just saw things the same and it was new, you know what I mean? Meeting someone that is trying to do the same thing you are doing in the urban market, in the same urban market place but not really being so urban. It was just; her mind was where my mind was. She understood me and she got my jokes. She thought my jokes were funny."
Dash expressed his belief that Aaliyah was the "one" and claimed the pair were not officially engaged, but had spoken about getting married prior to her death.
Aaliyah publicly never addressed the relationship between her and Dash as being anything but platonic. In May 2001, she hosted a party for Dash's 30th birthday at a New York City club, where they were spotted together and Dash was seen escorting her to a bathroom. Addressing this, Aaliyah stated that she and Dash were just "very good friends" and chose to "keep it at that" for the time being. Just two weeks before her death, Aaliyah traveled from New Jersey to East Hampton, New York to visit Dash at the summer house he shared with Jay-Z.
The couple were separated for long periods at a time, as Dash recalled that Aaliyah continuously shot films and would be gone for months often to come back shortly and continue her schedule. Dash was also committed to "his own thing", which did not make matters any better. Despite this, they were understanding that the time they had together was special. Dash remembered they would "be in a room full of people talking to each other and it felt like everyone was listening but it would be just us. It would be like we were the only ones in the room". Dash always felt their time together was essential and Aaliyah was the person he was interested in being with, which is why, as he claimed, they had begun speaking about engagement. The relationship was mentioned in the lyrics of Jay-Z's remix to her song "Miss You", released after her death.
Death.
On August 25, 2001, at 6:50 p.m. (EDT), Aaliyah and the members of the record company boarded a twin-engine Cessna 402B (registration N8097W) at the Marsh Harbour Airport in Abaco Islands, The Bahamas, to travel to the Opa-locka Airport in Florida, after they completed filming the music video for "Rock the Boat". They had a flight scheduled the following day, but with filming finishing early, Aaliyah and her entourage were eager to return to the United States and made the decision to leave immediately. The designated airplane was smaller than the Cessna 404 on which they had originally arrived, but the whole party and all of the equipment were accommodated on board. The plane crashed shortly after takeoff, about from the end of the runway and exploded.
Aaliyah and the eight others on board—pilot Luis Morales III, hair stylist Eric Forman, Anthony Dodd, security guard Scott Gallin, family friend Keith Wallace, make-up stylist Christopher Maldonado, and Blackground Records employees Douglas Kratz and Gina Smith—were all killed. Gallin survived the initial impact and spent his last moments worrying about Aaliyah's condition, according to ambulance drivers. The plane was identified as being owned by Florida-based company Skystream by Kathleen Bergen, spokeswoman for the US Federal Aviation Administration (FAA) in Atlanta. Initial reports of the crash identified Luis Morales as "L Marael".
According to findings from an inquest conducted by the coroner's office in The Bahamas, Aaliyah suffered from "severe burns and a blow to the head", in addition to severe shock and a weak heart. The coroner theorized that she went into such a state of shock that even if she had survived the crash, her recovery would have been nearly impossible given the severity of her injuries. The bodies were taken to the morgue at Princess Margaret Hospital in Nassau, where they were kept for relatives to help identify them. Some of the bodies were badly burned in the crash.
As the subsequent investigation determined, when the aircraft attempted to depart, it was over its maximum take-off weight by and was carrying one excess passenger, according to its certification.
The National Transportation Safety Board report stated that "the airplane was seen lifting off the runway, and then nose down, impacting in a marsh on the south side of the departure end of runway 27 and then exploding in flames." It indicated that the pilot was not approved to fly the plane. Morales falsely obtained his FAA license by showing hundreds of hours never flown, and he may also have falsified how many hours he had flown in order to get a job with his employer, Blackhawk International Airways. Additionally, an autopsy performed on Morales revealed traces of cocaine and alcohol in his system.
Aaliyah's funeral was held on August 31, 2001, at the St. Ignatius Loyola Church in Manhattan. Her body was set in a silver-plated copper-deposit casket, which was carried in a glass horse-drawn hearse. An estimated 800 mourners were in attendance at the procession. Among those in attendance at the private ceremony were Missy Elliott, Timbaland, Gladys Knight, Lil' Kim and Sean Combs. After the service, 22 white doves were released to symbolize each year of Aaliyah's life. Aaliyah was interred in a private room at the left end of a corridor in the Rosewood Mausoleum at the Ferncliff Cemetery in Hartsdale, New York. The inscription at the bottom of Aaliyah's portrait at the funeral read: "We Were Given a Queen, We Were Given an Angel."
After Aaliyah's death, the German newspaper "Die Zeit" published excerpts from an interview done shortly before her death, in which she described a reoccurring dream: "It is dark in my favorite dream. Someone is following me. I don't know why. I'm scared. Then suddenly I lift off. Far away. How do I feel? As if I am swimming in the air. Free. Weightless. Nobody can reach me. Nobody can touch me. It's a wonderful feeling."
Posthumous career.
Immediately after Aaliyah's death, there was uncertainty over whether the music video for "Rock the Boat" would ever air. It made its world premiere on BET's "Access Granted" on October 9, 2001. She won two posthumous awards at the American Music Awards of 2002; Favorite Female R&B Artist and Favorite R&B/Soul Album for "Aaliyah". Her second and final film, "Queen of the Damned", was released in February 2002. Before its release, Aaliyah's brother, Rashad, re-dubbed some of her lines during post-production. It grossed US$15.2 million in its first weekend, ranking number one at the box office. On the first anniversary of Aaliyah's death, a candlelight vigil was held in Times Square; millions of fans observed a moment of silence; and throughout the United States, radio stations played her music in remembrance. In December 2002, a collection of previously unreleased material was released as Aaliyah's first posthumous album, "I Care 4 U". A portion of the proceeds was donated to the Aaliyah Memorial Fund, a program that benefits the Revlon UCLA Women's Cancer Research Program and Harlem's Sloan Kettering Cancer Center. It debuted at number three on the "Billboard" 200, selling 280,000 copies in its first week. The album's lead single, "Miss You", peaked at number three on the "Billboard" Hot 100 and topped the Hot R&B/Hip-Hop Songs chart. In August of the following year, clothing retailer Christian Dior donated profits from sales in honor of Aaliyah.
In 2005, Aaliyah's second compilation album, "Ultimate Aaliyah" was released in the UK by Blackground Records. "Ultimate Aaliyah" is a three disc set, which included a greatest hits audio CD and a DVD. Andy Kellman of AllMusic remarked ""Ultimate Aaliyah" adequately represents the shortened career of a tremendous talent who benefited from some of the best songwriting and production work by Timbaland, Missy Elliott, and R. Kelly." A documentary movie "Aaliyah Live in Amsterdam" was released in 2011, shortly before the tenth anniversary of Aaliyah's death. The documentary, by Pogus Caesar, contained previously unseen footage shot of her career beginnings in 1995 when she was appearing in the Netherlands.
In March 2012, music producer Jeffrey "J-Dub" Walker announced on his Twitter account that a song "Steady Ground", which he produced for Aaliyah's third album, would be included in the forthcoming posthumous Aaliyah album. This second proposed posthumous album would feature this song using demo vocals, as Walker claims the originals were somehow lost by his sound engineer. Aaliyah's brother Rashad later refuted Walker's claim, claiming that "no official album being released and supported by the Haughton family."
On August 5, 2012, a song entitled "Enough Said" was released online. The song was produced by Noah "40" Shebib and features Canadian rapper Drake. Four days later, Jomo Hankerson confirmed a posthumous album is being produced and that it is scheduled to be released by the end of 2012 by Blackground Records. The album was reported to include 16 unreleased songs and have contributions from Aaliyah's longtime collaborators Timbaland and Missy Elliott, among others. On August 13, Timbaland and Missy Elliott dismissed rumors about being contacted or participating for the project. Elliott's manager Mona Scott-Young said in a statement to "XXL", "Although Missy and Timbaland always strive to keep the memory of their close friend alive, we have not been contacted about the project nor are there any plans at this time to participate. We've seen the reports surfacing that they have been confirmed to participate but that is not the case. Both Missy and Timbaland are very sensitive to the loss still being felt by the family so we wanted to clear up any misinformation being circulated." Elliott herself said, "Tim and I carry Aaliyah with us everyday, like so many of the people who love her. She will always live in our hearts. We have nothing but love and respect for her memory and for her loved ones left behind still grieving her loss. They are always in our prayers."
In June 2013, Aaliyah was featured on a new track by Chris Brown, titled "Don't Think They Know"; with Aaliyah singing the song's hook. The video features dancing holographic versions of Aaliyah. The song appears on Brown's sixth studio album, "X". Timbaland voiced his disapproval for "Enough Said" and "Don't Think They Know" in July 2013. He exclaimed, "Aaliyah music only work with its soulmate, which is me". Soon after, Timbaland apologized to Chris Brown over his remarks, which he explained were made due to Aaliyah and her death being a "very sensitive subject".
In January 2014, producer Noah "40" Shebib confirmed that the posthumous album was shelved due to the negative reception surrounding Drake's involvement. Shebib added, "Aaliyah's mother saying, 'I don't want this out' was enough for me ... I walked away very quickly."
Aaliyah's vocals were reported to be featured on the T-Pain mixtape, "The Iron Way", on the track "Girlfriend", but were pulled after being met with criticism by fans and many in attendance at a New York listening session that he hosted for the project. In response to the criticism, T-Pain questioned if Aaliyah's legacy was driven by her death and claimed that were she still alive, she would be seen as trying to emulate Beyoncé. According to T-Pain, he was given her vocals from a session she had done prior to her death after being approached to work on a track for a posthumous Aaliyah album and completing the song, calling the exchange "just like a swap."
She was featured on the Tink track "Million", which was released in May 2015 and contained samples from her song "One in a Million". Collaborator Timbaland was involved in the song's creation, having previously claimed that Aaliyah appeared to him in a dream and stressed that Tink was "the one".
In August 2015, Timbaland confirmed that he had unreleased vocals from Aaliyah and stated a "sneak peek" would be coming soon.
In September 2015, "Aaliyah by Xyrena", an official tribute fragrance was announced.
On December 19, Timbaland uploaded a snippet of a new Aaliyah song title "He Keeps Me Shakin" on his Instagram account and it will be released December 25, 2015 on Timbaland mixtape "King Stays King".
Legacy.
Aaliyah has been credited for helping redefine R&B, pop and hip hop in the 1990s, "leaving an indelible imprint on the music industry as a whole." According to Billboard Aaliyah revolutionized R&B with her sultry mix of pop, soul and hip hop. In a 2001 review of her eponymous album Rolling Stone magazine professed that Aaliyah's impact on R&B and pop has been enormous. Steve Huey of AllMusic wrote Aaliyah ranks among the "elite" artists of the R&B genre, as she "played a major role in popularizing the stuttering, futuristic production style that consumed hip-hop and urban soul in the late 1990s." Bruce Britt of "music world" on Broadcast Music, Inc's. website stated that by combining "schoolgirl charm with urban grit", Aaliyah helped define the teen-oriented sound that has resulted in contemporary pop phenom's like Brandy, Christina Aguilera and Destiny's Child.
Described as one of "R&B's most important artists" during the 1990s, her second studio album, "One in a Million", became one of the most influential R&B albums of the decade. Music critic Simon Reynolds cited "Are You That Somebody?" as "the most radical pop single" of 1998. Kelefah Sanneh of "The New York Times" wrote that rather than being the song's focal point, Aaliyah "knew how to disappear into the music, how to match her voice to the bass line", and consequently "helped change the way popular music sounds; the twitchy, beat-driven songs of Destiny's Child owe a clear debt to 'Are You That Somebody'." Sanneh asserted that by the time of her death in 2001, Aaliyah "had recorded some of the most innovative and influential pop songs of the last five years." Music publication pop dust called Aaliyah an unlikely queen of the underground mainly due to her influence on the underground alternative music scene which consist of heavy sampling of her music and many references that are made to her discography by underground artist, pop dust also mentioned that Aaliyah's forward thinking music that she did with timbaland and the experimental music being made by many underground alternative artist are some what cut from the same cloth. While compiling a list of artist that take cues from Aaliyah MTV Hive mentioned that it’s easy to spot her influence on underground movements like dubstep, strains of indie pop, and in the lo-fi R&B movements. With sales of 8.1 million albums in the United States and an estimated 24 to 32 million albums worldwide, Aaliyah earned the nicknames "Princess of R&B" and "Queen of Urban Pop", as she "proved she was a muse in her own right". Ernest Hardy of "Rolling Stone" dubbed her as the "undisputed queen of the midtempo come-on". Aaliyah has been referred to as a pop icon and a R&B icon for her impact and contributions to those respective genres. Japanese pop singer Hikaru Utada has said several times that "It was when I heard Aaliyah's "Age Ain't Nothing but a Number" that I got hooked on R&B.", after which Utada released her debut album "First Love" with heavy R&B influences. Another Japanese pop singer Crystal Kay has expressed how she admired Aaliyah when she was growing up and how she would practice dancing while watching her music videos.
Aaliyah was honored at the 2001 MTV Video Music Awards by Janet Jackson, Missy Elliott, Timbaland, Ginuwine and her brother, Rashad, who all paid tribute to her. In the same year, the United States Social Security Administration ranked the name Aaliyah one of the 100 most popular names for newborn girls. Aaliyah was ranked as one of "The Top 40 Women of the Video Era" in VH1's 2003 "The Greatest" series. She was also ranked at number 18 on BET's "Top 25 Dancers of All Time". Aaliyah appeared on both 2000 and 2001 list of "Maxim" Hot 100 in position 41 and the latter at 14. In 2002 VH1 created the 100 sexiest artist list and Aaliyah was ranked at number 36. In memory of Aaliyah, the Entertainment Industry Foundation created the Aaliyah Memorial Fund to donate money raised to charities she supported. In December 2009, "Billboard" magazine ranked Aaliyah at number 70 on its Top Artists of the Decade, while her eponymous album was ranked at number 181 on the magazine's Top 200 Albums of the Decade. She is listed by "Billboard" as the tenth most successful female R&B artist of the past 25 years, and 27th most successful R&B artist overall. In 2012, VH1 ranked her number 48 in "VH1's Greatest Women in Music". Also in 2012, Aaliyah was ranked at number 10 on Complex magazine's 100 hottest female singers of all-time list and number 22 on their 90 hottest women of the 90's list.In 2014, "NME" ("New Musical Express") ranked Aaliyah at number 18 on NME's 100 most influential artist list. Aaliyah's dress that she wore at the 2000 MTV Video Music Award's was featured in the most memorable fashion moments at the VMA's list by the fashion publication Harper's Bazaar In October 2015 Aaliyah was featured in the 10 women who became Denim Style icons list created by the fashion publication Vogue.
Aaliyah's work has influenced numerous artists including Adele, Ciara, Beyoncé, Monica, Chris Brown, Rihanna, Azealia Banks, Sevyn Streeter, Keyshia Cole, J. Cole, Kelly Rowland, Zendaya, Rita Ora, The xx, Arctic Monkeys, Speedy Ortiz, Chelsea Wolfe, Haim, Angel Haze, Kiesza, Naya Rivera, Cassie, Hayley Williams, Jessie Ware, Yeasayer and Omarion. Canadian R&B singer Keshia Chanté who was said to play as her in her pending biopic back in 2008, complimented the singer's futuristic style in music and fashion.
Chanté backed out of the biopic after speaking to Diane Haughton, but has expressed a willingness to do the project if "the right production comes along and the family's behind it". Keisha also mentioned that Aaliyah had been part of her life "since I was 6."
R&B singer and friend Brandy said about the late singer "She came out before Monica and I did, she was our inspiration. At the time, record companies did not believe in kid acts and it was just inspiring to see someone that was winning and winning being themselves. When I met her I embraced her, I was so happy to meet her." Rapper Drake said that the singer has had the biggest influence on his career. He also has a tattoo of the singer behind his back. Solange Knowles remarked on the tenth anniversary of her death that she idolized Aaliyah and proclaimed that she would never be forgotten. Adam Levine, the lead vocalist of the pop rock group Maroon 5, remembers that listening to "Are You That Somebody?" convinced him to pursue a more soulful sound than that of his then-band Kara's Flowers. Erika Ramirez, associate editor of Billboard.com, said at the time of Aaliyah's career "there weren't many artists using the kind of soft vocals the ways she was using it, and now you see a lot of artists doing that and finding success," her reasoning for Aaliyah's continued influence on current artists. She argued that Aaliyah's second album "One in a Million" was "very much ahead of its time, with the bass and electro kind of R&B sounds that they produced", referring to collaborators Timbaland and Missy Elliott and that the sound, which "really stood out" at its time, was being replicated.
In 2012, British singer-songwriter Katy B released the song "Aaliyah" as a tribute to Aaliyah's legacy and lasting impression on R&B music. The song first appeared on Katy B's "Danger" EP and featured Jessie Ware on guest vocals. In 2016, Swedish singer-songwriter Erik Hassle released a song titled "If Your Man Only Knew" which serves as a tribute to Aaliyah's 1996 single "If Your Girl Only Knew" 
There has been continuing belief that Aaliyah would have achieved greater career success had it not been for her death. Emil Wilbekin mentioned the deaths of The Notorious B.I.G. and Tupac Shakur in conjunction with hers and added: "Her just-released third album and scheduled role in a sequel to "The Matrix" could have made her another Janet Jackson or Whitney Houston". Director of "Queen of the Damned" Michael Rymer said of Aaliyah, "God, that girl could have gone so far" and spoke of her having "such a clarity about what she wanted. Nothing was gonna step in her way. No ego, no nervousness, no manipulation. There was nothing to stop her."
On July 18, 2014, it was announced that Alexandra Shipp replaced Zendaya for the role of Aaliyah for the Lifetime TV biopic movie "", which premiered on November 15, 2014. Zendaya drew criticism because people felt that she was too light skinned and did not greatly resemble Aaliyah. She voiced her strong respect for Aaliyah before dropping out of the project. She explained her choice to withdraw from the film in videos on Instagram. Aaliyah's family has been vocal in their disapproving of the film. Her cousin Jomo Hankerson stated the family would prefer a "major studio release along the lines" of "What's Love Got to Do with It", the biopic based on the life of Tina Turner. Aaliyah's family has consulted a lawyer to stop Lifetime from using "any of the music, or any of the photographs and videos" they own and Jomo Hankerson claimed the TV network "didn't reach out." On August 9, 2014, it was announced that Chattrisse Dolabaille and Izaak Smith had been cast as Aaliyah's collaborators Missy Elliott and Timbaland. Dolabaille received criticism for her appearance in comparison with that of Missy Elliot. Despite negative reviews, the film's premiere drew 3.2 million viewers, becoming the second highest rated television movie of 2014.
A feature film, scheduled for release in theaters in 2015, was reported in August 2014 to star B. Simone and have involvement from Aaliyah's uncle Barry Hankerson. The film is stated to feature unreleased music by the late singer. The script will be written by author Zane. It was announced in February 2015 that a tribute dinner would be held for Aaliyah by The Sugar Club in Dublin later that month.

</doc>
<doc id="2147" url="https://en.wikipedia.org/wiki?curid=2147" title="Armour">
Armour

Armour (spelt armor in the US) is a protective covering that is used to prevent damage from being inflicted to an object, individual, or vehicle by direct contact weapons or projectiles, usually during combat, or from damage caused by a potentially dangerous environment or action (e.g., cycling, construction sites, etc.). Personal armour is used to protect soldiers and war animals. Vehicle armour is used on warships and armoured fighting vehicles.
A second use of the term armour describes armoured forces, armoured weapons, and their role in combat. After the evolution of armoured warfare, mechanised infantry and their weapons came to be referred to collectively as "armour".
Etymology.
The word "armour" began to appear in the Middle Ages as a derivative of Old French. It is dated from 1297 as a "mail, defensive covering worn in combat". The word originates from the Old French "armure", itself derived from the Latin "armatura" meaning "arms and/or equipment", with the root "armare" meaning "arms or gear".
Personal.
Armour has been used throughout recorded history. It has been made from a variety of materials, beginning with rudimentary leather protection and evolving through mail and metal plate into today's modern composites. For much of military history the manufacture of metal personal armour has dominated the technology and employment of armour. Armour drove the development of many important technologies of the Ancient World, including wood lamination, mining, metal refining, vehicle manufacture, leather processing, and later decorative metal working. Its production was influential in the industrial revolution, and furthered commercial development of metallurgy and engineering. Armour was the single most influential factor in the development of firearms, which in turn revolutionised warfare.
History.
Significant factors in the development of armour include the economic and technological necessities of its production. For instance, plate armour first appeared in Medieval Europe when water-powered trip hammers made the formation of plates faster and cheaper. Also, modern militaries usually do not equip their forces with the best armour available because it would be prohibitively expensive. At times the development of armour has paralleled the development of increasingly effective weaponry on the battlefield, with armourers seeking to create better protection without sacrificing mobility.
Well-known armour types in European history include the lorica hamata, lorica squamata, and the lorica segmentata of the Roman legions, the mail hauberk of the early medieval age, and the full steel plate harness worn by later medieval and renaissance knights, and breast and back plates worn by heavy cavalry in several European countries until the first year of World War I (1914–15). The samurai warriors of feudal Japan utilised many types of armour for hundreds of years up to the 19th century.
Early.
Cuirasses and helmets were manufactured in Japan as early as the 4th century."Tankō", worn by foot soldiers and "keikō", worn by horsemen were both pre-samurai types of early Japanese armour constructed from iron plates connected together by leather thongs. Japanese lamellar armour ("keiko") passed through Korea and reached Japan around the 5th century. These early Japanese lamellar armours took the form of a sleeveless jacket and a helmet.
Armour did not always cover all of the body; sometimes no more than a helmet and leg plates were worn. The rest of the body was generally protected by means of a large shield. Examples of armies equipping their troops in this fashion were the Aztecs (13th to 15th century CE).
In East Asia many types of armour were commonly used at different times by various cultures including, scale armour, lamellar armour, laminar armour, plated mail, mail, plate armour and brigandine. Around the dynastic Tang, Song, and early Ming Period, cuirasses and plates (mingguangjia) were also used, with more elaborate versions for officers in war. The Chinese, during that time used partial plates for "important" body parts instead of covering their whole body since too much plate armour hinders their martial arts movement. The other body parts were covered in cloth, leather, lamellar, and/or Mountain pattern. In pre-Qin dynasty times, leather armour was made out of various animals, with more exotic ones such as the rhinoceros.
Mail, sometimes called "chainmail", made of interlocking iron rings is believed to have first appeared some time after 300 BC. Its invention is credited to the Celts, the Romans were thought to have adopted their design.
Gradually, small additional plates or discs of iron were added to the mail to protect vulnerable areas. Hardened leather and splinted construction were used for arm and leg pieces. The coat of plates was developed, an armour made of large plates sewn inside a textile or leather coat.
Early plate in Italy, and elsewhere in the 13th–15th century, were made of iron. Iron armour could be carburised or case hardened to give a surface of harder steel. Plate armour became cheaper than mail by the 15th century as it required much less labour and labour had become much more expensive after the Black Death, though it did require larger furnaces to produce larger blooms. Mail continued to be used to protect those joints which could not be adequately protected by plate, such as the armpit, crook of the elbow and groin. Another advantage of plate was that a lance rest could be fitted to the breast plate.
The small skull cap evolved into a bigger true helmet, the bascinet, as it was lengthened downward to protect the back of the neck and the sides of the head. Additionally, several new forms of fully enclosed helmets were introduced in the late 14th century.
Probably the most recognised style of armour in the World became the plate armour associated with the knights of the European Late Middle Ages, but continuing to the early 17th century Age of Enlightenment in all European countries.
By about 1400 the full harness of plate armour had been developed in armouries of Lombardy. Heavy cavalry dominated the battlefield for centuries in part because of their armour.
In the early 15th century, advances in weaponry allowed infantry to defeat armoured knights on the battlefield. The quality of the metal used in armour deteriorated as armies became bigger and armour was made thicker, necessitating breeding of larger cavalry horses. If during the 14–15th centuries armour seldom weighed more than 15 kg, then by the late 16th century it weighed 25 kg. The increasing weight and thickness of late 16th century armour therefore gave substantial resistance.
In the early years of low velocity firearms, full suits of armour, or breast plates actually stopped bullets fired from a modest distance. Crossbow bolts, if still used, would seldom penetrate good plate, nor would any bullet unless fired from close range. In effect, rather than making plate armour obsolete, the use of firearms stimulated the development of plate armour into its later stages. For most of that period, it allowed horsemen to fight while being the targets of defending arquebuseers without being easily killed. Full suits of armour were actually worn by generals and princely commanders right up to the second decade of the 18th century. It was the only way they could be mounted and survey the overall battlefield with safety from distant musket fire.
The horse was afforded protection from lances and infantry weapons by steel plate barding. This gave the horse protection and enhanced the visual impression of a mounted knight. Late in the era, elaborate barding was used in parade armour.
Later.
Gradually, starting in the mid-16th century, one plate element after another was discarded to save weight for foot soldiers.
Back and breast plates continued to be used throughout the entire period of the 18th century and through Napoleonic times, in many European (heavy) cavalry units, until the early 20th century. From their introduction, muskets could pierce plate armour, so cavalry had to be far more mindful of the fire. In Japan armour continued to be used until the end of the samurai era, with the last major fighting in which armour was used happening in 1868. Samurai armour had one last short lived use in 1877 during the Satsuma Rebellion.
Though the age of the knight was over, armour continued to be used in many capacities. Soldiers in the American Civil War bought iron and steel vests from peddlers (both sides had considered but rejected body armour for standard issue). The effectiveness of the vests varied widely—some successfully deflected bullets and saved lives, but others were poorly made and resulted in tragedy for the soldiers. In any case the vests were abandoned by many soldiers due to their weight on long marches as well as the stigma they got for being cowards from their fellow troops.
At the start of World War I, thousands of the French Cuirassiers rode out to engage the German Cavalry. By that period, the shiny armour plate was covered in dark paint and a canvas wrap covered their elaborate Napoleonic style helmets. Their armour was meant to protect only against sabres and light lances. The cavalry had to beware of high velocity rifles and machine guns, unlike the foot soldiers, who at least had a trench to protect them.
Present.
Today, ballistic vests, also known as flak jackets, made of ballistic cloth (e.g. kevlar, dyneema, twaron, spectra etc.) and ceramic or metal plates are common among police forces, security staff, corrections officers and some branches of the military.
The US Army has adopted Interceptor body armour, which uses Enhanced Small Arms Protective Inserts (E-S.A.P.I) in the chest, sides and back of the armour. Each plate is rated to stop a range of ammunition including 3 hits from a 7.62×51 NATO AP round at a range of . Dragon Skin body armour is another ballistic vest which is currently in testing with mixed results.
Other types.
The first modern production technology for armour plating was used by navies in the construction of the Ironclad warship, reaching its pinnacle of development with the battleship. The first tanks were produced during World War I. Aerial armour has been used to protect pilots and aircraft systems since the First World War.
In modern ground forces' usage, the meaning of armour has expanded to include the role of troops in combat. After the evolution of armoured warfare, mechanised infantry were mounted in armoured fighting vehicles and replaced light infantry in many situations. In modern armoured warfare, armoured units equipped with tanks and infantry fighting vehicles serve the historic role of both the battle cavalry, light cavalry and dragoons, and belong to the armoured branch.
History.
Ships.
The first ironclad battleship, with iron armour over a wooden hull, "La Gloire", was launched by the French Navy in 1859 prompting the British Royal Navy to build a counter. The following year they launched HMS "Warrior", which was twice the size and had iron armour over an iron hull. After the first battle between two ironclads took place in 1862 during the American Civil War, it became clear that the ironclad had replaced the unarmoured line-of-battle ship as the most powerful warship afloat.
Ironclads were designed for several roles, including as high seas battleships, coastal defence ships, and long-range cruisers. The rapid evolution of warship design in the late 19th century transformed the ironclad from a wooden-hulled vessel which carried sails to supplement its steam engines into the steel-built, turreted battleships and cruisers familiar in the 20th century. This change was pushed forward by the development of heavier naval guns (the ironclads of the 1880s carried some of the heaviest guns ever mounted at sea), more sophisticated steam engines, and advances in metallurgy which made steel shipbuilding possible.
The rapid pace of change in the ironclad period meant that many ships were obsolete as soon as they were complete, and that naval tactics were in a state of flux. Many ironclads were built to make use of the ram or the torpedo, which a number of naval designers considered the crucial weapons of naval combat. There is no clear end to the ironclad period, but towards the end of the 1890s the term "ironclad" dropped out of use. New ships were increasingly constructed to a standard pattern and designated battleships or armoured cruisers.
Trains.
Armoured trains saw use during the 19th century in the American Civil War (1861–1865), the Franco-Prussian War (1870–1871), the First and Second Boer Wars (1880–81 and 1899–1902),the Polish–Soviet War (1919–1921); the First (1914–1918) and Second World Wars (1939–1945) and the First Indochina War (1946–1954). The most intensive use of armoured trains was during the Russian Civil War (1918–1920).
Armoured cars saw use during World wars 1 and 2.
During the Second Boer War on 15 November 1899, Winston Churchill, then a war-correspondent, was travelling on board an armoured train when it was ambushed by "Boer commandos". Churchill and many of the train's garrison were captured, though many others escaped, including wounded placed on the train's engine.
Aircraft.
With the development of effective anti-aircraft artillery in the period before the Second World War, military pilots, once the "knights of the air" during the First World War, became far more vulnerable to ground fire. As a response armour plating was added to aircraft to protect aircrew and vulnerable areas such as fuel tanks and engine.
Present.
Tank armour has progressed from the Second World War armour forms, now incorporating not only harder composites, but also reactive armour designed to defeat shaped charges. As a result of this, the main battle tank (MBT) conceived in the Cold War era can survive multiple RPG strikes with minimal effect on the crew or the operation of the vehicle. The light tanks that were the last descendants of the light cavalry during the Second World War have almost completely disappeared from the world's militaries due to increased lethality of the weapons available to the vehicle-mounted infantry.
The armoured personnel carrier (APC) was devised during World War I. It allows the safe and rapid movement of infantry in a combat zone, minimising casualties and maximising mobility. APCs are fundamentally different from the previously used armoured half-tracks in that they offer a higher level of protection from artillery burst fragments, and greater mobility in more terrain types. The basic APC design was substantially expanded to an Infantry fighting vehicle (IFV) when properties of an armoured personnel carrier and a light tank were combined in one vehicle.
Naval armour has fundamentally changed from the Second World War doctrine of thicker plating to defend against shells, bombs and torpedos. Passive defence naval armour is limited to kevlar or steel (either single layer or as spaced armour) protecting particularly vital areas from the effects of nearby impacts. Since ships cannot carry enough armour to completely prevent penetration by anti-ship missiles, they depend more on destroying an incoming missile before it hits, or causing it to miss its target.
Although the role of the ground attack aircraft significantly diminished after the Korean War, it re-emerged during the Vietnam War, and in the recognition of this, the US Air Force authorised the design and production of what became the A-10 dedicated anti-armour and ground-attack aircraft that first saw action in the Gulf War.
References.
"Ballistic Protection Levels." BulletproofME.com Body Armor. ArmorUP L.P., n.d. 19 Oct. 2014

</doc>
<doc id="2148" url="https://en.wikipedia.org/wiki?curid=2148" title="Armoured fighting vehicle">
Armoured fighting vehicle

An armoured (or armored) fighting vehicle (AFV) is a combat vehicle, protected by strong armour and generally armed with weapons, which combines operational mobility, tactical offensive, and defensive capabilities. AFVs can be wheeled or tracked. It is not uncommon for AFVs to be simply referred to as "armour".
Armoured fighting vehicles are classified according to their intended role on the battlefield and characteristics. This classification is not absolute; at different times different countries will classify the same vehicle in different roles. For example, armoured personnel carriers were generally replaced by infantry fighting vehicles in a similar role, but the latter has some capabilities lacking in the former. There may also be hybrid vehicles, such as the Stryker family of AFVs; the M1128 Mobile Gun System, an armoured car which mounts a large 105 mm gun normally used in tank destroyers, but can theoretically be reconfigured to the M1126 Infantry Carrier Vehicle.
Successful general-purpose armoured fighting vehicles often also serve as the base of a whole family of specialized vehicles, for example, the M113 and MT-LB tracked carriers, and the MOWAG Piranha wheeled AFV.
Evolution of AFVs.
The AFV classification did not exist prior to the key innovations leading to the invention of the internal combustion engine, armour plating, the continuous track and the advent of armoured warfare in the 20th century.
History.
Modern armoured fighting vehicles are the realization of an ancient concept: that of providing troops with mobile protection and suppressive firepower. War machines with rudimentary armour have been used in battle for millennia. These designs historically struggled between the paradox of exposed-mobility, effective-firepower and cumbersome-protection.
Siege machines.
Siege engines, such as battering rams and trebuchets, would often be armoured in order to protect the crews from the defenders. Helepolis was the largest and most powerful siege tower ever erected; during the Siege of Rhodes, the city defenders' superior defenses and firepower stalemated the attacking army, which were being slaughtered, denied mobility and unable to mount an effective counter-measures. Demetrius ordered the construction of a "city-capturing" fortress, with multiple catapult positions and protective encasements, that could carry troops while moving toward the enemy fortifications.
The idea of a vehicle with a tortoise like cover has been known since antiquity. Frequently cited is Leonardo da Vinci's 15th century sketch of a mobile, protected gun platform; the drawings show a conical, wooden shelter with apertures for cannons around the circumference. The machine was to be mounted on four wheels which would be turned by the crew through a system of hand cranks and cage (or "lantern") gears. Leonardo quoted "I will build armored wagons which will be safe and invulnerable to enemy attacks. There will be no obstacle which it cannot overcome." However, modern replicas have demonstrated that the human crew would have been able to move it over only short distances.
War wagons.
The chariot was used as a mobile archery platform and as a "battle taxi". The original chariot was a fast, light, open, two-wheeled conveyance drawn by two or more horses hitched side by side. It was used for ancient warfare during the bronze and the iron ages. The war wagon were medieval weapon-platforms development during the Hussite Wars around 1420 by Hussite forces rebelling in Bohemia. These heavy wagon were given protective sides with firing slits and heavy firepower from either a cannon or a force of hand-gunners and crossbowmen, supported by infantry using pikes and flails. Heavy arquebuses mounted on wagons were called "arquebus à croc". These carried a ball of about .
Pre-World wars AFVs.
The first modern AFVs were armoured cars, dating back virtually to the invention of the motor car. The Motor Scout was designed and built by British inventor F.R. Simms in 1898. It was the first armed petrol engine powered vehicle ever built. The vehicle was a De Dion-Bouton quadricycle with a mounted Maxim machine gun on the front bar. An iron shield in front of the car protected the driver.
Armoured car development.
The first armoured car was the Simms' Motor War Car, designed by Simms and built by Vickers, Sons & Maxim in 1899. The vehicle had Vickers armour 6 mm thick and was powered by a four-cylinder 3.3-litre 16 hp Cannstatt Daimler engine giving it a maximum speed of around . The armament, consisting of two Maxim guns, was carried in two turrets with 360° traverse.
Another early armoured car of the period was the French Charron, Girardot et Voigt 1902, presented at the "Salon de l'Automobile et du cycle" in Brussels, on 8 March 1902. The vehicle was equipped with a Hotchkiss machine gun, and with 7 mm armour for the gunner. Armoured cars were first used in large numbers on both sides during World War I as scouting vehicles which offered armoured protection to the crew.
World War I AFVs.
Tank development.
The development of the AFV took a great leap forward during World War I, when the tracked tank was developed by Britain and France to break the stalemate on the Western Front. The tank was envisioned as an armoured machine that could cross ground under fire from machine guns and respond with fire from mounted guns. It was to move on caterpillar tracks to enable it to cross ground broken up by shellfire and trenches.
In Great Britain, the Landships Committee was formed by the First Lord of the Admiralty, Winston Churchill on 20 February 1915. The Director of Naval Construction for the Royal Navy, Eustace Tennyson d'Eyncourt, was appointed to head the Committee in view of his experience with the engineering methods it was felt might be required. The first design, Little Willie, ran for the first time in September 1915 and served to develop the form of the track but an improved design, better able to cross trenches, swiftly followed and in January 1916 the prototype, nicknamed "Mother", was adopted as the design for future tanks. Production models of "Male" tanks (armed with naval cannon and machine guns) and "Females" (carrying only machine-guns) would go on to fight in history's first tank action at the Somme in September 1916. Great Britain produced about 2,600 tanks of various types during the war.
In 1916, the French pioneered the use of a full 360° rotation turret in a tank for the first time, with the creation of the Renault FT light tank, with the turret containing the tank's main armament. In addition to the traversible turret, another innovative feature of the FT was its engine located at the rear. This pattern, with the gun located in a mounted turret and the engine at the back, became the standard for most succeeding tanks across the world. The FT was the most numerous tank of the War; over 3,000 were made by late 1918.
Troop Transports development.
The tank proved highly successful, and as technology improved it became a weapon that could cross large distances at much higher speeds than supporting infantry and artillery. The need to provide the units that would fight alongside the tank led to the development of a wide range of specialised AFVs, especially during the Second World War.
The Armoured personnel carrier, designed to transport infantry troops to the frontline, emerged towards the end of WWI. During the first actions with tanks it had become clear that close contact with infantry was essential, so that ground won by the tanks could be secured. Troops on foot were vulnerable to enemy fire, but they could not be transported in the tank because of the intense heat and noxious atmosphere. In 1917, Lieutenant G.R. Rackham was ordered to design an armoured vehicle that could fight and carry troops or supplies. The Mark IX tank was built by Armstrong, Whitworth & Co., although just three vehicles were finished at the time of the Armistice and only 34 were built in total.
Interwar AFVs.
Different tank classifications emerged in the interwar period. The tankette was conceived as a mobile, two-man model, mainly intended for reconnaissance. In 1925 Sir John Carden and Vivian Loyd produced the first such design – the Carden Loyd tankette. Tankettes saw use in the Italian Royal Army during the Italian invasion of Ethiopia, the Spanish Civil War, and almost every place Italian soldiers fought during World War II. The Imperial Japanese Army also used them for jungle warfare.
World War II AFVs.
Self-propelled artillery development.
The British Gun Carrier Mark I was the first Self-propelled artillery and was fielded in 1917. It was based on the first tank, the British Mark I and carried a heavy field gun. The next major advance was the Birch gun developed for the motorised warfare experimental brigade (the Experimental Mechanized Force). This mounted a field gun, capable of the usual artillery trajectories, on a tank style chassis.
During WWII, most nations developed self-propelled artillery vehicles. These had mounted guns on a tracked chassis (often that of an obsolete or superseded tank) and provide an armoured superstructure to protect the gun and its crew. The first British design, "Bishop", carried the 25 pdr gun-howitzer, but in a mounting that severely limited the gun's performance. It was replaced by the more effective Sexton. The Germans created many examples of lightly armored self-propelled anti-tank guns using captured French equipment (example Marder I), their own obsolete light tank chassis (Marder II), or ex-Czech chassis (Marder III). These led to better protected tank destroyers, built on medium tank chassis such as the Jagdpanzer IV and Jagdpanther.
Anti-aircraft development.
The Self-propelled anti-aircraft weapon debuted in WWI. The German 77 mm anti-aircraft gun, was truck-mounted and used to great effect against British tanks, and the British QF 3 inch 20 cwt was mounted on trucks for use on the Western Front. Although the Birch gun was a general purpose artillery piece on an armoured tracked chassis, it was capable of being elevated for anti-aircraft use. Vickers Armstrong developed one of the first SPAAGs based on the chassis of the Mk.E 6-ton light tank/Dragon Medium Mark IV tractor, mounting a Vickers QF-1 "Pom-Pom" gun of 40 mm. The Germans fielded the SdKfz 10/4 and 6/2, cargo halftracks mounting single 20 mm or 37 mm AA guns (respectively) by the start of the War.
Post World Wars AFVs.
By the end of World War II, most modern armies had vehicles to carry infantry, artillery and anti-aircraft weaponry. Most modern AFVs are superficially similar in design to their World War II counterparts, but with significantly better armour, weapons, engines and suspension. The increase in the capacity of transport aircraft has allowed AFVs to be practically transported by air. Many armies are replacing some or all of their traditional heavy vehicles with lighter airmobile versions, often with wheels instead of tracks.
Design.
Armour.
The level of armour protection between AFVs varies greatly – a main battle tank will normally be designed to take hits from other tank guns and anti-tank missiles, whilst light reconnaissance vehicles are often only armoured "just in case". Whilst heavier armour provides better protection, it makes vehicles less mobile (for a given engine power), limits its air-transportability, increases cost, uses more fuel and may limit the places it can go – for example, many bridges may be unable to support the weight of a main battle tank. A trend toward composite armour is taking place in place of steel – composites are stronger for a given weight, allowing the tank to be lighter for the same protection as steel armour, or better protected for the same weight. Armour is being supplemented with active protection systems on a number of vehicles, allowing the AFV to protect itself from incoming projectiles.
The level of protection also usually varies considerably throughout the individual vehicle too, depending on the role of the vehicle and the likely direction of attack. For example, a main battle tank will usually have the heaviest armour on the hull front and the turret, lighter armour on the sides of the hull and the thinnest armour on the top and bottom of the tank. Other vehicles – such as the MRAP family – may be primarily armoured against the threat from IEDs and so will have heavy, sloped armour on the bottom of the hull.
Weaponry.
Weaponry varies by a very wide degree between AFVs – lighter vehicles for infantry carrying, reconnaissance or specialist roles may have only a Cannon or Machine gun (or no armament at all), whereas heavy self propelled artillery will carry large guns, mortars or rocket launchers. These weapons may be mounted on a pintle, affixed directly to the vehicle or placed in a turret or cupola.
The greater the recoil a weapon on an AFV is, the larger the turret ring needs to be. A larger turret ring necessitates a larger vehicle. To avoid listing to the side, turrets are usually located at the centre of the vehicle on vehicles that are capable of amphibious operations.
Grenade launchers provide a versatile launch platform for a plethora of munitions including, smoke, phosphorus, tear gas, illumination, anti-personnel, infrared and radar-jamming rounds.
Turret stabilization is an important capability because it enables firing on the move and prevents crew fatigue.
Engine.
Modern AFVs have primarily used either petrol (gasoline) or diesel piston engines. More recently gas turbines have been used. Most early AFVs used petrol engines, as they offer a good power-to-weight ratio. However, they fell out of favour during World War Two due to the flammability of the fuel.
Most current AFVs are powered by a diesel engine; modern technology including the use of turbo-charging help to overcome the lower power-to-weight ratio of diesel engines compared to petrol.
Gas turbine (turboshaft) engines offer a very high power-to-weight ratio and were starting to find favour in the late 20th century – however they offer very poor fuel consumption and as such some armies are switching from gas turbines back to diesel engines (i.e. the Russian T-80 used a gas turbine engine, whereas the later T-90 does not). The US M1 Abrams is a notable example of a gas turbine powered tank.
Modern classification by type and role.
Notable armoured fighting vehicles extending from post-World War I to today.
Tank.
The tank is an all terrain AFV designed to fill almost all battlefield roles and to engage enemy forces by the use of direct fire in the frontal assault role. Though several configurations have been tried, particularly in the early experimental "golden days" of tank development, a standard, mature design configuration has since emerged to a generally accepted pattern. This features a main tank gun or artillery gun, mounted in a fully rotating turret atop a tracked automotive hull, with various additional secondary weapon systems throughout.
Philosophically, the tank is, by its very nature, an offensive weapon. Being a protective encasement with at least one gun position, it is essentially a pillbox or small fortress (though these are static fortifications of a purely defensive nature) that can move toward the enemy – hence its offensive utility. Psychologically, the tank is a force multiplier that has a positive morale effect on the infantry it accompanies. It also instills fear in the opposing force who can often hear and even feel their arrival.
Historical classification.
Historically classified by tactical role, tanks have been sorted into three categories:
As an alternative to classification by tonnes: Cavalry tank, cruiser tank, fast tank, infantry tank, expeditionary tank, and assault-breakthrough tank have been used by various countries to classify tanks by operational role. Tankette is used to describe particularly small one or two-man vehicles, typically armed with one or more machine guns.
In modern use, the heavy tank has fallen out of favour, being supplanted by more heavily-armed and armoured descendant of the medium tanks – the universal main battle tank. The light tank has, in many armies, lost favour to cheaper, faster, lighter armoured cars and tank destroyers; however, light tanks (or similar vehicles with other names) are still in service with a number of forces as reconnaissance vehicles, most notably the Russian Marines with the PT-76, the British Army with the Scimitar, and the Chinese Army with the Type 63.
Main battle tank.
Modern main battle tanks or "universal tanks" incorporate recent advances in automotive, artillery, and armour technology to combine the best characteristics of the historic medium and heavy tanks into a single, all around type. They are also the most expensive to mass-produce. It is distinguished by its high level of firepower, mobility and armour protection relative to other vehicles of its era. It can cross comparatively rough terrain at high speeds, but its heavy-dependency on fuel, maintenance, and ammunition makes it logistically demanding. It has the heaviest armour of any AFV on the battlefield, and carries a powerful precision-guided munition weapon systems that may be able to engage a wide variety of both ground targets and air targets. Despite significant advances in anti-tank warfare, it still remains the most versatile and fearsome land-based weapon-systems of the 21st-century, valued for its shock action and high survivability.
Tankette.
A tankette is a tracked armed and armoured vehicle resembling a small "ultra-light tank" roughly the size of a car, mainly intended for light infantry support or scouting. They were one or two-man vehicles armed with a machine gun. Colloquially it may also simply mean a "small tank".
Tankettes were designed and built by several nations between the 1920s and 1940s. They were very popular with smaller countries. Some saw some combat (with limited success) in World War II. However, the vulnerability of their light armour eventually caused the concept to be abandoned.
"Super"-heavy tank.
The term "super-heavy tank" has been used to describe armoured fighting vehicles of extreme size, generally over 75 tonnes. Programs have been initiated on several occasions with the aim of creating an invincible siegeworks/breakthrough vehicle for penetrating enemy formations and fortifications without fear of being destroyed in combat. Examples were designed in World War I and World War II (such as the Panzer VIII Maus), along with a few in the Cold War. However, few working prototypes were built and there are no clear evidence any of these vehicles saw combat, as their immense size would have made most designs impractical.
Flame tank.
A flame tank is an otherwise-standard tank equipped with a flamethrower, most commonly used to supplement combined arms attacks against fortifications, confined spaces, or other obstacles. The type only reached significant use in the Second World War, during which the United States, Soviet Union, Germany, Italy, Japan and the United Kingdom (including members of the British Commonwealth) all produced flamethrower-equipped tanks. Usually, the flame projector replaced one of the tank's machineguns, however, some flame projectors replaced the tank's main gun. Fuel for the flame weapon was generally carried inside the tank, although a few designs mounted the fuel externally, such as the armoured trailer used on the Churchill Crocodile.
Flame tanks have been superseded by thermobaric weapons such as the Russian TOS-1.
Infantry tank.
The idea for this tank was developed during World War I by the British and French. The infantry tank was designed to work in concert with infantry in the assault, moving mostly at a walking pace, and carrying heavy armour to survive defensive fire. Its main purpose was to suppress enemy fire, crush obstacles such as barbed-wire entanglements, and protect the infantry on their advance into and through enemy lines by giving mobile overwatch and cover. The French Renault FT was the first iteration of this concept.
The British and French retained the concept between the wars and into the Second World War era. Because infantry tanks did not need to be fast, they could carry heavy armour. One of the best-known infantry tanks was the Matilda II of World War II. Other examples include the French R-35, the British Valentine, and the British Churchill.
Cruiser tank.
A cruiser tank, or cavalry tank, was designed to move fast and exploit penetrations of the enemy front. The idea originated in "Plan 1919", a British plan to break the trench deadlock of World War I in part via the use of high-speed tanks. The first cruiser tank was the British Whippet.
Between the wars, this concept was implemented in the "fast tanks" pioneered by J. Walter Christie. These led to the Soviet BT tank series and the British cruiser tank series.
During World War II, British cruiser tanks were designed to complement infantry tanks, exploiting gains made by the latter to attack and disrupt the enemy rear areas. In order to give them the required speed, cruiser designs sacrificed armour and armament compared to the infantry tanks. Pure British cruisers were generally replaced by more capable medium tanks such as the US Sherman and, to a lesser extent, the Cromwell by 1943.
The Soviet fast tank ("bistrokhodniy tank", or BT tank) classification also came out of the infantry/cavalry concept of armoured warfare and formed the basis for the British cruisers after 1936. The T-34 was a development of this line of tanks as well, though their armament, armour, and all-round capability places them firmly in the medium tank category.
Armoured car.
The armoured car is a wheeled, often lightly armoured, vehicle adapted as a fighting machine. Its earliest form consisted of a motorised ironside chassis fitted with firing ports. By World War I, this had evolved into a mobile fortress equipped with command equipment, searchlights, and machine guns for self-defence. It was soon proposed that the requirements for the armament and layout of armoured cars be somewhat similar to those on naval craft, resulting in turreted vehicles. The first example carried a single revolving cupola with a Vickers gun; modern armoured cars may boast heavier armament – ranging from twin machine guns to large calibre cannon.
Some multi-axled wheeled fighting vehicles can be quite heavy, and superior to older or smaller tanks in terms of armour and armament. Others are often used in military marches and processions, or for the escorting of important figures. Under peacetime conditions, they form an essential part of most standing armies. Armoured car units can move without the assistance of transporters and cover great distances with fewer logistical problems than tracked vehicles.
During World War II, armoured cars were used for reconnaissance alongside scout cars. Their guns were suitable for some defence if they encountered enemy armoured vehicles, but they were not intended to engage enemy tanks. Armoured cars have since been used in the offensive role against tanks with varying degrees of success, most notably during the South African Border War, Toyota War, the Invasion of Kuwait, and other lower-intensity conflicts.
Aerosani.
An "aerosani" (, literally "aerosled") is a type of propeller-driven snowmobile, running on skis, used for communications, mail deliveries, medical aid, emergency recovery and border patrolling in northern Russia, as well as for recreation. Aerosanis were used by the Soviet Red Army during the Winter War and World War II.
The first aerosanis may have been built by young Igor Sikorsky in 1909–10, before he built multi-engine airplanes and helicopters. They were very light plywood vehicles on skis, propelled by old airplane engines and propellers.
Scout car.
A scout car is military armored reconnaissance vehicle, capable of off-road mobility and often carrying mounted weapons such as machine guns for offensive capabilities and crew protection. They often only carry an operational crew aboard, which differentiates them from wheeled armored personnel carriers (APCs) and Infantry Mobility Vehicles (IMVs), but early scout cars, such as the open-topped US M3 Scout Car could carry a crew of seven. The term is often used synonymously with the more general term armored car, which also includes armored civilian vehicles. They are also differentiated by being designed and built for purpose, as opposed to improved technicals which might serve in the same role.
Reconnaissance vehicle.
A reconnaissance vehicle, also known as a scout vehicle, is a military vehicle used for forward reconnaissance. Both tracked and wheeled reconnaissance vehicles are in service. In some nations, light tanks such as the M551 Sheridan and AMX-13 are also used by scout platoons. Reconnaissance vehicles are usually designed with a low profile or small size and are lightly armoured, relying on speed and cover to escape detection. Their armament ranges from a medium machine gun to a large cannon. Modern examples are often fitted with ATGMs and a wide range of sensors.
Some armoured personnel carriers and infantry mobility vehicle, such as the M113, TPz Fuchs, and Cadillac Gage Commando double in the reconnaissance role.
Internal security vehicle.
An internal security vehicle (ISV), also known as an armoured security vehicle (ASV), is a combat vehicle used for supporting contingency operations. Security vehicles are typically armed with a turreted heavy machine gun and auxiliary medium machine gun. The vehicle is designed to minimize firepower dead space and the vehicles weapons can be depressed to a maximum of 12°. Non-lethal water cannons and tear gas cannons can provide suppressive fire in lieu of unnecessary deadly fire.
The vehicle must be protected against weapons typical of riots. Protection from incendiary devices is achieved though coverage of the air intake and exhaust ports as well as a strong locking mechanism on the fuel opening. Turret and door locks prevent access to the interior of the vehicle by rioters. Vision blocks, ballistic glass and window shutters and outside surveillance cameras allow protected observation from within the vehicle. Wheeled 4x4 and 6x6 configurations are typical of security vehicles. Tracked security vehicles are often cumbersome and leave negative political connotations for being perceived as an imperial invading force.
Improvised fighting vehicle.
An improvised fighting vehicle is a combat vehicle resulting from modifications to a civilian or military non-combat vehicle in order to give it a fighting capability. Such modifications usually consist of the grafting of armour plating and weapon systems. Various militaries have procured such vehicles, ever since the introduction of the first automobiles into military service.
During the early days, the absence of a doctrine for the military use of automobiles or of an industry dedicated to producing them, lead to much improvisation in the creation of early armoured cars, and other such vehicles. Later, despite the advent of arms industries in many countries, several armies still resorted to using ad hoc contraptions, often in response to unexpected military situations, or as a result of the development of new tactics for which no available vehicle was suitable. The construction of improvised fighting vehicles may also reflect a lack of means for the force that uses them. This is especially true in developing countries, where various armies and guerrilla forces have used them, as they are more affordable than military-grade combat vehicles.
Modern examples include military gun truck used by units of regular armies or other official government armed forces, based on a conventional cargo truck, that is able to carry a large weight of weapons and armour. They have mainly been used by regular armies to escort military convoys in regions subject to ambush by guerrilla forces. "Narco tanks", used by Mexican drug cartels in the Mexican Drug War, are built from such trucks, which combines operational mobility, tactical offensive, and defensive capabilities.
Troop carriers.
Troop-carrying AFVs are divided into three main types – armoured personnel carriers (APCs), infantry fighting vehicles (IFVs) and infantry mobility vehicles (IMV). The main difference between the three is their intended role – the APC is designed purely to transport troops and is armed for self-defense only – whereas the IFV is designed to provide close-quarters and anti-armour fire support to the infantry it carries. IMV is a wheeled armored personnel carrier serving as a military patrol, reconnaissance or security vehicle.
Armoured personnel carrier.
Armoured personnel carriers (APCs) are intended to carry infantry quickly and relatively safely to the point where they are deployed. In the Battle of Amiens, 8 August 1918, the British Mk V* (Mark Five Star) tank carried a small number of machine gunners as an experiment, but the men were debilitated by the conditions inside the vehicle. Later that year the first purpose-built APC, the British Mk IX (Mark Nine), appeared. In 1944, the Canadian general Guy Simonds ordered the conversion of redundant armoured vehicles to carry troops (generically named "Kangaroos"). This proved highly successful, even without training, and the concept was widely used in the 21st Army Group. Post-war, specialised designs were built, such as the Soviet BTR-60 and US M113.
Infantry fighting vehicle.
An infantry fighting vehicle (IFV), also known as a mechanized infantry combat vehicle (MICV), is a type of armoured fighting vehicle used to carry infantry into battle and provide direct fire support. The first example of an IFV was the West German Schützenpanzer Lang HS.30 which served in the Bundeswehr from 1958 until the early 1980s.
IFVs are similar to armoured personnel carriers (APCs) and infantry carrier vehicles (ICVs), designed to transport a section or squad of infantry (generally between five and ten men) and their equipment. They are differentiated from APCs— which are purely "troop-transport" vehicles armed only for self-defense— because they are designed to give direct fire support to the dismounted infantry and so usually have significantly enhanced armament. IFVs also often have improved armour and some have firing ports (allowing the infantry to fire personal weapons while mounted).
They are typically armed with an autocannon of 20 to 40 mm calibre, 7.62mm machine guns, anti-tank missiles (ATGMs) and/or surface-to-air missiles (SAMs). IFVs are usually tracked, but some wheeled vehicles fall into this category. IFVs are generally less heavily armed and armoured than main battle tanks. They sometimes carry anti-tank missiles to protect and support infantry against armoured threats, such as the NATO TOW missile and Soviet Bastion, which offer a significant threat to tanks. Specially-equipped IFVs have taken on some of the roles of light tanks; they are used by reconnaissance organizations, and light IFVs are used by airborne units which must be able to fight without the heavy firepower of tanks.
Infantry mobility vehicle.
An infantry mobility vehicle (IMV) or protected patrol vehicle (PPV) is a wheeled armored personnel carrier (APC) serving as a military patrol, reconnaissance or security vehicle. Examples include the ATF Dingo, AMZ Dzik, AMZ Tur, Mungo ESK, and Bushmaster IMV. This term also applies to the vehicles currently being fielded as part of the MRAP program.
IMVs were developed in response to the threats of modern counter insurgency warfare, with an emphasis on Ambush Protection and Mine-Resistance. Similar vehicles existed long before the term IMV was coined, such as the French VAB and South African Buffel. The term is coming more into use to differentiate light 4x4 wheeled APCs from the traditional 8x8 wheeled APCs. It is a neologism for what might have been classified in the past as an armoured scout car, such as the BRDM, but the IMV is distinguished by having a requirement to carry dismountable infantry. The up-armoured M1114 Humvee variant can be seen as an adaptation of the unarmoured Humvee to serve in the IMV role.
Amphibious vehicles.
Many modern military vehicles, ranging from light wheeled command and reconnaissance, through armoured personnel carriers and tanks, are manufactured with amphibious capabilities. Contemporary wheeled armoured amphibians include the French Véhicule de l'Avant Blindé and Véhicule Blindé Léger. The latter is a small, lightly armoured 4x4 all-terrain vehicle that is fully amphibious and can swim at 5.4 km/h. The VAB ("Véhicule de l'Avant Blindé" - "Armoured Vanguard Vehicle") is a fully amphibious armoured personnel carrier powered in the water by two water jets, that entered service in 1976 and produced in numerous configurations, ranging from basic personnel carrier, anti-tank missile platform.
During the Cold War the Soviet bloc states developed a number of amphibious APCs, fighting vehicles and tanks, both wheeled and tracked. Most of the vehicles the Soviets designed were amphibious, or could ford deep water. Wheeled examples are the BRDM-1 and BRDM-2 4x4 armoured scout cars, as well as the BTR-60, BTR-70, BTR-80 and BTR-94 8x8 armoured personnel carriers and the BTR-90 infantry fighting vehicle.
The United States started developing a long line of Landing Vehicle Tracked (LVT) designs from ca. 1940. The US Marine Corps currently uses the AAV7-A1 Assault Amphibious Vehicle, which was to be succeeded by the Expeditionary Fighting Vehicle, which was capable of planing on water and can achieve water speeds of 37–46 km/h. The EFV project has been cancelled.
A significant amount of tracked armoured vehicles that are primarily intended for land-use, have some amphibious cability, tactically useful inland, reducing dependence on bridges. They use their tracks, sometimes with added propeller or water jets for propulsion. As long as the banks have a shallow enough slopes to enter or leave the water they can cross rivers and water obstacles.
Some heavy tanks can operate amphibiously with a fabric skirt to add buoyancy. The Sherman DD tank used in the Normandy landings had this setup. When in water the waterproof float screen was raised and propellers deployed. Some modern vehicles use a similar skirt.
Armoured engineering vehicle.
Modern engineering AFV's utilize chassis based on main battle tank platforms: these vehicles are as well armoured and protected as tanks, designed to keep up tanks, conduct obstacle breaching operations to help tanks get to wherever it needs to be, perform utility functions necessary to expedite mission objectives of tanks, and to conduct other earth-moving and engineering work on the battlefield. These vehicles go by different names depending upon the country of use or manufacture. In the United States the term "combat engineer vehicle (CEV)" is used, in the United Kingdom the term "Armoured Vehicle Royal Engineers (AVRE)" is used, while in Canada and other commonwealth nations the term "armoured engineer vehicle (AEV)" is used. There is no set template for what such a vehicle will look like, yet likely features include a large dozer blade or mine ploughs, a large calibre demolition cannon, augers, winches, excavator arms and cranes, or lifting booms.
It should be noted that while the term "armoured engineer vehicle" is used specifically to describe these multi-purpose tank-based engineering vehicles, that term is also used more generically in British and Commonwealth militaries to describe all heavy tank-based engineering vehicles used in the support of mechanized forces. Thus, "armoured engineer vehicle" used generically would refer to AEV, AVLB, Assault Breachers, and so on. Good examples of this type of vehicle include the UK Trojan AVRE, the Russian IMR, and the US M728 Combat Engineer Vehicle.
Assault breacher vehicle.
An assault breacher vehicle (ABV), also known as a explosive ordnance disposal vehicle (EODV), or simply Breacher, is especially designed to clear pathways for troops and other vehicles through minefields and along roadside bombs and Improvised Explosive Devices. These vehicles are based on a tank-chassis with 1,500+ horsepower engines, but fitted with a 50-caliber machine gun and a front-mounted 5-meter-wide plow, supported by metallic skis that glide on the dirt and typically equipped with at least of Mine Clearing Line Charges: rockets carrying C-4 explosives up to 100–150 meters forward, detonating hidden bombs at a safe distance, so that troops and vehicles can pass through safely. They were called ""the answer"" to the deadliest threat facing NATO troops in modern asymmetrical conflict.
Armoured bulldozer.
The armored bulldozer is a basic tool of combat engineering. These combat engineering vehicles combine the earth moving capabilities of the bulldozer with armor which protects the vehicle and its operator in or near combat. Most are civilian bulldozers modified by addition of vehicle armor/military equipment, but some are tanks stripped of armament and fitted with a dozer blade. Some tanks have bulldozer blades while retaining their armament, but this does not make them armored bulldozers as such, because combat remains the primary role – earth moving is a secondary task.
Armoured recovery vehicle.
An armoured recovery vehicle (ARV) is a type of vehicle recovery armoured fighting vehicle used to repair battle- or mine-damaged as well as broken-down armoured vehicles during combat, or to tow them out of the danger zone for more extensive repairs. To this end the term ""Armoured Repair and Recovery Vehicle" (ARRV)" is also used.
ARVs are normally built on the chassis of a main battle tank (MBT), but some are also constructed on the basis of other armoured fighting vehicles, mostly armoured personnel carriers (APCs). ARVs are usually built on the basis of a vehicle in the same class as they are supposed to recover; a tank-based ARV is used to recover tanks, while an APC-based one recovers APCs, but does not have the power to tow a much heavier tank.
Armoured vehicle-launched bridge.
An armoured vehicle-launched bridge (AVLB) is a combat support vehicle, sometimes regarded as a subtype of combat engineering vehicle, designed to assist militaries in rapidly deploying tanks and other armoured fighting vehicles across rivers. The AVLB is usually a tracked vehicle converted from a tank chassis to carry a folding metal bridge instead of weapons. The AVLB's job is to allow armoured or infantry units to cross water, when a river too deep for vehicles to wade through is reached, and no bridge is conveniently located (or sufficiently sturdy, a substantial concern when moving 60-ton tanks).
The bridge layer unfolds and launches its cargo, providing a ready-made bridge across the obstacle in only minutes. Once the span has been put in place, the AVLB vehicle detaches from the bridge, and moves aside to allow traffic to pass. Once all of the vehicles have crossed, it crosses the bridge itself and reattaches to the bridge on the other side. It then retracts the span ready to move off again. A similar procedure can be employed to allow crossings of small chasms or similar obstructions. AVLBs can carry bridges of or greater in length. By using a tank chassis, the bridge layer is able to cover the same terrain as main battle tanks, and the provision of armour allows them to operate even in the face of enemy fire. However, this is not a universal attribute: some exceptionally sturdy 6x6 or 8x8 truck chassis have lent themselves to bridge-layer applications.
Combat engineer section carriers.
The combat engineer section carriers are used to transport sappers (combat engineers) and can be fitted with a bulldozer's blade and other mine-breaching devices. They are often used as APCs because of their carrying ability and heavy protection. They are usually armed with machine guns and grenade launchers and usually tracked to provide enough tractive force to push blades and rakes. Some examples are the U.S. M113 APC, IDF Puma, Nagmachon, Husky, and U.S. M1132 ESV (a Stryker variant).
Air defense vehicles.
An anti-aircraft vehicle, also known as a self-propelled anti-aircraft weapon (SPAA) or self-propelled air defense system (SPAD), is a mobile vehicle with a dedicated anti-aircraft capability. The Russian equivalent of SPAAG is ZSU (from "zenitnaya samokhodnaya ustanovka" – "anti-aircraft self-propelled mount"). Specific weapon systems used include machine guns, autocannons, larger guns, or missiles, and some mount both guns and longer-ranged missiles. Platforms used include both trucks and heavier combat vehicles such as APCs and tanks, which add protection from aircraft, artillery, and small arms fire for front line deployment. Anti-aircraft guns are usually mounted in a quickly-traversing turret with a high rate of elevation, for tracking fast-moving aircraft. They are often in dual or quadruple mounts, allowing a high rate of fire. Today, missiles (generally mounted on similar turrets) have largely supplanted anti-aircraft guns.
Self-propelled artillery.
Self-propelled artillery vehicles give mobility to artillery. Within the term are covered self-propelled guns (or howitzers) and rocket artillery. They are highly mobile, usually based on tracked chassis carrying either a large howitzer or other field gun or alternatively a mortar or some form of rocket or missile launcher. They are usually used for long-range indirect bombardment support on the battlefield.
In the past, self-propelled artillery has included direct-fire "Gun Motor Carriage" vehicles such as assault guns and tank destroyers (also known as self-propelled anti-tank guns). These have been heavily armoured vehicles, the former providing danger-close fire-support for infantry and the latter acting as specialized anti-tank vehicles.
Modern self-propelled artillery vehicles may superficially resemble tanks, but they are generally lightly armoured, too lightly to survive in direct-fire combat. However, they protect their crews against shrapnel and small arms and are therefore usually included as armoured fighting vehicles. Many are equipped with machine guns for defence against enemy infantry.
The key advantage of self-propelled over towed artillery is that it can be brought into action much faster. Before the towed artillery can be used, it has to stop, unlimber and set up the guns. To move position, the guns must be limbered up again and brought — usually towed — to the new location. By comparison self-propelled artillery in combination with modern communications can stop at a chosen location and begin firing almost immediately, then quickly move on to a new position. This ability is very useful in a mobile conflict and particularly on the advance.
Conversely, towed artillery was and remains cheaper to build and maintain. It is also lighter and can be taken to places that self-propelled guns cannot reach, so despite the advantages of the self-propelled artillery, towed guns remain in the arsenals of many modern armies.
Assault gun.
An assault gun is a gun or howitzer mounted on a motor vehicle or armoured chassis, designed for use in the direct fire role in support of infantry when attacking other infantry or fortified positions.
Historically the custom-built fully armored assault guns usually mounted the gun or howitzer in a fully enclosed casemate on a tank chassis. The use of a casemate instead of a gun turret limited these weapons field of fire, but allowed a larger gun to be fitted relative to the chassis, more armour to be fitted for the same weight, and provided a cheaper construction. In most cases, these turretless vehicles also presented a lower profile as a target for the enemy.
Mortar carrier.
A mortar carrier is a self-propelled artillery vehicle carrying a mortar as its primary weapon. Mortar carriers cannot be fired while on the move and some must be dismounted to fire. In U.S. Army doctrine, mortar carriers provide close and immediate indirect fire support for maneuver units while allowing for rapid displacement and quick reaction to the tactical situation. The ability to relocate not only allows fire support to be provided where it is needed faster but also allows these units to avoid counter-battery fire. Mortar carriers have traditionally avoided direct contact with the enemy. Many units report never using secondary weapons in combat.
Prior to the Iraq War, American 120 mm mortar platoons reorganized from six M1064 mortar carriers and two M577 fire direction centers (FDC) to four M1064 and one FDC. The urban environment of Iraq made it difficult to utilize mortars. New technologies such as mortar ballistic computers and communication equipment and are being integrated. Modern era combat is becoming more reliant on direct fire support from mortar carrier machine guns.
Multiple rocket launcher.
A multiple rocket launcher is a type of unguided rocket artillery system. Like other rocket artillery, multiple rocket launchers are less accurate and have a much lower (sustained) rate of fire than batteries of traditional artillery guns. However, they have the capability of simultaneously dropping many hundreds of kilograms of explosive, with devastating effect.
The Korean Hwacha is an example of an early weapon system with a resemblance to the modern-day multiple rocket launcher. The first modern multiple rocket launcher was the German "Nebelwerfer" of the 1930s, a small towed artillery piece. Only later in World War II did the Allies deploy similar weapons in the form of the Land Mattress.
The first self-propelled multiple rocket launchers – and arguably the most famous – were the Soviet BM-13 Katyushas, first used during World War II and exported to Soviet allies afterwards. They were simple systems in which a rack of launch rails was mounted on the back of a truck. This set the template for modern multiple rocket launchers. The Americans mounted tubular launchers atop M4 Sherman tanks to create the T34 Calliope rocket launching tank, only used in small numbers, as their closest equivalent to the Katyusha.
Tank destroyer.
Tank destroyers and tank hunters are armed with an anti-tank gun or missile launcher, and are designed specifically to engage enemy armoured vehicles. Many have been based on a tracked tank chassis, while others are wheeled. Since World War II, main battle tanks have largely replaced gun-armed tank destroyers; although lightly armoured anti tank guided missile (ATGM) carriers are commonly used for supplementary long-range anti-tank engagements.
In post-Cold War conflict, the resurgence of expeditionary warfare has seen the emergence of gun-armed wheeled vehicles, sometimes called "protected gun systems", which may bear a superficial resemblance to tank destroyers, but are employed as direct fire support units typically providing support in low intensity operations such as Iraq and Afghanistan. These have the advantage of easier deployment, as only the largest air transports can carry a main battle tank, and their smaller size makes them more effective in urban combat.
Many forces' IFVs carry anti-tank missiles in every infantry platoon, and attack helicopters have also added anti-tank capability to the modern battlefield. But there are still dedicated anti-tank vehicles with very heavy long-range missiles, or intended for airborne use. There have also been dedicated anti-tank vehicles built on ordinary armoured personnel carrier or armoured car chassis. Examples include the U.S. M901 ITV (Improved TOW Vehicle) and the Norwegian NM142, both on an M113 chassis, several Soviet ATGM launchers based on the BRDM reconnaissance car, the British FV438 Swingfire and FV102 Striker and the German Raketenjagdpanzer series built on the chassis of the HS 30 and Marder IFV.
Armoured train.
An armoured train is a railway train protected with armour. They are usually equipped with railroad cars armed with artillery and machine guns. They were mostly used during the late 19th and early 20th century, when they offered an innovative way to quickly move large amounts of firepower. Their use was discontinued in most countries when road vehicles became much more powerful and offered more flexibility, and because armoured trains were too vulnerable to track sabotage as well as attacks from the air. However, the Russian Federation used improvised armoured trains in the Second Chechen War in the late 1990s and 2000s.
The railroad cars on an armoured train were designed for many tasks such as carrying guns and machine guns, infantry units, anti-aircraft guns. During World War II, the Germans would sometimes put a "Fremdgerät" (such as a captured French Somua S-35 or Czech PzKpfw 38(t) light tank, or Panzer II light tank) on a flatbed car which could be quickly offloaded by means of a ramp and used away from the range of the main railway line to chase down enemy partisans.
Different types of armour were used to protect from attack by tanks. In addition to various metal plates, concrete and sandbags were used in some cases for improvised armoured trains.
Armoured trains were sometimes escorted by a kind of rail-tank called a draisine. One such example was the 'Littorina' armoured trolley which had a cab in the front and rear, each with a control set so it could be driven down the tracks in either direction. Littorina mounted two dual 7.92mm MG13 machine gun turrets from Panzer I light tanks.

</doc>
<doc id="2151" url="https://en.wikipedia.org/wiki?curid=2151" title="Anton Drexler">
Anton Drexler

Anton Drexler (13 June 1884 – 24 February 1942) was a German far-right political leader of the 1920s who was instrumental in the formation of the pan-German and anti-Semitic German Workers' Party ("Deutsche Arbeiterpartei" – DAP), the antecedent of the Nazi Party ("Nationalsozialistische Deutsche Arbeiterpartei" – NSDAP). Drexler served as mentor to Adolf Hitler during his early days in politics.
Biography.
Born in Munich, Drexler was a machine-fitter before becoming a railway toolmaker and locksmith in Berlin. He joined the Fatherland Party during World War I. In March 1918 Drexler founded a branch of the "Freien Arbeiterausschuss für einen guten Frieden" (Free Workers' Committee for a Good Peace) league. Thereafter in 1918, Karl Harrer (a journalist and member of the Thule Society), convinced Drexler and several others to form the "Politischer Arbeiterzirkel" (Political Workers' Circle) in 1918. The members met periodically for discussions with themes of nationalism and racism directed against the Jews. Drexler was a poet and a member of the völkisch agitators. Together with Harrer, Gottfried Feder and Dietrich Eckart, he founded the German Workers' Party (DAP) in Munich on 5 January 1919.
At a meeting of the Party in Munich in September 1919, the main speaker was Gottfried Feder. When he had finished speaking, Adolf Hitler got involved in a heated political argument with a visitor, Professor Baumann, who questioned the soundness of Feder's arguments against capitalism and proposed that Bavaria should break away from Prussia and found a new South German nation with Austria. In vehemently attacking the man's arguments he made an impression on the other party members with his oratory skills, and according to Hitler, the "professor" left the hall acknowledging unequivocal defeat. Drexler approached Hitler and thrust a booklet into his hand. It was "My Political Awakening" and, according to Hitler, it reflected the ideals he already believed in. Impressed with Hitler, Drexler invited him to join the DAP. Hitler accepted on 12 September 1919, becoming the party's 55th member. In less than a week, Hitler received a postcard from Drexler stating he had officially been accepted as a DAP member and he should come to a "committee" meeting to discuss it. Hitler attended the "committee" meeting held at the run-down Alte Rosenbad beer-house.
Hitler began to make the party more public, and he organized their biggest meeting yet of 2,000 people, for 24 February 1920 in the Hofbräuhaus in Munich. It was in this speech that Hitler, for the first time, enunciated the twenty-five points of the "German Worker's Party'"s manifesto that had been drawn up by Drexler, Feder, and Hitler. Through these points he gave the organisation a much bolder stratagem with a clear foreign policy (abrogation of The Treaty of Versailles, a Greater Germany, Eastern expansion, exclusion of Jews from citizenship). On the same day the party was renamed the "Nationalsozialistische Deutsche Arbeiterpartei" – NSDAP (Nazi Party). Such was the significance of this particular move in publicity that Karl Harrer resigned from the party in disagreement.
By 1921, Hitler was rapidly becoming the undisputed leader of the Party. In June 1921, while Hitler and Eckart were on a fundraising trip to Berlin, a mutiny broke out within the NSDAP in Munich. Members of its executive committee wanted to merge with the rival German Socialist Party (DSP). Hitler returned to Munich on 11 July and angrily tendered his resignation. The committee members realised that the resignation of their leading public figure and speaker would mean the end of the party. Hitler announced he would rejoin on the condition that he would replace Drexler as party chairman, and that the party headquarters would remain in Munich. The committee agreed; he rejoined the party as member 3,680. Drexler was thereafter moved to the purely symbolic position of honorary president, and left the Party in 1923.
Drexler was also a member of a "völkisch" political club for affluent members of Munich society known as the Thule Society. His membership in the NSDAP ended when it was temporarily outlawed in 1923 following the Beer Hall Putsch, in which Drexler had not taken part. In 1924 he was elected to the Bavarian state parliament for another party, in which he served as vice-president until 1928. He had no part in the NSDAP's refounding in 1925, and rejoined only after Hitler had come to power in 1933. He received the party's Blood Order in 1934 and was still occasionally used as a propaganda tool until about 1937, but he was never again allowed any real power or to play an active part in the movement. He died in Munich in February 1942.
In popular culture.
In the 2003 miniseries "", British actor Robert Glenister plays Drexler, although Drexler is portrayed without his trademark spectacles and moustache.

</doc>
<doc id="2152" url="https://en.wikipedia.org/wiki?curid=2152" title="All Quiet on the Western Front">
All Quiet on the Western Front

All Quiet on the Western Front () is a novel by Erich Maria Remarque, a German veteran of World War I. The book describes the German soldiers' extreme physical and mental stress during the war, and the detachment from civilian life felt by many of these soldiers upon returning home from the front.
The novel was first published in November and December 1928 in the German newspaper "Vossische Zeitung" and in book form in late January 1929. The book and its sequel, "The Road Back" (1930), were among the books banned and burned in Nazi Germany. "All Quiet on the Western Front" sold 2.5 million copies in 22 languages in its first 18 months in print.
In 1930, the book was adapted as an Academy-Award winning film of the same name, directed by Lewis Milestone.
Title and translation.
The 1929 English translation by Arthur Wesley Wheen gives the title as " All Quiet on the Western Front". The literal translation of "Im Westen nichts Neues" is "In the West Nothing New," with "West" being the Western Front; the phrase refers to the content of an official communiqué at the end of the novel.
Brian Murdoch's 1993 translation would render the phrase as "there was nothing new to report on the Western Front" within the narrative. Explaining his retention of the original book-title, he says:
Although it does not match the German exactly, Wheen's title has justly become part of the English language and is retained here with gratitude.
The phrase "all quiet on the Western Front" has become a colloquial expression meaning stagnation, or lack of visible change, in any context.
Plot summary.
The book tells the story of Paul Bäumer, a German soldier who—urged on by his school teacher—joins the German army shortly after the start of World War I. His class was "scattered over the platoons amongst Frisian fishermen, peasants, and labourers." Bäumer arrives at the Western Front with his friends and schoolmates (Leer, Müller, Kropp and a number of other characters). There they meet Stanislaus Katczinsky, an older soldier, nicknamed Kat, who becomes Paul's mentor. While fighting at the front, Bäumer and his comrades have to engage in frequent battles and endure the treacherous and filthy conditions of trench warfare.
At the very beginning of the book, Erich Maria Remarque says "This book is to be neither an accusation nor a confession, and least of all an adventure, for death is not an adventure to those who stand face to face with it. It will try simply to tell of a generation of men who, even though they may have escaped (its) shells, were destroyed by the war." The book does not focus on heroic stories of bravery, but rather gives a view of the conditions in which the soldiers find themselves. The monotony between battles, the constant threat of artillery fire and bombardments, the struggle to find food, the lack of training of young recruits (meaning lower chances of survival), and the overarching role of random chance in the lives and deaths of the soldiers are described in detail.
The battles fought here have no names and seem to have little overall significance, except for the impending possibility of injury or death for Bäumer and his comrades. Only pitifully small pieces of land are gained, about the size of a football field, which are often lost again later. Remarque often refers to the living soldiers as old and dead, emotionally drained and shaken. "We are not youth any longer. We don't want to take the world by storm. We are fleeing from ourselves, from our life. We were eighteen and had begun to love life and the world; and we had to shoot it to pieces."
Paul's visit on leave to his home highlights the cost of the war on his psyche. The town has not changed since he went off to war; however, he finds that he does "not belong here anymore, it is a foreign world." He feels disconnected from most of the townspeople. His father asks him "stupid and distressing" questions about his war experiences, not understanding "that a man cannot talk of such things." An old schoolmaster lectures him about strategy and advancing to Paris, while insisting that Paul and his friends know only their "own little sector" of the war but nothing of the big picture.
Indeed, the only person he remains connected to is his dying mother, with whom he shares a tender, yet restrained relationship. The night before he is to return from leave, he stays up with her, exchanging small expressions of love and concern for each other. He thinks to himself, "Ah! Mother, Mother! How can it be that I must part from you? Here I sit and there you are lying; we have so much to say, and we shall never say it." In the end, he concludes that he "ought never to have come on leave."
Paul feels glad to be reunited with his comrades. Soon after, he volunteers to go on a patrol and kills a man for the first time in hand-to-hand combat. He watches the man die, in pain for hours. He feels remorse and asks forgiveness from the man's corpse. He is devastated and later confesses to Kat and Albert, who try to comfort him and reassure him that it is only part of the war.
They are then sent on what Paul calls a "good job." They must guard a supply depot in a village that was evacuated due to being shelled too heavily. During this time, the men are able to adequately feed themselves; unlike the near-starvation conditions in the German trenches. In addition, the men enjoy themselves while living off the spoils from the village and officers' luxuries from the supply depot (such as fine cigars). While evacuating the villagers (enemy civilians), Paul and Albert are taken by surprise by artillery fired at the civilian convoy and wounded by a shell. On the train back home, Albert takes a turn for the worse and cannot complete the journey, instead being sent off the train to recuperate in a Catholic hospital.
Paul uses a combination of bartering and manipulation to stay by Albert's side. Albert eventually has his leg amputated, while Paul is deemed fit for service and returned to the front.
By now, the war is nearing its end and the German Army is retreating. In despair, Paul watches as his friends fall one by one. It is the death of Kat that eventually makes Paul careless about living. In the final chapter, he comments that peace is coming soon, but he does not see the future as bright and shining with hope. Paul feels that he has no aims or goals left in life and that their generation will be different and misunderstood. When he dies at the end of the novel, the situation report from the frontline states, "All is Quiet on the Western Front," symbolizing the insignificance of one individual's death during the war.
Themes.
One of the major themes of the novel is the difficulty of soldiers to revert to civilian life after having experienced extreme combat situations. Remarque comments in the preface that "Quiet on the Western Front will try simply to tell of a generation of men who, even though they may have escaped its shells, were destroyed by the war." This internal destruction can be found as early as the first chapter as Paul comments that, although all the boys are young, their youth has left them. In addition, the massive loss of life and negligible gains from the fighting are constantly emphasized. Soldiers' lives are thrown away by their commanding officers who are stationed comfortable away from the front, ignorant of the daily terrors of the front line. 
Main characters.
Paul Bäumer.
Paul Bäumer is the main character and narrator. At 18 years of age, Paul enlists in the German Army and is deployed to the Western Front where he experiences the severe psychological and physical effects of the war. 
Before the war, Paul was a creative, sensitive and passionate person, writing poems and having a clear love for his family. But as the war changed his attitude and personality, poems and other aspects of his past life become something Paul was no longer linked to, since the horrors of war trained him to disconnect himself from his feelings. He feels he can't tell anyone about his experiences, and feels like an outsider where his family is concerned.
By the end of the book, Paul realises that he no longer knows what to do with himself and decides that he has nothing more to lose. The war appears to have snuffed out his hopes and dreams, which he feels he can never regain. After years of fighting in the war, Paul is finally killed in October 1918, on an extraordinarily quiet, peaceful day. The army report that day contains only one phrase: “All quiet on the Western Front.” As Paul dies, his face is calm, “as though almost glad the end had come."
Albert Kropp.
Kropp was in Paul's class at school and is described as the clearest thinker of the group as well as the smallest. Kropp is wounded towards the end of the novel and undergoes an amputation. Both he and Bäumer end up spending time in a Catholic hospital together, Bäumer suffering from shrapnel wounds to the leg and arm. Though Kropp initially plans to commit suicide if he requires an amputation, the book suggests he postponed suicide because of the strength of military camaraderie. Kropp and Bäumer part ways when Bäumer is recalled to his regiment after recovering. Paul comments that saying farewell was "very hard, but it is something a soldier learns to deal with."
Haie Westhus.
Haie is described as being tall and strong, and a peat-digger by profession. Overall, his size and behavior make him seem older than Paul, yet he is the same age as Paul and his school-friends (roughly 19 at the start of the book). Haie in addition has a good sense of humor. During combat, he is injured in his back, fatally (Chapter 6) — the resulting wound is large enough for Paul to see Haie's breathing lung when Himmelstoß (Himmelstoss) carries him to safety.
Fredrich Müller.
Müller is about 18 and a half years of age, one of Bäumer's classmates, when he also joins the German army as a volunteer to go to the war. Carrying his old school books with him to the battlefield, he constantly reminds himself of the importance of learning and education. Even while under enemy fire, he "mutters propositions in physics". He became interested in Kemmerich's boots and inherits them when Kemmerich dies early in the novel. He is killed later in the book after being shot point-blank in the stomach with a "light pistol" (flare gun). As he was dying "quite conscious and in terrible pain", he gave his boots which he inherited from Kemmerich to Paul.
Stanislaus "Kat" Katczinsky.
Kat has the most positive influence on Paul and his comrades on the battlefield. Katczinsky was a cobbler in civilian life; he is older than Paul Bäumer and his comrades, about 40 years old, and serves as their leadership figure. He also represents a literary model highlighting the differences between the younger and older soldiers. While the older men have already had a life of professional and personal experience before the war, Bäumer and the men of his age have had little life experience or time for personal growth.
Kat is also well known for his ability to scavenge nearly any item needed, especially food. At one point he secures four boxes of lobster. Bäumer describes Kat as possessing a sixth sense. One night, Bäumer along with a group of other soldiers are holed up in a factory with neither rations nor comfortable bedding. Katczinsky leaves for a short while, returning with straw to put over the bare wires of the beds. Later, to feed the hungry men, Kat brings bread, a bag of horse flesh, a lump of fat, a pinch of salt and a pan in which to cook the food.
Kat is hit by shrapnel at the end of the story, leaving him with a smashed shin. Paul carries him back to camp on his back, only to discover upon their arrival that a stray splinter had hit Kat in the back of the head and killed him on the way. He is thus the last of Paul's close friends to die in battle. It is Kat's death that eventually makes Bäumer careless whether he survives the war or not, but that he can face the rest of his life without fear. "Let the months and the years come, they can take nothing from me, they can take nothing more. I am so alone, and so without hope that I can confront them without fear."
Tjaden.
One of Bäumer's non-schoolmate friends. Before the war Tjaden was a locksmith. A big eater with a grudge against the former postman-turned corporal Himmelstoß (thanks to his strict 'disciplinary actions'), he manages to forgive Himmelstoß later in the book. Throughout the book, Paul frequently remarks on how much of an eater he is, yet somehow manages to stay as "thin as a rake." Tjaden appears in the sequel, "The Road Back".
Minor characters.
Kantorek.
Kantorek was the schoolmaster of Paul and his friends, including Kropp, Leer, Müller, and Behm. Behaving "in a way that cost nothing," Kantorek is a strong supporter of the war and encourages Bäumer and other students in his class to join the war effort. Among twenty enlistees was Joseph Behm, the first of the class to die in battle. In an example of tragic irony, Behm was the only one who did not want to enter the war.
Kantorek is a hypocrite, urging the young men he teaches to fight in the name of patriotism, while not voluntarily enlisting himself. In a twist of fate, Kantorek is later called up as a soldier as well. He very reluctantly joins the ranks of his former students, only to be drilled and taunted by Mittelstädt, one of the students he had earlier persuaded to enlist.
Peter Leer.
Leer is an intelligent soldier in Bäumer's company, and one of his classmates. He is very popular with women; when he and his comrades meet three French women, he is the first to seduce one of them. Bäumer describes Leer's ability to attract women by saying "Leer is an old hand at the game". In chapter 11, Leer is hit by a shell fragment, which also hits Bertinck. The shrapnel tears open Leer's hip, causing him to bleed to death quickly. His death causes Paul to ask himself, "What use is it to him now that he was such a good mathematician in school?"
Bertinck.
Lieutenant Bertinck is the leader of Bäumer's company. His men have a great respect for him, and Bertinck has great respect for his men. He permits them to eat the rations of the men that had been killed in action, standing up to the chef Ginger who would only allow them their allotted share. Bertinck is genuinely despondent when he learns that few of his men had survived an engagement.
When he and the other characters are trapped in a trench under heavy attack, Bertinck, who has been injured in the firefight, spots a flamethrower team advancing on them. He gets out of cover and takes aim on the flamethrower but misses, and gets hit by enemy fire. With his next shot he kills the flamethrower, and immediately afterwards an enemy shell explodes on his position blowing off his chin. The same explosion also fatally wounds Leer.
Himmelstoß.
Corporal Himmelstoß (spelled Himmelstoss in some editions) was a postman before enlisting in the war. He is a power-hungry corporal with special contempt for Paul and his friends, taking sadistic pleasure in punishing the minor infractions of his trainees during their basic training in preparation for their deployment. Paul later figures that the training taught by Himmelstoß made them "hard, suspicious, pitiless, and tough" but most importantly it taught them comradeship. However, Bäumer and his comrades have a chance to get back at Himmelstoß because of his punishments, mercilessly whipping him on the night before they board trains to go to the front.
Himmelstoß later joins them at the front, revealing himself as a coward who shirks his duties for fear of getting hurt or killed, and pretends to be wounded because of a scratch on his face. Paul Bäumer beats him because of it and when a lieutenant comes along looking for men for a trench charge, Himmelstoß joins and leads the charge. He carries Haie Westhus's body to Bäumer after he is fatally wounded. Matured and repentant through his experiences Himmelstoß later asks for forgiveness from his previous charges. As he becomes the new staff cook, to prove his friendship he secures two pounds of sugar for Bäumer and half a pound of butter for Tjaden.
Detering.
Detering is a farmer who constantly longs to return to his wife and farm. He is also fond of horses and is angered when he sees them used in combat. He says, "It is of the vilest baseness to use horses in the war," when the group hears several wounded horses writhe and scream for a long time before dying during a bombardment. He tries to shoot them to put them out of misery, but is stopped by Kat to keep their current position hidden. He is driven to desert when he sees a cherry tree in blossom, which reminds him of home too much and inspires him to leave. He is found by military police and court-martialed, and is never heard from again.
Josef Hamacher.
Hamacher is a patient at the Catholic hospital where Paul and Albert Kropp are temporarily stationed. He has an intimate knowledge of the workings of the hospital. He also has a "Special Permit," certifying him as sporadically not responsible for his actions due to a head wound, though he is clearly quite sane and exploiting his permit so he can stay in the hospital and away from the war as long as possible.
Franz Kemmerich.
A young boy of only 19 years. Franz Kemmerich had enlisted in the army for World War I along with his best friend and classmate, Bäumer. Kemmerich is shot in the leg early in the story; his injured leg has to be amputated, and he dies shortly after. In anticipation of Kemmerich's imminent death, Müller was eager to get his boots. While in the hospital, someone steals Kemmerich's watch that he intended to give to his mother, causing him great distress and prompting him to ask about his watch every time his friends visit him in the hospital. Paul later finds the watch and hands it over to Kemmerich's mother, only to lie Franz died instantly and painlessly when questioned.
Joseph Behm.
A student in Paul's class. Behm was the only student that was not quickly influenced by Kantorek's patriotism to join the war. Eventually, due to pressure from friends and Kantorek, he joins the war. He is the first of Paul's friends to die. He is blinded in no man's land and believed to be dead by his friends. The next day, when he is seen walking blindly around no-man's-land, it is discovered that he was only unconscious. However, he is killed before he can be rescued.
Publication and Reception.
From November 10 to December 9, 1928, "All Quiet on the Western Front" was published in serial form in Vossische Zeitung magazine. It was released in book form the following year to smashing success, selling one and a half million copies that same year. Although publishers had worried that interest in the Great War had waned more than 10 years after the armistice, Remarque's realistic depiction of trench warfare from the perspective of young soldiers struck a chord with the war's survivors—soldiers and civilians alike—and provoked strong reactions, both positive and negative, around the world.
With "All Quiet on the Western Front", Remarque emerged as an eloquent spokesperson for a generation that had been, in his own words, "destroyed by war, even though it might have escaped its shells." Remarque's harshest critics, in turn, were his countrymen, many of whom felt the book denigrated the German war effort, and that Remarque had exaggerated the horrors of war to further his pacifist agenda. The strongest voices against Remarque came from the emerging National Socialist Party and its ideological allies. In 1933, when the Nazis rose to power, "All Quiet on the Western Front" became one of the first degenerate books to be publicly burnt; in 1930 screenings of the Academy Award-winning film based on the book were met with Nazi-organized protests and mob attacks on both movie theatres and audience members.
However, objections to Remarque’s portrayal of the German army personnel during World War I were not limited to the Nazis. Dr. Karl Kroner (de) objected to Remarque’s depiction of the medical personnel as being inattentive, uncaring, or absent from frontline action. Dr. Kroner was specifically worried that the book would perpetuate German stereotypes abroad that had subsided since the First World War. He offered the following clarification: “People abroad will draw the following conclusions: if German doctors deal with their own fellow countrymen in this manner, what acts of inhumanity will they not perpetuate against helpless prisoners delivered up into their hands or against the populations of occupied territory?” 
A fellow patient of Remarque’s in the military hospital in Duisburg objected to the negative depictions of the nuns and patients, and of the general portrayal of soldiers: “There were soldiers to whom the protection of homeland, protection of house and homestead, protection of family were the highest objective, and to whom this will to protect their homeland gave the strength to endure any extremities.”
These criticisms suggest that perhaps experiences of the war and the personal reactions of individual soldiers to their experiences may be more diverse than Remarque portrays them; however, it is beyond question that Remarque gives voice to a side of the war and its experience that was overlooked or suppressed at the time. This perspective is crucial to understanding the true effects of World War I. The evidence can be seen in the lingering depression that Remarque and many of his friends and acquaintances were suffering a decade later.
In contrast, "All Quiet on the Western Front" was trumpeted by pacifists as an anti-war book. Remarque makes a point in the opening statement that the novel does not advocate any political position, but is merely an attempt to describe the experiences of the soldier.
The main artistic criticism was that it was a mediocre attempt to cash in on public sentiment. The enormous popularity the work received was a point of contention for some literary critics, who scoffed at the fact that such a simple work could be so earth-shattering. Much of this literary criticism came from Salomo Friedlaender, who wrote a book "Hat Erich Maria Remarque wirklich gelebt?" "Did Erich Maria Remarque really live?". Another author, Max Joseph Wolff, wrote a parody titled "Vor Troja nichts Neues" "Before Troja, nothing new" under the pseudonym "Emil Marius Requark". Friedlaender’s criticism's were mainly personal in nature—he attacked Remarque as being ego-centric and greedy. Remarque publicly stated that he wrote "All Quiet on the Western Front" for personal reasons, not for profit, as Friedlaender had claimed.
Adaptations.
Film.
In 1930, an American film of the novel was made, directed by Lewis Milestone; with a screenplay by Maxwell Anderson, George Abbott, Del Andrews, C. Gardner Sullivan; and with uncredited work by Walter Anthony and Milestone. It stars Louis Wolheim, Lew Ayres, John Wray, Arnold Lucy, and Ben Alexander.
The film won the Academy Award for Best Picture in 1930 for its producer Carl Laemmle Jr., and an Academy Award for Directing for Lewis Milestone. It was the first all-talking non-musical film to win the Best Picture Oscar. It also received two further nominations: Best Cinematography, for Arthur Edeson, and Best Writing Achievement for Abbott, Anderson and Andrews.
In June 2009, an announcement was made that "All Quiet on the Western Front" would be remade. Director Mimi Leder was linked with the film in 2011, but as of 2013, the film was still listed as being in pre-production.
TV film.
In 1979, the film was remade for CBS television by Delbert Mann, starring Richard Thomas of "The Waltons" as Paul Bäumer and Ernest Borgnine as Kat. The movie was filmed in Czechoslovakia.
Music.
Elton John's album "Jump Up!" (1982) features the song, "All Quiet on the Western Front" (written by Elton and Bernie Taupin). The song is a sorrowful rendition of the novel's story ("It's gone all quiet on the Western Front / Male Angels sigh / ghosts in a flooded trench / As Germany dies").
Radio.
On November 9, 2008, a radio adaptation of the novel was broadcast on BBC Radio 3, starring Robert Lonsdale as Paul Bäumer and Shannon Graney as Katczinsky. Its screenplay was written by Dave Sheasby, and the show was directed by David Hunter.
Theatre.
In 2009, prior to a UK Tour, Nottingham Playhouse commissioned a play of the book by Robin Kingsland.
Audiobook.
In 2010, Hachette Audio UK published an audiobook adaptation of the novel, narrated by Tom Lawrence. It was well received by critics and listeners.

</doc>
<doc id="2154" url="https://en.wikipedia.org/wiki?curid=2154" title="African Americans">
African Americans

African Americans (also referred to as Black Americans or Afro-Americans) are an ethnic group of Americans (citizens or residents of the United States) with total or partial ancestry from any of the Black racial groups of Africa. The term may also be used to include only those individuals who are descended from enslaved Africans. As a compound adjective the term is usually hyphenated as African-American.
African Americans constitute the third largest racial and ethnic group in the United States (after White Americans and Hispanic and Latino Americans). Most African Americans are of West and Central African descent and are descendants of enslaved blacks within the boundaries of the present United States. On average, African Americans are of 78 percent West African, 19 percent European and 3 percent Native American heritage, with large variation between individuals. Immigrants from some African, Caribbean, Central American, and South American nations and their descendants may or may not also self-identify with the term.
African-American history starts in the 16th century, with Africans forcibly taken as slaves to Spanish America, and in the 17th century with African slaves taken to English colonies in North America. After the founding of the United States, black people continued to be enslaved, with four million denied freedom from bondage prior to the Civil War. Believed to be inferior to white people, they were treated as second-class citizens. The Naturalization Act of 1790 limited U.S. citizenship to whites only, and only white men of property could vote. These circumstances were changed by Reconstruction, development of the black community, participation in the great military conflicts of the United States, the elimination of racial segregation, and the Civil Rights Movement which sought political and social freedom. In 2008, Barack Obama became the first African American to be elected president of the United States.
History.
Slavery era.
The first African slaves arrived in the present-day United States as part of the San Miguel de Gualdape colony (most likely located in the Winyah Bay area of present-day South Carolina), founded by Spanish explorer Lucas Vázquez de Ayllón in 1526. The ill-fated colony was almost immediately disrupted by a fight over leadership, during which the slaves revolted and fled the colony to seek refuge among local Native Americans. De Ayllón and many of the colonists died shortly afterwards of an epidemic and the colony was abandoned. The settlers and the slaves who had not escaped, returned to Haiti, whence they had come.
The first recorded Africans in British North America (including most of the future United States) were "20 and odd negroes" who came to Jamestown, Virginia via Cape Comfort in August 1619 as indentured servants. As English settlers died from harsh conditions, more and more Africans were brought to work as laborers. Typically, young men or women would sign a contract of indenture in exchange for transportation to the New World. The landowner received 50 acres of land from the state (headrights) for each servant purchased (around £6 per person to 9 months income in the 17th century) from a ship's captain. An indentured servant (who could be white or black) would work for several years (usually four to seven) without wages. The status of indentured servants in early Virginia and Maryland was similar to slavery. Servants could be bought, sold, or leased and they could be physically beaten for disobedience or running away. Unlike slaves, they were freed after their term of service expired or was bought out, their children did not inherit their status, and on their release from contract they received "a year's provision of corn, double apparel, tools necessary" and a small cash payment called "freedom dues".
Africans could legally raise crops and cattle to purchase their freedom. They raised families, married other Africans and sometimes intermarried with Native Americans or English settlers. By the 1640s and 1650s, several African families owned farms around Jamestown and some became wealthy by colonial standards and purchased indentured servants of their own. In 1640, the Virginia General Court recorded the earliest documentation of lifetime slavery when they sentenced John Punch, a Negro, to lifetime servitude under his master Hugh Gwyn for running away. One of Dutch African arrivals, Anthony Johnson, would later own one of the first black "slaves", John Casor, resulting from the court ruling of a civil case.
The popular conception of a race-based slave system did not fully develop until the 18th century. The Dutch West India Company introduced slavery in 1625 with the importation of eleven black slaves into New Amsterdam (present-day New York City). All the colony's slaves, however, were freed upon its surrender to the British. Massachusetts was the first British colony to legally recognize slavery in 1641. In 1662 Virginia passed a law that children of enslaved women (who were of African descent and thus foreigners) took the status of the mother, rather than that of the father, as under English common law. This principle was called "partus sequitur ventrum". By an act of 1699, the colony ordered all free blacks deported, virtually defining as slaves all persons of African descent who remained in the colony. In 1670 the colonial assembly passed a law prohibiting free and baptized negroes (and Indians) from purchasing Christians (in this act meaning English or European whites) but allowing them to buy persons "of their owne nation".
The earliest African-American congregations and churches were organized before 1800 in both northern and southern cities following the Great Awakening. By 1775, Africans made up 20% of the population in the American colonies, which made them the second largest ethnic group after the English. During the 1770s, Africans, both enslaved and free, helped rebellious English colonists secure American Independence by defeating the British in the American Revolution. Africans and Englishmen fought side by side and were fully integrated.
Blacks played a role in both sides in the American Revolution. Activists in the Patriot cause included James Armistead, Prince Whipple and Oliver Cromwell. The slaves not only constituted a large investment (slaves were valuable), they produced America's most valuable product and export: cotton. They not only helped build the U.S. Capitol, they built the White House and other District of Columbia buildings. (Washington was a slave trading center.) Similar building projects existed in slaveholding states. In 1863, during the American Civil War, President Abraham Lincoln signed the Emancipation Proclamation. The proclamation declared that all slaves in Confederate-held territory were free. Advancing Union troops enforced the proclamation with Texas being the last state to be emancipated, in 1865. Slavery in Union-held Confederate territory continued, at least on paper, until the passage of the Thirteenth Amendment in 1865.
Reconstruction and Jim Crow.
[[File:Jesse Owens3.jpg|thumb|150px|Jesse Owens shook racial stereotypes both with Nazis and
segregationists in the USA at the 1936 Berlin Olympics.]]
African Americans quickly set up congregations for themselves, as well as schools and community/civic associations, to have space away from white control or oversight. While the post-war Reconstruction era was initially a time of progress for African Americans, that period ended in 1876. By the late 1890s, Southern states enacted Jim Crow laws to enforce racial segregation and disenfranchisement. Most African Americans obeyed the Jim Crow laws, in order to avoid racially motivated violence. To maintain self-esteem and dignity, African Americans such as Anthony Overton and Mary McLeod Bethune continued to build their own schools, churches, banks, social clubs, and other businesses.
In the last decade of the 19th century, racially discriminatory laws and racial violence aimed at African Americans began to mushroom in the United States. These discriminatory acts included racial segregation—upheld by the United States Supreme Court decision in Plessy v. Ferguson in 1896—which was legally mandated by southern states and nationwide at the local level of government, voter suppression or disenfranchisement in the southern states, denial of economic opportunity or resources nationwide, and private acts of violence and mass racial violence aimed at African Americans unhindered or encouraged by government authorities.
Great Migration and Civil Rights Movement.
The desperate conditions of African Americans in the South that sparked the Great Migration of the early 20th century, combined with a growing African-American community in the Northern United States, led to a movement to fight violence and discrimination against African Americans that, like abolitionism before it, crossed racial lines. The Civil Rights Movement from 1954 to 1968 was directed at abolishing racial discrimination against African Americans, particularly in the Southern United States. The March on Washington for Jobs and Freedom and the conditions which brought it into being are credited with putting pressure on President John F. Kennedy and Lyndon B. Johnson.
Johnson put his support behind passage of the Civil Rights Act of 1964 that banned discrimination in public accommodations, employment, and labor unions, and the Voting Rights Act of 1965, which expanded federal authority over states to ensure black political participation through protection of voter registration and elections. By 1966, the emergence of the Black Power movement, which lasted from 1966 to 1975, expanded upon the aims of the Civil Rights Movement to include economic and political self-sufficiency, and freedom from white authority.
During the postwar period, many African Americans continued to be economically disadvantaged relative to other Americans. Average black income stood at 54 percent of that of white workers in 1947, and 55 percent in 1962. In 1959, median family income for whites was $5,600, compared with $2,900 for nonwhite families. In 1965, 43 percent of all black families fell into the poverty bracket, earning under $3,000 a year. The Sixties saw improvements in the social and economic conditions of many black Americans.
From 1965 to 1969, black family income rose from 54 to 60 percent of white family income. In 1968, 23 percent of black families earned under $3,000 a year, compared with 41 percent in 1960. In 1965, 19 percent of black Americans had incomes equal to the national median, a proportion that rose to 27 percent by 1967. In 1960, the median level of education for blacks had been 10.8 years, and by the late Sixties the figure rose to 12.2 years, half a year behind the median for whites.
Post-Civil Rights era.
Politically and economically, African Americans have made substantial strides during the post-civil rights era. In 1989, Douglas Wilder became the first African American elected governor in U.S. history. Clarence Thomas became the second African-American Supreme Court Justice. In 1992 Carol Moseley-Braun of Illinois became the first African-American woman elected to the U.S. Senate. There were 8,936 black officeholders in the United States in 2000, showing a net increase of 7,467 since 1970. In 2001 there were 484 black mayors.
In 2005, the number of Africans immigrating to the United States, in a single year, surpassed the peak number who were involuntarily brought to the United States during the Atlantic Slave Trade. On November 4, 2008, Democratic Senator Barack Obama defeated Republican Senator John McCain to become the first African American to be elected President. At least 95 percent of African-American voters voted for Obama. He also received overwhelming support from young and educated whites, a majority of Asians, Hispanics, and Native Americans picking up a number of new states in the Democratic electoral column. Obama lost the overall white vote, although he won a larger proportion of white votes than any previous nonincumbent Democratic presidential candidate since Jimmy Carter. Four years later, Obama was reelected president by a similar margin on November 6, 2012.
Demographics.
In 1790, when the first U.S. Census was taken, Africans (including slaves and free people) numbered about 760,000—about 19.3% of the population. In 1860, at the start of the Civil War, the African-American population had increased to 4.4 million, but the percentage rate dropped to 14% of the overall population of the country. The vast majority were slaves, with only 488,000 counted as "freemen". By 1900, the black population had doubled and reached 8.8 million.
In 1910, about 90% of African Americans lived in the South. Large numbers began migrating north looking for better job opportunities and living conditions, and to escape Jim Crow laws and racial violence. The Great Migration, as it was called, spanned the 1890s to the 1970s. From 1916 through the 1960s, more than 6 million black people moved north. But in the 1970s and 1980s, that trend reversed, with more African Americans moving south to the Sun Belt than leaving it.
The following table of the African-American population in the United States over time shows that the African-American population, as a percentage of the total population, declined until 1930 and has been rising since then.
By 1990, the African-American population reached about 30 million and represented 12% of the U.S. population, roughly the same proportion as in 1900.
At the time of the 2000 Census, 54.8% of African Americans lived in the South. In that year, 17.6% of African Americans lived in the Northeast and 18.7% in the Midwest, while only 8.9% lived in the western states. The west does have a sizable black population in certain areas, however. California, the nation's most populous state, has the fifth largest African-American population, only behind New York, Texas, Georgia, and Florida. According to the 2000 Census, approximately 2.05% of African Americans identified as Hispanic or Latino in origin, many of whom may be of Brazilian, Puerto Rican, Dominican, Cuban, Haitian, or other Latin American descent. The only self-reported "ancestral" groups larger than African Americans are the Irish and Germans. Because many African Americans trace their ancestry to colonial American origins, some simply self-identify as "American".
According to the 2010 US Census, nearly 3% of people who self-identified as black had recent ancestors who immigrated from another country. Self-reported non-Hispanic black immigrants from the Caribbean, mostly from Jamaica and Haiti, represented 0.9% of the US population, at 2.6 million. Self-reported black immigrants from Sub-Saharan Africa also represented 0.9%, at about 2.8 million. Additionally, self-identified Black Hispanics represented 0.4% of the United States population, at about 1.2 million people, largely found within the Puerto Rican and Dominican communities. Self-reported black immigrants hailing from other countries in the Americas, such as Brazil and Canada, as well as several European countries, represented less than 0.1% of the population. Mixed-Race Hispanic and non-Hispanic Americans who identified as being part black, represented 0.9% of the population. Of the 12.6% of United States residents who identified as black, around 10.3% were "native black American" or ethnic African Americans, who are direct descendants of West/Central Africans brought to the U.S. as slaves. These individuals make up well over 80% of all blacks in the country. When including people of mixed-race origin, about 13.5% of the US population self-identified as black or "mixed with black". However, according to the U.S. census bureau, evidence from the 2000 Census indicates that many African and Caribbean immigrant ethnic groups do not identify as "Black, African Am., or Negro". Instead, they wrote in their own respective ethnic groups in the "Some Other Race" write-in entry. As a result, the census bureau devised a new, separate "African American" ethnic group category in 2010 for ethnic African Americans. Following lobbying led by the Arab American Institute, a national organization representing Arab Americans, the census bureau also announced in 2014 that it may establish an additional new ethnic category for populations from the Middle East, North Africa and the Arab world.
U.S. cities.
Almost 58% of African Americans lived in metropolitan areas in 2000. With over 2 million black residents, New York City had the largest black urban population in the United States in 2000, overall the city has a 28% black population. Chicago has the second largest black population, with almost 1.6 million African Americans in its metropolitan area, representing about 18 percent of the total metropolitan population.
After 100 years of African-Americans leaving the south in large numbers seeking better opportunities in the west and north (Great Migration), there is now a reverse trend. A growing percentage of African-Americans from the west and north are migrating to the southern region of the U.S. for economic and cultural reasons. New York City, Chicago, and Los Angeles have the highest decline in African Americans, while Atlanta, Dallas, and Houston have the highest increase respectively.
Among cities of 100,000 or more, Detroit, Michigan had the highest percentage of black residents of any U.S. city in 2010, with 82%. Other large cities with African-American majorities include Jackson, Mississippi (79.4%), Miami Gardens, Florida (76.3%), Baltimore, Maryland (63%), Birmingham, Alabama (62.5%), Memphis, Tennessee (61%), New Orleans, Louisiana (60%), Montgomery, Alabama (56.6%), Flint, Michigan (56.6%), Savannah, Georgia (55.0%), Augusta, Georgia (54.7%), Atlanta, Georgia (54%, see African Americans in Atlanta), Cleveland, Ohio (53.3%), Newark, New Jersey (52.35%), Washington, D.C. (50.7%), Richmond, Virginia (50.6%), Mobile, Alabama (50.6%), Baton Rouge, Louisiana (50.4%), and Shreveport, Louisiana (50.4%).
The nation's most affluent community with an African-American majority resides in View Park–Windsor Hills, California with an annual median income of $159,618. Other affluent predominately African-American communities include Prince George's County in Maryland (namely Mitchellville, Woodmore, and Upper Marlboro), Dekalb County in Georgia, Charles City County in Virginia, Baldwin Hills in California, Hillcrest and Uniondale in New York, and Cedar Hill, DeSoto, and Missouri City in Texas. Queens County, New York is the only county with a population of 65,000 or more where African Americans have a higher median household income than White Americans.
Seatack is currently the oldest African-American community in the United States. It survives today with a vibrant and active civic community.
Education.
By 2012, African Americans had advanced greatly in education attainment. They still lagged overall compared to white or Asian Americans but surpassed other ethnic minorities, with 19 percent earning bachelor's degrees and 6 percent earning advanced degrees. Between 1995 and 2009, freshmen college enrollment for African Americans increased by 73 percent and only 15 percent for whites. Predominantly black schools for kindergarten through twelfth grade students were common throughout the U.S. before the 1970s. By 1972, however, desegregation efforts meant that only 25% of Black students were in schools with more than 90% non-white students. However, since then, a trend towards re-segregation affected communities across the country: by 2011, 2.9 million African-American students were in such overwhelmingly minority schools, including 53% of Black students in school districts that were formerly under desegregation orders.
Historically black colleges and universities (HBCUs), which were originally set up when segregated colleges did not admit African Americans, continue to thrive and educate students of all races today. The majority of HBCUs were established in the southeastern United States, Alabama has the most HBCUs of any state.
As late as 1947, about one third of African Americans over 65 were considered to lack the literacy to read and write their own names. By 1969, illiteracy as it had been traditionally defined, had been largely eradicated among younger African Americans.
US Census surveys showed that by 1998, 89 percent of African Americans aged 25 to 29 had completed a high-school education, less than whites or Asians, but more than Hispanics. On many college entrance, standardized tests and grades, African Americans have historically lagged behind whites, but some studies suggest that the achievement gap has been closing. Many policy makers have proposed that this gap can and will be eliminated through policies such as affirmative action, desegregation, and multiculturalism.
The average high school graduation rate of blacks in the United States has steadily increased to 71% in 2013. Separating this statistic into component parts shows it varies greatly depending upon the state and the school district examined. 38% of black males graduated in the state of New York but in Maine 97% graduated and exceeded the white male graduation rate by 11 percentage points. In much of the southeastern United States and some parts of the southwestern United States the graduation rate of white males was in fact below 70% such as in Florida where a 62% of white males graduated high school. Examining specific school districts paints an even more complex picture. In the Detroit school district the graduation rate of black males was 20% but 7% for white males. In the New York City school district 28% of black males graduate high school compared to 57% of white males. In Newark County 76% of black males graduated compared to 67% for white males.
In Chicago, Marva Collins, an African-American educator, created a low cost private school specifically for the purpose of teaching low-income African-American children whom the public school system had labeled as being "learning disabled". One article about Marva Collins' school stated,
Working with students having the worst of backgrounds, those who were working far below grade level, and even those who had been labeled as 'unteachable,' Marva was able to overcome the obstacles. News of third grade students reading at ninth grade level, four-year-olds learning to read in only a few months, outstanding test scores, disappearance of behavioral problems, second-graders studying Shakespeare, and other incredible reports, astounded the public. During the 2006–2007 school year, Collins' school charged $5,500 for tuition, and parents said that the school did a much better job than the Chicago public school system. Meanwhile, during the 2007–2008 year, Chicago public school officials claimed that their budget of $11,300 per student was not enough.
Economic status.
[[File:US Homeownership by Race 2009.png|thumb|450px|US Homeownership by Race 2009|The US homeownership rate according to race.
Economically, African Americans have benefited from the advances made during the Civil Rights era, particularly among the educated, but not without the lingering effects of historical marginalization when considered as a whole. The racial disparity in poverty rates has narrowed. The black middle class has grown substantially. In 2010, 45% of African Americans owned their homes, compared to 67% of all Americans. The poverty rate among African Americans has decreased from 26.5% in 1998 to 24.7% in 2004, compared to 12.7% for all Americans.
African Americans have a combined buying power of over $892 billion currently and likely over $1.1 trillion by 2012. In 2002, African American-owned businesses accounted for 1.2 million of the US's 23 million businesses. African American-owned businesses account for approximately 2 million US businesses. Black-owned businesses experienced the largest growth in number of businesses among minorities from 2002 to 2011.
In 2004, African-American men had the third-highest earnings of American minority groups after Asian Americans and non-Hispanic whites.
Twenty-five percent of blacks had white-collar occupations (management, professional, and related fields) in 2000, compared with 33.6% of Americans overall. In 2001, over half of African-American households of married couples earned $50,000 or more. Although in the same year African Americans were over-represented among the nation's poor, this was directly related to the disproportionate percentage of African-American families headed by single women; such families are collectively poorer, regardless of ethnicity.
In 2006, the median earnings of African-American men was more than black and non-black American women overall, and in all educational levels. At the same time, among American men, income disparities were significant; the median income of African-American men was approximately 76 cents for every dollar of their European American counterparts, although the gap narrowed somewhat with a rise in educational level.
Overall, the median earnings of African-American men were 72 cents for every dollar earned of their Asian American counterparts, and $1.17 for every dollar earned by Hispanic men. On the other hand, by 2006, among American women with post-secondary education, African-American women have made significant advances; the median income of African-American women was more than those of their Asian-, European- and Hispanic American counterparts with at least some college education.
The US public sector is the single most important source of employment for African Americans. During 2008–2010, 21.2% of all Black workers were public employees, compared with 16.3% of non-Black workers. Both before and after the onset of the Great Recession, African Americans were 30% more likely than other workers to be employed in the public sector.
The public sector is also a critical source of decent-paying jobs for Black Americans. For both men and women, the median wage earned by Black employees is significantly higher in the public sector than in other industries.
In 1999, the median income of African-American families was $33,255 compared to $53,356 of European Americans. In times of economic hardship for the nation, African Americans suffer disproportionately from job loss and underemployment, with the black underclass being hardest hit. The phrase "last hired and first fired" is reflected in the Bureau of Labor Statistics unemployment figures. Nationwide, the October 2008 unemployment rate for African Americans was 11.1%, while the nationwide rate was 6.5%.
The income gap between black and white families is also significant. In 2005, employed blacks earned 65% of the wages of whites, down from 82% in 1975. "The New York Times" reported in 2006 that in Queens, New York, the median income among African-American families exceeded that of white families, which the newspaper attributed to the growth in the number of two-parent black families. It noted that Queens was the only county with more than 65,000 residents where that was true.
In 2011, it was reported that 72% of black babies were born to unwed mothers. The poverty rate among single-parent black families was 39.5% in 2005, according to Williams, while it was 9.9% among married-couple black families. Among white families, the respective rates were 26.4% and 6% in poverty.
Health.
The life expectancy for Black men in 2008 was 70.8 years. Life expectancy for Black women was 77.5 years in 2008. In 1900, when information on Black life expectancy started being collated, a Black man could expect to live to 32.5 years and a Black woman 33.5 years. In 1900, White men lived an average of 46.3 years and White women lived an average of 48.3 years. African-American life expectancy at birth is persistently five to seven years lower than European Americans.
Black people have higher rates of obesity, diabetes and hypertension than the US average. For adult Black men, the rate of obesity was 31.6% in 2010. For adult Black women, the rate of obesity was 41.2% in 2010. African Americans have higher rates of mortality than does any other racial or ethnic group for 8 of the top 10 causes of death. The cancer incidence rate among African Americans is 10% higher than among European Americans.
Violence has an impact upon African-American life expectancy. A report from the U.S. Department of Justice states "In 2005, homicide victimization rates for blacks were 6 times higher than the rates for whites". The report also found that "94% of black victims were killed by blacks."
AIDS is one of the top three causes of death for African-American men aged 25–54 and for African-American women aged 35–44 years. In the United States, African Americans make up about 48% of the total HIV-positive population and make up more than half of new HIV cases. The main route of transmission for women is through unprotected heterosexual sex. African-American women are 19 times more likely to contract HIV than other women.
Washington, D.C. has the nation's highest rate of HIV/AIDS infection, at 3%. This rate is comparable to what is seen in West Africa, and is considered a severe epidemic.</ref> Dr. Ray Martins, Chief Medical Officer at the Whitman-Walker Clinic, the largest provider of HIV care in Washington D.C., estimated that the actual underlying percent with HIV/AIDS in the city is "closer to five percent".
Sexuality.
According to a Gallup survey conducted from June to September 2012, it found that 4.6 percent of Black or African Americans self identify as LGBT; this is greater than the estimated 3.4 percent of American adults that self identify as LGBT in the total population.
Religion.
The majority of African Americans are Protestant, many of whom follow the historically black churches. The term Black church refers to churches which minister to predominantly African-American congregations. Black congregations were first established by freed slaves at the end of the 17th century, and later when slavery was abolished more African Americans were allowed to create a unique form of Christianity that was culturally influenced by African spiritual traditions.
According to a 2007 survey, more than half of the African-American population are part of the historically black churches. The largest Protestant denomination among African Americans are the Baptists, distributed mainly in four denominations, the largest being the National Baptist Convention, USA and the National Baptist Convention of America. The second largest are the Methodists, the largest sects are the African Methodist Episcopal Church and the African Methodist Episcopal Zion Church.
Pentecostals are distributed among several different religious bodies, with the Church of God in Christ as the largest among them by far. About 16% of African-American Christians are members of white Protestant communions, these denominations (which include the United Church of Christ) mostly have a 2 to 3% African-American membership. There are also large numbers of Roman Catholics, constituting 5% of the African-American population. Of the total number of Jehovah's Witnesses, 22% are black.
Some African Americans follow Islam. Historically, between 15 and 30% of enslaved Africans brought to the Americas were Muslims, but most of these Africans were converted to Christianity during the era of American slavery. During the twentieth century, some African Americans converted to Islam, mainly through the influence of black nationalist groups that preached with distinctive Islamic practices; including the Moorish Science Temple of America, and the largest organization, the Nation of Islam, founded in the 1930s, which attracted at least 20,000 people by 1963, prominent members included activist Malcolm X and boxer Muhammad Ali.
Malcolm X is considered the first person to start the movement among African Americans towards mainstream Islam, after he left the Nation and made the pilgrimage to Mecca. In 1975, Warith Deen Mohammed, the son of Elijah Muhammad took control of the Nation after his father's death and guided the majority of its members to orthodox Islam. However, a few members rejected these changes, in particular Louis Farrakhan, who revived the Nation of Islam in 1978 based on its original teachings.
African-American Muslims constitute 20% of the total U.S. Muslim population, the majority are Sunni or orthodox Muslims, some of these identify under the community of W. Deen Mohammed. The Nation of Islam led by Louis Farrakhan has a membership ranging from 20,000–50,000 members.
There are relatively few African-American Jews; estimates of their number range from 20,000 to 200,000. Most of these Jews are part of mainstream groups such as the Reform, Conservative, or Orthodox branches of Judaism; although there are significant numbers of people who are part of non-mainstream Jewish groups, largely the Black Hebrew Israelites, whose beliefs include the claim that African Americans are descended from the Biblical Israelites.
Confirmed atheists are less than one half of one-percent, similar to numbers for Hispanics.
Language.
African American Vernacular English (AAVE) is a variety (dialect, ethnolect, and sociolect) of American English, commonly spoken by urban working-class and largely bi-dialectal middle-class African Americans. Non-linguists sometimes call it "Ebonics" (a term that also has other meanings and connotations).
African American Vernacular English evolved during the antebellum period through interaction between speakers of 16th and 17th century English of Great Britain and Ireland and various West African languages. As a result, the variety shares parts of its grammar and phonology with the Southern American English dialect. Where African American Vernacular English differs from Standard American English (SAE) is in certain pronunciation characteristics, tense usage and grammatical structures that were derived from West African languages, particularly those belonging to the Niger-Congo family.
Virtually all habitual speakers of African American Vernacular English can understand and communicate in Standard American English. As with all linguistic forms, AAVE's usage is influenced by various factors, including geographical, educational and socioeconomic background, as well as formality of setting. Additionally, there are many literary uses of this variety of English, particularly in African-American literature.
Some of the new words used by the people include "fleek" which means on point and "throwing shade" which means offending someone..
Genetics.
Y-DNA.
According to a Y-DNA study by Sims et al. (2007), the majority (~60%) of African Americans belong to various subclades of the E3a (E1b1a) paternal haplogroup. This is the most common genetic paternal lineage found today among West/Central African males, and is also a signature of the historical Bantu migrations. The next most frequent Y-DNA haplogroup observed among African Americans is the R1b clade, which around 15% of African Americans carry. This lineage is most common today among Northwestern European males. The remaining African Americans mainly belong to the paternal haplogroup I (~7%), which is also frequent in Northwestern Europe.
mtDNA.
According to an mtDNA study by Salas et al. (2005), the maternal lineages of African Americans are most similar to haplogroups that are today especially common in West Africa (>55%), followed closely by West-Central Africa and Southwestern Africa (<41%). The characteristic West African haplogroups L1b, L2b,c,d, and L3b,d and West-Central African haplogroups L1c and L3e in particular occur at high frequencies among African Americans. As with the paternal DNA of African Americans, contributions from other parts of the continent to their maternal gene pool are insignificant.
Genome-wide studies.
According to a genome-wide study by Bryc et al. (2009), the overall ancestry of African Americans was formed through historic admixture between West/Central Africans (more frequently females) and Europeans (more frequently males). Consequently, African Americans have a genome-wide average of 78.1% West African ancestry and 18.5% European ancestry, with large variation among individuals (ranging from 99% to 1% West African ancestry). The West African ancestral component in African Americans is also primarily affiliated with speakers from the non-Bantu branches of the Niger-Congo (Niger-Kordofanian) family.
Correspondingly, Montinaro et al. (2014) observed that around 50% of the overall ancestry of African Americans traces back to the Niger-Congo-speaking Yoruba of southwestern Nigeria and southern Benin, reflecting the centrality of this West Africa region in the Atlantic Slave Trade. The next most frequent ancestral component found among African Americans was derived from Great Britain, in keeping with historical records. It constitutes a little over 10% of their overall ancestry, and is most similar to the Northwest European ancestral component also carried by Barbadians. Zakharaia et al. (2009) found a similar proportion of Yoruba associated ancestry in their African-American samples, with a minority also drawn from Mandenka and Bantu populations. Additionally, the researchers observed an average European ancestry of 21.9%, again with significant variation between individuals. Bryc et al. (2009) note that populations from other parts of the continent may also constitute adequate proxies for the ancestors of some African-American individuals; namely, ancestral populations from Guinea Bissau, Senegal and Sierra Leone in West Africa and Angola in Southern Africa.
Altogether, genetic studies suggest that African Americans are a multiracial people. According to DNA analysis led in 2006 by Penn State geneticist Mark D. Shriver, around 58 percent of African Americans have at least 12.5% European ancestry (equivalent to one European great-grandparent and his/her forebears), 19.6 percent of African Americans have at least 25% European ancestry (equivalent to one European grandparent and his/her forebears), and 1 percent of African Americans have at least 50% European ancestry (equivalent to one European parent and his/her forebears). According to Shriver, around 5 percent of African Americans also have at least 12.5% Native American ancestry (equivalent to one Native American great-grandparent and his/her forebears).
Traditional names.
African-American names are part of the cultural traditions of African Americans. Prior to the 1950s and 1960s, most African-American names closely resembled those used within European American culture. Babies of that era were generally given a few common names, with children using nicknames to distinguish the various people with the same name. With the rise of 1960s civil rights movement, there was a dramatic increase in names of various origins.
By the 1970s and 1980s, it had become common among African Americans to invent new names for themselves, although many of these invented names took elements from popular existing names. Prefixes such as La/Le, Da/De, Ra/Re and Ja/Je, and suffixes like -ique/iqua, -isha and -aun/-awn are common, as are inventive spellings for common names. The book "Baby Names Now: From Classic to Cool--The Very Last Word on First Names" places the origins of "La" names in African-American culture in New Orleans.
Even with the rise of inventive names, it is still common for African Americans to use biblical, historical, or traditional European names. Daniel, Christopher, Michael, David, James, Joseph, and Matthew were thus among the most frequent names for African-American boys in 2013.
The name LaKeisha is typically considered American in origin, but has elements of it that were drawn from both French and West/Central African roots. Other names like LaTanisha, JaMarcus, DeAndre, and Shaniqua were created in the same way. Punctuation marks are seen more often within African-American names than other American names, such as the names Mo'nique and D'Andre.
Contemporary issues.
African Americans have improved their social and economic standing significantly since the Civil Rights Movement and recent decades have witnessed the expansion of a robust, African -middle class across the United States. Unprecedented access to higher education and employment in addition to representation in the highest levels of American government has been gained by African Americans in the post-civil rights era.
One of the most serious and long standing issues within African-American communities is poverty. Poverty itself is a hardship as it is related to marital stress and dissolution, health problems, low educational attainment, deficits in psychological functioning, and crime. In 2004, 24.7% of African-American families lived below the poverty level. In 2007, the average African-American income was $33,916, compared with $54,920 for whites.
Politics and social issues.
Collectively, African Americans are more involved in the American political process than other minority groups in the United States, indicated by the highest level of voter registration and participation in elections among these groups in 2004. African Americans collectively attain higher levels of education than immigrants to the United States. African Americans also have the highest level of Congressional representation of any minority group in the U.S.
The large majority of African Americans support the Democratic Party. In the 2004 Presidential Election, Democrat John Kerry received 88% of the African-American vote compared to 11% for Republican George W. Bush. Although there is an African-American lobby in foreign policy, it has not had the impact that African-American organizations have had in domestic policy.
Until the New Deal, African Americans were supporters of the Republican Party because it was Republican President Abraham Lincoln who helped in granting freedom to American slaves; at the time, the Republicans and Democrats represented the sectional interests of the North and South, respectively, rather than any specific ideology, and both right and left were represented equally in both parties.
The African-American trend of voting for Democrats can be traced back to the 1930s during the Great Depression, when Franklin D. Roosevelt's New Deal program provided economic relief to African Americans; Roosevelt's New Deal coalition turned the Democratic Party into an organization of the working class and their liberal allies, regardless of region. The African-American vote became even more solidly Democratic when Democratic presidents John F. Kennedy and Lyndon B. Johnson pushed for civil rights legislation during the 1960s. In 1960, nearly a third of African Americans voted for Republican Richard Nixon.
After over 50 years, marriage rates for "all" Americans began to decline while divorce rates and out-of-wedlock births have climbed. These changes have been greatest among African Americans. After more than 70 years of racial parity black marriage rates began to fall behind whites. Single-parent households have become common, and according to US census figures released in January 2010, only 38 percent of black children live with both their parents.
In 2008, Democrats overwhelmingly voted 70% against California Proposition 8, African Americans voted 58% in favor of it while 42% voted against Proposition 8. On May 9, 2012, Barack Obama, the first African-American president, became the first US president to support same-sex marriage. After Obama's endorsement there is a rapid growth in support for same-sex marriage among African Americans. Now 59% of African Americans support same-sex marriage, which is higher than support among the national average (53%) and white Americans (50%).
Polls in North Carolina, Pennsylvania, Missouri, Maryland, Ohio, Florida, and Nevada have also shown an increase in support for same sex marriage among African Americans. On November 6, 2012, Maryland, Maine, and Washington all voted for approve of same-sex marriage, along with Minnesota rejecting a constitutional amendment banning same-sex marriage. Exit polls in Maryland show about 50% of African Americans voted for same-sex marriage, showing a vast evolution among African Americans on the issue and was crucial in helping pass same-sex marriage in Maryland.
Blacks hold far more conservative opinions on abortion, extramarital sex, and raising children out of wedlock than Democrats as a whole. On financial issues, however, African Americans are in line with Democrats, generally supporting a more progressive tax structure to provide more government spending on social services.
Political legacy.
African Americans have fought in every war in the history of the United States.
The gains made by African Americans in the Civil Rights Movement and in the Black Power movement not only obtained certain rights for African Americans, but changed American society in far-reaching and fundamentally important ways. Prior to the 1950s, Black Americans in the South were subject to de jure discrimination, or Jim Crow laws. They were often the victims of extreme cruelty and violence, sometimes resulting in deaths: by the post WWII era, African Americans became increasingly discontented with their long-standing inequality. In the words of Martin Luther King, Jr., African Americans and their supporters challenged the nation to "rise up and live out the true meaning of its creed that all men are created equal ..."
The Civil Rights Movement marked an enormous change in American social, political, economic and civic life. It brought with it boycotts, sit-ins, nonviolent demonstrations and marches, court battles, bombings and other violence; prompted worldwide media coverage and intense public debate; forged enduring civic, economic and religious alliances; and disrupted and realigned the nation's two major political parties.
Over time, it has changed in fundamental ways the manner in which blacks and whites interact with and relate to one another. The movement resulted in the removal of codified, "de jure" racial segregation and discrimination from American life and law, and heavily influenced other groups and movements in struggles for civil rights and social equality within American society, including the Free Speech Movement, the disabled, the women's movement, Native Americans, and migrant workers.
News media and coverage.
Some activists and academics contend that news media coverage of African-American news concerns or dilemmas is inadequate or the news media present distorted images of African Americans. To combat this, Robert L. Johnson founded Black Entertainment Television, a network that targets young African Americans and urban audiences in the United States. Most programming on the network consists of rap and R&B music videos and urban-oriented movies and series. The channel also shows syndicated television series, original programs, and some public affairs programs. On Sunday mornings, BET broadcasts a lineup of network-produced Christian programming; other, non-affiliated Christian programs are also shown during the early morning hours daily. BET is now a global network that reaches 90 million households in the United States, Caribbean, Canada, and the United Kingdom.
In addition to BET there is Centric, which is a spin-off cable television channel of BET, created originally as "BET on Jazz" to showcase jazz music-related programming, especially that of black jazz musicians. Programming has been expanded to include a block of urban programs as well as some R&B, soul, and world music.
TV One is another African-American-oriented network and a direct competitor to BET, targeting African-American adults with a broad range of programming. The network airs original lifestyle and entertainment-oriented shows, movies, fashion and music programming, as well as classic series such as 227, Good Times, Martin, Boston Public and It's Showtime at the Apollo. The network primarily owned by Radio One. Founded and controlled by Catherine Hughes, it is one of the nation's largest radio broadcasting companies and the largest African-American-owned radio broadcasting company in the United States.
Other African-American networks scheduled to launch in 2009 are the Black Television News Channel founded by former Congressman J. C. Watts and Better Black Television founded by Percy Miller. In June 2009, NBC News launched a new website named The Grio in partnership with the production team that created the black documentary film "Meeting David Wilson". It is the first African-American video news site that focuses on underrepresented stories in existing national news. The Grio consists of a broad spectrum of original video packages, news articles, and contributor blogs on topics including breaking news, politics, health, business, entertainment and Black History.
Culture in the United States.
From their earliest presence in North America, African Americans have contributed literature, art, agricultural skills, cuisine, clothing styles, music, language, and social and technological innovation to American culture. The cultivation and use of many agricultural products in the United States, such as yams, peanuts, rice, okra, sorghum, grits, watermelon, indigo dyes, and cotton, can be traced to West African and African-American influences. Notable examples include George Washington Carver, who created 300 products from peanuts, 118 products from sweet potatoes, and 75 products from pecans; and George Crum, who invented the potato chip in 1853. Soul food is a variety of cuisine popular among African Americans. It is closely related to the cuisine of the Southern United States. The descriptive terminology may have originated in the mid-1960s, when "soul" was a common definer used to describe African-American culture (for example, soul music). African Americans were the first peoples in the United States to make fried chicken, along with Scottish immigrants to the South. Although the Scottish had been frying chicken before they emigrated, they lacked the spices and flavor that African Americans had used when preparing the meal. The Scottish American settlers therefore adopted the African-American method of seasoning chicken. However, fried chicken was generally a rare meal in the African-American community, and was usually reserved for special events or celebrations.
Music.
African-American music is one of the most pervasive African-American cultural influences in the United States today and is among the most dominant in mainstream popular music. Hip hop, R&B, funk, rock and roll, soul, blues, and other contemporary American musical forms originated in black communities and evolved from other black forms of music, including blues, doo-wop, barbershop, ragtime, bluegrass, jazz, and gospel music.
African-American-derived musical forms have also influenced and been incorporated into virtually every other popular musical genre in the world, including country and techno. African-American genres are the most important ethnic vernacular tradition in America, as they have developed independent of African traditions from which they arise more so than any other immigrant groups, including Europeans; make up the broadest and longest lasting range of styles in America; and have, historically, been more influential, interculturally, geographically, and economically, than other American vernacular traditions.
African Americans have also had an important role in American dance. Bill T. Jones, a prominent modern choreographer and dancer, has included historical African-American themes in his work, particularly in the piece "Last Supper at Uncle Tom's Cabin/The Promised Land". Likewise, Alvin Ailey's artistic work, including his "Revelations" based on his experience growing up as an African American in the South during the 1930s, has had a significant influence on modern dance. Another form of dance, Stepping, is an African-American tradition whose performance and competition has been formalized through the traditionally black fraternities and sororities at universities.
Literature and academics.
Many African-American authors have written stories, poems, and essays influenced by their experiences as African Americans. African-American literature is a major genre in American literature. Famous examples include Langston Hughes, James Baldwin, Richard Wright, Zora Neale Hurston, Ralph Ellison, Nobel Prize winner Toni Morrison, and Maya Angelou.
African-American inventors have created many widely used devices in the world and have contributed to international innovation. Norbert Rillieux created the technique for converting sugar cane juice into white sugar crystals. Moreover, Rillieux left Louisiana in 1854 and went to France, where he spent ten years working with the Champollions deciphering Egyptian hieroglyphics from the Rosetta Stone. Most slave inventors were nameless, such as the slave owned by the Confederate President Jefferson Davis who designed the ship propeller used by the Confederate navy.
By 1913 over 1,000 inventions were patented by black Americans. Among the most notable inventors were Jan Matzeliger, who developed the first machine to mass-produce shoes, and Elijah McCoy, who invented automatic lubrication devices for steam engines. Granville Woods had 35 patents to improve electric railway systems, including the first system to allow moving trains to communicate. Garrett A. Morgan developed the first automatic traffic signal and gas mask.
Lewis Howard Latimer invented an improvement for the incandescent light bulb. More recent inventors include Frederick McKinley Jones, who invented the movable refrigeration unit for food transport in trucks and trains. Lloyd Quarterman worked with six other black scientists on the creation of the atomic bomb (code named the Manhattan Project.) Quarterman also helped develop the first nuclear reactor, which was used in the atomically powered submarine called the Nautilus.
A few other notable examples include the first successful open heart surgery, performed by Dr. Daniel Hale Williams, and the air conditioner, patented by Frederick McKinley Jones. Dr. Mark Dean holds three of the original nine patents on the computer on which all PCs are based. More current contributors include Otis Boykin, whose inventions included several novel methods for manufacturing electrical components that found use in applications such as guided missile systems and computers, and Colonel Frederick Gregory, who was not only the first black astronaut pilot but the person who redesigned the cockpits for the last three space shuttles. Gregory was also on the team that pioneered the microwave instrumentation landing system.
Terminology.
The term "African American" carries important political overtones. Earlier terms used to describe Americans of African ancestry referred more to skin color than to ancestry, and were conferred upon the group by colonists and Americans of European ancestry; people with dark skins were considered inferior in fact and in law. The terms (such as "colored", "person of color", or "negro") were included in the wording of various laws and legal decisions which some thought were being used as tools of white supremacy and oppression. There developed among blacks in America a growing desire for a term of self-identification of their own choosing.
In the 1980s the term "African American" was advanced on the model of, for example, German-American or Irish-American to give descendants of American slaves and other American blacks who lived through the slavery era a heritage and a cultural base. The term was popularized in black communities around the country via word of mouth and ultimately received mainstream use after Jesse Jackson publicly used the term in front of a national audience in 1988. Subsequently, major media outlets adopted its use.
Surveys show that the majority of Black Americans have no preference for "African American" versus "Black", although they have a slight preference for "Black" in personal settings and "African American" in more formal settings.
Many African Americans expressed a preference for the term, because it was formed in the same way as the terms for the many other ethnic groups currently living in the nation. Some argued further that, because of the historical circumstances surrounding the capture, enslavement and systematic attempts to de-Africanize blacks in the United States under chattel slavery, most African Americans are unable to trace their ancestry to a specific African nation; hence, the entire continent serves as a geographic marker.
The term embraces pan-Africanism as earlier enunciated by prominent African thinkers such as Marcus Garvey, W. E. B. Du Bois and George Padmore. The term "Afro-Usonian", and variations of such, are more rarely used.
Identity.
Since 1977, in an attempt to keep up with changing social opinion, the United States government has officially classified black people (revised to "black" or "African American" in 1997) as "having origins in any of the black racial groups of Africa." Other federal offices, such as the United States Census Bureau, adhere to the Office of Management and Budget standards on race in its data collection and tabulations efforts. In preparation for the United States 2010 Census, a marketing and outreach plan, called "2010 Census Integrated Communications Campaign Plan" (ICC) recognized and defined African Americans as black people born in the United States. From the ICC perspective, African Americans are one of three groups of black people in the United States.
The ICC plan was to reach the three groups by acknowledging that each group has its own sense of community that is based on geography and ethnicity. The best way to market the census process toward any of the three groups is to reach them through their own unique communication channels and not treat the entire black population of the U.S. as though they are all African Americans with a single ethnic and geographical background. The U.S. Department of Justice Federal Bureau of Investigation categorizes black or African-American people as "A person having origins in any of the black racial groups of Africa" through racial categories used in the UCR Program adopted from the Statistical Policy Handbook (1978) and published by the Office of Federal Statistical Policy and Standards, U.S. Department of Commerce, derived from the 1977 Office of Management and Budget classification.
Admixture.
Historically, "race mixing" between black and white people was taboo in the United States. So-called anti-miscegenation laws, barring blacks and whites from marrying or having sex, were established in colonial America as early as 1691, and endured in many Southern states until the Supreme Court ruled them unconstitional in "Loving v. Virginia" (1967). The taboo among American whites surrounding white-black relations is a historical consequence of the oppression and racial segregation of African Americans. Historian David Brion Davis notes the racial mixing that occurred during slavery was frequently attributed by the planter class to the "lower-class white males" but Davis concludes that "there is abundant evidence that many slaveowners, sons of slaveowners, and overseers took black mistresses or in effect raped the wives and daughters of slave families." A famous example was Thomas Jefferson's mistress, Sally Hemings.
Harvard University historian Henry Louis Gates, Jr. wrote in 2009 that "African Americans[...] are a racially mixed or mulatto people—deeply and overwhelmingly so" (see genetics). After the Emancipation Proclamation, Chinese American men married African-American women in high proportions to their total marriage numbers due to few Chinese American women being in the United States. African slaves and their descendants have also had a history of cultural exchange and intermarriage with Native Americans, although they did not necessarily retain social, cultural or linguistic ties to Native peoples. There are also increasing intermarriages and offspring between non-Hispanic blacks and Hispanics of any race, especially between Puerto Ricans and African Americans (American-born blacks). According to author M.M. Drymon, many African Americans identify as having Scots-Irish ancestry.
Racially mixed marriages have become increasingly accepted in the United States since the Civil Rights movement and up to the present day. Approval in national opinion polls have risen from 36% in 1978, to 48% in 1991, 65% in 2002, 77% in 2007. A Gallup poll conducted in 2013 found that 84% of whites and 96% of blacks approved of interracial marriage, and 87% overall.
The African-American experience.
In her book "The End of Blackness", as well as in an essay on the liberal website "Salon", author Debra Dickerson has argued that the term "black" should refer strictly to the descendants of Africans who were brought to America as slaves, and not to the sons and daughters of black immigrants who lack that ancestry. In her opinion, President Barack Obama, who is the son of a Kenyan immigrant, although technically black, is not African-American. She makes the argument that grouping all people of African descent together regardless of their unique ancestral circumstances would inevitably deny the lingering effects of slavery within the American community of slave descendants, in addition to denying black immigrants recognition of their own unique ancestral backgrounds. "Lumping us all together", Dickerson wrote, "erases the significance of slavery and continuing racism while giving the appearance of progress".
Similar viewpoints have been expressed by Stanley Crouch in a "New York Daily News" piece, Charles Steele, Jr. of the Southern Christian Leadership Conference and African-American columnist David Ehrenstein of the "LA Times" who accused white liberals of flocking to blacks who were "Magic Negros", a term that refers to a black person with no past who simply appears to assist the mainstream white (as cultural protagonists/drivers) agenda. Ehrenstein went on to say "He's there to assuage white 'guilt' they feel over the role of slavery and racial segregation in American history."
Former Secretary of State Condoleezza Rice (who was famously mistaken for a "recent American immigrant" by French President Nicolas Sarkozy), said "descendants of slaves did not get much of a head start, and I think you continue to see some of the effects of that." She has also rejected an immigrant designation for African Americans and instead prefers the term "black" or "white" to denote the African and European U.S. founding populations.
Terms no longer in common use.
Before the independence of the Thirteen Colonies until the abolition of slavery in 1865, an African-American slave was commonly known as a "negro". "Free negro" was the legal status in the territory of an African-American person who was not a slave. The term "Colored" later also began to be used until the second quarter of the 20th century, when it was considered outmoded and generally gave way again to the exclusive use of "negro". By the 1940s, the term was commonly capitalized ("Negro"); but by the mid-1960s, it was considered disparaging. By the end of the 20th century, "negro" had come to be considered inappropriate and was rarely used and perceived as a pejorative. The term is rarely used by younger black people, but remained in use by many older African Americans who had grown up with the term, particularly in the southern U.S. The term remains in use in some contexts, such as the United Negro College Fund, an American philanthropic organization that funds scholarships for black students and general scholarship funds for 39 private historically black colleges and universities, as well as in Latin America where Spanish and Portuguese are spoken. Pronounced slightly differently, it is the word for the color "black", and is rarely perceived as a pejorative.
There are many other deliberately insulting terms. Many were in common use (e.g., "nigger"), but had become unacceptable in normal discourse before the end of the 20th century. One exception is the use, among the black community, of the slur "nigger" rendered as "nigga", representing the pronunciation of the word in African American Vernacular English. This usage has been popularized by the rap and hip-hop music cultures and is used as part of an in-group lexicon and speech. It is not necessarily derogatory and, when used among black people, the word is often used to mean "homie" or "friend".
Acceptance of intra-group usage of the word "nigga" is still debated, although it has established a foothold among younger generations. The NAACP denounces the use of both "nigga" and "nigger". Mixed-race usage of "nigga" is still considered taboo, particularly if the speaker is white. However, trends indicate that usage of the term in intragroup settings is increasing even among white youth due to the popularity of rap and hip hop culture.
See also.
Diaspora:
Lists:

</doc>
<doc id="2161" url="https://en.wikipedia.org/wiki?curid=2161" title="Artistic License">
Artistic License

The Artistic License (version 1.0) is a software license used for certain free and open source software packages, most notably the standard implementation of the Perl programming language and most CPAN modules, which are dual-licensed under the Artistic License and the GNU General Public License (GPL). 
History.
Artistic License 1.0.
The original Artistic License was written by Larry Wall. The name of the license is a reference to the concept of artistic license.
Whether or not the original Artistic License is a free software license is largely unsettled. It was criticized by the Free Software Foundation as being "too vague; some passages are too clever for their own good, and their meaning is not clear." The Free Software Foundation has also explicitly called the original Artistic License a non-free license. The FSF recommended that the license not be used on its own, but approved the common AL/GPL dual-licensing approach for Perl projects.
In response to this, Bradley Kuhn, who later worked for the Free Software Foundation, made a minimal redraft to clarify the ambiguous passages. This was released as the Clarified Artistic License, and was approved by the FSF. It is used by the Paros Proxy, the JavaFBP toolkit and NcFTP.
The terms of the Artistic License 1.0 were at issue in a 2007 federal district court decision in the US, which suggested that FOSS-like licenses could only be enforced through contract law rather than through copyright law, in contexts where contract damages would be difficult to establish. On appeal, a federal appellate court "determined that the terms of the Artistic License are enforceable copyright conditions". The case was remanded to the District Court which did not apply the superior court's criteria (on the grounds that in the interim, the Supreme Court had changed the applicable law). However, this left undisturbed the finding that a free and open source license nonetheless has economic value.
Artistic License 2.0.
In response to the Request for comments process for improving the licensing position for Perl 6, Kuhn's draft was extensively rewritten by Roberta Cairney and Allison Randal for readability and legal clarity, with input from the Perl community. This resulted in the Artistic License 2.0 which has been approved as both a free software and open source license.
The Artistic license 2.0 is also notable for its excellent license compatibility with other FOSS licenses due to a relicensing clause, a property other licenses like the GPL are missing.
It has been adopted by some of the Perl 6 implementations, and has been used by the Parrot virtual machine since version 0.4.13. It is also used by the SNEeSe emulator, which was formerly licensed under the Clarified Artistic License.
The OSI recommends that all developers and projects licensing their products with the Artistic License adopt Artistic License 2.0.

</doc>
<doc id="2162" url="https://en.wikipedia.org/wiki?curid=2162" title="Afrikaans">
Afrikaans

Afrikaans (, , or ) is a West Germanic language spoken in South Africa, Namibia, and to a lesser extent, Botswana and Zimbabwe. It evolved from the Dutch vernacular of South Holland spoken by the mainly Dutch settlers of what is now South Africa, where it gradually began to develop distinguishing characteristics in the course of the 18th century. Hence, it is a daughter language of Dutch, and was previously referred to as "Cape Dutch" (a term also used to refer collectively to the early Cape settlers) or "kitchen Dutch" (a derogatory term used to refer to Afrikaans in its earlier days). The term is ultimately derived from Dutch "Afrikaans-Hollands" meaning "African Dutch". It is the first language of most of the Afrikaner and Coloured people of Southern Africa.
Although Afrikaans has adopted words from other languages, including Portuguese, the Bantu languages, Malay, German and the Khoisan languages, an estimated 90 to 95% of Afrikaans vocabulary is of Dutch origin.
Therefore, differences with Dutch often lie in the more analytic morphology and grammar of Afrikaans, and a spelling that expresses Afrikaans pronunciation rather than standard Dutch. There is a large degree of mutual intelligibility between the two languages—especially in written form.
With about 7 million native speakers in South Africa, or 13.5% of the population, it is the third-most-spoken language in the country. It has the widest geographical and racial distribution of all the eleven official languages of South Africa, and is widely spoken and understood as a second or third language. It is the majority language of the western half of South Africa—the provinces of the Northern Cape and Western Cape—and the first language of 75.8% of Coloured South Africans (3.4 million people), 60.8% of White South Africans (2.7 million) and at 4.6% the second most spoken first-language among Asian South Africans (58,000). About 1.5% of black South Africans (600,000 people) speak it as their first language. Large numbers of speakers of Bantu languages and English-speaking South Africans also speak it as their second language. It is taught in schools, with about 10.3 million second language students. One reason for the expansion of Afrikaans is its development in the public realm: it is used in newspapers, radio programs, TV, and several translations of the Bible have been published since the first one was completed in 1933.
In neighbouring Namibia, Afrikaans is widely spoken as a second language and used as "lingua franca", while as a native language it is spoken in 11% of households, mainly concentrated in the capital Windhoek and the southern regions of Hardap and ǁKaras. It, along with German, was among the official languages of Namibia until the country became independent in 1990, 25% of the population of Windhoek spoke Afrikaans at home. Today, both Afrikaans and German survive as recognised regional language in the country, although only English has official status within the government.
Estimates of the total number of Afrikaans-speakers range between 15 and 23 million.
History.
Origin.
The Afrikaans language arose in the Dutch Cape Colony, through a gradual divergence from European Dutch dialects, during the course of the 18th century. As early as the mid-18th century and as recently as the mid-20th century, Afrikaans was known in standard Dutch as a "kitchen language" (Afr. "kombuistaal"), lacking the prestige accorded, for example even by the educational system in Africa, to languages spoken outside Africa; other early epithets setting apart "Kaaps Hollands" ("Cape Dutch", i.e. Afrikaans) as putatively beneath official Dutch standards included "geradbraakt/gebroken/onbeschaafd Hollands" ("mutilated/broken/uncivilised Dutch"), as well as "verkeerd Nederlands" ("incorrect Dutch"). An estimated 90 to 95% of Afrikaans vocabulary is ultimately of Dutch origin, and there are few lexical differences between the two languages; however, Afrikaans has a considerably more regular morphology, grammar, and spelling. There is a degree of mutual intelligibility between the two languages, particularly in written form.
Afrikaans acquired some lexical and syntactical borrowings from other languages such as Malay, Khoisan languages, Portuguese, and of the Bantu languages, and Afrikaans has also been significantly influenced by South African English. Nevertheless, Dutch speakers are confronted with fewer noncognates when listening to Afrikaans than the other way round. Mutual intelligibility thus tends to be asymmetrical, as it is easier for Dutch speakers to understand Afrikaans than for Afrikaans speakers to understand Dutch. In general, mutual intelligibility between Dutch and Afrikaans is better than between Dutch and Frisian or between Danish and Swedish. The South African poet writer Breyten Breytenbach, attempting to visualize the language distance for anglophones once remarked that the differences between (Standard) Dutch and Afrikaans are comparable to those between the Received Pronunciation and Southern American English.
Development.
Beginning in about 1815, Afrikaans started to replace Malay as the language of instruction in Muslim schools in South Africa, written with the Arabic alphabet. Later, Afrikaans, now written with the Latin alphabet, started to appear in newspapers and political and religious works in around 1850.
In 1875, a group of Afrikaans-speakers from the Cape formed the Genootskap vir Regte Afrikaanders (Society for Real Afrikaners), and published a number of books in Afrikaans including grammars, dictionaries, religious materials and histories. In 1925, Afrikaans was recognised by the South African government as a real language, rather than simply a slang version of Dutch proper.
Recognition.
Afrikaans was considered a Dutch dialect in South Africa until the early 20th century, when it became recognised as a distinct language under South African law, alongside Standard Dutch, which it eventually replaced as an official language. A relative majority of the first settlers whose descendants today are the Afrikaners were from the United Provinces (now Netherlands and Flanders), though up to one-sixth of the community was also of French Huguenot origin, and a seventh from Germany.
The workers and slaves who contributed to the development of Afrikaans were Asians (especially Malays) and Malagasys, as well as the Khoi, San, and Bantu peoples who also lived in the area. African creole people in the early 18th century — documented on the cases of Hendrik Bibault and patriarch Oude Ram — were the first to call themselves "Afrikaner" (Africans). Only much later in the second half of the 19th century did the Boers adopt this attribution, too. The Khoi and mixed-race groups became collectively referred to as 'Coloureds'.
Monument.
The Afrikaans Language Monument (Afrikaans: "Afrikaanse Taalmonument") is located on a hill overlooking Paarl, Western Cape Province, South Africa. Officially opened on 10 October 1975, it commemorates the 50th anniversary of Afrikaans being declared an official language of South Africa in distinction to Dutch. It was erected in Paarl on the 100th anniversary of the founding of the Genootskap van Regte Afrikaners (Society of Real Afrikaners), an organisation which helped to strengthen Afrikaners' identity and pride in their language.
Standardisation.
The linguist Paul Roberge suggested the earliest 'truly Afrikaans' texts are doggerel verse from 1795 and a dialogue transcribed by a Dutch traveller in 1825. Printed material among the Afrikaners at first used only standard European Dutch. By the mid-19th century, more and more were appearing in Afrikaans, which was very much still regarded as a set of regional dialects.
In 1861, L.H. Meurant published his ' ("Conversation between Claus Truthsayer and John Doubter"), which is considered by some to be the first authoritative Afrikaans text. Abu Bakr Effendi also compiled his Arabic Afrikaans Islamic instruction book between 1862 and 1869, although this was only published and printed in 1877. The first Afrikaans grammars and dictionaries were published in 1875 by the ' ('Society for Real Afrikaners') in Cape Town.
The First and Second Boer Wars further strengthened the position of Afrikaans. The official languages of the Union of South Africa were English and Dutch until Afrikaans was subsumed under Dutch on 5 May 1925.
The main Afrikaans dictionary is the Woordeboek van die Afrikaanse Taal (WAT) (Dictionary of the Afrikaans Language), which is as yet incomplete owing to the scale of the project, but the one-volume dictionary in household use is the Verklarende Handwoordeboek van die Afrikaanse Taal (HAT). The official orthography of Afrikaans is the "Afrikaanse Woordelys en Spelreëls", compiled by Die Taalkommissie.
The Afrikaans Bible.
A major landmark in the development of the language was the translation of the whole Bible into Afrikaans. Before this, most Cape Dutch-Afrikaans speakers had to rely on the Dutch Statenbijbel. This Statenvertaling had its origins with the Synod of Dordrecht of 1618 and was thus in an archaic form of Dutch. This was hard for Dutch and Cape Dutch speakers to understand, and increasingly unintelligible for Afrikaans speakers.
C. P. Hoogehout, Arnoldus Pannevis, and Stephanus Jacobus du Toit were the first Afrikaans Bible translators. Important landmarks in the translation of the Scriptures were in 1878 with C. P. Hoogehout's translation of the "Evangelie volgens Markus" (Gospel of Mark, lit. Gospel according to Mark); however, this translation was never published. The manuscript is to be found in the South African National Library, Cape Town.
The first official translation of the entire Bible into Afrikaans was in 1933 by J. D. du Toit, E. E. van Rooyen, J. D. Kestell, H. C. M. Fourie, and BB Keet. This monumental work established Afrikaans as "", that is "a pure and proper language" for religious purposes, especially amongst the deeply Calvinist Afrikaans religious community that previously had been rather sceptical of a Bible translation that varied from the Dutch version that they were used to.
In 1983 a fresh translation marked the 50th anniversary of the 1933 version and provided a much-needed revision. The final editing of this edition was done by E. P. Groenewald, A. H. van Zyl, P. A. Verhoef, J. L. Helberg and W. Kempen.
Geographic distribution.
Sociolinguistics.
Some state that instead of "Afrikaners", which refers to an ethnic group, the terms "Afrikaanses" or "Afrikaanssprekendes" (lit. Afrikaans speakers) should be used for people of any ethnic origin who speak Afrikaans. Linguistic identity has not yet established which terms shall prevail, and all three are used in common parlance. The white Afrikaans-speaking community started being referred to colloquially as "The Boere". The terms "boerseun" (farm boy) and "boeremeisie" (farm girl) became popular among young white Afrikaners for expressing national pride, whether-or-not they actually grew up on a farm.
Afrikaans is also widely spoken in Namibia. Before independence, Afrikaans had equal status with German as an official language. Since independence in 1990, Afrikaans has had constitutional recognition as a national, but not official, language. There is a much smaller number of Afrikaans speakers among Zimbabwe's white minority, as most have left the country since 1980. Afrikaans was also a medium of instruction for schools in Bophuthatswana Bantustan.
Many South Africans living and working in Belgium, the Netherlands, the United Kingdom, Republic of Ireland, Australia, New Zealand, Canada, the United States and Kuwait are also Afrikaans-speaking. They have access to Afrikaans websites, news sites such as Netwerk24.com and Sake24, and radio broadcasts over the web, such as those from Radio Sonder Grense and Radio Pretoria.
Afrikaans has been influential in the development of South African English. Many Afrikaans loanwords have found their way into South African English, such as "bakkie" ("pickup truck"), "braai" ("barbecue"), "naartjie" ("tangerine"), "tekkies" (American "sneakers", British "trainers", Canadian "runners"). A few words in standard English are derived from Afrikaans, such as "aardvark" (lit. "earth pig"), "trek" ("pioneering journey", in Afrikaans lit. "pull" but used also for "migrate"), "spoor" ("animal track"), "veld" ("Southern African grassland" in Afrikaans, lit. "field"), "commando" from Afrikaans "kommando" meaning small fighting unit, "boomslang" ("tree snake") and "apartheid" ("segregation"; more accurately "apartness" or "the state or condition of being apart").
In 1976, secondary school pupils in Soweto began a rebellion in response to the government's decision that Afrikaans be used as the language of instruction for half the subjects taught in non-White schools (with English continuing for the other half). Although English is the mother tongue of only 8.2% of the population, it is the language most widely understood, and the second language of a majority of South Africans. Afrikaans is more widely spoken than English in the Northern and Western Cape provinces, several hundred kilometres from Soweto. The Black community's opposition to Afrikaans and preference for continuing English instruction was underlined when the government rescinded the policy one month after the uprising: 96% of Black schools chose English (over Afrikaans or native languages) as the language of instruction. Also, due to Afrikaans being viewed as the language of the white oppressor by some, pressure has been increased to remove Afrikaans as a teaching language in South African universities, resulting in bloody student protests in 2015. 
Under South Africa's Constitution of 1996, Afrikaans remains an official language, and has equal status to English and nine other languages. The new policy means that the use of Afrikaans is now often reduced in favour of English, or to accommodate the other official languages. In 1996, for example, the South African Broadcasting Corporation reduced the amount of television airtime in Afrikaans, while South African Airways dropped its Afrikaans name "" from its livery. Similarly, South Africa's diplomatic missions overseas now only display the name of the country in English and their host country's language, and not in Afrikaans.
In spite of these moves, the language has remained strong, and Afrikaans newspapers and magazines continue to have large circulation figures. Indeed, the Afrikaans-language general-interest family magazine "Huisgenoot" has the largest readership of any magazine in the country. In addition, a pay-TV channel in Afrikaans called KykNet was launched in 1999, and an Afrikaans music channel, MK (Musiek kanaal) (lit. 'Music Channel'), in 2005. A large number of Afrikaans books are still published every year, mainly by the publishers Human & Rousseau, Tafelberg Uitgewers, Struik, and Protea Boekhuis.
Afrikaans has two monuments erected in its honour. The first was erected in Burgersdorp, South Africa, in 1893, and the second, nowadays better-known Afrikaans Language Monument (""), was built in Paarl, South Africa, in 1975.
When the British design magazine "Wallpaper" described Afrikaans as "one of the world's ugliest languages" in its September 2005 article about the monument, South African billionaire Johann Rupert (chairman of the Richemont Group), responded by withdrawing advertising for brands such as Cartier, Van Cleef & Arpels, Montblanc and Alfred Dunhill from the magazine. The author of the article, Bronwyn Davies, was an English-speaking South African.
Modern Dutch and Afrikaans share over 90 per cent of their vocabulary. Afrikaans speakers are able to learn Dutch within a comparatively short time. Native Dutch speakers pick up written Afrikaans even more quickly, due to its simplified grammar, whereas understanding spoken Afrikaans might need more effort. Afrikaans speakers can learn Dutch pronunciation with little training. This has enabled Dutch and Belgian companies to outsource their call centre operations to South Africa.
Current status.
Post-apartheid South Africa has seen a loss of preferential treatment by the government for Afrikaans, in terms of education, social events, media (TV and radio), and general status throughout the country, given that it now shares its place as official language with ten other languages. Nevertheless, Afrikaans remains more prevalent in the media – radio, newspapers and television – than any of the other official languages, except English. More than 300 book titles in Afrikaans are published annually. South African census figures suggest a growing number of speakers in all 9 provinces, a total of 6.85 million in 2011 compared to 5.98 million a decade earlier. The South African Institute of Race Relations (SAIRR) project that a growing majority will be Coloured Afrikaans speakers. Afrikaans speakers enjoy higher employment rates than other South African language groups, though half a million remain unemployed.
Despite the challenges of demotion and emigration that it faces in South Africa, the Afrikaans vernacular remains competitive, being popular in DSTV pay channels and several internet sites, while generating high newspaper and music CD sales. A resurgence in Afrikaans popular music since the late 1990s has invigorated the language, especially among a younger generation of South Africans. A recent trend is the increased availability of pre-school educational CDs and DVDs. Such media also prove popular with the extensive Afrikaans-speaking expatriate communities who seek to retain language proficiency in a household context.
After years of slumber, Afrikaans language cinema is showing signs of new vigour. The 2007 film "Ouma se slim kind", the first full-length Afrikaans movie since Paljas of 1998, is seen as the dawn of a new era in Afrikaans cinema. Several short films have been created and more feature-length movies, such as "Poena is Koning" and "Bakgat" (both in 2008) have been produced, besides the 2011 Afrikaans-language film "Skoonheid", which was the first Afrikaans film to screen at the Cannes Film Festival. The film "Platteland" was also released in 2011.
Afrikaans seems to be returning to the SABC. SABC3 announced early in 2009 that it would increase Afrikaans programming due to the "growing Afrikaans-language market and need for working capital as Afrikaans advertising is the only advertising that sells in the current South African television market". In April 2009, SABC3 started screening several Afrikaans-language programmes. Further latent support for the language derives from its de-politicised image in the eyes of younger-generation South Africans, who less and less often view it as "the language of the oppressor". Indeed, there is a groundswell movement within Afrikaans to be inclusive, and to promote itself along with the other indigenous official languages. 
In Namibia, the percentage of Afrikaans speakers declined from 11.4% (2001 Census) to 10.4% (2011 Census). The major concentrations are in Hardap (41.0%), ǁKaras (36.1%), Erongo (20.5%), Khomas (18.5%), Omaheke (10.0%), Otjozondjupa (9.4%), Kunene (4.2%), and Oshikoto (2.3%).
Dialects.
Following early dialectical studies of Afrikaans, it was theorised that three main historical dialects probably existed after the Great Trek in the 1830s. These dialects are the Northern Cape, Western Cape, and Eastern Cape dialects. Northern Cape dialect may have resulted from contact between Dutch settlers and the Khoi-Khoi people between the Great Karoo and the Kunene, and Eastern Cape dialect between the Dutch and the Xhosa. Remnants of these dialects still remain in present-day Afrikaans, although the standardising effect of Standard Afrikaans has contributed to a great levelling of differences in modern times.
There is also a prison cant, known as soebela or sombela, which is based on Afrikaans, yet heavily influenced by Zulu. This language is used as a secret language in prison and is taught to initiates.
Kaapse Afrikaans.
The term "Kaapse Afrikaans" ("Cape Afrikaans") is sometimes erroneously used to refer to the entire Western Cape dialect; it is more commonly used for a particular sociolect spoken in the Cape Peninsula of South Africa. Kaapse Afrikaans was once spoken by all population groups. However, it became increasingly restricted to the Cape Coloured ethnic group in Cape Town and environs.
Kaapse Afrikaans preserves some features more similar to Dutch than to Afrikaans.
Kaapse Afrikaans has some other features not typically found in Afrikaans.
Kaapse Afrikaans is also characterised by much code-switching between English and Afrikaans, especially in the inner-city and lower socio-economic status areas of Cape Town.
An example of characteristic Kaapse Afrikaans:
Oranjerivierafrikaans.
The term "Oranjerivierafrikaans" ("Afrikaans of the Orange River") is sometimes erroneously used to refer to the Northern Cape dialect; it is more commonly used for the regional peculiarities of standard Afrikaans spoken in the Upington/Orange River wine district of South Africa.
Some of the characteristics of Oranjerivierafrikaans are the plural form "-goed" ("Ma-goed", "meneergoed"), variant pronunciation such as in "kjerk" ("Church") and "gjeld" ("money") and the ending "-se", which indicates possession.
Expatriate geolect.
Although Afrikaans is mainly spoken in South Africa and Namibia, smaller Afrikaans-speaking populations live in Argentina, Australia, Botswana, Brazil, Canada, Lesotho, Malawi, the Netherlands, New Zealand, Swaziland, the UAE, the United Kingdom, Republic of Ireland, the USA, Zambia, and Zimbabwe. Most Afrikaans-speaking people living outside Africa are emigrants and their descendants. Because of emigration and migrant labour, more than 100,000 Afrikaans speakers may live in the United Kingdom.
Grammar.
In Afrikaans grammar, there is no distinction between the infinitive and present forms of verbs, with the exception of the verbs 'to be' and 'to have':
In addition, verbs do not conjugate differently depending on the subject. For example,
Only a handful of Afrikaans verbs have a preterite, namely the auxiliary "wees" ("to be"), the modal verbs, and the verb "dink" ("to think"). The preterite of "mag" ("may") is rare in contemporary Afrikaans.
All other verbs use the perfect tense ("hê" + past participle) for the past. Therefore, there is no distinction in Afrikaans between "I drank" and "I have drunk". (Also in colloquial German, and to some extent Dutch, the past tense is often replaced with the perfect.)
When telling a longer story, Afrikaans speakers usually avoid the perfect and simply use the present tense, or historical present tense instead (as is possible, but less common, in English as well).
A particular feature of Afrikaans is its use of the double negative; it is classified in Afrikaans as "ontkennende vorm" and is something that is absent from the other West Germanic standard languages. For example,
Both French and San origins have been suggested for double negation in Afrikaans. While double negation is still found in Low Franconian dialects in West-Flanders and in some "isolated" villages in the centre of the Netherlands (i.e. Garderen), it takes a different form, which is not found in Afrikaans. The following is an example:
The "-ne" was the Middle Dutch way to negate but it has been suggested that since "-ne" became highly non-voiced, nie or niet was needed to complement the "-ne". With time the "-ne" disappeared in most Dutch dialects.
The double negative construction has been fully grammaticalized in standard Afrikaans and its proper use follows a set of fairly complex rules as the examples below show:
The Dutch word "het" ("it" in English) does not correspond to "het" in Afrikaans. The Dutch words corresponding to Afrikaans "het" are "heb", "hebt", "heeft" and "hebben".
A notable exception to this is the use of the negating grammar form that coincides with negating the English present participle. In this case there is only a single negation.
Certain words in Afrikaans arise due to grammar. For example, "moet nie", which literally means "must not", usually becomes "moenie"; although one does not have to write or say it like this, virtually all Afrikaans speakers will change the two words to "moenie" in the same way as "do not" shifts to "don't" in English.
Orthography.
There are many parallels to the Dutch orthography conventions and those used for Afrikaans. There are 26 letters.
In Afrikaans, many consonants are dropped from the earlier Dutch spelling. For example, "slechts" ('only') in Dutch becomes "slegs" in Afrikaans. Also, Afrikaans and some Dutch dialects make no distinction between and , having merged the latter into the former; while the word for "south" is written ' in Dutch, it is spelled ' in Afrikaans (as well as dialectal Dutch writings) to represent this merger. Similarly, the Dutch digraph "ĳ", normally pronounced as , is written as "y", except where it replaces the Dutch suffix "–lijk" which is pronounced as or , as in " > ".
Another difference is the indefinite article, ' in Afrikaans and in Dutch. "A book" is ' in Afrikaans, whereas it is either ' or ' in Dutch. This "" is usually pronounced as just a weak vowel, .
The diminutive suffix in Afrikaans is "-tjie", whereas in Dutch it is "-tje", hence a "bit" is in Afrikaans and in Dutch.
The letters "c", "q", "x", and "z" occur almost exclusively in borrowings from French, English, Greek and Latin. This is usually because words that had "c" and "ch" in the original Dutch are spelled with "k" and "g", respectively, in Afrikaans. Similarly original "qu" and "x" are spelt "kw" and "ks" respectively. For example, ' instead of "equatoriaal", and ' instead of "excuus".
The vowels with diacritics in non-loanword Afrikaans are: "á", "é", "è", "ê", "ë", "í", "î", "ï", "ó", "ô", "ú", "û", "ý". Diacritics are ignored when alphabetising, though they are still important, even when typing the diacritic forms may be difficult. For example, ' instead of the 3 e's alongside each other: "geeet", which can never occur in Afrikaans, or ', which translates to "say", whereas "" is a possessive form.
Initial apostrophes.
A few short words in Afrikaans take initial apostrophes. In modern Afrikaans, these words are always written in lower case (except if the entire line is uppercase), and if they occur at the beginning of a sentence, the next word is capitalised. Three examples of such apostrophed words are '. The last (the indefinite article) is the only apostrophed word that is common in modern written Afrikaans, since the other examples are shortened versions of other words (' and "" respectively) and are rarely found outside of a poetic context.
Here are a few examples:
The apostrophe and the following letter are regarded as two separate characters, and are never written using a single glyph, although a single character variant of the indefinite article appears in Unicode, ŉ.
Table of characters.
For more on the pronunciation of the letters below, see "".
Afrikaans phrases.
Afrikaans is a very centralised language, meaning that most of the vowels are pronounced in a very centralised (or schwa-like) way. Although there are many different dialects and accents, the transcription would be fairly standard.
In the Dutch language the word "Afrikaans" means African, in the general sense. Consequently, Afrikaans is commonly denoted as "Zuid-Afrikaans". This ambiguity also exists in Afrikaans itself and is either resolved in the context of its usage, or by using "Afrikaan" for an African person, and "Afrika-" in the adjective sense.
The following Afrikaans sentences, which have the same meaning in English, are also written identically though their pronunciation differs:
Sample text.
Psalm 23 1983 translation:
"Die Here is my Herder, ek kom niks kort nie."<br>
"Hy laat my in groen weivelde rus. Hy bring my by waters waar daar vrede is."<br>
"Hy gee my nuwe krag. Hy lei my op die regte paaie tot eer van Sy naam."<br>
"Selfs al gaan ek deur donker dieptes, sal ek nie bang wees nie, want U is by my. In U hande is ek veilig."
Psalm 23 alternative translation:
"Die Here is my Herder, niks sal my ontbreek nie."<br>
"Hy laat my neerlê in groen weivelde; na waters waar rus is, lei Hy my heen."<br>
"Hy verkwik my siel; Hy lei my in die spore van geregtigheid, om sy Naam ontwil."<br>
"Al gaan ek ook in 'n dal van doodskaduwee, ek sal geen onheil vrees nie; want U is met my: u stok en u staf die vertroos my."
Lord's Prayer (Afrikaans New Living translation)
"Ons Vader in die hemel, laat U Naam geheilig word."<br>
"Laat U koningsheerskappy spoedig kom."<br>
"Laat U wil hier op aarde uitgevoer word soos in die hemel."<br>
"Gee ons die porsie brood wat ons vir vandag nodig het."<br>
"En vergeef ons ons sondeskuld soos ons ook óns skuldenaars vergewe het."<br>
"Bewaar ons sodat ons nie aan verleiding sal toegee nie; en bevry ons van die greep van die Bose."<br>
"Want van U is die koninkryk,"<br>
"en die krag,"<br>
"en die heerlikheid,"<br>
"tot in ewigheid. Amen"
Lord's Prayer (Original translation):
"Onse Vader wat in die hemel is,"<br>
"laat U Naam geheilig word;"<br>
"laat U koninkryk kom;"<br>
"laat U wil geskied op die aarde,"<br>
"net soos in die hemel."<br>
"Gee ons vandag ons daaglikse brood;"<br>
"en vergeef ons ons skulde"<br>
"soos ons ons skuldenaars vergewe"<br>
"en laat ons nie in die versoeking nie"<br>
"maar verlos ons van die Bose"<br>
"Want aan U behoort die koninkryk"<br>
"en die krag"<br>
"en die heerlikheid"<br>
"tot in ewigheid. Amen"

</doc>
<doc id="2163" url="https://en.wikipedia.org/wiki?curid=2163" title="Aeolus">
Aeolus

Aeolus (; , "Aiolos" , Modern Greek: ), a name shared by three mythical characters, was the ruler of the winds in Greek mythology. These three personages are often difficult to tell apart, and even the ancient mythographers appear to have been perplexed about which Aeolus was which. Diodorus Siculus made an attempt to define each of these three (although it is clear that he also became muddled), and his opinion is followed here. Briefly, the first Aeolus was a son of Hellen and eponymous founder of the Aeolian race; the second was a son of Poseidon, who led a colony to islands in the Tyrrhenian Sea; and the third Aeolus was a son of Hippotes who is mentioned in "Odyssey" and the "Aeneid" as the Keeper of the Winds. All three men named Aeolus appear to be connected genealogically, although the precise relationship, especially regarding the second and third Aeolus, is often ambiguous.
Son of Hellen.
This Aeolus was son of Hellen and the nymph Orseis, and a brother of Dorus, Xuthus and, in some sources, of Amphictyon (who is otherwise a brother of Hellen). Described as the ruler of Aeolia (later called Thessaly) and held to be the founder of the Aeolic branch of the Greek nation, this Aeolus married Enarete, daughter of Deimachus (otherwise unknown). Aeolus and Enarete had many children, although the precise number and identities of these children vary from author to author in the ancient sources. The great extent of country which this race occupied, and the desire of each part of it to trace its origin to some descendant of Aeolus, probably gave rise to the varying accounts about the number of his children. Some scholars contend that the most ancient and genuine story told of only four sons of Aeolus: Sisyphus, Athamas, Cretheus, and Salmoneus, as the representatives of the four main branches of the Aeolic race. Other sons included Deioneus, Perieres, Cercaphas and perhaps Magnes (usually regarded as a brother of Macedon) and Aethlius. Another son is named Mimas, who provides a link to the third Aeolus in a genealogy that seems very contrived. Calyce, Peisidice, Perimede and Alcyone were counted among the daughters of Aeolus and Enarete. This Aeolus also had an illegitimate daughter named Arne, begotten on Melanippe, daughter of the Centaur Chiron. This Arne became the mother of the second Aeolus, by the god Poseidon.
Son of Poseidon.
This Aeolus was a son of Poseidon by Arne, daughter of Aeolus. He had a twin brother named Boeotus. Arne confessed to her father that she was with child by the god Poseidon; her father, however, did not believe her, and handed her over to a man named Metapontus, King of Icaria. When Bœotus and Aeolus were born, they were raised by Metapontus; but their stepmother (Autolyte, wife of Metapontus) quarrelled with their mother Arne, prompting Bœotus and Aeolus to kill Autolyte and flee from Icaria. Bœotus (accompanied by Arne) went to southern Thessaly, and founded Boeotia; but Aeolus went to a group of islands in the Tyrrhenian Sea, which received from him the name of the Aeolian Islands; according to some accounts this Aeolus founded the town of Lipara. Although his home has been traditionally identified as one of the Aeolian Islands (there is little consensus as to which), near Sicily, an alternative location has been suggested at Gramvousa off the northwest coast of Crete. Aeolus had six sons and six daughters, whom in Homer he wed to one another and the family lived happily together. Later writers were shocked by the incest: in Hyginus, the day Aeolus learned that one of his sons, Macareus, had committed incest with his sister Canace he expelled Macareus and threw the child born of this incestuous union to the dogs, and sent his daughter a sword by which she was to kill herself. Other late accounts claim that Macareus had a daughter named Amphissa, beloved by Apollo.
Son of Hippotes.
This Aeolus is most frequently conflated with Aeolus, the son of Poseidon, god of the sea. It is difficult to differentiate this Aeolus from the second Aeolus, as their identities seem to have been merged by many ancient writers. The father of this third Aeolus is given as Hippotes, son of Mimas, a son of the first Aeolus (son of Hellen). According to some accounts, Hippotes married the same Melanippe who was the mother of Arne. This Aeolus lived on the floating island of Aeolia and was visited by Odysseus and his crew in the "Odyssey." He gave hospitality for a month and provided for a west wind to carry them home. He also provided a gift of a bag containing all winds but the west, which Odysseus's crew members unwittingly opened just before they were to reach Ithaca. Unfortunately, they were blown back to Aeolia, where Aeolus refused to provide any further help, because he believed that their short and unsuccessful voyage meant that the gods did not favour them. This Aeolus was perceived by post-Homeric authors as a god, rather than as a mortal and simple Keeper of the Winds (as in the "Odyssey"). 
Like the previous, this Aeolus was said to have had twelve children - six sons and six daughters. According to Diodorus, he was father of six sons by Cyane, daughter of Liparus (the eponym of the island Lipara, whom Aeolus assisted in conquering lands above Surrentum, Italy). The sons' names were Agathyrnus, Astyochus, Androcles, Iocastus, Pheraemon, Xuthus, whereas the daughters are not mentioned at all. The sons were said to have become kings: Iocastus of the region in southern Italy as far as Rhegium; Pheraemon and Androcles of the part of Sicily between the Strait of Messina and Lilybaeum; Xuthus of Leontini; Agathyrnus of what was known as Agathyrnitis, having founded Agathyrnum; and Astyochus of Lipara. All were said to have been remembered as just and pious rulers. Another list of Aeolus' children is found in scholia on the "Odyssey". The latter source gives the sons' names as Androcles, Chrysippus, Iocastus, Phalacrus, Pheraemon, Xuthus, and the daughters' as Aeole, Astycrateia, Dia, Hephaestia, Iphthe, Periboea; their mother in this account is Telepora or Telepatra, daughter of Laestrygon.
Parthenius of Nicaea recorded a love affair between Odysseus and Aeolus' daughter Polymele; the latter was said to have ended up betrothed to her own brother Diores. 
In the "Aeneid" by Virgil, Juno offers Aeolus the nymph Deiopea as a wife if he will release his winds upon the fleet of Aeneas.

</doc>
<doc id="2166" url="https://en.wikipedia.org/wiki?curid=2166" title="ABC">
ABC

ABC are the first three letters of the Latin script known as the alphabet.
ABC or abc may refer to:

</doc>
<doc id="2167" url="https://en.wikipedia.org/wiki?curid=2167" title="Alford plea">
Alford plea

An "Alford" plea (also called a Kennedy plea in West Virginia, an
"Alford" guilty plea, and the "Alford" doctrine) in United States law is a guilty plea in criminal court, whereby a defendant in a criminal case does not admit to the criminal act and asserts innocence. In entering an Alford plea, the defendant admits that the evidence the prosecution has would be likely to persuade a judge or jury to find the defendant guilty beyond a reasonable doubt.
Origin.
The "Alford" guilty plea is named after the United States Supreme Court case of "North Carolina v. Alford" (1970). Henry Alford had been indicted on a charge of first-degree murder in 1963. Evidence in the case included testimony from witnesses that Alford had said, after the victim's death, that he had killed the individual. Court testimony showed that Alford and the victim had argued at the victim's house. Alford left the house, and afterwards the victim received a fatal gunshot wound when he opened the door responding to a knock.
Alford was faced with the possibility of capital punishment if convicted by a jury trial. The death penalty was the default sentence by North Carolina law at the time, if two requisites in the case were satisfied: the defendant had to have pleaded not guilty, and the jury did not instead recommend a life sentence. Had he pleaded guilty to first-degree murder, Alford would have had the possibility of a life sentence and would have avoided the death penalty, but he did not want to admit guilt. Nonetheless, Alford pleaded guilty to second-degree murder and said he was doing so to avoid a death sentence, were he to be convicted of first-degree murder, after attempting to contest that charge. Alford was sentenced to 30 years in prison, after the trial judge accepted the plea bargain and ruled that the defendant had been adequately advised by his defense lawyer.
Alford appealed and requested a new trial, arguing he was forced into a guilty plea because he was afraid of receiving a death sentence. The Supreme Court of North Carolina ruled that the defendant had voluntarily entered the guilty plea, with knowledge of what that meant. Following this ruling, Alford petitioned for a writ of habeas corpus in the United States District Court for the Middle District of North Carolina, which upheld the initial ruling, and subsequently to the United States Court of Appeals for the Fourth Circuit, which ruled that Alford's plea was not voluntary, because it was made under fear of the death penalty. "I just pleaded guilty because they said if I didn't, they would gas me for it," wrote Alford in one of his appeals.
The case was then appealed to the U.S. Supreme Court. Supreme Court Justice Byron White wrote the majority decision, which held that for the plea to be accepted, the defendant must have been advised by a competent lawyer who was able to inform the individual that his best decision in the case would be to enter a guilty plea. The Court ruled that the defendant can enter such a plea "when he concludes that his interests require a guilty plea and the record strongly indicates guilt". The Court allowed the guilty plea with a simultaneous protestation of innocence only because there was enough evidence to show that the prosecution had a strong case for a conviction, and the defendant was entering such a plea to avoid this possible sentencing. The Court went on to note that even if the defendant could have shown that he would not have entered a guilty plea "but for" the rationale of receiving a lesser sentence, the plea itself would not have been ruled invalid. As evidence existed that could have supported Alford's conviction, the Supreme Court held that his guilty plea was allowable while the defendant himself still maintained that he was not guilty.
Alford died in prison, in 1975.
Definition.
The "Dictionary of Politics: Selected American and Foreign Political and Legal Terms" defines the term Alford plea as: "A plea under which a defendant may choose to plead guilty, not because of an admission to the crime, but because the prosecutor has sufficient evidence to place a charge and to obtain conviction in court. The plea is commonly used in local and state courts in the United States." According to "University of Richmond Law Review", "When offering an Alford plea, a defendant asserts his innocence but admits that sufficient evidence exists to convict him of the offense." "A Guide to Military Criminal Law" notes that under the Alford plea, "the defendant concedes that the prosecution has enough evidence to convict, but the defendant still refuses to admit guilt." The book "Plea Bargaining's Triumph: A History of Plea Bargaining in America" published by Stanford University Press defines the plea as one in "which the defendant adheres to his/her claim of innocence even while allowing that the government has enough evidence to prove his/her guilt beyond a reasonable doubt". According to the book "Gender, Crime, and Punishment" published by Yale University Press, "Under the Alford doctrine, a defendant does not admit guilt but admits that the state has sufficient evidence to find him or her guilty, should the case go to trial." "Webster's New World Law Dictionary" defines Alford plea as: "A guilty plea entered as part of a plea bargain by a criminal defendant who denies committing the crime or who does not actually admit his guilt. In federal courts, such plea may be accepted as long as there is evidence that the defendant is actually guilty."
The Alford guilty plea is "a plea of guilty containing a protestation of innocence". The defendant pleads guilty, but does not have to specifically admit to the guilt itself. The defendant maintains a claim of innocence, but agrees to the entry of a conviction in the charged crime. Upon receiving an Alford guilty plea from a defendant, the court may immediately pronounce the defendant guilty and impose sentence as if the defendant had otherwise been convicted of the crime. Sources disagree, as may differing states' laws, as to what category of plea the "Alford" plea falls under: Some sources state that the Alford guilty plea is a form of "nolo contendere", where the defendant in the case states "no contest" to the factual matter of the case as given in the charges outlined by the prosecution. Others hold that an "Alford" plea is simply one form of a guilty plea, and, as with other guilty pleas, the judge must see there is some factual basis for the plea.
Defendants can take advantage of the ability to use the Alford guilty plea, by admitting there is enough evidence to convict them of a higher crime, while at the same time pleading guilty to a lesser charge. Defendants usually enter an Alford guilty plea if they want to avoid a possible worse sentence were they to lose the case against them at trial. It affords defendants the ability to accept a plea bargain, while maintaining innocence.
Court and government usage.
This form of guilty plea has been frequently used in local and state courts in the United States, though it constitutes a small percentage of all plea bargains in the U.S. This form of plea is not allowed in courts of the United States military. In 2000 the United States Department of Justice noted, "In an Alford plea the defendant agrees to plead guilty because he or she realizes that there is little chance to win acquittal because of the strong evidence of guilt. About 17% of State inmates and 5% of Federal inmates submitted either an Alford plea or a no contest plea, regardless of the type of attorney. This difference reflects the relative readiness of State courts, compared to Federal courts, to accept an alternative plea."
In the 1995 case "State of Idaho v. Howry" before the Idaho Court of Appeals, the Court commented on the impact of the Alford guilty plea on later sentencing. The Court ruled, "Although an Alford plea allows a defendant to plead guilty amid assertions of innocence, it does not require a court to accept those assertions. The sentencing court may, of necessity, consider a broad range of information, including the evidence of the crime, the defendant's criminal history and the demeanor of the defendant, including the presence or absence of remorse." In the 1999 South Carolina Supreme Court case "State v. Gaines", the Court held that Alford guilty pleas were to be held valid in the absence of a specific on-the-record ruling that the pleas were voluntary – provided that the sentencing judge acted appropriately in accordance with the rules for acceptance of a plea made voluntarily by the defendant. The Court held that a ruling that the plea was entered into voluntarily is implied by the act of sentencing.
In the 2006 case before the United States Court of Appeals for the Fifth Circuit, "Ballard v. Burton", Judge Carl E. Stewart writing for the Court held that an Alford guilty plea is a "variation of an ordinary guilty plea". In October 2008, the United States Department of Justice defined an Alford plea as: "the defendant maintains his or her innocence with respect to the charge to which he or she offers to plead guilty".
In March 2009, the Minnesota House of Representatives characterized the Alford plea as: "a form of a guilty plea in which the defendant asserts innocence but acknowledges on the record that the prosecutor could present enough evidence to prove guilt." The Minnesota Judicial Branch similarly states: "Alford Plea: A plea of guilty that may be accepted by a court even where the defendant does not admit guilt. In an Alford plea, defendant has to admit that he has reviewed the state's evidence, a reasonable jury could find him guilty, and he wants to take advantage of a plea offer that has been made. Court has discretion as to whether to accept this type of plea."
The U.S. Attorneys' Manual states that in the federal system, Alford pleas "should be avoided except in the most unusual circumstances, even if no plea agreement is involved and the plea would cover all pending charges." U.S. Attorneys are required to obtain the approval of the Assistant Attorney General with supervisory responsibility over the subject matter before accepting such a plea.
Commentary.
In his book "American Criminal Justice" (1972), Jonathan D. Caplan comments on the Supreme Court decision, noting, "The "Alford" decision recognizes the plea-bargaining system, acknowledging that a man may maintain his innocence but still plead guilty in order to minimize his potential loss." Caplan comments on the impact of the Supreme Court's decision to require evidence of guilt in such a plea: "By requiring that there be some evidence of guilt in such a situation, the decision attempts to protect the 'really' innocent from the temptations to which plea-bargaining and defense attorneys may subject them."
Major Steven E. Walburn argues in a 1998 article in "The Air Force Law Review" that this form of guilty plea should be adopted for usage by the United States military. "In fairness to an accused, if, after consultation with his defense counsel, he knowingly and intelligently determines that his best interest is served by an Alford-type guilty plea, he should be free to choose this path. The system should not force him to lie under oath, nor to go to trial with no promise of the ultimate outcome concerning guilt or punishment. We must trust the accused to make such an important decision for himself. The military provides an accused facing court-martial with a qualified defense attorney. Together, they are in the best position to properly weigh the impact his decision, and the resulting conviction, will have upon himself and his family," writes Walburn. He emphasizes that when allowing these pleas, "trial counsel should establish as strong a factual basis as possible", in order to minimize the possible negative outcomes to "the public's perception of the administration of justice within the military".
Stephanos Bibas writes in a 2003 analysis for "Cornell Law Review" that Judge Frank H. Easterbrook and a majority of scholars "praise these pleas as efficient, constitutional means of resolving cases". Bibas notes that prominent plea bargain critic Albert Alschuler supports the use of this form of plea, writing, "He views them as a lesser evil, a way to empower defendants within a flawed system. As long as we have plea bargaining, he maintains, innocent defendants should be free to use these pleas to enter advantageous plea bargains without lying. And guilty defendants who are in denial should be empowered to use these pleas instead of being forced to stand trial." Bibas instead asserts that this form of plea is "unwise and should be abolished". Bibas argues, "These procedures may be constitutional and efficient, but they undermine key values served by admissions of guilt in open court. They undermine the procedural values of accuracy and public confidence in accuracy and fairness, by convicting innocent defendants and creating the perception that innocent defendants are being pressured into pleading guilty. More basically, they allow guilty defendants to avoid accepting responsibility for their wrongs."
Legal scholar Jim Drennan, an expert on the court system at the Institute of Government at the University of North Carolina at Chapel Hill, told the "Winston-Salem Journal" in a 2007 interview that the ability to use this form of guilty plea as an option in courts had a far-reaching effect throughout the United States. Drennan commented, "We have lots of laws, but human interaction creates unique circumstances and the law has to adapt." He said of the Supreme Court case, "They had to make a decision about what to do. One of the things the court has to do is figure out how to answer new questions, and that is what happened in this case."

</doc>
<doc id="2170" url="https://en.wikipedia.org/wiki?curid=2170" title="ABCD">
ABCD

ABCD is a list of the first four letters in the English alphabet. It may also refer to:'"

</doc>
<doc id="2171" url="https://en.wikipedia.org/wiki?curid=2171" title="Anti-realism">
Anti-realism

In analytic philosophy, the term anti-realism describes any position involving either the denial of an objective reality or the denial that verification-transcendent statements are either true or false. This latter construal is sometimes expressed by saying "there is no fact of the matter as to whether or not P". Thus, one may speak of anti-realism with respect to other minds, the past, the future, universals, mathematical entities (such as natural numbers), moral categories, the material world, or even thought. The two construals are clearly distinct but often confused. For example, an "anti-realist" who denies that other minds exist (i.e., a solipsist) is quite different from an "anti-realist" who claims that there is no fact of the matter as to whether or not there are unobservable other minds (i.e., a logical behaviorist).
Anti-realism in philosophy.
Michael Dummett.
The term "anti-realism" was coined by Michael Dummett, who introduced it in his paper "Realism" to re-examine a number of classical philosophical disputes involving such doctrines as nominalism, conceptual realism, idealism and phenomenalism. The novelty of Dummett's approach consisted in seeing these disputes as analogous to the dispute between intuitionism and Platonism in the philosophy of mathematics.
According to intuitionists (anti-realists with respect to mathematical objects), the truth of a mathematical statement consists in our ability to prove it. According to platonists (realists), the truth of a statement consists in its correspondence to objective reality. Thus, intuitionists are ready to accept a statement of the form "P or Q" as true only if we can prove P or if we can prove Q:
this is called the disjunction property. In particular, we cannot in general claim that "P or not P" is true (the law of Excluded Middle), since in some cases we may not be able to prove the statement "P" nor prove the statement "not P". Similarly, intuitionists object to the existence property for classical logic, where one can prove formula_1, without being able to produce any term formula_2 of which formula_3 holds.
Dummett argues that the intuitionistic notion of truth lies at the bottom of various classical forms of anti-realism. He uses this notion to re-interpret phenomenalism, claiming that it need not take the form of a reductionism (often considered untenable).
Dummett's writings on anti-realism also draw heavily on the later writings of Wittgenstein concerning meaning and rule following. In fact, Dummett's writings on anti-realism can be seen as an attempt to integrate central ideas from the "Philosophical Investigations" into analytical philosophy.
Anti-realism in the sense that Dummett uses the term is also often called semantic anti-realism.
Hilary Putnam's "internal realism".
Despite being at one time a defender of metaphysical realism, Hilary Putnam later abandoned this view in favor of a position he termed "internal realism".
Precursors.
Doubts about the possibility of definite truth have been expressed since ancient times, for instance in the skepticism of Pyrrho. Anti-realism about matter or physical entities also has a long history. It can be found in the idealism of
Berkeley, as well as Hegel and other post-Kantians.
Metaphysical realism vis-à-vis internal realism.
Anti-realist arguments.
Idealists are skeptics about the physical world, maintaining either: 1) that nothing exists outside the mind, or 2) that we would have no access to a mind-independent reality even if it may exist; the latter case often takes the form of a denial of the idea that we can have unconceptualised experiences (see Myth of the Given). Conversely, most realists (specifically, indirect realists) hold that perceptions or sense data are caused by mind-independent objects. But
this introduces the possibility of another kind of skepticism: since our understanding of causality is that the same effect can be produced by multiple causes, there is a lack of determinacy about what one is really perceiving. A concrete example of a situation where an individual's sensory input might be caused by something other than what he thinks is causing it is the brain in a vat scenario.
On a more abstract level, model theoretic arguments hold that a given set of symbols in a theory can be mapped onto any number of sets of real-world objects — each set being a "model" of the theory — providing the interrelationships between the objects are the same. (Compare with symbol grounding.)
Anti-realism in science.
In philosophy of science, anti-realism applies chiefly to claims about the non-reality of "unobservable" entities such as electrons or genes, which are not detectable with human senses. For a brief discussion comparing such anti-realism to its opposite, realism, see (Okasha 2002, ch. 4). Ian Hacking (1999, p. 84) also uses the same definition. One prominent position in the philosophy of science is instrumentalism, which is a non-realist position. Non-realism takes a purely agnostic view towards the existence of unobservable entities: unobservable entity X serves simply as an instrument to aid in the success of theory Y. We need not determine the existence or non-existence of X. Some scientific anti-realists argue further, however, and deny that unobservables exist even as non-truth conditioned instruments.
Anti-realism in mathematics.
Realism in the philosophy of mathematics is the claim that mathematical entities such as number have a mind-independent existence. The main forms are empiricism, which associates numbers with concrete physical objects; and Platonism, according to which numbers are abstract, non-physical entities. 
The "epistemic argument" against Platonism has been made by Paul Benacerraf and Hartry Field. Platonism posits that mathematical objects are "abstract" entities. By general agreement, abstract entities cannot
interact causally with concrete, physical entities. ("the truth-values of our mathematical assertions depend on facts involving platonic entities that reside in a realm outside of space-time") Whilst our knowledge of concrete, physical objects is based on our ability to perceive them, and therefore to causally interact with them, there is no parallel account of how mathematicians come to have knowledge of abstract objects. ("An account of mathematical truth ..must be consistent with the possibility of mathematical knowledge"). Another way of making the point is that if the Platonic world were to disappear, it would make no difference to the ability of mathematicians to generate proofs, etc., which is already fully accountable in terms of physical processes in their brains.
Field developed his views into fictionalism. Benacerraf also developed the philosophy of mathematical structuralism, according to which there are no mathematical objects. Nonetheless, some versions of structuralism are compatible with some versions of realism.
The argument hinges on the idea that a satisfactory naturalistic account of thought processes in terms of brain processes can be given for mathematical reasoning along with everything else. One line of defense is to maintain that this is false, so that mathematical reasoning uses some special intuition that involves contact with the Platonic realm. A modern form of this argument is given by Sir Roger Penrose.
Another line of defense is to maintain that abstract objects are relevant to mathematical reasoning in a way that is non causal, and not analogous to perception. This argument is developed by Jerrold Katz in his book "Realistic Rationalism".
A more radical defense is to deny the separation of physical world and the platonic world, i.e. the mathematical universe hypothesis. In that case, a mathematician's knowledge of mathematics is one mathematical object making contact with another.

</doc>
<doc id="2174" url="https://en.wikipedia.org/wiki?curid=2174" title="Arsenal F.C.">
Arsenal F.C.

Arsenal Football Club is a professional football club based in Holloway, London, that plays in the Premier League, the top flight of English football. The club has won 12 FA Cups, the most of any English club, 13 League titles, two League Cups, 14 FA Community Shields, and one UEFA Cup Winners' Cup and Inter-Cities Fairs Cup.
Arsenal was the first club from the south of England to join The Football League, in 1893. They entered the First Division in 1904, and have since accumulated the second most points. Relegated only once, in 1913, they continue the longest streak in the top division. In the 1930s, Arsenal won five League Championships and two FA Cups, and another FA Cup and two Championships after the war. In 1970–71, they won their first League and FA Cup Double. Between 1988 and 2005, they won five League titles and five FA Cups, including two more Doubles. They completed the 20th century with the highest average league position.
Herbert Chapman won Arsenal's first national trophies, but died prematurely. The WM formation, floodlights, and shirt numbers have all been attributed to him. He added the white sleeves and brighter red to Arsenal's kit. Arsène Wenger has served the longest and won the most trophies. His teams set several English records: the longest win streak; the longest unbeaten run; and the only 38 match season unbeaten.
In 1886, Woolwich munitions workers founded the club as Dial Square. In 1913, the club crossed the city to Arsenal Stadium in Highbury. They became Tottenham Hotspur's nearest club, commencing the North London derby. In 2006, they moved to the Emirates Stadium in nearby Holloway. Arsenal earned €435.5m in 2014–15, with the Emirates Stadium generating the highest revenue in world football. Based on social media activity from 2014–15, Arsenal's fanbase is the fifth largest in the world. Forbes estimates the club was worth $1.3 billion in 2015.
History.
Arsenal Football Club was formed as Dial Square in 1886 by workers at the Royal Arsenal in Woolwich, south-east London, and were renamed Royal Arsenal shortly afterwards. The club was renamed again to Woolwich Arsenal after becoming a limited company in 1893. The club became the first southern member of the Football League in 1893, starting out in the Second Division, and won promotion to the First Division in 1904. The club's relative geographic isolation resulted in lower attendances than those of other clubs, which led to the club becoming mired in financial problems and effectively bankrupt by 1910, when they were taken over by businessmen Henry Norris and William Hall. Norris sought to move the club elsewhere, and in 1913, soon after relegation back to the Second Division, Arsenal moved to the new Arsenal Stadium in Highbury, north London; they dropped "Woolwich" from their name the following year. Arsenal only finished in fifth place in the second division during the last pre-war competitive season of 1914–15, but were nevertheless elected to rejoin the First Division when competitive football resumed in 1919–20, at the expense of local rivals Tottenham Hotspur. Some books have reported that this election to division 1 was achieved by dubious means.
Arsenal appointed Herbert Chapman as manager in 1925. Having already won the league twice with Huddersfield Town in 1923–24 and 1924–25 (see Seasons in English football), Chapman brought Arsenal their first period of major success. His revolutionary tactics and training, along with the signings of star players such as Alex James and Cliff Bastin, laid the foundations of the club's domination of English football in the 1930s. Under his guidance Arsenal won their first major trophies – victory in the 1930 FA Cup Final preceded two League Championships, in 1930–31 and 1932–33. In addition, Chapman was behind the 1932 renaming of the local London Underground station from "Gillespie Road" to "Arsenal", making it the only Tube station to be named specifically after a football club.
Chapman died suddenly of pneumonia in early 1934, leaving Joe Shaw and George Allison to carry on his successful work. Under their guidance, Arsenal won three more titles, in 1933–34, 1934–35 and 1937–38, and the 1936 FA Cup while also becoming known as the "Bank of England club." As key players retired, Arsenal had started to fade by the decade's end, and then the intervention of the Second World War meant competitive professional football in England was suspended.
After the war, Arsenal enjoyed a second period of success under Allison's successor Tom Whittaker, winning the league in 1947–48 and 1952–53, and the FA Cup in 1950. Their fortunes waned thereafter; unable to attract players of the same calibre as they had in the 1930s, the club spent most of the 1950s and 1960s in trophyless mediocrity. Even former England captain Billy Wright could not bring the club any success as manager, in a stint between 1962 and 1966.
Arsenal began winning silverware again with the surprise appointment of club physiotherapist Bertie Mee as manager in 1966. After losing two League Cup finals, they won their first European trophy, the 1969–70 Inter-Cities Fairs Cup. This was followed by an even greater triumph: their first League and FA Cup double in 1970–71. This marked a premature high point of the decade; the Double-winning side was soon broken up and the following decade was characterised by a series of near misses, starting with Arsenal finishing as FA Cup runners up in 1972, and First Division runners-up in 1972–73.
Terry Neill was recruited by the Arsenal board to replace Bertie Mee on 9 July 1976 and at the age of 34 he became the youngest Arsenal manager to date. With new signings like Malcolm Macdonald and Pat Jennings, and a crop of talent in the side such as Liam Brady and Frank Stapleton, the club enjoyed their best form since the 1971 double, reaching a trio of FA Cup finals (1978, 1979 and 1980), and losing the 1980 European Cup Winners' Cup Final on penalties. The club's only success during this time was a last-minute 3–2 victory over Manchester United in the 1979 FA Cup Final, widely regarded as a classic.
The return of former player George Graham as manager in 1986 brought a third period of glory. Arsenal won the League Cup in 1987, Graham's first season in charge. This was followed by a League title win in 1988–89, won with a last-minute goal in the final game of the season against fellow title challengers Liverpool. Graham's Arsenal won another title in 1990–91, losing only one match, won the FA Cup and League Cup double in 1993, and a second European trophy, the European Cup Winners' Cup, in 1994. Graham's reputation was tarnished when he was found to have taken kickbacks from agent Rune Hauge for signing certain players, and he was dismissed in 1995. His replacement, Bruce Rioch, lasted for only one season, leaving the club after a dispute with the board of directors.
The club's success in the late 1990s and first decade of the 21st century owed a great deal to the 1996 appointment of Arsène Wenger as manager. Wenger brought new tactics, a new training regime and several foreign players who complemented the existing English talent. Arsenal won a second League and Cup double in 1997–98 and a third in 2001–02. In addition, the club reached the final of the 1999–2000 UEFA Cup (losing on penalties to Galatasaray), were victorious in the 2003 and 2005 FA Cups, and won the Premier League in 2003–04 without losing a single match, an achievement which earned the side the nickname "The Invincibles". The feat came within a run of 49 league matches unbeaten from 7 May 2003 to 24 October 2004, a national record.
Arsenal finished in either first or second place in the league in eight of Wenger's first eleven seasons at the club, although on no occasion were they able to retain the title. As of July 2013, they were one of only five teams, the others being Manchester United, Blackburn Rovers, Chelsea, and Manchester City, to have won the Premier League since its formation in 1992. Arsenal had never progressed beyond the quarter-finals of the Champions League until 2005–06; in that season they became the first club from London in the competition's fifty-year history to reach the final, in which they were beaten 2–1 by Barcelona. In July 2006, they moved into the Emirates Stadium, after 93 years at Highbury.
Arsenal reached the final of the 2007 and 2011 League Cups, losing 2–1 to Chelsea and Birmingham City respectively. The club had not gained a major trophy since the 2005 FA Cup until 17 May 2014, when Arsenal beat Hull City in the 2014 FA Cup Final, coming back from a 2–0 deficit to win the match 3–2. This qualified them for the 2014 FA Community Shield where they would play Premier League champions Manchester City. They recorded a resounding 3–0 win in the game, winning their second trophy in three months. Nine months after their Community Shield triumph, Arsenal appeared in the FA Cup final for the second year in a row, thrashing Aston Villa 4–0 in the final and becoming the most successful club in the tournament's history with 12 titles. On 2 August 2015, Arsenal beat Chelsea 1–0 at Wembley Stadium to retain the Community Shield and earn their 14th Community Shield title.
Crest.
Unveiled in 1888, Royal Arsenal's first crest featured three cannon viewed from above, pointing northwards, similar to the coat of arms of the Metropolitan Borough of Woolwich (nowadays transferred to the coat of arms of the Royal Borough of Greenwich). These can sometimes be mistaken for chimneys, but the presence of a carved lion's head and a cascabel on each are clear indicators that they are cannon. This was dropped after the move to Highbury in 1913, only to be reinstated in 1922, when the club adopted a crest featuring a single cannon, pointing eastwards, with the club's nickname, "The Gunners", inscribed alongside it; this crest only lasted until 1925, when the cannon was reversed to point westward and its barrel slimmed down.
In 1949, the club unveiled a modernised crest featuring the same style of cannon below the club's name, set in blackletter, and above the coat of arms of the Metropolitan Borough of Islington and a scroll inscribed with the club's newly adopted Latin motto, "Victoria Concordia Crescit" "victory comes from harmony", coined by the club's programme editor Harry Homer. For the first time, the crest was rendered in colour, which varied slightly over the crest's lifespan, finally becoming red, gold and green. Because of the numerous revisions of the crest, Arsenal were unable to copyright it. Although the club had managed to register the crest as a trademark, and had fought (and eventually won) a long legal battle with a local street trader who sold "unofficial" Arsenal merchandise, Arsenal eventually sought a more comprehensive legal protection. Therefore, in 2002 they introduced a new crest featuring more modern curved lines and a simplified style, which was copyrightable. The cannon once again faces east and the club's name is written in a sans-serif typeface above the cannon. Green was replaced by dark blue. The new crest was criticised by some supporters; the Arsenal Independent Supporters' Association claimed that the club had ignored much of Arsenal's history and tradition with such a radical modern design, and that fans had not been properly consulted on the issue.
Until the 1960s, a badge was worn on the playing shirt only for high-profile matches such as FA Cup finals, usually in the form of a monogram of the club's initials in red on a white background.
The monogram theme was developed into an Art Deco-style badge on which the letters A and C framed a football rather than the letter F, the whole set within a hexagonal border. This early example of a corporate logo, introduced as part of Herbert Chapman's rebranding of the club in the 1930s, was used not only on Cup Final shirts but as a design feature throughout Highbury Stadium, including above the main entrance and inlaid in the floors. From 1967, a white cannon was regularly worn on the shirts, until replaced by the club crest, sometimes with the addition of the nickname "The Gunners", in the 1990s.
In the 2011–12 season, Arsenal celebrated their 125th year anniversary. The celebrations included a modified version of the current crest worn on their jerseys for the season. The crest was all white, surrounded by 15 oak leaves to the right and 15 laurel leaves to the left. The oak leaves represent the 15 founding members of the club who met at the Royal Oak pub. The 15 laurel leaves represent the design detail on the six pence pieces paid by the founding fathers to establish the club. The laurel leaves also represent strength. To complete the crest, 1886 and 2011 are shown on either sides of the motto "Forward" at the bottom of the crest.
Colours.
For much of Arsenal's history, their home colours have been bright red shirts with white sleeves and white shorts, though this has not always been the case. The choice of red is in recognition of a charitable donation from Nottingham Forest, soon after Arsenal's foundation in 1886. Two of Dial Square's founding members, Fred Beardsley and Morris Bates, were former Forest players who had moved to Woolwich for work. As they put together the first team in the area, no kit could be found, so Beardsley and Bates wrote home for help and received a set of kit and a ball. The shirt was redcurrant, a dark shade of red, and was worn with white shorts and socks with blue and white hoops.
In 1933, Herbert Chapman, wanting his players to be more distinctly dressed, updated the kit, adding white sleeves and changing the shade to a brighter pillar box red. Two possibilities have been suggested for the origin of the white sleeves. One story reports that Chapman noticed a supporter in the stands wearing a red sleeveless sweater over a white shirt; another was that he was inspired by a similar outfit worn by the cartoonist Tom Webster, with whom Chapman played golf.
Regardless of which story is true, the red and white shirts have come to define Arsenal and the team have worn the combination ever since, aside from two seasons. The first was 1966–67, when Arsenal wore all-red shirts; this proved unpopular and the white sleeves returned the following season. The second was 2005–06, the last season that Arsenal played at Highbury, when the team wore commemorative redcurrant shirts similar to those worn in 1913, their first season in the stadium; the club reverted to their normal colours at the start of the next season. In the 2008–09 season, Arsenal replaced the traditional all-white sleeves with red sleeves with a broad white stripe.
Arsenal's home colours have been the inspiration for at least three other clubs. In 1909, Sparta Prague adopted a dark red kit like the one Arsenal wore at the time; in 1938, Hibernian adopted the design of the Arsenal shirt sleeves in their own green and white strip. In 1920, Sporting Clube de Braga's manager returned from a game at Highbury and changed his team's green kit to a duplicate of Arsenal's red with white sleeves and shorts, giving rise to the team's nickname of "Os Arsenalistas". These teams still wear those designs to this day.
For many years Arsenal's away colours were white shirts and either black or white shorts. In the 1969–70 season, Arsenal introduced an away kit of yellow shirts with blue shorts. This kit was worn in the 1971 FA Cup Final as Arsenal beat Liverpool to secure the double for the first time in their history. Arsenal reached the FA Cup final again the following year wearing the red and white home strip and were beaten by Leeds United. Arsenal then competed in three consecutive FA Cup finals between 1978 and 1980 wearing their "lucky" yellow and blue strip, which remained the club's away strip until the release of a green and navy away kit in 1982–83. The following season, Arsenal returned to the yellow and blue scheme, albeit with a darker shade of blue than before.
When Nike took over from Adidas as Arsenal's kit provider in 1994, Arsenal's away colours were again changed to two-tone blue shirts and shorts. Since the advent of the lucrative replica kit market, the away kits have been changed regularly, with Arsenal usually releasing both away and third choice kits. During this period the designs have been either all blue designs, or variations on the traditional yellow and blue, such as the metallic gold and navy strip used in the 2001–02 season, the yellow and dark grey used from 2005 to 2007, and the yellow and maroon of 2010 to 2013.
As of 2009, the away kit is changed every season, and the outgoing away kit becomes the third-choice kit if a new home kit is being introduced in the same year.
Kit manufacturers and shirt sponsors.
Arsenal's shirts have been made by manufacturers including Bukta (from the 1930s until the early 1970s), Umbro (from the 1970s until 1986), Adidas (1986–1994), Nike (1994–2014), and Puma (from 2014). Like those of most other major football clubs, Arsenal's shirts have featured sponsors' logos since the 1980s; sponsors include JVC (1982–1999), Sega (1999–2002), O2 (2002–2006), and Emirates (from 2006).
Stadiums.
For most of their time in south-east London, Arsenal played at the Manor Ground in Plumstead, apart from a three-year period at the nearby Invicta Ground between 1890 and 1893. The Manor Ground was initially just a field, until the club installed stands and terracing for their first Football League match in September 1893. They played their home games there for the next twenty years (with two exceptions in the 1894–95 season), until the move to North London in 1913.
Widely referred to as Highbury, Arsenal Stadium was the club's home from September 1913 until May 2006. The original stadium was designed by the renowned football architect Archibald Leitch, and had a design common to many football grounds in the UK at the time, with a single covered stand and three open-air banks of terracing. The entire stadium was given a massive overhaul in the 1930s: new Art Deco West and East stands were constructed, opening in 1932 and 1936 respectively, and a roof was added to the North Bank terrace, which was bombed during the Second World War and not restored until 1954.
Highbury could hold more than 60,000 spectators at its peak, and had a capacity of 57,000 until the early 1990s. The Taylor Report and Premier League regulations obliged Arsenal to convert Highbury to an all-seater stadium in time for the 1993–94 season, thus reducing the capacity to 38,419 seated spectators. This capacity had to be reduced further during Champions League matches to accommodate additional advertising boards, so much so that for two seasons, from 1998 to 2000, Arsenal played Champions League home matches at Wembley, which could house more than 70,000 spectators.
Expansion of Highbury was restricted because the East Stand had been designated as a Grade II listed building and the other three stands were close to residential properties. These limitations prevented the club from maximising matchday revenue during the 1990s and first decade of the 21st century, putting them in danger of being left behind in the football boom of that time.
After considering various options, in 2000 Arsenal proposed building a new 60,361-capacity stadium at Ashburton Grove, since named the Emirates Stadium, about 500 metres south-west of Highbury.
The project was initially delayed by red tape and rising costs,
and construction was completed in July 2006, in time for the start of the 2006–07 season.
The stadium was named after its sponsors, the airline company Emirates, with whom the club signed the largest sponsorship deal in English football history, worth around £100 million;
some fans referred to the ground as Ashburton Grove, or the Grove, as they did not agree with corporate sponsorship of stadium names.
The stadium will be officially known as Emirates Stadium until at least 2028, and the airline will be the club's shirt sponsor until the end of the 2018–19 season. From the start of the 2010–11 season on, the stands of the stadium have been officially known as North Bank, East Stand, West Stand and Clock end.
Arsenal's players train at the Shenley Training Centre in Hertfordshire, a purpose-built facility which opened in 1999. Before that the club used facilities on a nearby site owned by the University College of London Students' Union. Until 1961 they had trained at Highbury. Arsenal's Academy under-18 teams play their home matches at Shenley, while the reserves play their games at Meadow Park, which is also the home of Boreham Wood F.C..
Supporters.
Arsenal fans often refer to themselves as "Gooners", the name derived from the team's nickname, "The Gunners". The fanbase is large and generally loyal, and virtually all home matches sell out; in 2007–08 Arsenal had the second-highest average League attendance for an English club (60,070, which was 99.5% of available capacity), and, as of 2015, the third-highest all-time average attendance. Arsenal have the seventh highest average attendance of European football clubs only behind Borussia Dortmund, FC Barcelona, Manchester United, Real Madrid, Bayern Munich, and Schalke. The club's location, adjoining wealthy areas such as Canonbury and Barnsbury, mixed areas such as Islington, Holloway, Highbury, and the adjacent London Borough of Camden, and largely working-class areas such as Finsbury Park and Stoke Newington, has meant that Arsenal's supporters have come from a variety of social classes.
Like all major English football clubs, Arsenal have a number of domestic supporters' clubs, including the Arsenal Football Supporters' Club, which works closely with the club, and the Arsenal Independent Supporters' Association, which maintains a more independent line. The Arsenal Supporters' Trust promotes greater participation in ownership of the club by fans. The club's supporters also publish fanzines such as "The Gooner", "Gunflash" and the satirical "Up The Arse!". In addition to the usual English football chants, supporters sing "One-Nil to the Arsenal" (to the tune of "Go West").
There have always been Arsenal supporters outside London, and since the advent of satellite television, a supporter's attachment to a football club has become less dependent on geography. Consequently, Arsenal have a significant number of fans from beyond London and all over the world; in 2007, 24 UK, 37 Irish and 49 other overseas supporters clubs were affiliated with the club. A 2011 report by SPORT+MARKT estimated Arsenal's global fanbase at 113 million. The club's social media activity was the fifth highest in world football during the 2014–15 season.
Arsenal's longest-running and deepest rivalry is with their nearest major neighbours, Tottenham Hotspur; matches between the two are referred to as North London derbies. Other rivalries within London include those with Chelsea, Fulham and West Ham United. In addition, Arsenal and Manchester United developed a strong on-pitch rivalry in the late 1980s, which intensified in recent years when both clubs were competing for the Premier League title – so much so that a 2003 online poll by the Football Fans Census listed Manchester United as Arsenal's biggest rivals, followed by Tottenham and Chelsea. A 2008 poll listed the Tottenham rivalry as more important.
Ownership and finances.
The largest shareholder on the Arsenal board is American sports tycoon Stan Kroenke. Kroenke first launched a bid for the club in April 2007, and faced competition for shares from Red and White Securities, which acquired its first shares off David Dein in August 2007. Red & White Securities was co-owned by Russian billionaire Alisher Usmanov and Iranian London-based financier Farhad Moshiri, though Usmanov bought Moshiri's stake in 2016. Kroenke came close to the 30% takeover threshold in November 2009, when he increased his holding to 18,594 shares (29.9%). In April 2011, Kroenke achieved a full takeover by purchasing the shareholdings of Nina Bracewell-Smith and Danny Fiszman, taking his shareholding to 62.89%. As of June 2015, Kroenke owns 41,698 shares (67.02%) and Red & White Securities own 18,695 shares (30.04%). Ivan Gazidis has been the club's Chief Executive since 2009.
Arsenal's parent company, Arsenal Holdings plc, operates as a non-quoted public limited company, whose ownership is considerably different from that of other football clubs. Only 62,217 shares in Arsenal have been issued, and they are not traded on a public exchange such as the FTSE or AIM; instead, they are traded relatively infrequently on the ICAP Securities and Derivatives Exchange, a specialist market. On 10 March 2016, a single share in Arsenal had a mid price of £15,670, which sets the club's market capitalisation value at approximately £975m. Most football clubs aren't listed on an exchange, which makes direct comparisons of their values difficult. Business magazine Forbes valued Arsenal as a whole at $1.3 billion in 2015. Consultants Brand Finance valued the club's brand and intangible assets at $703m in 2015, and consider Arsenal an AAA global brand. Research by the Henley Business School modelled the club's value at £1.118 billion in 2015, the second highest in the Premier League.
Arsenal's financial results for the 2014–15 season show group revenue of £344.5m, with a profit before tax of £24.7m. The footballing core of the business showed a revenue of £329.3m. The Deloitte Football Money League is a publication that homogenizes and compares clubs' annual revenue. They put Arsenal's footballing revenue at £331.3m (€435.5m), ranking Arsenal seventh among world football clubs. Arsenal and Deloitte both list the match day revenue generated by the Emirates Stadium as £100.4m, more than any other football stadium in the world.
In popular culture.
Arsenal have appeared in a number of media "firsts". On 22 January 1927, their match at Highbury against Sheffield United was the first English League match to be broadcast live on radio. A decade later, on 16 September 1937, an exhibition match between Arsenal's first team and the reserves was the first football match in the world to be televised live. Arsenal also featured in the first edition of the BBC's "Match of the Day", which screened highlights of their match against Liverpool at Anfield on 22 August 1964. BSkyB's coverage of Arsenal's January 2010 match against Manchester United was the first live public broadcast of a sports event on 3D television.
As one of the most successful teams in the country, Arsenal have often featured when football is depicted in the arts in Britain. They formed the backdrop to one of the earliest football-related films, "The Arsenal Stadium Mystery" (1939). The film centres on a friendly match between Arsenal and an amateur side, one of whose players is poisoned while playing. Many Arsenal players appeared as themselves and manager George Allison was given a speaking part. More recently, the book "Fever Pitch" by Nick Hornby was an autobiographical account of Hornby's life and relationship with football and Arsenal in particular. Published in 1992, it formed part of the revival and rehabilitation of football in British society during the 1990s. The book was twice adapted for the cinema – the 1997 British film focuses on Arsenal's 1988–89 title win, and a 2005 American version features a fan of baseball's Boston Red Sox.
Arsenal have often been stereotyped as a defensive and "boring" side, especially during the 1970s and 1980s; many comedians, such as Eric Morecambe, made jokes about this at the team's expense. The theme was repeated in the 1997 film "The Full Monty", in a scene where the lead actors move in a line and raise their hands, deliberately mimicking the Arsenal defence's offside trap, in an attempt to co-ordinate their striptease routine. Another film reference to the club's defence comes in the film "Plunkett & Macleane", in which two characters are named Dixon and Winterburn after Arsenal's long-serving full backs – the right-sided Lee Dixon and the left-sided Nigel Winterburn.
The 1991 television comedy sketch show "Harry Enfield & Chums" featured a sketch from the characters Mr Cholmondly-Warner and Grayson where the Arsenal team of 1933, featuring exaggerated parodies of fictitious amateur players take on the Liverpool team of 1991.
In the community.
In 1985, Arsenal founded a community scheme, "Arsenal in the Community", which offered sporting, social inclusion, educational and charitable projects. The club support a number of charitable causes directly and in 1992 established The Arsenal Charitable Trust, which by 2006 had raised more than £2 million for local causes. An ex-professional and celebrity football team associated with the club also raised money by playing charity matches.
In the 2009–10 season Arsenal announced that they had raised a record breaking £818,897 for the Great Ormond Street Hospital Children's Charity. The original target was £500,000.
Statistics and records.
Arsenal's tally of 13 League Championships is the third highest in English football, after Manchester United (20) and Liverpool (18),
and they were the first club to reach 8 League Championships. They hold the highest number of FA Cup trophies, 12. The club is one of only six clubs to have won the FA Cup twice in succession, in 2002 and 2003, and 2014 and 2015. Arsenal have achieved three League and FA Cup "Doubles" (in 1971, 1998 and 2002), a feat only previously achieved by Manchester United (in 1994, 1996 and 1999). They were the first side in English football to complete the FA Cup and League Cup double, in 1993. Arsenal were also the first London club to reach the final of the UEFA Champions League, in 2006, losing the final 2–1 to Barcelona.
Arsenal have one of the best top-flight records in history, having finished below fourteenth only seven times. The league wins and points they have accumulated are the second most in English top flight football. They have been in the top flight for the most consecutive seasons (90 as of 2015–16). Arsenal also have the highest average league finishing position for the 20th century, with an average league placement of 8.5.
Arsenal hold the record for the longest run of unbeaten League matches (49 between May 2003 and October 2004). This included all 38 matches of their title-winning 2003–04 season, when Arsenal became only the second club to finish a top-flight campaign unbeaten, after Preston North End (who played only 22 matches) in 1888–89. They also hold the record for the longest top flight win streak.
Arsenal set a Champions League record during the 2005–06 season by going ten matches without conceding a goal, beating the previous best of seven set by A.C. Milan. They went a record total stretch of 995 minutes without letting an opponent score; the streak ended in the final, when Samuel Eto'o scored a 76th-minute equaliser for Barcelona.
David O'Leary holds the record for Arsenal appearances, having played 722 first-team matches between 1975 and 1993. Fellow centre half and former captain Tony Adams comes second, having played 669 times. The record for a goalkeeper is held by David Seaman, with 564 appearances.
Thierry Henry is the club's top goalscorer with 228 goals in all competitions between 1999 and 2012, having surpassed Ian Wright's total of 185 in October 2005. Wright's record had stood since September 1997, when he overtook the longstanding total of 178 goals set by winger Cliff Bastin in 1939. Henry also holds the club record for goals scored in the League, with 175, a record that had been held by Bastin until February 2006.
Arsenal's record home attendance is 73,707, for a UEFA Champions League match against RC Lens on 25 November 1998 at Wembley Stadium, where the club formerly played home European matches because of the limits on Highbury's capacity. The record attendance for an Arsenal match at Highbury is 73,295, for a 0–0 draw against Sunderland on 9 March 1935, while that at Emirates Stadium is 60,161, for a 2–2 draw with Manchester United on 3 November 2007.
Managers.
There have been eighteen permanent and five caretaker managers of Arsenal since the appointment of the club's first professional manager, Thomas Mitchell in 1897. The club's longest-serving manager, in terms of both length of tenure and number of games overseen, is Arsène Wenger, who was appointed in 1996. Wenger is also Arsenal's only manager from outside the United Kingdom. Two Arsenal managers have died in the job – Herbert Chapman and Tom Whittaker.
Honours.
Arsenal F.C., statto.com, Bernard Joy, Michael Slade, the Rec.Sport.Soccer Statistics Foundation, and the AISA Arsenal History Society collectively cover all listed trophies.
Seasons in bold are Double winning seasons, when the club won two FA tournaments or an FA tournament and a league title. The "2003–04" season was the only 38-match season unbeaten in English football history.
Arsenal Ladies.
Arsenal Ladies are the women's football club affiliated to Arsenal. Founded in 1987, they turned semi-professional in 2002 and are managed by Clare Wheatley. Arsenal Ladies are the most successful team in English women's football. In the 2008–09 season, they won all three major English trophies – the FA Women's Premier League, FA Women's Cup and FA Women's Premier League Cup, and, as of 2009, were the only English side to have won the UEFA Women's Cup, having done so in the 2006–07 season as part of a unique quadruple. The men's and women's clubs are formally separate entities but have quite close ties; Arsenal Ladies are entitled to play once a season at the Emirates Stadium, though they usually play their home matches at Boreham Wood.

</doc>
<doc id="2175" url="https://en.wikipedia.org/wiki?curid=2175" title="Cuisine of the United States">
Cuisine of the United States

The cuisine of the United States reflects its history. The European colonization of the Americas yielded the introduction of a number of ingredients and cooking styles to the latter. The various styles continued expanding well into the 19th and 20th centuries, proportional to the influx of immigrants from many foreign nations; such influx developed a rich diversity in food preparation throughout the country.
Early Native Americans utilized a number of cooking methods in early American Cuisine that have been blended with early European cooking methods to form the basis of American Cuisine. When the colonists came to the colonies, they farmed animals for clothing and meat in a similar fashion to what they had done in Europe. They had cuisine similar to their previous British cuisine. The American colonial diet varied depending on the settled region in which someone lived. Commonly hunted game included deer, bear, buffalo, and wild turkey. A number of fats and oils made from animals served to cook much of the colonial foods. Prior to the Revolution, New Englanders consumed large quantities of rum and beer, as maritime trade provided them relatively easy access to the goods needed to produce these items: rum was the distilled spirit of choice, as the main ingredient, molasses, was readily available from trade with the West Indies. In comparison to the northern colonies, the southern colonies were quite diverse in their agricultural diet and did not have a central region of culture.
During the 18th and 19th centuries, Americans developed many new foods. During the Progressive Era (1890s–1920s) food production and presentation became more industrialized. One characteristic of American cooking is the fusion of multiple ethnic or regional approaches into completely new cooking styles. A wave of celebrity chefs began with Julia Child and Graham Kerr in the 1970s, with many more following after the rise of cable channels such as Food Network.
History.
Pre-colonial cuisine.
Seafood.
Seafood in the United States originated with the Native Americans, who often ate cod, lemon sole, flounder, herring, halibut, sturgeon, smelt, drum on the East Coast, and olachen and salmon on the West Coast. Whale was hunted by Native Americans off the Northwest coast, especially by the Makah, and used for their meat and oil. Seal and walrus were also eaten, in addition to eel from New York's Finger Lakes region. Catfish was also popular amongst native peoples, including the Modocs. Crustacean included shrimp, lobster, crayfish, and dungeness crabs in the Northwest and blue crabs in the East. Other shellfish include abalone and geoduck on the West Coast, while on the East Coast the surf clam, quahog, and the soft-shell clam. Oysters were eaten on both shores, as were mussels and periwinkles.
Cooking methods.
Early Native Americans utilized a number of cooking methods in early American Cuisine that have been blended with early European cooking methods to form the basis of American Cuisine. Grilling meats was common. Spit roasting over a pit fire was common as well. Vegetables, especially root vegetables were often cooked directly in the ashes of the fire. As early Native Americans lacked pottery that could be used directly over a fire, they developed a technique which has caused many anthropologists to call them "Stone Boilers". They would heat rocks directly in a fire and then add the rocks to a pot filled with water until it came to a boil so that it would cook the meat or vegetables in the boiling water. In what is now the Southwestern United States, they also created adobe ovens called hornos to bake items such as cornmeal bread. Other parts of America, made ovens of dug pits, these pits were also used to steam foods by adding heated rocks or embers and then seaweed or corn husks placed on top to steam fish and shellfish as well as vegetables; potatoes would be added while still in skin and corn while in-husk, this would later be referred to as a clambake by the colonists.
Colonial period.
When the colonists came to Virginia, Massachusetts, or any of the other English colonies on the eastern seaboard of North America, their initial attempts at survival included planting crops familiar to them from back home in England. In the same way, they farmed animals for clothing and meat in a similar fashion. Through hardships and eventual establishment of trade with Britain, the West Indies and other regions, the colonists were able to establish themselves in the American colonies with a cuisine similar to their previous British cuisine. There were some exceptions to the diet, such as local vegetation and animals, but the colonists attempted to use these items in the same fashion as they had their equivalents or ignore them entirely if they could. The manner of cooking for the American colonists followed along the line of British cookery up until the Revolution. The British sentiment followed in the cookbooks brought to the New World as well.
In 1796, the first American cookbook was published.
There was a general disdain for French cookery, even with the French Huguenots in South Carolina and French-Canadians. One of the cookbooks that proliferated in the colonies was The Art of Cookery Made Plain and Easy written by Hannah Glasse, wrote of disdain for the French style of cookery, referring to "the blind folly of this age that would rather be imposed on by a French booby, than give encouragement to a good English cook!" Of the French recipes she does add to the text she speaks out flagrantly against the dishes as she "… think it an odd jumble of trash." Reinforcing the anti-French sentiment was the French and Indian War from 1754–1764. This created a large anxiety against the French, which influenced the English to either deport many of the French, or as in the case of many Acadians from Nova Scotia, they forcibly relocated to Louisiana. The Acadian French did create a large French influence in the diet of those settled in Louisiana, but had little or no influence outside of Louisiana - except among the Acadian Francophones who settled eastern Maine at the same time they colonised New Brunswick.
Common ingredients.
The American colonial diet varied depending on the settled region in which someone lived. Local cuisine patterns had established by the mid-18th century. The New England colonies were extremely similar in their dietary habits to those that many of them had brought from England. A striking difference for the colonists in New England compared to other regions was seasonality. While in the southern colonies, they could farm almost year round, in the northern colonies, the growing seasons were very restricted. In addition, colonists' close proximity to the ocean gave them a bounty of fresh fish to add to their diet, especially in the northern colonies. Wheat, however, the grain used to bake bread back in England was almost impossible to grow, and imports of wheat were far from cost productive. Substitutes in cases such as this included cornmeal. The Johnnycake was a poor substitute to some for wheaten bread, but acceptance by both the northern and southern colonies seems evident.
As many of the New Englanders were originally from England, game hunting was useful when they immigrated to the New World. Many of the northern colonists depended upon their ability to hunt, or upon others from whom they could purchase game. Hunting was the preferred method of protein consumption (as opposed to animal husbandry, which required much more work to defend the kept animals against Native Americans or the French).
Livestock and game.
Commonly hunted game included deer, bear, buffalo, and wild turkey. The larger muscles of the animals were roasted and served with currant sauce, while the other smaller portions went into soups, stews, sausages, pies, and pastries. In addition to game, colonists' protein intake was supplemented by mutton. The Spanish in Florida originally introduced sheep to the New World, but this development never quite reached the North, and there they were introduced by the Dutch and English. The keeping of sheep was a result of the English non-practice of animal husbandry. The animals provided wool when young and mutton upon maturity after wool production was no longer desirable. The forage-based diet for sheep that prevailed in the Colonies produced a characteristically strong, gamy flavor and a tougher consistency, which required aging and slow cooking to tenderize.
Fats and oils.
A number of fats and oils made from animals served to cook much of the colonial foods. Many homes had a sack made of deerskin filled with bear oil for cooking, while solidified bear fat resembled shortening. Rendered pork fat made the most popular cooking medium, especially from the cooking of bacon. Pork fat was used more often in the southern colonies than the northern colonies as the Spanish introduced pigs earlier to the South. The colonists enjoyed butter in cooking as well, but it was rare prior to the American Revolution, as cattle were not yet plentiful.
Alcoholic drinks.
Prior to the Revolution, New Englanders consumed large quantities of rum and beer, as maritime trade provided them relatively easy access to the goods needed to produce these items: Rum was the distilled spirit of choice, as the main ingredient, molasses, was readily available from trade with the West Indies. Further into the interior, however, one would often find colonists consuming whiskey, as they did not have similar access to sugar cane. They did have ready access to corn and rye, which they used to produce their whiskey. However, until the Revolution, many considered whiskey to be a coarse alcohol unfit for human consumption, as many believed that it caused the poor to become raucous and unkempt drunkards. In addition to these alcohol-based products produced in America, imports were seen on merchant shelves, including wine and brandy.
Southern variations.
In comparison to the northern colonies, the Southern Colonies were quite diverse in their agricultural diet and did not have a central region of culture. The uplands and the lowlands made up the two main parts of the southern colonies. The slaves and poor of the south often ate a similar diet, which consisted of many of the indigenous New World crops. Salted or smoked pork often supplement the vegetable diet. Rural poor often ate squirrel, possum, rabbit and other woodland animals. Those on the "rice coast" often ate ample amounts of rice, while the grain for the rest of the southern poor and slaves was cornmeal used in breads and porridges. Wheat was not an option for most of those who lived in the southern colonies.
The diet of the uplands often included cabbage, string beans, and white potatoes, while most avoided sweet potatoes and peanuts at the time. Those who could grow or afford wheat often had biscuits as part of their breakfast, along with healthy portions of pork. Salted pork was a staple of any meal, as it was used in the preparations of vegetables for flavor, in addition to being eaten directly as a protein.
The lowlands, which included much of the Acadian French regions of Louisiana and the surrounding area, included a varied diet heavily influenced by Africans and Caribbeans, rather than just the French. As such, rice played a large part of the diet as it played a large part of the diets of the Africans and Caribbean. In addition, unlike the uplands, the lowlands subsistence of protein came mostly from coastal seafood and game meats. Much of the diet involved the use of peppers, as it still does today. Interestingly, although the English had an inherent disdain for French foodways, as well as many of the native foodstuff of the colonies, the French had no such disdain for the indigenous foodstuffs. In fact, they had a vast appreciation for the native ingredients and dishes.
Post-colonial cuisine.
During the 18th and 19th centuries, Americans developed many new foods. Some, such as Rocky Mountain oysters, stayed regional; some spread throughout the nation but with little international appeal, such as peanut butter (a core ingredient of the famous peanut butter and jelly sandwich); and some spread throughout the world, such as popcorn, Coca-Cola and its competitors, fried chicken, cornbread, unleavened muffins such as the poppyseed muffin, and brownies.
Modern cuisine.
During the Progressive Era (1890s–1920s) food production and presentation became more industrialized. Major railroads featured upscale cuisine in their dining cars. Restaurant chains emerged with standardized decor and menus, most famously the Fred Harvey restaurants along the route of the Sante Fe Railroad in the Southwest.
At the universities, nutritionists and home economists taught a new scientific approach to food. During World War I the Progressives' moral advice about food conservation was emphasized in large-scale state and federal programs designed to educate housewives. Large-scale foreign aid during and after the war brought American standards to Europe.
Newspapers and magazines ran recipe columns, aided by research from corporate kitchens, which were major food manufactures like General Mills, Campbell's, and Kraft Foods. One characteristic of American cooking is the fusion of multiple ethnic or regional approaches into completely new cooking styles. For example, spaghetti is Italian, while hot dogs are German; a popular meal, especially among young children, is spaghetti containing slices of hot dogs. The fact that most Americans don't really even see this as a fusion recipe shows just how common this trend is. Since the 1960s Asian cooking has played a particularly large role in American fusion cuisine.
New York City is home to a diverse and cosmopolitan demographic, and since the nineteenth century, the city's world class chefs created complicated dishes with rich ingredients like Lobster Newberg, waldorf salad, vichyssoise, eggs benedict, and the New York strip steak out of a need to entertain and impress consumers in expensive bygone restaurants like Delmonico's and still standing establishments like the Waldorf-Astoria Hotel.
Some dishes that are typically considered American have their origins in other countries. American cooks and chefs have substantially altered these dishes over the years, to the degree that the dishes now enjoyed around the world are considered to be American. Hot dogs and hamburgers are both based on traditional German dishes, but in their modern popular form they can be reasonably considered American dishes.
Pizza is based on the traditional Italian dish, brought by Italian immigrants to the United States, but varies highly in style based on the region of development since its arrival. For example, "Chicago" style has focus on a thicker, taller crust, whereas a "New York Slice" is known to have a much thinner crust which can be folded. These different types of pizza can be advertised throughout the country and are generally recognizable and well-known, with some restaurants going so far as to import New York City tap water from a thousand or more miles away to recreate the signature style in other regions.
Many companies in the American food industry developed new products requiring minimal preparation, such as frozen entrees. Many of these recipes have become very popular. For example, the General Mills "Betty Crocker's Cookbook", first published in 1950, was a popular book in American homes.
A wave of celebrity chefs began with Julia Child and Graham Kerr in the 1970s, with many more following after the rise of cable channels like Food Network. By the beginning of the 21st century regional variations in consumption of meat began to reduce, as more meat was consumed overall. Saying they eat too much protein, the "2015–2020 Dietary Guidelines for Americans" asked men and teenage boys to increase their consumption of underconsumed foods such as vegetables.
New American.
During the 1980s, upscale restaurants introduced a mixing of cuisines that contain Americanized styles of cooking with foreign elements commonly referred as New American cuisine. New American cuisine refers to a type of fusion cuisine which assimilates flavors from the melting pot of traditional American cooking techniques mixed with flavors from other cultures and sometimes molecular gastronomy components.
Regional cuisines.
Generally speaking, in the present day 21st century, the modern cuisine of the United States is very much regional in nature. Excluding Alaska and Hawaii the terrain spans 3,000 miles West to East and more than a thousand North to South.
Northeast.
New England.
New England is a Northeastern region of the United States, including the six states of Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, and Vermont, with its cultural capital Boston, founded in 1630. The Native American cuisine became part of the cookery style that the early colonists brought with them. Tribes like the Nipmuck, Wampanoag, and other Algonquian cultures were noted for slashing and burning areas to create meadows and bogs that would attract animals like moose and deer, but also encourage the growth of plants like black raspberries, blueberries, and cranberries. In the forest they would have collected nuts of species like the shagbark hickory, American hazel, and American chestnuts and fruits like wild grapes and black cherries. All of these eventually showed up in the kitchens of colonial New England women and many were sent back to England and other portions of Europe to be catalogued by scientists, collectors, and horticulturalists.
The style of New England cookery originated from its colonial roots, that is to say practical, frugal and willing to eat anything other than what they were used to from their British roots. Most of the initial colonists came from East Anglia in England, with other groups following them over the ages like francophone regions of Canada (this was especially true of Northern New England, where there are still many speakers of a dialect of French), from Ireland, from Southern Italy, and most recently from Haiti, Brazil, the Dominican Republic, and Portugal. The oldest forms of the cuisine date to the early 17th century and in the case of Massachusetts, out of the entire country only the state of Virginia can claim recipes that are older. Most of the cuisine started with one-pot cookery, which resulted in such dishes as succotash, chowder, baked beans, and others. Starches are fairly simple, and typically encompass just a handful of classics like potatoes and cornmeal, and a few native breads like Anadama bread, johnnycakes, bulkie rolls, Parker house rolls, popovers, and New England brown bread. This region is fairly conservative with its spices, but typical spices include nutmeg, ginger, cinnamon, cloves, and allspice, especially in desserts, and for savory foods, thyme, black pepper, sea salt, and sage. Typical condiments include maple syrup, grown from the native sugar maple, molasses, and the famous cranberry sauce.
New England is noted for having a heavy emphasis on seafood, a legacy inherited from coastal tribes like the Wampanoag and Narragansett, who equally used the rich fishing banks offshore for sustenance. Favorite fish include cod, salmon, winter flounder, haddock, striped bass, pollock, hake, bluefish, and, in southern New England, tautog. All of these are prepared numerous ways, such as frying cod for fish fingers, grilling bluefish over hot coals for summertime, smoking salmon or serving a whole poached one chilled for feasts with a dill sauce, or, on cold winter nights, serving haddock baked in casserole dish with a creamy sauce and crumbled breadcrumbs as a top so it forms a crust. Clam cakes, a savory fritter based on chopped clams, are a specialty of Rhode Island. Farther inland, brook trout, largemouth bass, and herring are sought after, especially in the rivers and icy finger lakes in upper New England.
Meat is present though not as prominent, and typically is either stewed in dishes like Yankee pot roast and New England boiled dinner or braised, as in a picnic ham; these dishes suit the weather better as summers are humid and hot but winters are raw and cold, getting below 0 °C for most of the winter and only just above it by March. The roasting of whole turkeys began here as a centerpiece for large American banquets, and like all other East Coast tribes, the Native American tribes of New England prized wild turkeys as a source of sustenance and later Anglophone settlers were enamored of cooking them using methods they knew from Europe: often that meant trussing the bird and spinning it on a string or spit roasting. Today turkey meat is a key ingredient in soups, and also a favorite in several sandwiches like the Pilgrim (sandwich). For lunch, hot roast beef is sometimes chopped finely into small pieces and put on a roll with salami and American or provolone cheese to make a steak bomb. Bacon is often maple cured, and it is often the drippings from this bacon that are an ingredient in corn chowder. Veal consumption was prevalent in the North Atlantic States prior to World War II. A variety of linguiça is favored as a breakfast food, brought with Portuguese fisherman and Brazilian immigrants. In contrast with some parts of the United States, lamb (although less so mutton or goat) is a popular roasted or grilled meat across diverse groups in New England. Dairy farming and its resultant products figure strongly on the ingredient list, and homemade ice cream is a summertime staple of the region: it was a small seasonal roadside stand in Vermont that eventually became the world famous Ben and Jerry's ice cream. Vermont in particular is famous for producing farmhouse style cheeses, especially a type of cheddar. The recipe goes all the way back to colonial times when English settlers brought the recipe with them from England and found the rocky landscape eminently suitable to making the cheese. Today Vermont has more artisanal cheese makers per capita than any other state, and diversity is such that interest in goat's milk cheeses has become prominent.
Crustaceans and mollusks are also an essential ingredient in the regional cookery. Maine is noted for harvesting peekytoe crab and Jonah crab and making crab bisques, based on cream with 35% milkfat, and crabcakes out of them, and often they appear on the menu as far south as to be out of region in New York City, where they are sold to four star restaurants. Squid are heavily fished for and eaten as fried calamari, and often are an ingredient in Italian American cooking in this region. Whelks are eaten in salad, and most famous of all is the lobster, which is indigenous to the coastal waters of the region and are a feature of many dishes, baked, boiled, roasted, and steamed, or simply eaten as a sandwich, chilled with mayonnaise and chopped celery in Maine and Massachusetts, or slathered with melted butter on Long Island and in Connecticut.
Shellfish of all sorts are part of the diet, and shellfish of the coastal regions include little neck clams, sea scallops, blue mussels, oysters, soft shell clams and razor shell clams. Much of this shellfish contributes to New England tradition, the clambake. The clambake as known today is a colonial interpretation of an American Indian tradition. In summer, oysters and clams are dipped in batter and fried, often served in a basket with french fries, or commonly on a wheaten bun as a clam roll. Oysters are otherwise eaten chilled on a bed of crushed ice on the half shell with mignonette sauce, and are often branded on where they were harvested. Large quahogs are stuffed with breadcrumbs and seasoning and baked in their shells, and smaller ones often find their way into clam chowder. Other preparations include clams casino, clams on the half shell served stuffed with herbs like oregano and streaky bacon.
The fruits of the region include the "Vitis labrusca" grapes used in grape juice made by companies such as Welch's, along with jelly, Kosher wine by companies like Mogen David and Manischewitz along with other wineries that make higher quality wines. Apples from New England include the traditional varieties Baldwin, Lady, Mother, Pomme Grise, Porter, Roxbury Russet, Wright, Sops of Wine, Hightop Sweet, Peck's Pleasant, Titus Pippin, Westfield-Seek-No-Further, and Duchess of Oldenburg. Beach plums a small native species with fruits the size of a pinball, are sought after in summer to make into a jam. Cranberries are another fruit indigenous to the region, often collected in autumn in huge flooded bogs. Thereafter they are juiced so they can be drunk fresh for breakfast, or dried and incorporated into salads. Winter squashes like pumpkin and butternut squashes have been a staple for generations owing to their ability to keep for long periods over icy New England winters and being an excellent source of beta carotene; in summer, they are replaced with pattypan and zucchini, the latter brought to the region by immigrants from Southern Italy a century ago. Blueberries are a very common summertime treat owing to them being an important crop, and find their way into muffins, pies and pancakes. Typical favorite desserts are quite diverse, and encompass hasty pudding, blueberry pie, whoopie pies, Boston cream pie, pumpkin pie, Joe Frogger cookies, hand crafted ice cream, Hermit cookies, and most famous of all, the chocolate chip cookie, invented in Massachusetts in the 1930s.
Southern New England, particularly along the coast, shares many specialties with the Mid-Atlantic, including especially dishes from Jewish and Italian-American cuisine. Coastal Connecticut is known for distinctive kinds of pizza, locally called apizza, differing in texture (thin and slightly blackened) and toppings (such as clams) from pizza further south in the so-called pizza belt, which stretches from New Haven southward through New York, New Jersey, and into Maryland.
Mid-Atlantic.
The mid-Atlantic states comprise the states of New York, New Jersey, Delaware, Pennsylvania, and Northern Maryland. The oldest major settlement in this area of the country is found in the most populous city in the nation, New York City, founded in 1653 by the Dutch and today this city is a major cultural capital of the United States. The influences on cuisine in this region are extremely eclectic owing to the fact that it has been and continues to be a gateway for international culture as well as a gateway for new immigrants. Going back to colonial times, each new group has left their mark on homegrown cuisine and in turn the cities in this region disperse trends to the wider United States. In addition to importing and trading the finest specialty foods from all over the world, cities like New York and Philadelphia have had the past influence of Dutch, Italian, German, Irish, British, and Jewish cuisines, and that continues to this day. Baltimore has become the crossroads between North and South, a distinction it has held since the end of the Civil War.
A global power city, New York City is internationally known for its extremely diverse and cosmopolitan dining scene and possesses the entire world spectrum of dining options within its city limits. Some of the most exclusive and prestigious restaurants and nightclubs in the world are headquartered in New York City and compete fiercely for good reviews in the Food and Dining section of The New York Times, online guides, and Zagat's, the last of which is widely considered the premier American dining guide, published yearly and headquartered in New York City. Many of the more complicated dishes with rich ingredients like Lobster Newberg, waldorf salad, vichyssoise, eggs benedict, and the New York strip steak were born out of a need to entertain and impress the well to do in expensive bygone restaurants like Delmonico's and still standing establishments like the Waldorf-Astoria Hotel, and today that tradition remains alive as some of the most expensive and exclusive restaurants in the country are found in this region. Modern commercial American cream cheese was developed in 1872, when William Lawrence, from Chester, New York, while looking for a way to recreate the soft, French cheese Neufchâtel, accidentally came up with a way of making an "unripened cheese" that is heavier and creamier; other dairymen came up with similar creations independently.
Since the first reference to an alcoholic mixed drink called a cocktail comes from New York State in 1803, it is thus not a surprise that there have been many cocktails invented in New York and the surrounding environs. Even today New York City bars are noted for being highly influential in making national trends. Cosmopolitans, Long Island iced teas, Manhattans, Rob roys, Tom Collins, Aviations, and Greyhounds were all invented in New York bars, and the gin martini was popularized in New York in speakeasies during the 1920s, as evidenced by its appearance in the works of New Yorker and American writer F. Scott Fitzgerald. Like its neighbor Philadelphia, many rare and unusual liquors and liqueurs often find their way into a mixologist's cupboard or restaurant wine list. New York State is the third most productive area in the country for wine grapes, just behind the more famous California and Washington. It has AVA's near the Finger Lakes, the Catskills, and Long Island, and in the Hudson Valley has the second most productive area in the country for growing apples, making it a center for hard cider production, just like New England. Pennsylvania has been growing rye since Germans began to emigrate to the area at the end of the 17th century and required a grain they knew from Germany. Therefore, overall it is not unusual to find New York grown Gewürtztraminer and Riesling, Pennsylvania rye whiskey, or marques of locally produced ciders like Original Sin on the same menu.
Since their formative years, New York City, Philadelphia, and Baltimore have welcomed immigrants of every kind to their shores, and all three have been an important gateway through which new citizens to the general United States arrive. Traditionally natives have eaten cheek to jowl with newcomers for centuries as the newcomers would open new restaurants and small businesses and all the different groups would interact. Even in colonial days this region was a very diverse mosaic of peoples, as settlers from Switzerland, Wales, England, Ulster, Wallonia, Holland, Gelderland, the British Channel Islands, and Sweden sought their fortune in this region. This is very evident in many signature dishes and local foods, all of which have evolved to become American dishes in their own right. The original Dutch settlers of New York brought recipes they knew and understood from the Netherlands and their mark on local cuisine is still apparent today: in many quarters of New York their version of apple pie with a streusel top is still baked, while originating in the colony of New Amsterdam their predilection for waffles in time evolved into the American national recipe and forms part of a New York City brunch, and they also made coleslaw, originally a Dutch salad, but today accented with the later 18th century introduction of mayonnaise. The internationally famous American doughnut began its life originally as a New York pastry that arrived in the 18th century as the Dutch olykoek.
Crab cakes were once a kind of English croquette, but over time as spices have been added they and the Maryland crab feast became two of Baltimore's signature dishes; fishing for the blue crab is a favorite summer pastime in the waters off Maryland, New Jersey, Long Island, and Delaware where they may grace the table at summer picnics . Other mainstays of the region have been present since the early years of American history, like oysters from Cape May, the Chesapeake Bay, and Long Island, and lobster and tuna from the coastal waters found in New York and New Jersey, which are exported to the major cities as an expensive delicacy or a favorite locavore's quarry at the multitude of farmer's markets, very popular in this region. Philadelphia Pepper Pot, a tripe stew was originally a British dish but today is a classic of home cooking in Pennsylvania alongside bookbinder soup, a type of turtle soup.
In the winter, New York City pushcarts sell roasted chestnuts, a delicacy dating back to English Christmas traditions, and it was in New York and Pennsylvania that the earliest Christmas cookies were introduced: Germans introduced crunchy molasses based gingerbread and sugar cookies in Pennsylvania, and the Dutch introduced cinnamon based cookies, all of which have become part of the traditional Christmas meal.Scrapple was originally a type of savory pudding that early Pennsylvania Germans made to preserve the offal of a pig slaughter. The Philadelphia soft pretzel was originally brought to Eastern Pennsylvania in the early 18th century, and later, 19th century immigrants sold them to the masses from pushcarts to make them the city's best-known bread product, having evolved into its own unique recipe.
After the 1820s, new groups began to arrive and the character of the region began to change. There had been some Irish from Ulster prior to 1820, however largely they had been Protestants with somewhat different culture and (often) a different language than the explosion of emigrants that came to Castle Garden and Locust Point in Baltimore in their masses starting in the 1840s. The Irish arrived in America in a rather woeful state, as Ireland at the time was often plagued by some of the worst poverty in Europe and often heavy disenfranchisement among the masses. In addition, they were the first to face challenges other groups did not have: they were the first large wave of Catholics. For example, Catholic canon law mandated until the 1960s that all Catholics were forbidden from eating red meat on Fridays and during Lent. Unsurprisingly, many Irishmen found their fortunes working as longshoremen, which would have given their families access to fish and shellfish whenever a fisherman made berth, which was frequent on the busy docks of Baltimore and New York. Though there had been some activity in Baltimore in founding a see earlier by the 
Carroltons, the Irish were the first major wave of Catholic worship in this region, and that meant bishops and cardinals sending away to Europe for wine: part of the Catholic mass includes every parishioner taking a sip of wine from the chalice as part of the Eucharist. Taverns had existed prior to their emigration to America in the region, though the Irish brought their particular brand of pub culture and founded some of the first saloons and bars that served stout. The Irish were the first immigrant group to arrive in this region in massive numbers, and these immigrants also founded some of the earliest saloons and bars in this region, of which McSorley's is an example.
Immigrants from Southern Europe, namely Sicily, Campania, Lazio, and Calabria, appeared between 1880-1960 in New York, New Jersey, Pennsylvania, and Eastern Maryland hoping to escape the extreme poverty and corruption endemic to Italy; typically they were employed in manual labor or factory work but it is because of them that dishes like spaghetti with meatballs, New York–style pizza, calzones, and baked ziti exist, and Americans of today are very familiar with semolina based pasta noodles.
New York–style hot dogs came about with German speaking emigrants from Austria and Germany, particularly with the frankfurter sausage and the smaller wiener sausage. Today, the New York–style hot dog with sauerkraut, mustard, and the optional cucumber pickle relish is such a part of the local fabric, that it is one of the favorite comestibles of New York City. Hot dogs are a typical street food sold year round in all by the most inclement weather from thousands of pushcarts. As with all other stadiums in Major League Baseball they are an essential for New York Yankees and the New York Mets games though it is the local style of preparation that predominates without exception. Hot dogs are also the focus of a televised eating contest on the Fourth of July in Coney Island, at Nathan's Famous, one of the earliest hot dog stands opened in the United States in 1916.
A summertime treat, Italian ice, began its life as a lemon flavored penny lick brought to Philadelphia by Italians; its Hispanic counterpart, piragua, is a common and evolving shaved ice treat brought to New York City by Puerto Ricans in the 1930s. Unlike the original dish which included flavors like tamarind, mango, coconut, piragua is evolving to include flavors like grape, a fruit not grown in Puerto Rico.
Taylor ham, a meat delicacy of New Jersey, first appeared around the time of the Civil War and today is often served for breakfast with eggs and cheese on a kaiser roll, the bread upon which this is served was brought to the area by Austrians in the second half of the nineteenth century and is a very common roll for sandwiches at lunchtime, usually tipped with poppyseeds. This breakfast meat is generally known as pork roll in southern New Jersey and Philadelphia, and Taylor ham in northern New Jersey.
Other dishes came about during the early 20th century and have much to do with delicatessen fare, set up largely by Jewish immigrants from Eastern Europe who came to America incredibly poor. Most often they were completely unable to partake in the outdoor food markets that the general population utilized as most of the food for sale was not kosher. The influence of European Jewry before their destruction in the Holocaust on modern mid Atlantic cooking remains extremely strong and reinforced by their many descendants in the region. American-style pickles were brought by Polish Jews, now a common addition to hamburgers and sandwiches, and Hungarian Jews brought a recipe for almond horns that now is a common regional cookie, diverting from the original recipe in dipping the ends in dark chocolate. New York–style cheesecake has copious amounts of cream and eggs because animal rennet is not kosher and thus could not be sold to a large number of the deli's clientele. New York inherited its bagels and bialys from Jews, as well as Challah bread. Pastrami first entered the country via Romanian Jews, and is a feature of many sandwiches, often eaten on marble rye, a bread that was born in the mid Atlantic. Whitefish salad, lox, and matzoh ball soup are now standard fare made to order at local diners and delicatessens, but started their life as foods that made up a strict dietary code.
Like other groups before them, many of their dishes passed into the mainstream enough so that they became part of diner fare by the end of the 20th century, a type of restaurant that is now more numerous in this region than any other and formerly the subject matter of artist Edward Hopper. In the past this sort of establishment was the haven of the short order cook grilling or frying simple foods for the working man. Today typical service would include regional staples like beef on weck, manhattan clam chowder, the club sandwich, Buffalo wings, Philadelphia cheesesteak, the black and white cookie, shoofly pie, snapper soup, Smith Island cake, grape pie, milkshakes, and the egg cream, a vanilla or chocolate fountain drink with a frothy top and fizzy taste. As in Hopper's painting from 1942, many of these businesses are open 24 hours a day.
Pacific and Hawaiian cuisine.
Hawaii is often considered to be one of the most culturally diverse U.S. states, as well as being the only state with an Asian majority population and being one of the few places where United States territory extends into the tropics. As a result, Hawaiian cuisine borrows elements of a variety of cuisines, particularly those of Asian and Pacific-rim cultures, as well as traditional native Hawaiian and a few additions from the American mainland. American influence of the last 150 years has brought cattle, goats, and sheep to the islands, introducing cheese, butter, and yogurt products, as well as crops like red cabbage. Just to name a few, major Asian and Polynesian influences on modern Hawaiian cuisine are from Japan, Korea, Vietnam, China (especially near the Pearl River delta), Samoa, and the Philippines. From Japan, the concept of serving raw fish as a meal with rice was introduced, as was soft tofu, setting the stage for the very popular dish called poke. From Korea, immigrants to Hawaii brought a love of spicy garlic marinades for meat and kimchi. From China, their version of char siu baau became modern manapua, a type of steamed pork bun with a spicy filling. Filipinos brought vinegar, bagoong, and lumpia, and during the 20th century immigrants from American Samoa brought the open pit fire umu and the Vietnamese introduced lemongrass and fish sauce. Each East Asian culture brought several different kinds of noodles, including udon, ramen, mei fun, and pho, and today these are common lunchtime meals.
Much of this cuisine mixes and melts into traditions like the infamous lu'au, whose traditional elaborate fare was once the prerogative of kings and queens but today is the subject of parties for both tourists and also private parties for the "‘ohana" (meaning family and close friends.) Traditionally, women and men ate separately under the Hawaiian "kapu" system, a system of religious beliefs that honored the Hawaiian gods similar to the Maori tapu system, though in this case had some very specific prohibitions towards females eating things like coconut, pork, turtle meat, and bananas as these were considered parts of the male gods. Punishment for violation could be very severe, as a woman might endanger a man's mana, or soul, by eating with him or otherwise by eating the forbidden food because doing so dishonored all the male gods. As the system broke down after 1810, introductions of foods from laborers on plantations began to be included at feasts and much cross pollination occurred, where Asian foodstuffs mixed with Polynesian foodstuffs like breadfruit, kukui nuts, and purple sweet potatoes.
Some notable Hawaiian fare includes seared ahi tuna, opakapaka (snapper) with passionfruit, Hawaiian island-raised lamb, beef and meat products, Hawaiian plate lunch, and Molokai shrimp. Seafood traditionally is caught fresh in Hawaiian waters, and particular delicacies are '"ula poni" , "papaikualoa", "‘opihi", and "‘opihi malihini" , better known as Hawaiian spiny lobster, Kona crab, Hawaiian limpet, and abalone, the last brought over with Japanese immigrants. Some cuisine also incorporates a broad variety of produce and locally grown agricultural products, including tomatoes, sweet Maui onions, taro, and macadamia nuts. Tropical fruits equally play an important role in the cuisine as a flavoring in cocktails and in desserts, including local cultivars of bananas, sweetsop, mangoes, lychee, coconuts, papayas, and lilikoi (passionfruit). Pineapples have been an island staple since the 19th century and figure into many marinades and drinks.
Midwest.
Midwestern cuisine today covers everything from barbecue to the Chicago-style hot dog, though many of its classics are very simple, hearty fare. Mostly this region was completely untouched by European and American settlers until after the American Civil War, and excepting Missouri and the heavily forested states near the Great Lakes was mainly populated by interwarring nomadic tribes like the Sioux, Osage, Arapaho, and Cheyenne. As with most other Native American tribes, these tribes consumed the Three Sisters of beans, maize, and squash, but also for thousands of years followed the herds of bison and hunted them first on foot and then, after the spread of mustangs from the Southwest due to the explorations of conquistadors, on horseback, typically using bow and arrow. There are buffalo jumps dating back nearly ten thousand years and several photographs and written accounts of trappers and homesteaders attesting to their dependence on the buffalo and to a lesser degree elk, and today this region after nearly wiping them out to nothingness has taken to raising them alongside cattle as an important source of sustenance, to be sold all over the land at premium prices.
This region today comprises the states near the Great Lakes and also the Great Plains; much of it is prairie with a very flat terrain where the blue sky meets a neverending horizon. Winters are bitterly cold, windy, and wet. Often that means very harsh blizzards especially near the Great Lakes where Arctic winds blow off of Canada and where the ice on rivers and lakes freezes reliably thick enough for ice hockey to be a favorite pastime in the region and for icefishing for pike and muskies to be ubiquitous in Minnesota, Wisconsin, and Michigan, where they often there after become part of the local tradition of the fish fry. Population density is extremely low away from the Great Lakes and very small towns dominated by enormous farms are the rule with larger cities being the exception. Detroit, Cleveland, St. Louis, Cincinnati, Indianapolis, Milwaukee, Minneapolis and her twin sister city across the river St. Paul dominate the landscape in wealth and size, owing to their ties with manufacturing, finance, transportation, and meatpacking. Smaller places like Cheyenne, Omaha, Tulsa, and Kansas City make up local capitals, but the king of them all is Chicago, third largest city in the country.
The Upper Midwest includes the states of Illinois, Minnesota, Wisconsin, Ohio, Indiana, and Michigan. Non-Native American settlement began here earlier than anywhere else in the region, and thus the food available here ranges from the sublime to the bizarre. As with all of the Midwest, the primary meats here are beef and poultry, since the Midwest has been raising turkeys, chickens, and geese for over a hundred and fifty years; chickens have been so common for so long that the Midwest has several native breeds that are prized for both backyard farming and for farmer's markets, such as the Buckeye and Wyandotte; one, Billina, appears as a character in the second book of the Oz series by L. Frank Baum. Favorite fruits of the region include a few native plants inherited from Native American tribes like the pawpaw and the American persimmons are also highly favored. As with the American South, pawpaws are the region's largest native fruit, about the size of a mango, and are often found growing wild in the region come September, whereafter they are made into preserves and cakes and command quite a price at farmer's markets in Chicago. The American persimmon is often smaller than it is Japanese cousin, about the size of a small plum, but in the Midwest and portions of the East it is the main ingredient in a steamed pudding called persimmon pudding, topped with crème anglaise. Other crops inherited from the Native Americans include wild rice, which grows on the banks of lakes and is a local favorite for fancy meals.
Typical fruits of the region are cold weather crops. Once it was believed that the region had winters that were far too harsh for apple growing, but then a breeder in Minnesota came forth with the Wealthy apple and thence came forth the third most productive region for apple growing in the region, with local varieties comprising Wolf River, Enterprise, Melrose, Paula Red, Rome Beauty, Honeycrisp, and the world famous Red Delicious. Cherries are important to Michigan and Wisconsin grows many cranberries, a legacy of early 19th century emigration of New England farmers. Crabapple jelly is a favorite condiment of the region.
The American South.
When referring to the American South as a region, typically it should indicate Southern Maryland and the states that were once part of the Old Confederacy, with the dividing line between the East and West jackknifing about 100 miles west of Dallas, Texas, and mostly south of the old Mason-Dixon line. Cities found in this area include New Orleans, Miami, Atlanta Washington DC, Memphis, Charleston , and Charlotte with Houston, Texas being the largest. These states are much more closely tied to each other and have been part of US territory for much longer than states much farther west than East Texas, and in the case of food, the influences and cooking styles are strictly separated as the terrain begins to change to prairie and desert from bayou and hardwood forest.
This section of the country has some of the oldest known foodways in the land, with some recipes almost 400 years old. Native American influences are still quite visible in the use of cornmeal as an essential staple and found in the Southern predilection for hunting wild game, in particular wild turkey, deer, woodcock, and various kinds of waterfowl; for example, coastal North Carolina is a place where hunters will seek tundra swan as a part of Christmas dinner; the original English and Scottish settlers would have rejoiced at this revelation owing to the fact that such was banned amongst the commoner class, and naturally, their descendants have not forgotten . Native Americans also consumed turtles and catfish, specifically the snapping turtle and blue catfish, both very important parts of the diet in the South today. Catfish are often caught with one' s bare hands, gutted, breaded, and fried to make a Southern variation on English fish and chips and turtles are turned into stews and soups. Native American tribes of the region such as the Cherokee or Choctaw often cultivated or gathered local plants like pawpaw, maypop,spicebush,sassafras, and several sorts of squash and maize, and the aforementioned fruits still are cultivated as food in a Southerner's back garden. Maize is to this day found in dishes for breakfast, lunch and dinner in the form of grits, hoecakes, baked cornbread, and spoonbread, and nuts like the hickory, black walnut and pecan are very commonly included in desserts and pastries as varied as mince pies, pecan pie, pecan rolls and honey buns (both are types of sticky bun), and quick breads, which were themselves invented in the South during the American Civil War.
European influence began soon after the settlement of Jamestown in 1607 and the earliest recipes emerging by the end of the 17th century. Specific influences from Europe were quite varied, and remain traditional and essential to the modern cookery overall. To the upper portion of the South, French Huguenots brought the concept of making rouxs to make sauces and soups, and later French settlers hunted for frogs in the swamps to make frog's legs. German speakers often settled in Appalachia on small farms or in the backcountry away from the coast, and invented an American breakfast delicacy that is now nationally beloved, apple butter, based on their recipe for apfelkraut, and later introduced red cabbage and rye. From the UK, an enormous amount of influence was bestowed upon the South, specifically foodways found in 17th and 18th century Ulster, the borderlands between England and Scotland, the Scottish Highlands, portions of Wales, the West Midlands and Black Country. Settlers bound for America fled the tumult of the Civil War and troubles in the plantation of Ireland and the Highland Clearances, and very often ships manifests show their belongings nearly always included their wives' cookpots or bakestones and seed stock for plants like peaches, plums, and apples to grow orchards, which they planted in their hundreds: today, the biggest fruit crop of the region is the yellow peach, and noted apple varieties include Carolina Red June, Arkansas Black, Carter Blue, Magnum Bonum, and the infamous Golden Delicious. Each group brought foods and ideas from their region which gave birth in time to American whiskey and Kentucky bourbon, derived from the recipes of Celtic peoples, tipsy cakes, derived from 18th century recipes for English trifle, and all of the above made the staple meat of the South pork, to this day the meat no Southerner can cook without.
Excepting Kentucky, where mutton is the meat of choice, or Southern Maryland, where the custom is to take the carcass of an entire bull and roast it over coals for many hours, pork is the irreplaceable mainstay of Southern style barbecue and features in other preparations like sausages and sandwiches. Among both African Americans and Caucasian Americans in the antebellum period, corn and pork were a staple of the diet. For breakfast, it is a feature of country sausage, which in turn are an ingredient in the Southern breakfast dish of biscuits and gravy. Head cheese is a popular sliced meat of the region, taken from the pig's head, and pickled pig's feet have always been a cheap snack since they were introduced by Scotch-Irish settlers; today they are often served in bars.. Baby back ribs, hog maw, cracklins, and even whole pig roasts in specially constructed ovens are found in all parts of the South, as are its two best known condiments, barbecue sauce and hot sauce, with hundreds of local variations. In Virginia and the Appalachians, the mainstay for special occasions is the country ham, often served for Christmas and cured with salt or hickory, with the Virginia recipe often feeding the hogs peanuts for finishing and giving the ham a distinct taste, and red pepper flakes in ham cured in Tennessee. Accompanying many meals is the southern style fluffy biscuit, where the leavening agent is sodium bicarbonate and often includes buttermilk, and for breakfast they often accompany country ham, grits, and scrambled eggs.
Desserts in the South tend to be quite rich and very much a legacy of entertaining to impress guests, since a Southern housewife was (and to a degree still is) expected to show her hospitality by laying out as impressive a banquet as she is able to manage. Desserts are vast and encompass Lane cake, sweet potato pie, peach cobbler, pecan pie, hummingbird cake, Jefferson Davis pie, peanut brittle, coconut cake, apple fritters, peanut cookies, Moravian spice cookies, chess pie, doberge cake, Lady Baltimore cake, bourbon balls, and caramel cake. American style sponge cakes tend to be the rule rather than the exception as is American style buttercream, a place where Southern baking intersects with the rest of the United States. Nuts like pecan and hickory tend to be revered as garnishes for these desserts, and make their way into local bakeries as fillings for chocolates.
In the parts of the South which face the Atlantic Ocean, French influences often were dictated by where French Huguenots settled, however it is Louisiana that got the lion's share of older French cooking methods from Poitou and Normandy via Nova Scotia, most of which are foodways that pre-date the codification of haute cuisine during the reign of Louis XIV and have more in common with rustic cuisines of the 17th and 18th century than anything ever found at the French court in Versailles or the bistros of 19th and 20th century Paris; this is especially true of Cajun cuisine. Louisiana is a state named for Louis XIV and to this day French is still a commonly spoken tongue in the areas south of New Orleans. The Cajuns and their dialect have occupied Southern Louisiana since the 1700s owing to "Le Grande Dérangement", an event in which they were forcibly evicted by the English Crown from their lands in Canada and made to occupy more marginal lands on the bayou. Cajun French is more closely related to dialects spoken in Northern Maine, New Brunswick, and to a lesser degree Haiti than anything spoken in modern France, and likewise their terminology, methodology, and culture concerning food is much more closely related to the styles of these former French colonies even today. Unlike other areas of the South, Cajuns were and still are largely Catholics and thus much of what they eat is seasonal; for example pork is an important component of the Cajun "boucherie" (a large community event where the hog is butchered, prepared with a fiery spice mix, and eaten snout to tail) but it is never consumed in the five weeks of Lent, when such would be forbidden. Cajun cuisine tends to focus on what is locally available, historically because Cajuns were often poorer independent farmers and not plantation owners but today it is because such is deeply imbedded in local culture. Boudin is a type of sausage found only in this area of the country, and it is often by far more spicy than anything found in France or Belgium. Chaudin is unique to the area, and the method of cooking is comparable to the Scottish dish haggis: the stuffing includes onions, rice, bell peppers, spices, and pork sewn up in the stomach of a pig, and served in slices piping hot. Crayfish are a staple of the Cajun grandmother's cookpot, as are blue crabs, shrimp, corn on the cob, and red potatoes, since these are the basic ingredients of the Louisiana crawfish boil.
Since the end of the Civil War, New Orleans has had a thriving fine dining scene that predates the much younger 20th century metropoli of Atlanta and Miami. It was here that cocktails like the sazerac and hurricane were invented as well as the liqueur Southern Comfort. New Orleans has been the capital of Creole culture since before Louisiana was a state; this culture was and still is quite distinct from the rural culture of Cajuns and dovetails with what would have been eaten in antebellum Louisiana plantation culture long ago. Cooking to impress and show one's wealth was a staple of Creole culture, which often mixed Spanish, Italian, French, and African methods, producing rich dishes like oysters bienville, pompano en papillote, and even the muffaletta sandwich. However, Louisiana Creole cuisine tends to diverge from the original ideas brought to the region in ingredients: profiteroles, for example, use a near identical choux pastry to that which is found in modern Paris but often use vanilla or chocolate ice cream rather than custard as the filling, pralines nearly always use pecan and not almonds, and bananas foster came about when New Orleans was a key port for the import of bananas from the Caribbean Sea. Gumbos tend to be thickened using the gooey innards of okra, something an African cook would be familiar with, whereas Cajuns would be more likely to use the leaves of the sassafrass tree and neither of them ever used the andouille currently known in France, since French andouille uses tripe whereas Louisiana andouille is made from a Boston butt, usually inflected with pepper flakes, and smoked for hours over pecan wood. Other ingredients that are native to Louisiana and not found in the cuisine of modern France would include rice, which has been a staple of both Creole and Cajun cooking for generations, and sugarcane, which has been grown in Louisiana since the early 1800s. Ground cayenne pepper is a key spice of the region, as is the meat of the American alligator, something settlers learned from the Choctaws and Houma.
African influences came with slaves from Ghana, Benin, Mali, Ivory Coast, Angola, Sierra Leone, Nigeria, and other portions of West Africa, and the mark they and their descendants have made on Southern food is very strong today. Crops like okra, sorghum, sesame seeds, eggplant, chili peppers, and many different kinds of melons were brought with them from West Africa along with the incredibly important introduction of rice to the Carolinas and later to Texas and Louisiana, whence it became a staple grain of the region and still remains a staple today, found in dishes like Hoppin John, rice and beans, dirty rice, purloo, and Charleston red rice. Other crops, like sugar cane, kidney beans, and certain spices would have been familiar to slaves through contact with British colonies in the Caribbean and also brought with them. Like the poorer indentured servants that came to the South, slaves often got the leftovers of what was slaughtered for the consumption of the master of the plantation and so many recipes had to be adapted for offal, like pig's ears and fatbacks though other methods encouraged low and slow methods of cooking to tenderize the tougher cuts of meat, like braising, smoking, and pit roasting, the last of which was a method known to West Africans in the preparation of roasting goat. It is from this class of people that Southern cuisine gets barbecue and fried chicken, in the latter case with Scottish immigrants bringing the cooking method and West Africans bringing the spices . Other recipes certainly brought by Africans involve peanuts, as evidenced by the local nickname for the legume in Southern dialects of American English: "goober", taken from the Kongo word for peanut, "nguba". The 300-year-old recipe for peanut soup is a classic of Southern cuisine that has never stopped being eaten, handed down to the descendants of Virginia slaves and adapted to be creamier and less spicy than the original African dish. Boiled peanuts are a common food served at bars as a snack and have been eaten in the South for was long as there have been pots to boil them, and fried green tomatoes first appeared after the Civil War, thereafter becoming a common way for sharecroppers to use up the last of the tomatoes of summer before the weather cooled in October.
Certain portions of the South often have their own very distinct subtypes of cuisine owing to local history and landscape: though Cajun cuisine is more famous, Floridian cuisine, for example, has a very distinct way of cooking that includes ingredients her other Southern sisters do not use, especially points south of Tampa and Orlando. The Spanish Crown had control of the state until the early 19th century and used the southern tip as an outpost beginning in the 1500s, but Florida kept and still maintains ties with the Caribbean Sea, including the Bahamas Haiti, Cuba, Puerto Rico, and Jamaica, and also holds the legacy of Anglophone settlers foodways and the foodways of the Seminole tribe of Native Americans. Thus, tomatoes, bell peppers, plantains, Caribbean lobsters, heart of palm, figs, especially taken from the Florida strangler fig, citrus, especially Hamlin oranges, grapefruits, tangerines, limes, and clementine oranges, habañero peppers, scotch bonnet peppers, mangoes, blue crab, conch, Florida stone crab, red drum, dorado, and marlins tend to be local favorite ingredients. Dairy is available in this region, but it is less emphasized due to the year round warmth. Traditional key lime pie, a dessert from the islands off the coast of Miami, is made with condensed milk to form the custard with the eye wateringly tart limes native to the Florida Keys in part because milk would spoil in an age before refrigeration. Pork in this region tends to be roasted in methods similar to those found in Puerto Rico and Cuba, owing to mass emigration from those countries in the 20th century, especially in the counties surrounding Miami.
Orange blossom honey is a specialty of the state, and is widely available in farmer's markets.
Ptarmigan, grouse, crow blackbirds, dove, ducks and other game fowl are consumed in the United States. Rat stew is consumed in American cuisine in the state of West Virginia. West Virginians also eat roadkill. In the American state of Arkansas, beaver tail stew is consumed in Cotton town. Squirrel, raccoon, possum, bear, muskrat, chipmunk, skunk, opossum, groundhog, pheasant, armadillo and rabbit are also consumed in the United States.
Cuisine in the West.
Cooking in the American West gets its influence from Native American and Hispanophone cultures, and other European settlers into the part of the country. Common dishes vary depending on the area. For instance, the Northwestern region encompasses Oregon, Washington, and Northern California, and all rely on local seafood and a few classics of their own. In New Mexico, Colorado, Nevada, Arizona, Utah, West Texas, and Southern California, Mexican flavors and influences are extremely common, especially from the Mexican states of Chihuahua, Baja California, and Sonora.
Northwest.
The Pacific Northwest as a region generally includes Alaska and the state of Washington near the Canadian border and terminates near Sacramento, California. Here, the terrain is mostly temperate rainforest on the Coast mixed with pine forest as one approaches the Canadian border inland. One of the core favorite foodstuffs is Pacific salmon, native to many of the larger rivers of the area and often smoked or grilled on cedar planks. In Alaska, wild game like ptarmigan and moose meat feature extensively since much of the state is wilderness. Fresh fish like steelhead trout, Pacific cod, Pacific halibut, and pollock are fished for extensively and feature on the menu of many restaurants, as do a plethora of fresh berries and vegetables, like Cameo apples from Washington state, the headquarters of the U.S. apple industry, cherries from Oregon, blackberries, and marionberries, a feature of many pies. Hazelnuts are grown extensively in this region and are a feature of baking, such as in chocolate hazelnut pie, an Oregon favorite, and Almond Roca is a local candy.
This region is also heavily dominated by some notable wineries producing a high quality product, with Sonoma found within this region as well as the newer vinicultural juggernauts of Washington State, like the Yakima Valley. The first plantings of vineyards in the United States began many miles to the South on the Pacific coast in what is now San Diego, because the Franciscan friars that settled Alta California required wines they could use for their table and for the Eucharist, and the variety they planted, the mission grape, is still available on a limited basis. Today, French, Spanish, and Italian varietals are sold by the hogshead, and much of the area directly north of San Francisco is under vine, in particular Pinot noir, Garnacha, and Ruffina and several Tuscan varietals.
Like its counterpart on the opposite coast to the East, there is a grand variety of shellfish in this region. Geoducks are a native species of giant clam that have incredibly long necks, and they are eaten by the bucket full as well as shipped to Asia for millions of dollars as they are believed to be an aphrodisiac. Gaper clams are a favorite food, often grilled or steamed in a sauce, as is the native California abalone, which although protected as a food source is a traditional foodway predating settlement by whites and today features heavily in the cooking of fine restaurants as well as in home cooking, in mirin-flavored soups (the influence of Japanese cooking is strong in the region) noodle dishes and on the barbecue. Olympia oysters are served on the half shell as well as the Kumamoto oyster, introduced by Japanese immigrants and a staple at dinner as an appetizer. California mussels are a delicacy of the region, and have been a feature of the cooking for generations.
Crabs are a delicacy, and included in this are Alaskan king crab, red crab, yellow crab, and the world famous Dungeness crab. Californian and Oregonian sportsmen pursue the last three extensively using hoop nets, and prepare them in a multitude of ways. Alaska king crab, able to get up to 10 kg, is often served steamed for a whole table with lemon butter sauce or put in chunks of salad with avocado, and native crabs are the base of dishes like the California roll, cioppino, a tomato based fisherman's stew, and Crab Louie, another kind of salad native to San Francisco. Favorite grains are mainly wheat, and the region is famous for sourdough bread. Cheeses of the region include Humboldt Fog, Cougar Gold and Teleme.
Southwest and Southern California.
The states of the Four Corners (Arizona, New Mexico, Colorado and Utah) plus Nevada, Southern California and West Texas make up a large chunk of the United States and there is a distinct Hispanic accent to the cookery here, with each having a cultural capital in Salt Lake City, Phoenix, Santa Fe, Las Vegas, Denver, and Los Angeles. This region was part of the Spanish Empire for more than two centuries before California's statehood in the 1830s, and today is the home of a large population of immigrants from Mexico and Central America; Spanish is a commonly spoken secondary language here and the state of New Mexico has its own distinct dialect that is protected as the state's second official language . With the exception of Southern California, the signature meat is beef, since this is one of the two regions in which cowboys lived and modern cattle ranchers still eke out their living today. High quality beefstock is a feature that has been present in the region for more than 200 years and the many cuts of beef are unique to the United States. These cuts of meat are different from the related Mexican cuisine over the border in that certain kind of offal, like "lengua" (tongue) "cabeza" (head) and "tripas" (tripe) are considered less desirable and are thus less emphasized. Typical cuts would include the ribs, brisket, sirloin, flank steak, skirt steak, and t bone.
Historically, Spanish settlers that came to the region found it completely unsuitable to the mining operations that much older settlements in Mexico had to offer as the technology of the age was not yet advanced enough to get at the silver that would later be found in the region. They had no knowledge of the gold to be discovered in California, something nobody would find until 1848. Instead, in order to make the pueblos prosper, they adapted the old rancho system of places like Andalusia in Spain and brought the earliest beefstock, among these were breeds that would go feral and become the Texas longhorn, and Churro sheep, still used as breeding stock because they are easy to keep and well adapted to the extremely arid and hot climate, where temperatures easily exceed 38 °C. Later cowboys learned from their management practices, many of which still stand today, like the practical management of stock on horseback using the Western saddle. Likewise, settlers learned the cooking methods of those who came before and local tribes as well: for example, portions of Arizona and New Mexico still use the aforementioned beehive shaped clay contraption called an "horno", an outdoor wood fired oven both Native American tribes like the Navajo and Spaniards used for roasting meat, maize, and baking bread. Other meats that see frequent use in this region are elk meat, a favorite in crown roasts and burgers, and nearer the Mexican border rattlesnake, often skinned and stewed. The taste for alcohol in this region tends toward light and clean flavors found in tequila, a staple of this region since the days of the Wild West and a staple in the bartender's arsenal for cocktails, especially in Las Vegas. In Utah, a state heavily populated by Mormons, alcohol is frowned upon by the Church of Latter Day Saints but still available in area bars in Salt Lake City.
Introduction of agriculture was limited prior to the 20th century and the development of better irrigation techniques, but included the addition of peaches, a crop still celebrated by Native American tribes like the Havasupai, and oranges; today in Arizona, Texas, and New Mexico the favored orange today is the Moro blood orange, which often finds its way into the local cuisine, like cakes and marmalade. Pine nuts are a particular regional specialty and feature often in fine dining; indeed in Nevada the Native American tribes that live there are by treaty given rights to exclusive harvest.
Cuisine in this region tends to have certain key ingredients: tomatoes, onions, black beans, pinto beans, rice, bell peppers, and cheese, in particular Monterey Jack, invented in Southern California in the 19th century and itself often further altered into pepperjack. where spicy jalapeño peppers are incorporated into the cheese to create a smoky taste. Chili peppers play an important role in the cuisine, with a few native to the region (Anaheim pepper, Hatch pepper), these still grown by Spanish speakers in New Mexico. In the 20th century a few more recent additions have arrived like the poblano pepper, rocoto pepper, ghost pepper, thai chili pepper, and Korean pepper, the last three especially when discussing Southern California and its large population from East and South Asia. Cornbread is consumed in this area, however the recipe differs from ones in the East in that the batter is cooked in a cast iron skillet. Outdoor cooking is popular and still utilizes an old method settlers brought from the East with them, in which a cast iron dutch oven is covered with the coals of the fire and stacked or hung from a tripod: this is very different from the earthenware pots of Mexico. Tortillas are still made the traditional way in this area and form an important component of the spicy breakfast burrito, which contains ham, eggs, and salsa or pico de gallo. They also comprise the regular burrito, which contains any combination of marinated meats, vegetables, and piquant chilis, the quesadilla, a much loved grilled dish where cheese and other ingredients are stuffed between two tortillas and served by the slice, and the steak fajita, where sliced skirt steak sizzles in a skillet with caramelized onions.
Unlike Mexico, tortillas of this region also may incorporate vegetable matter like spinach into the flatbread dough to make wraps, which were invented in Southern California. Food here tends to use pungent spices and condiments, typically chili verde sauce, various kinds of hot sauce, sriracha sauce, chili powder, cayenne pepper, white pepper, cumin, paprika, onion powder, thyme and black pepper. Nowhere is this fiery mix of spice more evident than in the dishes chili con carne, a meaty stew, and cowboy beans, both of which are a feature of regional cookoffs. Southern California has several additions like five spice powder , rosemary, curry powder, kimchi, and lemongrass, with many of these brought by recent immigration to the region and often a feature of Southern California's fusion cuisine, very popular in fine dining.
In Texas, the local barbecue is often entirely made up of beef brisket or large rib racks, where the meat is seasoned with a spice rub and cooked over coals of mesquite, and in other portions of the state they smoke their meat and peppery sausages over high heat using pecan, apple, and oak and served it with a side of pickled vegetables, a legacy of German and Czech settlers of the late 1800s. California is home to Santa Maria-style barbecue, where the spices involved generally are black pepper, paprika, and garlic salt, and grilled over the coals of coast live oak. Native American additions may include Navajo frybread and corn on the cob, often roasted on the grill in its husk. A typical accompaniment or appetizer of all these states is the tortilla chip, which sometimes includes cornmeal from cultivars of corn that are blue or red in addition to the standard yellow of sweetcorn, and is served with salsa of varying hotness. Tortilla chips also are an ingredient in the Tex Mex dish nachos, where these chips are loaded with any combination of ground beef, melted Monterey Jack,cheddar, or Colby cheese, guacamole, sour cream, and salsa, and Texas usually prefers a version of potato salad as a side dish. For alcohol, a key ingredient is tequila: this spirit has been made on both sides of the US-Mexican border for generations, and in modern cuisine it is a must have in a bartender's arsenal as well as an addition to dishes for sauteeing.
Southern California is located more towards the coast and has had more contact with immigration from the West Pacific and Baja California, in addition to having the international city of Los Angeles as its capital. Here, the prime mode of transportation is by car. Drive through fast food was invented in this area, but so was the concept of the gourmet burger movement, giving birth to chains like In and Out Burger and Sonic Drive-In, with many variations of burgers including chili, multiple patties, avocado, special sauces, and angus or wagyu beef; common accompaniments include thick milkshakes in various flavors like mint, chocolate, peanut butter, vanilla, strawberry, and mango. Smoothies are a common breakfast item made with fresh fruit juice, yogurt, and crushed ice. Agua fresca, a drink originated by Mexican immigrants, is a common hot weather beverage sold in many supermarkets and at mom and pop stands, available in citrus, watermelon, and strawberry flavors; the California version usually served chilled without grain in it.
The weather in Southern California is such that the temperature rarely drops below 12 °C in winter, thus, sun loving crops like pistachios, kiwifruit, avocadoes, strawberries, and tomatoes are staple crops of the region, the last often dried in the sun and a feature of salads and sandwiches. Olive oil is a staple cooking oil of the region and has been since the days of Junípero Serra; today the mission olive is a common tree growing in a Southern Californian's back garden; as a crop olives are increasingly a signature of the region along with Valencia oranges and Meyer lemons. Soybeans, bok choy, Japanese persimmon, thai basil, Napa cabbage, nori, mandarin oranges, water chestnuts, and mung beans are other crops brought to the region from East Asia and are common additions to salads as the emphasis on fresh produce in both Southern and Northern California is very strong. Other vegetables and herbs have a distinct Mediterranean flavor which would include oregano, basil, summer squash, eggplant, and broccoli, with all of the above extensively available at farmers' markets all around Southern California. Naturally, salads native to Southern California tend to be hearty affairs, like Cobb salad and Chinese chicken salad, and dressings like green goddess and ranch dressing a staple. California-style pizza tends to have disparate ingredients with an emphasis on vegetables, with any combination of chili oil, prawns, eggs, chicken, shiitake mushrooms, olives, bell pepper, goat cheese, and feta cheese. Peanut noodles tend to include a sweet dressing with lo mein noodles and chopped peanuts.
Fresh fish and shellfish in Southern California tends to be expensive in restaurants, but by no means out of reach of the masses. Every year since the end of WWII, the Pismo clam festival has taken place where the local population takes a large species of clam and bakes, stuffs, and roasts it to their heart's content as it is a regional delicacy. Fishing for pacific species of octopus and the Humboldt squid are common, and both are a feature of East Asian and other L.A. fish markets.Lingcod is a coveted regional fish that is often caught in the autumn off the coast of San Diego and in the Channel Islands and often served baked. California sheephead are often grilled and are much sought after by spear fishermen and the immigrant Chinese population, in which case it is basket steamed. Most revered of all in recent years is the California spiny lobster, a beast that can grow to be 20 kg, and is a delicacy that now rivals the fishery for Dungeness crab in its importance.
Ethnic and immigrant influence.
The demand for ethnic foods in the United States reflects the nation's changing diversity as well as its development over time. According to the National Restaurant Association, 
Restaurant industry sales are expected to reach a record high of $476 billion in 2005, an increase of 4.9 percent over 2004... Driven by consumer demand, the ethnic food market reached record sales in 2002, and has emerged as the fastest growing category in the food and beverage product sector, according to USBX Advisory Services. Minorities in the U.S. spend a combined $142 billion on food and by 2010, America's ethnic population is expected to grow by 40 percent.
A movement began during the 1980s among popular leading chefs to reclaim America's ethnic foods within its regional traditions, where these trends originated. One of the earliest was Paul Prudhomme, who in 1984 began the introduction of his influential cookbook, "Paul Prodhomme's Louisiana Kitchen", by describing the over 200-year history of Creole and Cajun cooking; he aims to "preserve and expand the Louisiana tradition." Prodhomme's success quickly inspired other chefs. Norman Van Aken embraced a Floridian type cuisine fused with many ethnic and globalized elements in his "Feast of Sunlight" cookbook in 1988. The movement finally gained fame around the world when California became swept up in the movement, then seemingly started to lead the trend itself, in, for example, the popular restaurant Chez Panisse in Berkeley. Examples of the Chez Panisse phenomenon, chefs who embraced a new globalized cuisine, were celebrity chefs like Jeremiah Tower and Wolfgang Puck, both former colleagues at the restaurant. Puck went on to describe his belief in contemporary, new style American cuisine in the introduction to "The Wolfgang Puck Cookbook":
Another major breakthrough, whose originators were once thought to be crazy, is the mixing of ethnic cuisines. It is not at all uncommon to find raw fish listed next to tortillas on the same menu. Ethnic crossovers also occur when distinct elements meet in a single recipe. This country is, after all, a huge melting pot. Why should its cooking not illustrate the American transformation of diversity into unity?
Puck's former colleague, Jeremiah Tower became synonymous with California Cuisine and the overall American culinary revolution. Meanwhile, the restaurant that inspired both Puck and Tower became a distinguished establishment, popularizing its so called "mantra" in its book by Paul Bertolli and owner Alice Waters, "Chez Panisse Cooking", in 1988. Published well after the restaurants' founding in 1971, this new cookbook from the restaurant seemed to perfect the idea and philosophy that had developed over the years. The book embraced America's natural bounty, specifically that of California, while containing recipes that reflected Bertoli and Waters' appreciation of both northern Italian and French style foods.
Early ethnic influences.
While the earliest cuisine of the United States was influenced by indigenous Native Americans, the cuisine of the thirteen colonies or the culture of the antebellum American South; the overall culture of the nation, its gastronomy and the growing culinary arts became ever more influenced by its changing ethnic mix and immigrant patterns from the 18th and 19th centuries unto the present. Some of the ethnic groups that continued to influence the cuisine were here in prior years; while others arrived more numerously during "The Great Transatlantic Migration" (of 1870—1914) or other mass migrations.
Some of the ethnic influences could be found in the nation from after the American Civil War and into the History of United States continental expansion during most of the 19th century. Ethnic influences already in the nation at that time would include the following groups and their respective cuisines:
Later ethnic and immigrant influence.
Mass migrations of immigrants to the United States came in several waves. Historians identify several waves of migration to the United States: one from 1815–1860, in which some five million English, Irish, Germanic, Scandinavian, and others from northwestern Europe came to the United States; one from 1865–1890, in which some 10 million immigrants, also mainly from northwestern Europe, settled, and a third from 1890–1914, in which 15 million immigrants, mainly from central, eastern, and southern Europe (many Austrian, Hungarian, Turkish, Lithuanian, Russian, Jewish, Greek, Italian, and Romanian) settled in the United States.
Together with earlier arrivals to the United States (including the indigenous Native Americans, Hispanic and Latino Americans, particularly in the West, Southwest, and Texas; African Americans who came to the United States in the Atlantic slave trade; and early colonial migrants from Britain, France, Germany, Spain, and elsewhere), these new waves of immigrants had a profound impact on national or regional cuisine. Some of these more prominent groups include the following:
Italian, Mexican and Chinese (Cantonese) cuisines have indeed joined the mainstream. These three cuisines have become so ingrained in the American culture that they are no longer foreign to the American palate. According to the study, more than nine out of 10 consumers are familiar with and have tried these foods, and about half report eating them frequently. The research also indicates that Italian, Mexican and Chinese (Cantonese) have become so adapted to such an extent that "authenticity" is no longer a concern to customers.
Contributions from these ethnic foods have become as common as traditional "American" fares such as hot dogs, hamburgers, beef steak, which are derived from German cuisine, (chicken-fried steak, for example, is a variation on German schnitzel), cherry pie, Coca-Cola, milkshakes, fried chicken (Fried chicken is of Scottish and African influence) and so on. Nowadays, Americans also have a ubiquitous consumption of foods like pizza and pasta, tacos and burritos to "General Tso's chicken" and fortune cookies. Fascination with these and other ethnic foods may also vary with region.
Notable American chefs.
American chefs have been influential both in the food industry and in popular culture. An important 19th Century American chef was Charles Ranhofer of Delmonico's Restaurant in New York City. American cooking has been exported around the world, both through the global expansion of restaurant chains such as T.G.I. Friday's and McDonald's and the efforts of individual restaurateurs such as Bob Payton, credited with bringing American-style pizza to the UK.
The first generation of television chefs such as Robert Carrier and Julia Child tended to concentrate on cooking based primarily on European, especially French and Italian, cuisines. Only during the 1970s and 1980s did television chefs such as James Beard and Jeff Smith shift the focus towards home-grown cooking styles, particularly those of the different ethnic groups within the nation. Notable American restaurant chefs include Thomas Keller, Charlie Trotter, Grant Achatz, Alfred Portale, Paul Prudhomme, Paul Bertolli, Frank Stitt, Alice Waters, Patrick O'Connell and celebrity chefs like Mario Batali, David Chang, Alton Brown, Emeril Lagasse, Cat Cora, Michael Symon, Bobby Flay, Ina Garten, Todd English, Anthony Bourdain, and Paula Deen.
Regional chefs are emerging as localized celebrity chefs with growing broader appeal, such as Peter Merriman (Hawaii Regional Cuisine), Jerry Traunfeld, Alan Wong (Pacific Rim cuisine), Norman Van Aken (New World Cuisine – fusion Latin, Caribbean, Asian, African and American), and Mark Miller (American Southwest cuisine).

</doc>
<doc id="2176" url="https://en.wikipedia.org/wiki?curid=2176" title="Ahmad Shah Massoud">
Ahmad Shah Massoud

Ahmad Shah Massoud (Dari Persian: ; September 2, 1953September 9, 2001) was an Afghan political and military leader, who was a powerful military commander during the resistance against the Soviet occupation between 1979 and 1989 and in the following years of civil war. He was assassinated on September 9, 2001.
Massoud came from an ethnic Tajik, Sunni Muslim background in the Panjshir valley of northern Afghanistan. He began studying engineering at Polytechnical University of Kabul in the 1970s, where he became involved with fundamentalist Muslim anti-communist movements around Burhanuddin Rabbani, a leading Islamist. He was part of a Pakistan-backed failed rebellion against Mohammed Daoud Khan's government. After the Soviet occupation of 1979, his role as an insurgent leader earned him the nickname of "Lion of Panjshir" () among his followers. In 1992, after he disturbed the UN plan to install an interim government to replace that of President Mohammad Najibullah, he was appointed as the minister of defense through the Peshawar Accord, a peace and power-sharing agreement, in the post-communist Islamic State of Afghanistan. His militia fought to defend the capital against militias led by Gulbuddin Hekmatyar, Abdul Rasul Sayyaf, Abdul Ali Mazari, Abdul Rashid Dostum and eventually the Taliban, who started to lay siege to the capital in January 1995 after the city had seen fierce fighting; at least 60,000 civilians were killed, many more injured, public property, government offices and the Kabul Museum had been looted, and two thirds of the population had fled the city.
Following the rise of the Taliban in 1996, Massoud, who rejected the Taliban's fundamentalist interpretation of Islam, returned to armed opposition until he eventually fled to Kulob, Tajikistan, destroying the Salang Tunnel on his way north. He became the military and political leader of the United Islamic Front for the Salvation of Afghanistan (also known in the West as the "Northern Alliance"). He was assassinated, probably at the instigation of al-Qaeda, in a suicide bombing on September 9, 2001, just two days before the September 11 attacks in the
United States which led to the North Atlantic Treaty Organisation invading Afghanistan, allying with Massoud's forces.
Massoud was posthumously named "National Hero" by the order of President Hamid Karzai after the Taliban were ousted from power. The date of Massoud’s death, September 9, is observed as a national holiday known as "Massoud Day". His followers call him "Amir Sāhib-e Shahīd" ().
Early life.
Ahmad Shah Massoud was born in 1953 in Bazarak, Panjshir, to a well-to-do family native to the Panjshir valley. His name at birth was "Ahmed Shah"; he took the name "Massoud" as a "nom de guerre" when he went into the resistance movement in 1974. His father, Dost Mohammad Khan, was a colonel in the Royal Afghan Army. From his native Panjshir, his family moved briefly to Herat and then to Kabul, where Massoud spent most of his childhood.
Massoud attended the renowned Franco-Afghan Lycée Esteqlal. Regarded as a gifted student, he studied engineering at Kabul University after his graduation from the Lycée. Massoud spoke Dari, Pashto, Urdu and French and had good English reading skills.
In 1973, Mohammed Daoud Khan was brought to power in a coup d'état backed by the People's Democratic Party of Afghanistan, and the Republic of Afghanistan was established. These developments gave rise to an Islamist movement opposed to the increasing communist and Soviet influence over Afghanistan. During that time, while studying at Kabul University, Massoud became involved with the Muslim Youth (Sazman-i Jawanan-i Musulman), the student branch of the Jamiat-e Islami (Islamic Society), whose chairman then was the professor Burhanuddin Rabbani. Kabul University was a centre for political debate and activism during that time.
By 1975, after a failed uprising by the Muslim Youth, a "profound and long-lasting schism" within the Islamist movement began to emerge. The Islamic Society split between supporters of the more moderate forces around Massoud and Rabbani, who led the Jamiat-i Islami, and more radical Islamist elements surrounding Gulbuddin Hekmatyar, who founded the Hezb-i Islami. The conflict reached such a point that Hekmatyar reportedly tried to kill Massoud, then 22 years old.
The Soviet invasion and PDPA communism.
Communist revolution in Afghanistan (1978).
The government of Mohammed Daoud Khan tried to scale back the communist People's Democratic Party of Afghanistan's influence, dismissing PDPA members from their government posts, appointing conservatives to replace them, and finally announcing the dissolution of the PDPA, with the arrests of senior party members.
On April 27, 1978, the PDPA and military units loyal to it killed Daoud Khan, his immediate family, and bodyguards in a violent coup, and seized control of the capital Kabul. The new PDPA government, led by a revolutionary council, did not enjoy the support of the masses. It announced and implemented a doctrine hostile to political dissent, whether inside or outside the party.
The PDPA started reforms along Marxist–Leninist and Soviet lines. The reforms and the PDPA's affinity to the Soviet Union were met with strong resistance by the population, especially as the government attempted to enforce its Marxist policies by arresting or executing those who resisted. Between 50,000 and 100,000 people were estimated to have been arrested and killed by communist troops in the countryside alone. Due to the repression, large parts of the country, especially the rural areas, organized into open revolt against the PDPA government. By spring 1979 unrest had reached 24 out of 28 Afghan provinces, including major urban areas. Over half of the Afghan army either deserted or joined the insurrection.
Believing that an uprising against the Soviet-backed communists would be supported by the people, Massoud, on July 6, 1979, started an insurrection in the Panjshir, which initially failed. Massoud decided to avoid conventional confrontation with the larger government forces and to wage a guerrilla war. He subsequently took full control of Panjshir, pushing out Afghan communist troops. Oliver Roy writes that in the following period, Massoud's "personal prestige and the efficiency of his military organisation persuaded many local commanders to come and learn from him."
Resistance against the Soviet Union (1979–1989).
Following the 1979 Soviet invasion and occupation of Afghanistan, Massoud devised a strategic plan for expelling the invaders and overthrowing the communist regime. The first task was to establish a popularly based resistance force that had the loyalty of the people. The second phase was "active defense" of the Panjshir stronghold, while carrying out asymmetric warfare. In the third phase, the "strategic offensive", Massoud's forces would gain control of large parts of Northern Afghanistan. The fourth phase was the "general application" of Massoud's principles to the whole country, and the defeat of the Afghan communist government.
From the start of the war, Massoud's mujahideen attacked the occupying Soviet forces, ambushing Soviet and Afghan communist convoys travelling through the Salang Pass, and causing fuel shortages in Kabul.
The Soviets mounted a series of offensives against the Panjshir. Between 1980 and 1985, these offensives were conducted twice a year. Despite engaging more men and hardware on each occasion, the Soviets were unable to defeat Massoud's forces. In 1982, the Soviets began deploying major combat units in the Panjshir, numbering up to 30,000 men. Massoud pulled his troops back into subsidiary valleys, where they occupied fortified positions. When the Soviet columns advanced onto these positions, they fell into ambushes. When the Soviets withdrew, Afghan army garrisons took over their positions. Massoud and his mujahideen forces attacked and recaptured them one by one.
In 1983, the Soviets offered Massoud a temporary truce, which he accepted in order to rebuild his own forces and give the civilian population a break from Soviet attacks. He put the respite to good use. In this time he created the Shura-e Nazar (Supervisory Council), which subsequently united 130 commanders from 12 Afghan provinces in their fight against the Soviet army. This council existed outside the Peshawar parties, which were prone to internecine rivalry and bickering, and served to smooth out differences between resistance groups, due to political and ethnic divisions. It was the predecessor of what could have become a unified Islamic Afghan army.
Relations with the party headquarters in Peshawar were often strained, as Rabbani insisted on giving Massoud no more weapons and supplies than to other Jamiat commanders, even those who did little fighting. To compensate for this deficiency, Massoud relied on revenues drawn from exports of emeralds and lapis lazuli, that are traditionally exploited in Northern Afghanistan.
To organize support for the mujahideen, Massoud established an administrative system that enforced law and order ("nazm") in areas under his control. The Panjshir was divided into 22 bases ("qarargah") governed by a military commander and a civilian administrator, and each had a judge, a prosecutor and a public defender. Massoud's policies were implemented by different committees: an economic committee was charged with funding the war effort. The health committee provided health services, assisted by volunteers from foreign humanitarian non-governmental organizations, such as Aide médicale internationale. An education committee was charged with the training of the military and administrative cadre. A culture committee and a judiciary committee were also created.
This expansion prompted Babrak Karmal to demand that the Red Army resume their offensives, in order to crush the Panjshir groups. However, Massoud had received warning of the attack through his intelligence agents in the government and he evacuated all 130,000 inhabitants from the valley into the Hindukush mountains, leaving the Soviet bombings to fall on empty ground and the Soviet battalions to face the mountains.
With the defeat of the Soviet-Afghan attacks, Massoud carried out the next phase of his strategic plan, expanding the resistance movement and liberating the northern provinces of Afghanistan. In August 1986, he captured Farkhar in Takhar Province. In November 1986, his forces overran the headquarters of the government's 20th division at Nahrin in Baghlan Province, scoring an important victory for the resistance. This expansion was also carried out through diplomatic means, as more mujahideen commanders were persuaded to adopt the Panjshir military system.
Despite almost constant attacks by the Red Army and the Afghan army, Massoud increased his military strength. Starting in 1980 with a force of less than 1,000 ill-equipped guerrillas, the Panjshir valley mujahideen grew to a 5,000-strong force by 1984. After expanding his influence outside the valley, Massoud increased his resistance forces to 13,000 fighters by 1989. These forces were divided into different types of units: the locals (mahalli) were tasked with static defense of villages and fortified positions. The best of the mahalli were formed into units called grup-i zarbati (shock troops), semi-mobile groups that acted as reserve forces for the defense of several strongholds. A different type of unit was the mobile group (grup-i-mutaharek), a lightly equipped commando-like formation numbering 33 men, whose mission was to carry out hit-and-run attacks outside the Panjshir, sometimes as far as 100 km from their base. These men were professional soldiers, well-paid and trained, and, from 1983 on, they provided an effective strike force against government outposts. Uniquely among the mujahideen, these groups wore uniforms, and their use of the "pakul" made this headwear emblematic of the Afghan resistance.
Massoud's military organization was an effective compromise between the traditional Afghan method of warfare and the modern principles of guerrilla warfare which he had learned from the works of Mao Zedong and Che Guevara. His forces were considered the most effective of all the various Afghan resistance movements.
The United States provided Massoud with comparatively less support than other factions. Part of the reason was that it permitted its funding and arms distribution to be administered by Pakistan, which favored the rival mujahideen leader Gulbuddin Hekmatyar. In an interview, Massoud said, "We thought the CIA knew everything. But they didn't. They supported some bad people Hekmatyar." Primary advocates for supporting Massoud were the US State Department's Edmund McWilliams and Peter Tomsen, who were on the ground in Afghanistan and Pakistan. Others included two Heritage Foundation foreign policy analysts, Michael Johns and James A. Phillips, both of whom championed Massoud as the Afghan resistance leader most worthy of U.S. support under the Reagan Doctrine. Thousands of foreign Islamic volunteers entered Afghanistan to fight with the mujahideen against the Soviet troops.
The Soviet army and the Afghan communist army were mainly defeated by Massoud and his mujahideen in numerous small engagements between 1984 and 1988. In 1989, after describing the Soviet Union's military engagement in Afghanistan "a bleeding wound", Soviet General Secretary Mikhail Gorbachev began a withdrawal of Soviet troops from the nation. On February 15, 1989, in what was depicted as an improbable victory for the mujahideen, the last Soviet soldier left the nation.
Fall of the Afghan communist regime (1992).
After the departure of Soviet troops in 1989, the People's Democratic Party of Afghanistan regime, then headed by Mohammad Najibullah, held its own against the mujahideen. Backed by a massive influx of weapons from the Soviet Union, the Afghan armed forces reached a level of performance they had never reached under direct Soviet tutelage. They maintained control over all of Afghanistan's major cities. By 1992, however, after the collapse of the Soviet Union, the regime began to crumble. Food and fuel shortages undermined the capacities of the government's army, and a resurgence of factionalism split the regime between Khalq and Parcham supporters.
A few days after Najibullah had lost control of the nation, his army commanders and governors arranged to turn over authority to resistance commanders and local warlords throughout the country. Joint councils ("shuras") were immediately established for local government, in which civil and military officials of the former government were usually included. In many cases, prior arrangements for transferring regional and local authority had been made between foes.
Collusions between military leaders quickly brought down the Kabul government. In mid-January 1992, within three weeks of the demise of the Soviet Union, Massoud was aware of conflict within the government's northern command. General Abdul Momim, in charge of the Hairatan border crossing at the northern end of Kabul's supply highway, and other non-Pashtun generals based in Mazari Sharif, feared removal by Najibullah and replacement by Pashtun officers. When the generals rebelled, Abdul Rashid Dostum, who held general rank as head of the Jowzjani militia, also based in Mazari Sharif, took over.
He and Massoud reached a political agreement, together with another major militia leader, Sayyed Mansour, of the Ismaili community based in Baghlan Province. These northern allies consolidated their position in Mazar-i-Sharif on March 21. Their coalition covered nine provinces in the north and northeast. As turmoil developed within the government in Kabul, no government force stood between the northern allies and the major air force base at Bagram, some seventy kilometers north of Kabul. By mid-April 1992, the Afghan air force command at Bagram had capitulated to Massoud. On March 18, 1992, Najibullah announced he would resign. On April 17, as his government fell, he tried to escape but was stopped at Kabul Airport by Dostum's forces. He took refuge at the United Nations mission, where he remained unharmed until 1996, while Massoud controlled the area surrounding the mission.
Senior communist generals and officials of the Najibullah administration acted as a transitional authority to transfer power to Ahmad Shah Massoud's alliance. The Kabul interim authority invited Massoud to enter Kabul as the new Head of State, but he held back. Massoud ordered his forces, positioned to the north of Kabul, not to enter the capital until a political solution was in place. He called on all the senior Afghan party leaders, many then based in exile in Peshawar, to work out a political settlement acceptable to all sides and parties.
Pakistani interference and war in Afghanistan (1992–today).
War in Kabul and other parts of the country (1992–1996).
Peace and power-sharing agreement (1992).
With United Nations support, most Afghan political parties decided to appoint a legitimate national government to succeed communist rule, through an elite settlement. While the external Afghan party leaders were residing in Peshawar, the military situation around Kabul involving the internal commanders was tense. A 1991 UN peace process brought about some negotiations, but the attempted elite settlement did not develop. In April 1992, resistance leaders in Peshawar tried to negotiate a settlement. Massoud supported the Peshawar process of establishing a broad coalition government inclusive of all resistance parties, but Hekmatyar sought to become the sole ruler of Afghanistan, stating, "In our country coalition government is impossible because, this way or another, it is going to be weak and incapable of stabilizing the situation in Afghanistan."
Massoud wrote:
All the parties had participated in the war, in jihad in Afghanistan, so they had to have their share in the government, and in the formation of the government. Afghanistan is made up of different nationalities. We were worried about a national conflict between different tribes and different nationalities. In order to give everybody their own rights and also to avoid bloodshed in Kabul, we left the word to the parties so they should decide about the country as a whole. We talked about it for a temporary stage and then after that the ground should be prepared for a general election.
A recorded radio communication between the two leaders showed the divide as Massoud asked Hekmatyar: "The Kabul regime is ready to surrender, so instead of the fighting we should gather. ... The leaders are meeting in Peshawar. ... The troops should not enter Kabul, they should enter later on as part of the government." Hekmatyar's response: "We will march into Kabul with our naked sword. No one can stop us. ... Why should we meet the leaders?" Massoud answered: "It seems to me that you don't want to join the leaders in Peshawar nor stop your threat, and you are planning to enter Kabul ... in that case I must defend the people."
At that point Osama bin Laden, trying to mediate, urged Hekmatyar to "go back with your brothers" and to accept a compromise. Bin Laden reportedly "hated Ahmad Shah Massoud". Bin Laden was involved in ideological and personal disputes with Massoud and had sided with Gulbuddin Hekmatyar against Massoud in the inner-Afghan conflict since the late 1980s. But Hekmatyar refused to accept a compromise, confident that he would be able to gain sole power in Afghanistan.
On April 24, 1992, the leaders in Peshawar agreed on and signed the Peshawar Accord, establishing the post-communist Islamic State of Afghanistan. The creation of the Islamic State was welcomed by the General Assembly of the United Nations and the Islamic State of Afghanistan was recognized as the legitimate entity representing Afghanistan until June 2002, when its successor, the Islamic Republic of Afghanistan, was established under the interim government of Hamid Karzai. Under the 1992 Peshawar Accord, the Defense Ministry was given to Massoud while the Prime Ministership was given to Hekmatyar. Hekmatyar refused to sign. With the exception of Hekmatyar's Hezb-e Islami, all of the other Peshawar resistance parties were unified under this peace and power-sharing accord in April 1992.
War against Hekmatyar (1992–1995).
Although repeatedly offered the position of prime minister, Gulbuddin Hekmatyar refused to recognize the peace and power-sharing agreement. His Hezb-e Islami militia initiated a massive bombardment campaign against the Islamic State and the capital city Kabul. Gulbuddin Hekmatyar received operational, financial and military support from neighboring Pakistan. The Director of the Centre for Arab and Islamic Studies at the Australian National University, Amin Saikal, writes in "Modern Afghanistan: A History of Struggle and Survival" that without Pakistan's support, Hekmatyar "would not have been able to target and destroy half of Kabul." Saikal states that Pakistan wanted to install a favorable regime under Hekmatyar in Kabul so that it could use Afghan territory for access to Central Asia.
Hekmatyar's rocket bombardments and the parallel escalation of violent conflict between two militias, Ittihad and Wahdat, which had entered some suburbs of Kabul, led to a breakdown in law and order. Shia Iran and Sunni Wahabbi Saudi Arabia, as competitors for regional hegemony, encouraged conflict between the Ittihad and Wahdat factions. On the one side was the Shia Hazara Hezb-i Wahdat of Abdul Ali Mazari and on the other side, the Sunni Pashtun Ittihad-i Islami of Abdul Rasul Sayyaf.
According to Human Rights Watch, Iran was strongly supporting the Hezb-i Wahdat forces, with Iranian intelligence officials providing direct orders, while Saudi Arabia supported Sayyaf and his Ittihad-i Islami faction to maximize Wahhabi influence. Kabul descended into lawlessness and chaos, as described in reports by Human Rights Watch and the Afghanistan Justice Project. Massoud's Jamiat commanders, the interim government, and the International Committee of the Red Cross (ICRC) repeatedly tried to negotiate ceasefires, which broke down in only a few days. Another militia, the Junbish-i Milli of former communist general Abdul Rashid Dostum, was backed by Uzbekistan. Uzbek president Karimov was keen to see Dostum controlling as much of Afghanistan as possible, especially in the north. Dostum repeatedly changed allegiances.
The Afghanistan Justice Project (AJP) says, that "while anti-government Hizb-i Islami is frequently named as foremost among the factions responsible for the deaths and destruction in the bombardment of Kabul, it was not the only perpetrator of these violations." According to the AJP, "the scale of the bombardment and kinds of weapons used represented disproportionate use of force" in a capital city with primarily residential areas by all the factions involved—including the government forces. Crimes were committed by individuals within the different armed factions. Gulbuddin Hekmatyar released 10,000 dangerous criminals from the main prisons into the streets of Kabul to destabilize the city and cut off Kabul from water, food and energy supplies. The Iran-controlled Wahdat of Abdul Ali Mazari, as well as the Ittihad of Abdul Rasul Sayyaf supported by Saudi Arabia, targeted civilians of the 'opposite side' in systematic atrocities. Abdul Rashid Dostum allowed crimes as a perceived payment for his troops. The Taliban, placing Kabul under a two-year siege and bombardment campaign from early 1995 onwards, in later years committed massacres against civilians, compared by United Nations observers to those that happened during the War in Bosnia.
"The major criticism of Massoud's human rights record" is the escalation of the Afshar military operation in 1993. A report by the Afghanistan Justice Project describes Massoud as failing to prevent atrocities carried out by his forces and those of their factional ally, Ittihad-i Islami, against civilians on taking the suburb of Afshar during a military operation against an anti-state militia allied to Gulbuddin Hekmatyar. They shelled residential areas in the capital city in February 1993. Critics said that Massoud should have foreseen these problems. A meeting convened by Massoud on the next day ordered a halt to killing and looting, but it failed to stop abuses. Human Rights Watch, in a report based largely on the material collected by the Afghanistan Justice Project, concurs that Massoud's Jamiat forces bear a share of the responsibility for human rights abuses throughout the war, including the indiscriminate targeting of civilians in Afshar, and that Massoud was personally implicated in some of these abuses. Roy Gutman has argued that the witness reports about Afshar cited in the AJP report implicated only the Ittihad forces, and that these had not been under Massoud's direct command.
Anthony Davis, who studied and observed Massoud's forces from 1981 to 2001, reported that during the observed period, there was "no pattern of repeated killings of enemy civilians or military prisoners" by Massoud's forces. Edward Girardet, who covered Afghanistan for over three decades, was also in Kabul during the war. He states that while Massoud was able to control most of his commanders well during the anti-Soviet and anti-Taliban resistance, he was not able to control every commander in Kabul. According to this and similar testimonies, this was due to a breakdown of law and order in Kabul and a war on multiple fronts, which they say, Massoud personally had done all in his power to prevent.
In 1993, Massoud created the Cooperative Mohammad Ghazali Culture Foundation ("Bonyad-e Farhangi wa Ta'wani Mohammad-e Ghazali") to further humanitarian assistance and politically independent Afghan culture. The Ghazali Foundation provided free medical services during some days of the week to residents of Kabul who were unable to pay for medical treatment. The Ghazali Foundation's department for distribution of auxiliary goods was the first partner of the Red Cross. The Ghazali Foundation's department of family consultation was a free advisory board, which was accessible seven days a week for the indigent. Although Massoud was responsible for the financing of the foundation, he did not interfere with its cultural work. A council led the foundation and a jury, consisting of impartial university lecturers, decided on the works of artists. The Ghazali foundation enabled Afghan artists to exhibit their works at different places in Kabul, and numerous artists and authors were honoured for their works; some of them neither proponents of Massoud nor the Islamic State government.
In March 1993, Massoud resigned his government position in exchange for peace, as requested by Hekmatyar, who considered him as a personal rival. According to the Islamabad Accord, Burhanuddin Rabbani, belonging to the same party as Massoud, remained president, while Gulbuddin Hekmatyar took the long-offered position of prime minister. Two days after the Islamabad Accord was put into effect, however, Hekmatyar's allies of Hezb-e Wahdat renewed rocket attacks in Kabul.
Both the Wahhabi Pashtun Ittehad-i Islami of Abdul Rasul Sayyaf backed by Saudi Arabia and the Shia Hazara Hezb-e Wahdat supported by Iran remained involved in heavy fighting against each other. Hekmatyar was afraid to enter Kabul proper, and chaired only one cabinet meeting. The author Roy Gutman of the United States Institute of Peace wrote in "How We Missed the Story: Osama bin Laden, the Taliban, and the Hijacking of Afghanistan":
Hekmatyar had become prime minister ... But after chairing one cabinet meeting, Hekmatyar never returned to the capital, fearing, perhaps, a lynching by Kabulis infuriated over his role in destroying their city. Even his close aides were embarrassed. Hekmatyar spokesman Qutbuddin Helal was still setting up shop in the prime minister's palace when the city came under Hezb[-i Islami] rocket fire late that month. "We are here in Kabul and he is rocketing us. Now we have to leave. We can't do anything," he told Massoud aides.
Hekmatyar, who was generally opposed to coalition government and struggled for undisputed power, had conflicts with other parties over the selection of cabinet members. His forces started major attacks against Kabul for one month. The President, Burhanuddin Rabbani, was attacked when he attempted to meet Hekmatyar. Massoud resumed his responsibilities as minister of defense.
In May 1993, a new effort was made to reinstate the Islamabad Accord. In August, Massoud reached out to Hekmatyar in an attempt to broaden the government. By the end of 1993, however, Hekmatyar and the former communist general and militia leader, Abdul Rashid Dostum, were involved in secret negotiations encouraged by Pakistan's secret Inter-Services Intelligence, Iran's intelligence service, and Uzbekistan's Karimov administration. They planned a coup to oust the Rabbani administration and to attack Massoud in his northern areas.
In January 1994, Hekmatyar and Dostum mounted a bombardment campaign against the capital and attacked Massoud's core areas in the northeast. Amin Saikal writes, Hekmatyar had the following objectives in all his operations:
The first was to make sure that Rabbani and Massoud were not allowed to consolidate power, build a credible administration, or expand their territorial control, so that the country would remain divided into small fiefdoms, run by various Muajhideen leaders and local warlords or a council of such elements, with only some of them allied to Kabul. The second was to ensure the Rabbani government acquired no capacity to dispense patronage, and to dissuade the Kabul population from giving more than limited support to the government. The third was to make Kabul an unsafe city for representatives of the international community and to prevent the Rabbani government from attracting the international support needed to begin the post-war reconstruction of Afghanistan and generate a level of economic activity which would enhance its credibility and popularity.
By mid-1994, Hekmatyar and Dostum were on the defensive in Kabul against Islamic State forces led by Massoud. By early 1995, the Islamic State had been able to secure the capital. Bombardment of the capital came to a halt. The government began to restore some law and order, and to start basic public services. Massoud initiated a nationwide political process with the goal of national consolidation and democratic elections. But the Taliban, which had emerged over the course of 1994 in southern Afghanistan, were already at the doors of the capital city.
Southern Afghanistan had been neither under the control of foreign-backed militias nor the government in Kabul, but was ruled by local Pashtun leaders, such as Gul Agha Sherzai, and their militias. In 1994, the Taliban (a movement originating from Jamiat Ulema-e-Islam-run religious schools for Afghan refugees in Pakistan) also developed in Afghanistan as a politico-religious force, reportedly in opposition to the tyranny of the local governor. When the Taliban took control of Kandahar in 1994, they forced the surrender of dozens of local Pashtun leaders who had presided over a situation of complete lawlessness and atrocities. In 1994, the Taliban took power in several provinces in southern and central Afghanistan.
Taliban siege of Kabul (1995–1996).
As the Islamic State had been able to consolidate control over the capital, the government took steps to restore law and order. Courts started to work again also convicting individuals inside government troops who had committed crimes. Massoud initiated a nationwide political process with the goal of national consolidation and democratic elections. He arranged a conference in three parts uniting political and cultural personalities, governors, commanders, clergymen and representatives, in order to reach a lasting agreement. Massoud, like most people in Afghanistan, saw this conference as a small hope for democracy and for free elections. His favourite for candidacy to the presidency was Dr. Mohammad Yusuf, the first democratic prime minister under Zahir Shah, the former king. In the first meeting representatives from 15 different Afghan provinces met, in the second meeting there were already 25 provinces participating.
Massoud also invited the Taliban to join the peace process wanting them to be a partner in providing stability to Afghanistan during such a process. Against the advice of his security personnel, he went to talk to some Taliban leaders in Maidan Shar, Taliban territory. The Taliban declined to join the peace process leading towards general elections. When Massoud returned to Kabul unharmed, the Taliban leader who had received him as his guest paid with his life: he was killed by other senior Taliban for failing to assassinate Massoud while the possibility had presented itself.
Neighboring Pakistan exerted strong influence over the Taliban. A publication with the George Washington University describes: "Initially, the Pakistanis supported ... Gulbuddin Hekmatyar ... When Hekmatyar failed to deliver for Pakistan, the administration began to support a new movement of religious students known as the Taliban." Many analysts like Amin Saikal describe the Taliban as developing into a proxy force for Pakistan's regional interests which the Taliban decline. The Taliban started shelling Kabul in early 1995 but were defeated by forces of the Islamic State government under Ahmad Shah Massoud. () Amnesty International, referring to the Taliban offensive, wrote in a 1995 report:
The Taliban's early victories in 1994 were followed by a series of defeats that resulted in heavy losses. The Taliban's first major offensive against the important western city of Herat, under the rule of Islamic state ally Ismail Khan, in February 1995 was defeated when Massoud airlifted 2,000 of his own core forces from Kabul to help defend Herat. Ahmed Rashid writes: "The Taliban had now been decisively pushed back on two fronts by the government and their political and military leadership was in disarray. Their image as potential peacemakers was badly dented, for in the eyes of many Afghans they had become nothing more than just another warlord party." International observers already speculated that the Taliban as a country-wide organization might have "run its course".
Mullah Omar, however, consolidated his control of the Taliban and with foreign help rebuilt and re-equipped his forces. Pakistan increased its support to the Taliban. Its military advisers oversaw the restructuring of Taliban forces. The country provided armored pick-up trucks and other military equipment. Saudi Arabia provided the funding. Furthermore, there was a massive influx of 25,000 new Taliban fighters, many of them recruited in Pakistan. This enabled the Taliban to capture Herat to the west of Kabul in a surprise attack against the forces of Ismail Khan in September 1995. A nearly one-year siege and bombardment campaign against Kabul, however, was again defeated by Massoud's forces.
Massoud and Rabbani meanwhile kept working on an internal Afghan peace process—successfully. By February 1996, all of Afghanistan's armed factions—except for the Taliban—had agreed to take part in the peace process and to set up a peace council to elect a new interim president. Many Pashtun areas under Taliban control had representatives also advocating for a peace agreement with the Islamic State government. But Taliban leader Mullah Omar and the Kandaharis surrounding him wanted to expand the war. At that point the Taliban leadership and their foreign supporters decided they needed to act quickly before the government could consolidate the new understanding between the parties. The Taliban moved against Jalalabad, under the control of the Pashtun Jalalabad Shura, to the east of Kabul. Part of the Jalalabad Shura was bribed with millions of dollars by the Taliban's foreign sponsors, especially Saudi Arabia, to vacate their positions. The Taliban's battle for Jalalabad was directed by Pakistani military advisers. Hundreds of Taliban crossed the Afghan-Pakistani border moving on Jalalabad from Pakistan and thereby suddenly placed to the east of Kabul. This left the capital city Kabul "wide open" to many sides as Ismail Khan had been defeated to the west, Gulbuddin Hekmatyar had vacated his positions to the south and the fall and surrender of Jalalabad had suddenly opened a new front to the east. At that point Massoud decided to conduct a strategic retreat through a northern corridor, according to Ahmed Rashid, "knowing he could not defend from attacks coming from all four points of the compass. Nor did he want to lose the support of Kabul's population by fighting for the city and causing more bloodshed." On September 26, 1996, as the Taliban with military support by Pakistan and financial support by Saudi Arabia prepared for another major offensive, Massoud ordered a full retreat from Kabul. The Taliban marched into Kabul on September 27, 1996, and established the Islamic Emirate of Afghanistan. Massoud and his troops retreated to the northeast of Afghanistan which became the base for the still internationally recognized Islamic State of Afghanistan.
Resistance against the Taliban (1996–2001).
United Front against the Taliban.
Ahmad Shah Massoud created the United Front (Northern Alliance) against the Taliban advance. The United Front included forces and leaders from different political backgrounds as well as from all ethnicities of Afghanistan. From the Taliban conquest in 1996 until November 2001, the United Front controlled territory in which roughly 30% of Afghanistan's population was living, in provinces such as Badakhshan, Kapisa, Takhar and parts of Parwan, Kunar, Nuristan, Laghman, Samangan, Kunduz, Ghōr and Bamyan.
Meanwhile, the Taliban imposed their repressive regime in the parts of Afghanistan under their control. Hundreds of thousands of people fled to Northern Alliance territory, Pakistan and Iran. Massoud's soldiers held some 1,200 Taliban prisoners in the Panjshir Valley, 122 of them foreign Muslims who had come to Afghanistan to fight a jihad.
In 1998, after the defeat of Abdul Rashid Dostum's faction in Mazar-i-Sharif, Ahmad Shah Massoud remained the only main leader of the United Front in Afghanistan and the only leader who was able to defend vast parts of his area against the Taliban. Most major leaders including the Islamic State's President Burhanuddin Rabbani, Abdul Rashid Dostum, and others, were living in exile. During this time, commentators remarked that "The only thing standing in the way of future Taliban massacres is Ahmad Shah Massoud."
Massoud stated that the Taliban repeatedly offered him a position of power to make him stop his resistance. He declined, declaring the differences between their ideology and his own pro-democratic outlook on society to be insurmountable.
Massoud wanted to convince the Taliban to join a political process leading towards democratic elections in a foreseeable future. He also predicted that without assistance from Pakistan and external extremist groups, the Taliban would lose their hold on power.
In early 2001, the United Front employed a new strategy of local military pressure and global political appeals. Resentment was increasingly gathering against Taliban rule from the bottom of Afghan society including the Pashtun areas. At the same time, Massoud was very wary not to revive the failed Kabul government of the early 1990s. Already in 1999 the United Front leadership ordered the training of police forces specifically to keep order and protect the civilian population in case the United Front would be successful.
Cross-factional negotiations.
From 1999 onwards, a renewed process was set into motion by the Tajik Ahmad Shah Massoud and the Pashtun Abdul Haq to unite all the ethnicities of Afghanistan. Massoud united the Tajiks, Hazara and Uzbeks as well as several Pashtun commanders under his United Front. Besides meeting with Pashtun tribal leaders and acting as a point of reference, Abdul Haq received increasing numbers of Pashtun Taliban themselves who were secretly approaching him. Some commanders who had worked for the Taliban military apparatus agreed to the plan to topple the Taliban regime as the Taliban lost support even among the Pashtuns. Senior diplomat and Afghanistan expert Peter Tomsen wrote that ""‘Lion of Kabul’ [Abdul Haq and the ‘Lion of Panjshir’ Shah Massoud would make a formidable anti-Taliban team if they combined forces. Haq, Massoud, and Karzai, Afghanistan’s three leading moderates, could transcend the Pashtun—non-Pashtun, north-south divide."" Steve Coll referred to this plan as a "grand Pashtun-Tajik alliance". The senior Hazara and Uzbek leaders took part in the process just like later Afghan president Hamid Karzai. They agreed to work under the banner of the exiled Afghan king Zahir Shah in Rome.
In November 2000, leaders from all ethnic groups were brought together in Massoud's headquarters in northern Afghanistan, travelling from other parts of Afghanistan, Europe, the United States, Pakistan and India to discuss a Loya Jirga for a settlement of Afghanistan's problems and to discuss the establishment of a post-Taliban government. In September 2001, an international official who met with representatives of the alliance remarked, ""It's crazy that you have this today ... Pashtuns, Tajiks, Uzbeks, Hazara ... They were all ready to buy in to the process"."
In early 2001, Ahmad Shah Massoud with leaders from all ethnicities of Afghanistan addressed the European Parliament in Brussels, asking the international community to provide humanitarian aid to the people of Afghanistan. He stated that the Taliban and Al Qaeda had introduced "a very wrong perception of Islam" and that without the support of Pakistan and Bin Laden the Taliban would not be able to sustain their military campaign for up to a year. On that visit to Europe, he also warned the US about Bin Laden.
The areas of Massoud.
Life in the areas under direct control of Massoud was different from the life in the areas under Taliban or Dostum's control. In contrast to the time of chaos in which all structures had collapsed in Kabul, Massoud was able to control most of the troops under his direct command well during the period starting in late 1996. Massoud always controlled the Panjshir, Takhar, parts of Parwan and Badakhshan during the war. Some other provinces (notably Kunduz, Baghlan, Nuristan and the north of Kabul) were captured by his forces from the Taliban and lost again from time to time as the frontlines varied.
Massoud created democratic institutions which were structured into several committees: political, health, education and economic. Still, many people came to him personally when they had a dispute or problem and asked him to solve their problems.
In September 2000, Massoud signed the Declaration of the Essential Rights of Afghan Women drafted by Afghan women. The declaration established gender equality in front of the law and the right of women to political participation, education, work, freedom of movement and speech. In the areas of Massoud, women and girls did not have to wear the Afghan burqa by law. They were allowed to work and to go to school. Although it was a time of war, girls' schools were operating in some districts. In at least two known instances, Massoud personally intervened against cases of forced marriage in favour of the women to make their own choice.
While it was Massoud's stated personal conviction that men and women are equal and should enjoy the same rights, he also had to deal with Afghan traditions which he said would need a generation or more to overcome. In his opinion, that could only be achieved through education. Author Pepe Escobar wrote in "Massoud: From Warrior to Statesman":
Humayun Tandar, who took part as an Afghan diplomat in the 2001 International Conference on Afghanistan in Bonn, said that "strictures of language, ethnicity, region were stifling for Massoud. That is why ... he wanted to create a unity which could surpass the situation in which we found ourselves and still find ourselves to this day." This applied also to strictures of religion. Jean-José Puig describes how Massoud often led prayers before a meal or at times asked his fellow Muslims to lead the prayer but also did not hesitate to ask the Jewish Princeton Professor Michael Barry or his Christian friend Jean-José Puig: "Jean-José, we believe in the same God. Please, tell us the prayer before lunch or dinner in your own language."
International relations.
U.S. policy regarding Massoud, the Taliban and Afghanistan remains ambiguous and differed between the various U.S. government agencies.
In 1997, U.S. State Department's Robin Raphel suggested to Massoud he should surrender to the Taliban. He soundly rejected the proposal.
At one point in the war, in 1997, two top foreign policy officials in the Clinton administration flew to northern Afghanistan in an attempt to convince Massoud not to take advantage of a strategic opportunity to make crucial gains against the Taliban.
In 1998, a U.S. Defense Intelligence Agency analyst, Julie Sirrs, visited Massoud's territories privately, having previously been denied official permission to do so by her agency. She reported that Massoud had conveyed warnings about strengthened ties between the Taliban and foreign Islamist terrorists. Returning home, she was sacked from her agency for insubordination, because at that time the U.S. administration had no trust in Massoud.
In the meantime, the only collaboration between Massoud and another U.S. intelligence service, the Central Intelligence Agency (CIA), consisted of an effort to trace Osama bin Laden following the 1998 embassy bombings. The U.S. and the European Union provided no support to Massoud for the fight against the Taliban.
A change of policy, lobbied for by CIA officers on the ground who had visited the area of Massoud, regarding support to Massoud, was underway in the course of 2001. According to Steve Coll's book "Ghost Wars" (who won the 2005 Pulitzer Prize for General Non-Fiction):
U.S. Congressman Dana Rohrabacher also recalled: etween Bush's inauguration and 9/11, I met with the new national security staff on 3 occasions, including one meeting with Condoleezza Rice to discuss Afghanistan. There were, in fact, signs noted in an overview story in The Washington Post about a month ago that some steps were being made to break away from the previous administration's Afghan policy. CIA lawyers, working with officers in the Near East Division and Counterterrorist Center, began to draft a formal, legal presidential finding for Bush's signature authorizing a new covert action program in Afghanistan, the first in a decade that sought to influence the course of the Afghan war in favour of Massoud. This change in policy was finalized in August 2001 when it was too late.
After Pakistan had funded, directed and supported the Taliban's rise to power in Afghanistan, Massoud and the United Front received some assistance from India. India was particularly concerned about Pakistan's Taliban strategy and the Islamic militancy in its neighborhood; it provided U.S.$70 million in aid including two Mi-17 helicopters, three additional helicopters in 2000 and US$8 million worth of high-altitude equipment in 2001. Also In the 1990s, India had run a famous field hospital at Farkor on the Tajik-Afghan border to treat wounded fighters from the then Northern Alliance that was battling the Taliban regime in Afghanistan.
It was at the very same hospital that the Northern Alliance leader Ahmed Shah Masood was pronounced dead after being assassinated just two days before the 9/11 terror strikes in 2001. Furthermore, the alliance supposedly also received minor aid from Tajikistan, Russia and Iran because of their opposition to the Taliban and the Pakistani control over the Taliban's Emirate. Their support, however, remained limited to the most needed things. Meanwhile, Pakistan engaged up to 28,000 Pakistani nationals and regular Pakistani army troops to fight alongside the Taliban and Al Qaeda forces against Massoud.
In April 2001, the president of the European Parliament, Nicole Fontaine (who called Massoud the "pole of liberty in Afghanistan"), invited Massoud with the support of French and Belgian politicians to address the European Parliament in Brussels, Belgium. In his speech, he asked for humanitarian aid for the people of Afghanistan. Massoud further went on to warn that his intelligence agents had gained limited knowledge about a large-scale terrorist attack on U.S. soil being imminent.
Assassination.
Massoud, then aged 48, was the target of a suicide attack at Khwaja Bahauddin, in Takhar Province in northeastern Afghanistan on September 9, 2001. The attackers' names were alternately given as Dahmane Abd al-Sattar, husband of Malika El Aroud, and Bouraoui el-Ouaer; or 34-year-old Karim Touzani and 26-year-old Kacem Bakkali.
The attackers claimed to be Belgians originally from Morocco. According to "Le Monde" they transited through the municipality of Molenbeek. Their passports turned out to be stolen and their nationality was later determined to be Tunisian. Waiting for almost three weeks (during which they also interviewed Burhanuddin Rabbani and Abdul Rasul Sayyaf) for an interview opportunity, on September 8, 2001, an aide to Massoud recalls the would-be suicide attackers "were so worried" and threatened to leave if the interview did not happen in the next 24 hours (until September 10, 2001). They were finally granted an interview. During the interview, they set off a bomb composed of explosives hidden in the camera and in a battery-pack belt. Commander Massoud died in a helicopter that was taking him to an Indian military field hospital at Farkhor in nearby Tajikistan. The explosion also killed Mohammed Asim Suhail, a United Front official, while Mohammad Fahim Dashty and Massoud Khalili were injured. One of the suicide attackers, Bouraoui, was killed by the explosion, while Dahmane Abd al-Sattar was captured and shot while trying to escape.
Despite initial denials by the United Front, news of Massoud's death was reported almost immediately, appearing on the BBC, and in European and North American newspapers on September 10, 2001. On September 16, the United Front officially announced that Massoud had died of injuries in the suicide attack. Massoud was buried in his home village of Bazarak in the Panjshir Valley. The funeral, although in a remote rural area, was attended by hundreds of thousands of people. ().
Massoud had survived assassination attempts over a period of 26 years, including attempts made by al-Qaeda, the Taliban, the Pakistani ISI and before them the Soviet KGB, the Afghan communist KHAD and Hekmatyar. The first attempt on Massoud's life was carried out by Hekmatyar and two Pakistani ISI agents in 1975 when Massoud was 22 years old. In early 2001, al-Qaeda would-be assassins were captured by Massoud's forces while trying to enter his territory.
The assassination of Massoud is considered to have a strong connection to the September 11 attacks in 2001 on U.S. soil, which killed nearly 3,000 people. It appeared to have been the major terrorist attack which Massoud had warned against in his speech to the European Parliament several months earlier.
Analysts believe Osama bin Laden ordered Massoud's assassination to help his Taliban protectors and ensure he would have their co-operation in Afghanistan. Following the assassination, bin Laden had an emissary deliver Dahmane Abd al-Sattar's widow a cassette of him speaking of his love for his wife and his decision to blow himself up, as well as $500 in an envelope to settle a debt. The Pakistani Inter-Services Intelligence (ISI) and Abdul Rasul Sayyaf, an Afghan Wahhabi Islamist, have also been mentioned as possible organizers or collaborators of the Massoud assassins. The assassins are said to have entered United Front (Northern Alliance) territory under the auspices of Abdul Rasul Sayyaf and had his assistance in bypassing "normal security procedures."
Investigative commission
In April 2003, the Karzai administration announced the creation of a commission to investigate the assassination of Massoud. In 2003, French investigators announced that they and the FBI had been able to trace the provenance of the camera used in the assassination, which had been stolen in France some time earlier.
Legacy.
National Hero of Afghanistan.
Massoud was the only chief Afghan leader who never left Afghanistan in the fight against the Soviet Union and later in the fight against the Taliban Emirate. In the areas under his direct control, such as Panjshir, some parts of Parwan and Takhar, Massoud established democratic institutions. One refugee who cramped his family of 27 into an old jeep to flee from the Taliban to the area of Massoud described Massoud's territory in 1997 as "the last tolerant corner of Afghanistan".
One man holds a greater political punch than all 18 living presidential candidates combined. Though already dead for three years... Since his death on September 9, 2001 at the hands of two al Qaeda-linked Islamic radicals, Massoud has been transformed from mujahedin to national hero—if not saint. Pictures of Massoud, the Afghan mujahedin who battled the Soviets, other warlords, and the Taliban for more than 20 years, vastly outnumber those of any other Afghan including those of Karzai.
Today Panjshir, the home of Massoud,
is arguably the most peaceful place in the entire country. A small US military reconstruction team is based here, but there are none of the signs of foreign occupation that exist elsewhere. Even Afghan soldiers are few and far between. Instead, the people like to boast about how they keep their own security.
The road near the Afghanistan Embassy is a "symbol of ties" that binds the two nations that have always "enjoyed excellent relations"
Lion of Panjshir.
Massoud's byname, "Lion of Panjshir" (, "Shir-e-Panjshir"), earned for his role during the Soviet occupation, is a rhyming play on words in Persian, as the name of the valley means "five lions".
The "Wall Street Journal" referred to Massoud as "The Afghan Who Won the Cold War", referring to the global significance of the Soviet defeat in Afghanistan for the subsequent collapse of the Eastern Bloc.
Views on Pakistan and potential al-Qaeda attacks.
In the spring 2001, Ahmad Shah Massoud had addressed the European Parliament in Brussels, saying that Pakistan was behind the situation in Afghanistan. He also said that he believed that, without the support of Pakistan, Osama bin Laden, and Saudi Arabia, the Taliban would not be able to sustain their military campaign for up to a year. He said the Afghan population was ready to rise against them. Addressing the United States specifically, he warned that should the U.S. not work for peace in Afghanistan and put pressure on Pakistan to cease their support to the Taliban, the problems of Afghanistan would soon become the problems of the U.S. and the world.
Declassified Defense Intelligence Agency (DIA) documents from November 2001 show that Massoud had gained "limited knowledge... regarding the intentions of Al-Qaeda to perform a terrorist act against the U.S. on a scale larger than the 1998 bombing of the U.S. Embassies in Kenya and Tanzania." They noted that he warned about such attacks.
Personal life.
Massoud was married to Sediqa Massoud. They have one son (Ahmad born in 1989) and five daughters (Fatima born in 1992, Mariam born in 1993, Ayesha born in 1995, Zohra born in 1996 and Nasrine born in 1998). In 2005 Sediqa Massoud published a personal account on her life with Massoud (co-authored by two women's rights activists and friends of Sediqa Massoud, and Marie-Francoise Colombani) called ""Pour l'amour de Massoud"" (For the love of Massoud), in which she describes a decent and loving husband.
The family has a great deal of prestige in the politics of Afghanistan. One of his six brothers, Ahmad Zia Massoud, was the Vice President of Afghanistan from 2004 until 2009 under the first democratically elected government of Afghanistan. Unsuccessful attempts have been made on the life of Ahmad Zia Massoud in 2004 and late 2009. The Associated Press reported that 8 Afghans died in the attempt on Ahmad Zia Massoud's life. Ahmad Zia Massoud leads the National Front of Afghanistan (a United Front group).
Another brother, Ahmad Wali Massoud, was Afghanistan's Ambassador to the United Kingdom from 2002 to 2006. He is a member of Abdullah Abdullah's National Coalition of Afghanistan (another United Front group).

</doc>
<doc id="2178" url="https://en.wikipedia.org/wiki?curid=2178" title="Atlantis">
Atlantis

Atlantis (, "island of Atlas") is a fictional island mentioned within an allegory on the hubris of nations in Plato's works "Timaeus" and "Critias", where it represents the antagonist naval power that besieges "Ancient Athens", the pseudo-historic embodiment of Plato's ideal state (see "The Republic"). In the story, Athens repels the Atlantean attack, unlike any other nation of the (western) known world, supposedly giving testament to the superiority of Plato's concept of a state. At the end of the story, Atlantis eventually falls out of favor with the gods and famously submerges into the Atlantic Ocean.
Despite its minor importance in Plato's work, the Atlantis story has had a considerable impact on literature. The allegorical aspect of Atlantis was taken up in utopian works of several Renaissance writers, such as Francis Bacon's "New Atlantis" and Thomas More's "Utopia". On the other hand, 19th-century amateur scholars misinterpreted Plato's account as historical tradition, most notably in Ignatius L. Donnelly's "". Plato's vague indications of the time of the events—more than 9,000 years before his day—and the alleged location of Atlantis—"beyond the Pillars of Hercules"—has led to much pseudoscientific speculation. As a consequence, Atlantis has become a byword for any and all supposed advanced prehistoric lost civilizations and continues to inspire contemporary fiction, from comic books to films.
While present-day philologists and historians accept the story's fictional character, there is still debate on what served as its inspiration. The fact that Plato borrowed some of his allegories and metaphors—most notably the story of Gyges—from older traditions has caused a number of scholars to investigate possible inspiration of Atlantis from Egyptian records of the Thera eruption, the Sea Peoples invasion, or the Trojan War. Others have rejected this chain of tradition as implausible and insist that Plato designed the story from scratch, drawing loose inspiration from contemporary events like the failed Athenian invasion of Sicily in 415–413 BC or the destruction of Helike in 373 BC.
Plato's account.
In one version of events Plato did not hear the original myth of Atlantis, but instead it was told to Solon 300 years prior, who heard it from Egyptian priests who read it from existing texts. Plato heard it from Critias, who is related to Solon, who said it was a story passed down for 3 generations prior to reaching him (according to RA Freund 2012). E Voegelin on the contrary, states that Plato entirely created the myth to aid the transmission of meaning in Timaeus, and that the story with regards to Solon has no basis in historical fact.
Plato's dialogues "Timaeus" and "Critias", written in 360 BC, contain the earliest references to Atlantis. For unknown reasons, Plato never completed "Critias". Plato introduced Atlantis in "Timaeus":
The four people appearing in those two dialogues are the politicians Critias and Hermocrates as well as the philosophers Socrates and Timaeus of Locri, although only Critias speaks of Atlantis. In his works Plato makes extensive use of the Socratic method in order to discuss contrary positions within the context of a supposition.
The "Timaeus" begins with an introduction, followed by an account of the creations and structure of the universe and ancient civilizations. In the introduction, Socrates muses about the perfect society, described in Plato's "Republic" (c. 380 BC), and wonders if he and his guests might recollect a story which exemplifies such a society. Critias mentions an allegedly historical tale that would make the perfect example, and follows by describing Atlantis as is recorded in the "Critias". In his account, ancient Athens seems to represent the "perfect society" and Atlantis its opponent, representing the very antithesis of the "perfect" traits described in the "Republic".
"Critias".
According to Critias, the Hellenic gods of old divided the land so that each god might have their own lot; Poseidon was appropriately, and to his liking, bequeathed the island of Atlantis. The island was larger than Ancient Libya and Asia Minor combined, but it was later sunk by an earthquake and became an impassable mud shoal, inhibiting travel to any part of the ocean. The Egyptians, Plato asserted, described Atlantis as an island consisting mostly of mountains in the northern portions and along the shore and encompassing a great plain in an oblong shape in the south "extending in one direction three thousand "stadia" 555 km; 345 mi, but across the center inland it was two thousand stadia 370 km; 230 mi." Fifty stadia 6 mi from the coast was a mountain that was low on all sides ... broke it off all round about ... the central island itself was five stades in diameter 0.92 km; 0.57 mi.
In Plato's myth, Poseidon fell in love with Cleito, the daughter of Evenor and Leucippe, who bore him five pairs of male twins. The eldest of these, Atlas, was made rightful king of the entire island and the ocean (called the Atlantic Ocean in his honor), and was given the mountain of his birth and the surrounding area as his fiefdom. Atlas's twin Gadeirus, or Eumelus in Greek, was given the extremity of the island towards the pillars of Hercules. The other four pairs of twins—Ampheres and Evaemon, Mneseus and Autochthon, Elasippus and Mestor, and Azaes and Diaprepes—were also given "rule over many men, and a large territory."
Poseidon carved the mountain where his love dwelt into a palace and enclosed it with three circular moats of increasing width, varying from one to three stadia and separated by rings of land proportional in size. The Atlanteans then built bridges northward from the mountain, making a route to the rest of the island. They dug a great canal to the sea, and alongside the bridges carved tunnels into the rings of rock so that ships could pass into the city around the mountain; they carved docks from the rock walls of the moats. Every passage to the city was guarded by gates and towers, and a wall surrounded each of the city's rings. The walls were constructed of red, white and black rock quarried from the moats, and were covered with brass, tin and the precious metal orichalcum, respectively.
According to Critias, 9,000 years before his lifetime a war took place between those outside the Pillars of Hercules at the Strait of Gibraltar and those who dwelt within them. The Atlanteans had conquered the parts of Libya within the Pillars of Hercules as far as Egypt and the European continent as far as Tyrrhenia, and subjected its people to slavery. The Athenians led an alliance of resistors against the Atlantean empire, and as the alliance disintegrated, prevailed alone against the empire, liberating the occupied lands.
"But at a later time there occurred portentous earthquakes and floods, and one grievous day and night befell them, when the whole body of your warriors was swallowed up by the earth, and the island of Atlantis in like manner was swallowed up by the sea and vanished; wherefore also the ocean at that spot has now become impassable and unsearchable, being blocked up by the shoal mud which the island created as it settled down."
The logographer Hellanicus of Lesbos wrote an earlier work titled "Atlantis", of which only a few fragments survive. Hellanicus' work appears to have been a genealogical one concerning the daughters of Atlas (Ἀτλαντὶς in Greek means "of Atlas"), but some authors have suggested a possible connection with Plato's island. John V. Luce notes that when Plato writes about the genealogy of Atlantis's kings, he writes in the same style as Hellanicus, suggesting a similarity between a fragment of Hellanicus's work and an account in the "Critias". Rodney Castleden suggests that Plato may have borrowed his title from Hellanicus, who may have based his work on an earlier work about Atlantis.
Castleden has pointed out that Plato wrote of Atlantis in 359 BC, when he returned to Athens from Sicily. He notes a number of parallels between the physical organisation and fortifications of Syracuse and Plato's description of Atlantis. Gunnar Rudberg was the first who elaborated upon the idea that Plato's attempt to realize his political ideas in the city of Syracuse could have heavily inspired the Atlantis account.
Interpretations.
Ancient.
Some ancient writers viewed Atlantis as fiction; others believed it to be real. The philosopher Crantor, a student of Plato's student Xenocrates, is often cited as an example of a writer who thought the story to be historical fact. His work, a commentary on Plato's "Timaeus", is lost, but Proclus, a Neoplatonist of the 5th century AD, reports on it. The passage in question has been represented in the modern literature either as claiming that Crantor actually visited Egypt, had conversations with priests, and saw hieroglyphs confirming the story or as claiming that he learned about them from other visitors to Egypt. Proclus wrote:
The next sentence is often translated "Crantor adds, that this is testified by the prophets of the Egyptians, who assert that these particulars are narrated by Plato are written on pillars which are still preserved." But in the original, the sentence starts not with the name Crantor but with the ambiguous "He"; whether this referred to Crantor or to Plato is the subject of considerable debate. Proponents of both Atlantis as a myth and Atlantis as history have argued that the word refers to Crantor.
Alan Cameron argues that it should be interpreted as referring to Plato, and that when Proclus writes that "we must bear in mind concerning this whole feat of the Athenians, that it is neither a mere myth nor unadorned history, although some take it as history and others as myth", he is treating "Crantor's view as mere personal opinion, nothing more; in fact he first quotes and then dismisses it as representing one of the two unacceptable extremes".
Cameron also points out that whether "he" refers to Plato or to Crantor, the statement does not support conclusions such as Otto Muck's "Crantor came to Sais and saw there in the temple of Neith the column, completely covered with hieroglyphs, on which the history of Atlantis was recorded. Scholars translated it for him, and he testified that their account fully agreed with Plato's account of Atlantis" or J. V. Luce's suggestion that Crantor sent "a special enquiry to Egypt" and that he may simply be referring to Plato's own claims.
Another passage from Proclus' commentary on the "Timaeus" gives a description of the geography of Atlantis: That an island of such nature and size once existed is evident from what is said by certain authors who investigated the things around the outer sea. For according to them, there were seven islands in that sea in their time, sacred to Persephone, and also three others of enormous size, one of which was sacred to Hades, another to Ammon, and another one between them to Poseidon, the extent of which was a thousand stadia [200 km]; and the inhabitants of it—they add—preserved the remembrance from their ancestors of the immeasurably large island of Atlantis which had really existed there and which for many ages had reigned over all islands in the Atlantic sea and which itself had like-wise been sacred to Poseidon. Now these things Marcellus has written in his "Aethiopica"". Marcellus remains unidentified.
Other ancient historians and philosophers who believed in the existence of Atlantis were Strabo and Posidonius.
The 4th-century historian Ammianus Marcellinus, relying on a lost work by Timagenes, a historian writing in the 1st century BC, writes that the Druids of Gaul said that part of the inhabitants of Gaul had migrated there from distant islands. Some have understood Ammianus's testimony as a claim that at the time of Atlantis's actual sinking into the sea, its inhabitants fled to western Europe; but Ammianus in fact says that "the Drasidae (Druids) recall that a part of the population is indigenous but others also migrated in from islands and lands beyond the Rhine" ("Res Gestae" 15.9), an indication that the immigrants came to Gaul from the north (Britain, the Netherlands or Germany), not from a theorized location in the Atlantic Ocean to the south-west. Instead, the Celts that dwelled along the ocean were reported to venerate twin gods (Dioscori) that appeared to them coming from that ocean.
Jewish and Christian.
The Hellenistic Jewish philosopher Philo in the early 1st century AD wrote about the destruction of Atlantis in his "On the Eternity of the World", xxvi. 141, in a longer passage allegedly citing Aristotle's successor Theophrastus:
There is the possibility that Clement of Rome cryptically referred to Atlantis in his First Epistle of Clement, 20: 8:
The theologian Joseph Barber Lightfoot ("Apostolic Fathers", 1885, II, p. 84) noted on this passage: "Clement may possibly be referring to some known, but hardly accessible land, lying without the pillars of Hercules. But more probably he contemplated some unknown land in the far west beyond the ocean, like the fabled Atlantis of Plato ..."
Other early Christian writers wrote about Atlantis, though they had mixed views on whether it once existed or was an untrustworthy myth of pagan origin. Tertullian believed Atlantis was once real and wrote that in the Atlantic Ocean once existed "isle that was equal in size to Libya or Asia" referring to Plato's geographical description of Atlantis. The early Christian apologist writer Arnobius also believed Atlantis once existed but blamed its destruction on pagans.
Cosmas Indicopleustes in the 6th century wrote of Atlantis in his "Christian Topography" in an attempt to prove his theory that the world was flat and surrounded by water:
A Hebrew treatise on computational astronomy dated to AD 1378/79, alludes to the Atlantis myth in a discussion concerning the determination of zero points for the calculation of longitude:
Modern.
Aside from Plato's original account, modern interpretations regarding Atlantis are an amalgamation of diverse, speculative movements that began in the 16th century. Contemporary perceptions of Atlantis share roots with Mayanism, which can be traced to the beginning of the Modern Age, when European imaginations were fueled by their initial encounters with the indigenous peoples of the New World. From this era sprang apocalyptic and utopian visions that would inspire many subsequent generations of theorists.
Most of these interpretations are considered pseudohistory, pseudoscience, or pseudoarchaeology, as they have presented their works as academic or scientific, but lack the standards or criteria.
The Flemish cartographer and geographer Abraham Ortelius is believed to have been the first person to imagine that the continents were joined together before drifting to their present positions. In the 1596 edition of his "Thesaurus Geographicus" he wrote: "Unless it be a fable, the island of Gadir or Gades will be the remaining part of the island of Atlantis or America, which was not sunk (as Plato reports in the "Timaeus") so much as torn away from Europe and Africa by earthquakes and flood... The traces of the ruptures are shown by the projections of Europe and Africa and the indentations of America in the parts of the coasts of these three said lands that face each other to anyone who, using a map of the world, carefully considered them. So that anyone may say with Strabo in Book 2, that what Plato says of the island of Atlantis on the authority of Solon is not a figment."
Atlantis pseudohistory.
Early influential literature.
The term "utopia" (from "no place") was coined by Sir Thomas More in his 16th-century work of fiction "Utopia". Inspired by Plato's Atlantis and travelers' accounts of the Americas, More described an imaginary land set in the New World. His idealistic vision established a connection between the Americas and utopian societies, a theme which was further solidified by Sir Francis Bacon in "The New Atlantis" (c. 1623). A character in the narrative gives a history of Atlantis that is similar to Plato's and places Atlantis in America. People had begun believing that the Mayan and Aztec ruins could possibly be the remnants of Atlantis.
Impact of Mayanism.
Much speculation began as to the origins of the Maya, which led to a variety of narratives and publications that tried to rationalize the discoveries within the context of the Bible and which had undertones of racism in their connections between the Old and New World. The Europeans believed the indigenous people to be inferior and incapable of building that which was now in ruins and by sharing a common history they insinuate that another race must have been responsible.
In the middle and late 19th century, several renowned Mesoamerican scholars, starting with Charles Etienne Brasseur de Bourbourg, and including Edward Herbert Thompson and Augustus Le Plongeon, formally proposed that Atlantis was somehow related to Mayan and Aztec culture.
The French scholar Brasseur de Bourbourg traveled extensively through Mesoamerica in the mid-1800s, and was renowned for his translations of Mayan texts, most notably the sacred book Popol Vuh, as well as a comprehensive history of the region. However, soon after these publications, Brasseur de Bourbourg lost his academic credibility, due to his claim that the Maya peoples had descended from the Toltecs, who he believed were the surviving population of the racially superior civilization of Atlantis. His work combined with the skillful, romantic illustrations of Jean Frederic Waldeck, which visually alluded to Egypt and other aspects of the Old World, creating an authoritative fantasy and exciting much interest in the connections between worlds.
Inspired by Brasseur de Bourbourg's diffusion theories, the pseudoarchaeologist Augustus Le Plongeon traveled to Mesoamerica and performed some of the first excavations of many famous Mayan ruins. Le Plongeon invented narratives, such as the kingdom of Moo saga, which romantically drew connections between himself, his wife Alice, and Egyptian deities Osiris and Isis, as well as with Heinrich Schliemann, who had just discovered the ancient city of Troy from Homer's epic poetry. He also believed that he had found connections between the Greek and Mayan languages, which produced a narrative of the destruction of Atlantis.
Ignatius Donnelly.
The 1882 publication of "" by Ignatius L. Donnelly stimulated much popular interest in Atlantis. He was greatly inspired by early works in Mayanism, and like them attempted to establish that all known ancient civilizations were descended from Atlantis, which he saw as a technologically sophisticated, more advanced culture. Donnelly drew parallels between creation stories in the Old and New Worlds, attributing the connections to Atlantis, where he believed existed the Biblical Garden of Eden. As implied by the title of his book, he also believed that Atlantis was destroyed by the Great Flood mentioned in the Bible.
Donnelly is credited as the "father of the 19th century Atlantis revival" and is the reason the myth endures today. He unintentionally promoted an alternative method of inquiry to history and science, and the idea that myths contain hidden information that opens them to "ingenious" interpretation by people who believe they have new or special insight.
Madame Blavatsky and the Theosophists.
The Russian mystic Helena Petrovna Blavatsky and her partner Henry Steel Olcott founded their Theosophical Society in the 1870s with a philosophy that combined western romanticism and eastern religious concepts. Blavatsky and her followers in this group are often cited as the founders of New Age and other spiritual movements.
Blavatsky took up Donnelly's interpretations when she wrote "The Secret Doctrine" (1888), which she claimed was originally dictated in Atlantis itself. She maintained that the Atlanteans were cultural heroes (contrary to Plato, who describes them mainly as a military threat). She believed in a form of racial evolution (as opposed to primate evolution), in which the Atlanteans were the fourth "Root Race", succeeded by the fifth and most superior "Aryan race" (her own race). The Theosophists believed that the civilization of Atlantis reached its peak between 1,000,000 and 900,000 years ago but destroyed itself through internal warfare brought about by the inhabitants' dangerous use of psychic and supernatural powers. Rudolf Steiner, the founder of anthroposophy and Waldorf Schools, along with other well known Theosophists, such as Annie Besant, also wrote of cultural evolution in much the same vein.
Some subsequent occultists have followed Blavatsky, at least to the point of tracing the lineage of occult practices back to Atlantis. Among the most famous is Dion Fortune in her "Esoteric Orders and Their Work".
Nazism and occultism.
Blavatsky was also inspired by the work of the 18th-century astronomer Jean-Sylvain Bailly, who had "Orientalized" the Atlantis myth in his mythical continent of Hyperborea, a reference to Greek myths featuring a Northern European region of the same name, home to a giant, godlike race. Her reshaping of this theory in "The Secret Doctrine" provided the Nazis with a mythological precedent and pretense for their ideological platform and subsequent genocide.
Julius Evola's writing in 1934 also suggested that the Atlanteans were Hyperborean, Nordic supermen who originated at the North Pole (see Thule). Similarly, Alfred Rosenberg (in "The Myth of the Twentieth Century", 1930) spoke of a "Nordic-Atlantean" or "Aryan-Nordic" master race.
Edgar Cayce.
Edgar Cayce was a man from humble upbringings in Kentucky who allegedly possessed psychic abilities, which were performed from a trance-like state. In addition to allegedly healing the sick from this state, he also spoke frequently on the topic of Atlantis. In his "life readings," he purportedly revealed that many of his subjects were reincarnations of people who had lived on Atlantis. By tapping into their collective consciousness, the "Akashic Records" (a term borrowed from Theosophy), he was able to give detailed descriptions of the lost continent. He also asserted that Atlantis would "rise" again in the 1960s (sparking much popularity of the myth in that decade) and that there is a "Hall of Records" beneath the Egyptian Sphinx, which holds the historical texts of Atlantis.
Recent times.
As continental drift became widely accepted during the 1960s, and the increased understanding of plate tectonics demonstrated the impossibility of a lost continent in the geologically recent past, most "Lost Continent" theories of Atlantis began to wane in popularity.
Plato scholar Julia Annas, Regents Professor of Philosophy at the University of Arizona, had this to say on the matter:
One of the proposed explanations for the historical context of the Atlantis story is a warning of Plato to his contemporary fourth-century fellow-citizens against their striving for naval power.
Kenneth Feder points out that Critias's story in the "Timaeus" provides a major clue. In the dialogue, Critias says, referring to Socrates' hypothetical society:
Feder quotes A. E. Taylor, who wrote, "We could not be told much more plainly that the whole narrative of Solon's conversation with the priests and his intention of writing the poem about Atlantis are an invention of Plato's fancy."
Location hypotheses.
Since Donnelly's day, there have been dozens of locations proposed for Atlantis, to the point where the name has become a generic concept, divorced from the specifics of Plato's account. This is reflected in the fact that many proposed sites are not within the Atlantic at all. Few today are scholarly or archaeological hypotheses, while others have been made by psychic (e.g., Edgar Cayce) or other pseudoscientific means. (The Atlantis researchers Jacques Collina-Girard and Georgeos Díaz-Montexano, for instance, each claim the other's hypothesis is pseudoscience.) Many of the proposed sites share some of the characteristics of the Atlantis story (water, catastrophic end, relevant time period), but none has been demonstrated to be a true historical Atlantis.
In or near the Mediterranean Sea.
Most of the historically proposed locations are in or near the Mediterranean Sea: islands such as Sardinia, Crete, Santorini (Thera), Sicily, Cyprus, and Malta; land-based cities or states such as Troy, Tartessos, and Tantalus (in the province of Manisa, Turkey); Israel-Sinai or Canaan; and northwestern Africa. The Thera eruption, dated to the 17th or 16th century BC, caused a large tsunami that some experts hypothesize devastated the Minoan civilization on the nearby island of Crete, further leading some to believe that this may have been the catastrophe that inspired the story.
A. G. Galanopoulos argued that Plato's dating of 9,000 years before Solon's time was the result of an error in translation, probably from Egyptian into Greek, which produced "thousands" instead of "hundreds". Such an error would also rescale Plato's Atlantis to the size of Crete, while leaving the city the size of the crater on Thera; 900 years before Solon would be the 15th century BC. In the area of the Black Sea the following locations have been proposed: Bosporus and Ancomah (a legendary place near Trabzon).
Others have theorized that before the 6th Century, the "Pillars of Hercules" may have applied to mountains on either side of the Gulf of Laconia, and may also have been part of the pillar cult of the Aegean.
In the Atlantic Ocean.
The location of Atlantis in the Atlantic Ocean has a certain appeal given the closely related names. Popular culture often places Atlantis there, perpetuating the original Platonic setting. Several hypotheses place the sunken island in northern Europe, including Doggerland in the North Sea, and Sweden (by Olof Rudbeck in "Atland", 1672–1702). Doggerland, as well as Viking Bergen Island, is thought to have been flooded by a megatsunami following the Storegga slide c. 6100 BC. Some have proposed the Celtic Shelf as a possible location, and that there is a link to Ireland.
The Canary Islands and Madeira Islands have also been identified as a possible location, west of the Straits of Gibraltar but in relative proximity to the Mediterranean Sea. However, detailed studies of their geomorphology and geology have demonstrated that they have been steadily uplifted, without any significant periods of subsidence, over the last 4 million years, by geologic processes such as erosional unloading, gravitational unloading, lithospheric flexure induced by adjacent islands, and volcanic underplating. Various islands or island groups in the Atlantic were also identified as possible locations, notably the Azores. Similarly, cores of sediment covering the ocean bottom surrounding the Azores and other evidence demonstrate that it has been an undersea plateau for millions of years. The submerged island of Spartel near the Strait of Gibraltar has also been suggested.
In Europe.
In 2011, a team, working on a documentary for the National Geographic Channel, led by Professor Richard Freund from the University of Hartford, claimed to have found evidence of Atlantis in southwestern Andalusia. The team identified its possible location within the marshlands of the Doñana National Park, in the area that once was the Lacus Ligustinus, between the Huelva, Cádiz and Seville provinces, and speculated that Atlantis had been destroyed by a tsunami, extrapolating results from a previous study by Spanish researchers, published four years earlier.
Spanish scientists have dismissed Freund's speculations, claiming that he sensationalised their work. The anthropologist Juan Villarías-Robles, who works with the Spanish National Research Council, said, "Richard Freund was a newcomer to our project and appeared to be involved in his own very controversial issue concerning King Solomon's search for ivory and gold in Tartessos, the well documented settlement in the Doñana area established in the first millennium BC", and described Freund's claims as "fanciful".
A similar theory had previously been put forward by a German researcher, Rainer W. Kühne, but based only on satellite imagery and placing Atlantis in the Marismas de Hinojos, north of the city of Cádiz. Before that, the historian Adolf Schulten had stated in the 1920s that Plato had used Tartessos as the basis for his Atlantis myth.
Other locations.
Several writers have speculated that Antarctica is the site of Atlantis, while others have proposed Caribbean locations such the alleged Cuban sunken city off the Guanahacabibes peninsula in Cuba, the Bahamas, and the Bermuda Triangle. Areas in the Pacific and Indian Oceans have also been proposed including Indonesia (i.e. Sundaland). Likewise some have speculated that the continent of South America bears striking similarities to the description of Atlantis by Plato, particularly the Altiplano region of the Andes. The stories of a lost continent off the coast of India, named "Kumari Kandam," have inspired some to draw parallels to Atlantis.
Literary interpretations.
Ancient versions.
In order to give his account of Atlantis validation from the past, Plato mentions that the story was old and known to Solon, who had at least begun a poem on the subject that was to be greater than the works of Hesiod and Homer. Plato himself, though banishing poetry-making from his Republic, had exempted certain classes of poem, among which praise of the gods and of good men was included. This therefore gives a clue as to how the allegory of Atlantis was to be interpreted. It is a poetic ideal to which good men should aspire, but from which there is the danger of falling short, with catastrophic results. There has also been a suggestion that another literary forerunner was an epic written by Hellanicus of Lesbos, a fragment of which may be Oxyrhynchus Papyrus 11, 1359.
In the new era, the 3rd century Neoplatonist Zoticus wrote an epic poem based on Plato's account of Atlantis. But Plato's work may already have inspired parodic imitation too. Writing only a few decades after the "Timaeus" and "Critias", the historian Theopompus of Chios wrote of a land beyond the ocean known as Meropis. This description was included in Book 8 of his "Philippica", which contains a dialogue between Silenus and King Midas. Silenus describes the Meropids, a race of men who grow to twice normal size, and inhabit two cities on the island of Meropis: "Eusebes" (, "Pious-town") and "Machimos" (, "Fighting-town"). He also reports that an army of ten million soldiers crossed the ocean to conquer Hyperborea, but abandoned this proposal when they realized that the Hyperboreans were the luckiest people on earth. Heinz-Günther Nesselrath has argued that these and other details of Silenus' story are meant as imitation and exaggeration of the Atlantis story, for the purpose of exposing Plato's ideas to ridicule.
Utopias and dystopias.
The creation of Utopian and dystopian fictions was renewed after the Renaissance, most notably in Francis Bacon’s "New Atlantis" (1627), the description of an ideal society that he located off the western coast of America. Thomas Heyrick (1649-1694) followed him with “The New Atlantis” (1687), a satirical poem in three parts. His new continent of uncertain location, perhaps even a floating island either in the sea or the sky, serves as background for his exposure of what he described in a second edition as “A True Character of Popery and Jesuitism”.
The title of "The New Atalantis" by Delarivier Manley (1709), distinguished from the two others by the single letter, is an equally dystopian work but set this time on a fictional Mediterranean island. In it sexual violence and exploitation is made a metaphor for the hypocritical behaviour of politicians in their dealings with the general public. In Manley’s case, the target of satire was the Whig Party, while in David Maclean Parry's "The Scarlet Empire" (1906) it is Socialism as practised in foundered Atlantis itself. It was followed in Russia by Velemir Khlebnikov's poem "The Fall of Atlantis" ("Gibel' Atlantidy", 1912), which is set in a future rationalist dystopia that has discovered the secret of immortality and is so dedicated to progress that it has lost touch with the past. When the High Priest of this ideology is tempted by a slave girl into an act of irrationality, he murders her and precipitates a second flood, above which her severed head floats vengefully among the stars.
A slightly later work, "The Ancient of Atlantis" (Boston, 1915) by Albert Armstrong Manship, expounds the Atlantean wisdom that is to redeem the earth. Its three parts consist of a verse narrative of the life and training of an Atlantean wise one, followed by his Utopian moral teachings and then a psychic drama set in modern times in which a reincarnated child embodying the lost wisdom is reborn on earth.
In Hispanic eyes Atlantis had a more intimate interpretation. The land had been a colonial power which, though it had brought civilization to ancient Europe, had also enslaved its peoples. Its tyrannical fall from grace had contributed to the fate that had overtaken it, but now its disappearance had unbalanced the world. This was the point of view of Jacint Verdaguer’s vast mythological epic "L’Atlantida" (1877). After the sinking of the former continent, Hercules travels east across the Atlantic to found the city of Barcelona and then departs westwards again to the Hesperides. The story is told by a hermit to a shipwrecked mariner, who is inspired to follow in his tracks and so “call the New World into existence to redress the balance of the Old”. This mariner, of course, was Christopher Columbus.
Verdaguer’s poem was written in Catalan but was widely translated in both Europe and Hispano-America. One response was the similarly titled Argentinian "Atlantida" of Olegario Victor Andrade (1881), which sees in “Enchanted Atlantis that Plato foresaw, a golden promise to the fruitful race” of Latins. The bad example of the colonising world remains, however. Jose Juan Tablada characterises its threat in his “De Atlántida” (1894) through the beguiling picture of the lost world populated by the underwater creatures of Classical myth, among whom is the Siren of its final stanza with 
There is a similar ambivalence in Janus Djurhuus’ six-stanza “Atlantis” (1917), where a celebration of the Faroese linguistic revival grants it an ancient pedigree by linking Greek to Norse legend. In the poem a female figure rising from the sea against a background of Classical palaces is recognised as a priestess of Atlantis. The poet recalls “that the Faroes lie there in the north Atlantic Ocean/ where before lay the poet-dreamt lands,” but also that in Norse belief such a figure only appears to those about to drown.
A land lost in the distance.
The fact that Atlantis is a lost land has made of it a metaphor for something no longer attainable. For the American poet Edith Willis Linn Forbes (1865-1945), “The Lost Atlantis” stands for idealisation of the past; the present moment can only be treasured once that is realised. Ella Wheeler Wilcox finds the location of “The Lost Land” (1910) in one’s carefree youthful past. Similarly, for the Irish poet Eavan Boland in “Atlantis, a lost sonnet” (2007), the idea was defined when “the old fable-makers searched hard for a word/ to convey that what is gone is gone forever”.
For male poets too the idea of Atlantis is constructed from what cannot be obtained. Charles Bewley in his Newdigate Prize poem (1910) thinks it grows from dissatisfaction with one’s condition, 
in a dream of Atlantis. Similarly for the Australian Gary Catalano in a 1982 prose poem, it is “a vision that sank under the weight of its own perfection”. W.H. Auden, however, suggests a way out of such frustration through the metaphor of journeying towards Atlantis in his poem of 1941. While travelling, he advises the one setting out, you will meet with many definitions of the goal in view, only realising at the end that the way has all the time led inward.
Epic narratives.
A few late 19th century verse narratives complement the genre fiction that was beginning to be written at the same period. Two of them report the disaster that overtook the continent as related by long-lived survivors. In Frederick Tennyson’s "Atlantis" (1888) an ancient Greek mariner sails west and discovers an inhabited island which is all that remains of the former kingdom. He learns of its end and views the shattered remnant of its former glory, from which a few had escaped to set up the Mediterranean civilisations. In the second, "Mona, Queen of Lost Atlantis: An Idyllic Re-embodiment of Long Forgotten History" (Los Angeles CA 1925) by James Logue Dryden (1840-1925), the story is told in a series of visions. A Seer is taken to Mona’s burial chamber in the ruins of Atlantis, where she revives and describes the catastrophe. There follows a survey of the lost civilisations of Hyperborea and Lemuria as well as Atlantis, accompanied by much spiritualist lore.
William Walton Hoskins (1856-1919) admits to the readers of his "Atlantis and other poems" (Cleveland OH, 1881) that he is only 24. Its melodramatic plot concerns the poisoning of the descendent of god-born kings. The usurping poisoner is poisoned in his turn, following which the continent is swallowed in the waves. Asian gods people the landscape of "The Lost Island" (Ottawa 1889) by Edward Taylor Fletcher (1816–97). An angel foresees impending catastrophe and that the people will be allowed to escape if their semi-divine rulers will sacrifice themselves. A final example, Edward N. Beecher’s "The Lost Atlantis or The Great Deluge of All" (Cleveland OH, 1898) is just a doggerel vehicle for its author’s opinions: that the continent was the location of the Garden of Eden; that Darwin’s theory of evolution is correct, as are Donnelly’s views.
Atlantis was to become a theme in Russia following the 1890s, taken up in unfinished poems by Valery Bryusov and Konstantin Balmont, as well as in a drama by the schoolgirl Larisa Reisner. One other long narrative poem was published in New York by George V. Golokhvastoff. His 250-page "The Fall of Atlantis" (1938) records how a High Priest, distressed by the prevailing degeneracy of the ruling classes, seeks to create an androgynous being from royal twins as a means to overcome this polarity. When he is unable to control the forces unleashed by his occult ceremony, the continent is destroyed. 
Artistic representations.
Music.
The Spanish composer Manuel de Falla worked on a dramatic cantata based on Verdaguer’s "L’Atlántida", during the last 20 years of his life. The name has been affixed to symphonies by Janis Ivanovs (1941), Richard Nanes, and Vaclav Buzek (2009). There was also the symphonic celebration of Alan Hovhaness: "Fanfare for the New Atlantis" (Op. 281, 1975).
Painting and sculpture.
Paintings of the submersion of Atlantis are comparatively rare. In the 17th century there was François de Nomé’s “The Fall of Atlantis”, which shows a tidal wave surging towards a Baroque city frontage. The style of architecture apart, it is not very different from Nicholas Roerich’s “The Last of Atlantis” of 1928.
The most dramatic depiction of the catastrophe was Leon Bakst’s “Ancient Terror” ("Terror Antiquus", 1908), although it does not name Atlantis directly. It is a mountain-top view of a rocky bay breached by the sea, which is washing inland about the tall structures of an ancient city. A streak of lightning crosses the upper half of the picture, while below it rises the impassive figure of an enigmatic goddess who holds a blue dove between her breasts. Vyacheslav Ivanov identified the subject as Atlantis in a public lecture on the painting given in 1909, the year it was first exhibited, and he has been followed by other commentators in the years since.
Sculptures referencing Atlantis have often been stylized single figures. One of the earliest was Einar Jónsson’s "The King of Atlantis" (1919–1922), now in the garden of his museum in Reykjavik. It represents a single figure, clad in a belted skirt and wearing a large triangular helmet, who sits on an ornate throne supported between two young bulls. The walking female titled "Atlantis" (1946) by Ivan Meštrović was from a series inspired by ancient Greek figures with the symbolical meaning of unjustified suffering.
In the case of the fountain feature known as "The Man of Atlantis" (2003) by Luk van Soom, the 4-metre tall figure wearing a diving suit steps from a plinth into the spray. It looks light-hearted but the artist’s comment on it makes a serious point: "Because habitable land will be scarce, it is no longer improbable that we will return to the water in the long term. As a result, a portion of the population will mutate into fish-like creatures. Global warming and rising water levels are practical problems for the world in general and here in the Netherlands in particular".
Robert Smithson’s "Hypothetical Continent (Map of broken clear glass, Atlantis)" was first created as a photo project on Loveladies Island NJ in 1969 and then recreated as a gallery installation. On this he commented that he liked “landscapes that suggest prehistory”, and this is borne out by the original conceptual drawing of the work that includes an inset map of the continent sited off the coast of Africa and at the straits into the Mediterranean.
See also.
Underwater geography:
General:

</doc>
