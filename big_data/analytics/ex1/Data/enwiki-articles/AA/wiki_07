<doc id="856" url="https://en.wikipedia.org/wiki?curid=856" title="Apple Inc.">
Apple Inc.

Apple Inc. is an American multinational technology company headquartered in Cupertino, California, that designs, develops, and sells consumer electronics, computer software, and online services. Its hardware products include the iPhone smartphone, the iPad tablet computer, the Mac personal computer, the iPod portable media player, and the Apple Watch smartwatch. Apple's consumer software includes the OS X and iOS operating systems, the iTunes media player, the Safari web browser, and the iLife and iWork creativity and productivity suites. Its online services include the iTunes Store, the iOS App Store and Mac App Store, and iCloud.
Apple was founded by Steve Jobs, Steve Wozniak, and Ronald Wayne on April 1, 1976, to develop and sell personal computers. It was incorporated as Apple Computer, Inc. on January 3, 1977, and was renamed as Apple Inc. on January 9, 2007, to reflect its shifted focus toward consumer electronics. Apple () joined the Dow Jones Industrial Average on March 19, 2015.
Apple is the world's largest information technology company by revenue, the world's largest technology company by total assets, and the world's second-largest mobile phone manufacturer. In November 2014, in addition to being the largest publicly traded corporation in the world by market capitalization, Apple became the first U.S. company to be valued at over US$700 billion. The company employs 115,000 permanent full-time employees and maintains 475 retail stores in seventeen countries . It operates the online Apple Store and iTunes Store, the latter of which is the world's largest music retailer. There are over one billion actively used Apple products worldwide .
Apple's worldwide annual revenue totaled $233 billion for the fiscal year ending in September 2015. To put this into perspective this revenue generation accounts for approximately 1.25% of the total United States GDP. The company enjoys a high level of brand loyalty and, according to the 2014 edition of the Interbrand Best Global Brands report, is the world's most valuable brand with a valuation of $118.9 billion. By the end of 2014, the corporation continued to receive significant criticism regarding the labor practices of its contractors and its environmental and business practices, including the origins of source materials.
History.
1976–84: Founding and incorporation.
Apple was established on April 1, 1976, by Steve Jobs, Steve Wozniak and Ronald Wayne to sell the Apple I personal computer kit. The Apple I kits were computers single-handedly designed and hand-built by Wozniak and first shown to the public at the Homebrew Computer Club. The Apple I was sold as a motherboard (with CPU, RAM, and basic textual-video chips), which was less than what is now considered a complete personal computer. The Apple I went on sale in July 1976 and was market-priced at $666.66 ($ in dollars, adjusted for inflation).
Apple was incorporated January 3, 1977, without Wayne, who sold his share of the company back to Jobs and Wozniak for $800. Multimillionaire Mike Markkula provided essential business expertise and funding of $250,000 during the incorporation of Apple. During the first five years of operations revenues grew exponentially, doubling about every four months. Between September 1977 and September 1980 yearly sales grew from $775,000 to $118m, an average annual growth rate of 533%.
The Apple II, also invented by Wozniak, was introduced on April 16, 1977, at the first West Coast Computer Faire. It differed from its major rivals, the TRS-80 and Commodore PET, because of its character cell-based color graphics and open architecture. While early Apple II models used ordinary cassette tapes as storage devices, they were superseded by the introduction of a 5 1/4 inch floppy disk drive and interface called the Disk II. The Apple II was chosen to be the desktop platform for the first "killer app" of the business world: VisiCalc, a spreadsheet program. VisiCalc created a business market for the Apple II and gave home users an additional reason to buy an Apple II: compatibility with the office. Before VisiCalc, Apple had been a distant third place competitor to Commodore and Tandy.
By the end of the 1970s, Apple had a staff of computer designers and a production line. The company introduced the Apple III in May 1980 in an attempt to compete with IBM and Microsoft in the business and corporate computing market. Jobs and several Apple employees, including Jef Raskin, visited Xerox PARC in December 1979 to see the Xerox Alto. Xerox granted Apple engineers three days of access to the PARC facilities in return for the option to buy 100,000 shares (800,000 split-adjusted shares) of Apple at the pre-IPO price of $10 a share.
Jobs was immediately convinced that all future computers would use a graphical user interface (GUI), and development of a GUI began for the Apple Lisa. In 1982, however, he was pushed from the Lisa team due to infighting. Jobs took over Jef Raskin's low-cost-computer project, the Macintosh. A race broke out between the Lisa team and the Macintosh team over which product would ship first. Lisa won the race in 1983 and became the first personal computer sold to the public with a GUI, but was a commercial failure due to its high price tag and limited software titles.
On December 12, 1980, Apple went public at $22 per share, generating more capital than any IPO since Ford Motor Company in 1956 and instantly creating more millionaires (about 300) than any company in history.
1984–91: Success with Macintosh.
In 1984, Apple launched the Macintosh, the first personal computer to be sold without a programming language at all. Its debut was signified by "1984", a $1.5 million television commercial directed by Ridley Scott that aired during the third quarter of Super Bowl XVIII on January 22, 1984. The commercial is now hailed as a watershed event for Apple's success and a "masterpiece".
The Macintosh initially sold well, but follow-up sales were not strong due to its high price and limited range of software titles. The machine's fortunes changed with the introduction of the LaserWriter, the first PostScript laser printer to be sold at a reasonable price, and PageMaker, an early desktop publishing package. It has been suggested that the combination of these three products were responsible for the creation of the desktop publishing market. The Macintosh was particularly powerful in the desktop publishing market due to its advanced graphics capabilities, which had necessarily been built in to create the intuitive Macintosh GUI.
In 1985, a power struggle developed between Jobs and CEO John Sculley, who had been hired two years earlier. The Apple board of directors instructed Sculley to "contain" Jobs and limit his ability to launch expensive forays into untested products. Rather than submit to Sculley's direction, Jobs attempted to oust him from his leadership role at Apple. Sculley found out that Jobs had been attempting to organize a coup and called a board meeting at which Apple's board of directors sided with Sculley and removed Jobs from his managerial duties. Jobs resigned from Apple and founded NeXT Inc. the same year.
After Jobs' departure, the Macintosh product line underwent a steady change of focus to higher price points, the so-called "high-right policy" named for the position on a chart of price vs. profits. Jobs had argued the company should produce products aimed at the consumer market and aimed for a $1000 price for the Macintosh, which they were unable to meet. Newer models selling at higher price points offered higher profit margin, and appeared to have no effect on total sales as power users snapped up every increase in power. Although some worried about pricing themselves out of the market, the high-right policy was in full force by the mid-1980s, notably due to Jean-Louis Gassée's mantra of "fifty-five or die", referring to the 55% profit margins of the Macintosh II.
This policy began to backfire in the last years of the decade as new desktop publishing programs appeared on PC clones that offered some or much of the same functionality of the Macintosh but at far lower price points. The company lost its monopoly in this market, and had already estranged many of its original consumer customer base who could no longer afford their high priced products. The Christmas season of 1989 was the first in the company's history that saw declining sales, and led to a 20% drop in Apple's stock price. Gassée's objections were overruled, and he was forced from the company in 1990. Later that year, Apple introduced three lower cost models, the Macintosh Classic, Macintosh LC and Macintosh IIsi, all of which saw significant sales due to pent up demand.
In 1991 Apple introduced the PowerBook, replacing the "luggable" Macintosh Portable with a design that set the current shape for almost all modern laptops. The same year, Apple introduced System 7, a major upgrade to the operating system which added color to the interface and introduced new networking capabilities. It remained the architectural basis for Mac OS until 2001. The success of the PowerBook and other products brought increasing revenue. For some time, Apple was doing incredibly well, introducing fresh new products and generating increasing profits in the process. The magazine "MacAddict" named the period between 1989 and 1991 as the "first golden age" of the Macintosh.
Apple believed the Apple II series was too expensive to produce and took away sales from the low-end Macintosh. In 1990, Apple released the Macintosh LC, which featured a single expansion slot for the Apple IIe Card to help migrate Apple II users to the Macintosh platform; the Apple IIe was discontinued in 1993.
1991–97: Decline, restructuring, acquisitions.
The success of Apple's lower-cost consumer models, especially the LC, also led to cannibalization of their higher priced machines. To address this, management introduced several new brands, selling largely identical machines at different price points aimed at different markets. These were the high-end Quadra, the mid-range Centris line, and the ill-fated Performa series. This led to significant market confusion, as customers did not understand the difference between models.
Apple also experimented with a number of other unsuccessful consumer targeted products during the 1990s, including digital cameras, portable CD audio players, speakers, video consoles, the eWorld online service, and TV appliances. Enormous resources were also invested in the problem-plagued Newton division based on John Sculley's unrealistic market forecasts. Ultimately, none of these products helped and Apple's market share and stock prices continued to slide.
Throughout this period, Microsoft continued to gain market share with Windows by focusing on delivering software to cheap commodity personal computers, while Apple was delivering a richly engineered but expensive experience. Apple relied on high profit margins and never developed a clear response; instead, they sued Microsoft for using a GUI similar to the Apple Lisa in "Apple Computer, Inc. v. Microsoft Corp.". The lawsuit dragged on for years before it was finally dismissed. At this time, a series of major product flops and missed deadlines sullied Apple's reputation, and Sculley was replaced as CEO by Michael Spindler.
By the early 1990s, Apple was developing alternative platforms to the Macintosh, such as A/UX. The Macintosh platform itself was becoming outdated because it was not built for multitasking and because several important software routines were programmed directly into the hardware. In addition, Apple was facing competition from OS/2 and UNIX vendors such as Sun Microsystems. The Macintosh would need to be replaced by a new platform or reworked to run on more powerful hardware.
In 1994, Apple allied with IBM and Motorola in the AIM alliance with the goal of creating a new computing platform (the PowerPC Reference Platform), which would use IBM and Motorola hardware coupled with Apple software. The AIM alliance hoped that PReP's performance and Apple's software would leave the PC far behind and thus counter Microsoft. The same year, Apple introduced the Power Macintosh, the first of many Apple computers to use Motorola's PowerPC processor.
In 1996, Spindler was replaced by Gil Amelio as CEO. Amelio made numerous changes at Apple, including extensive layoffs and cut costs. After numerous failed attempts to improve Mac OS, first with the Taligent project and later with Copland and Gershwin, Amelio chose to purchase NeXT and its NeXTSTEP operating system and bring Steve Jobs back to Apple.
1997–2007: Return to profitability.
The NeXT deal was finalized on February 9, 1997, bringing Jobs back to Apple as an advisor. On July 9, 1997, Amelio was ousted by the board of directors after overseeing a three-year record-low stock price and crippling financial losses. Jobs acted as the interim CEO and began restructuring the company's product line; it was during this period that he identified the design talent of Jonathan Ive, and the pair worked collaboratively to rebuild Apple's status.
At the 1997 Macworld Expo, Jobs announced that Apple would join Microsoft to release new versions of Microsoft Office for the Macintosh, and that Microsoft had made a $150 million investment in non-voting Apple stock. On November 10, 1997, Apple introduced the Apple Online Store, which was tied to a new build-to-order manufacturing strategy.
On August 15, 1998, Apple introduced a new all-in-one computer reminiscent of the Macintosh 128K: the iMac. The iMac design team was led by Ive, who would later design the iPod and the iPhone. The iMac featured modern technology and a unique design, and sold almost 800,000 units in its first five months.
During this period, Apple completed numerous acquisitions to create a portfolio of digital production software for both professionals and consumers. In 1998, Apple purchased Macromedia's Key Grip software project, signaling an expansion into the digital video editing market. The sale was an outcome of Macromedia's decision to solely focus upon web development software. The product, still unfinished at the time of the sale, was renamed "Final Cut Pro" when it was launched on the retail market in April 1999. The development of Key Grip also led to Apple's release of the consumer video-editing product iMovie in October 1999. Next, Apple successfully acquired the German company Astarte, which had developed DVD authoring technology, as well as Astarte's corresponding products and engineering team in April 2000. Astarte's digital tool DVDirector was subsequently transformed into the professional-oriented DVD Studio Pro software product. Apple then employed the same technology to create iDVD for the consumer market. In 2002, Apple purchased Nothing Real for their advanced digital compositing application Shake, as well as Emagic for the music productivity application Logic. The purchase of Emagic made Apple the first computer manufacturer to own a music software company. The acquisition was followed by the development of Apple's consumer-level GarageBand application. The release of iPhoto in the same year completed the iLife suite.
Mac OS X, based on NeXT's OPENSTEP and BSD Unix, was released on March 24, 2001 after several years of development. Aimed at consumers and professionals alike, Mac OS X aimed to combine the stability, reliability and security of Unix with the ease of use afforded by an overhauled user interface. To aid users in migrating from Mac OS 9, the new operating system allowed the use of OS 9 applications within Mac OS X via the Classic Environment.
On May 19, 2001, Apple opened the first official Apple Retail Stores in Virginia and California. On October 23 of the same year, Apple debuted the iPod portable digital audio player. The product, which was first sold on November 10, 2001, was phenomenally successful with over 100 million units sold within six years. In 2003, Apple's iTunes Store was introduced. The service offered online music downloads for $0.99 a song and integration with the iPod. The iTunes store quickly became the market leader in online music services, with over 5 billion downloads by June 19, 2008.
At the Worldwide Developers Conference keynote address on June 6, 2005, Jobs announced that Apple would begin producing Intel-based Mac computers in 2006. On January 10, 2006, the new MacBook Pro and iMac became the first Apple computers to use Intel's Core Duo CPU. By August 7, 2006, Apple made the transition to Intel chips for the entire Mac product line—over one year sooner than announced. The Power Mac, iBook and PowerBook brands were retired during the transition; the Mac Pro, MacBook, and MacBook Pro became their respective successors. On April 29, 2009, "The Wall Street Journal" reported that Apple was building its own team of engineers to design microchips. Apple also introduced Boot Camp in 2006 to help users install Windows XP or Windows Vista on their Intel Macs alongside Mac OS X.
Apple's success during this period was evident in its stock price. Between early 2003 and 2006, the price of Apple's stock increased more than tenfold, from around $6 per share (split-adjusted) to over $80. In January 2006, Apple's market cap surpassed that of Dell. Nine years prior, Dell's CEO Michael Dell had said that if he ran Apple he would "shut it down and give the money back to the shareholders." Although Apple's market share in computers had grown, it remained far behind competitors using Microsoft Windows, accounting for about 8% of desktops and laptops in the US.
Since 2001, Apple's design team has progressively abandoned the use of translucent colored plastics first used in the iMac G3. This design change began with the titanium-made PowerBook and was followed by the iBook's white polycarbonate structure and the flat-panel iMac.
2007–11: Success with mobile devices.
During his keynote speech at the Macworld Expo on January 9, 2007, Jobs announced that Apple Computer, Inc. would thereafter be known as "Apple Inc.", because the company had shifted its emphasis from computers to consumer electronics. This event also saw the announcement of the iPhone and the Apple TV. The following day, Apple shares hit $97.80, an all-time high at that point. In May, Apple's share price passed the $100 mark. Apple would achieve widespread success with its iPhone, iPod Touch and iPad products, which introduced innovations in mobile phones, portable music players and personal computers respectively. Furthermore, by early 2007, 800,000 Final Cut Pro users were registered.
In an article posted on Apple's website on February 6, 2007, Jobs wrote that Apple would be willing to sell music on the iTunes Store without digital rights management (DRM), thereby allowing tracks to be played on third-party players, if record labels would agree to drop the technology. On April 2, 2007, Apple and EMI jointly announced the removal of DRM technology from EMI's catalog in the iTunes Store, effective in May 2007. Other record labels eventually followed suit and Apple published a press release in January 2009 to announce the corresponding changes to the iTunes Store.
In July 2008, Apple launched the App Store to sell third-party applications for the iPhone and iPod Touch. Within a month, the store sold 60 million applications and registered an average daily revenue of $1 million, with Jobs speculating in August 2008 that the App Store could become a billion-dollar business for Apple. By October 2008, Apple was the third-largest mobile handset supplier in the world due to the popularity of the iPhone.
On December 16, 2008, Apple announced that 2009 would be the last year the corporation would attend the Macworld Expo, after more than 20 years of attendance, and that senior vice president of Worldwide Product Marketing Philip Schiller would deliver the 2009 keynote address in lieu of the expected Jobs. The official press release explained that Apple was "scaling back" on trade shows in general, including Macworld Tokyo and the Apple Expo in Paris, France, primarily because the enormous successes of the Apple Retail Stores and website had rendered trade shows a minor promotional channel.
On January 14, 2009, an internal memo from Jobs announced that he would be taking a six-month medical leave of absence from Apple until the end of June 2009 and would spend the time focusing on his health. In the email, Jobs stated that "the curiosity over my personal health continues to be a distraction not only for me and my family, but everyone else at Apple as well", and explained that the break would allow the company "to focus on delivering extraordinary products". Despite Jobs's absence, Apple recorded its best non-holiday quarter (Q1 FY 2009) during the recession with revenue of $8.16 billion and profit of $1.21 billion.
After years of speculation and multiple rumored "leaks", Apple announced a large screen, tablet-like media device known as the iPad on January 27, 2010. The iPad ran the same touch-based operating system as the iPhone, and many iPhone apps were compatible with the iPad. This gave the iPad a large app catalog on launch, despite very little development time before the release. Later that year on April 3, 2010, the iPad was launched in the US. It sold more than 300,000 units on its first day, and 500,000 by the end of the first week. In May of the same year, Apple's market cap exceeded that of competitor Microsoft for the first time since 1989.
In June 2010, Apple released the iPhone 4, which introduced video calling, multitasking, and a new uninsulated stainless steel design that acted as the phone's antenna. Later that year, Apple again refreshed its iPod line of MP3 players by introducing a multi-touch iPod Nano, an iPod Touch with FaceTime, and an iPod Shuffle that brought back the buttons of earlier generations. Additionally, on October 20, Apple updated the MacBook Air laptop, iLife suite of applications, and unveiled Mac OS X Lion, the last version with the name "Mac OS X".
In October 2010, Apple shares hit an all-time high, eclipsing $300.
On January 6, 2011, the company opened its Mac App Store, a digital software distribution platform similar to the iOS App Store.
Alongside peer entities such as Atari and Cisco Systems, Apple was featured in the documentary "Something Ventured" which premiered in 2011 and explored the three-decade era that led to the establishment and dominance of Silicon Valley.
On January 17, 2011, Jobs announced in an internal Apple memo that he would take another medical leave of absence, for an indefinite period, to allow him to focus on his health. Chief operating officer Tim Cook assumed Jobs's day-to-day operations at Apple, although Jobs would still remain "involved in major strategic decisions". Apple became the most valuable consumer-facing brand in the world. In June 2011, Jobs surprisingly took the stage and unveiled iCloud, an online storage and syncing service for music, photos, files and software which replaced MobileMe, Apple's previous attempt at content syncing.
This would be the last product launch Jobs would attend before his death. It has been argued that Apple has achieved such efficiency in its supply chain that the company operates as a monopsony (one buyer, many sellers) and can dictate terms to its suppliers. In July 2011, due to the American debt-ceiling crisis, Apple's financial reserves were briefly larger than those of the U.S. Government.
On August 24, 2011, Jobs resigned his position as CEO of Apple. He was replaced by Cook and Jobs became Apple's chairman. Prior to this, Apple did not have a chairman and instead had two co-lead directors, Andrea Jung and Arthur D. Levinson, who continued with those titles until Levinson became Chairman of the Board in November.
2011–15: Post-Jobs era.
On October 5, 2011, Apple announced that Jobs had died, marking the end of an era for Apple. The first major product announcement by Apple following Jobs's passing occurred on January 19, 2012, when Apple's Phil Schiller introduced iBooks Textbooks for iOS and iBook Author for Mac OS X in New York City. Jobs had stated in his biography that he wanted to reinvent the textbook industry and education.
From 2011 to 2012, Apple released the iPhone 4S and iPhone 5, which featured improved cameras, an "intelligent software assistant" named Siri, and cloud-sourced data with iCloud; the third and fourth generation iPads, which featured Retina displays; and the iPad Mini, which featured a 7.9-inch screen in contrast to the iPad's 9.7-inch screen. These launches were successful, with the iPhone 5 (released September 21, 2012) becoming Apple's biggest iPhone launch with over 2 million pre-orders and sales of 3 million iPads in three days following the launch of the iPad Mini and fourth generation iPad (released November 3, 2012). Apple also released a third-generation 13-inch MacBook Pro with a Retina display and new iMac and Mac Mini computers.
On October 29, 2011, Apple purchased C3 Technologies, a mapping company, for $240 million, becoming the third mapping company Apple has purchased. On January 10, 2012, Apple paid $500 million to acquire Anobit, an Israeli hardware company that developed and supplied a proprietary memory signal processing technology that improved the performance of the flash-memory used in iPhones and iPads. On July 24, 2012, during a conference call with investors, Tim Cook said that he loved India, but that Apple was going to expect larger opportunities outside of India. Cook cited the reason as the 30% sourcing requirement from India.
On August 20, 2012, Apple's rising stock rose the company's value to a world-record $624 billion. This beat the non-inflation-adjusted record for market capitalization set by Microsoft in 1999. On August 24, 2012, a US jury ruled that Samsung should pay Apple $1.05 billion (£665m) in damages in an intellectual property lawsuit. Samsung appealed the damages award, which the Court reduced by $450 million. The Court further granted Samsung's request for a new trial. On November 10, 2012, Apple confirmed a global settlement that would dismiss all lawsuits between Apple and HTC up to that date, in favor of a ten-year license agreement for current and future patents between the two companies. It is predicted that Apple will make $280 million a year from this deal with HTC.
A previously confidential email written by Jobs a year before his death, was presented during the proceedings of the "Apple Inc. v. Samsung Electronics Co." lawsuits and became publicly available in early April 2014. With a subject line that reads "Top 100 – A," the email was sent only to the company's 100 most senior employees and outlines Jobs's vision of Apple Inc.'s future under 10 subheadings. Notably, Jobs declares a "Holy War with Google" for 2011 and schedules a "new campus" for 2015.
In March 2013, Apple filed a patent for an augmented reality (AR) system that can identify objects in a live video stream and present information corresponding to these objects through a computer-generated information layer overlaid on top of the real-world image. Later in 2013, Apple acquired Embark Inc., a small Silicon Valley-based mapping company that builds free transit apps to help smartphone users navigate public transportation in U.S. cities, and PrimeSense, an Israeli 3D sensing company based in Tel Aviv. In December 2013, Apple Inc. purchased social analytics firm Topsy. Topsy is one of a small number of firms with real-time access to the messages that appear on Twitter and can "do real-time analysis of the trends and discussions happening on Twitter". The company also made several high profile hiring decisions in 2013. On July 2, 2013, Apple recruited Paul Deneve, Belgian President and CEO of Yves Saint Laurent as a vice president reporting directly to Tim Cook. A mid-October 2013 announcement revealed that Burberry executive Angela Ahrendts will commence as a senior vice president at Apple in mid-2014. Ahrendts oversaw Burberry's digital strategy for almost eight years and, during her tenure, sales increased to about US$3.2 billion and shares gained more than threefold.
At the Worldwide Developer's Conference on June 10, 2013, Apple announced the seventh iOS operating system alongside OS X Mavericks, the tenth version of Mac OS X, and a new Internet radio service called iTunes Radio. iTunes Radio, iOS 7 and OS X Mavericks were released fall 2013. On December 6, 2013, Apple Inc. launched iBeacon across its 254 U.S. retail stores. Using Bluetooth wireless technology, iBeacon senses the user's exact location within the Apple store and sends the user messages about products, events and other information, tailored to the user's location.
Alongside Google vice-president Vint Cerf and AT&T CEO Randall Stephenson, Cook attended a closed-door summit held by President Obama on August 8, 2013, in regard to government surveillance and the Internet in the wake of the Edward Snowden NSA incident. On February 4, 2014, Cook met with Abdullah Gül, the President of Turkey, in Ankara to discuss the company's involvement in the Fatih project. Cook also confirmed that Turkey's first Apple Retail Store would be opened in Istanbul in April 2014.
An anonymous Apple employee revealed to the "Bloomberg" media publication that the opening of a Tokyo, Japan, store was planned for 2014. A Japanese analyst has stated, "For Apple, the Japanese market is appealing in terms of quantity and price. There is room to expand tablet sales and a possibility the Japanese market expands if Apple’s mobile carrier partners increase." , Apple operated three stores in Tokyo. On October 1, 2013, Apple India executives unveiled a plan to expand further into the Indian market, following Cook's acknowledgment of the country in July 2013 when sales results showed that iPhone sales in India grew 400% during the second quarter of 2013.
Apple Inc. reported that the company sold 51 million iPhones in the Q1 of 2014 (an all-time quarterly record), compared to 47.8 million in the year-ago quarter. Apple also sold 26 million iPads during the quarter, also an all-time quarterly record, compared to 22.9 million in the year-ago quarter. The Company sold 4.8 million Macs, compared to 4.1 million in the year-ago quarter. On May 28, 2014, Apple confirmed its intent to acquire Dr. Dre and Jimmy Iovine's audio company Beats Electronics—producer of the "Beats by Dr. Dre" line of headphones and speaker products, and operator of the music streaming service Beats Music—for $3 billion, and to sell their products through Apple's retail outlets and resellers. Iovine felt that Beats had always "belonged" with Apple, as the company modeled itself after Apple's "unmatched ability to marry culture and technology." In August 2014, an Apple representative confirmed to the media that Anand Lal Shimpi, editor and publisher of the "AnandTech" website, had been recruited by Apple without elaborating on Lal Shimpi's role.
2015–present: "The only thing that has changed is everything" and fortieth anniversary.
In 2016, it was revealed that Apple would be making its first original scripted series, a six-episode drama about the life of Dr. Dre. Music Video director Paul Hunter will direct the series.
Products.
Mac.
Macs that are currently being sold are:
Apple sells a variety of computer accessories for Macs, including Thunderbolt Display, Magic Mouse, Magic Trackpad, Wireless Keyboard, Battery Charger, the AirPort wireless networking products, and Time Capsule.
iPod.
On October 23, 2001, Apple introduced the iPod digital music player. Several updated models have since been introduced, and the iPod brand is now the market leader in portable music players by a significant margin. More than 350 million units have shipped . Apple has partnered with Nike to offer the Nike+iPod Sports Kit, enabling runners to synchronize and monitor their runs with iTunes and the Nike+ website.
Apple currently sells three variants of the iPod:
iPhone.
At the Macworld Conference & Expo in January 2007, Steve Jobs introduced the long-anticipated iPhone, a convergence of an Internet-enabled smartphone and iPod. The first-generation iPhone was released on June 29, 2007 for $499 (4 GB) and $599 (8 GB) with an AT&T contract. On February 5, 2008, it was updated to have 16 GB of memory, in addition to the 8 GB and 4 GB models. It combined a 2.5G quad band GSM and EDGE cellular phone with features found in handheld devices, running scaled-down versions of Apple's Mac OS X (dubbed iPhone OS, later renamed iOS), with various Mac OS X applications such as Safari and Mail. It also includes web-based and Dashboard apps such as Google Maps and Weather. The iPhone features a touchscreen display, Bluetooth, and Wi-Fi (both "b" and "g").
A second version, the iPhone 3G, was released on July 11, 2008 with a reduced price of $199 for the 8 GB version and $299 for the 16 GB version. This version added support for 3G networking and assisted-GPS navigation. The flat silver back and large antenna square of the original model were eliminated in favor of a glossy, curved black or white back. Software capabilities were improved with the release of the App Store, which provided iPhone-compatible applications to download. On April 24, 2009, the App Store surpassed one billion downloads. On June 8, 2009, Apple announced the iPhone 3GS. It provided an incremental update to the device, including faster internal components, support for faster 3G speeds, video recording capability, and voice control.
At the Worldwide Developers Conference (WWDC) on June 7, 2010, Apple announced the redesigned iPhone 4. It featured a 960x640 display, the Apple A4 processor, a gyroscope for enhanced gaming, a 5MP camera with LED flash, front-facing VGA camera and FaceTime video calling. Shortly after its release, reception issues were discovered by consumers, due to the stainless steel band around the edge of the device, which also serves as the phone's cellular signal and Wi-Fi antenna. The issue was corrected by a "Bumper Case" distributed by Apple for free to all owners for a few months. In June 2011, Apple overtook Nokia to become the world's biggest smartphone maker by volume. On October 4, 2011, Apple unveiled the iPhone 4S, which was first released on October 14, 2011. It features the Apple A5 processor and Siri voice assistant technology, the latter of which Apple had acquired in 2010. It also features an updated 8MP camera with new optics. Apple began a new accessibility feature, Made for iPhone Hearing Aids with the iPhone 4S. Made for iPhone Hearing Aids feature Live Listen, it can help you hear a conversation in a noisy room or hear someone speaking across the room. Apple sold 4 million iPhone 4S phones in the first three days of availability.
On September 12, 2012, Apple introduced the iPhone 5. It has a 4-inch display, 4G LTE connectivity, and the upgraded Apple A6 chip, among several other improvements. Two million iPhones were sold in the first twenty-four hours of pre-ordering and over five million handsets were sold in the first three days of its launch. Upon the launch of the iPhone 5S and iPhone 5C, Apple set a new record for first-weekend smartphone sales by selling over nine million devices in the first three days of its launch. The release of the iPhone 5S and 5C was the first time that Apple simultaneously launched two models.
A patent filed in July 2013 revealed the development of a new iPhone battery system that uses location data in combination with data on the user's habits to moderate the handsets power settings accordingly. Apple is working towards a power management system that will provide features such as the ability of the iPhone to estimate the length of time a user will be away from a power source to modify energy usage and a detection function that adjusts the charging rate to best suit the type of power source that is being used.
In a March 2014 interview, Apple designer Jonathan Ive used the iPhone as an example of Apple's ethos of creating high-quality, life-changing products. He explained that the phones are comparatively expensive due to the intensive effort that is used to make them:
On September 9, 2014, Apple introduced the iPhone 6, alongside the iPhone 6 Plus that both have screen sizes over 4-inches. One year later, Apple introduced the iPhone 6S, and iPhone 6S Plus, which introduced a new technology called 3D Touch, including an increase of the rear camera to 12 MP, and the FaceTime camera to 5 MP. On March 21, 2016, Apple introduced the iPhone SE that has a 4-inch size last used with the 5S and has the same internal hardware as the 6S.
iPad.
On January 27, 2010, Apple introduced their much-anticipated media tablet, the iPad, which runs a modified version of iOS. It offers multi-touch interaction with multimedia formats including newspapers, e-books, photos, videos, music, word processing documents, video games, and most existing iPhone apps using a 9.7-inch screen. It also includes a mobile version of Safari for web browsing, as well as access to the App Store, iTunes Library, iBookstore, Contacts, and Notes. Content is downloadable via Wi-Fi and optional 3G service or synced through the user's computer. AT&T was initially the sole U.S. provider of 3G wireless access for the iPad.
On March 2, 2011, Apple introduced the iPad 2, which had a faster processor and a camera on the front and back. It also added support for optional 3G service provided by Verizon in addition to AT&T. The availability of the iPad 2 was initially limited as a result of a devastating earthquake and tsunami in Japan in March 2011.
The third-generation iPad was released on March 7, 2012 and marketed as "the new iPad". It added LTE service from AT&T or Verizon, an upgraded A5X processor, and Retina display. The dimensions and form factor remained relatively unchanged, with the new iPad being a fraction thicker and heavier than the previous version and featuring minor positioning changes.
On October 23, 2012, Apple's fourth-generation iPad came out, marketed as the "iPad with Retina display". It added the upgraded A6X processor and replaced the traditional 30-pin dock connector with the all-digital Lightning connector. The iPad Mini was also introduced. It featured a reduced 7.9-inch display and much of the same internal specifications as the iPad 2.
On October 22, 2013, Apple introduced the iPad Air and the iPad Mini with Retina Display, both featuring a new 64-bit Apple A7 processor.
The iPad Air 2 was unveiled on October 16, 2014. It added better graphics and central processing and a camera burst mode as well as minor updates. The iPad Mini 3 was unveiled at the same time.
Since its launch, iPad users have downloaded over three billion apps. The total number of App Store downloads, , is over 100 billion.
On September 9, 2015, Apple announced the iPad Pro, an iPad with a 12.9-inch display that supports two new accessories, the Smart Keyboard and Apple Pencil. A 9.7-inch iPad Pro was announced on March 21, 2016.
Apple Watch.
The Apple Watch smartwatch was launched by Cook on September 9, 2014, and released on April 24, 2015. The wearable device consists of fitness-tracking capabilities that are similar to Fitbit, and must be used in combination with an iPhone to work (only the iPhone 5, or later models, are compatible with the Apple Watch).
Apple TV.
At the 2007 Macworld conference, Jobs demonstrated the Apple TV (previously known as the iTV), a set-top video device intended to bridge the sale of content from iTunes with high-definition televisions. The device links up to a user's TV and syncs, either via Wi-Fi or a wired network, with one computer's iTunes library and streams content from an additional four. The Apple TV originally incorporated a 40 GB hard drive for storage, included outputs for HDMI and component video, and played video at a maximum resolution of 720p. On May 31, 2007, a 160 GB hard disk drive was released alongside the existing 40 GB model. A software update released on January 15, 2008 allowed media to be purchased directly from the Apple TV.
In September 2009, Apple discontinued the original 40 GB Apple TV and now continues to produce and sell the 160 GB Apple TV. On September 1, 2010, Apple released a completely redesigned Apple TV. The new device is 1/4 the size, runs quieter, and replaces the need for a hard drive with media streaming from any iTunes library on the network along with 8 GB of flash memory to cache media downloaded. Like the iPad and the iPhone, Apple TV runs on an A4 processor. The memory included in the device is half of that in the iPhone 4 at 256 MB; the same as the iPad, iPhone 3GS, third and fourth-generation iPod Touch.
It has HDMI out as the only video out source. Features include access to the iTunes Store to rent movies and TV shows (purchasing has been discontinued), streaming from internet video sources, including YouTube and Netflix, and media streaming from an iTunes library. Apple also reduced the price of the device to $99. A third generation of the device was introduced at an Apple event on March 7, 2012, with new features such as higher resolution (1080p) and a new user interface.
At the September 9, 2015 event, Apple unveiled an overhauled Apple TV, which now runs a variant of OS X, tvOS, and contains 32GB or 64 GB of NAND Flash to store games, programs, and to cache the current media playing. The release also coincided with the opening of a separate Apple TV App Store and a new Siri Remote with a touchpad, gyroscope and microphone.
Software.
Apple develops its own operating system to run on Macs, OS X, the latest version being OS X El Capitan (version 10.11). Apple also independently develops computer software titles for its OS X operating system. Much of the software Apple develops is bundled with its computers. An example of this is the consumer-oriented iLife software package that bundles iMovie, iPhoto and GarageBand. For presentation, page layout and word processing, iWork is available, which includes Keynote, Pages, and Numbers. iTunes, QuickTime media player, and Software Update are available as free downloads for both OS X and Windows.
Apple also offers a range of professional software titles. Their range of server software includes the operating system OS X Server; Apple Remote Desktop, a remote systems management application; and Xsan, a Storage Area Network file system. For the professional creative market, there is Final Cut Pro, a video production suite; Logic Pro, a comprehensive music toolkit; and Motion, an advanced effects composition program.
Apple also offers online services with iCloud, which provides cloud storage and syncing for a wide range of data, including email, contacts, calendars, photos and documents. It also offers iOS device backup, and is able to integrate directly with third-party apps for even greater functionality. iCloud is the fourth generation of online services provided by Apple, and was preceded by MobileMe, .Mac and iTools, all which met varying degrees of success.
Electric vehicles.
According to the "Sydney Morning Herald", Apple wants to start producing an electric car with autonomous driving as soon as 2020. Apple has made efforts to recruit battery development engineers and other electric automobile engineers from A123 Systems, LG Chem, Samsung Electronics, Panasonic, Toshiba, Johnson Controls and Tesla Motors.
Corporate identity.
Logo.
According to Steve Jobs, the company's name was inspired by his visit to an apple farm while on a fruitarian diet. Jobs thought the name "Apple" was "fun, spirited and not intimidating".
Apple's first logo, designed by Ron Wayne, depicts Sir Isaac Newton sitting under an apple tree. It was almost immediately replaced by Rob Janoff's "rainbow Apple", the now-familiar rainbow-colored silhouette of an apple with a bite taken out of it. Janoff presented Jobs with several different monochromatic themes for the "bitten" logo, and Jobs immediately took a liking to it. However, Jobs insisted that the logo be colorized to humanize the company. The logo was designed with a bite so that it would not be confused with a cherry. The colored stripes were conceived to make the logo more accessible, and to represent the fact the Apple II could generate graphics in color. This logo is often erroneously referred to as a tribute to Alan Turing, with the bite mark a reference to his method of suicide. Both Janoff and Apple deny any homage to Turing in the design of the logo.
On August 27, 1999 (the year following the introduction of the iMac G3), Apple officially dropped the rainbow scheme and began to use monochromatic logos nearly identical in shape to the previous rainbow incarnation. An Aqua-themed version of the monochrome logo was used from 1999 to 2003, and a glass-themed version was used from 2007 to 2013.
Steve Jobs and Steve Wozniak were Beatles fans, but Apple Inc. had name and logo trademark issues with Apple Corps Ltd., a multimedia company started by the Beatles in 1967. This resulted in a series of lawsuits and tension between the two companies. These issues ended with settling of their most recent lawsuit in 2007.
Advertising.
Apple's first slogan, "Byte into an Apple", was coined in the late 1970s. From 1997 to 2002, the slogan "Think Different" was used in advertising campaigns, and is still closely associated with Apple. Apple also has slogans for specific product lines — for example, "iThink, therefore iMac" was used in 1998 to promote the iMac, and "Say hello to iPhone" has been used in iPhone advertisements. "Hello" was also used to introduce the original Macintosh, Newton, iMac ("hello (again)"), and iPod.
From the introduction of the Macintosh in 1984 with the 1984 Super Bowl commercial to the more modern 'Get a Mac' adverts, Apple has been recognized in for its efforts towards effective advertising and marketing for its products. However, claims made by later campaigns were criticized, particularly the 2005 Power Mac ads. Apple's product commercials gained a lot of attention as a result of their eye-popping graphics and catchy tunes. Musicians who benefited from an improved profile as a result of their songs being included on Apple commercials include Canadian singer Feist with the song "1234" and Yael Naïm with the song "New Soul".
Brand loyalty.
Apple's high level of brand loyalty is considered unusual for any product. Apple evangelists were actively engaged by the company at one time, but this was after the phenomenon had already been firmly established. Apple evangelist Guy Kawasaki has called the brand fanaticism "something that was stumbled upon," while Ive explained in 2014 that "People have an incredibly personal relationship" with Apple's products. Apple Store openings can draw crowds of thousands, with some waiting in line as much as a day before the opening or flying in from other countries for the event. The opening of New York City's Fifth Avenue "Cube" store had a line half a mile long; a few Mac fans used the setting to propose marriage. The line for the Ginza opening in Tokyo was estimated to include thousands of people and exceeded eight city blocks. The high level of brand loyalty has been criticized and ridiculed, applying the epithet "Apple fanboy" and mocking the lengthy lines before a product launch. An internal memo leaked in 2015 suggested the company planned to discourage long lines and direct customers to purchase its products on its website.
"Fortune" magazine named Apple the most admired company in the United States in 2008, and in the world from 2008 to 2012. On September 30, 2013, Apple surpassed Coca-Cola to become the world's most valuable brand in the Omnicom Group's "Best Global Brands" report. Boston Consulting Group has ranked Apple as the world's most innovative brand every year since 2005.
John Sculley told "The Guardian" newspaper in 1997: "People talk about technology, but Apple was a marketing company. It was the marketing company of the decade." Research in 2002 by NetRatings indicate that the average Apple consumer was usually more affluent and better educated than other PC company consumers. The research indicated that this correlation could stem from the fact that on average Apple Inc. products were more expensive than other PC products.
In response to a query about the devotion of loyal Apple consumers, Jonathan Ive responded:
What people are responding to is much bigger than the object. They are responding to something rare—a group of people who do more than simply make something work, they make the very best products they possibly can. It’s a demonstration against thoughtlessness and carelessness.
Home page.
The Apple website home page has been used to commemorate, or pay tribute to, milestones and events outside of Apple's product offerings:
Headquarters.
Apple Inc.'s world corporate headquarters are located in the middle of Silicon Valley, at 1–6 Infinite Loop, Cupertino, California. This Apple campus has six buildings that total and was built in 1993 by Sobrato Development Cos.
Apple has a satellite campus in neighboring Sunnyvale, California, where it houses a testing and research laboratory. AppleInsider published article in March 2014 claiming that Apple has a tucked away a top-secret facility where is developing the SG5 electric vehicle project codenamed "Titan" under the shell company name SixtyEight Research.
In 2006, Apple announced its intention to build a second campus in Cupertino about east of the current campus and next to Interstate 280. The new campus building will be designed by Norman Foster. The Cupertino City Council approved the proposed "spaceship" design campus on October 15, 2013, after a 2011 presentation by Jobs detailing the architectural design of the new building and its environs. The new campus is planned to house up to 13,000 employees in one central, four-storied, circular building surrounded by extensive landscape. It will feature a café with room for 3,000 sitting people and parking underground as well as in a parking structure. The 2.8 million square foot facility will also include Jobs's original designs for a fitness center and a corporate auditorium.
Apple's headquarters for Europe, the Middle East and Africa (EMEA) are located in Cork in the south of Ireland. The facility, which opened in 1980, was Apple's first location outside of the United States. Apple Sales International, which deals with all of Apple's international sales outside of the USA, is located at Apple's campus in Cork along with Apple Distribution International, which similarly deals with Apple's international distribution network. On April 20, 2012, Apple added 500 new jobs at its European headquarters, increasing the total workforce from around 2,800 to 3,300 employees. The company will build a new office block on its Hollyhill Campus to accommodate the additional staff. Its UK headquarters is at Stockley Park on the outskirts of London.
In February 2015, Apple opened their new 180,000-square-foot headquarters in Herzliya, Israel, which will accommodate approximately 800 employees. This opening was Apple's third office located within Israel; the first, also in Herzliya, was obtained as part of the Anobit acquisition, and the other is a research center in Haifa.
Stores.
Apple has 475 retail stores () in 17 countries and an online store available in 39 countries. Each store is designed to suit the needs of the location and regulatory authorities. Apple has received numerous architectural awards for its store designs, particularly its midtown Manhattan location on Fifth Avenue.
The Apple Store in Regent Street, London, was the first to open in Europe in November 2004, and is the most profitable shop in London with the highest sales per square foot, taking £60,000,000 pa, or £2,000 per square foot. The Regent Street store was surpassed in size by the nearby Apple Store in Covent Garden, which was surpassed in size by the Grand Central Terminal Apple Store, New York City, in December 2011.
Of the 43,000 Apple employees in the United States 30,000 work at Apple Stores. Apple Store employees make above average pay for retail employees and are offered money toward college as well as gym memberships, 401k plans, healthcare plans, product discounts, and reduced price on purchase of stock.
Corporate affairs.
Corporate culture.
Apple was one of several highly successful companies founded in the 1970s that bucked the traditional notions of corporate culture. Jobs often walked around the office barefoot even after Apple became a Fortune 500 company. By the time of the "1984" television commercial, Apple's informal culture had become a key trait that differentiated it from its competitors. According to a 2011 report in "Fortune," this has resulted in a corporate culture more akin to a startup rather than a multinational corporation.
As the company has grown and been led by a series of differently opinionated chief executives, it has arguably lost some of its original character. Nonetheless, it has maintained a reputation for fostering individuality and excellence that reliably attracts talented workers, particularly after Jobs returned to the company. Numerous Apple employees have stated that projects without Jobs's involvement often take longer than projects with it. To recognize the best of its employees, Apple created the Apple Fellows program which awards individuals who make extraordinary technical or leadership contributions to personal computing while at the company. The Apple Fellowship has so far been awarded to individuals including Bill Atkinson, Steve Capps, Rod Holt, Alan Kay, Guy Kawasaki, Al Alcorn, Don Norman, Rich Page, and Steve Wozniak.
At Apple, employees are specialists who are not exposed to functions outside their area of expertise. Jobs saw this as a means of having "best-in-class" employees in every role. For instance, Ron Johnson—Senior Vice President of Retail Operations until November 1, 2011—was responsible for site selection, in-store service, and store layout, yet had no control of the inventory in his stores (this was done by Cook, who had a background in supply-chain management). Apple is also known for strictly enforcing accountability. Each project has a "directly responsible individual," or "DRI" in Apple jargon. As an example, when iOS senior vice president Scott Forstall refused to sign Apple's official apology for numerous errors in the redesigned Maps app, he was forced to resign. Unlike other major U.S. companies Apple provides a relatively simple compensation policy for executives that does not include perks enjoyed by other CEOs like country club fees or private use of company aircraft. The company typically grants stock options to executives every other year.
Customer service.
In 1999 Apple retained Eight Inc. as a strategic retail design partner and began creating the Apple retail stores. Tim Kobe of Eight Inc. prepared an "Apple Retail" white paper for Jobs, outlining the ability of separate Apple retail stores to directly drive the Apple brand experience—Kobe used their recently completed work with The North Face and Nike as a basis for the white paper. The first two Apple Stores opened on May 19, 2001 in Tysons Corner, Virginia, and Glendale, California. More than 7,700 people visited Apple’s first two stores in the opening weekend, spending a total of US$599,000. , Apple maintains 425 retail stores in fourteen countries. In addition to Apple products, the stores sell third-party products like software titles, digital cameras, camcorders and handheld organizers.
A media article published in July 2013 provided details about Apple's "At-Home Apple Advisors" customer support program that serves as the corporation's call center. The advisors are employed within the U.S. and work remotely after undergoing a four-week training program and testing period. The advisors earn between US$9 and $12 per hour and receive intensive management to ensure a high quality of customer support.
Manufacturing.
The company's manufacturing, procurement and logistics enable it to execute massive product launches without having to maintain large, profit-sapping inventories. In 2011, Apple's profit margins were 40 percent, compared with between 10 and 20 percent for most other hardware companies. Cook's catchphrase to describe his focus on the company's operational arm is: “Nobody wants to buy sour milk”.
During the Mac's early history Apple generally refused to adopt prevailing industry standards for hardware, instead creating their own. This trend was largely reversed in the late 1990s, beginning with Apple's adoption of the PCI bus in the 7500/8500/9500 Power Macs. Apple has since joined the industry standards groups to influence the future direction of technology standards such as USB, AGP, HyperTransport, Wi-Fi, NVMe, PCIe and others in its products. FireWire is an Apple-originated standard that was widely adopted across the industry after it was standardized as IEEE 1394 and is a legally mandated port in all Cable TV boxes in the United States.
Labor practices.
The company advertised its products as being made in America until the late 1990s; however, as a result of outsourcing initiatives in the 2000s, almost all of its manufacturing is now handled abroad. According to a report by the "New York Times", Apple insiders "believe the vast scale of overseas factories as well as the flexibility, diligence and industrial skills of foreign workers have so outpaced their American counterparts that “Made in the U.S.A.” is no longer a viable option for most Apple products".
In 2006, the "Mail on Sunday" reported on the working conditions of the Chinese factories where contract manufacturers Foxconn and Inventec produced the iPod. The article stated that one complex of factories that assembled the iPod and other items had over 200,000 workers living and working within it. Employees regularly worked more than 60 hours per week and made around $100 per month. A little over half of the workers' earnings was required to pay for rent and food from the company.
Apple immediately launched an investigation after the 2006 media report, and worked with their manufacturers to ensure acceptable working conditions. In 2007, Apple started yearly audits of all its suppliers regarding worker's rights, slowly raising standards and pruning suppliers that did not comply. Yearly progress reports have been published since 2008. In 2011, Apple admitted that its suppliers' child labor practices in China had worsened.
The Foxconn suicides occurred between January and November 2010, when 18 Foxconn (Chinese: 富士康) employees attempted suicide, resulting in 14 deaths—the company was the world’s largest contract electronics manufacturer, for clients including Apple, at the time. The suicides drew media attention, and employment practices at Foxconn were investigated by Apple. Apple issued a public statement about the suicides, and company spokesperson Steven Dowling said:
is saddened and upset by the recent suicides at Foxconn ... A team from Apple is independently evaluating the steps they are taking to address these tragic events and we will continue our ongoing inspections of the facilities where our products are made.
The statement was released after the results from the company's probe into its suppliers' labor practices were published in early 2010. Foxconn was not specifically named in the report, but Apple identified a series of serious labor violations of labor laws, including Apple's own rules, and some child labor existed in a number of factories. Apple committed to the implementation of changes following the suicides.
Also in 2010, workers in China planned to sue iPhone contractors over poisoning by a cleaner used to clean LCD screens. One worker claimed that he and his coworkers had not been informed of possible occupational illnesses. After a high suicide rate in a Foxconn facility in China making iPads and iPhones, albeit a lower rate than that of China as a whole, workers were forced to sign a legally binding document guaranteeing that they would not kill themselves. Workers in factories producing Apple products have also been exposed to n-hexane, a neurotoxin that is a cheaper alternative than alcohol for cleaning the products.
A 2014 BBC investigation found excessive hours and other problems persisted, despite Apple's promise to reform factory practice after the 2010 Foxconn suicides. The Pegatron factory was once again the subject of review, as reporters gained access to the working conditions inside through recruitment as employees. While the BBC maintained that the experiences of its reporters showed that labor violations were continuing since 2010, Apple publicly disagreed with the BBC and stated: “We are aware of no other company doing as much as Apple to ensure fair and safe working conditions".
In December 2014, the Institute for Global Labour and Human Rights published a report which documented inhumane conditions for the 15,000 workers at a Zhen Ding Technology factory in Shenzhen, China, which serves as a major supplier of circuit boards for Apple's iPhone and iPad. According to the report, workers are pressured into 65 hour work weeks which leaves them so exhausted that they often sleep during lunch breaks. They are also made to reside in "primitive, dark and filthy dorms" where they sleep "on plywood, with six to ten workers in each crowded room." Omnipresent security personnel also routinely harass and beat the workers.
Environmental practices and initiatives.
Energy.
Following a Greenpeace protest, Apple released a statement on April 17, 2012, committing to ending its use of coal and shifting to 100% renewable clean energy. By 2013 Apple was using 100% renewable energy to power their data centers. Overall, 75% of the company's power came from clean renewable sources.
In 2010, Climate Counts, a nonprofit organization dedicated to directing consumers toward the greenest companies, gave Apple a score of 52 points out of a possible 100, which puts Apple in their top category "Striding". This was an increase from May 2008, when Climate Counts only gave Apple 11 points out of 100, which placed the company last among electronics companies, at which time Climate Counts also labeled Apple with a "stuck icon", adding that Apple at the time was "a choice to avoid for the climate conscious consumer".
, 100% of Apple's U.S. operations run on renewable energy, 100% of Apple's data centers run on renewable energy and 87% of Apple's global operations run on renewable energy. The Electronic Product Environmental Assessment Tool (EPEAT) allows consumers to see the effect a product has on the environment. Each product receives a Gold, Silver, or Bronze rank depending on its efficiency and sustainability. Every Apple tablet, notebook, desktop computer, and display that EPEAT ranks achieves a Gold rating, the highest possible.
In May 2015, Greenpeace evaluated the state of the Green Internet and commended Apple on their environmental practices saying, "Apple’s commitment to renewable energy has helped set a new bar for the industry, illustrating in very concrete terms that a 100% renewable Internet is within its reach, and providing several models of intervention for other companies that want to build a sustainable Internet."
During an event on March 21, 2016, Apple provided a status update on its environmental initiative to be 100% renewable in all of its worldwide operations. Lisa P. Jackson, Apple's vice president of Environment, Policy and Social Initiatives who reports directly to CEO, Tim Cook, announced that as of March 2016, 93% of Apple's worldwide operations are powered with renewable energy. Also featured was the company's efforts to use sustainable paper in their product packaging; 99% of all paper used by Apple in the product packaging comes from post-consumer recycled paper or sustainably-managed forests, as the company continues its move to all paper packaging for all of its products. Apple working in partnership with Conservation Fund, have preserved 36,000 acres of working forests in Maine and North Carolina. Another partnership announced is with the World Wildlife Fund to preserve up to 1,000,000 acres of forests in China. Featured was the company's installation of a 40 MW solar power plant in the Sichuan province of China that was tailor made to coexist with the indigenous yaks that eat hay produced on the land, by raising the panels to be several feet off of the ground so the yaks and their feed would be unharmed grazing beneath the array. This installation alone compensates for more than all of the energy used in Apple's Stores and Offices in the whole of China, negating the company's energy carbon footprint in the country. In Singapore, Apple has worked with the Singaporean government to cover the rooftops of 800 buildings in the city-state with solar panels allowing Apple's Singapore operations to be run on 100% renewable energy. Liam was introduced to the world, an advanced robotic disassembler and sorter designed by Apple Engineers in California specifically for recycling outdated or broken iPhones. Reuses and recycles parts from traded in products.
Toxins.
Following further campaigns by Greenpeace, in 2008, Apple became the first electronics manufacturer to fully eliminate all polyvinyl chloride (PVC) and brominated flame retardants (BFRs) in its complete product line. In June 2007, Apple began replacing the cold cathode fluorescent lamp (CCFL) backlit LCD displays in its computers with mercury-free LED-backlit LCD displays and arsenic-free glass, starting with the upgraded MacBook Pro. Apple offers comprehensive and transparent information about the CO2e, emissions, materials, and electrical usage concerning every product they currently produce or have sold in the past (and which they have enough data needed to produce the report), in their portfolio on their homepage. Allowing consumers to make informed purchasing decisions on the products they offer for sale. In June 2009, Apple's iPhone 3GS was free of PVC, arsenic, and BFRs. All Apple products now have mercury-free LED-backlit LCD displays, arsenic-free glass, and non-PVC cables. All Apple products have EPEAT Gold status and beat the latest Energy Star guidelines in each product's respective regulatory category.
In November 2011, Apple was featured in Greenpeace's Guide to Greener Electronics, which ranks electronics manufacturers on sustainability, climate and energy policy, and how "green" their products are. The company ranked fourth of fifteen electronics companies (moving up five places from the previous year) with a score of 4.6/10 down from 4.9. Greenpeace praises Apple's sustainability, noting that the company exceeded its 70% global recycling goal in 2010. It continues to score well on the products rating with all Apple products now being free of PVC plastic and BFRs. However, the guide criticizes Apple on the Energy criteria for not seeking external verification of its greenhouse gas emissions data and for not setting out any targets to reduce emissions. In January 2012, Apple requested that its cable maker, Volex, begin producing halogen-free USB and power cables.
Finance.
Apple is the world's largest information technology company by revenue and the world's second-largest mobile phone manufacturer. It is also the largest publicly traded corporation in the world by market capitalization, with an estimated market capitalization of $446 billion by January 2014. On February 17, 2015, Apple became the first US corporation to be valued at over $750B. , Apple maintains 475 retail stores in seventeen countries, of which 207 are outside the U.S., as well as the online Apple Store and iTunes Store, the latter of which is the world's largest music retailer. It employs 115,000 permanent full-time employees and 3,300 temporary full-time employees worldwide.
In its fiscal year ending in September 2011, Apple Inc. reported a total of $108 billion in annual revenues—a significant increase from its 2010 revenues of $65 billion—and nearly $82 billion in cash reserves. On March 19, 2012, Apple announced plans for a $2.65-per-share dividend beginning in fourth quarter of 2012, per approval by their board of directors. On September 2012, Apple reached a record share price of more than $705 and closed at above 700. With 936,596,000 outstanding shares (),
The company's worldwide annual revenue in 2013 totaled $170 billion. In May 2013, Apple entered the top ten of the Fortune 500 list of companies for the first time, rising 11 places above its 2012 ranking to take the sixth position. , Apple has around US$234 billion of cash and marketable securities, of which 90% is located outside the United States for tax purposes.
Apple amassed 65% of all profits made by the eight largest worldwide smartphone manufacturers in quarter one of 2014, according to a report by Canaccord Genuity. In the first quarter of 2015, the company garnered 92% of all earnings made by the eight largest global smartphone makers.
Tax practices.
Apple has created subsidiaries in low-tax places such as Ireland, the Netherlands, Luxembourg and the British Virgin Islands to cut the taxes it pays around the world. According to "The New York Times," in the 1980s Apple was among the first tech companies to designate overseas salespeople in high-tax countries in a manner that allowed the company to sell on behalf of low-tax subsidiaries on other continents, sidestepping income taxes. In the late 1980s Apple was a pioneer of an accounting technique known as the "Double Irish with a Dutch sandwich," which reduces taxes by routing profits through Irish subsidiaries and the Netherlands and then to the Caribbean.
British Conservative Party Member of Parliament Charlie Elphicke published research on October 30, 2012, which showed that some multinational companies, including Apple Inc., were making billions of pounds of profit in the UK, but were paying an effective tax rate to the UK Treasury of only 3 percent, well below standard corporation tax. He followed this research by calling on the Chancellor of the Exchequer George Osborne to force these multinationals, which also included Google and The Coca-Cola Company, to state the effective rate of tax they pay on their UK revenues. Elphicke also said that government contracts should be withheld from multinationals who do not pay their fair share of UK tax. In June 2014 the European Commissioner for Competition launched an investigation of Apple's tax practices in Ireland, as part of a wider probe of multi-national companies' tax arrangements in various European countries.
It is a matter of public record that Apple Inc. is the single largest taxpayer to the Department of the Treasury of the United States of America with an effective tax rate of approximately of 26% as of the Second Quarter of the Apple Fiscal Year 2016.
In 2015 Reuters reported that Apple had earnings abroad of $54.4 billion which were untaxed by the IRS of the United States. Under U.S. tax law governed by the IRC, corporations don't pay income tax on overseas profits unless the profits repatriated into the United States and as such Apple has the fiscal responsibility to its shareholders to leave it overseas until a repatriation holiday or comprehensive tax reform takes place in the United States.
Litigation.
Apple has been a participant in various legal proceedings and claims since it began operation. In particular, Apple is known for and promotes itself as actively and aggressively enforcing its intellectual property interests. Some litigation examples include "Apple v. Samsung", "Apple v. Microsoft", "Motorola Mobility v. Apple Inc.", and "Apple Corps v. Apple Computer". Apple has also had to defend itself against charges on numerous occasions of violating intellectual property rights. Most have been dismissed in the courts as shell companies known as patent trolls, with no evidence of actual use of patents in question.
Privacy Stance.
Apple has made clear its stance on Privacy and as such has made available Transparency Reports on the Governmental Requests it receives. Apple states plainly, "On devices running iOS 8 and later versions, your personal data is placed under the protection of your passcode.] For all devices running iOS 8 and later versions, Apple will not perform iOS data extractions in response to government search warrants because the files to be extracted are protected by an encryption key that is tied to the user’s passcode, which Apple does not possess."
In its latest “Who Has Your Back?” report, once again the Electronic Frontier Foundation (EFF) awarded Apple 5 out of 5 stars “commend Apple for its strong stance regarding user rights, transparency, and privacy.”
Charitable causes.
, Apple is a partner of Product Red, a campaign whose mission is to prevent the transmission of HIV from mother to child by 2015. , Apple's contributions to the campaign reached $75 million, making the company Product Red's largest partner.
In November 2012, Apple donated $2.5 million to the American Red Cross to aid relief efforts after Hurricane Sandy.
On April 14, 2016, Apple and the World Wide Fund for Nature (WWF) announced that they have engaged in a partnership to, "help protect life on our planet." Apple released a special page in the iTunes App Store, Apps for Earth. In the arrangement, Apple has committed that through April 24, WWF will receive 100% of the proceeds from the applications participating in the App Store via both the purchases of any paid apps and the In-App Purchases.
References.
Sources

</doc>
<doc id="857" url="https://en.wikipedia.org/wiki?curid=857" title="Aberdeenshire">
Aberdeenshire

Aberdeenshire () is one of the 32 council areas of Scotland.
It takes its name from the old County of Aberdeen which had substantially different boundaries. Modern Aberdeenshire includes all of what was once Kincardineshire, as well as part of Banffshire. The old boundaries are still officially used for a few purposes, namely land registration and lieutenancy.
Aberdeenshire Council is headquartered at Woodhill House, in Aberdeen, making it the only Scottish council whose headquarters are located outside its jurisdiction. Aberdeen itself forms a different council area (Aberdeen City). Aberdeenshire borders Angus and Perth and Kinross to the south, and Highland and Moray to the west.
Traditionally, it has been economically dependent upon the primary sector (agriculture, fishing, and forestry) and related processing industries. Over the last 40 years, the development of the oil and gas industry and associated service sector has broadened Aberdeenshire's economic base, and contributed to a rapid population growth of some 50% since 1975. Its land represents 8% of Scotland's overall territory. It covers an area of .
History.
Aberdeenshire has a rich prehistoric and historic heritage. It is the locus of a large number of Neolithic and Bronze Age archaeological sites, including Longman Hill, Kempstone Hill, Catto Long Barrow and Cairn Lee. The area was settled in the Bronze Age by the Beaker culture, who arrived from the south around 2000-1800 BC. Stone circles and cairns were constructed predominantly in this era. In the Iron Age, the people built hill forts were built. Around the 1st century AD, the Taexali people, who have left little history, were believed to have resided along the coast. The Picts were the next documented inhabitants of the area, and were no later than 800-900 AD. The Romans also were in the area during this period, as they left signs at Kintore. Christianity influenced the inhabitants early on, and there were Celtic monasteries at Old Deer and Monymusk.
Since medieval times there have been a number of traditional paths that crossed the Mounth (a spur of mountainous land that extends from the higher inland range to the North Sea slightly north of Stonehaven) through present-day Aberdeenshire from the Scottish Lowlands to the Highlands. Some of the most well known and historically important trackways are the Causey Mounth and Elsick Mounth.
Aberdeenshire played an important role in the fighting between the Scottish clans. Clan MacBeth and the Clan Canmore were two of the larger clans. Macbeth fell at Lumphanan in 1057. During the Anglo-Norman penetration, other families arrives such as House of Balliol, Clan Bruce, and Clan Cumming (Comyn). When the fighting amongst these newcomers resulted in the Scottish Wars of Independence, the English king Edward I traveled across the area twice, in 1296 and 1303. In 1307, Robert the Bruce was victorious near Inverurie. Along with his victory came new families, namely the Forbeses and the Gordons.
These new families set the stage for the upcoming rivalries during the 14th and 15th centuries. This rivalry grew worse during and after the Protestant Reformation, when religion was another reason for conflict between the clans. The Gordon family adhered to Catholicism and the Forbes to Protestantism. Three universities were founded in the area prior to the 17th century, King's College in Old Aberdeen (1494), Marischal College in Aberdeen (1593), and the University of Fraserburgh (1597).
After the end of the Revolution of 1688, an extended peaceful period was interrupted only by such fleeting events such as the Rising of 1715 and the Rising of 1745. The latter resulted in the end of the ascendancy of Episcopalianism and the feudal power of landowners. An era began of increased agricultural and industrial progress. During the 17th century, Aberdeenshire was the location of more fighting, centered around the Marquess of Montrose and the English Civil Wars. This period also saw increased wealth due to the increase in trade with Germany, Poland, and the Low Countries.
The present council area is named after the historic county of Aberdeen, which had different boundaries and was abolished in 1975 under the Local Government (Scotland) Act 1973. It was replaced by Grampian Regional Council and five district councils: Banff and Buchan, Gordon, Kincardine and Deeside, Moray and the City of Aberdeen. The current Aberdeenshire consists of all of former Aberdeenshire, former Kincardineshire and the northeast portions of Banffshire. Local government functions were shared between the two levels. In 1996, under the Local Government etc (Scotland) Act 1994, the Banff and Buchan district, Gordon district and Kincardine and Deeside district were merged to form the present Aberdeenshire council area, with the other two districts becoming autonomous council areas.
Demographics.
The population of the council area has risen over 50% since 1971 to approximately 247,600, representing 4.7% of Scotland's total. Aberdeenshire's population has increased by 9.1% since 2001, while Scotland's total population grew by 3.8%.
The census lists a relatively high proportion of under 16s and slightly fewer people of working-age compared with the Scottish average.
The fourteen biggest settlements in Aberdeenshire (with 2011 population estimates) are:
Economy.
Aberdeenshire's Gross Domestic Product (GDP) is estimated at £3,496m (2011), representing 5.2% of the Scottish total. Aberdeenshire's economy is closely linked to Aberdeen City's (GDP £7,906m) and in 2011 the region as a whole was calculated to contribute 16.8% of Scotland's GDP. Between 2012 and 2014 the combined Aberdeenshire and Aberdeen City economic forecast GDP growth rate is 6.8%, the highest growth rate of any local council area and above the Scottish rate of 4.8%.
A significant proportion of Aberdeenshire's working residents commute to Aberdeen City for work, varying from 11.5% from Fraserburgh to 65% from Westhill.
Average Gross Weekly Earnings (for full-time employees employed in work places in Aberdeenshire in 2011) are £570.60. This is lower than the Scottish average by £4.10 and a fall of 2.6% on the 2010 figure. The average gross weekly pay of people resident in Aberdeenshire is much higher, at £641.90, as many people commute out
of Aberdeenshire, principally into Aberdeen City.
Total employment (excluding farm data) in Aberdeenshire is estimated at 93,700 employees (Business Register and
Employment Survey 2009). The majority of employees work within the service sector, predominantly in public administration, education and health. Almost 19% of employment is within the public sector. Aberdeenshire's economy remains closely linked to Aberdeen City's and the North Sea oil industry, with many employees in oil related jobs.
The average monthly unemployment (claimant count) rate for Aberdeenshire in 2011 was 1.5%. This is lower than the average rates for Aberdeen City (2.3%), Scotland (4.2%) and the UK (3.8%).
Governance and politics.
The council has 68 councillors, elected in 19 multi-member wards by Single Transferable Vote. The 2012 elections resulted in the following representation:
The overall political composition of the council, following subsequent defections and by-elections, is as follows:
The Council's Revenue Budget for 2012/13 totals approx £548 million. The Education, Learning and Leisure Service takes the largest share of budget (52.3%), followed by Housing and Social Work (24.3%), Infrastructure Services (15.9%), Joint Boards (such as Fire and Police) and Misc services (7.9%) and Trading Activities (0.4%).
21.5% of the revenue is raised locally through the Council Tax. Average Band D Council Tax is £1,141 (2012/13), no change on the previous year.
The current chief executive of the Council is Jim Savege and the elected Council Co-Leaders are Richard Thomson and Martin Kitts-Hayes. Aberdeenshire also has a Provost, who is Councillor Hamish Vernal.
The council has devolved power to six area committees: Banff and Buchan; Buchan; Formartine; Garioch; Marr; and Kincardine and Mearns. Each area committee takes decisions on local issues such as planning applications, and the split is meant to reflect the diverse circumstances of each area. (Boundary map)
Notable features.
The following significant structures or places are within Aberdeenshire:
Hydrology and climate.
There are numerous rivers and burns in Aberdeenshire, including Cowie Water, Carron Water, Burn of Muchalls, River Dee, River Don, River Ury, River Ythan, Water of Feugh, Burn of Myrehouse, Laeca Burn and Luther Water. Numerous bays and estuaries are found along the seacoast of Aberdeenshire, including Banff Bay, Ythan Estuary, Stonehaven Bay and Thornyhive Bay. Aberdeenshire is in the rain shadow of the Grampians, therefore it is a generally dry climate, with portions of the coast, receiving of moisture annually. Summers are mild and winters are typically cold in Aberdeenshire; Coastal temperatures are moderated by the North Sea such that coastal areas are typically cooler in the summer and warmer in winter than inland locations. Coastal areas are also subject to haar, or coastal fog.

</doc>
<doc id="859" url="https://en.wikipedia.org/wiki?curid=859" title="Aztlan Underground">
Aztlan Underground

Aztlan Underground is a fusion band from Los Angeles. Since early 1989, Aztlan Underground has played Rapcore. Indigenous drums, flutes, and rattles are commonplace in its musical compositions.
This unique sound is the backdrop for the band's message of dignity for indigenous people, all of humanity, and Earth. Aztlan Underground has been cultivating a grass roots audience across the country, which has become a large and loyal underground following. Their music includes spoken word pieces and elements of punk, hip hop, rock, funk, jazz, and indigenous music, among others.
The artists are Chenek "DJ Bean" (turntables, samples and percussion), Yaotl (vocals, indigenous percussion), Joe "Peps" (bass, rattles), Alonzo Beas (guitars, synth), Caxo (drums, indigenous percussion), and Bulldog (vocals, flute).
Aztlan Underground appeared on television on Culture Clash on Fox in 1993, was part of "Breaking Out", a concert on pay per view in 1998, and was featured in the independent films "Algun Dia" and "Frontierlandia".
The band has been mentioned or featured in various newspapers and magazines: the Vancouver Sun, Northshore News (Vancouver, Canada newspaper), New Times (Los Angeles weekly entertainment newspaper), BLU Magazine (underground hip hop magazine), BAM Magazine (Southern California), La Banda Elastica Magazine, and the Los Angeles Times Calendar section. It is also the subject of a chapter in "It's Not About A Salary", by Brian Cross. They also opened for Rage Against the Machine in Mexico City.
It was nominated in the New Times 1998 "Best Latin Influenced" category, the BAM Magazine 1999 "Best Rock en Español" category, and the LA Weekly 1999 "Best Hip Hop" category.
Aztlan Underground were signed to a Basque record label in 1999 which enabled them to tour Spain extensively and perform in France and Portugal.
Other parts of the world that Aztlan Underground have performed include Canada, Australia, and Venezuela.
The band completed their third album and released it exclusively digital on August 29, 2009. The band is set to begin writing a new record this year.
Aztlan Underground were nominated for four Native American Music Award categories for the Nammys 2010. See Nammys.com
Discography.
"Decolonize".
Year:1995
"Sub-Verses".
Year:1998
"Aztlan Underground".
Year:2009

</doc>
<doc id="863" url="https://en.wikipedia.org/wiki?curid=863" title="American Civil War">
American Civil War

The American Civil War was a civil war fought from 1861 to 1865, to determine the survival of the United States of America as it defeated the bid for independence by the breakaway Confederate States of America. Among the 34 states in January 1861, seven Southern slave states individually declared their secession from the U.S. and formed the Confederate States of America. War broke out in April 1861 when they attacked a U.S. fortress, Fort Sumter, and ended with the surrender of all the Confederate armies in spring 1865.
The "Confederacy" colloquially known as the "South", grew to include eleven states; it claimed two more states and several western territories. The Confederacy was never diplomatically recognized by any foreign country. The states that remained loyal were known as the "Union" or the "North". The war had its origin in the factious issue of slavery, especially the extension of slavery into the western territories. Four years of intense combat left 750,000 soldiers dead, and destroyed much of the South's infrastructure. The Confederacy collapsed and slavery was abolished in the entire country. The Reconstruction Era (1863-1877) overlapped and followed the war, with its fitful process of restoring national unity, strengthening the national government, and guaranteeing civil rights to the freed slaves.
History.
In the 1860 presidential election, Republicans, led by Abraham Lincoln, supported banning slavery in all the U.S. territories, something the Southern states viewed as a violation of their constitutional rights and as being part of a plan to eventually abolish slavery. The Republican Party, dominant in the North, secured a majority of the electoral votes, and Lincoln was elected the first Republican president, but before his inauguration, seven slave states with cotton-based economies formed the Confederacy. The first six to secede had the highest proportions of slaves in their populations, a total of 48.8 percent. Eight remaining slave states continued to reject calls for secession. Outgoing Democratic President James Buchanan and the incoming Republicans rejected secession as illegal. Lincoln's March 4, 1861 inaugural address declared his administration would not initiate civil war. Speaking directly to "the Southern States," he reaffirmed, "I have no purpose, directly or indirectly to interfere with the institution of slavery in the United States where it exists. I believe I have no lawful right to do so, and I have no inclination to do so." Confederate forces seized numerous federal forts within territory claimed by the Confederacy. Efforts at compromise failed, and both sides prepared for war. The Confederates assumed that European countries were so dependent on "King Cotton" that they would intervene; none did, and none recognized the new Confederate States of America.
Hostilities began on April 12, 1861, when Confederate forces fired upon Fort Sumter. While in the Western Theater the Union made significant permanent gains, in the Eastern Theater, battle was inconclusive in 1861–62. The autumn 1862 Confederate campaigns into Maryland and Kentucky failed, dissuading British intervention. Lincoln issued the Emancipation Proclamation, which made ending slavery a war goal. To the west, by summer 1862 the Union destroyed the Confederate river navy, then much of their western armies and seized New Orleans. The 1863 Union siege of Vicksburg split the Confederacy in two at the Mississippi River. In 1863, Robert E. Lee's Confederate incursion north ended at the Battle of Gettysburg. Western successes led to Ulysses S. Grant's command of all Union armies in 1864. Inflicting an ever-tightening naval blockade of Confederate ports, the Union marshaled the resources and manpower to attack the Confederacy from all directions, leading to the fall of Atlanta to William T. Sherman and his march to the sea. The last significant battles raged around the Siege of Petersburg. Lee's escape attempt ended with his surrender at Appomattox Court House, on April 9, 1865. While the military war was coming to an end, the political reintegration of the nation was to take another 12 years of the Reconstruction Era.
The American Civil War was one of the earliest true industrial wars. Railroads, the telegraph, steamships, and mass-produced weapons were employed extensively. The mobilization of civilian factories, mines, shipyards, banks, transportation and food supplies all foreshadowed the impact of industrialization in World War I. It remains the deadliest war in American history. From 1861 to 1865, it has been traditionally estimated that about 620,000 died but recent scholarship argues that 750,000 soldiers died, along with an undetermined number of civilians. By one estimate, the war claimed the lives of 10 percent of all Northern males 20–45 years old, and 30 percent of all Southern white males aged 18–40.
Causes of secession.
The causes of the Civil War were complex and have been controversial since the war began. James C. Bradford wrote that the issue has been further complicated by historical revisionists, who have tried to offer a variety of reasons for the war. Slavery was the central source of escalating political tension in the 1850s. The Republican Party was determined to prevent any spread of slavery, and many Southern leaders had threatened secession if the Republican candidate, Lincoln, won the 1860 election. After Lincoln won without carrying a single Southern state, many Southern whites felt that disunion had become their only option, because they thought that they were losing representation, which would hamper their ability to promote pro-slavery acts and policies.
Root causes.
Slavery.
Contemporary actors, the Union and Confederate leadership and fighting soldiers on both sides believed that slavery caused the Civil War. Union men mainly believed the war was to emancipate the slaves. Confederates fought to protect southern society, and slavery as an integral part of it. From the anti-slavery perspective, the issue was primarily about whether the system of slavery was an anachronistic evil that was incompatible with Republicanism in the United States. The strategy of the anti-slavery forces was containment — to stop the expansion and thus put slavery on a path to gradual extinction. The slave-holding interests in the South denounced this strategy as infringing upon their Constitutional rights. Southern whites believed that the emancipation of slaves would destroy the South's economy because of the alleged laziness of blacks under free labor.
Slavery was illegal in the North, having been outlawed in the late 18th and early 19th century. It was fading in the border states and in Southern cities, but was expanding in the highly profitable cotton districts of the South and Southwest. Subsequent writers on the American Civil War looked to several factors explaining the geographic divide, including sectionalism, protectionism, and state's rights.
Sectionalism.
Sectionalism refers to the different economies, social structure, customs and political values of the North and South. It increased steadily between 1800 and 1860 as the North, which phased slavery out of existence, industrialized, urbanized, and built prosperous farms, while the deep South concentrated on plantation agriculture based on slave labor, together with subsistence farming for poor freedmen. In the 1840s and 50s, the issue of accepting slavery (in the guise of rejecting slave-owning bishops and missionaries) split the nation's largest religious denominations (the Methodist, Baptist and Presbyterian churches) into separate Northern and Southern denominations.
Historians have debated whether economic differences between the industrial Northeast and the agricultural South helped cause the war. Most historians now disagree with the economic determinism of historian Charles A. Beard in the 1920s and emphasize that Northern and Southern economies were largely complementary. While socially different, the sections economically benefited each other.
Protectionism.
Historically, southern slave-holding states, because of their low cost manual labor, had little perceived need for mechanization, and supported having the right to sell cotton and purchase manufactured goods from any nation. Northern states, which had heavily invested in their still-nascent manufacturing, could not compete with the full-fledged industries of Europe in offering high prices for cotton imported from the South and low prices for manufactured exports in return. Thus, northern manufacturing interests supported tariffs and protectionism while southern planters demanded free trade.
The Democrats in Congress, controlled by Southerners, wrote the tariff laws in the 1830s, 1840s, and 1850s, and kept reducing rates so that the 1857 rates were the lowest since 1816. The Whigs and Republicans complained because they favored high tariffs to stimulate industrial growth, and Republicans called for an increase in tariffs in the 1860 election. The increases were only enacted in 1861 after Southerners resigned their seats in Congress. The tariff issue was and is sometimes cited–long after the war–by Lost Cause historians and neo-Confederate apologists. In 1860–61 none of the groups that proposed compromises to head off secession raised the tariff issue. Pamphleteers North and South rarely mentioned the tariff, and when some did, for instance, Matthew Fontaine Maury and John Lothrop Motley, they were generally writing for a foreign audience.
State's rights.
The South argued that each state had the right to secede–leave the Union–at any time, that the Constitution was a "compact" or agreement among the states. Northerners (including President Buchanan) rejected that notion as opposed to the will of the Founding Fathers who said they were setting up a perpetual union. Historian James McPherson writes concerning states' rights and other non-slavery explanations:
Territorial crisis.
Between 1803 and 1854, the United States achieved a vast expansion of territory through purchase, negotiation, and conquest. At first, the new states carved out of these territories entering the union were apportioned equally between slave and free states. It was over territories west of the Mississippi that the proslavery and antislavery forces collided.
With the conquest of northern Mexico west to California in 1848, slaveholding interests looked forward to expanding into these lands and perhaps Cuba and Central America as well. 
Northern "free soil" interests vigorously sought to curtail any further expansion of slave territory. The Compromise of 1850 over California balanced a free soil state with stronger fugitive slave laws for a political settlement after four years of strife in the 1840s. But the states admitted following California were all free: Minnesota (1858), Oregon (1859) and Kansas (1861). In the southern states the question of the territorial expansion of slavery westward again became explosive. Both the South and the North drew the same conclusion: "The power to decide the question of slavery for the territories was the power to determine the future of slavery itself."
By 1860, four doctrines had emerged to answer the question of federal control in the territories, and they all claimed they were sanctioned by the Constitution, implicitly or explicitly. The first of these "conservative" theories, represented by the Constitutional Union Party, argued that the Missouri Compromise apportionment of territory north for free soil and south for slavery should become a Constitutional mandate. The Crittenden Compromise of 1860 was an expression of this view. The second doctrine of Congressional preeminence, championed by Abraham Lincoln and the Republican Party, insisted that the Constitution did not bind legislators to a policy of balance – that slavery could be excluded in a territory as it was done in the Northwest Ordinance at the discretion of Congress, thus Congress could restrict human bondage, but never establish it. The Wilmot Proviso announced this position in 1846.
Senator Stephen A. Douglas proclaimed the doctrine of territorial or "popular" sovereignty – which asserted that the settlers in a territory had the same rights as states in the Union to establish or disestablish slavery as a purely local matter. The Kansas–Nebraska Act of 1854 legislated this doctrine. In Kansas Territory, years of pro and anti-slavery violence and political conflict erupted; the congressional House of Representatives voted to admit Kansas as a free state in early 1860, but its admission in the Senate was delayed until January 1861, after the 1860 elections when southern senators began to leave.
The fourth theory was advocated by Mississippi Senator Jefferson Davis, one of state sovereignty ("states' rights"), also known as the "Calhoun doctrine", named after the South Carolinian political theorist and statesman John C. Calhoun. Rejecting the arguments for federal authority or self-government, state sovereignty would empower states to promote the expansion of slavery as part of the Federal Union under the U.S. Constitution. "States' rights" was an ideology formulated and applied as a means of advancing slave state interests through federal authority. As historian Thomas L. Krannawitter points out, the "Southern demand for federal slave protection represented a demand for an unprecedented expansion of federal power." These four doctrines comprised the major ideologies presented to the American public on the matters of slavery, the territories and the U.S. Constitution prior to the 1860 presidential election.
National elections.
Beginning in the American Revolution and accelerating after the War of 1812, the people of the United States grew in their sense of country as an important example to the world of a national republic of political liberty and personal rights. Previous regional independence movements such as the Greek revolt in the Ottoman Empire, division and redivision in the Latin American political map, and the British-French Crimean triumph leading to an interest in redrawing Europe along cultural differences, all conspired to make for a time of upheaval and uncertainty about the basis of the nation-state. In the world of 19th century self-made Americans, growing in prosperity, population and expanding westward, "freedom" could mean personal liberty or property rights. The unresolved difference would cause failure—first in their political institutions, then in their civil life together.
Nationalism and honor.
Nationalism was a powerful force in the early 19th century, with famous spokesmen such as Andrew Jackson and Daniel Webster. While practically all Northerners supported the Union, Southerners were split between those loyal to the entire United States (called "unionists") and those loyal primarily to the southern region and then the Confederacy. C. Vann Woodward said of the latter group,
Perceived insults to Southern collective honor included the enormous popularity of "Uncle Tom's Cabin" (1852) and the actions of abolitionist John Brown in trying to incite a slave rebellion in 1859.
While the South moved toward a Southern nationalism, leaders in the North were also becoming more nationally minded, and rejected any notion of splitting the Union. The Republican national electoral platform of 1860 warned that Republicans regarded disunion as treason and would not tolerate it: "We denounce those threats of disunion ... as denying the vital principles of a free government, and as an avowal of contemplated treason, which it is the imperative duty of an indignant people sternly to rebuke and forever silence." The South ignored the warnings: Southerners did not realize how ardently the North would fight to hold the Union together.
Lincoln's election.
The election of Abraham Lincoln in November 1860 was the final trigger for secession. Efforts at compromise, including the "Corwin Amendment" and the "Crittenden Compromise", failed.
Southern leaders feared that Lincoln would stop the expansion of slavery and put it on a course toward extinction. The slave states, which had already become a minority in the House of Representatives, were now facing a future as a perpetual minority in the Senate and Electoral College against an increasingly powerful North. Before Lincoln took office in March 1861, seven slave states had declared their secession and joined to form the Confederacy.
Outbreak of the war.
Secession crisis.
The election of Lincoln caused the legislature of South Carolina to call a state convention to consider secession. Prior to the war, South Carolina did more than any other Southern state to advance the notion that a state had the right to nullify federal laws and, even, secede from the United States. The convention summoned unanimously voted to secede on December 20, 1860 and adopted the "Declaration of the Immediate Causes Which Induce and Justify the Secession of South Carolina from the Federal Union". It argued for states' rights for slave owners in the South, but contained a complaint about states' rights in the North in the form of opposition to the Fugitive Slave Act, claiming that Northern states were not fulfilling their federal obligations under the Constitution. The "cotton states" of Mississippi, Florida, Alabama, Georgia, Louisiana, and Texas followed suit, seceding in January and February 1861.
Among the ordinances of secession passed by the individual states, those of three – Texas, Alabama, and Virginia – specifically mentioned the plight of the 'slaveholding states' at the hands of northern abolitionists. The rest make no mention of the slavery issue, and are often brief announcements of the dissolution of ties by the legislatures. However, at least four states – South Carolina, Mississippi, Georgia, and Texas – also passed lengthy and detailed explanations of their causes for secession, all of which laid the blame squarely on the movement to abolish slavery and that movement's influence over the politics of the northern states. The southern states believed slaveholding was a constitutional right because of the Fugitive slave clause of the Constitution.
These states agreed to form a new federal government, the Confederate States of America, on February 4, 1861. They took control of federal forts and other properties within their boundaries with little resistance from outgoing President James Buchanan, whose term ended on March 4, 1861. Buchanan said that the Dred Scott decision was proof that the South had no reason for secession, and that the Union "... was intended to be perpetual," but that, "The power by force of arms to compel a State to remain in the Union," was not among the "... enumerated powers granted to Congress." One quarter of the U.S. Army – the entire garrison in Texas – was surrendered in February 1861 to state forces by its commanding general, David E. Twiggs, who then joined the Confederacy.
As Southerners resigned their seats in the Senate and the House, Republicans were able to pass bills for projects that had been blocked by Southern Senators before the war, including the Morrill Tariff, land grant colleges (the Morill Act), a Homestead Act, a transcontinental railroad (the Pacific Railway Acts), the National Banking Act and the authorization of United States Notes by the Legal Tender Act of 1862. The Revenue Act of 1861 introduced the income tax to help finance the war.
On December 18, 1860, the Crittenden Compromise was proposed to re-establish the Missouri Compromise line by constitutionally banning slavery in territories to the north of the line while guaranteeing it to the south. The adoption of this compromise likely would have prevented the secession of every southern state apart from South Carolina, but Lincoln and the Republicans rejected it. It was then proposed to hold a national referendum on the compromise. The Republicans again rejected the idea, although a majority of both Northerners and Southerners would have voted in favor of it. A pre-war February Peace Conference of 1861 met in Washington, proposing a solution similar to that of the Crittenden compromise, it was rejected by Congress. The Republicans proposed an alternative compromise to not interfere with slavery where it existed but the South regarded it as insufficient. Nonetheless, the remaining eight slave states rejected pleas to join the Confederacy following a two-to-one no-vote in Virginia's First Secessionist Convention on April 4, 1861.
On March 4, 1861, Abraham Lincoln was sworn in as President. In his inaugural address, he argued that the Constitution was a "more perfect union" than the earlier Articles of Confederation and Perpetual Union, that it was a binding contract, and called any secession "legally void". He had no intent to invade Southern states, nor did he intend to end slavery where it existed, but said that he would use force to maintain possession of Federal property. The government would make no move to recover post offices, and if resisted, mail delivery would end at state lines. Where popular conditions did not allow peaceful enforcement of Federal law, U.S. Marshals and Judges would be withdrawn. No mention was made of bullion lost from U.S. mints in Louisiana, Georgia and North Carolina. In Lincoln's inaugural address, he stated that it would be U.S. policy to only collect import duties at its ports; there could be no serious injury to the South to justify armed revolution during his administration. His speech closed with a plea for restoration of the bonds of union, famously calling on "the mystic chords of memory" binding the two regions.
The South sent delegations to Washington and offered to pay for the federal properties and enter into a peace treaty with the United States. Lincoln rejected any negotiations with Confederate agents because he claimed the Confederacy was not a legitimate government, and that making any treaty with it would be tantamount to recognition of it as a sovereign government. Secretary of State William Seward who at that time saw himself as the real governor or "prime minister" behind the throne of the inexperienced Lincoln, engaged in unauthorized and indirect negotiations that failed. President Lincoln was determined to hold all remaining Union-occupied forts in the Confederacy, Fort Monroe in Virginia, in Florida, Fort Pickens, Fort Jefferson, and Fort Taylor, and in the cockpit of secession, Charleston, South Carolina's Fort Sumter.
Battle of Fort Sumter.
Fort Sumter was located in the middle of the harbor of Charleston, South Carolina, where the U.S. fort's garrison had withdrawn to avoid incidents with local militias in the streets of the city. Unlike Buchanan, who allowed commanders to relinquish possession to avoid bloodshed, Lincoln required Maj. Anderson to hold on until fired upon. Jefferson Davis ordered the surrender of the fort. Anderson gave a conditional reply that the Confederate government rejected, and Davis ordered P. G. T. Beauregard to attack the fort before a relief expedition could arrive. Troops under Beauregard bombarded Fort Sumter on April 12–13, forcing its capitulation.
The attack on Fort Sumter rallied the North to the defense of American nationalism. Historian Allan Nevins says:
However, much of the North's attitude was based on the false belief that only a minority of Southerners were actually in favor of secession and that there were large numbers of southern Unionists that could be counted on. Had Northerners realized that most Southerners really did favor secession, they might have hesitated at attempting the enormous task of conquering a united South.
Lincoln called on all the states to send forces to recapture the fort and other federal properties. He cited presidential powers given by the Militia Acts of 1792. With the scale of the rebellion apparently small so far, Lincoln called for only 75,000 volunteers for 90 days. The governor of Massachusetts had state regiments on trains headed south the next day. In western Missouri, local secessionists seized Liberty Arsenal. On May 3, 1861, Lincoln called for an additional 42,000 volunteers for a period of three years.
Four states in the middle and upper South had repeatedly rejected Confederate overtures, but now Virginia, Tennessee, Arkansas, and North Carolina refused to send forces against their neighbors, declared their secession, and joined the Confederacy. To reward Virginia, the Confederate capital was moved to Richmond. 
[[File:US Secession map 1863 (BlankMap derived).png|thumb|250px|right|US Secession map 1863. The Union vs. the Confederacy.
Attitude of the border states.
Maryland, Delaware, Missouri, and Kentucky were slave states that were opposed to both secession and coercing the South. They were later joined by West Virginia, which separated from Virginia and became a new state.
Maryland had numerous anti-Lincoln officials who tolerated anti-army rioting in Baltimore and the burning of bridges, both aimed at hindering the passage of troops to the South. Maryland's legislature voted to stay in the Union, but also rejected hostilities with the South, voting to close Maryland's rail lines to prevent them from being used for war. Lincoln responded by establishing martial law, and unilaterally suspending habeas corpus, in Maryland, along with sending in militia units from the North. Lincoln rapidly took control of Maryland and the District of Columbia, by seizing many prominent figures, including arresting 1/3 of the members of the Maryland General Assembly on the day it reconvened. All were held without trial, ignoring a ruling by the Chief Justice of the U.S. Supreme Court Roger Taney, a Maryland native, that only Congress (and not the president) could suspend habeas corpus (Ex parte Merryman). Indeed, federal troops imprisoned a prominent Baltimore newspaper editor, Frank Key Howard, Francis Scott Key's grandson, after he criticized Lincoln in an editorial for ignoring the Supreme Court Chief Justice's ruling.
In Missouri, an elected convention on secession voted decisively to remain within the Union. When pro-Confederate Governor Claiborne F. Jackson called out the state militia, it was attacked by federal forces under General Nathaniel Lyon, who chased the governor and the rest of the State Guard to the southwestern corner of the state. ("See also: Missouri secession"). In the resulting vacuum, the convention on secession reconvened and took power as the Unionist provisional government of Missouri.
Kentucky did not secede; for a time, it declared itself neutral. When Confederate forces entered the state in September 1861, neutrality ended and the state reaffirmed its Union status, while trying to maintain slavery. During a brief invasion by Confederate forces, Confederate sympathizers organized a secession convention, inaugurated a governor, and gained recognition from the Confederacy. The rebel government soon went into exile and never controlled Kentucky.
After Virginia's secession, a Unionist government in Wheeling asked 48 counties to vote on an ordinance to create a new state on October 24, 1861. A voter turnout of 34 percent approved the statehood bill (96 percent approving). The inclusion of 24 secessionist counties in the state and the ensuing guerrilla war engaged about 40,000 Federal troops for much of the war. Congress admitted West Virginia to the Union on June 20, 1863. West Virginia provided about 20,000–22,000 soldiers to both the Confederacy and the Union.
A Unionist secession attempt occurred in East Tennessee, but was suppressed by the Confederacy, which arrested over 3,000 men suspected of being loyal to the Union. They were held without trial.
War.
The Civil War was a contest marked by the ferocity and frequency of battle. Over four years, 237 named battles were fought, as were many more minor actions and skirmishes, which were often characterized by their bitter intensity and high casualties. "The American Civil War was to prove one of the most ferocious wars ever fought". Without geographic objectives, the only target for each side was the enemy's soldier.
Mobilization.
As the first seven states began organizing a Confederacy in Montgomery, the entire U.S. army numbered 16,000. However, Northern governors had begun to mobilize their militias. The Confederate Congress authorized the new nation up to 100,000 troops sent by governors as early as February. By May, Jefferson Davis was pushing for 100,000 men under arms for one year or the duration, and that was answered in kind by the U.S. Congress.
In the first year of the war, both sides had far more volunteers than they could effectively train and equip. After the initial enthusiasm faded, reliance on the cohort of young men who came of age every year and wanted to join was not enough. Both sides used a draft law—conscription—as a device to encourage or force volunteering; relatively few were actually drafted and served. The Confederacy passed a draft law in April 1862 for young men aged 18 to 35; overseers of slaves, government officials, and clergymen were exempt. The U.S. Congress followed in July, authorizing a militia draft within a state when it could not meet its quota with volunteers. European immigrants joined the Union Army in large numbers, including 177,000 born in Germany and 144,000 born in Ireland.
When the Emancipation Proclamation went into effect in January 1863, ex-slaves were energetically recruited by the states, and used to meet the state quotas. States and local communities offered higher and higher cash bonuses for white volunteers. Congress tightened the law in March 1863. Men selected in the draft could provide substitutes or, until mid-1864, pay commutation money. Many eligibles pooled their money to cover the cost of anyone drafted. Families used the substitute provision to select which man should go into the army and which should stay home. There was much evasion and overt resistance to the draft, especially in Catholic areas. The great draft riot in New York City in July 1863 involved Irish immigrants who had been signed up as citizens to swell the vote of the city's Democratic political machine, not realizing it made them liable for the draft. Of the 168,649 men procured for the Union through the draft, 117,986 were substitutes, leaving only 50,663 who had their personal services conscripted.
North and South, the draft laws were highly unpopular. In the North, some 120,000 men evaded conscription, many of them fleeing to Canada, and another 280,000 soldiers deserted during the war. At least 100,000 Southerners deserted, or about 10 percent. In the South, many men deserted temporarily to take care of their distressed families, then returned to their units. In the North, "bounty jumpers" enlisted to get the generous bonus, deserted, then went back to a second recruiting station under a different name to sign up again for a second bonus; 141 were caught and executed.
From a tiny frontier force in 1860, the Union and Confederate armies had grown into the "largest and most efficient armies in the world" within a few years. European observers at the time dismissed them as amateur and unprofessional, but British historian John Keegan's assessment is that each outmatched the French, Prussian and Russian armies of the time, and but for the Atlantic, would have threatened any of them with defeat.
Motivation.
Perman and Taylor (2010) say that historians are of two minds on why millions of men seemed so eager to fight, suffer and die over four years:
Prisoners.
At the start of the civil war, a system of paroles operated. Captives agreed not to fight until they were officially exchanged. Meanwhile, they were held in camps run by their own army where they were paid but not allowed to perform any military duties. The system of exchanges collapsed in 1863 when the Confederacy refused to exchange black prisoners. After that, about 56,000 of the 409,000 POWs died in prisons during the war, accounting for nearly 10 percent of the conflict's fatalities.
Naval war.
The small U.S. Navy of 1861 was rapidly enlarged to 6,000 officers and 45,000 men in 1865, with 671 vessels, having a tonnage of 510,396. Its mission was to blockade Confederate ports, take control of the river system, defend against Confederate raiders on the high seas, and be ready for a possible war with the British Royal Navy. Meanwhile, the main riverine war was fought in the West, where a series of major rivers gave access to the Confederate heartland, if the U.S. Navy could take control. In the East, the Navy supplied and moved army forces about, and occasionally shelled Confederate installations.
Union blockade.
By early 1861, General Winfield Scott had devised the Anaconda Plan to win the war with as little bloodshed as possible. Scott argued that a Union blockade of the main ports would weaken the Confederate economy. Lincoln adopted parts of the plan, but he overruled Scott's caution about 90-day volunteers. Public opinion, however, demanded an immediate attack by the army to capture Richmond.
In April 1861, Lincoln announced the Union blockade of all Southern ports; commercial ships could not get insurance and regular traffic ended. The South blundered in embargoing cotton exports in 1861 before the blockade was effective; by the time they realized the mistake, it was too late. "King Cotton" was dead, as the South could export less than 10 percent of its cotton. The blockade shut down the ten Confederate seaports with railheads that moved almost all the cotton, especially New Orleans, Mobile, and Charleston. By June 1861, warships were stationed off the principal Southern ports, and a year later nearly 300 ships were in service.
Modern navy evolves.
The Civil War occurred during the early stages of the industrial revolution and subsequently many naval innovations emerged during this time, most notably the advent of the ironclad warship. It began when the Confederacy, knowing they had to meet or match the Union's naval superiority, responded to the Union blockade by building or converting more than 130 vessels, including twenty-six ironclads and floating batteries. Only half of these saw active service. Many were equipped with ram bows, creating "ram fever" among Union squadrons wherever they threatened. But in the face of overwhelming Union superiority and the Union's own ironclad warships, they were unsuccessful.
The Confederacy experimented with a submarine, which did not work well, and with building an ironclad ship, the CSS "Virginia", which was based on rebuilding a sunken Union ship, the "Merrimack". On its first foray on March 8, 1862, the "Virginia" decimated the Union's wooden fleet, but the next day the first Union ironclad, the USS "Monitor", arrived to challenge it. The Battle of the Ironclads was a draw, but it marks the worldwide transition to ironclad warships.
The Confederacy lost the "Virginia" when the ship was scuttled to prevent capture, and the Union built many copies of the "Monitor". Lacking the technology to build effective warships, the Confederacy attempted to obtain warships from Britain.
Blockade runners.
British investors built small, fast, steam-driven blockade runners that traded arms and luxuries brought in from Britain through Bermuda, Cuba, and the Bahamas in return for high-priced cotton. The ships were so small that only a small amount of cotton went out. When the Union Navy seized a blockade runner, the ship and cargo were condemned as a Prize of war and sold, with the proceeds given to the Navy sailors; the captured crewmen were mostly British and they were simply released. The Southern economy nearly collapsed during the war. There were multiple reasons for this: the severe deterioration of food supplies, especially in cities, the failure of Southern railroads, the loss of control of the main rivers, foraging by Northern armies, and the seizure of animals and crops by Confederate armies. Most historians agree that the blockade was a major factor in ruining the Confederate economy, however, Wise argues that the blockade runners provided just enough of a lifeline to allow Lee to continue fighting for additional months, thanks to fresh supplies of 400,000 rifles, lead, blankets, and boots that the homefront economy could no longer supply.
Economic impact.
Surdam argues that the blockade was a powerful weapon that eventually ruined the Southern economy, at the cost of few lives in combat. Practically, the entire Confederate cotton crop was useless (although it was sold to Union traders), costing the Confederacy its main source of income. Critical imports were scarce and the coastal trade was largely ended as well. The measure of the blockade's success was not the few ships that slipped through, but the thousands that never tried it. Merchant ships owned in Europe could not get insurance and were too slow to evade the blockade; they simply stopped calling at Confederate ports.
To fight an offensive war, the Confederacy purchased ships from Britain, converted them to warships, and raided American merchant ships in the Atlantic and Pacific oceans. Insurance rates skyrocketed and the American flag virtually disappeared from international waters. However, the same ships were reflagged with European flags and continued unmolested. After the war, the U.S. demanded that Britain pay for the damage done, and Britain paid the U.S. $15 million in 1871.
Rivers.
The 1862 Union strategy called for simultaneous advances along four axis. McClellan would lead the main thrust in Virginia towards Richmond. Ohio forces were to advance through Kentucky into Tennessee, the Missouri Department would drive south along the Mississippi River, and the westernmost attack would originate from Kansas.
Ulysses Grant used river transport and Andrew Foote's gunboats of the Western Flotilla to threaten the Confederacy's "Gibraltar of the West" at Columbus, Kentucky. Grant was rebuffed at Belmont, but cut off Columbus. The Confederates, lacking their own gunboats, were forced to retreat and the Union took control of western Kentucky in March 1862.
In addition to ocean-going warships coming up the Mississippi, the Union Navy used timberclads, tinclads, and armored gunboats. Shipyards at Cairo, Illinois, and St. Louis built new boats or modified steamboats for action. They took control of the Red, Tennessee, Cumberland, Mississippi, and Ohio rivers after victories at Fort Henry and Fort Donelson, and supplied Grant's forces as he moved into Tennessee. At Shiloh (Pittsburg Landing), in Tennessee in April 1862, the Confederates made a surprise attack that pushed Union forces against the river as night fell. Overnight, the Navy landed additional reinforcements, and Grant counter-attacked. Grant and the Union won a decisive victory – the first battle with the high casualty rates that would repeat over and over. Memphis fell to Union forces and became a key base for further advances south along the Mississippi River. In April 1862, U.S. Naval forces under Farragut ran past Confederate defenses south of New Orleans. Confederates abandoned the city, which gave the Union a critical anchor in the deep South.
Naval forces assisted Grant in his long, complex campaign that resulted in the surrender of Vicksburg in July 1863, and full Union control of the Mississippi soon after.
Eastern theater.
Because of the fierce resistance of a few initial Confederate forces at Manassas, Virginia, in July 1861, a march by Union troops under the command of Maj. Gen. Irvin McDowell on the Confederate forces there was halted in the First Battle of Bull Run, or "First Manassas". McDowell's troops were forced back to Washington, D.C., by the Confederates under the command of Generals Joseph E. Johnston and P. G. T. Beauregard. It was in this battle that Confederate General Thomas Jackson received the nickname of "Stonewall" because he stood like a stone wall against Union troops.
Alarmed at the loss, and in an attempt to prevent more slave states from leaving the Union, the U.S. Congress passed the Crittenden–Johnson Resolution on July 25 of that year, which stated that the war was being fought to preserve the Union and not to end slavery.
Maj. Gen. George B. McClellan took command of the Union Army of the Potomac on July 26 (he was briefly general-in-chief of all the Union armies, but was subsequently relieved of that post in favor of Maj. Gen. Henry W. Halleck), and the war began in earnest in 1862. Upon the strong urging of President Lincoln to begin offensive operations, McClellan attacked Virginia in the spring of 1862 by way of the peninsula between the York River and James River, southeast of Richmond. Although McClellan's army reached the gates of Richmond in the Peninsula Campaign, Johnston halted his advance at the Battle of Seven Pines, then General Robert E. Lee and top subordinates James Longstreet and Stonewall Jackson defeated McClellan in the Seven Days Battles and forced his retreat. The Northern Virginia Campaign, which included the Second Battle of Bull Run, ended in yet another victory for the South. McClellan resisted General-in-Chief Halleck's orders to send reinforcements to John Pope's Union Army of Virginia, which made it easier for Lee's Confederates to defeat twice the number of combined enemy troops.
Emboldened by Second Bull Run, the Confederacy made its first invasion of the North. General Lee led 45,000 men of the Army of Northern Virginia across the Potomac River into Maryland on September 5. Lincoln then restored Pope's troops to McClellan. McClellan and Lee fought at the Battle of Antietam near Sharpsburg, Maryland, on September 17, 1862, the bloodiest single day in United States military history. Lee's army, checked at last, returned to Virginia before McClellan could destroy it. Antietam is considered a Union victory because it halted Lee's invasion of the North and provided an opportunity for Lincoln to announce his Emancipation Proclamation.
When the cautious McClellan failed to follow up on Antietam, he was replaced by Maj. Gen. Ambrose Burnside. Burnside was soon defeated at the Battle of Fredericksburg on December 13, 1862, when more than 12,000 Union soldiers were killed or wounded during repeated futile frontal assaults against Marye's Heights. After the battle, Burnside was replaced by Maj. Gen. Joseph Hooker.
Hooker, too, proved unable to defeat Lee's army; despite outnumbering the Confederates by more than two to one, he was humiliated in the Battle of Chancellorsville in May 1863. Gen. Stonewall Jackson was mortally wounded by his own men during the battle and subsequently died of complications. Gen. Hooker was replaced by Maj. Gen. George Meade during Lee's second invasion of the North, in June. Meade defeated Lee at the Battle of Gettysburg (July 1 to 3, 1863). This was the bloodiest battle of the war, and has been called the war's turning point. Pickett's Charge on July 3 is often considered the high-water mark of the Confederacy because it signaled the collapse of serious Confederate threats of victory. Lee's army suffered 28,000 casualties (versus Meade's 23,000). However, Lincoln was angry that Meade failed to intercept Lee's retreat, and after Meade's inconclusive fall campaign, Lincoln turned to the Western Theater for new leadership. At the same time, the Confederate stronghold of Vicksburg surrendered, giving the Union control of the Mississippi River, permanently isolating the western Confederacy, and producing the new leader Lincoln needed, Ulysses S. Grant.
Western theater.
While the Confederate forces had numerous successes in the Eastern Theater, they were defeated many times in the West. They were driven from Missouri early in the war as a result of the Battle of Pea Ridge. Leonidas Polk's invasion of Columbus, Kentucky ended Kentucky's policy of neutrality and turned that state against the Confederacy. Nashville and central Tennessee fell to the Union early in 1862, leading to attrition of local food supplies and livestock and a breakdown in social organization.
The Mississippi was opened to Union traffic to the southern border of Tennessee with the taking of Island No. 10 and New Madrid, Missouri, and then Memphis, Tennessee. In April 1862, the Union Navy captured New Orleans, which allowed Union forces to begin moving up the Mississippi. Only the fortress city of Vicksburg, Mississippi, prevented Union control of the entire river.
General Braxton Bragg's second Confederate invasion of Kentucky ended with a meaningless victory over Maj. Gen. Don Carlos Buell at the Battle of Perryville, although Bragg was forced to end his attempt at invading Kentucky and retreat due to lack of support for the Confederacy in that state. Bragg was narrowly defeated by Maj. Gen. William Rosecrans at the Battle of Stones River in Tennessee.
The one clear Confederate victory in the West was the Battle of Chickamauga. Bragg, reinforced by Lt. Gen. James Longstreet's corps (from Lee's army in the east), defeated Rosecrans, despite the heroic defensive stand of Maj. Gen. George Henry Thomas. Rosecrans retreated to Chattanooga, which Bragg then besieged.
The Union's key strategist and tactician in the West was Ulysses S. Grant, who won victories at Forts Henry and Donelson (by which the Union seized control of the Tennessee and Cumberland Rivers); the Battle of Shiloh; and the Battle of Vicksburg, which cemented Union control of the Mississippi River and is considered one of the turning points of the war. Grant marched to the relief of Rosecrans and defeated Bragg at the Third Battle of Chattanooga, driving Confederate forces out of Tennessee and opening a route to Atlanta and the heart of the Confederacy.
Trans-Mississippi.
Extensive guerrilla warfare characterized the trans-Mississippi region, as the Confederacy lacked the troops and the logistics to support regular armies that could challenge Union control. Roving Confederate bands such as Quantrill's Raiders terrorized the countryside, striking both military installations and civilian settlements. The "Sons of Liberty" and "Order of the American Knights" attacked pro-Union people, elected officeholders, and unarmed uniformed soldiers. These partisans could not be entirely driven out of the state of Missouri until an entire regular Union infantry division was engaged.
By 1864, these violent activities harmed the nationwide anti-war movement organizing against the re-election of Lincoln. Missouri not only stayed in the Union, Lincoln took 70 percent of the vote for re-election.
Numerous small-scale military actions south and west of Missouri sought to control Indian Territory and New Mexico Territory for the Union. The Union repulsed Confederate incursions into New Mexico in 1862, and the exiled Arizona government withdrew into Texas. In the Indian Territory, civil war broke out within tribes. About 12,000 Indian warriors fought for the Confederacy, and smaller numbers for the Union. The most prominent Cherokee was Brigadier General Stand Watie, the last Confederate general to surrender.
After the fall of Vicksburg in July 1863, General Kirby Smith in Texas was informed by Jefferson Davis that he could expect no further help from east of the Mississippi River. Although he lacked resources to beat Union armies, he built up a formidable arsenal at Tyler, along with his own Kirby Smithdom economy, a virtual "independent fiefdom" in Texas, including railroad construction and international smuggling. The Union in turn did not directly engage him. Its 1864 Red River Campaign to take Shreveport, Louisiana was a failure and Texas remained in Confederate hands throughout the war.
End of war.
Conquest of Virginia.
At the beginning of 1864, Lincoln made Grant commander of all Union armies. Grant made his headquarters with the Army of the Potomac, and put Maj. Gen. William Tecumseh Sherman in command of most of the western armies. Grant understood the concept of total war and believed, along with Lincoln and Sherman, that only the utter defeat of Confederate forces and their economic base would end the war. This was total war not in killing civilians but rather in taking provisions and forage and destroying homes, farms, and railroads, that Grant said "would otherwise have gone to the support of secession and rebellion. This policy I believe exercised a material influence in hastening the end." Grant devised a coordinated strategy that would strike at the entire Confederacy from multiple directions. Generals George Meade and Benjamin Butler were ordered to move against Lee near Richmond, General Franz Sigel (and later Philip Sheridan) were to attack the Shenandoah Valley, General Sherman was to capture Atlanta and march to the sea (the Atlantic Ocean), Generals George Crook and William W. Averell were to operate against railroad supply lines in West Virginia, and Maj. Gen. Nathaniel P. Banks was to capture Mobile, Alabama.
Grant's army set out on the Overland Campaign with the goal of drawing Lee into a defense of Richmond, where they would attempt to pin down and destroy the Confederate army. The Union army first attempted to maneuver past Lee and fought several battles, notably at the Wilderness, Spotsylvania, and Cold Harbor. These battles resulted in heavy losses on both sides, and forced Lee's Confederates to fall back repeatedly. An attempt to outflank Lee from the south failed under Butler, who was trapped inside the Bermuda Hundred river bend. Each battle resulted in setbacks for the Union that mirrored what they had suffered under prior generals, though unlike those prior generals, Grant fought on rather than retreat. Grant was tenacious and kept pressing Lee's Army of Northern Virginia back to Richmond. While Lee was preparing for an attack on Richmond, Grant unexpectedly turned south to cross the James River and began the protracted Siege of Petersburg, where the two armies engaged in trench warfare for over nine months.
Grant finally found a commander, General Philip Sheridan, aggressive enough to prevail in the Valley Campaigns of 1864. Sheridan was initially repelled at the Battle of New Market by former U.S. Vice President and Confederate Gen. John C. Breckinridge. The Battle of New Market was the Confederacy's last major victory of the war. After redoubling his efforts, Sheridan defeated Maj. Gen. Jubal A. Early in a series of battles, including a final decisive defeat at the Battle of Cedar Creek. Sheridan then proceeded to destroy the agricultural base of the Shenandoah Valley, a strategy similar to the tactics Sherman later employed in Georgia.
Meanwhile, Sherman maneuvered from Chattanooga to Atlanta, defeating Confederate Generals Joseph E. Johnston and John Bell Hood along the way. The fall of Atlanta on September 2, 1864, guaranteed the reelection of Lincoln as president. Hood left the Atlanta area to swing around and menace Sherman's supply lines and invade Tennessee in the Franklin-Nashville Campaign. Union Maj. Gen. John Schofield defeated Hood at the Battle of Franklin, and George H. Thomas dealt Hood a massive defeat at the Battle of Nashville, effectively destroying Hood's army.
Leaving Atlanta, and his base of supplies, Sherman's army marched with an unknown destination, laying waste to about 20 percent of the farms in Georgia in his "March to the Sea". He reached the Atlantic Ocean at Savannah, Georgia in December 1864. Sherman's army was followed by thousands of freed slaves; there were no major battles along the March. Sherman turned north through South Carolina and North Carolina to approach the Confederate Virginia lines from the south, increasing the pressure on Lee's army.
Lee's army, thinned by desertion and casualties, was now much smaller than Grant's. One last Confederate attempt to break the Union hold on Petersburg failed at the decisive Battle of Five Forks (sometimes called "the Waterloo of the Confederacy") on April 1. This meant that the Union now controlled the entire perimeter surrounding Richmond-Petersburg, completely cutting it off from the Confederacy. Realizing that the capital was now lost, Lee decided to evacuate his army. The Confederate capital fell to the Union XXV Corps, composed of black troops. The remaining Confederate units fled west after a defeat at Sayler's Creek.
Confederacy surrenders.
Initially, Lee did not intend to surrender, but planned to regroup at the village of Appomattox Court House, where supplies were to be waiting, and then continue the war. Grant chased Lee and got in front of him, so that when Lee's army reached Appomattox Court House, they were surrounded. After an initial battle, Lee decided that the fight was now hopeless, and surrendered his Army of Northern Virginia on April 9, 1865, at the McLean House. In an untraditional gesture and as a sign of Grant's respect and anticipation of peacefully restoring Confederate states to the Union, Lee was permitted to keep his sword and his horse, Traveller. On April 14, 1865, President Lincoln was shot by John Wilkes Booth, a Southern sympathizer. Lincoln died early the next morning, and Andrew Johnson became the president. Meanwhile, Confederate forces across the South surrendered as news of Lee's surrender reached them. On April 26, 1865, General Joseph E. Johnston surrendered nearly 90,000 men of the Army of Tennessee to Major General William T. Sherman at the Bennett Place near present-day Durham, North Carolina. It proved to be the largest surrender of Confederate forces, effectively bringing the war to an end. President Johnson officially declared a virtual end to the insurrection on May 9, 1865; President Jefferson Davis was captured the following day. On June 2, Kirby Smith officially surrendered his troops in the Trans-Mississippi Department. On June 23, Cherokee leader Stand Watie became the last Confederate General to surrender his forces.
Diplomacy.
Though the Confederacy hoped that Britain and France would join them against the Union, this was never likely, and so they instead tried to bring Britain and France in as mediators. The Union, under Lincoln and Secretary of State William H. Seward worked to block this, and threatened war if any country officially recognized the existence of the Confederate States of America. In 1861, Southerners voluntarily embargoed cotton shipments, hoping to start an economic depression in Europe that would force Britain to enter the war to get cotton, but this did not work. Worse, Europe developed other cotton suppliers, which they found superior, hindering the South's recovery after the war.
Cotton diplomacy proved a failure as Europe had a surplus of cotton, while the 1860–62 crop failures in Europe made the North's grain exports of critical importance. It also helped to turn European opinion further away from the Confederacy. It was said that "King Corn was more powerful than King Cotton", as U.S. grain went from a quarter of the British import trade to almost half. When Britain did face a cotton shortage, it was temporary, being replaced by increased cultivation in Egypt and India. Meanwhile, the war created employment for arms makers, ironworkers, and British ships to transport weapons.
Lincoln's foreign policy was deficient in 1861 in terms of appealing to European public opinion. Diplomats had to explain that United States was not committed to the ending of slavery, but instead they repeated legalistic arguments about the unconstitutionality of secession. Confederate spokesman, on the other hand, were much more successful by ignoring slavery and instead focusing on their struggle for liberty, their commitment to free trade, and the essential role of cotton in the European economy. In addition, the European aristocracy (the dominant factor in every major country) was "absolutely gleeful in pronouncing the American debacle as proof that the entire experiment in popular government had failed. European government leaders welcomed the fragmentation of the ascendant American Republic."
U.S. minister to Britain Charles Francis Adams proved particularly adept and convinced Britain not to boldly challenge the blockade. The Confederacy purchased several warships from commercial shipbuilders in Britain (CSS "Alabama", CSS "Shenandoah", CSS "Tennessee", CSS "Tallahassee", CSS "Florida", and some others). The most famous, the CSS "Alabama", did considerable damage and led to serious postwar disputes. However, public opinion against slavery created a political liability for European politicians, especially in Britain (which, through the Slavery Abolition Act of 1833, had begun to abolish slavery in most of her colonies in 1834).
War loomed in late 1861 between the U.S. and Britain over the "Trent" affair, involving the U.S. Navy's boarding of the British mail steamer "Trent" to seize two Confederate diplomats. However, London and Washington were able to smooth over the problem after Lincoln released the two. In 1862, the British considered mediation – though even such an offer would have risked war with the U.S. British Prime Minister Lord Palmerston reportedly read "Uncle Tom's Cabin" three times when deciding on this.
The Union victory in the Battle of Antietam caused them to delay this decision. The Emancipation Proclamation over time would reinforce the political liability of supporting the Confederacy. Despite sympathy for the Confederacy, France's own seizure of Mexico ultimately deterred them from war with the Union. Confederate offers late in the war to end slavery in return for diplomatic recognition were not seriously considered by London or Paris. After 1863, the Polish revolt against Russia further distracted the European powers, and ensured that they would remain neutral.
Union victory and aftermath.
Results.
The causes of the war, the reasons for its outcome, and even the name of the war itself are subjects of lingering contention today. The North and West grew rich while the once-rich South became poor for a century. The national political power of the slaveowners and rich southerners ended. Historians are less sure about the results of the postwar Reconstruction, especially regarding the second class citizenship of the Freedmen and their poverty.
Historians have debated whether the Confederacy could have won the war. Most scholars, such as James McPherson, argue that Confederate victory was at least possible. McPherson argues that the North's advantage in population and resources made Northern victory likely but not guaranteed. He also argues that if the Confederacy had fought using unconventional tactics, they would have more easily been able to hold out long enough to exhaust the Union.
Confederates did not need to invade and hold enemy territory to win, but only needed to fight a defensive war to convince the North that the cost of winning was too high. The North needed to conquer and hold vast stretches of enemy territory and defeat Confederate armies to win. Lincoln was not a military dictator, and could only continue to fight the war as long as the American public supported a continuation of the war. The Confederacy sought to win independence by out-lasting Lincoln; however, after Atlanta fell and Lincoln defeated McClellan in the election of 1864, all hope for a political victory for the South ended. At that point, Lincoln had secured the support of the Republicans, War Democrats, the border states, emancipated slaves, and the neutrality of Britain and France. By defeating the Democrats and McClellan, he also defeated the Copperheads and their peace platform.
Many scholars argue that the Union held an insurmountable long-term advantage over the Confederacy in industrial strength and population. Confederate actions, they argue, only delayed defeat. Civil War historian Shelby Foote expressed this view succinctly: "I think that the North fought that war with one hand behind its back ... If there had been more Southern victories, and a lot more, the North simply would have brought that other hand out from behind its back. I don't think the South ever had a chance to win that War."
A minority view among historians is that the Confederacy lost because, as E. Merton Coulter put it, "people did not will hard enough and long enough to win." Marxist historian Armstead Robinson agrees, pointing to a class conflict in the Confederates army between the slave owners and the larger number of non-owners. He argues that the non-owner soldiers grew embittered about fighting to preserve slavery, and fought less enthusiastically. He attributes the major Confederate defeats in 1863 at Vicksburg and Missionary Ridge to this class conflict. However, most historians reject the argument. James M. McPherson, after reading thousands of letters written by Confederate soldiers, found strong patriotism that continued to the end; they truly believed they were fighting for freedom and liberty. Even as the Confederacy was visibly collapsing in 1864–65, he says most Confederate soldiers were fighting hard. Historian Gary Gallagher cites General Sherman who in early 1864 commented, "The devils seem to have a determination that cannot but be admired." Despite their loss of slaves and wealth, with starvation looming, Sherman continued, "yet I see no sign of let up – some few deserters – plenty tired of war, but the masses determined to fight it out."
Also important were Lincoln's eloquence in rationalizing the national purpose and his skill in keeping the border states committed to the Union cause. The Emancipation Proclamation was an effective use of the President's war powers. The Confederate government failed in its attempt to get Europe involved in the war militarily, particularly Britain and France. Southern leaders needed to get European powers to help break up the blockade the Union had created around the Southern ports and cities. Lincoln's naval blockade was 95 percent effective at stopping trade goods; as a result, imports and exports to the South declined significantly. The abundance of European cotton and Britain's hostility to the institution of slavery, along with Lincoln's Atlantic and Gulf of Mexico naval blockades, severely decreased any chance that either Britain or France would enter the war.
Historian Don Doyle has argued that the Union victory had a major impact on the course of world history. The Union victory energized popular democratic forces. A Confederate victory, on the other hand, would have meant a new birth of slavery, not freedom. Historian Fergus Bordewich, following Doyle, argues that:
Costs.
The war produced at least 1,030,000 casualties (3 percent of the population), including about 620,000 soldier deaths—two-thirds by disease, and 50,000 civilians. Binghamton University historian J. David Hacker believes the number of soldier deaths was approximately 750,000, 20 percent higher than traditionally estimated, and possibly as high as 850,000. The war accounted for more American deaths than in all other U.S. wars combined.
Based on 1860 census figures, 8 percent of all white males aged 13 to 43 died in the war, including 6 percent in the North and 18 percent in the South. About 56,000 soldiers died in prison camps during the War. An estimated 60,000 men lost limbs in the war.
Union army dead, amounting to 15 percent of the over two million who served, was broken down as follows:
In addition there were 4,523 deaths in the Navy (2,112 in battle) and 460 in the Marines (148 in battle).
Black troops made up 10 percent of the Union death toll, they amounted to 15 percent of disease deaths but less than 3 percent of those killed in battle. Losses among African Americans were high, in the last year and a half and from all reported casualties, approximately 20 percent of all African Americans enrolled in the military lost their lives during the Civil War. Notably, their mortality rate was significantly higher than white soldiers:
While the figures of 360,000 army deaths for the Union and 260,000 for the Confederacy remained commonly cited, they are incomplete. In addition to many Confederate records being missing, partly as a result of Confederate widows not reporting deaths due to being ineligible for benefits, both armies only counted troops who died during their service, and not the tens of thousands who died of wounds or diseases after being discharged. This often happened only a few days or weeks later. Francis Amasa Walker, Superintendent of the 1870 Census, used census and Surgeon General data to estimate a minimum of 500,000 Union military deaths and 350,000 Confederate military deaths, for a total death toll of 850,000 soldiers. While Walker's estimates were originally dismissed because of the 1870 Census's undercounting, it was later found that the census was only off by 6.5%, and that the data Walker used would be roughly accurate.
Analyzing the number of dead by using census data to calculate the deviation of the death rate of men of fighting age from the norm suggests that at least 627,000 and at most 888,000, but most likely 761,000 soldiers, died in the war. This would break down to approximately 350,000 Confederate and 411,000 Union military deaths, going by the proportion of Union to Confederate battle losses.
Losses can be viewed as high considering that the defeat of Mexico in 1846–48 resulted in fewer than 2,000 soldiers killed in battle, and roughly 13,000 killed overall. One reason for the high number of battle deaths during the war was the use of Napoleonic tactics, such as charging. With the advent of more accurate rifled barrels, Minié balls and (near the end of the war for the Union army) repeating firearms such as the Spencer Repeating Rifle and the Henry Repeating Rifle, soldiers were mowed down when standing in lines in the open. This led to the adoption of trench warfare, a style of fighting that defined much of World War I.
The wealth amassed in slaves and slavery for the Confederacy's 3.5 million blacks effectively ended when Union armies arrived; they were nearly all freed by the Emancipation Proclamation. Slaves in the border states and those located in some former Confederate territory occupied before the Emancipation Proclamation were freed by state action or (on December 6, 1865) by the Thirteenth Amendment.
The war destroyed much of the wealth that had existed in the South. All accumulated investment Confederate bonds was forfeit; most banks and railroads were bankrupt. Income per person in the South dropped to less than 40 percent of that of the North, a condition that lasted until well into the 20th century. Southern influence in the U.S. federal government, previously considerable, was greatly diminished until the latter half of the 20th century. The full restoration of the Union was the work of a highly contentious postwar era known as Reconstruction.
Emancipation.
Slavery as a war issue.
While not all Southerners saw themselves as fighting to preserve slavery, most of the officers and over a third of the rank and file in Lee's army had close family ties to slavery. To Northerners, in contrast, the motivation was primarily to preserve the Union, not to abolish slavery. Abraham Lincoln consistently made preserving the Union the central goal of the war, though he increasingly saw slavery as a crucial issue and made ending it an additional goal. Lincoln's decision to issue the Emancipation Proclamation angered both Peace Democrats ("Copperheads") and War Democrats, but energized most Republicans. By warning that free blacks would flood the North, Democrats made gains in the 1862 elections, but they did not gain control of Congress. The Republicans' counterargument that slavery was the mainstay of the enemy steadily gained support, with the Democrats losing decisively in the 1863 elections in the northern state of Ohio when they tried to resurrect anti-black sentiment.
Emancipation Proclamation.
The Emancipation Proclamation enabled African-Americans, both free blacks and escaped slaves, to join the Union Army. About 190,000 volunteered, further enhancing the numerical advantage the Union armies enjoyed over the Confederates, who did not dare emulate the equivalent manpower source for fear of fundamentally undermining the legitimacy of slavery.
During the Civil War, sentiment concerning slaves, enslavement and emancipation in the United States was divided. In 1861, Lincoln worried that premature attempts at emancipation would mean the loss of the border states, and that "to lose Kentucky is nearly the same as to lose the whole game." Copperheads and some War Democrats opposed emancipation, although the latter eventually accepted it as part of total war needed to save the Union.
At first, Lincoln reversed attempts at emancipation by Secretary of War Simon Cameron and Generals John C. Frémont (in Missouri) and David Hunter (in South Carolina, Georgia and Florida) to keep the loyalty of the border states and the War Democrats. Lincoln warned the border states that a more radical type of emancipation would happen if his gradual plan based on compensated emancipation and voluntary colonization was rejected. But only the District of Columbia accepted Lincoln's gradual plan, which was enacted by Congress. When Lincoln told his cabinet about his proposed emancipation proclamation, Seward advised Lincoln to wait for a victory before issuing it, as to do otherwise would seem like "our last shriek on the retreat". Lincoln laid the groundwork for public support in an open letter published letter to abolitionist Horace Greeley's newspaper.
In September 1862, the Battle of Antietam provided this opportunity, and the subsequent War Governors' Conference added support for the proclamation. Lincoln issued his preliminary Emancipation Proclamation on September 22, 1862, and his final Emancipation Proclamation on January 1, 1863. In his letter to Albert G. Hodges, Lincoln explained his belief that "If slavery is not wrong, nothing is wrong ... And yet I have never understood that the Presidency conferred upon me an unrestricted right to act officially upon this judgment and feeling ... I claim not to have controlled events, but confess plainly that events have controlled me."
Lincoln's moderate approach succeeded in inducing border states, War Democrats and emancipated slaves to fight for the Union. The Union-controlled border states (Kentucky, Missouri, Maryland, Delaware and West Virginia) and Union-controlled regions around New Orleans, Norfolk and elsewhere, were not covered by the Emancipation Proclamation. All abolished slavery on their own, except Kentucky and Delaware.
Since the Emancipation Proclamation was based on the President's war powers, it only included territory held by Confederates at the time. However, the Proclamation became a symbol of the Union's growing commitment to add emancipation to the Union's definition of liberty. The Emancipation Proclamation greatly reduced the Confederacy's hope of getting aid from Britain or France. By late 1864, Lincoln was playing a leading role in getting Congress to vote for the Thirteenth Amendment, which made emancipation universal and permanent.
Texas v. White.
In "Texas v. White", the United States Supreme Court ruled that Texas had remained a state ever since it first joined the Union, despite claims that it joined the Confederate States; the court further held that the Constitution did not permit states to unilaterally secede from the United States, and that the ordinances of secession, and all the acts of the legislatures within seceding states intended to give effect to such ordinances, were "absolutely null", under the constitution.
Reconstruction.
Reconstruction began during the war, with the Emancipation Proclamation of January 1, 1863 and continued until 1877. It comprised multiple complex methods to resolve the outstanding issues of the war's aftermath, the most important of which were the three "Reconstruction Amendments" to the Constitution, which remain in effect to the present time: the 13th (1865), the 14th (1868) and the 15th (1870). From the Union perspective, the goals of Reconstruction were to consolidate the Union victory on the battlefield by reuniting the Union; to guarantee a "republican form of government for the ex-Confederate states; and to permanently end slavery—and prevent semi-slavery status.
President Johnson took a lenient approach and saw the achievement of the main war goals as realized in 1865, when each ex-rebel state repudiated secession and ratified the Thirteenth Amendment. Radical Republicans demanded proof that Confederate nationalism was dead and that the slaves were truly free. They came to the fore after the 1866 elections and undid much of Johnson's work. In 1872 the "Liberal Republicans" argued that the war goals had been achieved and that Reconstruction should end. They ran a presidential ticket in 1872 but were decisively defeated. In 1874, Democrats, primarily Southern, took control of Congress and opposed any more reconstruction. The Compromise of 1877 closed with a national consensus that the Civil War had finally ended. With the withdrawal of federal troops, however, whites retook control of every Southern legislature; the Jim Crow period of disenfranchisement and legal segregation was about to begin.
Memory and historiography.
The Civil War is one of the central events in American collective memory. There are innumerable statues, commemorations, books and archival collections. The memory includes the home front, military affairs, the treatment of soldiers, both living and dead, in the war's aftermath, depictions of the war in literature and art, evaluations of heroes and villains, and considerations of the moral and political lessons of the war. The last theme includes moral evaluations of racism and slavery, heroism in combat and behind the lines, and the issues of democracy and minority rights, as well as the notion of an "Empire of Liberty" influencing the world.
Professional historians have paid much more attention to the causes of the war, than to the war itself. Military history has largely developed outside academe, leading to a proliferation of solid studies by non-scholars who are thoroughly familiar with the primary sources, pay close attention to battles and campaigns, and write for the large public readership, rather than the small scholarly community. Bruce Canton and Shelby Foote are among the best-known writers. Practically every major figure in the war, both North and South, has had a serious biographical study.
Deeply religious Southerners saw the hand of God in history, which demonstrated His wrath at their sinfulness, or His rewards for their suffering. Historian Wilson Fallin has examined the sermons of white and black Baptist preachers after the War. Southern white preachers said:
In sharp contrast, Black preachers interpreted the Civil War as:
Lost Cause.
Memory of the war in the white South crystallized in the myth of the "Lost Cause", shaping regional identity and race relations for generations. Alan T. Nolan notes that the Lost Cause was expressly "a rationalization, a cover-up to vindicate the name and fame" of those in rebellion. Some claims revolve around the insignificance of slavery; some appeals highlight cultural differences between North and South; the military conflict by Confederate actors is idealized; in any case, secession was said to be lawful. The two important political legacies that flowed from the adoption of the Lost Cause analysis were that it facilitated the reunification of the North and the South, and it excused the "virulent racism" of the 19th century, sacrificing African-American progress to a white man's reunification. But the Lost Cause legacy to history is "a caricature of the truth. This caricature wholly misrepresents and distorts the facts of the matter" in every instance.
Beardian historiography.
The interpretation of the Civil War presented by Charles A. Beard and Mary R. Beard in "The Rise of American Civilization" (1927) was highly influential among historians and the general public until the Civil Rights Movement of the 1950s and 1960s. The Beards downplayed slavery, abolitionism, and issues of morality. They ignored constitutional issues of states' rights and even ignored American nationalism as the force that finally led to victory in the war. Indeed, the ferocious combat itself was passed over as merely an ephemeral event. Much more important was the calculus of class conflict. The Beards announced that the Civil War was really:
The Beards themselves abandoned their interpretation by the 1940s and it became defunct among historians in the 1950s, when scholars shifted to an emphasis on slavery. However, Beardian themes still echo among Lost Cause writers.
Civil War commemoration.
The American Civil War has been commemorated in many capacities ranging from the reenactment of battles, to statues and memorial halls erected, to films being produced, to stamps and coins with Civil War themes being issued, all of which helped to shape public memory. This varied advent occurred in greater proportions on the 100th and 150th anniversary.
Hollywood's take on the war has been especially influential in shaping public memory, as seen in such film classics as "Birth of a Nation" (1915), "Gone with the Wind" (1939), and more recently "Lincoln" (2012). Ken Burns produced a notable PBS series on television titled "The Civil War" (1990). It was digitally remastered and re-released in 2015.
See also.
General reference
Union
Confederacy
Ethnic articles
Topical articles
National articles
References.
Notes
Citations

</doc>
<doc id="864" url="https://en.wikipedia.org/wiki?curid=864" title="Andy Warhol">
Andy Warhol

Andy Warhol (; born Andrew Warhola; August 6, 1928 – February 22, 1987) was an American artist who was a leading figure in the visual art movement known as pop art. His works explore the relationship between artistic expression, celebrity culture, and advertisement that flourished by the 1960s. After a successful career as a commercial illustrator, Warhol became a renowned and sometimes controversial artist. The Andy Warhol Museum in his native city, Pittsburgh, Pennsylvania, holds an extensive permanent collection of art and archives. It is the largest museum in the United States dedicated to a single artist.
Warhol's art used many types of media, including hand drawing, painting, printmaking, photography, silk screening, sculpture, film, and music. He was also a pioneer in computer-generated art using Amiga computers that were introduced in 1984, two years before his death. He founded "Interview" magazine and was the author of numerous books, including "The Philosophy of Andy Warhol" and "". He managed and produced The Velvet Underground, a rock band which had a strong influence on the evolution of punk rock music. He is also notable as a gay man who lived openly as such before the gay liberation movement. His studio, The Factory, was a well known gathering place that brought together distinguished intellectuals, drag queens, playwrights, Bohemian street people, Hollywood celebrities, and wealthy patrons.
Warhol has been the subject of numerous retrospective exhibitions, books, and feature and documentary films. He coined the widely used expression "15 minutes of fame". Many of his creations are very collectible and highly valuable. The highest price ever paid for a Warhol painting is US$105 million for a 1963 canvas titled "Silver Car Crash (Double Disaster)". A 2009 article in "The Economist" described Warhol as the "bellwether of the art market". Warhol's works include some of the most expensive paintings ever sold.
Early life and beginnings (1928–49).
Warhol was born on August 6, 1928 in Pittsburgh, Pennsylvania. He was the fourth child of Ondrej Warhola (Americanized as Andrew Warhola, Sr., 1889–1942) and Julia ("née" Zavacká, 1892–1972), whose first child was born in their homeland and died before their move to the U.S.
His parents were working-class Lemko emigrants from Mikó (now called Miková), located in today's northeastern Slovakia, part of the former Austro-Hungarian Empire. Warhol's father immigrated to the United States in 1914, and his mother joined him in 1921, after the death of Warhol's grandparents. Warhol's father worked in a coal mine. The family lived at 55 Beelen Street and later at 3252 Dawson Street in the Oakland neighborhood of Pittsburgh. The family was Byzantine Catholic and attended St. John Chrysostom Byzantine Catholic Church. Andy Warhol had two older brothers—Pavol (Paul), the oldest, was born before the family emigrated; Ján was born in Pittsburgh. Pavol's son, James Warhola, became a successful children's book illustrator.
In third grade, Warhol had Sydenham's chorea (also known as St. Vitus' Dance), the nervous system disease that causes involuntary movements of the extremities, which is believed to be a complication of scarlet fever which causes skin pigmentation blotchiness. He became a hypochondriac, developing a fear of hospitals and doctors. Often bedridden as a child, he became an outcast at school and bonded with his mother. At times when he was confined to bed, he drew, listened to the radio and collected pictures of movie stars around his bed. Warhol later described this period as very important in the development of his personality, skill-set and preferences. When Warhol was 13, his father died in an accident.
As a teenager, Warhol graduated from Schenley High School in 1945. After graduating from high school, his intentions were to study art education at the University of Pittsburgh in the hope of becoming an art teacher, but his plans changed and he enrolled in the Carnegie Institute of Technology in Pittsburgh, where he studied commercial art. During his time there, Warhol joined the campus Modern Dance Club and Beaux Arts Society. He also served as art director of the student art magazine, "Cano", illustrating a cover in 1948 and a full-page interior illustration in 1949. These are believed to be his first two published artworks. Warhol earned a Bachelor of Fine Arts in pictorial design in 1949. Later that year, he moved to New York City and began a career in magazine illustration and advertising.
Career.
1950s.
During the 1950s, Warhol gained fame for his whimsical ink drawings of shoe advertisements. These were done in a loose, blotted-ink style, and figured in some of his earliest showings at the Bodley Gallery in New York. With the concurrent rapid expansion of the record industry and the introduction of the vinyl record, Hi-Fi, and stereophonic recordings, RCA Records hired Warhol, along with another freelance artist, Sid Maurer, to design album covers and promotional materials.
Warhol was an early adopter of the silk screen printmaking process as a technique for making paintings. His earliest silkscreening in painting involved hand-drawn images though this soon progressed to the use of photographically derived silkscreening in paintings. Prior to entering the field of fine art, Warhol's commercial art background also involved innovative techniques for image making that were somewhat related to printmaking techniques. When rendering commercial objects for advertising Warhol devised a technique that resulted in a characteristic image. His imagery used in advertising was often executed by means of applying ink to paper and then blotting the ink while still wet. This was akin to a printmaking process on the most rudimentary scale.
Warhol's work both as a commercial artist and later a fine artist displays a casual approach to image making, in which chance plays a role and mistakes and unintentional marks are tolerated. The resulting imagery in both Warhol's commercial art and later in his fine art endeavors is often replete with imperfection—smudges and smears can often be found. In his book "POPism" Warhol writes, "When you do something exactly wrong, you always turn up something."
1960s.
He began exhibiting his work during the 1950s. He held exhibitions at the Hugo Gallery and the Bodley Gallery in New York City; in California, his first West Coast gallery exhibition was on July 9, 1962, in the Ferus Gallery of Los Angeles. The exhibition marked his West Coast debut of pop art.
Andy Warhol's first New York solo pop art exhibition was hosted at Eleanor Ward's Stable Gallery November 6–24, 1962. The exhibit included the works "Marilyn Diptych", "100 Soup Cans", "100 Coke Bottles," and "100 Dollar Bills". At the Stable Gallery exhibit, the artist met for the first time poet John Giorno who would star in Warhol's first film, "Sleep", in 1963.
It was during the 1960s that Warhol began to make paintings of iconic American objects such as dollar bills, mushroom clouds, electric chairs, Campbell's Soup Cans, Coca-Cola bottles, celebrities such as Marilyn Monroe, Elvis Presley, Marlon Brando, Troy Donahue, Muhammad Ali, and Elizabeth Taylor, as well as newspaper headlines or photographs of police dogs attacking civil rights protesters. During these years, he founded his studio, "The Factory" and gathered about him a wide range of artists, writers, musicians, and underground celebrities. His work became popular and controversial. Warhol had this to say about Coca-Cola:
New York City's Museum of Modern Art hosted a Symposium on pop art in December 1962 during which artists such as Warhol were attacked for "capitulating" to consumerism. Critics were scandalized by Warhol's open embrace of market culture. This symposium set the tone for Warhol's reception. Throughout the decade it became increasingly clear that there had been a profound change in the culture of the art world, and that Warhol was at the center of that shift.
A pivotal event was the 1964 exhibit "The American Supermarket", a show held in Paul Bianchini's Upper East Side gallery. The show was presented as a typical U.S. small supermarket environment, except that everything in it—from the produce, canned goods, meat, posters on the wall, etc.—was created by six prominent pop artists of the time, among them the controversial (and like-minded) Billy Apple, Mary Inman, and Robert Watts. Warhol's painting of a can of Campbell's soup cost $1,500 while each autographed can sold for $6. The exhibit was one of the first mass events that directly confronted the general public with both pop art and the perennial question of what art is.
As an advertisement illustrator in the 1950s, Warhol used assistants to increase his productivity. Collaboration would remain a defining (and controversial) aspect of his working methods throughout his career; this was particularly true in the 1960s. One of the most important collaborators during this period was Gerard Malanga. Malanga assisted the artist with the production of silkscreens, films, sculpture, and other works at "The Factory", Warhol's aluminum foil-and-silver-paint-lined studio on 47th Street (later moved to Broadway). Other members of Warhol's Factory crowd included Freddie Herko, Ondine, Ronald Tavel, Mary Woronov, Billy Name, and Brigid Berlin (from whom he apparently got the idea to tape-record his phone conversations).
During the 1960s, Warhol also groomed a retinue of bohemian and counterculture eccentrics upon whom he bestowed the designation "Superstars", including Nico, Joe Dallesandro, Edie Sedgwick, Viva, Ultra Violet, Holly Woodlawn, Jackie Curtis, and Candy Darling. These people all participated in the Factory films, and some—like Berlin—remained friends with Warhol until his death. Important figures in the New York underground art/cinema world, such as writer John Giorno and film-maker Jack Smith, also appear in Warhol films of the 1960s, revealing Warhol's connections to a diverse range of artistic scenes during this time. Less well known was his support and collaboration with several teen-agers during this era, who would achieve prominence later in life including writer David Dalton, photographer Stephen Shore and artist Bibbe Hansen (mother of pop musician Beck).
Attempted murder (1968).
On June 3, 1968, radical feminist writer Valerie Solanas shot Warhol and Mario Amaya, art critic and curator, at Warhol's studio. Before the shooting, Solanas had been a marginal figure in the Factory scene. She authored in 1967 the "S.C.U.M. Manifesto", a separatist feminist tract that advocated the elimination of men; and appeared in the 1968 Warhol film "I, a Man". Earlier on the day of the attack, Solanas had been turned away from the Factory after asking for the return of a script she had given to Warhol. The script had apparently been misplaced.
Amaya received only minor injuries and was released from the hospital later the same day. Warhol was seriously wounded by the attack and barely survived: surgeons opened his chest and massaged his heart to help stimulate its movement again. He suffered physical effects for the rest of his life, including being required to wear a surgical corset. The shooting had a profound effect on Warhol's life and art.
Solanas was arrested the day after the assault. By way of explanation, she said that Warhol "had too much control over my life." She was eventually sentenced to three years under the control of the Department of Corrections. After the shooting, the Factory scene heavily increased security, and for many the "Factory 60s" ended.
Warhol had this to say about the attack: "Before I was shot, I always thought that I was more half-there than all-there—I always suspected that I was watching TV instead of living life. People sometimes say that the way things happen in movies is unreal, but actually it's the way things happen in life that's unreal. The movies make emotions look so strong and real, whereas when things really do happen to you, it's like watching television—you don't feel anything. Right when I was being shot and ever since, I knew that I was watching television. The channels switch, but it's all television."
1970s.
Compared to the success and scandal of Warhol's work in the 1960s, the 1970s were a much quieter decade, as he became more entrepreneurial. According to Bob Colacello, Warhol devoted much of his time to rounding up new, rich patrons for portrait commissions—including Shah of Iran Mohammad Reza Pahlavi, his wife Empress Farah Pahlavi, his sister Princess Ashraf Pahlavi, Mick Jagger, Liza Minnelli, John Lennon, Diana Ross, and Brigitte Bardot. Warhol's famous portrait of Chinese Communist leader Mao Zedong was created in 1973. He also founded, with Gerard Malanga, "Interview" magazine, and published "The Philosophy of Andy Warhol" (1975). An idea expressed in the book: "Making money is art, and working is art and good business is the best art."
Warhol used to socialize at various nightspots in New York City, including Max's Kansas City; and, later in the 1970s, Studio 54. He was generally regarded as quiet, shy, and a meticulous observer. Art critic Robert Hughes called him "the white mole of Union Square."
With his longtime friend Stuart Pivar, Warhol founded the New York Academy of Art in 1979.
1980s.
Warhol had a re-emergence of critical and financial success in the 1980s, partially due to his affiliation and friendships with a number of prolific younger artists, who were dominating the "bull market" of 1980s New York art: Jean-Michel Basquiat, Julian Schnabel, David Salle and other so-called Neo-Expressionists, as well as members of the Transavantgarde movement in Europe, including Francesco Clemente and Enzo Cucchi.
By this period, Warhol was being criticized for becoming merely a "business artist". In 1979, reviewers disliked his exhibits of portraits of 1970s personalities and celebrities, calling them superficial, facile and commercial, with no depth or indication of the significance of the subjects. They also criticized his 1980 exhibit of 10 portraits at the Jewish Museum in Manhattan, entitled "Jewish Geniuses", which Warhol—who was uninterested in Judaism and Jews—had described in his diary as "They're going to sell." In hindsight, however, some critics have come to view Warhol's superficiality and commerciality as "the most brilliant mirror of our times," contending that "Warhol had captured something irresistible about the zeitgeist of American culture in the 1970s."
Warhol also had an appreciation for intense Hollywood glamour. He once said: "I love Los Angeles. I love Hollywood. They're so beautiful. Everything's plastic, but I love plastic. I want to be plastic."
Death.
Warhol died in Manhattan, at 6:32 am, on February 22, 1987. According to news reports, he had been making good recovery from a routine gallbladder surgery at New York Hospital before dying in his sleep from a sudden post-operative cardiac arrhythmia. Prior to his diagnosis and operation, Warhol delayed having his recurring gallbladder problems checked, as he was afraid to enter hospitals and see doctors. His family sued the hospital for inadequate care, saying that the arrhythmia was caused by improper care and water intoxication. The malpractice case was quickly settled out of court; Warhol's family received an undisclosed sum of money.
Warhol's body was taken back to Pittsburgh, by his brothers, for burial. The wake was at Thomas P. Kunsak Funeral Home and was an open-coffin ceremony. The coffin was a solid bronze casket with gold plated rails and white upholstery. Warhol was dressed in a black cashmere suit, a paisley tie, a platinum wig, and sunglasses. He was posed holding a small prayer book and a red rose. The funeral liturgy was held at the Holy Ghost Byzantine Catholic Church on Pittsburgh's North Side. The eulogy was given by Monsignor Peter Tay. Yoko Ono and John Richardson were speakers. The coffin was covered with white roses and asparagus ferns. After the liturgy, the coffin was driven to St. John the Baptist Byzantine Catholic Cemetery in Bethel Park, a south suburb of Pittsburgh.
At the grave, the priest said a brief prayer and sprinkled holy water on the casket. Before the coffin was lowered, Paige Powell dropped a copy of "Interview" magazine, an "Interview" T-shirt, and a bottle of the Estee Lauder perfume "Beautiful" into the grave. Warhol was buried next to his mother and father. A memorial service was held in Manhattan for Warhol on April 1, 1987, at St. Patrick's Cathedral, New York.
Foundation.
Warhol's will dictated that his entire estate — with the exception of a few modest legacies to family members — would go to create a foundation dedicated to the "advancement of the visual arts". Warhol had so many possessions that it took Sotheby's nine days to auction his estate after his death; the auction grossed more than US$20 million.
In 1987, in accordance with Warhol's will, the Andy Warhol Foundation for the Visual Arts began. The foundation serves as the estate of Andy Warhol, but also has a mission "to foster innovative artistic expression and the creative process" and is "focused primarily on supporting work of a challenging and often experimental nature."
The Artists Rights Society is the U.S. copyright representative for the Andy Warhol Foundation for the Visual Arts for all Warhol works with the exception of Warhol film stills. The U.S. copyright representative for Warhol film stills is the Warhol Museum in Pittsburgh. Additionally, the Andy Warhol Foundation for the Visual Arts has agreements in place for its image archive. All digital images of Warhol are exclusively managed by Corbis, while all transparency images of Warhol are managed by Art Resource.
The Andy Warhol Foundation released its "20th Anniversary Annual Report" as a three-volume set in 2007: Vol. I, 1987–2007; Vol. II, Grants & Exhibitions; and Vol. III, Legacy Program. The Foundation remains one of the largest grant-giving organizations for the visual arts in the U.S.
Works.
Paintings.
By the beginning of the 1960s, Warhol had become a very successful commercial illustrator. His detailed and elegant drawings for I. Miller shoes were particularly popular. They consisted mainly of "blotted ink" drawings (or monoprints), a technique which he applied in much of his early art. Although many artists of this period worked in commercial art, most did so discreetly. Warhol was so successful, however, that his profile as an illustrator seemed to undermine his efforts to be taken seriously as an artist.
Pop art was an experimental form that several artists were independently adopting; some of these pioneers, such as Roy Lichtenstein, would later become synonymous with the movement. Warhol, who would become famous as the "Pope of Pop", turned to this new style, where popular subjects could be part of the artist's palette. His early paintings show images taken from cartoons and advertisements, hand-painted with paint drips. Marilyn Monroe was a pop art painting that Warhol had done and it was very popular. Those drips emulated the style of successful abstract expressionists (such as Willem de Kooning). Warhol's first pop art paintings were displayed in April 1961, serving as the backdrop for New York Department Store Bronwit Teller's window display. This was the same stage his Pop Art contemporaries Jasper Johns, James Rosenquist and Robert Rauschenberg had also once graced. Eventually, Warhol pared his image vocabulary down to the icon itself—to brand names, celebrities, dollar signs—and removed all traces of the artist's "hand" in the production of his paintings.
To him, part of defining a niche was defining his subject matter. Cartoons were already being used by Lichtenstein, typography by Jasper Johns, and so on; Warhol wanted a distinguishing subject. His friends suggested he should paint the things he loved the most. It was the gallerist Muriel Latow who came up with the ideas for both the soup cans and Warhol's dollar paintings. On November 23, 1961 Warhol wrote Latow a check for $50 which, according to the 2009 Warhol biography, "Pop, The Genius of Warhol", was payment for coming up with the idea of the soup cans as subject matter.
For his first major exhibition Warhol painted his famous cans of Campbell's Soup, which he claimed to have had for lunch for most of his life. The work sold for $10,000 at an auction on November 17, 1971, at Sotheby's New York.
He loved celebrities, so he painted them as well. From these beginnings he developed his later style and subjects. Instead of working on a signature subject matter, as he started out to do, he worked more and more on a signature style, slowly eliminating the handmade from the artistic process. Warhol frequently used silk-screening; his later drawings were traced from slide projections. At the height of his fame as a painter, Warhol had several assistants who produced his silk-screen multiples, following his directions to make different versions and variations.
In 1979, Warhol was commissioned by BMW to paint a Group 4 race version of the then elite supercar BMW M1 for the fourth installment in the BMW Art Car Project. Unlike the three artists before him, Warhol declined the use of a small scale practice model, instead opting to immediately paint directly onto the full scale automobile. It was indicated that Warhol spent only a total of 23 minutes to paint the entire car.
Warhol produced both comic and serious works; his subject could be a soup can or an electric chair. Warhol used the same techniques—silkscreens, reproduced serially, and often painted with bright colors—whether he painted celebrities, everyday objects, or images of suicide, car crashes, and disasters, as in the 1962–63 "Death and Disaster" series. The "Death and Disaster" paintings included "Red Car Crash", "Purple Jumping Man", and "Orange Disaster." One of these paintings, the diptych "Silver Car Crash", became the highest priced work of his when it sold at Sotheby's Contemporary Art Auction on Wednesday, November 13, 2013, for $105.4 million.
Some of Warhol's work, as well as his own personality, has been described as being Keatonesque. Warhol has been described as playing dumb to the media. He sometimes refused to explain his work. He has suggested that all one needs to know about his work is "already there 'on the surface.'"
His Rorschach inkblots are intended as pop comments on art and what art could be. His cow wallpaper (literally, wallpaper with a cow motif) and his oxidation paintings (canvases prepared with copper paint that was then oxidized with urine) are also noteworthy in this context. Equally noteworthy is the way these works—and their means of production—mirrored the atmosphere at Andy's New York "Factory". Biographer Bob Colacello provides some details on Andy's "piss paintings":
Warhol's first portrait of "Basquiat" (1982) is a black photosilkscreen over an oxidized copper "piss painting".
After many years of silkscreen, oxidation, photography, etc., Warhol returned to painting with a brush in hand in a series of more than 50 large collaborative works done with Jean-Michel Basquiat between 1984 and 1986. Despite negative criticism when these were first shown, Warhol called some of them "masterpieces," and they were influential for his later work.
The influence of the large collaborations with Basquiat can be seen in Warhol's "The Last Supper" cycle, his last and possibly his largest series.
Andy Warhol was commissioned in 1984 by the gallerist Alexander Iolas to produce work based on Leonardo da Vinci's "The Last Supper" for an exhibition at the old refectory of the Palazzo delle Stelline in Milan, opposite from the Santa Maria delle Grazie where Leonardo da Vinci's mural can be seen.
Warhol exceeded the demands of the commission and produced nearly 100 variations on the theme, mostly silkscreens and paintings, and among them a collaborative sculpture with Basquiat, the "Ten Punching Bags (Last Supper)".
The Milan exhibition that opened in January 1987 with a set of 22 silk-screens, was the last exhibition for both the artist and the gallerist.
The series of "The Last Supper" was seen by some as "arguably his greatest," but by others as "wishy-washy, religiose" and "spiritless." It is also the largest series of religious-themed works by any U.S. artist.
Warhol's ability to find the uncanny, silly, or seductive in any given object, whether said object is mundane or sensational, influenced many artists working through photo and media outlets, among a vast number of other mediums. Artist Maurizio Cattelan describes that it is difficult to separate daily encounters from the art of Andy Warhol: "That’s probably the greatest thing about Warhol: the way he penetrated and summarized our world, to the point that distinguishing between him and our everyday life is basically impossible, and in any case useless." Warhol was an inspiration towards Cattelan's magazine and photography compilations, such as "Permanent Food, Charley", and "Toilet" "Paper".
At the time of his death, Warhol was working on "Cars", a series of paintings for Mercedes-Benz.
A self-portrait by Andy Warhol (1963–64), which sold in New York at the May Post-War and Contemporary evening sale in Christie's, fetched $38.4 million.
On May 9, 2012, his classic painting "Double Elvis (Ferus Type)" sold at auction at Sotheby's in New York for US$33 million. With commission, the sale price totaled US$37,042,500, short of the $50 million that Sotheby's had predicted the painting might bring. The piece (silkscreen ink and spray paint on canvas) shows Elvis Presley in a gunslinger pose. It was first exhibited in 1963 at the Ferus Gallery in Los Angeles. Warhol made 22 versions of the "Double Elvis," nine of which are held in museums.
In November 2013, his "Silver Car Crash (Double Disaster)" diptych sold at Sotheby's Contemporary Art Auction for $105.4 million, a new record for the famed pop artist (pre-auction estimates at $80 million). Created in 1963, this work has only been seen in public once in the past 26 years. In November 2014, "Triple Elvis" sold for $81.9m (£51.9m) at auction in New York.
Films.
Warhol worked across a wide range of media—painting, photography, drawing, and sculpture. In addition, he was a highly prolific filmmaker. Between 1963 and 1968, he made more than 60 films, plus some 500 short black-and-white "screen test" portraits of Factory visitors. One of his most famous films, "Sleep", monitors poet John Giorno sleeping for six hours. The 35-minute film "Blow Job" is one continuous shot of the face of DeVeren Bookwalter supposedly receiving oral sex from filmmaker Willard Maas, although the camera never tilts down to see this. Another, "Empire" (1964), consists of eight hours of footage of the Empire State Building in New York City at dusk. The film "Eat" consists of a man eating a mushroom for 45 minutes. Warhol attended the 1962 premiere of the static composition by LaMonte Young called "Trio for Strings" and subsequently created his famous series of static films including "Kiss", "Eat", and "Sleep" (for which Young initially was commissioned to provide music). Uwe Husslein cites filmmaker Jonas Mekas, who accompanied Warhol to the Trio premiere, and who claims Warhol's static films were directly inspired by the performance.
"Batman Dracula" is a 1964 film that was produced and directed by Warhol, without the permission of DC Comics. It was screened only at his art exhibits. A fan of the "Batman" series, Warhol's movie was an "homage" to the series, and is considered the first appearance of a blatantly campy Batman. The film was until recently thought to have been lost, until scenes from the picture were shown at some length in the 2006 documentary "Jack Smith and the Destruction of Atlantis".
Warhol's 1965 film "Vinyl" is an adaptation of Anthony Burgess' popular dystopian novel "A Clockwork Orange". Others record improvised encounters between Factory regulars such as Brigid Berlin, Viva, Edie Sedgwick, Candy Darling, Holly Woodlawn, Ondine, Nico, and Jackie Curtis. Legendary underground artist Jack Smith appears in the film "Camp".
His most popular and critically successful film was "Chelsea Girls" (1966). The film was highly innovative in that it consisted of two 16 mm-films being projected simultaneously, with two different stories being shown in tandem. From the projection booth, the sound would be raised for one film to elucidate that "story" while it was lowered for the other. The multiplication of images evoked Warhol's seminal silk-screen works of the early 1960s.
Other important films include "Bike Boy", "My Hustler", "The Nude Restaurant", and "Lonesome Cowboys", a raunchy pseudo-western. These and other titles document gay underground and camp culture, and continue to feature prominently in scholarship about sexuality and art. "Blue Movie"—a film in which Warhol superstar Viva makes love and fools around in bed with a man for 33 minutes of the film's playing-time—was Warhol's last film as director. The film, a seminal film in the Golden Age of Porn, was at the time scandalous for its frank approach to a sexual encounter. "Blue Movie" was publicly screened in New York City in 2005 for the first time in more that 30 years.
After his June 3, 1968 shooting, a reclusive Warhol relinquished his personal involvement in filmmaking. His acolyte and assistant director, Paul Morrissey, took over the film-making chores for the Factory collective, steering Warhol-branded cinema towards more mainstream, narrative-based, B-movie exploitation fare with "Flesh", "Trash", and "Heat". All of these films, including the later "Andy Warhol's Dracula" and "Andy Warhol's Frankenstein", were far more mainstream than anything Warhol as a director had attempted. These latter "Warhol" films starred Joe Dallesandro—more of a Morrissey star than a true Warhol superstar.
In the early 1970s, most of the films directed by Warhol were pulled out of circulation by Warhol and the people around him who ran his business. After Warhol's death, the films were slowly restored by the Whitney Museum and are occasionally projected at museums and film festivals. Few of the Warhol-directed films are available on video or DVD.
Music.
In the mid-1960s, Warhol adopted the band the Velvet Underground, making them a crucial element of the Exploding Plastic Inevitable multimedia performance art show. Warhol, with Paul Morrissey, acted as the band's manager, introducing them to Nico (who would perform with the band at Warhol's request). In 1966 he "produced" their first album "The Velvet Underground & Nico", as well as providing its album art. His actual participation in the album's production amounted to simply paying for the studio time. After the band's first album, Warhol and band leader Lou Reed started to disagree more about the direction the band should take, and their artistic friendship ended. In 1989, after Warhol's death, Reed and John Cale re-united for the first time since 1972 to write, perform, record and release the concept album "Songs for Drella", a tribute to Warhol.
Warhol designed many album covers for various artists starting with the photographic cover of John Wallowitch's debut album, "This Is John Wallowitch!!!" (1964). He designed the cover art for The Rolling Stones' albums "Sticky Fingers" (1971) and "Love You Live" (1977), and the John Cale albums "The Academy in Peril" (1972) and "Honi Soit" in 1981. One of Warhol's last works was a portrait of Aretha Franklin for the cover of her 1986 gold album "Aretha", which was done in the style of the "Reigning Queens" series he had completed the year before.
Warhol strongly influenced the new wave/punk rock band Devo, as well as David Bowie. Bowie recorded a song called "Andy Warhol" for his 1971 album "Hunky Dory". Lou Reed wrote the song "Andy's Chest", about Valerie Solanas, the woman who shot Warhol, in 1968. He recorded it with the Velvet Underground, and this version was released on the "VU" album in 1985. Bowie would later play Warhol in the 1996 movie, "Basquiat". Bowie recalled how meeting Warhol in real life helped him in the role, and recounted his early meetings with him:
Books and print.
Beginning in the early 1950s, Warhol produced several unbound portfolios of his work.
The first of several bound self-published books by Warhol was "25 Cats Name Sam and One Blue Pussy", printed in 1954 by Seymour Berlin on Arches brand watermarked paper using his blotted line technique for the lithographs. The original edition was limited to 190 numbered, hand colored copies, using Dr. Martin's ink washes. Most of these were given by Warhol as gifts to clients and friends. Copy No. 4, inscribed "Jerry" on the front cover and given to Geraldine Stutz, was used for a facsimile printing in 1987, and the original was auctioned in May 2006 for US $35,000 by Doyle New York.
Other self-published books by Warhol include:
Warhol's book "A La Recherche du Shoe Perdu" (1955) marked his "transition from commercial to gallery artist". (The title is a play on words by Warhol on the title of French author Marcel Proust's "À la recherche du temps perdu".)
After gaining fame, Warhol "wrote" several books that were commercially published:
Warhol created the fashion magazine "Interview" that is still published today. The loopy title script on the cover is thought to be either his own handwriting or that of his mother, Julia Warhola, who would often do text work for his early commercial pieces.
Other media.
Although Andy Warhol is most known for his paintings and films, he authored works in many different media.
Producer and product.
Warhol had assistance in producing his paintings. This is also true of his film-making and commercial enterprises.
He founded the gossip magazine "Interview", a stage for celebrities he "endorsed" and a business staffed by his friends. He collaborated with others on all of his books (some of which were written with Pat Hackett.) He adopted the young painter Jean-Michel Basquiat, and the band The Velvet Underground, presenting them to the public as his latest interest, and collaborating with them. One might even say that he produced people (as in the Warholian "Superstar" and the Warholian portrait). He endorsed products, appeared in commercials, and made frequent celebrity guest appearances on television shows and in films (he appeared in everything from "Love Boat" to "Saturday Night Live" and the Richard Pryor movie "Dynamite Chicken").
In this respect Warhol was a fan of "Art Business" and "Business Art"—he, in fact, wrote about his interest in thinking about art as business in "The Philosophy of Andy Warhol from A to B and Back Again".
Personal life.
Sexuality.
Warhol was gay. Interviewed in 1980, he indicated that he was still a virgin—biographer Bob Colacello who was present at the interview felt it was probably true and that what little sex he had was probably "a mixture of voyeurism and masturbation—to use his [Andy's] word "abstract"". Warhol's assertion of virginity would seem to be contradicted by his hospital treatment in 1960 for condylomata, a sexually transmitted disease. It has also been contradicted by his lovers, including Warhol muse BillyBoy who has said they had sex to orgasm: "When he wasn't being Andy Warhol and when you were just alone with him he was an incredibly generous and very kind person. What seduced me was the Andy Warhol who I saw alone. In fact when I was with him in public he kind of got on my nerves….I'd say: 'You're just obnoxious, I can't bear you." Asked if Warhol was only a voyeur, Billy Name also denied it, saying: "He was the essence of sexuality. It permeated everything. Andy exuded it, along with his great artistic creativity….It brought a joy to the whole art world in New York." "But his personality was so vulnerable that it became a defense to put up the blank front." Warhol's lovers included John Giorno, Billy Name, Charles Lisanby, Jon Gould. His boyfriend of 12 years was Jed Johnson, whom he met in 1968, and who later achieved fame as an interior designer. 
The fact that Warhol's homosexuality influenced his work and shaped his relationship to the art world is a major subject of scholarship on the artist and is an issue that Warhol himself addressed in interviews, in conversation with his contemporaries, and in his publications ("e.g.", "Popism: The Warhol 1960s"). Throughout his career, Warhol produced erotic photography and drawings of male nudes. Many of his most famous works (portraits of Liza Minnelli, Judy Garland, and Elizabeth Taylor, and films such as "Blow Job", "My Hustler" and "Lonesome Cowboys") draw from gay underground culture and/or openly explore the complexity of sexuality and desire. As has been addressed by a range of scholars, many of his films premiered in gay porn theaters.
The first works that Warhol submitted to a fine art gallery, homoerotic drawings of male nudes, were rejected for being too openly gay. In "Popism", furthermore, the artist recalls a conversation with the film maker Emile de Antonio about the difficulty Warhol had being accepted socially by the then-more-famous (but closeted) gay artists Jasper Johns and Robert Rauschenberg. De Antonio explained that Warhol was "too swish and that upsets them." In response to this, Warhol writes, "There was nothing I could say to that. It was all too true. So I decided I just wasn't going to care, because those were all the things that I didn't want to change anyway, that I didn't think I 'should' want to change ... Other people could change their attitudes but not me". In exploring Warhol's biography, many turn to this period—the late 1950s and early 1960s—as a key moment in the development of his persona. Some have suggested that his frequent refusal to comment on his work, to speak about himself (confining himself in interviews to responses like "Um, no" and "Um, yes", and often allowing others to speak for him)—and even the evolution of his pop style—can be traced to the years when Warhol was first dismissed by the inner circles of the New York art world.
Religious beliefs.
Warhol was a practicing Ruthenian Catholic. He regularly volunteered at homeless shelters in New York City, particularly during the busier times of the year, and described himself as a religious person. Many of Warhol's later works depicted religious subjects, including two series, "Details of Renaissance Paintings" (1984) and "The Last Supper" (1986). In addition, a body of religious-themed works was found posthumously in his estate.
During his life, Warhol regularly attended Mass, and the priest at Warhol's church, Saint Vincent Ferrer, said that the artist went there almost daily, although he was not observed taking Communion or going to Confession and sat or knelt in the pews at the back. The priest thought he was afraid of being recognized; Warhol said he was self-conscious about being seen in a Roman Rite church crossing himself "in the Orthodox way" (right to left instead of the reverse).
His art is noticeably influenced by the eastern Christian tradition which was so evident in his places of worship.
Warhol's brother has described the artist as "really religious, but he didn't want people to know about that because was private". Despite the private nature of his faith, in Warhol's eulogy John Richardson depicted it as devout: "To my certain knowledge, he was responsible for at least one conversion. He took considerable pride in financing his nephew's studies for the priesthood".
Collections.
Warhol was an avid collector. His friends referred to his numerous collections, which filled not only his four-story townhouse, but also a nearby storage unit, as "Andy's Stuff." The true extent of his collections was not discovered until after his death, when the Andy Warhol Museum in Pittsburgh took in 641 boxes of his "Stuff."
Warhol's collections included airplane menus, unpaid invoices, pizza dough, pornographic pulp novels, newspapers, stamps, supermarket flyers, and cookie jars, among other eccentricities. It also included significant works of art, such as George Bellows's "Miss Bentham". One of his main collections was his wigs. Warhol owned mlore than 40 and felt very protective of his hairpieces, which were sewn by a New York wig-maker from hair imported from Italy. In 1985 a girl snatched Warhol's wig off his head. It was later discovered in Warhol's diary entry for that day that he wrote: "I don't know what held me back from pushing her over the balcony."
Another item found in Warhol's boxes at the museum in Pittsburgh was a mummified human foot from Ancient Egypt. The curator of anthropology at Carnegie Museum of Natural History felt that Warhol most likely found it at a flea market.
Media about Warhol.
Films.
Warhol appeared as himself in the film "Cocaine Cowboys" (1979).
After his death, Warhol was portrayed by Crispin Glover in Oliver Stone's film "The Doors" (1991), by David Bowie in Julian Schnabel's film "Basquiat" (1996), and by Jared Harris in Mary Harron's film "I Shot Andy Warhol" (1996).
Warhol appears as a character in Michael Daugherty's opera "Jackie O" (1997). Actor makes a brief cameo as Warhol in "" (1997).
Many films by avant-garde cineast Jonas Mekas have caught the moments of Warhol's life. Sean Gregory Sullivan depicted Warhol in the film "54" (1998). Guy Pearce portrayed Warhol in the film, "Factory Girl" (2007), about Edie Sedgwick's life. Actor Greg Travis portrays Warhol in a brief scene from the film "Watchmen" (2009).
In the film "Men in Black 3" (2012) Andy Warhol turns out to really be undercover MIB Agent W (played by Bill Hader). Warhol is throwing a party at The Factory in 1969, where he is looked up by MIB Agents K and J (J from the future). Agent W is desperate to end his undercover job ("I'm so out of ideas I'm painting soup cans and bananas, for Christ sakes!" and "You gotta fake my death, okay? I can't listen to sitar music anymore.")
Andy Warhol (portrayed by Tom Meeten) is one of main characters of the 2012 British television show "Noel Fielding's Luxury Comedy". The character is portrayed as having robot-like mannerisms.
Gus Van Sant was planning a version of Warhol's life with River Phoenix in the lead role just before Phoenix's death in 1993.
In the soon to be released 2016 feature, " The Billionaire Boys Club", Cary Elwes portrays Warhol in a film based on the true story about Ron Levin (portrayed by Kevin Spacey) a friend of Warhol's who was murdered in 1986.
Honors.
In 2002, the U.S. Postal Service issued an 18-cent stamp commemorating Warhol. Designed by Richard Sheaff of Scottsdale, Arizona, the stamp was unveiled at a ceremony at The Andy Warhol Museum and features Warhol's painting "Self-Portrait, 1964". In March 2011, a chrome statue of Andy Warhol and his Polaroid camera was revealed at Union Square in New York City.

</doc>
<doc id="868" url="https://en.wikipedia.org/wiki?curid=868" title="Alp Arslan">
Alp Arslan

Alp Arslan (Honorific in Turkish meaning "Heroic Lion"; in ; full Persianized name: "Diya ad-Dunya wa ad-Din Adud ad-Dawlah Abu Shuja Muhammad Alp Arslan ibn Dawud" ; 20 January 1029 – 15 December 1072), real name Muhammad bin Dawud Chaghri, was the second Sultan of the Seljuk Empire and great-grandson of Seljuk, the eponymous founder of the dynasty. As Sultan, Alp Arslan greatly expanded Seljuk territory and consolidated power, defeating rivals to his south and northwest. His victory over the Byzantines at Manzikert ushered in the Turkish settlement of Anatolia. For his military prowess and fighting skills he obtained the name "Alp Arslan", which means "Heroic Lion" in Turkish.
Early Career.
Alp Arslan accompanied his uncle, Tughril Bey on campaigns in the south against the Shia Fatimids while his father, Çağrı Bey remained in Khorasan. Upon Alp Arslan's return to Khorasan, he began his work in administration at his father's suggestion. While there, his father introduced him to Nizam al-Mulk, one of the most eminent statesmen in early Muslim history and Alp Arslan's future vizier.
After the death of his father, Alp Arslan succeeded him as governor of Khorasan in 1059. His uncle Tughril died in 1063 and was succeeded by Suleiman, Arslan's brother. Arslan and his uncle Kutalmish both contested this succession. ("see" Battle of Damghan (1063)) Arslan defeated Kutalmish for the throne and succeeded on 27 April 1064 as sultan of Great Seljuq, thus becoming sole monarch of Persia from the river Oxus to the Tigris.
In consolidating his empire and subduing contending factions, Arslan was ably assisted by Nizam al-Mulk, and the two are credited with helping to stabilize the empire after the death of Tughril. With peace and security established in his dominions, Arslan convoked an assembly of the states and in 1066, he declared his son Malik Shah I his heir and successor. With the hope of capturing Caesarea Mazaca, the capital of Cappadocia, he placed himself at the head of the Turkish cavalry, crossed the Euphrates, and entered and invaded the city. Along with Nizam al-Mulk, he then marched into Armenia and Georgia, which he conquered in 1064. After a siege of 25 days, the Seljuks captured Ani, the capital city of Armenia, and slaughtered its population. An account of the sack and massacres in Ani is given by the Arab historian Sibt ibn al-Jawzi, who quotes an eyewitness saying:
Byzantine struggle.
En route to fight the Fatimids in Syria in 1068, Alp Arslan invaded the Byzantine Empire. The Emperor Romanos IV Diogenes, assuming command in person, met the invaders in Cilicia. In three arduous campaigns, the Turks were defeated in detail and driven across the Euphrates in 1070. The first two campaigns were conducted by the emperor himself, while the third was directed by Manuel Comnenos, great-uncle of Emperor Manuel Comnenos. During this time, Arslan gained the allegiance of Rashid al-Dawla Mahmud, the Mirdasid emir of Aleppo.
In 1071 Romanos again took the field and advanced into Armenia with possibly 30,000 men, including a contingent of Cuman Turks as well as contingents of Franks and Normans, under Ursel de Baieul. Alp Arslan, who had moved his troops south to fight the Fatimids, quickly reversed to meet the Byzantines. At Manzikert, on the Murat River, north of Lake Van, the two forces waged the Battle of Manzikert. The Cuman mercenaries among the Byzantine forces immediately defected to the Turkish side. Seeing this, "the Western mercenaries rode off and took no part in the battle." To be exact, Romanos was betrayed by general Andronikos Doukas, son of the Caesar (Romanos's stepson), who pronounced him dead and rode off with a large part of the Byzantine forces at a critical moment. The Byzantines were totally routed.
Emperor Romanos IV was himself taken prisoner and conducted into the presence of Alp Arslan. After a ritual humiliation, Arslan treated him with generosity. After peace terms were agreed to, Arslan dismissed the Emperor, loaded with presents and respectfully attended by a military guard. The following conversation is said to have taken place after Romanos was brought as a prisoner before the Sultan:
Alp Arslan's victories changed the balance in near Asia completely in favour of the Seljuq Turks and Sunni Muslims. While the Byzantine Empire was to continue for nearly four more centuries, and the Crusades would contest the issue for some time, the victory at Manzikert signalled the beginning of Turkish ascendancy in Anatolia. Most historians, including Edward Gibbon, date the defeat at Manzikert as the beginning of the end of the Eastern Roman Empire.
State organization.
Alp Arslan's strength lay in the military realm. Domestic affairs were handled by his able vizier, Nizam al-Mulk, the founder of the administrative organization that characterized and strengthened the sultanate during the reigns of Alp Arslan and his son, Malik Shah. Military fiefs, governed by Seljuq princes, were established to provide support for the soldiery and to accommodate the nomadic Turks to the established Anatolian agricultural scene. This type of military fiefdom enabled the nomadic Turks to draw on the resources of the sedentary Persians, Turks, and other established cultures within the Seljuq realm, and allowed Alp Arslan to field a huge standing army without depending on tribute from conquest to pay his soldiers. He not only had enough food from his subjects to maintain his military, but the taxes collected from traders and merchants added to his coffers sufficiently to fund his continuous wars.
According to the poet Saadi Shirazi:
Arslan possessed a fort, which raised at the height of Alwand, from all were those within its walls, for its roads were a labyrinth, like the curls of a bride. From a learned traveler Arslan once inquired: "Didst thou ever, in thy wanderings, see a fort as strong as this?" "Splendid it is," was the travelers reply, "but methinks not it confers much strength. Before thee, did not other kings possess it for a while, then pass away? After thee, will not other kings assume control, and eat the fruits of the tree of thy hope?"
In the estimation of the wise, the world is a false gem that passes each moment from one hand to another. (the fort was sacked by the Mongols led by Hulagu).
Suleiman ibn Kutalmish was the son of the contender for Arslan's throne; he was appointed governor of the north-western provinces and assigned to completing the invasion of Anatolia. An explanation for this choice can only be conjectured from Ibn al-Athir’s account of the battle between Alp-Arslan and Kutalmish, in which he writes that Alp-Arslan wept for the latter's death and greatly mourned the loss of his kinsman.
Death.
After Manzikert, the dominion of Alp Arslan extended over much of western Asia. He soon prepared to march for the conquest of Turkestan, the original seat of his ancestors. With a powerful army he advanced to the banks of the Oxus. Before he could pass the river with safety, however, it was necessary to subdue certain fortresses, one of which was for several days vigorously defended by the governor, Yussuf el-Harezmi, a Khwarezmian. He was obliged to surrender, however, and was carried as a prisoner before the sultan, who condemned him to death. Yussuf, in desperation, drew his dagger and rushed upon the sultan. Alp Arslan, who took great pride in his reputation as an archer, motioned to his guards not to interfere. He drew his bow, but his foot slipped, the arrow glanced aside, and he received the assassin's dagger in his breast. Alp Arslan died from this wound four days later, on 25 November 1072, in his 42nd year, and he was taken to Merv to be buried next to his father, Chaghri Beg.
Legacy.
Alp Arslan is widely regarded as having begun Anatolianism, although unintentionally. His victory at Manzikert is often cited as the beginning of the end of Byzantine power in Anatolia, and the beginning of Turkish identity there.
Alp Arslan's conquest of Anatolia from the Byzantines is also seen as one of the pivotal precursors to the launch of the crusades.
From 2002 to July 2008 under Turkmen calendar reform, the month of August was named after Alp Arslan.

</doc>
<doc id="869" url="https://en.wikipedia.org/wiki?curid=869" title="American Film Institute">
American Film Institute

The American Film Institute (AFI) is a film organization that educates filmmakers and honors the heritage of the moving picture arts in the U.S. AFI is supported by private funding and public membership.
Membership.
The institute is composed of leaders from the film, entertainment, business and academic communities. A Board of Trustees chaired by Sir Howard Stringer and a Board of Directors chaired by Robert A. Daly guide the organization, which is led by President and CEO Bob Gazzale (as of October 2014). Prior leaders were founding director George Stevens, Jr. (from 1967 to 1980) and Jean Picker Firstenberg (from 1980 to 2007).
List of programs in brief.
AFI educational and cultural programs include:
History.
The American Film Institute was founded by a 1965 presidential mandate announced in the Rose Garden of the White House by Lyndon B. Johnson – to establish a national arts organization to preserve the legacy of American film heritage, educate the next generation of filmmakers and honor the artists and their work. Two years later, in 1967, AFI was established, supported by the National Endowment for the Arts, the Motion Picture Association of America and the Ford Foundation.
The original 22-member Board of Trustees included Chair Gregory Peck and Vice Chair Sidney Poitier as well as Francis Ford Coppola, Arthur Schlesinger, Jr., Jack Valenti and other representatives from the arts and academia.
The institute established a training program for filmmakers known then as the Center for Advanced Film Studies. Also created in the early years were a repertory film exhibition program at the Kennedy Center for the Performing Arts and the AFI Catalog of Feature Films — a scholarly source for American film history. The institute moved to its current eight-acre Hollywood campus in 1981. The film training program grew into the AFI Conservatory, an accredited graduate school.
AFI moved its presentation of first-run and auteur films from the Kennedy Center to the historic 1938 Art Deco AFI Silver Theatre and Cultural Center, which now hosts two major film festivals – AFI Fest and AFI Docs – making AFI the largest nonprofit film exhibitor in the world. AFI educates audiences and recognizes artistic excellence through its awards programs and 10 Top 10 Lists.
AFI Conservatory.
In 1969, the institute established the AFI Conservatory for Advanced Film Studies at Greystone, the Doheny Mansion in Beverly Hills, California. The first class included filmmakers Terrence Malick, David Lynch, Caleb Deschanel and Paul Schrader. That program grew into the AFI Conservatory, an accredited graduate film school located in the hills above Hollywood, California, providing training in six filmmaking disciplines: cinematography, directing, editing, producing, production design and screenwriting. Mirroring a professional production environment, Fellows collaborate to make more films than any other graduate level program. Admission to AFI Conservatory is highly selective, with a maximum of 140 graduates per year.
In 2013, Emmy and Oscar-winning director, producer and screenwriter James L. Brooks ("As Good as It Gets", "Broadcast News", "Terms of Endearment") joined AFI as Artistic Director of the AFI Conservatory where he provides leadership for the film program. Brooks' artistic role at the AFI Conservatory has a rich legacy that includes Daniel Petrie, Jr., Robert Wise and Frank Pierson. Award-winning director Bob Mandel serves as Dean of the AFI Conservatory.
Notable alumni.
AFI Conservatory's alumni have careers in film, television and on the web. They have been recognized with all of the major industry awards – Academy Award, Emmy Award, guild awards, and the Tony Award.
Among the alumni of AFI are Andrea Arnold, ("Red Road", "Fish Tank"), Darren Aronofsky ("Requiem for a Dream", "Black Swan"), Carl Colpaert ("Gas Food Lodging","Hurlyburly","Swimming with Sharks"), Doug Ellin ("Entourage"), Todd Field ("In the Bedroom", "Little Children"), Jack Fisk ("Badlands", "Days of Heaven","There Will Be Blood"), Carl Franklin ("One False Move", "Devil in a Blue Dress", "House of Cards"), Janusz Kamiński ("Lincoln", "Schindler's List", "Saving Private Ryan"), Matthew Libatique ("Noah", "Black Swan"), David Lynch ("Mulholland Drive", "Blue Velvet"), Terrence Malick ("Days of Heaven", "The Thin Red Line", "The Tree of Life"), Victor Nuñez, ("Ruby in Paradise", "Ulee's Gold"), Wally Pfister ("Memento", "The Dark Knight", "Inception"), Robert Richardson ("Platoon", "JFK", "Django Unchained") and many others.
AFI programs.
AFI Catalog of Feature Films.
The AFI Catalog, started in 1968, is a web-based filmographic database. A research tool for film historians, the catalog consists of entries on more than 60,000 feature films and 17,000 short films produced from 1893–2011, as well as AFI Awards Outstanding Movies of the Year from 2000 through 2010.
AFI Awards.
Each year the AFI Awards honor the ten outstanding films and the ten outstanding television programs. The Awards are announced in December and a private luncheon for award honorees takes place the following January.
The AFI Awards were first announced in 2000.
AFI 100 Years… series.
The AFI 100 Years... series, which ran from 1998 to 2008 and created jury-selected lists of America's best movies in categories such as Musicals, Laughs and Thrills, prompted new generations to experience classic American films. The juries consisted of over 1,500 artists, scholars, critics and historians, with movies selected based on the film's popularity over time, historical significance and cultural impact. "Citizen Kane" was voted the greatest American film twice.
AFI film festivals.
AFI operates two film festivals: AFI Fest in Los Angeles, and AFI Docs (formally known as Silverdocs) in Silver Spring, Maryland, and Washington, D.C..
AFI Fest.
AFI Fest is the American Film Institute's annual celebration of artistic excellence. The festival is a showcase for the best festival films of the year and an opportunity for master filmmakers and emerging artists to come together with audiences in the movie capital of the world. AFI Fest is the only festival of its stature that is free to the public. The Academy of Motion Picture Arts and Sciences recognizes AFI Fest as a qualifying festival for the Short Films category for the annual Academy Awards.
The festival has paid tribute to numerous influential filmmakers and artists over the years, including Agnès Varda, Pedro Almodóvar and David Lynch as Guest Artistic Directors, and has screened scores of films that have produced Oscar nominations and wins. The American Film Market (AFM) is the market partner of AFI Fest. Audi is the festival's presenting sponsor. Additional sponsors include American Airlines and Stella Artois.
AFI Docs.
Held annually in June, AFI Docs (formerly Silverdocs) is a documentary festival in Washington, D.C.. The festival attracts over 27,000 documentary enthusiasts.
AFI Silver Theatre and Cultural Center.
The AFI Silver Theatre and Cultural Center is a moving image exhibition, education and cultural center located in Silver Spring, Maryland. Anchored by the restoration of noted architect John Eberson's historic 1938 Silver Theatre, it features 32,000 square feet of new construction housing two stadium theatres, office and meeting space, and reception and exhibit areas.
The AFI Silver Theatre and Cultural Center presents film and video programming, augmented by filmmaker interviews, panels, discussions,and musical performances.
The AFI Directing Workshop for Women.
The Directing Workshop for Women is a training program committed to educating and mentoring participants in an effort to increase the number of women working professionally in screen directing. In this tuition-free program, each participant is required to complete a short film by the end of the year-long program.
Alumnae of the program include Maya Angelou, Anne Bancroft, Dyan Cannon, Ellen Burstyn, Jennifer Getzinger, Lesli Linka Glatter and Nancy Malone.
AFI Directors Series.
AFI released a set of hour-long programs reviewing the career of acclaimed directors. The Directors Series content was copyrighted in 1997 by Media Entertainment Inc and The American Film Institute, and the VHS and DVDs were released between 1999 and 2001 on Winstar TV and Video.
Directors featured included:

</doc>
<doc id="872" url="https://en.wikipedia.org/wiki?curid=872" title="Akira Kurosawa">
Akira Kurosawa

Kurosawa entered the Japanese film industry in 1936, following a brief stint as a painter. After years of working on numerous films as an assistant director and scriptwriter, he made his debut as a director in 1943, during World War II, with the popular action film "Sanshiro Sugata" (a.k.a. "Judo Saga"). After the war, the critically acclaimed "Drunken Angel" (1948), in which Kurosawa cast then-unknown actor Toshiro Mifune in a starring role, cemented the director's reputation as one of the most important young filmmakers in Japan. The two men would go on to collaborate on another 15 films. His wife Yōko Yaguchi was also an actress in one of his films.
"Rashomon", which premiered in Tokyo in August 1950, and which also starred Mifune, became, on September 10, 1951, the surprise winner of the Golden Lion at the Venice Film Festival and was subsequently released in Europe and North America. The commercial and critical success of this film opened up Western film markets for the first time to the products of the Japanese film industry, which in turn led to international recognition for other Japanese filmmakers. Throughout the 1950s and early 1960s, Kurosawa directed approximately a film a year, including a number of highly regarded films such as "Ikiru" (1952), "Seven Samurai" (1954) and "Yojimbo" (1961). After the mid-1960s, he became much less prolific, but his later work—including his final two epics, "Kagemusha" (1980) and "Ran" (1985)—continued to win awards, including the Palme d'Or for "Kagemusha", though more often abroad than in Japan.
In 1990, he accepted the Academy Award for Lifetime Achievement. Posthumously, he was named "Asian of the Century" in the "Arts, Literature, and Culture" category by "AsianWeek" magazine and CNN, cited as "one of the people who contributed most to the betterment of Asia in the past 100 years".
Life and career.
Childhood and youth (1910–35).
Kurosawa was born on 23 March 1910 in Ōimachi in the Ōmori district of Tokyo. His father Isamu, a member of a former samurai family from Akita Prefecture, worked as the director of the Army's Physical Education Institute's lower secondary school, while his mother Shima came from a merchant's family living in Osaka. Akira was the eighth and youngest child of the moderately wealthy family, with two of his siblings already grown up at the time of his birth and one deceased, leaving Kurosawa to grow up with three sisters and a brother.
In addition to promoting physical exercise, Isamu Kurosawa was open to western traditions and considered theater and motion pictures to have educational merit. He encouraged his children to watch films; young Akira viewed his first movies at the age of six. An important formative influence was his elementary school teacher Mr Tachikawa, whose progressive educational practices ignited in his young pupil first a love of drawing and then an interest in education in general. During this time, the boy also studied calligraphy and Kendo swordsmanship.
Another major childhood influence was Heigo Kurosawa, Akira's older brother by four years. In the aftermath of the Great Kantō earthquake of 1923, which devastated Tokyo, Heigo took the 13-year-old Akira to view the devastation. When the younger brother wanted to look away from the human corpses and animal carcasses scattered everywhere, Heigo forbade him to do so, instead encouraging Akira to face his fears by confronting them directly. Some commentators have suggested that this incident would influence Kurosawa's later artistic career, as the director was seldom hesitant to confront unpleasant truths in his work.
Heigo was academically gifted, but soon after failing to secure a place in Tokyo's foremost high school, he began to detach himself from the rest of the family, preferring to concentrate on his interest in foreign literature. In the late 1920s, Heigo became a benshi (silent film narrator) for Tokyo theaters showing foreign films, and quickly made a name for himself. Akira, who at this point planned to become a painter, moved in with him, and the two brothers became inseparable. Through Heigo, Akira devoured not only films but also theater and circus performances, while exhibiting his paintings and working for the left-wing Proletarian Artists' League. However, he was never able to make a living with his art, and, as he began to perceive most of the proletarian movement as "putting unfulfilled political ideals directly onto the canvas", he lost his enthusiasm for painting.
With the increasing production of talking pictures in the early 1930s, film narrators like Heigo began to lose work, and Akira moved back in with his parents. In July 1933, Heigo committed suicide. Kurosawa has commented on the lasting sense of loss he felt at his brother's death and the chapter of his autobiography ("Something Like an Autobiography") that describes it—written nearly half a century after the event—is titled, "A Story I Don't Want to Tell." Only four months later, Kurosawa's eldest brother also died, leaving Akira, at age 23, the only one of the Kurosawa brothers still living, together with his three surviving sisters.
Director in training (1935–41).
In 1935, the new film studio Photo Chemical Laboratories, known as P.C.L. (which later became the major studio, Toho), advertised for assistant directors. Although he had demonstrated no previous interest in film as a profession, Kurosawa submitted the required essay, which asked applicants to discuss the fundamental deficiencies of Japanese films and find ways to overcome them. His half-mocking view was that if the deficiencies were fundamental, there was no way to correct them. Kurosawa's essay earned him a call to take the follow-up exams, and director Kajirō Yamamoto, who was among the examiners, took a liking to Kurosawa and insisted that the studio hire him. The 25-year-old Kurosawa joined P.C.L. in February 1936.
During his five years as an assistant director, Kurosawa worked under numerous directors, but by far the most important figure in his development was Kajiro Yamamoto. Of his 24 films as A.D., he worked on 17 under Yamamoto, many of them comedies featuring the popular actor Kenichi Enomoto, known as "Enoken". Yamamoto nurtured Kurosawa's talent, promoting him directly from third assistant director to chief assistant director after a year. Kurosawa's responsibilities increased, and he worked at tasks ranging from stage construction and film development to location scouting, script polishing, rehearsals, lighting, dubbing, editing and second-unit directing. In the last of Kurosawa's films as an assistant director, "Horse" ("Uma", 1941), Kurosawa took over most of the production, as Yamamoto was occupied with the shooting of another film.
One important piece of advice Yamamoto gave Kurosawa was that a good director needed to master screenwriting. Kurosawa soon realized that the potential earnings from his scripts were much higher than what he was paid as an assistant director. Kurosawa would later write or co-write all of his own films. He also frequently wrote screenplays for other directors such as for Satsuo Yamamoto's film, A Triumph of Wings ("Tsubasa no gaika", 1942). This outside scriptwriting would serve Kurosawa as a lucrative sideline lasting well into the 1960s, long after he became world famous.
Wartime films and marriage (1942–45).
In the two years following the release of "Horse" in 1941, Kurosawa searched for a story he could use to launch his directing career. Towards the end of 1942, about a year after the Japanese attack on Pearl Harbor, novelist Tsuneo Tomita published his Musashi Miyamoto-inspired judo novel, "Sanshiro Sugata", the advertisements for which intrigued Kurosawa. He bought the book on its publication day, devoured it in one sitting, and immediately asked Toho to secure the film rights. Kurosawa's initial instinct proved correct as, within a few days, three other major Japanese studios also offered to buy the rights. Toho prevailed, and Kurosawa began pre-production on his debut work as director.
Shooting of "Sanshiro Sugata" began on location in Yokohama in December 1942. Production proceeded smoothly, but getting the completed film past the censors was an entirely different matter. The censorship office considered the work too "British-American" (an accusation tantamount, at that time, to a charge of treason), and it was only through the intervention of director Yasujirō Ozu, who championed the film, that "Sanshiro Sugata" was finally accepted for release on March 25, 1943. (Kurosawa had just turned 33.) The movie became both a critical and commercial success. Nevertheless, the censorship office would later decide to cut out some 18 minutes of footage, much of which is now considered lost.
He next turned to the subject of wartime female factory workers in "The Most Beautiful", a propaganda film which he shot in a semi-documentary style in early 1944. In order to coax realistic performances from his actresses, the director had them live in a real factory during the shoot, eat the factory food and call each other by their character names. He would use similar methods with his performers throughout his career.
During production, the actress playing the leader of the factory workers, Yōko Yaguchi, was chosen by her colleagues to present their demands to the director. She and Kurosawa were constantly at loggerheads, and it was through these arguments that the two, paradoxically, became close. They married on May 21, 1945, with Yaguchi two months pregnant (she never resumed her acting career), and the couple would remain together until her death in 1985. They would have two children: a son, Hisao, born December 20, 1945, who would serve as producer on some of his father's last projects, and Kazuko, a daughter, born April 29, 1954, who would become a costume designer.
Shortly before his marriage, Kurosawa was pressured by the studio against his will to direct a sequel to his debut film. The often blatantly propagandistic "Sanshiro Sugata Part II", which premiered in May 1945, is generally considered one of his weakest pictures.
Kurosawa decided to write the script for a film that would be both censor-friendly and less expensive to produce. "The Men Who Tread on the Tiger's Tail", based on the Kabuki play "Kanjinchō" and starring the comedian Enoken, with whom Kurosawa had often worked during his assistant director days, was completed in September 1945. By this time, Japan had surrendered and the occupation of Japan had begun. The new American censors interpreted the values allegedly promoted in the picture as overly "feudal" and banned the work. (It would not be released until 1952, the year another Kurosawa film, "Ikiru", was also released.) Ironically, while in production, the film had already been savaged by Japanese wartime censors as too Western and "democratic" (they particularly disliked the comic porter played by Enoken), so the movie most probably would not have seen the light of day even if the war had continued beyond its completion.
First postwar works (1946–50).
The war now ended, Kurosawa, absorbing the democratic ideals of the Occupation, sought to make films that would establish a new respect towards the individual and the self. The first such film, "No Regrets for Our Youth" (1946), inspired by both the 1933 Takigawa incident and the Hotsumi Ozaki wartime spy case, criticized Japan's prewar regime for its political oppression. Atypically for the director, the heroic central character is a woman, Yukie (Setsuko Hara), born into upper-middle-class privilege, who comes to question her values in a time of political crisis. The original script had to be extensively rewritten and, because of its controversial theme (and because the protagonist was a woman), the completed work divided critics, but it nevertheless managed to win the approval of audiences, who turned variations on the film's title ("No regrets for...") into something of a postwar catchphrase.
His next film, "One Wonderful Sunday" premiered in July 1947 to mixed reviews. It is a relatively uncomplicated and sentimental love story dealing with an impoverished postwar couple trying to enjoy, within the devastation of postwar Tokyo, their one weekly day off. The movie bears the influence of Frank Capra, D. W. Griffith and F. W. Murnau. Another film released in 1947 with Kurosawa's involvement was the action-adventure thriller, "Snow Trail", directed by Senkichi Taniguchi from Kurosawa's screenplay. It marked the debut of the intense young actor Toshiro Mifune. It was Kurosawa who, with his mentor Yamamoto, had intervened to persuade Toho to sign Mifune, during an audition in which the young man greatly impressed Kurosawa, but managed to alienate most of the other judges.
"Drunken Angel" is often considered the director's first major work. Although the script, like all of Kurosawa's occupation-era works, had to go through forced rewrites due to American censorship, Kurosawa felt that this was the first film in which he was able to express himself freely. A grittily realistic story of a doctor who tries to save a gangster (yakuza) with tuberculosis, it was also the director's first film with Toshiro Mifune, who would proceed to play either the main or a major character in all but one ("Ikiru") of the director's next 16 films. While Mifune was not cast as the protagonist in "Drunken Angel", his explosive performance as the gangster so dominates the drama that he shifted the focus from the title character, the alcoholic doctor played by Takashi Shimura, who had already appeared in several Kurosawa movies. However, Kurosawa did not want to smother the young actor's immense vitality, and Mifune's rebellious character electrified audiences in much the way that Marlon Brando's defiant stance would startle American film audiences a few years later. The film premiered in Tokyo in April 1948 to rave reviews and was chosen by the prestigious Kinema Junpo critics poll as the best film of its year, the first of three Kurosawa movies to be so honored.
Kurosawa, with producer Sōjirō Motoki and fellow directors and friends Kajiro Yamamoto, Mikio Naruse and Senkichi Taniguchi, formed a new independent production unit called Film Art Association (Eiga Geijutsu Kyōkai). For this organization's debut work, and first film for Daiei studios, Kurosawa turned to a contemporary play by Kazuo Kikuta and, together with Taniguchi, adapted it for the screen. "The Quiet Duel" starred Toshiro Mifune as an idealistic young doctor struggling with syphilis, a deliberate attempt by Kurosawa to break the actor away from being typecast as gangsters. Released in March 1949, it was a box office success, but is generally considered one of the director's lesser achievements.
His second film of 1949, also produced by Film Art Association and released by Shintoho, was "Stray Dog". It is a detective movie (perhaps the first important Japanese film in that genre) that explores the mood of Japan during its painful postwar recovery through the story of a young detective, played by Mifune, and his obsession over his handgun, stolen by a penniless young man who proceeds to use it to rob and murder. Adapted from an unpublished novel by Kurosawa in the style of a favorite writer of his, Georges Simenon, it was the director's first collaboration with screenwriter Ryuzo Kikushima, who would later help to script eight other Kurosawa films. A famous, virtually wordless sequence, lasting over eight minutes, shows the detective, disguised as an impoverished veteran, wandering the streets in search of the gun thief; it employed actual documentary footage of war-ravaged Tokyo neighborhoods shot by Kurosawa's friend, Ishirō Honda, the future director of "Gojira" (a.k.a. "Godzilla"). The film is considered a precursor to the contemporary police procedural and buddy cop film genres.
"Scandal", released by Shochiku in April 1950, was inspired by the director's personal experiences with, and anger towards, Japanese yellow journalism. The work is an ambitious mixture of courtroom drama and social problem film about free speech and personal responsibility, but even Kurosawa regarded the finished product as dramatically unfocused and unsatisfactory, and almost all critics agree.
However, it would be Kurosawa's "second" film of 1950, "Rashomon", that would ultimately win him a whole new audience.
International recognition (1950–58).
After finishing "Scandal", Kurosawa was approached by Daiei studios, which asked the director to make another film for them. Kurosawa picked a script by an aspiring young screenwriter, Shinobu Hashimoto. (They would eventually write nine films together.) It was based on Ryūnosuke Akutagawa's experimental short story "In a Grove", which recounts the murder of a samurai and the rape of his wife from various different and conflicting points-of-view. Kurosawa saw potential in the script, and with Hashimoto's help, polished and expanded it and then pitched it to Daiei, who were happy to accept the project due to its low budget.
Shooting of "Rashomon" began on July 7, 1950 and, after extensive location work in the primeval forest of Nara, wrapped on August 17. Just one week was spent in hurried post-production, hampered by a studio fire, and the finished film premiered at Tokyo's Imperial Theatre on August 25, expanding nationwide the following day. The movie was met by lukewarm reviews, with many critics puzzled by its unique theme and treatment, but it was nevertheless a moderate financial success for Daiei.
Kurosawa's next film, for Shochiku, was "The Idiot", an adaptation of the novel by the director's favorite writer, Fyodor Dostoyevsky. The filmmaker relocated the story from Russia to Hokkaido, but it is otherwise very faithful to the original, a fact seen by many critics as detrimental to the work. A studio-mandated edit shortened it from Kurosawa's original cut of 265 minutes (nearly four-and-a-half hours) to just 166 minutes, making the resulting narrative exceedingly difficult to follow. It is widely considered today to be one of the director's least successful works. Contemporary reviews were very negative, but the film was a moderate success at the box office, largely because of the popularity of one of its stars, Setsuko Hara.
Meanwhile, unbeknownst to Kurosawa, "Rashomon" had been entered in the prestigious Venice Film Festival, due to the efforts of Giuliana Stramigioli, a Japan-based representative of an Italian film company, who had seen and admired the movie and convinced Daiei to submit it. On September 10, 1951, "Rashomon" was awarded the festival's highest prize, the Golden Lion, shocking not only Daiei but the international film world, which at the time was largely unaware of Japan's decades-old cinematic tradition.
After Daiei very briefly exhibited a subtitled print of the film in Los Angeles, RKO purchased distribution rights to "Rashomon" in the United States. The company was taking a considerable gamble. It had put out only one prior subtitled film in the American market, and the only previous Japanese talkie commercially released in New York had been Mikio Naruse's comedy, "Wife! Be Like a Rose", in 1937: a critical and box-office flop. However, "Rashomon"s commercial run, greatly helped by strong reviews from critics and even the columnist Ed Sullivan, was very successful. (It earned $35,000 in its first three weeks at a single New York theater, an almost unheard-of sum at the time.) This success in turn led to a vogue in America for Japanese movies throughout the 1950s, replacing the enthusiasm for Italian neorealist cinema. (The film was also released, by other distributors, in France, West Germany, Denmark, Sweden and Finland.) Among the Japanese filmmakers whose work, as a result, began to win festival prizes and commercial release in the West were Kenji Mizoguchi ("The Life of Oharu", "Ugetsu", "Sansho the Bailiff") and, somewhat later, Yasujirō Ozu ("Tokyo Story", "An Autumn Afternoon")—artists highly respected in Japan but, prior to this period, almost totally unknown in the West. Later generations of Japanese filmmakers who would find acclaim outside Japan—from Nagisa Oshima and Shohei Imamura to Juzo Itami, Takeshi Kitano and Takashi Miike—were able to pass through the door that Kurosawa was the very first to open.
His career boosted by his sudden international fame, Kurosawa, now reunited with his original film studio, Toho (which would go on to produce his next 11 films), set to work on his next project, "Ikiru". The movie stars Takashi Shimura as a cancer-ridden Tokyo bureaucrat, Watanabe, on a final quest for meaning before his death. For the screenplay, Kurosawa brought in Hashimoto as well as writer Hideo Oguni, who would go on to co-write 12 Kurosawa films. Despite the work's grim subject matter, the screenwriters took a satirical approach, which some have compared to the work of Brecht, to both the bureaucratic world of its hero and the U.S. cultural colonization of Japan. (American pop songs figure prominently in the film.) Because of this strategy, the filmmakers are usually credited with saving the picture from the kind of sentimentality common to dramas about characters with terminal illnesses. "Ikiru" opened in October 1952 to rave reviews—it won Kurosawa his second Kinema Junpo "Best Film" award—and enormous box office success. It remains the most acclaimed of all the artist's films set in the modern era.
In December 1952, Kurosawa took his "Ikiru" screenwriters, Shinobu Hashimoto and Hideo Oguni, for a forty-five day secluded residence at an inn to create the screenplay for his next movie, "Seven Samurai". The ensemble work was Kurosawa's first proper samurai film, the genre for which he would become most famous. The simple story, about a poor farming village in Sengoku period Japan that hires a group of samurai to defend it against an impending attack by bandits, was given a full epic treatment, with a huge cast (largely consisting of veterans of previous Kurosawa productions) and meticulously detailed action, stretching out to almost three-and-a-half hours of screen time.
Three months were spent in pre-production and a month in rehearsals. Shooting took up 148 days spread over almost a year, interrupted by production and financing troubles and Kurosawa's health problems. The film finally opened in April 1954, half a year behind its original release date and about three times over budget, making it at the time the most expensive Japanese film ever made. (However, by Hollywood standards, it was a quite modestly budgeted production, even for that time). The film received positive critical reaction and became a big hit, quickly making back the money invested in it and providing the studio with a product that they could, and did, market internationally—though with extensive edits. Over time—and with the theatrical and home video releases of the uncut version—its reputation has steadily grown. It is now regarded by some commentators as the greatest Japanese film ever made, and in 1979, a poll of Japanese film critics also voted it the best Japanese film ever made.
In 1954, nuclear tests in the Pacific were causing radioactive rainstorms in Japan and one particular incident in March had exposed a Japanese fishing boat to nuclear fallout, with disastrous results. It is in this anxious atmosphere that Kurosawa's next film, "Record of a Living Being", was conceived. The story concerned an elderly factory owner (Toshiro Mifune) so terrified of the prospect of a nuclear attack that he becomes determined to move his entire extended family (both legal and extra-marital) to what he imagines is the safety of a farm in Brazil. Production went much more smoothly than the director's previous film, but a few days before shooting ended, Kurosawa's composer, collaborator and close friend Fumio Hayasaka died (of tuberculosis) at the age of 41. The film's score was finished by Hayasaka's student, Masaru Sato, who would go on to score all of Kurosawa's next eight films. "Record of a Living Being" opened in November 1955 to mixed reviews and muted audience reaction, becoming the first Kurosawa film to lose money during its original theatrical run. Today, it is considered by many to be among the finest films dealing with the psychological effects of the global nuclear stalemate.
Kurosawa's next project, "Throne of Blood", a lavishly produced adaptation of William Shakespeare's "Macbeth"—set, like "Seven Samurai", in the Sengoku Era—represented an ambitious transposition of the English work into a Japanese context. Kurosawa instructed his leading actress, Isuzu Yamada, to regard the work as if it were a cinematic version of a "Japanese" rather than a European literary classic. Appropriately, the acting of the players, particularly Yamada, draws heavily on the stylized techniques of the Noh theater. It was filmed in 1956 and released in January 1957 to a slightly less negative domestic response than had been the case with the director's previous film. Abroad, "Throne of Blood", regardless of the liberties it takes with its source material, quickly earned a place among the most celebrated Shakespeare adaptations.
Another adaptation of a classic European theatrical work followed almost immediately, with production of "The Lower Depths", based on a play by Maxim Gorky, taking place in May and June 1957. In contrast to the gigantic scope and sweep of "Throne of Blood", "The Lower Depths" was shot on only two confined sets, the better to emphasize the restricted nature of the characters' lives. Though faithful to the play, this adaptation of Russian material to a completely Japanese setting—in this case, the late Edo period—unlike his earlier "The Idiot", was regarded as artistically successful. The film premiered in September 1957, receiving a mixed response similar to that of "Throne of Blood". However, some critics rank it among the director's most underrated works.
Kurosawa's three consecutive movies after "Seven Samurai" had not managed to capture Japanese audiences in the way that that film had. The mood of the director's work had been growing increasingly pessimistic and dark, with the possibility of redemption through personal responsibility now very much questioned, particularly in "Throne of Blood" and "The Lower Depths". He recognized this, and deliberately aimed for a more light-hearted and entertaining film for his next production, while switching to the new widescreen format that had been gaining popularity in Japan. The resulting film, "The Hidden Fortress", is an action-adventure comedy-drama about a medieval princess, her loyal general and two peasants who all need to travel through enemy lines in order to reach their home region. Released in December 1958, "The Hidden Fortress" became an enormous box office success in Japan and was warmly received by critics. Today, the film is considered one of Kurosawa's most lightweight efforts, though it remains popular, not least because it is one of several major influences (as George Lucas himself has conceded) on Lucas' hugely popular 1977 space opera, "Star Wars".
Birth of a company and the end of an era (1959–65).
Starting with "Rashomon", Kurosawa's productions had become increasingly large in scope and so had the director's budgets. Toho, concerned about this development, suggested that he might help finance his own works, therefore making the studio's potential losses smaller, while in turn allowing himself more artistic freedom as co-producer. Kurosawa agreed, and the Kurosawa Production Company was established in April 1959, with Toho as majority shareholder.
Despite risking his own money, Kurosawa chose a story that was more directly critical of the Japanese business and political elites than any work. "The Bad Sleep Well", based on a script by Kurosawa's nephew Mike Inoue, is a revenge drama about a young man who climbs the hierarchy of a corrupt Japanese company with the intention of exposing the men responsible for his father's death. Its theme proved topical: while the film was in production, mass demonstrations were held against the new U.S.-Japan Security treaty, which was seen by many Japanese, particularly the young, as threatening the country's democracy by giving too much power to corporations and politicians. The film opened in September 1960 to positive critical reaction and modest box office success. The 25-minute opening sequence, depicting a corporate wedding reception interrupted by reporters and police (who arrest an executive for corruption), is widely regarded as one of Kurosawa's most skillfully executed set pieces, but the remainder of the film is often perceived as disappointing by comparison. The movie has also been criticized for employing the conventional Kurosawan hero to combat a social evil that cannot be resolved through the actions of individuals, however courageous or cunning.
"Yojimbo" ("The Bodyguard"), Kurosawa Production's second film, centers on a masterless samurai, Sanjuro, who strolls into a 19th-century town ruled by two opposing violent factions and provokes them into destroying each other. The director used this work to play with many genre conventions, particularly the Western, while at the same time offering an unprecedentedly (for the Japanese screen) graphic portrayal of violence. Some commentators have seen the Sanjuro character in this film as a fantasy figure who magically reverses the historical triumph of the corrupt merchant class over the samurai class. Featuring Tatsuya Nakadai in his first major role in a Kurosawa movie, and with innovative photography by Kazuo Miyagawa (who shot "Rashomon") and Takao Saito, the film premiered in April 1961 and was a critically and commercially successful venture, earning more than any previous Kurosawa film. The movie and its blackly comic tone were also widely imitated abroad. Sergio Leone's "A Fistful of Dollars" was a virtual (unauthorized) scene-by-scene remake.
Following the success of "Yojimbo", Kurosawa found himself under pressure from Toho to create a sequel. Kurosawa turned to a script he had written before "Yojimbo", reworking it to include the hero of his previous film. "Sanjuro" was the first of three Kurosawa films to be adapted from the work of the writer Shūgorō Yamamoto (the others would be "Red Beard" and "Dodeskaden"). It is lighter in tone and closer to a conventional period film than "Yojimbo", though its story of a power struggle within a samurai clan is portrayed with strongly comic undertones. The film opened on January 1, 1962, quickly surpassing "Yojimbo"s box office success and garnering positive reviews.
Kurosawa had meanwhile instructed Toho to purchase the film rights to "King's Ransom", a novel about a kidnapping written by American author and screenwriter Evan Hunter, under his pseudonym of Ed McBain, as one of his 87th Precinct series of crime books. The director intended to create a work condemning kidnapping, which he considered one of the very worst crimes. The suspense film, titled "High and Low", was shot during the latter half of 1962 and released in March 1963. It broke Kurosawa's box office record (the third film in a row to do so), became the highest grossing Japanese film of the year, and won glowing reviews. However, his triumph was somewhat tarnished when, ironically, the film was blamed for a wave of kidnappings which occurred in Japan about this time (he himself received kidnapping threats directed at his young daughter, Kazuko). "High and Low" is considered by many commentators to be among the director's strongest works.
Kurosawa quickly moved on to his next project, "Red Beard". Based on a short story collection by Shūgorō Yamamoto and incorporating elements from Dostoyevsky's novel "The Insulted and Injured", it is a period film, set in a mid-19th century clinic for the poor, in which Kurosawa's humanist themes receive perhaps their fullest statement. A conceited and materialistic, foreign-trained young doctor, Yasumoto, is forced to become an intern at the clinic under the stern tutelage of Doctor Niide, known as "Akahige" ("Red Beard"), played by Mifune. Although he resists Red Beard initially, Yasumoto comes to admire his wisdom and courage, and to perceive the patients at the clinic, whom he at first despised, as worthy of compassion and dignity.
Yūzō Kayama, who plays Yasumoto, was an extremely popular film and music star at the time, particularly for his "Young Guy" ("Wakadaishō") series of musical comedies, so signing him to appear in the film virtually guaranteed Kurosawa strong box-office. The shoot, the filmmaker's longest ever, lasted well over a year (after five months of pre-production), and wrapped in spring 1965, leaving the director, his crew and his actors exhausted. "Red Beard" premiered in April 1965, becoming the year's highest-grossing Japanese production and the third (and last) Kurosawa film to top the prestigious Kinema Jumpo yearly critics poll. It remains one of Kurosawa's best-known and most-loved works in his native country. Outside Japan, critics have been much more divided. Most commentators concede its technical merits and some praise it as among Kurosawa's best, while others insist that it lacks complexity and genuine narrative power, with still others claiming that it represents a retreat from the artist's previous commitment to social and political change.
The film marked something of an end of an era for its creator. The director himself recognized this at the time of its release, telling critic Donald Richie that a cycle of some kind had just come to an end and that his future films and production methods would be different. His prediction proved quite accurate. Beginning in the late 1950s, television began increasingly to dominate the leisure time of the formerly large and loyal Japanese cinema audience. And as film company revenues dropped, so did their appetite for risk—particularly the risk represented by Kurosawa's costly production methods.
"Red Beard" also marked the midway point, chronologically, in the artist's career. During his previous twenty-nine years in the film industry (which includes his five years as assistant director), he had directed twenty-three films, while during the remaining twenty-eight years, for many and complex reasons, he would complete only seven more. Also, for reasons never adequately explained, "Red Beard" would be his final film starring Toshiro Mifune. Yu Fujiki, an actor who worked on "The Lower Depths", observed, regarding the closeness of the two men on the set, "Mr. Kurosawa's heart was in Mr. Mifune's body." Donald Richie has described the rapport between them as a unique "symbiosis".
Hollywood detour (1966–68).
When Kurosawa's exclusive contract with Toho came to an end in 1966, the 56-year-old director was seriously contemplating change. Observing the troubled state of the domestic film industry, and having already received dozens of offers from abroad, the idea of working outside Japan appealed to him as never before.
For his first foreign project, Kurosawa chose a story based on a Life magazine article. The Embassy Pictures action thriller, to be filmed in English and called simply "Runaway Train", would have been his first in color. But the language barrier proved a major problem, and the English version of the screenplay was not even finished by the time filming was to begin in autumn 1966. The shoot, which required snow, was moved to autumn 1967, then canceled in 1968. Almost twenty years later, another foreigner working in Hollywood, Andrei Konchalovsky, would finally make "Runaway Train", though from a script totally different from Kurosawa's.
The director meanwhile had become involved in a much more ambitious Hollywood project. "Tora! Tora! Tora!", produced by 20th Century Fox and Kurosawa Production, would be a portrayal of the Japanese attack on Pearl Harbor from both the American and the Japanese points-of-view, with Kurosawa helming the Japanese half and an English-speaking filmmaker directing the American half. He spent several months working on the script with Ryuzo Kikushima and Hideo Oguni, but very soon the project began to unravel. The director chosen to film the American sequences turned out not to be the prestigious English filmmaker David Lean, as the producers had led Kurosawa to believe, but the much less celebrated special effects expert, Richard Fleischer. The budget was also cut, and the screen time allocated for the Japanese segment would now be no longer than 90 minutes—a major problem, considering that Kurosawa's script ran over four hours. After numerous revisions, a more or less finalized cut screenplay was agreed upon in May 1968. Shooting began in early December, but Kurosawa would last only a little over three weeks as director. He struggled to work with an unfamiliar crew and the requirements of a Hollywood production, while his working methods puzzled his American producers, who ultimately concluded that the director must be mentally ill. On Christmas Eve 1968, the Americans announced that Kurosawa had left the production due to "fatigue", effectively firing him. (He was ultimately replaced, for the film's Japanese sequences, with two directors, Kinji Fukasaku and Toshio Masuda.)
"Tora! Tora! Tora!", finally released to unenthusiastic reviews in September 1970, was, as Donald Richie put it, an "almost unmitigated tragedy" in Kurosawa's career. He had spent years of his life on a logistically nightmarish project to which he ultimately did not contribute a foot of film shot by himself. (He had his name removed from the credits, though the script used for the Japanese half was still his and his co-writers'.) He became estranged from his longtime collaborator, writer Ryuzo Kikushima, and never worked with him again. The project had inadvertently exposed corruption in his own production company (a situation reminiscent of his own movie, "The Bad Sleep Well"). His very sanity had been called into question. Worst of all, the Japanese film industry—and perhaps the man himself—began to suspect that he would never make another film.
A difficult decade (1969–77).
Knowing that his reputation was at stake following the much publicised "Tora! Tora! Tora!" debacle, Kurosawa moved quickly to a new project to prove he was still viable. To his aid came friends and famed directors Keisuke Kinoshita, Masaki Kobayashi and Kon Ichikawa, who together with Kurosawa established in July 1969 a production company called the Club of the Four Knights (Yonki no kai). Although the plan was for the four directors to create a film each, it has been suggested that the real motivation for the other three directors was to make it easier for Kurosawa to successfully complete a film, and therefore find his way back into the business.
The first project proposed and worked on was a period film to be called "Dora-Heita", but when this was deemed too expensive, attention shifted to "Dodesukaden", an adaptation of yet another Shūgorō Yamamoto work, again about the poor and destitute. The film was shot quickly (by Kurosawa's standards) in about nine weeks, with Kurosawa determined to show he was still capable of working quickly and efficiently within a limited budget. For his first work in color, the dynamic editing and complex compositions of his earlier pictures were set aside, with the artist focusing on the creation of a bold, almost surreal palette of primary colors, in order to reveal the toxic environment in which the characters live. It was released in Japan in October 1970, but though a minor critical success, it was greeted with audience indifference. The picture lost money and caused the Club of the Four Knights to dissolve. Initial reception abroad was somewhat more favorable, but "Dodesukaden" has since been typically considered an interesting experiment not comparable to the director's best work.
Unable to secure funding for further work and allegedly suffering from health problems, Kurosawa apparently reached the breaking point: on December 22, 1971, he slit his wrists and throat multiple times. The suicide attempt proved unsuccessful and the director's health recovered fairly quickly, with Kurosawa now taking refuge in domestic life, uncertain if he would ever direct another film.
In early 1973, the Soviet studio Mosfilm approached the filmmaker to ask if he would be interested in working with them. Kurosawa proposed an adaptation of Russian explorer Vladimir Arsenyev's autobiographical work "Dersu Uzala". The book, about a Goldi hunter who lives in harmony with nature until destroyed by encroaching civilization, was one that he had wanted to make since the 1930s. In December 1973, the 63-year-old Kurosawa set off for the Soviet Union with four of his closest aides, beginning a year-and-a-half stay in the country. Shooting began in May 1974 in Siberia, with filming in exceedingly harsh natural conditions proving very difficult and demanding. The picture wrapped in April 1975, with a thoroughly exhausted and homesick Kurosawa returning to Japan and his family in June. "Dersu Uzala" had its world premiere in Japan on August 2, 1975, and did well at the box office. While critical reception in Japan was muted, the film was better reviewed abroad, winning the Golden Prize at the 9th Moscow International Film Festival, as well as an Academy Award for Best Foreign Language Film. Today, critics remain divided over the film: some see it as an example of Kurosawa's alleged artistic decline, while others count it among his finest works.
Although proposals for television projects were submitted to him, he had no interest in working outside the film world. Nevertheless, the hard-drinking director did agree to appear in a series of television ads for Suntory whiskey, which aired in 1976. While fearing that he might never be able to make another film, the director nevertheless continued working on various projects, writing scripts and creating detailed illustrations, intending to leave behind a visual record of his plans in case he would never be able to film his stories.
Two epics (1978–86).
In 1977, American director George Lucas had released "Star Wars", a wildly successful science fiction film influenced by Kurosawa's "The Hidden Fortress", among other works. Lucas, like many other New Hollywood directors, revered Kurosawa and considered him a role model, and was shocked to discover that the Japanese filmmaker was unable to secure financing for any new work. The two met in San Francisco in July 1978 to discuss the project Kurosawa considered most financially viable: "Kagemusha", the epic story of a thief hired as the double of a medieval Japanese lord of a great clan. Lucas, enthralled by the screenplay and Kurosawa's illustrations, leveraged his influence over 20th Century Fox to coerce the studio that had fired Kurosawa just ten years earlier to produce "Kagemusha", then recruited fellow fan Francis Ford Coppola as co-producer.
Production began the following April, with Kurosawa in high spirits. Shooting lasted from June 1979 through March 1980 and was plagued with problems, not the least of which was the firing of the original lead actor, Shintaro Katsu—creator of the very popular Zatoichi character—due to an incident in which the actor insisted, against the director's wishes, on videotaping his own performance. (He was replaced by Tatsuya Nakadai, in his first of two consecutive leading roles in a Kurosawa movie.) The film was completed only a few weeks behind schedule and opened in Tokyo in April 1980. It quickly became a massive hit in Japan. The film was also a critical and box office success abroad, winning the coveted Palme d'Or at the 1980 Cannes Film Festival in May, though some critics, then and now, have faulted the film for its alleged coldness. Kurosawa spent much of the rest of the year in Europe and America promoting "Kagemusha", collecting awards and accolades, and exhibiting as art the drawings he had made to serve as storyboards for the film.
The international success of "Kagemusha" allowed Kurosawa to proceed with his next project, "Ran", another epic in a similar vein. The script, partly based on William Shakespeare's "King Lear", depicted a ruthless, bloodthirsty daimyo (warlord), played by Tatsuya Nakadai, who, after foolishly banishing his one loyal son, surrenders his kingdom to his other two sons, who then betray him, thus plunging the entire kingdom into war. As Japanese studios still felt wary about producing another film that would rank among the most expensive ever made in the country, international help was again needed. This time it came from French producer Serge Silberman, who had produced Luis Buñuel's final movies. Filming did not begin until December 1983 and lasted more than a year.
In January 1985, production of "Ran" was halted as Kurosawa's 64-year-old wife Yōko fell ill. She died on February 1. Kurosawa returned to finish his film and "Ran" premiered at the Tokyo Film Festival on May 31, with a wide release the next day. The film was a moderate financial success in Japan, but a larger one abroad and, as he had done with "Kagemusha", Kurosawa embarked on a trip to Europe and America, where he attended the film's premieres in September and October.
"Ran" won several awards in Japan, but was not quite as honored there as many of the director's best films of the 1950s and 1960s had been. The film world was shocked, however, when Japan passed over the film in favor of another as its official entry to compete for an Oscar nomination in the Best Foreign Film category. Both the producer and Kurosawa himself attributed this to a misunderstanding: because of the Academy's arcane rules, no one was sure whether "Ran" qualified as a "Japanese" film, a "French" film (due to its financing), or both, so it was not submitted at all. In response to what at least appeared to be a blatant snub by his own countrymen, the director Sidney Lumet led a successful campaign to have Kurosawa receive an Oscar nomination for Best Directing that year (Sydney Pollack ultimately won the award for directing Out of Africa). "Ran"s costume designer, Emi Wada, won the movie's only Oscar.
"Kagemusha" and "Ran", particularly the latter, are often considered to be among Kurosawa's finest works. After "Ran"s release, Kurosawa would point to it as his best film, a major change of attitude for the director who, when asked which of his works was his best, had always previously answered "my next one".
Final works and last years (1987–98).
For his next movie, Kurosawa chose a subject very different from any that he had ever filmed before. While some of his previous pictures (for example, "Drunken Angel" and "Kagemusha") had included brief dream sequences, "Dreams" was to be entirely based upon the director's own dreams. Significantly, for the first time in over forty years, Kurosawa, for this deeply personal project, wrote the screenplay alone. Although its estimated budget was lower than the films immediately preceding it, Japanese studios were still unwilling to back one of his productions, so Kurosawa turned to another famous American fan, Steven Spielberg, who convinced Warner Bros. to buy the international rights to the completed film. This made it easier for Kurosawa's son, Hisao, as co-producer and soon-to-be head of Kurosawa Production, to negotiate a loan in Japan that would cover the film's production costs. Shooting took more than eight months to complete, and "Dreams" premiered at Cannes in May 1990 to a polite but muted reception, similar to the reaction the picture would generate elsewhere in the world.
Kurosawa now turned to a more conventional story with "Rhapsody in August"—the director's first film fully produced in Japan since "Dodeskaden" over twenty years before—which explored the scars of the nuclear bombing which destroyed Nagasaki at the very end of World War II. It was adapted from a Kiyoko Murata novel, but the film's references to the Nagasaki bombing came from the director rather than from the book. This was his only movie to include a role for an American movie star: Richard Gere, who plays a small role as the nephew of the elderly heroine. Shooting took place in early 1991, with the film opening on May 25 that year to a largely negative critical reaction, especially in the United States, where the director was accused of promulgating naïvely anti-American sentiments.
Kurosawa wasted no time moving onto his next project: "Madadayo", or "Not Yet". Based on autobiographical essays by Hyakken Uchida, the film follows the life of a Japanese professor of German through the Second World War and beyond. The narrative centers on yearly birthday celebrations with his former students, during which the protagonist declares his unwillingness to die just yet—a theme that was becoming increasingly relevant for the film's 81-year-old creator. Filming began in February 1992 and wrapped by the end of September. Its release on April 17, 1993, was greeted by an even more disappointed reaction than had been the case with his two preceding works.
Kurosawa nevertheless continued to work. He wrote the original screenplays "The Sea is Watching" in 1993 and "After the Rain" in 1995. While putting finishing touches on the latter work in 1995, Kurosawa slipped and broke the base of his spine. Following the accident, he would use a wheelchair for the rest of his life, putting an end to any hopes of him directing another film. His longtime wish—to die on the set while shooting a movie—was never to be fulfilled.
After his accident, Kurosawa's health began to deteriorate. While his mind remained sharp and lively, his body was giving up, and for the last half year of his life, the director was largely confined to bed, listening to music and watching television at home. On September 6, 1998, Kurosawa died of a stroke in Setagaya, Tokyo, at the age of 88. Kurosawa was survived by his two children and four grandchildren, three from son Hisao's marriage to Hiroko Hayashi and one grandson, actor Takayuki Kato, from his daughter Kazuko Kurosawa.
Posthumous works.
Following Kurosawa's death, several posthumous works based on his unfilmed screenplays have been produced. "After the Rain", directed by Takashi Koizumi, was released in 1998, and "The Sea is Watching", directed by Kei Kumai, premiered in 2002. A script created by the Yonki no Kai ("Club of the Four Knights") (Kurosawa, Keisuke Kinoshita, Masaki Kobayashi, and Kon Ichikawa), around the time that "Dodeskaden" was made, finally was filmed and released (in 2000) as "Dora-Heita", by the only surviving founding member of the club, Kon Ichikawa.
Working methods, style and themes.
Working methods.
All biographical sources, as well as the filmmaker's own comments, indicate that Kurosawa was a completely "hands-on" director, passionately involved in every aspect of the filmmaking process. As one interviewer summarized, "he (co-)writes his scripts, oversees the design, rehearses the actors, sets up all the shots and then does the editing." His active participation extended from the initial concept to the editing and scoring of the final product.
Script.
Kurosawa emphasized time and again that the screenplay was the absolute foundation of a successful film and that, though a mediocre director can sometimes make a passable film out of a "good" script, even an excellent director can never make a good film out of a "bad" script. During the postwar period, he began the practice of collaborating with a rotating group of five screenwriters: Eijirō Hisaita, Ryuzo Kikushima, Shinobu Hashimoto, Hideo Oguni, and Masato Ide. Whichever members of this group happened to be working on a particular film would gather around a table, often at a hot-springs resort, where they would not be distracted by the outside world. ("Seven Samurai", for example, was written in this fashion.) Often they all (except Oguni, who acted as "referee") would work on exactly the same pages of the script, and Kurosawa would choose the best-written version from the different drafts of each particular scene. This method was adopted "so that each contributor might function as a kind of foil, checking the dominance of any one person's point-of-view."
In addition to the actual script, Kurosawa at this stage often produced extensive, fantastically detailed notes to elaborate his vision. For example, for "Seven Samurai", he created six notebooks with (among many other things) detailed biographies of the samurai, including what they wore and ate, how they walked, talked and behaved when greeted, and even how each tied his shoes. For the 101 peasant characters in the film, he created a registry consisting of 23 families and instructed the performers playing these roles to live and work as these "families" for the duration of shooting.
Shooting.
For his early films, although they were consistently well photographed, Kurosawa generally used standard lenses and deep-focus photography. Beginning with "Seven Samurai" (1954), however, Kurosawa's cinematic technique changed drastically with his extensive use of long lens and multiple cameras. The director claimed that he used these lenses and several cameras rolling at once to help the actors—allowing them to be photographed at some distance from the lens, and without any knowledge of which particular camera's image would be utilized in the final cut—making their performances much more natural. (In fact, Tatsuya Nakadai agreed that the multiple cameras greatly helped his performances with the director.) But these changes had a powerful effect as well on the look of the action scenes in that film, particularly the final battle in the rain. Says Stephen Prince: "He can use the telephoto lenses to get under the horses, in between their hooves, to plunge us into the chaos of that battle in a visual way that is really quite unprecedented, both in Kurosawa's own work and in the samurai genre as a whole."
With "The Hidden Fortress", Kurosawa began to utilize the widescreen (anamorphic) process for the first time in his work. These three techniques—long lenses, multiple cameras and widescreen—were in later works fully exploited, even in sequences with little or no overt action, such as the early scenes of "High and Low" that take place in the central character's home, in which they are employed to dramatize tensions and power relationships between the characters within a highly confined space.
For all his films, but particularly for his "jidaigeki", Kurosawa insisted on absolute authenticity of sets, costumes and props. Numerous instances of his fanatical devotion to detail have been recorded, of which the following are only a few examples.
For "Throne of Blood", in the scene where Washizu (Mifune) is attacked with arrows by his own men, the director had archers shoot real arrows, hollowed out and running along wires, toward Toshiro Mifune from a distance of about ten feet, with the actor carefully following chalk marks on the ground to avoid being hit. (Some of the arrows missed him by an inch; the actor, who admitted that he was not merely "acting" terrified in the film, suffered nightmares afterward).
For "Red Beard", to construct the gate for the clinic set, Kurosawa had his assistants dismantle rotten wood from old sets and then create the prop from scratch with this old wood, so the gate would look properly ravaged by time. For the same film, for teacups that appeared in the movie, his crew poured fifty years' worth of tea into the cups so they would appear appropriately stained.
For "Ran", art director Yoshirō Muraki, constructing the "third castle" set under the director's supervision, created the "stones" of that castle by having photographs taken of actual stones from a celebrated castle, then painting Styrofoam blocks to exactly resemble those stones and gluing them to the castle "wall" through a process known as "rough-stone piling", which required months of work. Later, before shooting the famous scene in which the castle is attacked and set on fire, in order to prevent the Styrofoam "stones" from melting in the heat, the art department coated the surface with four layers of cement, then painted the colors of the ancient stones onto the cement.
Editing.
Kurosawa both directed and edited most of his films, which is nearly unique among prominent filmmakers. Kurosawa often remarked that he shot a film simply in order to have material to edit, because the editing of a picture was the most important and creatively interesting part of the process for him. Kurosawa's creative team believed that the director's skill with editing was his greatest talent. Hiroshi Nezu, a longtime production supervisor on his films, said, "Among ourselves, we think that he is Toho's best director, that he is Japan's best scenarist, and that he is the best editor in the world. He is most concerned with the flowing quality which a film must have ... The Kurosawa film flows "over" the cut, as it were."
The director's frequent crew member Teruyo Nogami confirms this view. "Akira Kurosawa's editing was exceptional, the inimitable work of a genius ... No one was a match for him." She claimed that Kurosawa carried in his head all the information about all shots filmed, and if, in the editing room, he asked for a piece of film and she handed him the wrong one, he would immediately recognize the error, though she had taken detailed notes on each shot and he had not. She compared his mind to a computer, which could do with edited segments of film what computers do today.
Kurosawa's habitual method was to edit a film daily, bit by bit, during production. This helped particularly when he started using multiple cameras, which resulted in a large amount of film to assemble. "I always edit in the evening if we have a fair amount of footage in the can. After watching the rushes, I usually go to the editing room and work." Because of this practice of editing as he went along, the post-production period for a Kurosawa film could be startlingly brief: "Yojimbo" had its Japanese premiere on April 20, 1961, four days after shooting concluded on April 16.
"Kurosawa-gumi".
Throughout his career, Kurosawa worked constantly with people drawn from the same pool of creative technicians, crew members and actors, popularly known as the "Kurosawa-gumi" (Kurosawa group). The following is a partial list of this group, divided by profession. This information is derived from the IMDB pages for Kurosawa's films and Stuart Galbraith IV's filmography:
Composers: Fumio Hayasaka ("Drunken Angel", "Stray Dog", "Scandal", "Rashomon", "The Idiot", "Ikiru", "Seven Samurai", "Record of a Living Being"); Masaru Sato ("Throne of Blood", "The Lower Depths", "The Hidden Fortress", "The Bad Sleep Well", "Yojimbo", "Sanjuro", "High and Low", "Red Beard"); Tōru Takemitsu ("Dodeskaden", "Ran"); Shin’ichirō Ikebe ("Kagemusha", "Dreams", "Rhapsody in August", "Madadayo").
Cinematographers: Asakazu Nakai ("No Regrets for Our Youth", "One Wonderful Sunday", "Stray Dog", "Ikiru", "Seven Samurai", "Record of a Living Being", "Throne of Blood", "High and Low", "Red Beard", "Dersu Uzala", "Ran"); Kazuo Miyagawa ("Rashomon", "Yojimbo"); Takao Saitō ("Sanjuro", "High and Low", "Red Beard", "Dodeskaden", "Kagemusha", "Ran", "Dreams", "Rhapsody in August", "Madadayo").
Art Department: Yoshirō Muraki served as either assistant art director, art director or production designer for all Kurosawa's films (except for "Dersu Uzala") from "Drunken Angel" until the end of the director's career.
Production Crew: Teruyo Nogami served as script supervisor, production manager, associate director or assistant to the producer on all Kurosawa's films from "Rashomon" to the end of the director's career. Hiroshi Nezu was production supervisor or unit production manager on all the films from "Seven Samurai" to "Dodeskaden", except "Sanjuro". After retiring as a director, Ishirō Honda returned more than 30 years later to work again for his friend and former mentor as a directorial advisor, production coordinator and creative consultant on Kurosawa's last five films ("Kagemusha", "Ran", "Dreams", "Rhapsody in August" and "Madadayo"). Allegedly one segment of "Dreams" was actually directed by Honda following Kurosawa's detailed storyboards.
Actors: "Leading actors": Takashi Shimura (21 films); Toshiro Mifune (16 films), Susumu Fujita (8 films), Tatsuya Nakadai (6 films) and Masayuki Mori (5 films).
"Supporting performers" (in alphabetical order): Minoru Chiaki, Kamatari Fujiwara, Bokuzen Hidari, Fumiko Homma, Hisashi Igawa, Yunosuke Ito, Kyoko Kagawa, Daisuke Kato, Isao Kimura, Kokuten Kodo, Akitake Kono, Yoshio Kosugi, Koji Mitsui, Seiji Miyaguchi, Eiko Miyoshi, Nobuo Nakamura, Akemi Negishi, Denjiro Okochi, Noriko Sengoku, Gen Shimizu, Ichiro Sugai, Haruo Tanaka, Akira Terao, Eijiro Tono, Yoshio Tsuchiya, Kichijiro Ueda, Atsushi Watanabe, Isuzu Yamada, Tsutomu Yamazaki and Yoshitaka Zushi.
Style.
Virtually all commentators have noted Kurosawa's bold, dynamic style, which many have compared to the traditional Hollywood style of narrative moviemaking, one that emphasizes, in the words of one such scholar, "chronological, causal, linear and historical thinking". But it has also been claimed that, from his very first film, the director displayed a technique quite distinct from the seamless style of classic Hollywood. This technique involved a disruptive depiction of screen space through the use of numerous unrepeated camera setups, a disregard for the traditional 180-degree axis of action around which Hollywood scenes have usually been constructed, and an approach in which "narrative time becomes spatialized", with fluid camera movement often replacing conventional editing. The following are some idiosyncratic aspects of the artist's style.
Axial cut.
In his films of the 1940s and 1950s, Kurosawa frequently employs the "axial cut", in which the camera moves closer to, or further away from, the subject, not through the use of tracking shots or dissolves, but through a series of matched jump cuts. For example, in "Sanshiro Sugata II", the hero takes leave of the woman he loves, but then, after walking away a short distance, turns and bows to her, and then, after walking further, turns and bows once more. This sequence of shots is illustrated on film scholar David Bordwell's blog. The three shots are not connected in the film by camera movements or dissolves, but by a series of two jump cuts. The effect is to stress the duration of Sanshiro's departure.
In the opening sequence of "Seven Samurai" in the peasant village, the axial cut is used twice. When the villagers are outdoors, gathered in a circle, weeping and lamenting the imminent arrival of the bandits, they are glimpsed from above in extreme long shot, then, after the cut, in a much closer shot, then in an even closer shot at ground level as the dialogue begins. A few minutes later, when the villagers go to the mill to ask the village elder's advice, there is a long shot of the mill, with a slowly turning wheel in the river, then a closer shot of this wheel, and then a still closer shot of it. (As the mill is where the elder lives, these shots forge a mental association in the viewer's mind between that character and the mill.)
Cutting on motion.
A number of scholars have pointed out Kurosawa's tendency to "cut on motion": that is, to edit a sequence of a character or characters in motion so that an action is depicted in two or more separate shots, rather than one uninterrupted shot. One scholar, as an example, describes a tense scene in "Seven Samurai" in which the samurai Shichirôji, who is standing, wishes to console the peasant Manzo, who is sitting on the ground, and he gets down on one knee to talk to him. Kurosawa chooses to film this simple action in two shots rather than one (cutting between the two only "after" the action of kneeling has begun) to fully convey Shichirôji's humility. Numerous other instances of this device are evident in the movie. "Kurosawa breaks up the action, fragments it, in order to create an emotional effect."
Wipe.
A form of cinematic punctuation very strongly identified with Kurosawa is the wipe. This is an effect created through an optical printer, in which, when a scene ends, a line or bar appears to move across the screen, "wiping" away the image while simultaneously revealing the first image of the subsequent scene. As a transitional device, it is used as a substitute for the straight cut or the dissolve (though Kurosawa, of course, often used both of those devices as well). In his mature work, Kurosawa employed the wipe so frequently that it became a kind of signature. For example, one blogger has counted no fewer than 12 instances of the wipe in "Drunken Angel".
There are a number of theories concerning the purpose of this device, which, as James Goodwin notes, was common in silent cinema but became considerably rarer in the more "realistic" sound cinema. Goodwin claims that the wipes in "Rashomon", for instance, fulfill one of three purposes: emphasizing motion in traveling shots, marking narrative shifts in the courtyard scenes and marking temporal ellipses between actions (e.g., between the end of one character's testimony and the beginning of another's). He also points out that in "The Lower Depths", in which Kurosawa completely avoided the use of wipes, the director cleverly manipulated people and props "in order to slide new visual images in and out of view much as a wipe cut does".
An instance of the wipe used as a satirical device can be seen in "Ikiru". A group of women visit the local government office to petition the bureaucrats to turn a waste area into a children's playground. The viewer is then shown a series of point of view shots of various bureaucrats, connected by wipe transitions, each of whom refers the group to another department. Nora Tennessen comments in her blog (which shows one example) that "the wipe technique makes sequence funnier—images of bureaucrats are stacked like cards, each more punctilious than the last."
Image-sound counterpoint.
Kurosawa by all accounts always gave great attention to the soundtracks of his films (Teruyo Nogami's memoir gives many such examples). In the late 1940s, he began to employ music for what he called "counterpoint" to the emotional content of a scene, rather than merely to reinforce the emotion, as Hollywood traditionally did (and still does). The inspiration for this innovation came from a family tragedy. When news reached Kurosawa of his father's death in 1948, he wandered aimlessly through the streets of Tokyo. His sorrow was magnified rather than diminished when he suddenly heard the cheerful, vapid song "The Cuckoo Waltz", and he hurried to escape from this "awful music". He then told his composer, Fumio Hayasaka, with whom he was working on "Drunken Angel", to use "The Cuckoo Waltz" as ironic accompaniment to the scene in which the dying gangster, Matsunaga, sinks to his lowest point in the narrative.
This ironic approach to music can also be found in "Stray Dog", a film released a year after "Drunken Angel". In the climactic scene, the detective Murakami is fighting furiously with the murderer Yusa in a muddy field. The sound of a Mozart piece is suddenly heard, played on the piano by a woman in a nearby house. As one commentator notes, "In contrast to this scene of primitive violence, the serenity of the Mozart is, literally, other-worldly" and "the power of this elemental encounter is heightened by the music." Nor was Kurosawa's "ironic" use of the soundtrack limited to music. One critic observes that, in "Seven Samurai", "During episodes of murder and mayhem, birds chirp in the background, as they do in the first scene when the farmers lament their seemingly hopeless fate."
Recurring themes.
Master–disciple relationship.
Many commentators have noted the frequent occurrence in Kurosawa's work of the complex relationship between an older and a younger man, who serve each other as master and disciple, respectively. This theme was clearly an expression of the director's life experience. "Kurosawa revered his teachers, in particular Kajiro Yamamoto, his mentor at Toho", according to Joan Mellen. "The salutary image of an older person instructing the young evokes always in Kurosawa's films high moments of pathos." The critic Tadao Sato considers the recurring character of the "master" to be a type of surrogate father, whose role it is to witness the young protagonist's moral growth and approve of it.
In his very first film, "Sanshiro Sugata", after the judo master Yano becomes the title character's teacher and spiritual guide, "the narrative cast in the form of a chronicle studying the stages of the hero's growing mastery and maturity." The master-pupil relationship in the films of the postwar era—as depicted in such works as "Drunken Angel", "Stray Dog", "Seven Samurai", "Red Beard" and "Dersu Uzala"—involves very little direct instruction, but much learning through experience and example; Stephen Prince relates this tendency to the private and nonverbal nature of the concept of Zen enlightenment.
By the time of "Kagemusha", however, according to Prince, the meaning of this relationship has changed. A thief chosen to act as the double of a great lord continues his impersonation even after his master's death: "the relationship has become spectral and is generated from beyond the grave with the master maintaining a ghostly presence. Its end is death, not the renewal of commitment to the living that typified its outcome in earlier films." However, according to the director's biographer, in his final film, "Madadayo"—which deals with a teacher and his relationship with an entire group of ex-pupils—a sunnier vision of the theme emerges: "The students hold an annual party for their professor, attended by dozens of former students, now adults of varying age ... This extended sequence ... expresses, as only Kurosawa can, the simple joys of student-teacher relationships, of kinship, of being alive."
Heroic champion.
Kurosawa's is a "heroic" cinema, a series of dramas (mostly) concerned with the deeds and fates of larger-than-life heroes. Stephen Prince has identified the emergence of the unique Kurosawa protagonist with the immediate post-World War II period. The goal of the American Occupation to replace Japanese feudalism with individualism coincided with the director's artistic and social agenda: "Kurosawa welcomed the changed political climate and sought to fashion his own mature cinematic voice." The Japanese critic Tadao Sato concurs: "With defeat in World War II, many Japanese ... were dumbfounded to find that the government had lied to them and was neither just nor dependable. During this uncertain time Akira Kurosawa, in a series of first-rate films, sustained the people by his consistent assertion that the meaning of life is not dictated by the nation but something each individual should discover for himself through suffering." The filmmaker himself remarked that, during this period, "I felt that without the establishment of the self as a positive value there could be no freedom and no democracy."
The first such postwar hero was, atypically for the artist, a heroine—Yukie, played by Setsuko Hara, in "No Regrets for Our Youth". According to Prince, her "desertion of family and class background to assist a poor village, her perseverance in the face of enormous obstacles, her assumption of responsibility for her own life and for the well-being of others, and her existential loneliness ... are essential to Kurosawan heroism and make of Yukie the first coherent ... example." This "existential loneliness" is also exemplified by Dr. Sanada (Takashi Shimura) in "Drunken Angel": "Kurosawa insists that his heroes take their stand, alone, against tradition and battle for a better world, even if the path there is not clear. Separation from a corrupt social system in order to alleviate human suffering, as Sanada does, is the only honorable course."
Many commentators regard "Seven Samurai" as the ultimate expression of the artist's heroic ideal. Joan Mellen's comments are typical of this view: "Seven Samurai is above all a homage to the samurai class at its most noble ... Samurai for Kurosawa represent the best of Japanese tradition and integrity." Ironically, it is because of, not in spite of, the chaotic times of civil war depicted in the film that the seven rise to greatness. "Kurosawa locates the unexpected benefits no less than the tragedy of this historical moment. The upheaval forces samurai to channel the selflessness of their credo of loyal service into working for peasants." However, this heroism is futile because "there was already rising ... a merchant class which would supplant the warrior aristocracy." So the courage and supreme skill of the central characters will not prevent the ultimate destruction of themselves or their class.
As Kurosawa's career progressed he seemed to find it increasingly difficult to sustain the heroic ideal. As Prince notes, "Kurosawa's is an essentially tragic vision of life, and this sensibility ... impedes his efforts to realize a socially committed mode of filmmaking." Furthermore, the director's ideal of heroism is subverted by history itself: "When history is articulated as it is in "Throne of Blood", as a blind force ... heroism ceases to be a problem or a reality." According to Prince, the filmmaker's vision eventually became so bleak that he would come to view history merely as eternally recurring patterns of violence, within which the individual is depicted as not only unheroic, but utterly helpless (see "Cycles of violence" below).
Nature and weather.
Nature is a crucial element in Kurosawa's films. According to Stephen Prince, "Kurosawa's sensibility, like that of many Japanese artists, is keenly sensitive to the subtleties and beauties of season and scenery." He has never hesitated to exploit climate and weather as plot elements, to the point where they become "active participants in the drama ... The oppressive heat in "Stray Dog" and "Record of a Living Being" is omnipresent and becomes thematized as a signifier of a world disjointed by economic collapse and the atomic threat." The director himself once said, "I like hot summers, cold winters, heavy rains and snows, and I think most of my pictures show this. I like extremes because I find them most alive."
Wind is also a powerful symbol: "The persistent metaphor of Kurosawa's work is that of wind, the winds of change, of fortune and adversity." "The visually flamboyant battle [of "Yojimbo" takes place in the main street, as huge clouds of dust swirl around the combatants ... The winds that stir the dust ... have brought firearms to the town along with the culture of the West, which will end the warrior tradition."
It is also difficult not to notice the importance of rain to Kurosawa: "Rain in Kurosawa's films is never treated neutrally. When it occurs ... it is never a drizzle or a light mist but always a frenzied downpour, a driving storm." "The final battle "Seven Samurai" is a supreme spiritual and physical struggle, and it is fought in a blinding rainstorm, which enables Kurosawa to visualize an ultimate fusion of social groups ... but this climactic vision of classlessness, with typical Kurosawan ambivalence, has become a vision of horror. The battle is a vortex of swirling rain and mud ... The ultimate fusion of social identity emerges as an expression of hellish chaos."
Cycles of violence.
Beginning with "Throne of Blood" (1957), an obsession with historical cycles of inexorable savage violence—what Stephen Prince calls "the countertradition to the committed, heroic mode of Kurosawa's cinema"—first appears. According to Donald Richie, within the world of that film, "Cause and effect is the only law. Freedom does not exist." and Prince claims that its events "are inscribed in a cycle of time that infinitely repeats." (He uses as evidence the fact that Washizu's lord, unlike the kindly King Duncan of Shakespeare's play, had murdered his own lord years before to seize power, and is then murdered in turn by Washizu (the Macbeth character) for the same reason.) "The fated quality to the action of Macbeth ... was transposed by Kurosawa with a sharpened emphasis upon predetermined action and the crushing of human freedom beneath the laws of karma."
Prince claims that Kurosawa's last epics, "Kagemusha" and particularly "Ran", mark a major turning point in the director's vision of the world. In "Kagemusha", "where once the world of his films the individual could grasp events tightly and demand that they conform to his or her impulses, now the self is but the epiphenomenon of a ruthless and bloody temporal process, ground to dust beneath the weight and force of history." The following epic, "Ran", is "a relentless chronicle of base lust for power, betrayal of the father by his sons, and pervasive wars and murders." The historical setting of the film is used as "a commentary on what Kurosawa now perceives as the timelessness of human impulses toward violence and self-destruction." "History has given way to a perception of life as a wheel of endless suffering, ever turning, ever repeating", which is compared in many instances in the screenplay with hell. "Kurosawa has found hell to be both the inevitable outcome of human behavior and the appropriate visualization of his own bitterness and disappointment."
Criticisms.
In general.
In the early to mid-1950s, a number of critics belonging to the French New Wave championed the films of the older Japanese master, Kenji Mizoguchi, at the expense of Kurosawa's work. New Wave critic-filmmaker Jacques Rivette, said: "You can compare only what is comparable and that which aims high enough ... seems to be the only Japanese director who is completely Japanese and yet is also the only one that achieves a true universality, that of an individual." According to such French commentators, Mizoguchi seemed, of the two artists, the more authentically Japanese. But at least one film scholar has questioned the validity of this dichotomy between "Japanese" Mizoguchi and "Western" Kurosawa by pointing out that "Mizo" had been as influenced by Western cinema and Western culture in general as Kurosawa, and that this is reflected in his work.
A criticism frequently directed at Kurosawa's films is that the director's preoccupation with ethical and moral themes led him at times to create what some commentators regard as sentimental or naïve work. Speaking of the postwar "slice of life" drama "One Wonderful Sunday", for example, film scholar (and future politician) Audie Bock claimed that not even Kurosawa's celebrated prowess as an editor could save one particular scene from bathos: "The last sequence ... is an excruciating twelve minutes of the boy conducting an imaginary orchestra in an empty amphitheater while his girlfriend appeals directly to the camera for the viewer to join in. Angles and focal lengths change, details of leaves scattering in the wind are intercut, but nothing makes the scene go any faster."
Some controversy exists about the extent to which Kurosawa's films of the Second World War period could be considered propaganda. The cultural historian Peter B. High sees Kurosawa's wartime cinema as part of the propagandistic trend of Japan at war and as an example of many of these wartime conventions. High refers to his second film, "The Most Beautiful", as a "dark and gloomy rendition of the standard formulas of the front genre". Another controversy centers on his alleged refusal to acknowledge Japan's wartime guilt. In one of Kurosawa's last films, "Rhapsody in August", an elderly survivor of the atomic attack on Nagasaki is visited by her half-Japanese, half-American nephew, Clark (Richard Gere), who appears (at least to some viewers) to apologize, as an American, for the city's wartime destruction. The "New York Times" critic Vincent Canby wrote about this film: "A lot of people at Cannes were outraged that the film makes no mention of Pearl Harbor and Japan's atrocities in China ... If Clark can apologize for bombing Nagasaki, why can't Granny apologize for the raid on Pearl Harbor?"
A number of critics have reacted negatively to the female characters in Kurosawa's movies. Joan Mellen, in her examination of this subject, has maintained that, by the time of "Red Beard" (1965), "women in Kurosawa have become not only unreal and incapable of kindness, but totally bereft of autonomy, whether physical, intellectual, or emotional ... Women at their best may only imitate the truths men discover." Kurosawa scholar Stephen Prince concurs with Mellen's view, though less censoriously: "Unlike a male-oriented director like Sam Peckinpah, Kurosawa is not hostile to women, but his general lack of interest in them should be regarded as a major limitation of his work."
In Japan.
In Japan, both critics and other filmmakers have sometimes accused his work of elitism, because of his focus on exceptional, heroic individuals and groups of men. In her commentary on the deluxe DVD edition of "Seven Samurai", Joan Mellen maintains that certain shots of the samurai characters Kambei and Kyuzo, which to her reveal Kurosawa "privileging" these samurai, "support the argument voiced by several Japanese critics that Kurosawa was an elitist ... Kurosawa was hardly a progressive director, they argued, since his peasants could not discover among their own ranks leaders who might rescue the village. Instead, justifying the inequitable class structure of their society and ours, the peasants must rely on the aristocracy, the upper class, and in particular samurai, to ensure their survival ... Kurosawa defended himself against this charge in his interview with me. 'I wanted to say that after everything the peasants were the stronger, closely clinging to the earth ... It was the samurai who were weak because they were being blown by the winds of time.
Because of Kurosawa's popularity with European and American audiences from the early 1950s onward, he has not escaped the charge of deliberately catering to the tastes of Westerners to achieve or maintain that popularity. Joan Mellen, recording the violently negative reaction (in the 1970s) of the left-wing director Nagisa Oshima to Kurosawa and his work, states: "That Kurosawa had brought Japanese film to a Western audience meant Oshima that he must be pandering to Western values and politics." Kurosawa always strongly denied pandering to Western tastes: "He has never catered to a foreign audience" writes Audie Bock, "and has condemned those who do."
Kurosawa was often criticized by his countrymen for perceived "arrogant" behavior. It was in Japan that the (initially) disparaging nickname "Kurosawa Tennō"—"The Emperor Kurosawa"—was coined. "Like tennō", Yoshimoto claimed, "Kurosawa is said to cloister himself in his own small world, which is completely cut off from the everyday reality of the majority of Japanese. The nickname tennō is used in this sense to create an image of Kurosawa as a director who abuses his power solely for the purpose of self-indulgence."
Worldwide impact.
Reputation among filmmakers.
Many celebrated directors have been influenced by Kurosawa and/or have expressed admiration for his work. The filmmakers cited below are grouped according to three categories: (a) those who, like Kurosawa himself, established international critical reputations in the 1950s and early 1960s; (b) the so-called "New Hollywood" directors, that is, American moviemakers who, for the most part, established their reputations in the early to mid-1970s; and (c) other Asian directors.
Ingmar Bergman called his own film "The Virgin Spring" "touristic, a lousy imitation of Kurosawa", and added, "At that time my admiration for the Japanese cinema was at its height. I was almost a samurai myself!" Federico Fellini in an interview declared the director "the greatest living example of all that an author of the cinema should be"—despite admitting to having seen only one of his films, "Seven Samurai". Roman Polanski in 1965 cited Kurosawa as one of his three favorite filmmakers (with Fellini and Orson Welles), singling out "Seven Samurai", "Throne of Blood" and "The Hidden Fortress" for praise. Bernardo Bertolucci considered the Japanese master's influence to be seminal: "Kurosawa's movies and "La Dolce Vita" of Fellini are the things that pushed me, sucked me into being a film director."
Kurosawa's New Hollywood admirers have included Robert Altman, Francis Ford Coppola, Steven Spielberg, Martin Scorsese, George Lucas, and John Milius. Robert Altman, when he first saw "Rashomon" (during the period when he worked regularly in television rather than feature films), was so impressed by its cinematographer's achievement of shooting several shots with the camera aimed directly at the sun—allegedly it was the first film in which this was done successfully—that he claims he was inspired the very next day to begin incorporating shots of the sun into his television work. It was Coppola who said of Kurosawa, "One thing that distinguishes is that he didn't make one masterpiece or two masterpieces. He made, you know, "eight" masterpieces." Both Spielberg and Scorsese have praised the older man's role as teacher and role model—as a "sensei", to use the Japanese term. Spielberg has declared, "I have learned more from him than from almost any other filmmaker on the face of the earth", while Scorsese remarked, "Let me say it simply: Akira Kurosawa was my master, and ... the master of so many other filmmakers over the years." As already noted above, several of these moviemakers were also instrumental in helping Kurosawa obtain financing for his late films: Lucas and Coppola served as co-producers on "Kagemusha", while the Spielberg name, lent to the 1990 production, "Dreams", helped bring that picture to fruition.
As the first Asian filmmaker to achieve international prominence, Kurosawa has naturally served as an inspiration for other Asian "auteurs". Of "Rashomon", the most famous director of India, Satyajit Ray, said: "The effect of the film on me first seeing it in Calcutta in 1952 was electric. I saw it three times on consecutive days, and wondered each time if there was another film anywhere which gave such sustained and dazzling proof of a director's command over every aspect of film making." Other Asian admirers include the Japanese actor and director Takeshi Kitano, Hong Kong filmmaker John Woo and mainland Chinese director Zhang Yimou, who called Kurosawa "the quintessential Asian director".
Legacy.
Kurosawa Production Co., established in 1959, continues to oversee much of Kurosawa's legacy. The director's son, Hisao Kurosawa, is the current head of the company. Its American subsidiary, Kurosawa Enterprises, is located in Los Angeles. Rights to Kurosawa's works are held by Kurosawa Production and the film studios under which he worked, most notably Toho. Kurosawa Production works closely with the Akira Kurosawa Foundation, established in December 2003 and also run by Hisao Kurosawa. The foundation organizes an annual short film competition and spearheads Kurosawa-related projects, including a recently shelved one to build a memorial museum for the director.
In 1981, the Kurosawa Film Studio was opened in Yokohama; two additional locations have since been launched in Japan. A large collection of archive material, including scanned screenplays, photos and news articles, has been made available through the Akira Kurosawa Digital Archive, a Japanese website maintained by Ryukoku University Digital Archives Research Center in collaboration with Kurosawa Production. Anaheim University's Akira Kurosawa School of Film was launched in spring 2009 with the backing of Kurosawa Production. It offers online programs in digital film making, with headquarters in Anaheim and a learning center in Tokyo.
Two film awards have also been named in Kurosawa's honor. The Akira Kurosawa Award for Lifetime Achievement in Film Directing is awarded during the San Francisco International Film Festival, while the Akira Kurosawa Award is given during the Tokyo International Film Festival. In commemoration of the 100th anniversary of Kurosawa's birth in 2010, a project called AK100 was launched in 2008. The AK100 Project aims to "expose young people who are the representatives of the next generation, and all people everywhere, to the light and spirit of Akira Kurosawa and the wonderful world he created."
Anaheim University in cooperation with the Kurosawa Family established the Anaheim University Akira Kurosawa School of Film to offer online and blended learning programs on Akira Kurosawa and filmmaking.
Filmography.
As writer.
Work with Akira Kurosawa credited as writer, by year:

</doc>
<doc id="874" url="https://en.wikipedia.org/wiki?curid=874" title="Ancient Egypt">
Ancient Egypt

Ancient Egypt was a civilization of ancient Northeastern Africa, concentrated along the lower reaches of the Nile River in what is now the modern country of Egypt. It is one of six civilizations to arise independently. Egyptian civilization followed prehistoric Egypt and coalesced around 3150 BC (according to conventional Egyptian chronology) with the political unification of Upper and Lower Egypt under the first pharaoh Narmer (commonly referred to as Menes). The history of ancient Egypt occurred in a series of stable Kingdoms, separated by periods of relative instability known as Intermediate Periods: the Old Kingdom of the Early Bronze Age, the Middle Kingdom of the Middle Bronze Age and the New Kingdom of the Late Bronze Age.
Egypt reached the pinnacle of its power in the New Kingdom, during the Ramesside period, where it rivalled the Hittite Empire, Assyrian Empire and Mitanni Empire, after which it entered a period of slow decline. Egypt was invaded or conquered by a succession of foreign powers, such as the Canaanites/Hyksos, Libyans, the Nubians, the Assyrians, Babylonians, the Achaemenid Persians, and the Macedonians in the Third Intermediate Period and the Late Period of Egypt. In the aftermath of Alexander the Great's death, one of his generals, Ptolemy Soter, established himself as the new ruler of Egypt. This Greek Ptolemaic Kingdom ruled Egypt until 30 BC, when, under Cleopatra, it fell to the Roman Empire and became a Roman province.
The success of ancient Egyptian civilization came partly from its ability to adapt to the conditions of the Nile River valley for agriculture. The predictable flooding and controlled irrigation of the fertile valley produced surplus crops, which supported a more dense population, and social development and culture. With resources to spare, the administration sponsored mineral exploitation of the valley and surrounding desert regions, the early development of an independent writing system, the organization of collective construction and agricultural projects, trade with surrounding regions, and a military intended to defeat foreign enemies and assert Egyptian dominance. Motivating and organizing these activities was a bureaucracy of elite scribes, religious leaders, and administrators under the control of a pharaoh, who ensured the cooperation and unity of the Egyptian people in the context of an elaborate system of religious beliefs.
The many achievements of the ancient Egyptians include the quarrying, surveying and construction techniques that supported the building of monumental pyramids, temples, and obelisks; a system of mathematics, a practical and effective system of medicine, irrigation systems and agricultural production techniques, the first known planked boats, Egyptian faience and glass technology, new forms of literature, and the earliest known peace treaty, made with the Hittites. Egypt left a lasting legacy. Its art and architecture were widely copied, and its antiquities carried off to far corners of the world. Its monumental ruins have inspired the imaginations of travelers and writers for centuries. A new-found respect for antiquities and excavations in the early modern period by Europeans and Egyptians led to the scientific investigation of Egyptian civilization and a greater appreciation of its cultural legacy.
History.
The Nile has been the lifeline of its region for much of human history. The fertile floodplain of the Nile gave humans the opportunity to develop a settled agricultural economy and a more sophisticated, centralized society that became a cornerstone in the history of human civilization. Nomadic modern human hunter-gatherers began living in the Nile valley through the end of the Middle Pleistocene some 120,000 years ago. By the late Paleolithic period, the arid climate of Northern Africa became increasingly hot and dry, forcing the populations of the area to concentrate along the river region.
Predynastic period.
In Predynastic and Early Dynastic times, the Egyptian climate was much less arid than it is today. Large regions of Egypt were covered in treed savanna and traversed by herds of grazing ungulates. Foliage and fauna were far more prolific in all environs and the Nile region supported large populations of waterfowl. Hunting would have been common for Egyptians, and this is also the period when many animals were first domesticated.
By about 5500 BC, small tribes living in the Nile valley had developed into a series of cultures demonstrating firm control of agriculture and animal husbandry, and identifiable by their pottery and personal items, such as combs, bracelets, and beads. The largest of these early cultures in upper (Southern) Egypt was the Badari, which probably originated in the Western Desert; it was known for its high quality ceramics, stone tools, and its use of copper.
The Badari was followed by the Amratian (Naqada I) and Gerzeh (Naqada II) cultures, which brought a number of technological improvements. As early as the Naqada I Period, predynastic Egyptians imported obsidian from Ethiopia, used to shape blades and other objects from flakes. In Naqada II times, early evidence exists of contact with the Near East, particularly Canaan and the Byblos coast. Over a period of about 1,000 years, the Naqada culture developed from a few small farming communities into a powerful civilization whose leaders were in complete control of the people and resources of the Nile valley. Establishing a power center at Hierakonpolis, and later at Abydos, Naqada III leaders expanded their control of Egypt northwards along the Nile. They also traded with Nubia to the south, the oases of the western desert to the west, and the cultures of the eastern Mediterranean and Near East to the east. Royal Nubian burials at Qustul produced artifacts bearing the oldest-known examples of Egyptian dynastic symbols, such as the white crown of Egypt and falcon.
The Naqada culture manufactured a diverse selection of material goods, reflective of the increasing power and wealth of the elite, as well as societal personal-use items, which included combs, small statuary, painted pottery, high quality decorative stone vases, cosmetic palettes, and jewelry made of gold, lapis, and ivory. They also developed a ceramic glaze known as faience, which was used well into the Roman Period to decorate cups, amulets, and figurines. During the last predynastic phase, the Naqada culture began using written symbols that eventually were developed into a full system of hieroglyphs for writing the ancient Egyptian language.
Early Dynastic Period (c. 3050–2686 BC).
The transition to a unified state happened more gradually than ancient Egyptian writers represented, and there is no contemporary record of Menes. Some scholars now believe, however, that the mythical Menes may have been the pharaoh Narmer, who is depicted wearing royal regalia on the ceremonial "Narmer Palette," in a symbolic act of unification. In the Early Dynastic Period about 3150 BC, the first of the Dynastic pharaohs solidified control over lower Egypt by establishing a capital at Memphis, from which he could control the labour force and agriculture of the fertile delta region, as well as the lucrative and critical trade routes to the Levant. The increasing power and wealth of the pharaohs during the early dynastic period was reflected in their elaborate mastaba tombs and mortuary cult structures at Abydos, which were used to celebrate the deified pharaoh after his death. The strong institution of kingship developed by the pharaohs served to legitimize state control over the land, labour, and resources that were essential to the survival and growth of ancient Egyptian civilization.
[[File:NarmerPalette ROM-gamma.jpg|thumb|The Narmer Palette depicts the unification
of the Two Lands.]]
Old Kingdom (2686–2181 BC).
Along with the rising importance of a central administration arose a new class of educated scribes and officials who were granted estates by the pharaoh in payment for their services. Pharaohs also made land grants to their mortuary cults and local temples, to ensure that these institutions had the resources to worship the pharaoh after his death. Scholars believe that five centuries of these practices slowly eroded the economic power of the pharaoh, and that the economy could no longer afford to support a large centralized administration. As the power of the pharaoh diminished, regional governors called nomarchs began to challenge the supremacy of the pharaoh. This, coupled with severe droughts between 2200 and 2150 BC, is assumed to have caused the country to enter the 140-year period of famine and strife known as the First Intermediate Period.
First Intermediate Period (2181–1991 BC).
After Egypt's central government collapsed at the end of the Old Kingdom, the administration could no longer support or stabilize the country's economy. Regional governors could not rely on the king for help in times of crisis, and the ensuing food shortages and political disputes escalated into famines and small-scale civil wars. Yet despite difficult problems, local leaders, owing no tribute to the pharaoh, used their new-found independence to establish a thriving culture in the provinces. Once in control of their own resources, the provinces became economically richer—which was demonstrated by larger and better burials among all social classes. In bursts of creativity, provincial artisans adopted and adapted cultural motifs formerly restricted to the royalty of the Old Kingdom, and scribes developed literary styles that expressed the optimism and originality of the period.
Free from their loyalties to the pharaoh, local rulers began competing with each other for territorial control and political power. By 2160 BC, rulers in Herakleopolis controlled Lower Egypt in the north, while a rival clan based in Thebes, the Intef family, took control of Upper Egypt in the south. As the Intefs grew in power and expanded their control northward, a clash between the two rival dynasties became inevitable. Around 2055 BC the northern Theban forces under Nebhepetre Mentuhotep II finally defeated the Herakleopolitan rulers, reuniting the Two Lands. They inaugurated a period of economic and cultural renaissance known as the Middle Kingdom.
Middle Kingdom (2134–1690 BC).
The pharaohs of the Middle Kingdom restored the country's prosperity and stability, thereby stimulating a resurgence of art, literature, and monumental building projects. Mentuhotep II and his Eleventh Dynasty successors ruled from Thebes, but the vizier Amenemhat I, upon assuming kingship at the beginning of the Twelfth Dynasty around 1985 BC, shifted the nation's capital to the city of Itjtawy, located in Faiyum. From Itjtawy, the pharaohs of the Twelfth Dynasty undertook a far-sighted land reclamation and irrigation scheme to increase agricultural output in the region. Moreover, the military reconquered territory in Nubia that was rich in quarries and gold mines, while laborers built a defensive structure in the Eastern Delta, called the "Walls-of-the-Ruler", to defend against foreign attack.
With the pharaohs' having secured military and political security and vast agricultural and mineral wealth, the nation's population, arts, and religion flourished. In contrast to elitist Old Kingdom attitudes towards the gods, the Middle Kingdom experienced an increase in expressions of personal piety and what could be called a democratization of the afterlife, in which all people possessed a soul and could be welcomed into the company of the gods after death. Middle Kingdom literature featured sophisticated themes and characters written in a confident, eloquent style. The relief and portrait sculpture of the period captured
subtle, individual details that reached new heights of technical perfection.
The last great ruler of the Middle Kingdom, Amenemhat III, allowed Semitic-speaking Canaanite settlers from the Near East into the delta region to provide a sufficient labour force for his especially active mining and building campaigns. These ambitious building and mining activities, however, combined with severe Nile floods later in his reign, strained the economy and precipitated the slow decline into the Second Intermediate Period during the later Thirteenth and Fourteenth dynasties. During this decline, the Canaanite settlers began to seize control of the delta region, eventually coming to power in Egypt as the Hyksos.
Second Intermediate Period (1674–1549 BC) and the Hyksos.
Around 1785 BC, as the power of the Middle Kingdom pharaohs weakened, a Western Asian people called the Hyksos had already settled in the Eastern Delta town of Avaris, seized control of Egypt, and forced the central government to retreat to Thebes. The pharaoh was treated as a vassal and expected to pay tribute. The Hyksos ("foreign rulers") retained Egyptian models of government and identified as pharaohs, thus integrating Egyptian elements into their culture. They and other invaders introduced new tools of warfare into Egypt, most notably the composite bow and the horse-drawn chariot.
After their retreat, the native Theban kings found themselves trapped between the Canaanite Hyksos ruling the north and the Hyksos' Nubian allies, the Kushites, to the south of Egypt. After years of vassalage, Thebes gathered enough strength to challenge the Hyksos in a conflict that lasted more than 30 years, until 1555 BC. The pharaohs Seqenenre Tao II and Kamose were ultimately able to defeat the Nubians to the south of Egypt, but failed to defeat the Hyksos. That task fell to Kamose's successor, Ahmose I, who successfully waged a series of campaigns that permanently eradicated the Hyksos' presence in Egypt. He established a new dynasty. In the New Kingdom that followed, the military became a central priority for the pharaohs seeking to expand Egypt's borders and attempting to gain mastery of the Near East.
New Kingdom (1549–1069 BC).
The New Kingdom pharaohs established a period of unprecedented prosperity by securing their borders and strengthening diplomatic ties with their neighbours, including the Mitanni Empire, Assyria, and Canaan. Military campaigns waged under Tuthmosis I and his grandson Tuthmosis III extended the influence of the pharaohs to the largest empire Egypt had ever seen. Between their reigns, Hatshepsut generally promoted peace and restored trade routes lost during the Hyksos occupation, as well as expanding to new regions. When Tuthmosis III died in 1425 BC, Egypt had an empire extending from Niya in north west Syria to the fourth waterfall of the Nile in Nubia, cementing loyalties and opening access to critical imports such as bronze and wood.
The New Kingdom pharaohs began a large-scale building campaign to promote the god Amun, whose growing cult was based in Karnak. They also constructed monuments to glorify their own achievements, both real and imagined. The Karnak temple is the largest Egyptian temple ever built. The pharaoh Hatshepsut used such hyperbole and grandeur during her reign of almost twenty-two years. Her reign was very successful, marked by an extended period of peace and wealth-building, trading expeditions to Punt, restoration of foreign trade networks, and great building projects, including an elegant mortuary temple that rivaled the Greek architecture of a thousand years later, a colossal pair of obelisks, and a chapel at Karnak. Despite her achievements, Amenhotep II, the heir to Hatshepsut's nephew-stepson Tuthmosis III, sought to erase her legacy near the end of his father's reign and throughout his, touting many of her accomplishments as his. He also tried to change many established traditions that had developed over the centuries, which some suggest was a futile attempt to prevent other women from becoming pharaoh and to curb their influence in the kingdom.
Around 1350 BC, the stability of the New Kingdom seemed threatened further when Amenhotep IV ascended the throne and instituted a series of radical and chaotic reforms. Changing his name to Akhenaten, he touted the previously obscure sun deity Aten as the supreme deity, suppressed the worship of most other deities, and attacked the power of the temple that had become dominated by the priests of Amun in Thebes, whom he saw as corrupt. Moving the capital to the new city of Akhetaten (modern-day Amarna), Akhenaten turned a deaf ear to events in the Near East (where the Hittites, Mitanni, and Assyrians were vying for control). He was devoted to his new religion and artistic style. After his death, the cult of the Aten was quickly abandoned, the priests of Amun soon regained power and returned the capital to Thebes. Under their influence the subsequent pharaohs Tutankhamun, Ay, and Horemheb worked to erase all mention of Akhenaten's heresy, now known as the Amarna Period.
Around 1279 BC, Ramesses II, also known as Ramesses the Great, ascended the throne, and went on to build more temples, erect more statues and obelisks, and sire more children than any other pharaoh in history. A bold military leader, Ramesses II led his army against the Hittites in the Battle of Kadesh (in modern Syria) and, after fighting to a stalemate, finally agreed to the first recorded peace treaty, around 1258 BC. With both the Egyptians and Hittite Empire proving unable to gain the upper hand over one another, and both powers also fearful of the expanding Middle Assyrian Empire, Egypt withdrew from much of the Near East. The Hittites were thus left to compete unsuccessfully with the powerful Assyrians and the newly arrived Phrygians.
Egypt's wealth, however, made it a tempting target for invasion, particularly by the Libyan Berbers to the west, and the Sea Peoples, a conjectured confederation of seafarers from the Aegean. Initially, the military was able to repel these invasions, but Egypt eventually lost control of its remaining territories in southern Caanan, much of it falling to the Assyrians. The effects of external threats were exacerbated by internal problems such as corruption, tomb robbery, and civil unrest. After regaining their power, the high priests at the temple of Amun in Thebes accumulated vast tracts of land and wealth, and their expanded power splintered the country during the Third Intermediate Period.
Third Intermediate Period (1069–653 BC).
Following the death of Ramesses XI in 1078 BC, Smendes assumed authority over the northern part of Egypt, ruling from the city of Tanis. The south was effectively controlled by the High Priests of Amun at Thebes, who recognized Smendes in name only. During this time, Berber tribes from what was later to be called Libya had been settling in the western delta, and the chieftains of these settlers began increasing their autonomy. Libyan princes took control of the delta under Shoshenq I in 945 BC, founding the Libyan Berber, or Bubastite, dynasty that ruled for some 200 years. Shoshenq also gained control of southern Egypt by placing his family members in important priestly positions.
In the mid-ninth century BC, Egypt made a failed attempt to once more gain a foothold in Western Asia. Osorkon II of Egypt, along with a large alliance of nations and peoples, including Persia, Israel, Hamath, Phoenicia/Caanan, the Arabs, Arameans, and neo Hittites among others, engaged in the Battle of Karkar against the powerful Assyrian king Shalmaneser III in 853 BC. However, this coalition of powers failed and the Neo Assyrian Empire continued to dominate Western Asia.
Libyan Berber control began to erode as a rival native dynasty in the delta arose under Leontopolis. Also, the Nubians of the Kushites threatened Egypt from the lands to the south.
Drawing on millennia of interaction (trade, acculturation, occupation, assimilation, and war) with Egypt, the Kushite king Piye left his Nubian capital of Napata and invaded Egypt around 727 BC. Piye easily seized control of Thebes and eventually the Nile Delta. He recorded the episode on his stela of victory. Piye set the stage for subsequent Twenty-fifth dynasty pharaohs, such as Taharqa, to reunite the "Two lands" of Northern and Southern Egypt. The Nile valley empire was as large as it had been since the New Kingdom.
The Twenty-fifth dynasty ushered in a renaissance period for ancient Egypt. Religion, the arts, and architecture were restored to their glorious Old, Middle, and New Kingdom forms. Pharaohs, such as Taharqa, built or restored temples and monuments throughout the Nile valley, including at Memphis, Karnak, Kawa, Jebel Barkal, etc. It was during the Twenty-fifth dynasty that there was the first widespread construction of pyramids (many in modern Sudan) in the Nile Valley since the Middle Kingdom.
Piye made various unsuccessful attempts to extend Egyptian influence in the Near East, then controlled by Assyria. In 720 BC, he sent an army in support of a rebellion against Assyria, which was taking place in Philistia and Gaza. However, Piye was defeated by Sargon II and the rebellion failed. In 711 BC, Piye again supported a revolt against the Assyrians by the Israelites of Ashdod and was once again defeated by the Assyrian king Sargon II. Subsequently, Piye was forced from the Near East.
From the 10th century BC onwards, Assyria fought for control of the southern Levant. Frequently, cities and kingdoms of the southern Levant appealed to Egypt for aide in their struggles against the powerful Assyrian army. Taharqa enjoyed some initial success in his attempts to regain a foothold in the Near East. Taharqa aided the Judean King Hezekiah when Hezekiah and Jerusalem was besieged by the Assyrian king, Sennacherib. Scholars disagree on the primary reason for Assyria's abandonment of their siege on Jerusalem. Reasons for the Assyrian withdrawal range from conflict with the Egyptian/Kushite army to divine intervention to surrender to disease. Henry Aubin argues that the Kushite/Egyptian army saved Jerusalem from the Assyrians and prevented the Assyrians from returning to capture Jerusalem for the remainder of Sennacherib's life (20 years). Some argue that disease was the primary reason for failing to actually take the city, however Senacherib's annals claim Judah was forced into tribute regardless.
Sennacherib had been murdered by his own sons for destroying the rebellious city of Babylon, a city sacred to all Mesopotamians, the Assyrians included. In 674 BC Esarhaddon launched a preliminary incursion into Egypt, however this attempt was repelled by Taharqa. However, in 671 BC, Esarhaddon launched a full-scale invasion. Part of his army stayed behind to deal with rebellions in Phoenicia, and Israel. The remainder went south to Rapihu, then crossed the Sinai, and entered Egypt. Esarhaddon decisively defeated Taharqa, took Memphis, Thebes and all the major cities of Egypt, and Taharqa was chased back to his Nubian homeland. Esarhaddon now called himself "king of Egypt, "Patros", and Kush", and returned with rich booty from the cities of the delta; he erected a victory stele at this time, and paraded the captive Prince Ushankhuru, the son of Taharqa in Nineveh. Esarhaddon stationed a small army in northern Egypt and describes how "All Ethiopians (read Nubians/Kushites) I deported from Egypt, leaving not one left to do homage to me". He installed native Egyptian princes throughout the land to rule on his behalf. The conquest by Esarhaddon effectively marked the end of the short lived Kushite Empire.
However, the native Egyptian rulers installed by Esarhaddon were unable to retain full control of the whole country for long. Two years later, Taharqa returned from Nubia and seized control of a section of southern Egypt as far north as Memphis. Esarhaddon prepared to return to Egypt and once more eject Taharqa, however he fell ill and died in his capital, Nineveh, before he left Assyria. His successor, Ashurbanipal, sent an Assyrian general named Sha-Nabu-shu with a small, but well trained army, which conclusively defeated Taharqa at Memphis and once more drove him from Egypt. Taharqa died in Nubia two years later.
His successor, Tanutamun, also made a failed attempt to regain Egypt for Nubia. He successfully defeated Necho, the native Egyptian puppet ruler installed by Ashurbanipal, taking Thebes in the process. The Assyrians then sent a large army southwards. Tantamani (Tanutamun) was heavily routed and fled back to Nubia. The Assyrian army sacked Thebes to such an extent it never truly recovered. A native ruler, Psammetichus I was placed on the throne, as a vassal of Ashurbanipal, and the Nubians were never again to pose a threat to either Assyria or Egypt.
Late Period (672–332 BC).
With no permanent plans for conquest, the Assyrians left control of Egypt to a series of vassals who became known as the Saite kings of the Twenty-sixth Dynasty. By 653 BC, the Saite king Psamtik I (taking advantage of the fact that Assyria was involved in a fierce war conquering Elam and that few Assyrian troops were stationed in Egypt) was able to free Egypt relatively peacefully from Assyrian vassalage with the help of Lydian and Greek mercenaries, the latter of whom were recruited to form Egypt's first navy. Psamtik and his successors however were careful to maintain peaceful relations with Assyria. Greek influence expanded greatly as the city of Naukratis became the home of Greeks in the delta.
In 609 BC Necho II went to war with Babylonia, the Chaldeans, the Medians and the Scythians in an attempt to save Assyria, which after a brutal civil war was being overrun by this coalition of powers. However, the attempt to save Egypt's former masters failed. The Egyptians delayed intervening too long, and Nineveh had already fallen and King Sin-shar-ishkun was dead by the time Necho II sent his armies northwards. However, Necho easily brushed aside the Israelite army under King Josiah but he and the Assyrians then lost a battle at Harran to the Babylonians, Medes and Scythians. Necho II and Ashur-uballit II of Assyria were finally defeated at Carchemish in Aramea (modern Syria) in 605 BC. The Egyptians remained in the area for some decades, struggling with the Babylonian kings Nabopolassar and Nebuchadnezzar II for control of portions of the former Assyrian Empire in The Levant. However, they were eventually driven back into Egypt, and Nebuchadnezzar II even briefly invaded Egypt itself in 567 BC. The Saite kings based in the new capital of Sais witnessed a brief but spirited resurgence in the economy and culture, but in 525 BC, the powerful Persians, led by Cambyses II, began their conquest of Egypt, eventually capturing the pharaoh Psamtik III at the battle of Pelusium. Cambyses II then assumed the formal title of pharaoh, but ruled Egypt from his home of Susa in Persia (modern Iran), leaving Egypt under the control of a satrapy. A few temporarily successful revolts against the Persians marked the fifth century BC, but Egypt was never able to permanently overthrow the Persians.
Following its annexation by Persia, Egypt was joined with Cyprus and Phoenicia (modern Lebanon) in the sixth satrapy of the Achaemenid Persian Empire. This first period of Persian rule over Egypt, also known as the Twenty-seventh dynasty, ended after more than one-hundred years in 402 BC, and from 380–343 BC the Thirtieth Dynasty ruled as the last native royal house of dynastic Egypt, which ended with the kingship of Nectanebo II. A brief restoration of Persian rule, sometimes known as the Thirty-first Dynasty, began in 343 BC, but shortly after, in 332 BC, the Persian ruler Mazaces handed Egypt over to the Macedonian ruler Alexander the Great without a fight.
Ptolemaic Period.
In 332 BC, Alexander the Great conquered Egypt with little resistance from the Persians and was welcomed by the Egyptians as a deliverer. The administration established by Alexander's successors, the Macedonian Ptolemaic Kingdom, was based on an Egyptian model and based in the new capital city of Alexandria. The city showcased the power and prestige of Hellenistic rule, and became a seat of learning and culture, centered at the famous Library of Alexandria. The Lighthouse of Alexandria lit the way for the many ships that kept trade flowing through the city—as the Ptolemies made commerce and revenue-generating enterprises, such as papyrus manufacturing, their top priority.
Hellenistic culture did not supplant native Egyptian culture, as the Ptolemies supported time-honored traditions in an effort to secure the loyalty of the populace. They built new temples in Egyptian style, supported traditional cults, and portrayed themselves as pharaohs. Some traditions merged, as Greek and Egyptian gods were syncretized into composite deities, such as Serapis, and classical Greek forms of sculpture influenced traditional Egyptian motifs. Despite their efforts to appease the Egyptians, the Ptolemies were challenged by native rebellion, bitter family rivalries, and the powerful mob of Alexandria that formed after the death of Ptolemy IV. In addition, as Rome relied more heavily on imports of grain from Egypt, the Romans took great interest in the political situation in the country. Continued Egyptian revolts, ambitious politicians, and powerful Syriac opponents from the Near East made this situation unstable, leading Rome to send forces to secure the country as a province of its empire.
Roman Period.
Egypt became a province of the Roman Empire in 30 BC, following the defeat of Marc Antony and Ptolemaic Queen Cleopatra VII by Octavian (later Emperor Augustus) in the Battle of Actium. The Romans relied heavily on grain shipments from Egypt, and the Roman army, under the control of a prefect appointed by the Emperor, quelled rebellions, strictly enforced the collection of heavy taxes, and prevented attacks by bandits, which had become a notorious problem during the period. Alexandria became an increasingly important center on the trade route with the orient, as exotic luxuries were in high demand in Rome.
Although the Romans had a more hostile attitude than the Greeks towards the Egyptians, some traditions such as mummification and worship of the traditional gods continued. The art of mummy portraiture flourished, and some Roman emperors had themselves depicted as pharaohs, though not to the extent that the Ptolemies had. The former lived outside Egypt and did not perform the ceremonial functions of Egyptian kingship. Local administration became Roman in style and closed to native Egyptians.
From the mid-first century AD, Christianity took root in Egypt and it was originally seen as another cult that could be accepted. However, it was an uncompromising religion that sought to win converts from Egyptian Religion and Greco-Roman religion and threatened popular religious traditions. This led to the persecution of converts to Christianity, culminating in the great purges of Diocletian starting in 303, but eventually Christianity won out. In 391 the Christian Emperor Theodosius introduced legislation that banned pagan rites and closed temples. Alexandria became the scene of great anti-pagan riots with public and private religious imagery destroyed. As a consequence, Egypt's native religious culture was continually in decline. While the native population certainly continued to speak their language, the ability to read hieroglyphic writing slowly disappeared as the role of the Egyptian temple priests and priestesses diminished. The temples themselves were sometimes converted to churches or abandoned to the desert.
Government and economy.
Administration and commerce.
The pharaoh was the absolute monarch of the country and, at least in theory, wielded complete control of the land and its resources. The king was the supreme military commander and head of the government, who relied on a bureaucracy of officials to manage his affairs. In charge of the administration was his second in command, the vizier, who acted as the king's representative and coordinated land surveys, the treasury, building projects, the legal system, and the archives. At a regional level, the country was divided into as many as 42 administrative regions called nomes each governed by a nomarch, who was accountable to the vizier for his jurisdiction. The temples formed the backbone of the economy. Not only were they houses of worship, but were also responsible for collecting and storing the nation's wealth in a system of granaries and treasuries administered by overseers, who redistributed grain and goods.
Much of the economy was centrally organized and strictly controlled. Although the ancient Egyptians did not use coinage until the Late period, they did use a type of money-barter system, with standard sacks of grain and the "deben", a weight of roughly of copper or silver, forming a common denominator. Workers were paid in grain; a simple laborer might earn 5½ sacks (200 kg or 400 lb) of grain per month, while a foreman might earn 7½ sacks (250 kg or 550 lb). Prices were fixed across the country and recorded in lists to facilitate trading; for example a shirt cost five copper deben, while a cow cost 140 deben. Grain could be traded for other goods, according to the fixed price list. During the fifth century BC coined money was introduced into Egypt from abroad. At first the coins were used as standardized pieces of precious metal rather than true money, but in the following centuries international traders came to rely on coinage.
Social status.
Egyptian society was highly stratified, and social status was expressly displayed. Farmers made up the bulk of the population, but agricultural produce was owned directly by the state, temple, or noble family that owned the land. Farmers were also subject to a labor tax and were required to work on irrigation or construction projects in a corvée system. Artists and craftsmen were of higher status than farmers, but they were also under state control, working in the shops attached to the temples and paid directly from the state treasury. Scribes and officials formed the upper class in ancient Egypt, known as the "white kilt class" in reference to the bleached linen garments that served as a mark of their rank. The upper class prominently displayed their social status in art and literature. Below the nobility were the priests, physicians, and engineers with specialized training in their field. Slavery was known in ancient Egypt, but the extent and prevalence of its practice are unclear.
The ancient Egyptians viewed men and women, including people from all social classes except slaves, as essentially equal under the law, and even the lowliest peasant was entitled to petition the vizier and his court for redress. Although, slaves were mostly used as indentured servants. They were able to buy and sell, or work their way to freedom or nobility, and usually were treated by doctors in the workplace. Both men and women had the right to own and sell property, make contracts, marry and divorce, receive inheritance, and pursue legal disputes in court. Married couples could own property jointly and protect themselves from divorce by agreeing to marriage contracts, which stipulated the financial obligations of the husband to his wife and children should the marriage end. Compared with their counterparts in ancient Greece, Rome, and even more modern places around the world, ancient Egyptian women had a greater range of personal choices and opportunities for achievement. Women such as Hatshepsut and Cleopatra VI even became pharaohs, while others wielded power as Divine Wives of Amun. Despite these freedoms, ancient Egyptian women did not often take part in official roles in the administration, served only secondary roles in the temples, and were not as likely to be as educated as men. 
Legal system.
The head of the legal system was officially the pharaoh, who was responsible for enacting laws, delivering justice, and maintaining law and order, a concept the ancient Egyptians referred to as Ma'at. Although no legal codes from ancient Egypt survive, court documents show that Egyptian law was based on a common-sense view of right and wrong that emphasized reaching agreements and resolving conflicts rather than strictly adhering to a complicated set of statutes. Local councils of elders, known as "Kenbet" in the New Kingdom, were responsible for ruling in court cases involving small claims and minor disputes. More serious cases involving murder, major land transactions, and tomb robbery were referred to the "Great Kenbet", over which the vizier or pharaoh presided. Plaintiffs and defendants were expected to represent themselves and were required to swear an oath that they had told the truth. In some cases, the state took on both the role of prosecutor and judge, and it could torture the accused with beatings to obtain a confession and the names of any co-conspirators. Whether the charges were trivial or serious, court scribes documented the complaint, testimony, and verdict of the case for future reference.
Punishment for minor crimes involved either imposition of fines, beatings, facial mutilation, or exile, depending on the severity of the offense. Serious crimes such as murder and tomb robbery were punished by execution, carried out by decapitation, drowning, or impaling the criminal on a stake. Punishment could also be extended to the criminal's family. Beginning in the New Kingdom, oracles played a major role in the legal system, dispensing justice in both civil and criminal cases. The procedure was to ask the god a "yes" or "no" question concerning the right or wrong of an issue. The god, carried by a number of priests, rendered judgment by choosing one or the other, moving forward or backward, or pointing to one of the answers written on a piece of papyrus or an ostracon.
Agriculture.
A combination of favorable geographical features contributed to the success of ancient Egyptian culture, the most important of which was the rich fertile soil resulting from annual inundations of the Nile River. The ancient Egyptians were thus able to produce an abundance of food, allowing the population to devote more time and resources to cultural, technological, and artistic pursuits. Land management was crucial in ancient Egypt because taxes were assessed based on the amount of land a person owned.
Farming in Egypt was dependent on the cycle of the Nile River. The Egyptians recognized three seasons: "Akhet" (flooding), "Peret" (planting), and "Shemu" (harvesting). The flooding season lasted from June to September, depositing on the river's banks a layer of mineral-rich silt ideal for growing crops. After the floodwaters had receded, the growing season lasted from October to February. Farmers plowed and planted seeds in the fields, which were irrigated with ditches and canals. Egypt received little rainfall, so farmers relied on the Nile to water their crops. From March to May, farmers used sickles to harvest their crops, which were then threshed with a flail to separate the straw from the grain. Winnowing removed the chaff from the grain, and the grain was then ground into flour, brewed to make beer, or stored for later use.
The ancient Egyptians cultivated emmer and barley, and several other cereal grains, all of which were used to make the two main food staples of bread and beer. Flax plants, uprooted before they started flowering, were grown for the fibers of their stems. These fibers were split along their length and spun into thread, which was used to weave sheets of linen and to make clothing. Papyrus growing on the banks of the Nile River was used to make paper. Vegetables and fruits were grown in garden plots, close to habitations and on higher ground, and had to be watered by hand. Vegetables included leeks, garlic, melons, squashes, pulses, lettuce, and other crops, in addition to grapes that were made into wine.
Animals.
The Egyptians believed that a balanced relationship between people and animals was an essential element of the cosmic order; thus humans, animals and plants were believed to be members of a single whole. Animals, both domesticated and wild, were therefore a critical source of spirituality, companionship, and sustenance to the ancient Egyptians. Cattle were the most important livestock; the administration collected taxes on livestock in regular censuses, and the size of a herd reflected the prestige and importance of the estate or temple that owned them. In addition to cattle, the ancient Egyptians kept sheep, goats, and pigs. Poultry such as ducks, geese, and pigeons were captured in nets and bred on farms, where they were force-fed with dough to fatten them. The Nile provided a plentiful source of fish. Bees were also domesticated from at least the Old Kingdom, and they provided both honey and wax.
The ancient Egyptians used donkeys and oxen as beasts of burden, and they were responsible for plowing the fields and trampling seed into the soil. The slaughter of a fattened ox was also a central part of an offering ritual. Horses were introduced by the Hyksos in the Second Intermediate Period, and the camel, although known from the New Kingdom, was not used as a beast of burden until the Late Period. There is also evidence to suggest that elephants were briefly utilized in the Late Period, but largely abandoned due to lack of grazing land. Dogs, cats and monkeys were common family pets, while more exotic pets imported from the heart of Africa, such as lions, were reserved for royalty. Herodotus observed that the Egyptians were the only people to keep their animals with them in their houses. During the Predynastic and Late periods, the worship of the gods in their animal form was extremely popular, such as the cat goddess Bastet and the ibis god Thoth, and these animals were bred in large numbers on farms for the purpose of ritual sacrifice.
Natural resources.
Egypt is rich in building and decorative stone, copper and lead ores, gold, and semiprecious stones. These natural resources allowed the ancient Egyptians to build monuments, sculpt statues, make tools, and fashion jewelry. Embalmers used salts from the Wadi Natrun for mummification, which also provided the gypsum needed to make plaster. Ore-bearing rock formations were found in distant, inhospitable wadis in the eastern desert and the Sinai, requiring large, state-controlled expeditions to obtain natural resources found there. There were extensive gold mines in Nubia, and one of the first maps known is of a gold mine in this region. The Wadi Hammamat was a notable source of granite, greywacke, and gold. Flint was the first mineral collected and used to make tools, and flint handaxes are the earliest pieces of evidence of habitation in the Nile valley. Nodules of the mineral were carefully flaked to make blades and arrowheads of moderate hardness and durability even after copper was adopted for this purpose. Ancient Egyptians were among the first to use minerals such as sulfur as cosmetic substances.
The Egyptians worked deposits of the lead ore galena at Gebel Rosas to make net sinkers, plumb bobs, and small figurines. Copper was the most important metal for toolmaking in ancient Egypt and was smelted in furnaces from malachite ore mined in the Sinai. Workers collected gold by washing the nuggets out of sediment in alluvial deposits, or by the more labor-intensive process of grinding and washing gold-bearing quartzite. Iron deposits found in upper Egypt were utilized in the Late Period. High-quality building stones were abundant in Egypt; the ancient Egyptians quarried limestone all along the Nile valley, granite from Aswan, and basalt and sandstone from the wadis of the eastern desert. Deposits of decorative stones such as porphyry, greywacke, alabaster, and carnelian dotted the eastern desert and were collected even before the First Dynasty. In the Ptolemaic and Roman Periods, miners worked deposits of emeralds in Wadi Sikait and amethyst in Wadi el-Hudi.
Trade.
The ancient Egyptians engaged in trade with their foreign neighbors to obtain rare, exotic goods not found in Egypt. In the Predynastic Period, they established trade with Nubia to obtain gold and incense. They also established trade with Palestine, as evidenced by Palestinian-style oil jugs found in the burials of the First Dynasty pharaohs. An Egyptian colony stationed in southern Canaan dates to slightly before the First Dynasty. Narmer had Egyptian pottery produced in Canaan and exported back to Egypt.
By the Second Dynasty at latest, ancient Egyptian trade with Byblos yielded a critical source of quality timber not found in Egypt. By the Fifth Dynasty, trade with Punt provided gold, aromatic resins, ebony, ivory, and wild animals such as monkeys and baboons. Egypt relied on trade with Anatolia for essential quantities of tin as well as supplementary supplies of copper, both metals being necessary for the manufacture of bronze. The ancient Egyptians prized the blue stone lapis lazuli, which had to be imported from far-away Afghanistan. Egypt's Mediterranean trade partners also included Greece and Crete, which provided, among other goods, supplies of olive oil. In exchange for its luxury imports and raw materials, Egypt mainly exported grain, gold, linen, and papyrus, in addition to other finished goods including glass and stone objects.
Language.
Historical development.
The Egyptian language is a northern Afro-Asiatic language closely related to the Berber and Semitic languages. It has the second longest history of any language (after Sumerian), having been written from c. 3200 BC to the Middle Ages and remaining as a spoken language for longer. The phases of ancient Egyptian are Old Egyptian, Middle Egyptian (Classical Egyptian), Late Egyptian, Demotic and Coptic. Egyptian writings do not show dialect differences before Coptic, but it was probably spoken in regional dialects around Memphis and later Thebes.
Ancient Egyptian was a synthetic language, but it became more analytic later on. Late Egyptian develops prefixal definite and indefinite articles, which replace the older inflectional suffixes. There is a change from the older verb–subject–object word order to subject–verb–object. The Egyptian hieroglyphic, hieratic, and demotic scripts were eventually replaced by the more phonetic Coptic alphabet. Coptic is still used in the liturgy of the Egyptian Orthodox Church, and traces of it are found in modern Egyptian Arabic.
Sounds and grammar.
Ancient Egyptian has 25 consonants similar to those of other Afro-Asiatic languages. These include pharyngeal and emphatic consonants, voiced and voiceless stops, voiceless fricatives and voiced and voiceless affricates. It has three long and three short vowels, which expanded in Later Egyptian to about nine. The basic word in Egyptian, similar to Semitic and Berber, is a triliteral or biliteral root of consonants and semiconsonants. Suffixes are added to form words. The verb conjugation corresponds to the person. For example, the triconsonantal skeleton is the semantic core of the word 'hear'; its basic conjugation is ', 'he hears'. If the subject is a noun, suffixes are not added to the verb: ', 'the woman hears'.
Adjectives are derived from nouns through a process that Egyptologists call "nisbation" because of its similarity with Arabic. The word order is in verbal and adjectival sentences, and in nominal and adverbial sentences. The subject can be moved to the beginning of sentences if it is long and is followed by a resumptive pronoun. Verbs and nouns are negated by the particle "n", but "nn" is used for adverbial and adjectival sentences. Stress falls on the ultimate or penultimate syllable, which can be open (CV) or closed (CVC).
Writing.
Hieroglyphic writing dates from c. 3000 BC, and is composed of hundreds of symbols. A hieroglyph can represent a word, a sound, or a silent determinative; and the same symbol can serve different purposes in different contexts. Hieroglyphs were a formal script, used on stone monuments and in tombs, that could be as detailed as individual works of art. In day-to-day writing, scribes used a cursive form of writing, called hieratic, which was quicker and easier. While formal hieroglyphs may be read in rows or columns in either direction (though typically written from right to left), hieratic was always written from right to left, usually in horizontal rows. A new form of writing, Demotic, became the prevalent writing style, and it is this form of writing—along with formal hieroglyphs—that accompany the Greek text on the Rosetta Stone.
Around the first century AD, the Coptic alphabet started to be used alongside the Demotic script. Coptic is a modified Greek alphabet with the addition of some Demotic signs. Although formal hieroglyphs were used in a ceremonial role until the fourth century, towards the end only a small handful of priests could still read them. As the traditional religious establishments were disbanded, knowledge of hieroglyphic writing was mostly lost. Attempts to decipher them date to the Byzantine and Islamic periods in Egypt, but only in 1822, after the discovery of the Rosetta stone and years of research by Thomas Young and Jean-François Champollion, were hieroglyphs almost fully deciphered.
Literature.
Writing first appeared in association with kingship on labels and tags for items found in royal tombs. It was primarily an occupation of the scribes, who worked out of the "Per Ankh" institution or the House of Life. The latter comprised offices, libraries (called House of Books), laboratories and observatories. Some of the best-known pieces of ancient Egyptian literature, such as the Pyramid and Coffin Texts, were written in Classical Egyptian, which continued to be the language of writing until about 1300 BC. Later Egyptian was spoken from the New Kingdom onward and is represented in Ramesside administrative documents, love poetry and tales, as well as in Demotic and Coptic texts. During this period, the tradition of writing had evolved into the tomb autobiography, such as those of Harkhuf and Weni. The genre known as "Sebayt" ("instructions") was developed to communicate teachings and guidance from famous nobles; the Ipuwer papyrus, a poem of lamentations describing natural disasters and social upheaval, is a famous example.
The Story of Sinuhe, written in Middle Egyptian, might be the classic of Egyptian literature. Also written at this time was the Westcar Papyrus, a set of stories told to Khufu by his sons relating the marvels performed by priests. The Instruction of Amenemope is considered a masterpiece of near-eastern literature. Towards the end of the New Kingdom, the vernacular language was more often employed to write popular pieces like the Story of Wenamun and the Instruction of Any. The former tells the story of a noble who is robbed on his way to buy cedar from Lebanon and of his struggle to return to Egypt. From about 700 BC, narrative stories and instructions, such as the popular Instructions of Onchsheshonqy, as well as personal and business documents were written in the demotic script and phase of Egyptian. Many stories written in demotic during the Greco-Roman period were set in previous historical eras, when Egypt was an independent nation ruled by great pharaohs such as Ramesses II.
Culture.
Daily life.
Most ancient Egyptians were farmers tied to the land. Their dwellings were restricted to immediate family members, and were constructed of mud-brick designed to remain cool in the heat of the day. Each home had a kitchen with an open roof, which contained a grindstone for milling grain and a small oven for baking the bread. Walls were painted white and could be covered with dyed linen wall hangings. Floors were covered with reed mats, while wooden stools, beds raised from the floor and individual tables comprised the furniture.
The ancient Egyptians placed a great value on hygiene and appearance. Most bathed in the Nile and used a pasty soap made from animal fat and chalk. Men shaved their entire bodies for cleanliness; perfumes and aromatic ointments covered bad odors and soothed skin. Clothing was made from simple linen sheets that were bleached white, and both men and women of the upper classes wore wigs, jewelry, and cosmetics. Children went without clothing until maturity, at about age 12, and at this age males were circumcised and had their heads shaved. Mothers were responsible for taking care of the children, while the father provided the family's income.
Music and dance were popular entertainments for those who could afford them. Early instruments included flutes and harps, while instruments similar to trumpets, oboes, and pipes developed later and became popular. In the New Kingdom, the Egyptians played on bells, cymbals, tambourines, drums, and imported lutes and lyres from Asia. The sistrum was a rattle-like musical instrument that was especially important in religious ceremonies.
The ancient Egyptians enjoyed a variety of leisure activities, including games and music. Senet, a board game where pieces moved according to random chance, was particularly popular from the earliest times; another similar game was mehen, which had a circular gaming board. Juggling and ball games were popular with children, and wrestling is also documented in a tomb at Beni Hasan. The wealthy members of ancient Egyptian society enjoyed hunting and boating as well.
The excavation of the workers' village of Deir el-Madinah has resulted in one of the most thoroughly documented accounts of community life in the ancient world that spans almost four hundred years. There is no comparable site in which the organisation, social interactions, working and living conditions of a community were studied in such detail.
Cuisine.
Egyptian cuisine remained remarkably stable over time; indeed, the cuisine of modern Egypt retains some striking similarities to the cuisine of the ancients. The staple diet consisted of bread and beer, supplemented with vegetables such as onions and garlic, and fruit such as dates and figs. Wine and meat were enjoyed by all on feast days while the upper classes indulged on a more regular basis. Fish, meat, and fowl could be salted or dried, and could be cooked in stews or roasted on a grill.
Architecture.
The architecture of ancient Egypt includes some of the most famous structures in the world: the Great Pyramids of Giza and the temples at Thebes. Building projects were organized and funded by the state for religious and commemorative purposes, but also to reinforce the power of the pharaoh. The ancient Egyptians were skilled builders; using simple but effective tools and sighting instruments, architects could build large stone structures with accuracy and precision.
The domestic dwellings of elite and ordinary Egyptians alike were constructed from perishable materials such as mud bricks and wood, and have not survived. Peasants lived in simple homes, while the palaces of the elite were more elaborate structures. A few surviving New Kingdom palaces, such as those in Malkata and Amarna, show richly decorated walls and floors with scenes of people, birds, water pools, deities and geometric designs. Important structures such as temples and tombs that were intended to last forever were constructed of stone instead of bricks. The architectural elements used in the world's first large-scale stone building, Djoser's mortuary complex, include post and lintel supports in the papyrus and lotus motif.
The earliest preserved ancient Egyptian temples, such as those at Giza, consist of single, enclosed halls with roof slabs supported by columns. In the New Kingdom, architects added the pylon, the open courtyard, and the enclosed hypostyle hall to the front of the temple's sanctuary, a style that was standard until the Greco-Roman period. The earliest and most popular tomb architecture in the Old Kingdom was the mastaba, a flat-roofed rectangular structure of mudbrick or stone built over an underground burial chamber. The step pyramid of Djoser is a series of stone mastabas stacked on top of each other. Pyramids were built during the Old and Middle Kingdoms, but most later rulers abandoned them in favor of less conspicuous rock-cut tombs. The Twenty-fifth dynasty was a notable exception, as all Twenty-fifth dynasty pharaohs constructed pyramids.
Art.
The ancient Egyptians produced art to serve functional purposes. For over 3500 years, artists adhered to artistic forms and iconography that were developed during the Old Kingdom, following a strict set of principles that resisted foreign influence and internal change. These artistic standards—simple lines, shapes, and flat areas of color combined with the characteristic flat projection of figures with no indication of spatial depth—created a sense of order and balance within a composition. Images and text were intimately interwoven on tomb and temple walls, coffins, stelae, and even statues. The Narmer Palette, for example, displays figures that can also be read as hieroglyphs. Because of the rigid rules that governed its highly stylized and symbolic appearance, ancient Egyptian art served its political and religious purposes with precision and clarity.
Ancient Egyptian artisans used stone to carve statues and fine reliefs, but used wood as a cheap and easily carved substitute. Paints were obtained from minerals such as iron ores (red and yellow ochres), copper ores (blue and green), soot or charcoal (black), and limestone (white). Paints could be mixed with gum arabic as a binder and pressed into cakes, which could be moistened with water when needed.
Pharaohs used reliefs to record victories in battle, royal decrees, and religious scenes. Common citizens had access to pieces of funerary art, such as shabti statues and books of the dead, which they believed would protect them in the afterlife. During the Middle Kingdom, wooden or clay models depicting scenes from everyday life became popular additions to the tomb. In an attempt to duplicate the activities of the living in the afterlife, these models show laborers, houses, boats, and even military formations that are scale representations of the ideal ancient Egyptian afterlife.
Despite the homogeneity of ancient Egyptian art, the styles of particular times and places sometimes reflected changing cultural or political attitudes. After the invasion of the Hyksos in the Second Intermediate Period, Minoan-style frescoes were found in Avaris. The most striking example of a politically driven change in artistic forms comes from the Amarna period, where figures were radically altered to conform to Akhenaten's revolutionary religious ideas. This style, known as Amarna art, was quickly and thoroughly erased after Akhenaten's death and replaced by the traditional forms.
Religious beliefs.
Beliefs in the divine and in the afterlife were ingrained in ancient Egyptian civilization from its inception; pharaonic rule was based on the divine right of kings. The Egyptian pantheon was populated by gods who had supernatural powers and were called on for help or protection. However, the gods were not always viewed as benevolent, and Egyptians believed they had to be appeased with offerings and prayers. The structure of this pantheon changed continually as new deities were promoted in the hierarchy, but priests made no effort to organize the diverse and sometimes conflicting myths and stories into a coherent system. These various conceptions of divinity were not considered contradictory but rather layers in the multiple facets of reality.
Gods were worshiped in cult temples administered by priests acting on the king's behalf. At the center of the temple was the cult statue in a shrine. Temples were not places of public worship or congregation, and only on select feast days and celebrations was a shrine carrying the statue of the god brought out for public worship. Normally, the god's domain was sealed off from the outside world and was only accessible to temple officials. Common citizens could worship private statues in their homes, and amulets offered protection against the forces of chaos. After the New Kingdom, the pharaoh's role as a spiritual intermediary was de-emphasized as religious customs shifted to direct worship of the gods. As a result, priests developed a system of oracles to communicate the will of the gods directly to the people.
The Egyptians believed that every human being was composed of physical and spiritual parts or "aspects". In addition to the body, each person had a "šwt" (shadow), a "ba" (personality or soul), a "ka" (life-force), and a "name". The heart, rather than the brain, was considered the seat of thoughts and emotions. After death, the spiritual aspects were released from the body and could move at will, but they required the physical remains (or a substitute, such as a statue) as a permanent home. The ultimate goal of the deceased was to rejoin his "ka" and "ba" and become one of the "blessed dead", living on as an "akh", or "effective one". For this to happen, the deceased had to be judged worthy in a trial, in which the heart was weighed against a "feather of truth". If deemed worthy, the deceased could continue their existence on earth in spiritual form.
Burial customs.
The ancient Egyptians maintained an elaborate set of burial customs that they believed were necessary to ensure immortality after death. These customs involved preserving the body by mummification, performing burial ceremonies, and interring with the body goods the deceased would use in the afterlife. Before the Old Kingdom, bodies buried in desert pits were naturally preserved by desiccation. The arid, desert conditions were a boon throughout the history of ancient Egypt for burials of the poor, who could not afford the elaborate burial preparations available to the elite. Wealthier Egyptians began to bury their dead in stone tombs and use artificial mummification, which involved removing the internal organs, wrapping the body in linen, and burying it in a rectangular stone sarcophagus or wooden coffin. Beginning in the Fourth Dynasty, some parts were preserved separately in canopic jars.
By the New Kingdom, the ancient Egyptians had perfected the art of mummification; the best technique took 70 days and involved removing the internal organs, removing the brain through the nose, and desiccating the body in a mixture of salts called natron. The body was then wrapped in linen with protective amulets inserted between layers and placed in a decorated anthropoid coffin. Mummies of the Late Period were also placed in painted cartonnage mummy cases. Actual preservation practices declined during the Ptolemaic and Roman eras, while greater emphasis was placed on the outer appearance of the mummy, which was decorated.
Wealthy Egyptians were buried with larger quantities of luxury items, but all burials, regardless of social status, included goods for the deceased. Beginning in the New Kingdom, books of the dead were included in the grave, along with shabti statues that were believed to perform manual labor for them in the afterlife. Rituals in which the deceased was magically re-animated accompanied burials. After burial, living relatives were expected to occasionally bring food to the tomb and recite prayers on behalf of the deceased.
Military.
The ancient Egyptian military was responsible for defending Egypt against foreign invasion, and for maintaining Egypt's domination in the ancient Near East. The military protected mining expeditions to the Sinai during the Old Kingdom and fought civil wars during the First and Second Intermediate Periods. The military was responsible for maintaining fortifications along important trade routes, such as those found at the city of Buhen on the way to Nubia. Forts also were constructed to serve as military bases, such as the fortress at Sile, which was a base of operations for expeditions to the Levant. In the New Kingdom, a series of pharaohs used the standing Egyptian army to attack and conquer Kush and parts of the Levant.
Typical military equipment included bows and arrows, spears, and round-topped shields made by stretching animal skin over a wooden frame. In the New Kingdom, the military began using chariots that had earlier been introduced by the Hyksos invaders. Weapons and armor continued to improve after the adoption of bronze: shields were now made from solid wood with a bronze buckle, spears were tipped with a bronze point, and the Khopesh was adopted from Asiatic soldiers. The pharaoh was usually depicted in art and literature riding at the head of the army; it has been suggested that at least a few pharaohs, such as Seqenenre Tao II and his sons, did do so. However, it has also been argued that "kings of this period did not personally act as frontline war leaders, fighting alongside their troops." Soldiers were recruited from the general population, but during, and especially after, the New Kingdom, mercenaries from Nubia, Kush, and Libya were hired to fight for Egypt.
Technology, medicine, and mathematics.
Technology.
In technology, medicine and mathematics, ancient Egypt achieved a relatively high standard of productivity and sophistication. Traditional empiricism, as evidenced by the Edwin Smith and Ebers papyri (c. 1600 BC), is first credited to Egypt. The Egyptians created their own alphabet and decimal system.
Faience and glass.
The ancient Egyptians could fabricate a wide variety of objects from glass with great skill, but it is not clear whether they developed the process independently. It is also unclear whether they made their own raw glass or merely imported pre-made ingots, which they melted and finished. However, they did have technical expertise in making objects, as well as adding trace elements to control the color of the finished glass. A range of colors could be produced, including yellow, red, green, blue, purple, and white, and the glass could be made either transparent or opaque.
Medicine.
The medical problems of the ancient Egyptians stemmed directly from their environment. Living and working close to the Nile brought hazards from malaria and debilitating schistosomiasis parasites, which caused liver and intestinal damage. Dangerous wildlife such as crocodiles and hippos were also a common threat. The lifelong labors of farming and building put stress on the spine and joints, and traumatic injuries from construction and warfare all took a significant toll on the body. The grit and sand from stone-ground flour abraded teeth, leaving them susceptible to abscesses (though caries were rare).
The diets of the wealthy were rich in sugars, which promoted periodontal disease. Despite the flattering physiques portrayed on tomb walls, the overweight mummies of many of the upper class show the effects of a life of overindulgence. Adult life expectancy was about 35 for men and 30 for women, but reaching adulthood was difficult as about one-third of the population died in infancy.
Ancient Egyptian physicians were renowned in the ancient Near East for their healing skills, and some, such as Imhotep, remained famous long after their deaths. Herodotus remarked that there was a high degree of specialization among Egyptian physicians, with some treating only the head or the stomach, while others were eye-doctors and dentists. Training of physicians took place at the "Per Ankh" or "House of Life" institution, most notably those headquartered in Per-Bastet during the New Kingdom and at Abydos and Saïs in the Late period. Medical papyri show empirical knowledge of anatomy, injuries, and practical treatments.
Wounds were treated by bandaging with raw meat, white linen, sutures, nets, pads, and swabs soaked with honey to prevent infection, while opium thyme and belladona were used to relieve pain. The earliest records of burn treatment describe burn dressings that use the milk from mothers of male babies. Prayers were made to the goddess Isis. Moldy bread, honey and copper salts were also used to prevent infection from dirt in burns. Garlic and onions were used regularly to promote good health and were thought to relieve asthma symptoms. Ancient Egyptian surgeons stitched wounds, set broken bones, and amputated diseased limbs, but they recognized that some injuries were so serious that they could only make the patient comfortable until death occurred.
Maritime technology.
Early Egyptians knew how to assemble planks of wood into a ship hull and had mastered advanced forms of shipbuilding as early as 3000 BC. The Archaeological Institute of America reports that the oldest planked ships known are the Abydos boats. A group of 14 discovered ships in Abydos were constructed of wooden planks "sewn" together. Discovered by Egyptologist David O'Connor of New York University, woven straps were found to have been used to lash the planks together, and reeds or grass stuffed between the planks helped to seal the seams. Because the ships are all buried together and near a mortuary belonging to Pharaoh Khasekhemwy, originally they were all thought to have belonged to him, but one of the 14 ships dates to 3000 BC, and the associated pottery jars buried with the vessels also suggest earlier dating. The ship dating to 3000 BC was long and is now thought to perhaps have belonged to an earlier pharaoh. According to professor O'Connor, the 5,000-year-old ship may have even belonged to Pharaoh Aha.
Early Egyptians also knew how to assemble planks of wood with treenails to fasten them together, using pitch for caulking the seams. The "Khufu ship", a vessel sealed into a pit in the Giza pyramid complex at the foot of the Great Pyramid of Giza in the Fourth Dynasty around 2500 BC, is a full-size surviving example that may have filled the symbolic function of a solar barque. Early Egyptians also knew how to fasten the planks of this ship together with mortise and tenon joints.
In 2011 archaeologists from Italy, the United States, and Egypt excavating a dried-up lagoon known as Mersa Gawasis have unearthed traces of an ancient harbor that once launched early voyages like Hatshepsut's Punt expedition onto the open ocean. Some of the site's most evocative evidence for the ancient Egyptians' seafaring prowess include large ship timbers and hundreds of feet of ropes, made from papyrus, coiled in huge bundles. And in 2013 a team of Franco-Egyptian archaeologists discovered what is believed to be the world's oldest port, dating back about 4500 years, from the time of King Cheops on the Red Sea coast near Wadi el-Jarf (about 110 miles south of Suez).
In 1977, an ancient north-south canal dating to the Middle Kingdom of Egypt was discovered extending from Lake Timsah to the Ballah Lakes. It was dated to the Middle Kingdom of Egypt by extrapolating dates of ancient sites constructed along its course.
Mathematics.
The earliest attested examples of mathematical calculations date to the predynastic Naqada period, and show a fully developed numeral system. The importance of mathematics to an educated Egyptian is suggested by a New Kingdom fictional letter in which the writer proposes a scholarly competition between himself and another scribe regarding everyday calculation tasks such as accounting of land, labor, and grain. Texts such as the Rhind Mathematical Papyrus and the Moscow Mathematical Papyrus show that the ancient Egyptians could perform the four basic mathematical operations—addition, subtraction, multiplication, and division—use fractions, compute the volumes of boxes and pyramids, and calculate the surface areas of rectangles, triangles, and circles. They understood basic concepts of algebra and geometry, and could solve simple sets of simultaneous equations.
Mathematical notation was decimal, and based on hieroglyphic signs for each power of ten up to one million. Each of these could be written as many times as necessary to add up to the desired number; so to write the number eighty or eight hundred, the symbol for ten or one hundred was written eight times respectively. Because their methods of calculation could not handle most fractions with a numerator greater than one, they had to write fractions as the sum of several fractions. For example, they resolved the fraction "two-fifths" into the sum of "one-third" + "one-fifteenth". Standard tables of values facilitated this. Some common fractions, however, were written with a special glyph—the equivalent of the modern two-thirds is shown on the right.
Ancient Egyptian mathematicians had a grasp of the principles underlying the Pythagorean theorem, knowing, for example, that a triangle had a right angle opposite the hypotenuse when its sides were in a 3–4–5 ratio. They were able to estimate the area of a circle by subtracting one-ninth from its diameter and squaring the result:
a reasonable approximation of the formula π"r" 2.
The golden ratio seems to be reflected in many Egyptian constructions, including the pyramids, but its use may have been an unintended consequence of the ancient Egyptian practice of combining the use of knotted ropes with an intuitive sense of proportion and harmony.
Legacy.
The culture and monuments of ancient Egypt have left a lasting legacy on the world. The cult of the goddess Isis, for example, became popular in the Roman Empire, as obelisks and other relics were transported back to Rome. The Romans also imported building materials from Egypt to erect Egyptian-style structures. Early historians such as Herodotus, Strabo, and Diodorus Siculus studied and wrote about the land, which Romans came to view as a place of mystery.
During the Middle Ages and The Renaissance, Egyptian pagan culture was in decline after the rise of Christianity and later Islam, but interest in Egyptian antiquity continued in the writings of medieval scholars such as Dhul-Nun al-Misri and al-Maqrizi. In the seventeenth and eighteenth centuries, European travelers and tourists brought back antiquities and wrote stories of their journeys, leading to a wave of Egyptomania across Europe. This renewed interest sent collectors to Egypt, who took, purchased, or were given many important antiquities.
Although the European colonial occupation of Egypt destroyed a significant portion of the country's historical legacy, some foreigners left more positive marks. Napoleon, for example, arranged the first studies in Egyptology when he brought some 150 scientists and artists to study and document Egypt's natural history, which was published in the "Description de l'Égypte".
In the 20th century, the Egyptian Government and archaeologists alike recognized the importance of cultural respect and integrity in excavations. The Supreme Council of Antiquities now approves and oversees all excavations, which are aimed at finding information rather than treasure. The council also supervises museums and monument reconstruction programs designed to preserve the historical legacy of Egypt.

</doc>
<doc id="875" url="https://en.wikipedia.org/wiki?curid=875" title="Analog Brothers">
Analog Brothers

Analog Brothers were an experimental hip-hop crew featuring Ice Oscillator also known as Ice-T (keyboards, drums, vocals), Keith Korg also known as Kool Keith (bass, strings, vocals), Mark Moog also known as Marc Live (drums, "violyns" and vocals), Silver Synth also known as Black Silver (synthesizer, lazar bell and vocals), and Rex Roland also known as Pimp Rex (keyboards, vocals, production). Its album "Pimp to Eat" featured guest appearances by various members of Rhyme Syndicate, Odd Oberheim, Jacky jasper (who appears as Jacky Jasper on the song "We Sleep Days" and H-Bomb on "War"), D.J. Cisco from S.M., the Synth-a-Size Sisters and Teflon.
While the group only recorded one album together as the Analog Brothers, a few bootlegs of its live concert performances, including freestyles with original lyrics, have occasionally surfaced online. After "Pimp to Eat", the Analog Brothers continued performing together in various line ups. Kool Keith and Marc Live joined with Jacky jasper to release two albums as KHM. Marc Live rapped with Ice T's group SMG. Marc also formed a group with Black Silver called Live Black, but while five of their tracks were released on a demo CD sold at concerts, Live Black's first album has yet to be released.
In 2008, Ice-T and Black Silver toured together as Black Ice, and released an album together called Urban Legends.
In addition to all this, the Analog Brothers continue to make frequent appearances on each other's solo albums.

</doc>
<doc id="876" url="https://en.wikipedia.org/wiki?curid=876" title="Motor neuron disease">
Motor neuron disease

A motor neuron disease (MND) is any of five neurological disorders that selectively affect motor neurons, the cells that control voluntary muscles of the body. These five conditions are amyotrophic lateral sclerosis, primary lateral sclerosis, progressive muscular atrophy, progressive bulbar palsy and pseudobulbar palsy. They are neurodegenerative in nature and cause increasing disability and eventually, death.
Terminology.
Technically the term "motor neuron disease" includes five diseases: amyotrophic lateral sclerosis (ALS), primary lateral sclerosis, progressive muscular atrophy, progressive bulbar palsy and pseudobulbar palsy. 
In the United States the term is often used interchangeably with ALS. In the United Kingdom "motor neurone disease" may be used to mean ALS.
While MND refers to a specific subset of similar diseases, there are numerous other diseases of motor neurons that are referred to collectively as "motor neuron disorders", for instance disease belonging to spinal muscular atrophies. However, they are not classified as "motor neuron diseases" by the tenth International Statistical Classification of Diseases and Related Health Problems (ICD-10) which is the definition followed in this article.
Classification.
Motor neuron diseases affect either upper motor neurons (UMN) or lower motor neurons (LMN), or both:

</doc>
<doc id="877" url="https://en.wikipedia.org/wiki?curid=877" title="Abjad">
Abjad

An abjad () is a type of writing system where each symbol stands for a consonant, leaving the reader to supply the appropriate vowel.
It is a term suggested by Peter T. Daniels to replace the common terms "consonantary", "consonantal alphabet" or "syllabary" to refer to the family of scripts called West Semitic.
Etymology.
The name "abjad" (' ) is derived from pronouncing the first letters of the "Arabic" alphabet in order. The ordering (') of Arabic letters used to match that of the older Hebrew, Phoenician and Semitic alphabets; "".
Terminology.
According to the formulations of Daniels, abjads differ from alphabets in that only consonants, not vowels, are represented among the basic graphemes. Abjads differ from abugidas, another category defined by Daniels, in that in abjads, the vowel sound is "implied" by phonology, and where vowel marks exist for the system, such as nikkud for Hebrew and ḥarakāt for Arabic, their use is optional and not the dominant (or literate) form. Abugidas mark the vowels (other than the "inherent" vowel) with a diacritic, a minor attachment to the letter, or a standalone glyph. Some abugidas use a special symbol to "suppress" the inherent vowel so that the consonant alone can be properly represented. In a syllabary, a grapheme denotes a complete syllable, that is, either a lone vowel sound or a combination of a vowel sound with one or more consonant sounds.
The antagonism of abjad versus alphabet, as it was formulated by Daniels, has been rejected by other scholars because abjad is also used as a term not only for the Arabic numeral system but, which is most important in terms of historical grammatology, also as term for the alphabetic device (i.e. letter order) of ancient Northwest Semitic scripts in opposition to the 'south Arabian' order. This caused fatal effects on terminology in general and especially in (ancient) Semitic philology. Also, it suggests that consonantal alphabets, in opposition to for instance the Greek alphabet, were not yet true alphabets and not yet entirely complete, lacking something important to be a fully working script system. It has also been objected that, as a set of letters, an alphabet is not the mirror of what should be there in a language from a phonemic or even phonological point of view, rather, it is the data stock of what provides maximum efficiency with least effort from a semantic point of view.
Origins.
The first abjad to gain widespread usage was the Phoenician abjad. Unlike other contemporary scripts, such as Cuneiform and Egyptian hieroglyphs, the Phoenician script consisted of only about two dozen symbols. This made the script easy to learn, and Phoenician seafaring merchants took the script wherever they went.
The Phoenician abjad was a radical simplification of phonetic writing, since hieroglyphics required the writer to pick a hieroglyph starting with the same phoneme that the writer wants to write in order to write phonetically, much like how "man'yougana" (Chinese characters, or kanji, used solely for phonetic use) was used to represent Japanese phonetically before the invention of kana.
Phoenician gave way to a number of new writing systems, including the Greek alphabet and Aramaic, a widely used abjad. The Greek alphabet evolved into the modern western alphabets, such as Latin and Cyrillic, while Aramaic became the ancestor of many modern abjads and abugidas of Asia.
The impure abjads.
"Impure" abjads have characters for some vowels, optional vowel diacritics, or both. The term "pure" abjad refers to scripts entirely lacking in vowel indicators. However, most modern abjads, such as Arabic, Hebrew, Aramaic and Avestan, are "impure" abjads, that is, they also contain symbols for some of the vowel phonemes, although the said non-diacritic vowel letters are also used to write certain consonants, particularly approximants that sound similar to long vowels. An example of a "pure" abjad is (perhaps) very early forms of ancient Phoenician, though at some point (at least by the 9th century BC) it and most of the contemporary Semitic abjads had begun to overload a few of the consonant symbols with a secondary function as vowel markers, called "matres lectionis". This practice was at first rare and limited in scope, but it became increasingly common and more developed in later times.
Addition of vowels.
In the 9th century BC, the Greeks adapted the Phoenician script for use in their own language. The phonetic structure of the Greek language created too many ambiguities when the vowels went unrepresented, so the script was modified. They did not need letters for the guttural sounds represented by "aleph", "he", "heth" or "ayin", so these symbols were assigned vocalic values. The letters "waw" and "yod" were also adapted into vowel signs; along with "he", these were already used as "matres lectionis" in Phoenician. The major innovation of Greek was to dedicate these symbols exclusively and unambiguously to vowel sounds that could be combined arbitrarily with consonants. (As opposed to syllabaries such as Linear B which usually have vowel symbols but cannot combine them with consonants to form arbitrary syllables.)
Abugidas developed along a slightly different route. The basic consonantal symbol was considered to have an inherent "a" vowel sound. Hooks or short lines attached to various parts of the basic letter modify the vowel. In this way, the South Arabian alphabet evolved into the Ge'ez alphabet between the 5th century BC and the 5th century AD. Similarly, around the 3rd century BC, the Brāhmī script developed (from the Aramaic abjad, it has been hypothesized).
The other major family of abugidas, Canadian Aboriginal syllabics, was initially developed in the 1840s by missionary and linguist James Evans for the Cree and Ojibwe languages. Evans used features of Devanagari script and Pitman shorthand to create his initial abugida. Later in the 19th century, other missionaries adapted Evans' system to other Canadian aboriginal languages. Canadian syllabics differ from other abugidas in that the vowel is indicated by rotation of the consonantal symbol, with each vowel having a consistent orientation.
Abjads and the structure of Semitic languages.
The abjad form of writing is well-adapted to the morphological structure of the Semitic languages it was developed to write. This is because words in Semitic languages are formed from a root consisting of (usually) three consonants, the vowels being used to indicate inflectional or derived forms. For instance, according to Classical Arabic and Modern Standard Arabic, the Arabic root "Dh-B-Ḥ" (to slaughter) can be derived the forms ' (he slaughtered), ' (you (masculine singular) slaughtered), ' (he slaughtered), ' (he slaughters), and "" (slaughterhouse). In most cases, the absence of full glyphs for vowels makes the common root clearer, allowing readers to guess the meaning of unfamiliar words from familiar roots (especially in conjunction with context clues) and improving word recognition while reading for practiced readers.

</doc>
<doc id="878" url="https://en.wikipedia.org/wiki?curid=878" title="Abugida">
Abugida

An abugida (from Ge'ez አቡጊዳ "’äbugida"), and likewise an alphasyllabary, is a segmental writing system in which consonant–vowel sequences are written as a unit: each unit is based on a consonant letter, and vowel notation is secondary. This contrasts with a full alphabet, in which vowels have status equal to consonants, and with an abjad, in which vowel marking is absent or optional. (In less formal contexts, all three types of script may be termed alphabets.) The terms also contrast them with a syllabary, in which the symbols cannot be split into separate consonants and vowels. Abugidas include the extensive Brahmic family of scripts of South and Southeast Asia.
As is the case for syllabaries, the units of the writing system may consist of the representations both of syllables and of consonants. For scripts of the Brahmic family, the term "akshara" is used for the units.
"Abugida" as a term in linguistics was proposed by Peter T. Daniels in his 1990 typology of writing systems. "’Abugida" is an Ethiopian name for the Ge‘ez script, taken from four letters of that script, "ä bu gi da", in much the same way that "abecedary" is derived from Latin "a be ce de", and "alphabet" is derived from the names of the two first letters in the Greek alphabet, "alpha" and "beta". As Daniels used the word, an abugida is in contrast with a syllabary, where letters with shared consonants or vowels show no particular resemblance to one another, and also with an alphabet proper, where independent letters are used to denote both consonants and vowels. The term "alphasyllabary" was suggested for the Indic scripts in 1997 by William Bright, following South Asian linguistic usage, to convey the idea that "they share features of both alphabet and syllabary."
Abugidas were long considered to be syllabaries, or intermediate between syllabaries and alphabets, and the term "syllabics" is retained in the name of Canadian Aboriginal Syllabics. Other terms that have been used include "neosyllabary" (Février 1959), "pseudo-alphabet" (Householder 1959), "semisyllabary" (Diringer 1968; a word that has other uses) and "syllabic alphabet" (Coulmas 1996; this term is also a synonym for syllabary).
The formal definitions given by Daniels and Bright for abugida and alphasyllabary differ; some writing systems are abugidas but not alphasyllabaries, and some are alphasyllabaries but not abugidas.
General Description.
The formal definitions given by Daniels and Bright for abugida and alphasyllabary differ; some writing systems are abugidas but not alphasyllabaries, and some are alphasyllabaries but not abugidas. An abugida is defined as "a type of writing system whose basic characters denotes consonants followed by a particular vowel, and in which diacritics denote other vowels". (This 'particular vowel' is referred to as the "inherent" or "implicit" vowel, as opposed to the "explicit" vowels marked by the 'diacritics'.) An alphasyllabary is defined as "a type of writing system in which the vowels are denoted by subsidiary symbols not all of which occur in a linear order (with relation to the consonant symbols) that is congruent with their temporal order in speech". Bright did not require that an alphabet explicitly represent all vowels. Phagspa is an example of an abugida that is not an alphasyllabary, and modern Lao is an example of an alphasyllabary that is not an abugida, for its vowels are always explicit.
This description is expressed in terms of an abugida. Formally, an alphasyllabary that is not an abugida can be converted to an abugida by adding a purely formal vowel sound that is never used and declaring that to be the inherent vowel of the letters representing consonants. This may formally make the system ambiguous, but in 'practice' this is not a problem, for then the interpretation with the never used inherent vowel sound will always be a wrong interpretation! Note that the actual pronunciation may be complicated by interactions between the sounds apparently written just as the sounds of the letters in the English words "wan, gem" and "war" are affected by neighbouring letters.
The fundamental principles of an abugida apply to words made up of consonant-vowel (CV) syllables. The syllables are written as a linear sequences of the units of the script. Each syllable is either a letter that represents the sound of a consonant and the inherent vowel, or a letter with a modification to indicate the vowel, either by means of diacritics, or by changes in the form of the letter itself. If all modifications are by diacritics and all diacritics follow the direction of the writing of the letters, then the abugida is not an alphasyllabary.
However, most languages have words that are more complicated than a sequence of CV syllables, even ignoring tone.
The first complication is syllables that consist of just a vowel (V). Now, in some languages, this issue does not arise, for every syllable starts with a consonant. This is common in Semitic languages and in languages of mainland SE Asia, and for such languages this issue need not arise. For some languages, a zero consonant letter is used as though every syllable began with a consonant. For other languages, each vowel has a separate letter that is used for each syllable consisting of just the vowel. These letters are known as "independent vowels", and are found in most Indic scripts. These letters may be quite different to the corresponding diacritics, which by contrast are known as "dependent vowels". As a result of the spread of writing systems, independent vowels may be used to represent syllables beginning with a glottal stop, even for non-initial syllables.
The next two complications are sequences of consonants before a vowel (CCV) and syllables ending in a consonant (CVC). The simplest solution, which is not always available, is to break with the principle of writing words as a sequence of syllables and use a unit representing just a consonant (C). This unit may be represented with:
In a true abugida, the lack of distinctive marking may result from the diachronic loss of the inherent vowel, e.g. by syncope and apocope in Hindi.
When not handled by decomposition into C + CV, CCV syllables are handled by combining the two consonants. In the Indic scripts, the earliest method was simply to arrange them vertically, but the two consonants may merge as a conjunct consonant letters, where two or more letters are graphically joined in a ligature, or otherwise change their shapes. Rarely, one of the consonants may be replaced by a gemination mark, e.g. the Gurmukhi "". When they are arranged vertically, as in Burmese or Khmer, they are said to be 'stacked'. Often there has been a change to writing the two consonants side by side. In the latter case, the fact of combination may be indicated by a diacritic on one of the consonants or a change in the form of one of the consonants, e.g. the half forms of Devanagari. Generally, the reading order is top to bottom or the general reading order of the script, but sometimes the order is reversed.
The division of a word into syllables for the purposes of writing does not always accord with the natural phonetics of the language. For example, Brahmic scripts commonly handle a phonetic sequence CVC-CV as CV-CCV or CV-C-CV. However, sometimes phonetic CVC syllables are handled as single units, and the final consonant may be represented:
More complicated unit structures (e.g. CC or CCVC) are handled by combining the various techniques above.
Family-Specific Features.
There are three principal families of abugidas, depending on whether vowels are indicated by modifying consonants by "diacritics, distortion," or "orientation."
Tāna of the Maldives has dependent vowels and a zero vowel sign, but no inherent vowel.
Indic (Brahmic).
Indic scripts originated in India and spread to Southeast Asia. All surviving Indic scripts are descendants of the Brahmi alphabet. Today they are used in most languages of South Asia (although replaced by Perso-Arabic in Urdu, Kashmiri and some other languages of Pakistan and India), mainland Southeast Asia (Burma, Thailand, Laos, Cambodia; but not Malaysia or Vietnam), and Indonesian archipelago (Javanese, Balinese, Sundanese, etc.). The primary division is into North Indic scripts used in Northern India, Nepal, Tibet and Bhutan, and Southern Indic scripts used in South India, Sri Lanka and Southeast Asia.
South Indic letter forms are very rounded; North Indic less so, though Odia, Golmol and Litumol of Nepal script are rounded.
Most North Indic scripts' full letters incorporate a horizontal line at the top, with Gujarati and Odia as exceptions; South Indic scripts do not.
Indic scripts indicate vowels through dependent vowel signs (diacritics) around the consonants, often including a sign that explicitly indicates the lack of a vowel. If a consonant has no vowel sign, this indicates a default vowel. Vowel diacritics may appear above, below, to the left, to the right, or around the consonant.
The most widely used Indic script is Devanagari, shared by Hindi, Bhojpuri, Marathi, Konkani, Nepali, and often Sanskrit. A basic letter such as क in Hindi represents a syllable with the default vowel, in this case "ka" (). In some languages, including Hindi, it becomes a final closing consonant at the end of a word, in this case "k". The inherent vowel may be changed by adding vowel mark (diacritics), producing syllables such as कि "ki," कु "ku," के "ke," को "ko."
In many of the Brahmic scripts, a syllable beginning with a cluster is treated as a single character for purposes of vowel marking, so a vowel marker like ि "-i," falling before the character it modifies, may appear several positions before the place where it is pronounced. For example, the game cricket in Hindi is क्रिकेट "krikeţ;" the diacritic for appears before the consonant cluster , not before the . A more unusual example is seen in the Batak alphabet: Here the syllable "bim" is written "ba-ma-i-(virama)". That is, the vowel diacritic and virama are both written after the consonants for the whole syllable.
In many abugidas, there is also a diacritic to suppress the inherent vowel, yielding the bare consonant. In Devanagari, क् is "k," and ल् is "l". This is called the "virāma" or "halantam" in Sanskrit. It may be used to form consonant clusters, or to indicate that a consonant occurs at the end of a word. Thus in Sanskrit, a default vowel consonant such as क does not take on a final consonant sound. Instead, it keeps its vowel. For writing two consonants without a vowel in between, instead of using diacritics on the first consonant to remove its vowel, another popular method of special conjunct forms is used in which two or more consonant characters are merged to express a cluster, such as Devanagari: क्ल "kla." (Note that some fonts display this as क् followed by ल, rather than forming a conjunct. This expedient is used by ISCII and South Asian scripts of Unicode.) Thus a closed syllable such as "kal" requires two "aksharas" to write.
The Róng script used for the Lepcha language goes further than other Indic abugidas, in that a single "akshara" can represent a closed syllable: Not only the vowel, but any final consonant is indicated by a diacritic. For example, the syllable would be written as something like s̥̽, here with an underring representing and an overcross representing the diacritic for final . Most other Indic abugidas can only indicate a very limited set of final consonants with diacritics, such as or , if they can indicate any at all.
Ethiopic.
In Ethiopic, where the term "abugida" originates, the diacritics have been fused to the consonants to the point that they must be considered modifications of the form of the letters. Children learn each modification separately, as in a syllabary; nonetheless, the graphic similarities between syllables with the same consonant is readily apparent, unlike the case in a true syllabary.
Though now an abugida, the Ge'ez alphabet, until the advent of Christianity ("ca." AD 350), had originally been what would now be termed an "abjad". In the Ge'ez abugida (or "fidel"), the base form of the letter (also known as "fidel") may be altered. For example, ሀ "hä" (base form), ሁ "hu" (with a right-side diacritic that doesn't alter the letter), ሂ "hi" (with a subdiacritic that compresses the consonant, so it is the same height), ህ "hə" or (where the letter is modified with a kink in the left arm).
Canadian Aboriginal syllabics.
In the family known as Canadian Aboriginal syllabics, which was inspired by the Devanagari script of India, vowels are indicated by changing the orientation of the syllabogram. Each vowel has a consistent orientation; for example, Inuktitut ᐱ "pi," ᐳ "pu," ᐸ "pa;" ᑎ "ti," ᑐ "tu," ᑕ "ta". Although there is a vowel inherent in each, all rotations have equal status and none can be identified as basic. Bare consonants are indicated either by separate diacritics, or by superscript versions of the "aksharas"; there is no vowel-killer mark.
Borderline cases.
Vowelled abjads.
Consonantal scripts ("abjads") are normally written without indication of many vowels. However, in some contexts like teaching materials or scriptures, Arabic and Hebrew are written with full indication of vowels via diacritic marks ("harakat", "niqqud") making them effectively alphasyllabaries. The Brahmic and Ethiopic families are thought to have originated from the Semitic abjads by the addition of vowel marks.
The Arabic scripts used for Kurdish in Iraq and for Uyghur in Xinjiang, China, as well as the Hebrew script of Yiddish, are fully vowelled, but because the vowels are written with full letters rather than diacritics (with the exception of distinguishing between /a/ and /o/ in the latter) and there are no inherent vowels, these are considered alphabets, not abugidas.
Phagspa.
The imperial Mongol script called Phagspa was derived from the Tibetan abugida, but all vowels are written in-line rather than as diacritics. However, it retains the features of having an inherent vowel /a/ and having distinct initial vowel letters.
Pahawh.
Pahawh Hmong is a non-segmental script that indicates syllable onsets and rimes, such as consonant clusters and vowels with final consonants. Thus it is not segmental and cannot be considered an abugida. However, it superficially resembles an abugida with the roles of consonant and vowel reversed. Most syllables are written with two letters in the order rime–onset (typically vowel-consonant), even though they are pronounced as onset-rime (consonant-vowel), rather like the position of the vowel in Devanagari, which is written before the consonant. Pahawh is also unusual in that, while an inherent rime (with mid tone) is unwritten, it also has an inherent onset . For the syllable , which requires one or the other of the inherent sounds to be overt, it is that is written. Thus it is the rime (vowel) that is basic to the system.
Meroitic.
It is difficult to draw a dividing line between abugidas and other segmental scripts. For example, the Meroitic script of ancient Sudan did not indicate an inherent "a" (one symbol stood for both "m" and "ma," for example), and is thus similar to Brahmic family of abugidas. However, the other vowels were indicated with full letters, not diacritics or modification, so the system was essentially an alphabet that did not bother to write the most common vowel.
Shorthand.
Several systems of shorthand use diacritics for vowels, but they do not have an inherent vowel, and are thus more similar to Thaana and Kurdish script than to the Brahmic scripts. The Gabelsberger shorthand system and its derivatives modify the "following" consonant to represent vowels. The Pollard script, which was based on shorthand, also uses diacritics for vowels; the placements of the vowel relative to the consonant indicates tone. Pitman shorthand uses straight strokes and quarter-circle marks in different orientations as the principal "alphabet" of consonants; vowels are shown as light and heavy dots, dashes and other marks in one of 3 possible positions to indicate the various vowel-sounds. However, to increase writing speed, Pitman has rules for "vowel indication" using the positioning or choice of consonant signs so that writing vowel-marks can be dispensed with.
Development.
As the term "alphasyllabary" suggests, abugidas have been considered an intermediate step between alphabets and syllabaries. Historically, abugidas appear to have evolved from abjads (vowelless alphabets). They contrast with syllabaries, where there is a distinct symbol for each syllable or consonant-vowel combination, and where these have no systematic similarity to each other, and typically develop directly from logographic scripts. Compare the Devanagari examples above to sets of syllables in the Japanese hiragana syllabary: か "ka", き "ki", く "ku", け "ke", こ "ko" have nothing in common to indicate "k;" while ら "ra", り "ri", る "ru", れ "re", ろ "ro" have neither anything in common for "r", nor anything to indicate that they have the same vowels as the "k" set.
Most Indian and Indochinese abugidas appear to have first been developed from abjads with the Kharoṣṭhī and Brāhmī scripts; the abjad in question is usually considered to be the Aramaic one, but while the link between Aramaic and Kharosthi is more or less undisputed, this is not the case with Brahmi. The Kharosthi family does not survive today, but Brahmi's descendants include most of the modern scripts of South and Southeast Asia. Ge'ez derived from a different abjad, the Sabean script of Yemen; the advent of vowels coincided with the introduction of Christianity about AD 350.
"" is encumbered with the drawback of an excessively awkward and cumbrous written character... At first glance, an book seems to be all curves, and it takes a second look to notice that there is something inside each."" (G. A. Grierson, "Linguistic Survey of India", 1903)
The Ethiopic script is the elaboration of an abjad.
The Cree family was invented with full knowledge of the Devanagari system.
The Meroitic script was developed from Egyptian hieroglyphs, within which various schemes of 'group writing' had been used for showing vowels.

</doc>
<doc id="880" url="https://en.wikipedia.org/wiki?curid=880" title="ABBA">
ABBA

ABBA (stylised ᗅᗺᗷᗅ) were a Swedish pop group formed in Stockholm in 1972 by members Agnetha Fältskog, Björn Ulvaeus, Benny Andersson, and Anni-Frid Lyngstad. They became one of the most commercially successful acts in the history of popular music, topping the charts worldwide from 1974 to 1982. ABBA won the Eurovision Song Contest 1974 at The Dome in Brighton, UK, giving Sweden its first triumph in the contest, and were the most successful group ever to take part in the competition.
ABBA's record sales figure is uncertain and various estimates range from over 140 to over 500 million sold records. This makes them one of the best-selling music artists. ABBA was the first group from a non-English-speaking country to achieve consistent success in the charts of English-speaking countries, including the UK, Ireland, Canada, Australia, New Zealand, South Africa, and on a lesser scale, the U.S. The group also enjoyed significant success in Latin American markets, and recorded a collection of their hit songs in Spanish.
During the band's active years, Fältskog & Ulvaeus and Lyngstad & Andersson were married. At the height of their popularity, both relationships were suffering strain which ultimately resulted in the collapse of the Ulvaeus–Fältskog marriage in 1979 and the Andersson–Lyngstad marriage in 1981. These relationship changes were reflected in the group's music, with later compositions including more introspective, brooding, dark lyrics.
After ABBA disbanded in December 1982, Andersson and Ulvaeus achieved success writing music for the stage, while Lyngstad and Fältskog pursued solo careers with mixed success. ABBA's music declined in popularity until several films, notably "Muriel's Wedding" (1994) and "The Adventures of Priscilla, Queen of the Desert" (1994), revived public interest in the group and the spawning of several tribute bands. In 1999, ABBA's music was adapted into the successful musical "Mamma Mia!" that toured worldwide. A film of the same name, released in 2008, became the highest-grossing film in the United Kingdom that year.
ABBA were honoured at the 50th anniversary celebration of the Eurovision Song Contest in 2005, when their hit "Waterloo" was chosen as the best song in the competition's history. The group was inducted into the Rock and Roll Hall of Fame on 15 March 2010.
History.
1958–70: Before ABBA.
Member origins and collaboration.
Benny Andersson (born 16 December 1946 in Stockholm, Sweden) became (at age 18) a member of a popular Swedish pop-rock group, the Hep Stars, that performed covers, amongst other things, of international hits. The Hep Stars were known as "the Swedish Beatles". They also set up Hep House, their equivalent of Apple Corps. Andersson played the keyboard and eventually started writing original songs for his band, many of which became major hits, including "No Response" that hit number-three in 1965, "Sunny Girl", "Wedding", and "Consolation", all of which hit number-one in 1966. Andersson also had a fruitful songwriting collaboration with Lasse Berghagen, with whom he wrote his first Svensktoppen entry, "Sagan om lilla Sofie" ("The Story of Little Sophie"), in 1968.
Björn Ulvaeus (born 25 April 1945 in Gothenburg/Göteborg, Sweden) also began his musical career at 18 (as a singer and guitarist), when he fronted The Hootenanny Singers, a popular Swedish folk-skiffle group. Ulvaeus started writing English-language songs for his group, and even had a brief solo career alongside. The Hootenanny Singers and The Hep Stars sometimes crossed paths while touring. In June 1966, Ulvaeus and Andersson decided to write a song together. Their first attempt was "Isn't It Easy to Say", a song later recorded by The Hep Stars. Stig Anderson was the manager of The Hootenanny Singers and founder of the Polar Music label. He saw potential in the collaboration, and encouraged them to write more. The two also began playing occasionally with the other's bands on stage and on record, although it was not until 1969 that the pair wrote and produced some of their first real hits together: "Ljuva sextital" ("Sweet Sixties"), recorded by Brita Borg, and The Hep Stars' 1969 hit "Speleman" ("Fiddler").
Andersson wrote and submitted the song "Hej, Clown" for Melodifestivalen 1969, the national festival to select the Swedish entry to the Eurovision Song Contest. The song tied for first place, but re-voting relegated Andersson's song to second place. On that occasion Andersson briefly met his future spouse, singer Anni-Frid Lyngstad, who also participated in the contest. A month later, the two had become a couple. As their respective bands began to break up during 1969, Andersson and Ulvaeus teamed up and recorded their first album together in 1970, called "Lycka" ("Happiness"), which included original songs sung by both men. Their spouses were often present in the recording studio, and sometimes added backing vocals; Fältskog even co-wrote a song with the two. Ulvaeus still occasionally recorded and performed with The Hootenanny Singers until the summer of 1974, and Andersson took part in producing their records.
Agnetha Fältskog (born 5 April 1950 in Jönköping, Sweden) sang with a local dance band headed by Bernt Enghardt who sent a demo recording of the band to Karl Gerhard Lundkvist. The demo tape featured a song written and sung by Agnetha: "Jag var så kär". Lundkvist was so impressed with her voice that he was convinced she would be a star. After going through considerable effort to locate the singer, he arranged for Agnetha to come to Stockholm and to record two of her own songs. This led to Agnetha at the age of 18 having a number-one record in Sweden with a self-composed song, which later went on to sell over 80,000 copies. She was soon noticed by the critics and songwriters as a talented singer/songwriter of schlager style songs. Fältskog's main inspiration in her early years were singers such as Connie Francis. Along with her own compositions, she recorded covers of foreign hits and performed them on tours in Swedish folkparks. Most of her biggest hits were self-composed, which was quite unusual for a female singer in the 1960s. Agnetha released four solo LPs between 1968 and 1971. She had many successful singles in the Swedish charts.
During filming of a Swedish TV special in May 1969, Fältskog met Ulvaeus, and they married on 6 July 1971. Fältskog and Ulvaeus eventually were involved in each other's recording sessions, and soon even Andersson and Lyngstad added backing vocals to her third studio album, "Som jag är" ("As I Am") (1970). In 1972, Fältskog starred as Mary Magdalene in the original Swedish production of "Jesus Christ Superstar" and attracted favourable reviews. Between 1967 and 1975, Fältskog released five studio albums.
Anni-Frid "Frida" Lyngstad (born 15 November 1945 in Bjørkåsen in Ballangen, Norway) sang from the age of 13 with various dance bands, and worked mainly in a jazz-oriented cabaret style. She also formed her own band, the Anni-Frid Four. In the summer of 1967, she won a national talent competition with "En ledig dag" ("A Day Off") a Swedish version of the bossa nova song "A Day in Portofino", which is included in the EMI compilation "Frida 1967–1972". The first prize was a recording contract with EMI Sweden and to perform live on the most popular TV shows in the country. This TV performance, amongst many others, is included in the 3½-hour documentary "Frida – The DVD". Lyngstad released several schlager style singles on EMI without much success. When Benny Andersson started to produce her recordings in 1971, she had her first number-one single, "Min egen stad" ("My Own Town"), written by Benny and featuring all the future ABBA members on backing vocals. Lyngstad toured and performed regularly in the folkpark circuit and made appearances on radio and TV. She met Ulvaeus briefly in 1963 during a talent contest, and Fältskog during a TV show in early 1968.
Lyngstad finally linked up with her future bandmates in 1969. On 1 March 1969, she participated in the Melodifestival, where she met Andersson for the first time. A few weeks later they met again during a concert tour in southern Sweden and they soon became a couple. Andersson produced her single "Peter Pan" in September 1969—her first collaboration with Benny & Björn, as they had written the song. Andersson would then produce Lyngstad's debut studio album, "Frida", which was released in March 1971. Lyngstad also played in several revues and cabaret shows in Stockholm between 1969 and 1973. After ABBA formed, she recorded another successful album in 1975, "Frida ensam", which included a Swedish rendition of "Fernando", a hit on the Swedish radio charts before the English version was released.
First live performance and the start of "Festfolket".
An attempt at combining their talents occurred in April 1970 when the two couples went on holiday together to the island of Cyprus. What started as singing for fun on the beach ended up as an improvised live performance in front of the United Nations soldiers stationed on the island. Andersson and Ulvaeus were at this time recording their first album together, "Lycka", which was to be released in September 1970. Fältskog and Lyngstad added backing vocals on several tracks during June, and the idea of their working together saw them launch a stage act, "Festfolket" (which translates from Swedish to mean both "Party People" and "Engaged Couples"), on 1 November 1970 in Gothenburg.
The cabaret show attracted generally negative reviews, except for the performance of the Andersson and Ulvaeus hit "Hej, gamle man" ("Hello, Old Man")—the first Björn and Benny recording to feature all four. They also performed solo numbers from respective albums, but the lukewarm reception convinced the foursome to shelve plans for working together for the time being, and each soon concentrated on individual projects again.
First record together "Hej, gamle man".
"Hej, gamle man", a song about an old Salvation Army soldier, became the quartet's first hit. The record was credited to Björn & Benny and reached number-five on the sales charts and number-one on Svensktoppen, staying there for 15 weeks.
It was during 1971 that the four artists began working together more, adding vocals to the others' recordings. Fältskog, Andersson and Ulvaeus toured together in May, while Lyngstad toured on her own. Frequent recording sessions brought the foursome closer together during the summer.
1970–73: Forming the group.
After the 1970 release of "Lycka", two more singles credited to 'Björn & Benny' were released in Sweden, "Det kan ingen doktor hjälpa" ("No Doctor Can Help with That") and "Tänk om jorden vore ung" ("Imagine If the Earth Were Young"), with more prominent vocals by Fältskog and Lyngstad–and moderate chart success.
Fältskog and Ulvaeus, now married, started performing together with Andersson on a regular basis at the Swedish folkparks during the summer of 1971.
Stig Anderson, founder and owner of Polar Music, was determined to break into the mainstream international market with music by Andersson and Ulvaeus. "One day the pair of you will write a song that becomes a worldwide hit", he predicted. Stig Anderson encouraged Ulvaeus and Andersson to write a song for Melodifestivalen, and after two rejected entries in 1971, Andersson and Ulvaeus submitted their new song "Säg det med en sång" ("Say It with a Song") for the 1972 contest, choosing newcomer Lena Anderson to perform. The song came in third place, encouraging Stig Anderson, and became a hit in Sweden.
The first signs of foreign success came as a surprise, as the Andersson and Ulvaeus single "She's My Kind of Girl" was released through Epic Records in Japan in March 1972, giving the duo a Top 10 hit. Two more singles were released in Japan, "En Carousel" ("En Karusell" in Scandinavia, an earlier version of "Merry-Go-Round") and "Love Has Its Ways" (a song they wrote with Kōichi Morita).
First hit as Agnetha, Anni-Frid, Benny & Björn.
Ulvaeus and Andersson persevered with their songwriting and experimented with new sounds and vocal arrangements. "People Need Love" was released in June 1972, featuring guest vocals by the women, who were now given much greater prominence. Stig Anderson released it as a single, credited to "Björn & Benny, Agnetha & Anni-Frid". The song peaked at number 17 in the Swedish combined single and album charts, enough to convince them they were on to something. The single also became the first record to chart for the quartet in the United States, where it peaked at number 114 on the "Cashbox" singles chart and number 117 on the "Record World" singles chart. Labeled as "Björn & Benny (with Svenska Flicka)", it was released there through Playboy Records. However, according to Stig Anderson, "People Need Love" could have been a much bigger American hit, but a small label like Playboy Records did not have the distribution resources to meet the demand for the single from retailers and radio programmers.
The foursome decided to record their first album together in the autumn of 1972, and sessions began on 26 September 1972. The women shared lead vocals on "Nina, Pretty Ballerina" (a top ten hit in Austria) that day, and their voices in harmony for the first time gave the foursome an idea of the quality of their combined talents.
"Ring Ring".
In 1973, the band and their manager Stig Anderson decided to have another try at Melodifestivalen, this time with the song "Ring Ring". The studio sessions were handled by Michael B. Tretow, who experimented with a "wall of sound" production technique that became the wholly new sound. Stig Anderson arranged an English translation of the lyrics by Neil Sedaka and Phil Cody and they thought this would be a surefire winner. However, on 10 February 1973, the song came third in Melodifestivalen; thus it never reached the Eurovision Song Contest itself. Nevertheless, the group released their debut studio album, also called "Ring Ring". The album did well and the "Ring Ring" single was a hit in many parts of Europe and also in South Africa. However, Stig Anderson felt that the true breakthrough could only come with a UK or US hit.
When Agnetha Fältskog gave birth to her first child in 1973, she was replaced for a short period by Inger Brundin on a trip to West Germany.
Official naming.
In early 1973, Stig Anderson, tired of unwieldy names, started to refer to the group privately and publicly as ABBA. At first, this was a play on words, as Abba is also the name of a well-known fish-canning company in Sweden, and itself an acronym. However, since the fish-canners were unknown outside Sweden, Anderson came to believe the name would work in international markets. A competition to find a suitable name for the group was held in a Gothenburg newspaper. The group was impressed with the names "Alibaba", "FABB", and "Baba", but in the end all the entries were ignored and it was officially announced in the summer that the group were to be known as "ABBA." The group negotiated with the canners for the rights to the name. "ABBA" is an acronym formed from the first letters of each group member's first name: Agnetha, Björn, Benny and Anni-Frid. During a promotional photo, Benny flipped his "B" horizontally for fun, and from 1976 onwards the first 'B' in the logo version of the name was "mirror-image" reversed on the band's promotional material and ᗅᗺᗷᗅ® became the group's registered trademark.
The first time "ABBA" is found written on paper is on a recording session sheet from the Metronome Studio in Stockholm, dated 16 October 1973. This was first written as "Björn, Benny, Agnetha & Frida", but was subsequently crossed out with "ABBA" written in large letters on top.
Official logo.
The official logo, using the bold version of the News Gothic typeface, was designed by Rune Söderqvist, and appeared for the first time on the "Dancing Queen" single in August 1976, and subsequently on all later original albums and singles. But the idea for the official logo was made by the German photographer Wolfgang Heilemann on a "Dancing Queen" shoot for the teenage magazine Bravo. On the photo, the ABBA members held a giant initial letter of his/her name. After the pictures were made, Heilemann found out that one of the men held his letter backwards as in ᗅᗺᗷᗅ. They discussed it and the members of ABBA liked it.
Following their acquisition of the group's catalogue, Polygram began using variations of the ABBA logo, using a different font and adding a crown emblem to it in 1992 for the first release of the "ABBA Gold: Greatest Hits" compilation. When Universal Music purchased Polygram (and, thus, ABBA's label Polar Music International), control of the group's catalogue was returned to Stockholm. Since then, the original logo has been reinstated on all official products.
1973–76: Breakthrough.
Eurovision Song Contest 1974.
As the group entered the Melodifestivalen with "Ring Ring" but failed to qualify as the 1973 Swedish entry, Stig Anderson immediately started planning for the 1974 contest.
Ulvaeus, Andersson and Stig Anderson believed in the possibilities of using the Eurovision Song Contest as a way to make the music business aware of them as songwriters, as well as the band itself. In late 1973, they were invited by Swedish television to contribute a song for the Melodifestivalen 1974 and from a number of new songs, the upbeat song "Waterloo" was chosen; the group was now inspired by the growing glam rock scene in England.
ABBA won their national heats on Swedish television on 9 February 1974, and with this third attempt were far more experienced and better prepared for the Eurovision Song Contest. Winning the 1974 Contest on 6 April 1974 gave ABBA the chance to tour Europe and perform on major television shows; thus the band saw the "Waterloo" single chart in many European countries. "Waterloo" was ABBA's first number-one single in big markets such as the UK and West Germany. In the United States, the song peaked at number-six on the "Billboard" Hot 100 chart, paving the way for their first album and their first trip as a group there. Albeit a short promotional visit, it included their first performance on American television, "The Mike Douglas Show". The album "Waterloo" only peaked at number 145 on the "Billboard" 200 chart, but received unanimous high praise from the US critics: "Los Angeles Times" called it "a compelling and fascinating debut album that captures the spirit of mainstream pop quite effectively … an immensely enjoyable and pleasant project", while "Creem" characterized it as "a perfect blend of exceptional, lovable compositions".
ABBA's follow-up single, "Honey, Honey", peaked at number 27 on the US "Billboard" Hot 100, and was a number-two hit in West Germany. However, in the United Kingdom, ABBA's British record label, Epic, decided to re-release a remixed version of "Ring Ring" instead of "Honey, Honey", and a cover version of the latter by Sweet Dreams peaked at number 10. Both records debuted on the UK chart within one week of each other. "Ring Ring" failed to reach the Top 30 in the United Kingdom, increasing growing speculation that the group was simply a Eurovision one-hit wonder.
Post-Eurovision.
In November 1974, ABBA embarked on their first European tour, playing dates in Denmark, West Germany and Austria. It was not as successful as the band had hoped, since most of the venues did not sell out. Due to a lack of demand, they were even forced to cancel a few shows, including a sole concert scheduled in Switzerland. The second leg of the tour, which took them through Scandinavia in January 1975, was very different. They played to full houses everywhere and finally got the reception they had aimed for. Live performances continued during the summer of 1975 when ABBA embarked on a fourteen open-air date tour of Sweden and Finland. Their Stockholm show at the Gröna Lund amusement park had an estimated audience of 19,200. Björn Ulvaeus later said that "If you look at the singles we released straight after Waterloo, we were trying to be more like the Sweet, a semi-glam rock group, which was stupid because we were always a pop group."
In late 1974, "So Long" was released as a single in the United Kingdom but it received no airplay from Radio 1 and failed to chart. In the summer of 1975, ABBA released "I Do, I Do, I Do, I Do, I Do", which again received little airplay on Radio 1 but managed to climb the charts, to number 38. Later that year, the release of their self-titled third studio album "ABBA" and single "SOS" brought back their chart presence in the UK, where the single hit number six and the album peaked at number 13. "SOS" also became ABBA's second number-one single in Germany and their third in Australia. Success was further solidified with "Mamma Mia" reaching number-one in the United Kingdom, Germany and Australia. In the United States, "SOS" peaked at number 10 on the Record World Top 100 singles chart and number 15 on the "Billboard" Hot 100 chart, picking up the BMI Award along the way as one of the most played songs on American radio in 1975.
The success of the group in the United States had until that time been limited to single releases. By early 1976, the group already had four Top 30 singles on the US charts, but the album market proved to be tough to crack. The eponymous "ABBA " album generated three American hits, but it only peaked at number 165 on the "Cashbox" album chart and number 174 on the "Billboard" 200 chart. Opinions were voiced, by "Creem" in particular, that in the US ABBA had endured "a very sloppy promotional campaign". Nevertheless, the group enjoyed warm reviews from the American press. "Cashbox" went as far as saying that "there is a recurrent thread of taste and artistry inherent in Abba's marketing, creativity and presentation that makes it almost embarrassing to critique their efforts", while "Creem" wrote: "SOS is surrounded on this LP by so many good tunes that the mind boggles".
In Australia, the airing of the music videos for "I Do, I Do, I Do, I Do, I Do" and "Mamma Mia" on the nationally-broadcast TV pop show "Countdown" (which premiered in November 1974) saw the band rapidly gain enormous popularity, and "Countdown" become a key promoter of the group via their distinctive music videos. This started an immense interest for ABBA in Australia, resulting in both the single and album holding down the No. 1 positions on the charts for months.
1976–81: Superstardom.
In March 1976, the band released the compilation album "Greatest Hits", despite having had only six top 40 hits in the United Kingdom and the United States. Nevertheless, it became their first UK number-one album, and also took ABBA into the Top 50 on the US album charts for the first time, eventually selling more than a million copies there. At the same time, Germany released a compilation named "The Very Best of ABBA", also becoming a number-one album there whereas the "Greatest Hits" compilation followed a few months later to number-two on the German charts, despite all similarities with "The Very Best" album. Also included on "Greatest Hits" was a new single, "Fernando", which went to number-one in at least thirteen countries worldwide, including the United Kingdom, Germany and Australia, and the single went on to sell over 10 million copies worldwide. In Australia, the song occupied the top position for 14 weeks (and stayed in the chart for 40 weeks), tying with the Beatles' "Hey Jude" for longest-running number-one, and making "Fernando" one of the best-selling singles of all time in Australia. That same year, the group received its first international prize, with "Fernando" being chosen as the "Best Studio Recording of 1975". In the United States, "Fernando" reached the Top 10 of the Cashbox Top 100 singles chart and number 13 on the "Billboard" Hot 100. It also topped the "Billboard" Adult Contemporary chart, ABBA's first American number-one single on any chart.
In the UK, the group had nine Number One's in the chart between 1974 and 1980. The only recording artists with more UK number 1s by 1980 were Elvis Presley and The Beatles. The following songs were UK Number 1s: "Waterloo" ('74) "Mamma Mia" "Fernando" "Dancing Queen" "Knowing Me, Knowing You" "The Name of the Game" "Take a chance on me" "The Winner takes it all" & "Super Trouper" ('80).
The group's fourth studio album, "Arrival", a number-one bestseller in Europe and Australia, represented a new level of accomplishment in both songwriting and studio work, prompting rave reviews from more rock-oriented UK music weeklies such as "Melody Maker" and "New Musical Express", and mostly appreciative notices from US critics. Hit after hit flowed from "Arrival": "Money, Money, Money", another number-one in Germany and Australia, and "Knowing Me, Knowing You", ABBA's sixth consecutive German number-one as well as another UK number-one. The real sensation was "Dancing Queen", not only topping the charts in loyal markets the UK, Germany and Australia, but also reaching number-one in the United States. In South Africa, ABBA had astounding success with "Fernando", "Dancing Queen" and "Knowing Me, Knowing You" being among the top 20 best-selling singles for 1976–77. In 1977, "Arrival" was nominated for the inaugural BRIT Award in the category "Best International Album of the Year". By this time ABBA were popular in the United Kingdom, most of Western Europe, Australia and New Zealand. In "Frida – The DVD", Lyngstad explains how she and Fältskog developed as singers, as ABBA's recordings grew more complex over the years.
The band's popularity in the United States would remain on a comparatively smaller scale, and "Dancing Queen" became the only "Billboard" Hot 100 number-one single ABBA had there (they did, however, get three more singles to the number-one position on other "Billboard" charts, including "Billboard" Adult Contemporary and Hot Dance Club Play). Nevertheless, "Arrival" finally became a true breakthrough release for ABBA on the US album market where it peaked at number 20 on the "Billboard" 200 chart and was certified gold by RIAA.
European and Australian tour.
In January 1977, ABBA embarked on their first major tour. The group's status had changed dramatically and they were clearly regarded as superstars. They opened their much anticipated tour in Oslo, Norway, on 28 January, and mounted a lavishly produced spectacle that included a few scenes from their self-written mini-operetta "The Girl with the Golden Hair". The concert attracted immense media attention from across Europe and Australia. They continued the tour through Western Europe, visiting Gothenburg, Copenhagen, Berlin, Cologne, Amsterdam, Antwerp, Essen, Hanover, and Hamburg and ending with shows in the United Kingdom in Manchester, Birmingham, Glasgow and two sold-out concerts at London's Royal Albert Hall. Tickets for these two shows were available only by mail application and it was later revealed that the box-office received 3.5 million requests for tickets, enough to fill the venue 580 times. Along with praise ("ABBA turn out to be amazingly successful at reproducing their records", wrote "Creem"), there were complaints that "ABBA performed slickly...but with a zero personality coming across from a total of 16 people on stage" ("Melody Maker"). One of the Royal Albert Hall concerts was filmed as a reference for the filming of the Australian tour for what became "", though it is not exactly known how much of the concert was filmed.
After the European leg of the tour, in March 1977, ABBA played 11 dates in Australia before a total of 160,000 people. The opening concert in Sydney at the Sydney Showground on 3 March to an audience of 20,000 was marred by torrential rain with Lyngstad slipping on the wet stage during the concert. However, all four members would later recall this concert as the most memorable of their career. Upon their arrival in Melbourne, a civic reception was held at the Melbourne Town Hall and ABBA appeared on the balcony to greet an enthusiastic crowd of 6,000. In Melbourne, the group played three concerts at the Sidney Myer Music Bowl with 14,500 at each including the Australian Prime Minister Malcolm Fraser and his family. At the first Melbourne concert, an additional 16,000 people gathered outside the fenced-off area to listen to the concert. In Adelaide, the group performed one concert at West Lakes Football Stadium before 20,000 people, with another 10,000 listening outside. During the first of five concerts in Perth, there was a bomb scare with everyone having to evacuate the Entertainment Centre. The trip was accompanied by mass hysteria and unprecedented media attention ("Swedish ABBA stirs box-office in Down Under tour...and the media coverage of the quartet rivals that set to cover the upcoming Royal tour of Australia", wrote "Variety"), and is captured on film in "", directed by Lasse Hallström.
The Australian tour and its subsequent "ABBA: The Movie" produced some ABBA lore, as well. Fältskog's blonde good looks had long made her the band's "pin-up girl", a role she disdained. During the Australian tour, she performed in a skin-tight white jumpsuit, causing one Australian newspaper to use the headline "Agnetha's bottom tops dull show". When asked about this at a news conference, she replied: "Don't they have bottoms in Australia?"
In December 1977, ABBA followed up "Arrival" with the more ambitious fifth album "", released to coincide with the debut of "ABBA: The Movie". Although the album was less well received by UK reviewers, it did spawn more worldwide hits: "The Name of the Game" and "Take a Chance on Me", which both topped the UK charts, and peaked at number 12 and number three, respectively, on the "Billboard" Hot 100 chart in the US. Although "Take a Chance on Me" did not top the American charts, it proved to be ABBA's biggest hit single there, selling more copies than "Dancing Queen". "The Album" also included "Thank You for the Music", the B-side of "Eagle" in countries where the latter had been released as a single, and was belatedly released as an A-side single in the United Kingdom and Ireland in 1983. "Thank You for the Music" has become one of the best loved and best known ABBA songs without being released as a single during the group's lifetime.
Polar Music Studio formation.
By 1978 ABBA were one of the biggest bands in the world. They converted a vacant movie theatre into the Polar Music Studio, a state-of-the-art studio in Stockholm. The studio was used by several other bands; notably Genesis' "Duke" and Led Zeppelin's "In Through the Out Door" were recorded there. During May, the group went to the United States for a promotional campaign, performing alongside Andy Gibb on Olivia Newton-John's TV show. Recording sessions for the single "Summer Night City" were an uphill struggle, but upon release the song became another hit for the group. The track would set the stage for ABBA's foray into disco with their next album.
On 9 January 1979, the group performed "Chiquitita" at the Music for UNICEF Concert held at the United Nations General Assembly to celebrate UNICEF's Year of the Child. ABBA donated the copyright of this worldwide hit to the UNICEF; see Music for UNICEF Concert. The single was released the following week, and reached number-one in ten countries.
North American and European tours.
In mid-January 1979, Ulvaeus and Fältskog announced they were getting divorced. The news caused interest from the media and led to speculation about the band's future. ABBA assured the press and their fan base they were continuing their work as a group and that the divorce would not affect them. Nonetheless, the media continued to confront them with this in interviews. To escape the media swirl and concentrate on their writing, Andersson and Ulvaeus secretly travelled to Compass Point Studios in Nassau, Bahamas, where for two weeks they prepared their next album's songs.
The group's sixth studio album, "Voulez-Vous", was released in April 1979, the title track of which was recorded at the famous Criteria Studios in Miami, Florida, with the assistance of recording engineer Tom Dowd amongst others. The album topped the charts across Europe and in Japan and Mexico, hit the Top 10 in Canada and Australia and the Top 20 in the United States. None of the singles from the album reached number-one on the UK charts, but "Chiquitita", "Does Your Mother Know", "Angeleyes" (with "Voulez-Vous", released as a double A-side) and "I Have a Dream" were all UK Top 5 hits. In Canada, "I Have a Dream" became ABBA's second number-one on the RPM Adult Contemporary chart (after "Fernando" hit the top previously). Also in 1979, the group released their second compilation album, "Greatest Hits Vol. 2", which featured a brand new track: "Gimme! Gimme! Gimme! (A Man After Midnight)", another number-three hit in both the UK and Germany. In Russia during the late 1970s, the group was paid in oil commodities because of an embargo on the ruble.
On 13 September 1979, ABBA began their at the Northlands Coliseum in Edmonton, Canada, with a full house of 14,000. "The voices of the band, Agnetha's high sauciness combined with round, rich lower tones of Anni-Frid, were excellent...Technically perfect, melodically correct and always in perfect pitch...The soft lower voice of Anni-Frid and the high, edgy vocals of Agnetha were stunning", raved "Edmonton Journal".
During the next four weeks they played a total of 17 sold-out dates, 13 in the United States and four in Canada. The last scheduled ABBA concert in the United States in Washington, D.C. was cancelled due to Fältskog's emotional distress suffered during the flight from New York to Boston, when the group's private plane was subjected to extreme weather conditions and was unable to land for an extended period. They appeared at the Boston Music Hall for the performance 90 minutes late. The tour ended with a show in Toronto, Canada at Maple Leaf Gardens before a capacity crowd of 18,000. "ABBA plays with surprising power and volume; but although they are loud, they're also clear, which does justice to the signature vocal sound...Anyone who's been waiting five years to see Abba will be well satisfied", wrote "Record World".
On 19 October 1979, the tour resumed in Western Europe where the band played 23 sold-out gigs, including six sold-out nights at London's Wembley Arena.
Progression.
In March 1980, ABBA travelled to Japan where upon their arrival at Narita International Airport, they were besieged by thousands of fans. The group played eleven concerts to full houses, including six shows at Tokyo's Budokan. This tour was the last "on the road" adventure of their career.
In the summer of 1980, the group released the single "The Winner Takes It All" the group's eighth UK chart topper (and their first since 1978). The song is widely misunderstood as being written about Ulvaeus and Fältskog's marital tribulations; Ulvaeus wrote the lyrics, but has stated they were not about his own divorce; Fältskog has repeatedly stated she was not the loser in their divorce. In the United States, the single peaked at number-eight on the "Billboard" Hot 100 chart and became ABBA's second "Billboard" Adult Contemporary number-one. It was also re-recorded by Andersson and Ulvaeus with a slightly different backing track, by French chanteuse Mireille Mathieu at the end of 1980 – as "Bravo Tu As Gagné", with French lyrics by Alain Boublil. November the same year saw the release of ABBA's seventh album "Super Trouper", which reflected a certain change in ABBA's style with more prominent use of synthesizers and increasingly personal lyrics. It set a record for the most pre-orders ever received for a UK album after one million copies were ordered before release. The second single from the album, "Super Trouper", also hit number-one in the UK, becoming the group's ninth and final UK chart-topper. Another track from the "Super Trouper" album, "Lay All Your Love on Me", released in 1981 as a single only in selected territories, managed to top the "Billboard" Hot Dance Club Play chart and peaked at number-seven on the UK singles chart becoming, at the time, the highest ever charting release in UK chart history.
Also in 1980, ABBA recorded a compilation of Spanish-language versions of their hits called "Gracias Por La Música". This was released in Spanish-speaking countries as well as in Japan and Australia. The album became a major success, and along with the Spanish version of "Chiquitita", this signalled the group's breakthrough in Latin America. "ABBA Oro: Grandes Éxitos", the Spanish equivalent of "ABBA Gold: Greatest Hits", was released in 1999.
1981–82: Final album and performances.
In January 1981, Ulvaeus married Lena Källersjö, and manager Stig Anderson celebrated his 50th birthday with a party. For this occasion, ABBA recorded the track "Hovas Vittne" (a pun on the Swedish name for Jehovah's Witness and Anderson's birthplace, Hova) as a tribute to him, and released it only on 200 red vinyl copies, to be distributed to the guests attending the party. This single has become a sought-after collectible. In mid-February 1981, Andersson and Lyngstad announced they were filing for divorce. Information surfaced that their marriage had been an uphill struggle for years, and Benny had already met another woman, Mona Nörklit, whom he married in November 1981.
Andersson and Ulvaeus had songwriting sessions during the spring of 1981, and recording sessions began in mid-March. At the end of April, the group recorded a TV special, "Dick Cavett Meets ABBA" with the US talk show host Dick Cavett. "The Visitors", ABBA's eighth and final studio album, showed a songwriting maturity and depth of feeling distinctly lacking from their earlier recordings but still placing the band squarely in the pop genre, with catchy tunes and harmonies. Although not revealed at the time of its release, the album's title track, according to Ulvaeus, refers to the secret meetings held against the approval of totalitarian governments in Soviet-dominated states, while other tracks address topics like failed relationships, the threat of war, ageing, and loss of innocence. The album's only major single release, "One of Us", proved to be the last of ABBA's nine number-one singles in Germany, this being in December 1981; and the swansong of their sixteen Top 5 singles on the South African chart. "One of Us" was also ABBA's final Top 10 hit in the UK.
Although it topped the album charts across most of Europe, including the UK and Germany, "The Visitors" was not as commercially successful as its predecessors, showing a commercial decline in previously loyal markets such as France, Australia and Japan. A track from the album, "When All Is Said and Done", was released as a single in North America, Australia and New Zealand, and fittingly became ABBA's final Top 40 hit in the US (debuting on the US charts on 31 December 1981), while also reaching the US Adult Contemporary Top 10, and number-four on the RPM Adult Contemporary chart in Canada. The song's lyrics, as with "The Winner Takes It All" and "One of Us", dealt with the painful experience of separating from a long-term partner, though it looked at the trauma more optimistically. With the now publicised story of Andersson and Lyngstad's divorce, speculation increased of tension within the band. Also released in the United States was the title track of "The Visitors", which hit the Top Ten on the "Billboard" Hot Dance Club Play chart.
Final recording sessions.
In the spring of 1982, songwriting sessions had started and the group came together for more recordings. Plans were not completely clear, but a new album was discussed and the prospect of a small tour suggested. The recording sessions in May and June 1982 were a struggle, and only three songs were eventually recorded: "You Owe Me One", "I Am the City" and "Just Like That". Andersson and Ulvaeus were not satisfied with the outcome, so the tapes were shelved and the group took a break for the summer.
Back in the studio again in early August, the group had changed plans for the rest of the year: they settled for a Christmas release of a double album compilation of all their past single releases to be named "". New songwriting and recording sessions took place, and during October and December, they released the singles "The Day Before You Came"/"Cassandra" and "Under Attack"/"You Owe Me One", the A-sides of which were included on the compilation album. Neither single made the Top 20 in the United Kingdom, though "The Day Before You Came" became a Top 5 hit in many European countries such as Germany, the Netherlands and Belgium. The album went to number-one in the UK and Belgium, Top 5 in the Netherlands and Germany and Top 20 in many other countries. "Under Attack", the group's final release before disbanding, was a Top 5 hit in the Netherlands and Belgium.
"I Am the City" and "Just Like That" were left unreleased on "The Singles: The First Ten Years" for possible inclusion on the next projected studio album, though this never came to fruition. "I Am the City" was eventually released on the compilation album "" in 1993, while "Just Like That" has been recycled in new songs with other artists produced by Andersson and Ulvaeus. A reworked version of the verses ended up in the musical "Chess". The chorus section of "Just Like That" was eventually released on a retrospective box set in 1994, as well as in the "ABBA Undeleted" medley featured on disc 9 of "The Complete Studio Recordings". Despite a number of requests from fans, Ulvaeus and Andersson are still refusing to release ABBA's version of "Just Like That" in its entirety, even though the complete version surfaced on bootlegs.
The group travelled to London to promote "The Singles: The First Ten Years" in the first week of November 1982, appearing on "Saturday Superstore" and "The Late, Late Breakfast Show", and also to West Germany in the second week, to perform on Show Express. On 19 November 1982, ABBA appeared for the last time in Sweden on the TV programme Nöjesmaskinen, and on 11 December 1982, they made their last performance ever, transmitted to the UK on Noel Edmonds' "The Late, Late Breakfast Show", through a live link from a TV studio in Stockholm.
Final performances.
Andersson and Ulvaeus began collaborating with Tim Rice in early 1983 on writing songs for the musical project "Chess", while Fältskog and Lyngstad both concentrated on international solo careers. While Andersson and Ulvaeus were working on the musical, a further co-operation among the three of them came with the musical "Abbacadabra" that was produced in France for television. It was a children's musical utilising 14 ABBA songs. Alain and Daniel Boublil, who wrote "Les Misérables", had been in touch with Stig Anderson about the project, and the TV musical was aired over Christmas on French TV and later a Dutch version was also broadcast. Boublil previously also wrote the French lyric for Mireille Mathieu's version of "The Winner Takes It All".
Lyngstad, who had recently moved to Paris, participated in the French version, and recorded a single, "Belle", a duet with French singer Daniel Balavoine. The song was a cover of ABBA's 1976 instrumental track "Arrival". As the single "Belle" sold well in France, Cameron Mackintosh wanted to stage an English-language version of the show in London, with the French lyrics translated by David Wood and Don Black; Andersson and Ulvaeus got involved in the project, and contributed with one new song, "The Seeker". "Abbacadabra" premièred on 8 December 1983 at The Lyric Hammersmith Theatre in London, to mixed reviews and full houses for eight weeks, closing on 21 January 1984. Lyngstad was also involved in this production, recording "Belle" in English as "Time", a duet with actor and singer B. A. Robertson: the single sold well, and was produced and recorded by Andersson and Ulvaeus.
Anni-Frid Lyngstad performed "I Have A Dream" with a children's choir on French television in 1984, solo.
All four members made their final public appearance, as four friends more than as ABBA, in January 1986, when they recorded a video of themselves performing an acoustic version of "Tivedshambo", which was the first song written by their manager, Stig Anderson, for a Swedish TV show honouring Anderson on his 55th birthday. The four had not seen each other for more than two years. That same year they also performed privately at another friend's 40th birthday: their old tour manager, Claes af Geijerstam. They sang a self-written song titled "Der Kleine Franz" that was later to resurface in "Chess". Also in 1986, "ABBA Live" was released, featuring selections of live performances from the group's 1977 and 1979 tours. The four members were guests at the 50th birthday of Görel Hanser in 1999. Hanser was a long-time friend of all four, and also former secretary of Stig Anderson. Honouring Görel, ABBA performed a Swedish birthday song "Med En Enkel Tulipan" a cappella.
Benny Andersson has on several occasions performed old ABBA songs. In June 1992, he and Ulvaeus appeared with U2 at a Stockholm concert, singing the chorus of "Dancing Queen", and a few years later during the final performance of the B & B in Concert in Stockholm, Andersson joined the cast for an encore at the piano. Andersson frequently adds an ABBA song to the playlist when he performs with his BAO band. He also played the piano during new recordings of the ABBA songs "Like an Angel Passing Through My Room" with opera singer Anne Sofie von Otter, and "When All Is Said and Done" with Swede Viktoria Tolstoy. In 2002, Andersson and Ulvaeus both performed an a cappella rendition of the first verse of "Fernando" as they accepted their Ivor Novello award in London. Lyngstad performed and recorded an a cappella version of "Dancing Queen" with the Swedish group The Real Group in 1993, and has also re-recorded "I Have a Dream" with Swiss singer Dan Daniell in 2003.
Permanent break.
ABBA has never officially announced the end of the group, but it has long been considered dissolved. Their final public performance together as ABBA was on the British TV programme "The Late, Late Breakfast Show" (live from Stockholm) on 11 December 1982. While reminiscing on the "The Day Before You Came", Ulvaeus said: "we might have continued for a while longer if that had been a number one". In January 1983, Fältskog started recording sessions for a solo album, as Lyngstad had successfully released her album "Something's Going On" some months earlier. Ulvaeus and Andersson, meanwhile, started songwriting sessions for the musical "Chess". In interviews at the time, Björn and Benny denied the split of ABBA ("Who are we without our ladies? Initials of Brigitte Bardot?"), and Lyngstad and Fältskog kept claiming in interviews that ABBA would come together for a new album repeatedly during 1983 and 1984. Internal strife between the group and their manager escalated and the band members sold their shares in Polar Music during 1983. Except for a TV appearance in 1986, the foursome did not come together publicly again until they were reunited at the Swedish premiere of the "Mamma Mia!" movie on 4 July 2008.
In an interview with the "Sunday Telegraph", following the premiere, Ulvaeus and Andersson confirmed that there was nothing that could entice them back on stage again. Ulvaeus said: "We will never appear on stage again. [...] There is simply no motivation to re-group. Money is not a factor and we would like people to remember us as we were. Young, exuberant, full of energy and ambition. I remember Robert Plant saying Led Zeppelin were a cover band now because they cover all their own stuff. I think that hit the nail on the head."
However, on 3 January 2011, Fältskog, who has been long considered to be the most reclusive member of the group and possibly also the major obstacle to any reunion, raised the possibility of reuniting for a one-off engagement. She admitted that she has not yet brought the idea up to the other three members. In April 2013, she reiterated her hopes for reunion during an interview with "Die Zeit", stating: "If they ask me, I'll say yes."
In a May 2013 interview, Fältskog, aged 63 at the time, confirmed that an ABBA reunion will never eventuate: "I think we have to accept that it will not happen, because we are too old and each one of us has their own life. Too many years have gone by since we stopped, and there’s really no meaning in putting us together again." Fältskog further explained that the band members remained on amenable terms: "It’s always nice to see each other now and then and to talk a little and to be a little nostalgic." In an April 2014 interview, Fältskog, when asked about whether the band might reunite for a new recording said: "It's difficult to talk about this because then all the news stories will be: 'ABBA is going to record another song!' But as long as we can sing and play, then why not? I would love to, but it's up to Björn and Benny."
Solo careers.
Benny Andersson and Björn Ulvaeus.
In October 1984, Ulvaeus and Andersson together with lyricist Tim Rice released the musical concept double album "Chess". The singles "One Night in Bangkok" (with vocals by Murray Head and Anders Glenmark ) and "I Know Him So Well" (a duet by Barbara Dickson and Elaine Paige, and later also recorded by both Barbra Streisand and Whitney Houston) were both hugely successful. The former reached number-one in Australia, Germany, Spain and Switzerland; number-two in Austria, France and New Zealand; number-three in Canada, Norway, Sweden and the US, as well as reaching the top 10 in a few other countries. In May 1986, the musical premièred in London's West End, and ran for almost three years. "Chess" also opened on Broadway in April 1988, but closed within two months due to bad reviews. In Stockholm, the composers staged "Chess på svenska" ("Chess in Swedish") in 2003, with some new material, including the musical numbers ""Han är en man, han är ett barn"" ("He's a Man, He's a Child") and ""Glöm mig om du kan"" ("Forget Me If You Can"). In 2008, the musical was again revived for a successful staging at London's Royal Albert Hall which was subsequently released on DVD, and then in two successful separate touring productions in the United States and United Kingdom, in 2010.
Andersson and Ulvaeus' next project, "Kristina från Duvemåla", an epic Swedish musical, premiered in Malmö, in southern Sweden in October 1995. The musical ran for five years in Stockholm, and an English version has been in development for some considerable time. It has been reported that a Broadway production is in its earliest stages of pre-production. In the meantime, following some earlier workshops, a full presentation of the English translation of the musical in concert, now with the shortened name of ""Kristina"", took place to capacity crowds in September 2009 at New York's Carnegie Hall, and in April 2010 at London's Royal Albert Hall, followed by a CD release of the New York recordings.
Since 1983, besides "Chess" and "Kristina från Duvemåla", Benny Andersson has continued writing songs with Ulvaeus. The pair produced two English-language pop albums with Swedish duo Gemini in 1985 and 1987. In 1987, Andersson also released his first solo album on his own label, Mono Music, called ""Klinga mina klockor"" ("Ring My Bells"), all new material inspired by Swedish folk music – and followed it with his second album titled "November 1989".
During the 1990s, Andersson wrote music for the popular Swedish cabaret quartet Ainbusk Singers, giving them two hits: "Lassie" and ""Älska mig"" ("Love me"), and later produced "Shapes", an English-language album by the group's Josefin Nilsson with all-new material by Andersson and Ulvaeus. Andersson has also regularly written music for films (most notably to Roy Andersson's "Songs from the Second Floor"). In 2001, Andersson formed his own band, Benny Anderssons Orkester (BAO), which released three successful albums in 2001, 2004 and 2007. Andersson has the distinction of remaining the longest in the Swedish Radio Svensktoppen charts; the song ""Du är min man"" ("You Are My Man"), sung by Helen Sjöholm, spent 278 weeks there between 2004 and 2009. Andersson released his third album BAO 3 in October 2007, of new material with his band BAO and vocalists Helen Sjöholm and Tommy Körberg, as well as playing to full houses at two of Sweden's largest concert venues in October and November 2007, with an audience of 14,000.
Andersson and Ulvaeus have been highly involved in the worldwide productions of the musical "Mamma Mia!", alongside Lyngstad who attends premieres. They were also involved in the production of the successful film version of the musical, which opened in July 2008. Andersson produced the soundtrack utilising many of the musicians ABBA used on their albums and tours. Andersson made a cameo appearance in the movie as a 'fisherman' piano player in the "Dancing Queen" scene, while Ulvaeus is seen as a Greek god playing a lyre during the closing credits.
Andersson and Ulvaeus have continuously been writing new material; most recently the two wrote 7 songs for Anderssons 'BAO' 2011 album 'O Klang Och Jubeltid', performed as usual by vocalists Sjöholm, Körberg and Moreus. In July 2009, 'BAO' released their first international release, now named "The Benny Andersson Band", with the album "The Story of a Heart". The album was a compilation of 14 tracks from Andersson's five Swedish-language releases between 1987 and 2007, including five songs now recorded with lyrics by Ulvaeus in English, and the new title song premiered on BBC2's "Ken Bruce Show". A Swedish-language version of the title track, ""Sommaren Du Fick"" ("The Summer You Got"), was released as a single in Sweden prior to the English version, with vocals by Helen Sjöholm. In the spring of 2009, Andersson also released a single recorded by the staff at his privately owned Stockholm hotel "Hotel Rival", titled "2nd Best to None", accompanied by a video showing the staff at work. In 2008, Andersson and Ulvaeus wrote a song for Swedish singer Sissela Kyle, titled ""Jag vill bli gammal"" ("I Wanna Grow Old"), for her Stockholm stage show ""Your Days Are Numbered"", which was never recorded and released, but did get a TV performance. Ulvaeus also contributed lyrics to ABBA's 1976 instrumental track "Arrival" for Sarah Brightman's cover version recorded for her 2008 album "Winter Symphony". New English lyrics have also been written for Andersson's 1999 song ""Innan Gryningen"" (then also named "Millennium Hymn"), with the new title "The Silence of the Dawn" for Barbara Dickson (performed live, but not yet recorded and released). In 2007, they wrote the new song ""Han som har vunnit allt"" ("He Who's Won It All") for actor/singer Anders Ekborg. Björn wrote English lyrics for two older songs from Benny's solo albums: "I Walk with You Mama" ("Stockholm by Night", 1989) and "After the Rain" ("Efter regnet", 1987) for opera singer Anne Sofie von Otter, for her Andersson tribute album "I Let the Music Speak". Barbara Dickson recorded (but not yet released) a Björn & Benny song called 'The Day The Wall Came Tumbling Down'; the song eventually was released by Australian 'Mamma Mia!' musical star Anne Wood 201 album of ABBA covers, Divine Discontent. As of October 2012, Björn Ulvaeus has mentioned writing new material with Benny for a 'BAO' Christmas release (also mentioned as a BAO 'box'), and Benny is busy writing music for a Swedish language obscure musical, 'Hjälp Sökes' ('Help is Wanted') together with Kristina Lugn and Lars Rudolfsson, premiering 8 February 2013. Andersson has also written music for a documentary film about Olof Palme, re-recording the track 'Sorgmarch' from his last album throughout the film.
Agnetha Fältskog and Anni-Frid Lyngstad.
Both female members of ABBA pursued solo careers on the international scene after their work with the group. In 1982, Lyngstad chose Genesis drummer and vocalist Phil Collins to produce the album "Something's Going On" and unveiled the hit single and video "I Know There's Something Going On" in the autumn of that year. The single became a number-one hit in France (where it spent five weeks at the top), Belgium, Switzerland and Costa Rica. The track reached number-three in Austria, the Netherlands, Norway, Sweden and Poland, and was also a Top 10 hit in Germany, Italy, Finland, South Africa and Australia. Sveriges Television documented this historical event, by filming the whole recording process. The result became a one-hour TV documentary, including interviews with Lyngstad, Collins, Ulvaeus and Andersson as well as all the musicians. This documentary and the promotion videos from the album are included in "Frida - The DVD".
Lyngstad's second solo album after ABBA was called "Shine", produced by Steve Lillywhite. "Shine" was recorded in Paris and released in 1984. "Shine" was Lyngstad's final studio album release for twelve years. It featured "Slowly", the last known Andersson-Ulvaeus composition to have been recorded by one of the former female ABBA vocalists to date. The promotion videos and clips for "Shine" are included in "Frida – The DVD".
In 1980, Agnetha Fältskog recorded Nu tändas tusen juleljus (Now a thousand Christmas candles are lit), a Swedish Christmas album along with her 7-year-old daughter Linda. The album was released in 1981. Nu tändas tusen julejus, which was Fältskog's first Swedish language recording for the Polar Music label after having left CBS-Cupol, peaked at No. 6 on the Swedish album chart in January 1982, has been re-released on CD by Polar Music/PolyGram/Universal Music all through the 1990s and 2000s and is one of the best-selling Swedish Christmas albums of all time. The album name is derived from one of Scandinavia's best-known Christmas carols.
In 1983, Fältskog released the solo album "Wrap Your Arms Around Me" which achieved platinum sales in Sweden. This included the single "The Heat Is On", which was a hit all over Europe and Scandinavia. It reached number-one in Sweden and Norway and number-two in the Netherlands and Belgium. In the United States, Fältskog earned a "Billboard" Top 30 hit with "Can't Shake Loose". In Europe, the single "Wrap Your Arms Around Me" was another successful hit, topping the charts in Belgium and Denmark, reaching the Top 5 in Sweden, the Netherlands and South Africa, and the Top 20 in Germany and France. The album sold 1.2 million copies worldwide. The album was produced by the highly successful producer and songwriter Mike Chapman, also known for his work with The Sweet, Mud, Suzi Quatro, Blondie, Pat Benatar and The Knack.
"It's So Nice to be Rich" was Agnetha's fourth top ten hit in Sweden in 1983. Her duet with Tomas Ledin, "Never Again" was the first one.
Fältskog's second English-language solo album, "Eyes of a Woman", was released in March 1985, peaking at number-two in Sweden and another platinum seller and performing reasonably well in Europe. The album was produced by Eric Stewart of 10cc. The first single from the album was her self-penned "I Won't Let You Go". Agnetha's duet with Ola Håkansson "The Way You Are" was a number-one hit in Sweden in 1986 and was awarded double platinum.
In early 1987, Agnetha recorded an album "Kom följ med I vår karusell" ('Come ride with me on my carousel') with her son Christian. The album contained songs for children and was sung in Swedish. For the album Agnetha recorded duets with her son and with a choir of children. She also recorded a few solo songs. The production was modern and fresh. The single 'Pa Sondag' was much played at the radio and even made the Swedish top 10, unique for a song made for kids to enjoy.
Also in November 1987, Fältskog released her third post-ABBA solo album, the Peter Cetera-produced "I Stand Alone", which also included the "Billboard" Adult Contemporary duet with Cetera, "I Wasn't the One (Who Said Goodbye)", as well as the European charting singles "The Last Time" and "Let It Shine". The album was extremely successful in Sweden, where it spent eight weeks at number-one and was awarded double-platinum. Shortly after some minor European promotion for the album in early 1988, Fältskog withdrew from public life and halted her music career. In 1996, she released her autobiography, "As I Am", and a compilation album featuring her solo hits alongside some ABBA classics.
In 2004, she made a successful comeback, when she released the critically acclaimed album "My Colouring Book" containing 1960s covers who had the most impact on her teenage years as a music contender. It debuted at number-one in Sweden (achieving triple-platinum status), number-six in Germany, and number 12 in the UK, winning a silver award, and achieving gold status in Finland. The single "If I Thought You'd Ever Change Your Mind" (a cover of the Cilla Black 1960s song) became Fältskog's biggest solo hit in the United Kingdom, reaching number 11. The single peaked at number-two in Sweden and was a hit throughout Scandinavia and Europe. A further single, "When You Walk in the Room", was released but met with less success, only peaking at number 34 in the United Kingdom. In January 2007, she sang a live duet on stage with Swedish singer Tommy Körberg at the after party for the final performance of the musical, "Mamma Mia!", in Stockholm, at which Benny Andersson and Björn Ulvaeus were also present.
In 1992, Lyngstad had been asked and chosen to be the chairperson for the environmental organisation ""Artister för miljön"" (Artists for the Environment) in Sweden. She became chairperson for this organisation from 1992 to 1995. To mark her interests for the environment, she recorded the Julian Lennon song "Saltwater" and performed it live in Stockholm. She arranged and financed summer camps for poor children in Sweden, focusing on environmental and ecological issues. Her environmental work for this organisation led up to the decision to record again. The album "Djupa andetag" ("Deep Breaths") was released towards the end of 1996 and became a success in Sweden, where it reached number-one. The lyrics for the single from this album, "Även en blomma" ("Even a Flower"), deal with environmental issues. In 2004, Lyngstad recorded a song called "The Sun Will Shine Again", written especially for her and released with former Deep Purple member Jon Lord. The couple made several TV performances with this song in Germany. Lyngstad lives a relatively low-profile life but occasionally appears at a party or charity function. On 26 August 1992, she married Prince Heinrich Ruzzo Reuss von Plauen, of the German Reuss family. Von Plauen died of lymphoma in 1999 at the age of 49. In addition to losing her husband, Lyngstad had also lost her daughter Lise-Lotte in a car crash a year earlier.
On 15 November 2005, Lyngstad's 60th birthday, Universal released the "Frida Box Set", consisting of the solo albums she recorded for the Polar Label. Also included is the 3-hour documentary "Frida – The DVD". On this DVD, which covers her entire singing career, the viewer is guided by Lyngstad herself through the years from her TV debut in Sweden in 1967 to the TV performances she made in Germany in 2004. Many rare clips are included in the set and each performance is explained by Lyngstad herself. The interview with Lyngstad was filmed in the Swiss Alps in summer 2005.
Lyngstad returned to the recording studio in 2010 to record vocals for the Cat Stevens song "Morning Has Broken", for Swedish guitarist Georg Wadenius's October 2010 album "Reconnections". The album, which featured other guest vocalists, reached number 17 in the Swedish charts.
In May 2013, Fältskog released a solo album entitled "A" through the Verve music label. In a promotional interview, Fältskog explained that the album was unplanned and it was after she heard the first three songs that she felt that she "had to do this the album". She also revealed that she completed singing lessons prior to recording "A", as she felt "a bit rusty" in her throat. Fältskog stated that she would not be undertaking any tours or live performances in support of the album, explaining: "I'm not that young anymore. I don’t have the energy to do that, and also I don’t want to travel too much." The title of the album was conceived of by the studio production team. "A" has been very successful, earning her 4 Gold Records in UK where it peaked at number-six, Australia, Germany and Sweden. In both UK and Australia it was in the top 100 albums of 2013.
Resurgence of public interest.
The same year the members of ABBA went their separate ways, the French production of a "tribute" show (a children's TV musical named "Abbacadabra" using 14 ABBA songs) spawned new interest in the group's music.
After receiving little attention during the mid-to-late-1980s, ABBA's music experienced a resurgence in the early 1990s due to the UK synth-pop duo Erasure, who released a cover extended play featuring versions of ABBA songs which topped the charts in 1992. As U2 arrived in Stockholm for a concert in June of that year, the band paid homage to ABBA by inviting Björn Ulvaeus and Benny Andersson to join them on stage for a rendition of "Dancing Queen", playing guitar and keyboards. September 1992 saw the release of "", a new compilation album. The single "Dancing Queen" received radio airplay in the UK in summer 1992 to promote the album. The song returned to the Top 20 of the UK singles chart in August that year, this time peaking at number 16.
The enormous interest in the "ABBA Gold: Greatest Hits" compilation saw the release of "" in 1993.
In 1994, two Australian cult films caught the attention of the world's media, both focusing on admiration for ABBA: "The Adventures of Priscilla, Queen of the Desert" and "Muriel's Wedding". The same year, "Thank You for the Music", a four-disc box set comprising all the group's hits and stand-out album tracks, was released with the involvement of all four members. "By the end of the twentieth century", American critic Chuck Klosterman wrote a decade later, "it was far more contrarian to hate ABBA than to love them."
ABBA were soon recognised and embraced by other acts: Evan Dando of The Lemonheads recorded a cover version of "Knowing Me, Knowing You"; Sinéad O'Connor and Boyzone's Stephen Gately have recorded "Chiquitita"; Tanita Tikaram, Blancmange and Steven Wilson paid tribute to "The Day Before You Came". Cliff Richard covered "Lay All Your Love on Me", while Dionne Warwick, Peter Cetera, and Celebrity Skin recorded their versions of "SOS". U.S. alternative-rock musician Marshall Crenshaw has also been known to play a version of "Knowing Me, Knowing You" in concert appearances, while legendary English Latin pop songwriter Richard Daniel Roman has recognized ABBA as a major influence. Swedish metal guitarist Yngwie Malmsteen covered "Gimme! Gimme! Gimme! (A Man After Midnight)" with slightly altered lyrics.
Two different compilation albums of ABBA songs have been released. "ABBA: A Tribute" coincided with the 25th anniversary celebration and featured 17 songs, some of which were recorded especially for this release. Notable tracks include Go West's "One of Us", Army of Lovers "Hasta Mañana", Information Society's "Lay All Your Love on Me", Erasure's "Take a Chance on Me" (with MC Kinky), and Lyngstad's a cappella duet with The Real Group of "Dancing Queen". A second 12-track album was released in 1999, entitled "ABBAMANIA", with proceeds going to the Youth Music charity in England. It featured all new cover versions: notable tracks were by Madness ("Money, Money, Money"), Culture Club ("Voulez-Vous"), The Corrs ("The Winner Takes It All"), Steps ("Lay All Your Love on Me", "I Know Him So Well"), and a medley entitled "Thank ABBA for the Music" performed by several artists and as featured on the Brits Awards that same year.
In 1997, an ABBA tribute group was formed, the ABBA Teens, which was subsequently renamed the A-Teens to allow the group some independence. The group's first album, "The ABBA Generation", consisting solely of ABBA covers reimagined as 1990s pop songs, was a worldwide success and so were subsequent albums. The group disbanded in 2004 due to a grueling schedule and intentions to go solo.
In Sweden, the growing recognition of the legacy of Andersson and Ulvaeus resulted in the 1998 "B & B Concerts", a tribute concert (with Swedish singers who had worked with the songwriters through the years) showcasing not only their ABBA years, but hits both before and after ABBA. The concert was a success, and was ultimately released on CD. It later toured Scandinavia and even went to Beijing in the People's Republic of China for two concerts.
In 2000, ABBA was reported to have turned down an offer of approximately US$1,000,000,000 (one billion US dollars) to do a reunion tour consisting of 100 concerts.
For the 2004 semi-final of the Eurovision Song Contest, staged in Istanbul 30 years after ABBA had won the contest in Brighton, all four members made cameo appearances in a special comedy video made for the interval act, entitled "Our Last Video Ever". Other well-known stars such as Rik Mayall, Cher and Iron Maiden's Eddie also made appearances in the video. It was not included in the official DVD release of the Eurovision Contest, but was issued as a separate DVD release, retitled "The Last Video" at the request of the former ABBA members.
In 2005, all four members of ABBA appeared at the Stockholm premiere of the musical "Mamma Mia!".
On 22 October 2005, at the 50th anniversary celebration of the Eurovision Song Contest, "Waterloo" was chosen as the best song in the competition's history.
On 4 July 2008, all four ABBA members were reunited at the Swedish premiere of the film "Mamma Mia!". It was only the second time all of them had appeared together in public since 1986. During the appearance, they re-emphasized that they intended never to officially reunite, citing the opinion of Robert Plant that the re-formed Led Zeppelin was more like a cover band of itself than the original band. Ulvaeus stated that he wanted the band to be remembered as they were during the peak years of their success.
The compilation album "", originally released in 1992, returned to number-one in the UK album charts for the fifth time on 3 August 2008. On 14 August 2008, the "Mamma Mia! The Movie" film soundtrack went to number-one on the US "Billboard" charts, ABBA's first US chart-topping album. During the band's heyday the highest album chart position they had ever achieved in America was number 14.
In November 2008, all eight studio albums, together with a ninth of rare tracks, was released as "The Albums". It hit several charts, peaking at number-four in Sweden and reaching the Top 10 in several other European territories.
In 2008, Sony Computer Entertainment Europe, in collaboration with Universal Music Group Sweden AB, released "SingStar ABBA" on both the PlayStation 2 and PlayStation 3 games consoles, as part of the SingStar music video games. The PS2 version features 20 ABBA songs, while 25 songs feature on the PS3 version.
On 22 January 2009, Fältskog and Lyngstad appeared together on stage to receive the Swedish music award ""Rockbjörnen"" (for "lifetime achievement"). In an interview, the two women expressed their gratitude for the honorary award and thanked their fans.
On 25 November 2009, PRS for Music announced that the British public voted ABBA as the band they would most like to see re-form.
On 27 January 2010, ABBAWORLD, a 25-room touring exhibition featuring interactive and audiovisual activities, debuted at Earls Court Exhibition Centre in London. According to the exhibition's website, ABBAWORLD is "approved and fully supported" by the band members.
"Mamma Mia" was released as one of the first few non-premium song selections for the online RPG game "Bandmaster". On 17 May 2011, "Gimme! Gimme! Gimme!" was added as a non-premium song selection for the Bandmaster Philippines server. On 15 November 2011, Ubisoft released a dancing game called "" for the Wii.
In January 2012, Universal Music announced the re-release of ABBA's final album "The Visitors", featuring a previously unheard track "From a Twinkling Star to a Passing Angel".
A book entitled "ABBA: The Official Photo Book" was published in early 2014 to mark the 40-year anniversary of the band's Eurovision victory. The book reveals that part of the reason for the band's outrageous costumes were the Swedish tax laws at the time that allowed the cost of brazen outfits that were not suitable for public display to be deducted against Tax
On 20 January 2016 all four members of ABBA made a public appearance at "Mamma Mia" the party. It was their first since the "Mamma Mia" movie premiere 8 years earlier.
Artistry.
Recording process.
ABBA were perfectionists in the studio, working on tracks until they got them right rather than leaving them to come back later on.
The band created a basic rhythm track with a drummer, guitarist and bass player, and overlaid other arrangements and instruments. Vocals were then added, and orchestra overdubs were usually left until last. The reason for the wild costumes was Swedish tax law. The clothes could be deductible only if they could not be worn other than for performances. Choreography by Graham Tainton also contributed to their performance style.
The videos that accompanied some of the band's biggest hits are often cited as being among the earliest examples of the genre. Most of ABBA's videos (and "ABBA: The Movie") were directed by Lasse Hallström, who would later direct the films "My Life as a Dog", "The Cider House Rules" and "Chocolat".
ABBA made videos because their songs were hits in many different countries and personal appearances were not always possible. This was also done in an effort to minimize travelling, particularly to countries that would have required extremely long flights. Fältskog and Ulvaeus had two young children and Fältskog, who was also afraid of flying, was very reluctant to leave her children for such a long time. ABBA's manager, Stig Anderson, realized the potential of showing a simple video clip on television to publicize a single or album, thereby allowing easier and quicker exposure than a concert tour. Some of these videos became classics because of the 1970s-era costumes and early video effects, such as the grouping of the band members in different combinations of pairs, overlapping one singer's profile with the other's full face, and the contrasting of one member against another.
In 1976, ABBA participated in a high-profile advertising campaign by the Matsushita Electric Industrial (today's Panasonic), which was designed to promote the brand National. This campaign was designed initially for Australia, where "National" was still the primary brand used by Matsushita, who had not introduced the "Panasonic" brand to Australia yet despite its widespread use in other parts of the world such as the United States. However, the campaign was also aired in Japan. Five commercials, each approximately one minute long, were produced, each using the "National Song" sung by ABBA, which used the melody and instrumental arrangement of "Fernando", adapted with new lyrics promoting National, and working in several slogans used by National in their advertising.
Political position.
In September 2010, band members Andersson and Ulvaeus criticized the right-wing Danish People's Party (DF) for using the ABBA song "Mamma Mia" (with modified lyrics) at rallies. The band threatened to file a lawsuit against the DF, saying they never allowed their music to be used politically and that they had absolutely no interest in supporting the party. Their record label Universal Music later said that no legal action would be taken because an agreement had been reached.
Success in the United States.
During their active career, from 1972 to 1982, ABBA placed twenty singles on the "Billboard" Hot 100 fourteen of which made the top 40 (13 on the Cashbox Top 100) and ten of which made the Top 20 on both charts. A total of four of those singles reached the Top 10, including "Dancing Queen" which reached number-one in April 1977. While "Fernando" and "SOS" did not break the Top 10 on the "Billboard" Hot 100 chart, reaching number 13 and 15 respectively, they did reach the Top 10 on Cashbox ("Fernando") and Record World ("SOS") charts. Both "Dancing Queen" and "Take A Chance On Me" were certified gold by the Recording Industry Association of America for sales of over one million copies each.
The group also had 12 Top 20 singles on the "Billboard" Adult Contemporary chart with two of them, "Fernando" and "The Winner Takes It All", reaching number-one. "Lay All Your Love on Me" was ABBA's fourth number-one single on a "Billboard" chart, topping the Hot Dance Club Play chart. The singles "Dancing Queen" and "Take a Chance on Me" were certified gold (more than 1 million copies sold) by the RIAA.
Nine ABBA albums made their way into the top half of the "Billboard" 200 album chart, with seven of them reaching the Top 50 and four reaching the Top 20. "ABBA: The Album" was the highest-charting album of the group's career, peaking at No. 14. Five albums received RIAA gold certification (more than 500,000 copies sold), while three acquired platinum status (selling more than one million copies). In 1993, the "ABBA Gold: Greatest Hits" collection was released in the United States and has since become a seven-time platinum best-seller; it climbed to number-one on the "Billboard" Top Pop Catalog Albums chart and also peaked at number 11 on "Billboard" Comprehensive Albums chart.
On 15 March 2010, ABBA was inducted into the Rock and Roll Hall of Fame by Bee Gees members Barry Gibb and Robin Gibb. The ceremony was held at The Waldorf Astoria Hotel in New York City. The group was represented by Anni-Frid Lyngstad and Benny Andersson.
References.
Notes
Bibliography
Further reading

</doc>
<doc id="881" url="https://en.wikipedia.org/wiki?curid=881" title="Allegiance">
Allegiance

An allegiance is a duty of fidelity said to be owed, or freely committed, by the people, subjects or citizens to their state or sovereign.
Etymology.
From Middle English "ligeaunce" (see medieval Latin "ligeantia", "a liegance"). The "al-" prefix was probably added through confusion with another legal term, "allegeance", an "allegation" (the French "allegeance" comes from the English). "Allegiance" is formed from "liege," from Old French "liege", "liege, free", of Germanic origin. The connection with Latin "ligare", "to bind," is erroneous.
Usage.
The term "allegiance" was traditionally often used by English legal commentators in a larger sense, divided by them into natural and local, the latter applying to the deference which even a foreigner must pay to the institutions of the country in which he happens to live. However it is in its proper sense, in which it indicates national character and the subjection due to that character, that the word is more important.
In that sense it represents the feudal liege homage, which could be due only to one lord, while simple homage might be due to every lord under whom the person in question held land.
KINGÂ»NOBLEÂ»KNIGHTSÂ»PEASANTS
United Kingdom.
The English doctrine, which was at one time adopted in the United States, asserted that allegiance was indelible: "Nemo potest exuere patriam". Accordingly, as the law stood before 1870, every person who by birth or naturalisation satisfied the conditions set forth, though he should be removed in infancy to another country where his family resided, owed an allegiance to the British crown which he could never resign or lose, except by act of parliament or by the recognition of the independence or the cession of the portion of British territory in which he resided.
This refusal to accept any renunciation of allegiance to the Crown led to conflict with the United States over impressment, and then led to further conflicts even during the War of 1812, when thirteen Irish American prisoners of war were executed as traitors after the Battle of Queenston Heights; Winfield Scott urged American reprisal, but none was carried out.
Allegiance is the tie which binds the subject to the Sovereign in return for that protection which the Sovereign affords the subject. It was the mutual bond and obligation between monarch and subjects, whereby subjects are called his liege subjects, because they are bound to obey and serve him; and he is called their liege lord, because he should maintain and defend them ("Ex parte Anderson" (1861) 3 El & El 487; 121 ER 525; "China Navigation Co v Attorney-General" (1932) 48 TLR 375; "Attorney-General v Nissan" 1 All ER 629; "Oppenheimer v Cattermole" [1972 3 All ER 1106). The duty of the Crown towards its subjects is to govern and protect. The reciprocal duty of the subject towards the Crown is that of allegiance.
At common law allegiance is a true and faithful obedience of the subject due to his Sovereign. As the subject owes to his king his true and faithful allegiance and obedience, so the Sovereign
Natural allegiance and obedience is an incident inseparable to every subject, for parte Anderson" (1861) 3 El & El 487; 121 ER 525). Natural-born subjects owe allegiance wherever they may be. Where territory is occupied in the course of hostilities by an enemy's force, even if the annexation of the occupied country is proclaimed by the enemy, there can be no change of allegiance during the progress of hostilities on the part of a citizen of the occupied country ("R v Vermaak" (1900) 21 NLR 204 (South Africa)).
Allegiance is owed both to the Sovereign as a natural person and to the Sovereign in the political capacity ("Re Stepney Election Petition, Isaacson v Durant" (1886) 17 QBD 54 (per Lord Coleridge CJ)). Attachment to the person of the reigning Sovereign is not sufficient. Loyalty requires affection also to the office of the Sovereign, attachment to royalty, attachment to the law and to the constitution of the realm, and he who would, by force or by fraud, endeavour to prostrate that law and constitution, though he may retain his affection for its head, can boast but an imperfect and spurious species of loyalty ("R v O'Connell" (1844) 7 ILR 261).
There were four kinds of allegiances ("Rittson v Stordy" (1855) 3 Sm & G 230; "De Geer v Stone" (1882) 22 Ch D 243; "Isaacson v Durant" (1886) 54 LT 684; "Gibson, Gavin v Gibson" 3 KB 379; "Joyce v DPP" [1946 AC 347; "Collingwood v Pace" (1661) O Bridg 410; "Lane v Bennett" (1836) 1 M & W 70; "Lyons Corp v East India Co" (1836) 1 Moo PCC 175; "Birtwhistle v Vardill" (1840) 7 Cl & Fin 895; "R v Lopez, R v Sattler" (1858) Dears & B 525; Ex p Brown (1864) 5 B & S 280);
(a) "Ligeantia naturalis, absoluta, pura et indefinita", and this originally is due by nature and birthright, and is called "alta ligeantia", and those that owe this are called "subditus natus";
(b) "Ligeantia acquisita", not by nature but by acquisition or denization, being called a denizen, or rather denizon, because they are "subditus datus";
(c) "Ligeantia localis", by operation of law, when a friendly alien enters the country, because so long as they are in the country they are within the Sovereign's protection, therefore they owe the Sovereign a local obedience or allegiance ("R v Cowle" (1759) 2 Burr 834; "Low v Routledge" (1865) 1 Ch App 42; "Re Johnson, Roberts v Attorney-General" 1 Ch 821; "Tingley v Muller" [1917 2 Ch 144; "Rodriguez v Speyer" AC 59; "Johnstone v Pedlar" [1921 2 AC 262; "R v Tucker" (1694) Show Parl Cas 186; "R v Keyn" (1876) 2 Ex D 63; "Re Stepney Election Petn, Isaacson v Durant" (1886) 17 QBD 54);
(d) A legal obedience, where a particular law requires the taking of an oath of allegiance by subject or alien alike.
Natural allegiance was acquired by birth within the Sovereign's dominions (except for the issue of diplomats or of invading forces or of an alien in enemy occupied territory). The natural allegiance and obedience is an incident inseparable to every subject, for as soon as they are born they owe by birthright allegiance and obedience to the Sovereign ("Ex p. Anderson" (1861) 3 E & E 487). A natural-born subject owes allegiance wherever they may be, so that where territory is occupied in the course of hostilities by an enemy's force, even if the annexation of the occupied country is proclaimed by the enemy, there can be no change of allegiance during the progress of hostilities on the part of a citizen of the occupied country ("R v Vermaak" (1900) 21 NLR 204 (South Africa)).
Acquired allegiance was acquired by naturalisation or denization. Denization, or "ligeantia acquisita", appears to be threefold ("Thomas v Sorrel" (1673) 3 Keb 143);
Local allegiance was due by an alien while in the protection of the Crown. All friendly resident aliens incurred all the obligations of subjects ("The Angelique" (1801) 3 Ch Rob App 7). An alien, coming into a colony also became, temporarily a subject of the Crown, and acquired rights both within and beyond the colony, and these latter rights could not be affected by the laws of that colony ("Routledge v Low" (1868) LR 3 HL 100; 37 LJ Ch 454; 18 LT 874; 16 WR 1081, HL; "Reid v Maxwell" (1886) 2 TLR 790; "Falcon v Famous Players Film Co" 2 KB 474).
A resident alien owed allegiance even when the protection of the Crown was withdrawn owing to the occupation of an enemy, because the absence of the Crown's protection was temporary and involuntary ("de Jager v Attorney-Geneneral of Natal" AC 326).
Legal allegiance was due when an alien took an oath of allegiance required for a particular office under the Crown.
By the Naturalisation Act 1870, it was made possible for British subjects to renounce their nationality and allegiance, and the ways in which that nationality is lost are defined. So British subjects voluntarily naturalized in a foreign state are deemed aliens from the time of such naturalization, unless, in the case of persons naturalized before the passing of the act, they have declared their desire to remain British subjects within two years from the passing of the act. Persons who from having been born within British territory are British subjects, but who at birth became under the law of any foreign state subjects of such state, and also persons who though born abroad are British subjects by reason of parentage, may by declarations of alienage get rid of British nationality. Emigration to an uncivilized country leaves British nationality unaffected: indeed the right claimed by all states to follow with their authority their subjects so emigrating is one of the usual and recognized means of colonial expansion.
United States.
The doctrine that no man can cast off his native allegiance without the consent of his sovereign was early abandoned in the United States, and Chief Justice John Rutledge also declared in Talbot v. Janson, "a man may, at the same time, enjoy the rights of citizenship under two governments." On July 27, 1868, the day before the Fourteenth Amendment was adopted, U.S. Congress declared in the preamble of the Expatriation Act that "the right of expatriation is a natural and inherent right of all people, indispensable to the enjoyment of the rights of life, liberty and the pursuit of happiness," and (Section I) one of "the fundamental principles of this government" (United States Revised Statutes, sec. 1999). Every natural-born citizen of a foreign state who is also an American citizen and every natural-born American citizen who is a citizen of a foreign land owes a double allegiance, one to the United States, and one to his homeland (in the event of an immigrant becoming a citizen of the US), or to his adopted land (in the event of an emigrant natural born citizen of the US becoming a citizen of another nation). If these allegiances come into conflict, he or she may be guilty of treason against one or both. If the demands of these two sovereigns upon his duty of allegiance come into conflict, those of the United States have the paramount authority in American law; likewise, those of the foreign land have paramount authority in their legal system. In such a situation, it may be incumbent on the individual to renounce one of his citizenships to avoid possibly being forced into situations where countervailing duties are required of him, such as might occur in the event of war.
Oath of allegiance.
The oath of allegiance is an oath of fidelity to the sovereign taken by all persons holding important public office and as a condition of naturalization. By ancient common law it might be required of all persons above the age of 12, and it was repeatedly used as a test for the disaffected. In England it was first imposed by statute in the reign of Elizabeth I of England (1558) and its form has more than once been altered since. Up to the time of the revolution the promise was, "to be true and faithful to the king and his heirs, and truth and faith to bear of life and limb and terrene honour, and not to know or hear of any ill or damage intended him without defending him therefrom." This was thought to favour the doctrine of absolute non-resistance, and accordingly the convention parliament enacted the form that has been in use since that time – "I do sincerely promise and swear that I will be faithful and bear true allegiance to His Majesty ..."
In United States and some other republics, the oath is known as the Pledge of Allegiance. Instead of declaring fidelity to a monarch, the pledge is made to the flag, the republic, and to the core values of the country, specifically liberty and justice. The reciting of the pledge in the United States is voluntary because of the rights guaranteed to the people under the First Amendment to the United States Constitution.
In Islam.
The word used in the Arabic language for allegiance is "bay'at" (Arabic: بيعة), which means "taking hand". The practice is sanctioned in the Qur'an by Surah 48:10: "Verily, those who give thee their allegiance, they give it but to Allah Himself". The word is used for the oath of allegiance to an emir. It is also used for the initiation ceremony specific to many Sufi orders.

</doc>
<doc id="885" url="https://en.wikipedia.org/wiki?curid=885" title="Altenberg">
Altenberg

Altenberg (German for "old mountain") may refer to:
People.
Any place called Altenberg may have given rise to Altenberg as a family name, such as:
See also.
__NOTOC__

</doc>
