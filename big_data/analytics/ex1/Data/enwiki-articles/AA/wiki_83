<doc id="5648" url="https://en.wikipedia.org/wiki?curid=5648" title="Cornwall">
Cornwall

Cornwall ( or ; , ) is a ceremonial county and unitary authority area of England within the United Kingdom. It is bordered to the north and west by the Celtic Sea, to the south by the English Channel, and to the east by the county of Devon, over the River Tamar. Cornwall has a population of and covers an area of . The administrative centre, and only city in Cornwall, is Truro, although the town of Falmouth has the largest population for a civil parish and the conurbation of Camborne, Pool and Redruth has the highest total population.
Cornwall forms the westernmost part of the south-west peninsula of the island of Great Britain, and a large part of the Cornubian batholith is within Cornwall. This area was first inhabited in the Palaeolithic and Mesolithic periods. It continued to be occupied by Neolithic and then Bronze Age peoples, and later (in the Iron Age) by Brythons with distinctive cultural relations to neighbouring Wales and Brittany. There is little evidence that Roman rule was effective west of Exeter and few Roman remains have been found. Cornwall was the home of a division of the Dumnonii tribe – whose tribal centre was in the modern county of Devon – known as the Cornovii, separated from the Brythons of Wales after the Battle of Deorham, often coming into conflict with the expanding English kingdom of Wessex before King Athelstan in AD 936 set the boundary between English and Cornish at the high water mark of the eastern bank of the River Tamar. From the early Middle Ages, British language and culture was apparently shared by Brythons trading across both sides of the Channel, evidenced by the corresponding high medieval Breton kingdoms of Domnonée and Cornouaille and the Celtic Christianity common to both territories.
Historically tin mining was important in the Cornish economy, becoming increasingly significant during the High Middle Ages and expanding greatly during the 19th century when rich copper mines were also in production. In the mid-19th century, however, the tin and copper mines entered a period of decline. Subsequently china clay extraction became more important and metal mining had virtually ended by the 1990s. Traditionally, fishing (particularly of pilchards) and agriculture (notably dairy products and vegetables) were the other important sectors of the economy. Railways led to a growth of tourism in the 20th century; however, Cornwall's economy struggled after the decline of the mining and fishing industries. The area is noted for its wild moorland landscapes, its long and varied coastline, its attractive villages, its many place-names derived from the Cornish language, and its very mild climate. Extensive stretches of Cornwall's coastline, and Bodmin Moor, are protected as an Area of Outstanding Natural Beauty.
Cornwall is the homeland of the Cornish people and is recognised as one of the Celtic nations, retaining a distinct cultural identity that reflects its history. Some people question the present constitutional status of Cornwall, and a nationalist movement seeks greater autonomy within the United Kingdom in the form of a devolved legislative Cornish Assembly. On 24 April 2014 it was announced that Cornish people will be granted minority status under the European Framework Convention for the Protection of National Minorities.
Toponymy.
The name "Cornwall" derives from the combination of two separate terms from different languages. The "Corn-" part comes from the hypothesised original tribal name of the Celtic people who had lived here since the Iron Age, the "Cornovii". The second element "-wall" derives from the Old English "w(e)alh", meaning a "foreigner" or "Welshman". The name first appears in the "Anglo-Saxon Chronicle" in 891 as "On Corn walum". In the Domesday Book it was referred to as "Cornualia" and in c. 1198 as "Cornwal".
A latinisation of the name as "Cornubia" first appears in a mid-9th-century deed purporting to be a copy of one dating from c. 705. Another variation, with "Wales" reinterpreted as "Gallia", thus: "Cornugallia", is first attested in 1086. Finally, the Cornish language form of the name, "Kernow", which first appears around 1400, derives directly from the original "Cornowii". which is postulated from a single mention in the Ravenna Cosmography of around 700 (but based on earlier sources) of "Purocoronavis". This is considered to be a corruption of "Durocornovium", 'a fort or walled settlement of the Cornovii'. Its location is unidentified, but Tintagel or Carn Brea have been suggested.
In pre-Roman times, Cornwall was part of the kingdom of Dumnonia, and was later known to the Anglo-Saxons as ""West" Wales", to distinguish it from "North Wales" (modern-day Wales).
Cornwall is one of only a few places in Britain – London, Edinburgh, and Dover being other examples – to have a corresponding name in the French language: "Cornouailles" ().
History.
Prehistory, Roman and post-Roman periods.
The present human history of Cornwall begins with the reoccupation of Britain after the last Ice Age. The area now known as Cornwall was first inhabited in the Palaeolithic and Mesolithic periods. It continued to be occupied by Neolithic and then Bronze Age peoples. According to John T. Koch and others, Cornwall in the Late Bronze Age was part of a maritime trading-networked culture called the Atlantic Bronze Age, in modern-day Ireland, England, France, Spain and Portugal. During the British Iron Age Cornwall, like all of Britain south of the Firth of Forth, was inhabited by a Celtic people known as the Britons with distinctive cultural relations to neighbouring Wales and Brittany. The Common Brittonic spoken at the time eventually developed into several distinct tongues, including Cornish.
The first account of Cornwall comes from the 1st century BC Sicilian Greek historian Diodorus Siculus, supposedly quoting or paraphrasing the 4th-century BCE geographer Pytheas, who had sailed to Britain:
The identity of these merchants is unknown. It has been theorised that they were Phoenicians, but there is no evidence for this. Professor Timothy Champion, discussing Diodorus Siculus's comments on the tin trade, states that "Diodorus never actually says that the Phoenicians sailed to Cornwall. In fact, he says quite the opposite: the production of Cornish tin was in the hands of the natives of Cornwall, and its transport to the Mediterranean was organised by local merchants, by sea and then over land through France, well outside Phoenician control." (For further discussion of tin mining see the section on the economy below.)
There is little evidence that Roman rule was effective west of Exeter in Devon and few Roman remains have been found. However, after 410, Cornwall appears to have reverted to rule by Romano-Celtic chieftains of the Cornovii tribe as part of Dumnonia including one Marcus Cunomorus with at least one significant power base at Tintagel. 'King' Mark of Cornwall is a semi-historical figure known from Welsh literature, the Matter of Britain, and in particular, the later Norman-Breton medieval romance of Tristan and Yseult where he is regarded as a close kinsman of King Arthur; himself usually considered to be born of the Cornish people in folklore traditions derived from Geoffrey of Monmouth's "Historia Regum Britanniae".
Archaeology supports ecclesiastical, literary and legendary evidence for some relative economic stability and close cultural ties between the sub-Roman Westcountry, South Wales, Brittany and Ireland through the fifth and sixth centuries.
Conflict with Wessex.
The Battle of Deorham in 577 saw the separation of Dumnonia (and therefore Cornwall) from Wales, following which the Dumnonii often came into conflict with the expanding English kingdom of Wessex. The "Annales Cambriae" report that in 722 AD the Britons of Cornwall won a battle at "Hehil". It seems likely that the enemy the Cornish fought was a West Saxon force, as evidenced by the naming of King Ine of Wessex and his kinsman Nonna in reference to an earlier Battle of Lining in 710.
The "Anglo-Saxon Chronicle" stated in 815 (adjusted date) "and in this year king Ecgbryht raided in Cornwall from east to west." and thenceforth apparently held it as a ducatus or dukedom annexed to his regnum or kingdom of Wessex, but not wholly incorporated with it. The "Anglo-Saxon Chronicle" states that in 825 (adjusted date) a battle took place between the Wealas (Cornish) and the Defnas (men of Devon) at Gafulforda. In the same year Ecgbert, as a later document expresses it, "disposed of their territory as it seemed fit to him, giving a tenth part of it to God." In other words, he incorporated Cornwall ecclesiastically with the West Saxon diocese of Sherborne, and endowed Ealhstan, his fighting bishop, who took part in the campaign, with an extensive Cornish estate consisting of Callington and Lawhitton, both in the Tamar valley, and Pawton near Padstow.
In 838, the Cornish and their Danish allies were defeated by Egbert in the Battle of Hingston Down at Hengestesdune (probably Hingston Down in Cornwall). In 875, the last recorded king of Cornwall, Dumgarth, is said to have drowned. Around the 880s, Anglo-Saxons from Wessex had established modest land holdings in the eastern part of Cornwall; notably Alfred the Great who had acquired a few estates. William of Malmesbury, writing around 1120, says that King Athelstan of England (924–939) fixed the boundary between English and Cornish people at the east bank of the River Tamar.
Norman-Breton period.
One interpretation of the Domesday Book is that by this time the native Cornish landowning class had been almost completely dispossessed and replaced by English landowners, particularly Harold Godwinson himself. However, the Bodmin manumissions show that two leading Cornish figures nominally had Saxon names, but these were both glossed with native Cornish names. Naming evidence cited by medievalist Edith Ditmas suggests that many post-Conquest landowners in Cornwall were Breton allies of the Normans and further proposed this period for the early composition of the Tristan and Iseult cycle by poets such as Beroul from a pre-existing shared Brittonic oral tradition.
Soon after the Norman conquest most of the land was transferred to the new Breton-Norman aristocracy, with the lion's share going to Robert, Count of Mortain, half-brother of King William and the largest landholder in England after the king with his stronghold at Trematon Castle near the mouth of the Tamar. Cornwall and Devon west of Dartmoor showed a very different type of settlement pattern from that of Saxon Wessex and places continued, even after 1066, to be named in the Celtic Cornish tradition with Saxon architecture being uncommon.
Later medieval administration and society.
Subsequently, however, Norman absentee landlords became replaced by a new Cornu-Norman elite including scholars such as Richard Rufus of Cornwall. These families eventually became the new ruling class of Cornwall (typically speaking Norman French, Cornish, Latin and eventually English), many becoming involved in the operation of the Stannary Parliament system, Earldom and eventually the Duchy. The Cornish language continued to be spoken and it acquired a number of characteristics establishing its identity as a separate language from Breton.
Cornish piracy was active during the Elizabethan era on the west coast of Britain.
Christianity in Cornwall.
Many place names in Cornwall are associated with Christian missionaries described as coming from Ireland and Wales in the 5th century AD and usually called saints ("See" List of Cornish saints). The historicity of some of these missionaries is problematic. The patron saint of Wendron Parish Church, "Saint Wendrona" is another example. and it has been pointed out by Canon Doble that it was customary in the Middle Ages to ascribe such geographical origins to saints. Some of these saints are not included in the early lists of saints.
Saint Piran, after whom Perranporth is named, is generally regarded as the patron saint of Cornwall. However, in early Norman times it is likely that Saint Michael the Archangel was recognised as the patron saint and is still recognised by the Anglican Church as the "Protector of Cornwall". The title has also been claimed for Saint Petroc who was patron of the Cornish diocese prior to the Normans.
Celtic and Anglo-Saxon times.
The church in Cornwall until the time of Athelstan of Wessex observed more or less orthodox practices, being completely separate from the Anglo-Saxon church until then (and perhaps later). The See of Cornwall continued until much later: Bishop Conan apparently in place previously, but (re-?) consecrated in 931 AD by Athelstan. However, it is unclear whether he was the sole Bishop for Cornwall or the leading Bishop in the area. The situation in Cornwall may have been somewhat similar to Wales where each major religious house corresponded to a cantref (this has the same meaning as Cornish keverang) both being under the supervision of a Bishop. However, if this was so the status of keverangow before the time of King Athelstan is not recorded. However, it can be inferred from the districts included at this period that the minimum number would be three: Triggshire; Wivelshire; and the remaining area. Penwith, Kerrier, Pydar and Powder meet at a central point (Scorrier) which some have believed indicates a fourfold division imposed by Athelstan on a sub-kingdom.
Middle Ages.
The whole of Cornwall was in this period in the Archdeaconry of Cornwall within the Diocese of Exeter. From 1267 the archdeacons had a house at Glasney near Penryn. Their duties were to visit and inspect each parish annually and to execute the bishop's orders. Archdeacon Roland is recorded in the Domesday Book of 1086 as having land holdings in Cornwall but he was not Archdeacon of Cornwall, just an archdeacon in the Diocese of Exeter. In the episcopate of William Warelwast (1107–37) the first Archdeacon of Cornwall was appointed (possibly Hugo de Auco). Most of the parish churches in Cornwall in Norman times were not in the larger settlements, and the medieval towns which developed thereafter usually had only a chapel of ease with the right of burial remaining at the ancient parish church. Over a hundred holy wells exist in Cornwall, each associated with a particular saint, though not always the same one as the dedication of the church.
Various kinds of religious houses existed in mediaeval Cornwall though none of them were nunneries; the benefices of the parishes were in many cases appropriated to religious houses within Cornwall or elsewhere in England or France.
From the Reformation to the Victorian period.
In the 16th century there was some violent resistance to the replacement of Catholicism with Protestantism in the Prayer Book Rebellion. In 1548 the college at Glasney, a centre of learning and study established by the Bishop of Exeter, had been closed and looted (many manuscripts and documents were destroyed) which aroused resentment among the Cornish. They, among other things, objected to the English language Book of Common Prayer, protesting that the English language was still unknown to many at the time. The Prayer Book Rebellion was a cultural and social disaster for Cornwall; the reprisals taken by the forces of the Crown have been estimated to account for 10–11% of the civilian population of Cornwall. Culturally speaking, it saw the beginning of the slow decline of the Cornish language.
From that time Christianity in Cornwall was in the main within the Church of England and subject to the national events which affected it in the next century and a half. Roman Catholicism never became extinct, though openly practised by very few; there were some converts to Puritanism, Anabaptism and Quakerism in certain areas though they suffered intermittent persecution which more or less came to an end in the reign of William and Mary. During the 18th century Cornish Anglicanism was very much in the same state as Anglicanism in most of England. Wesleyan Methodist missions began during John Wesley's lifetime and had great success over a long period during which Methodism itself divided into a number of sects and established a definite separation from the Church of England.
From the early 19th to the mid-20th century Methodism was the leading form of Christianity in Cornwall but it is now in decline. The Church of England was in the majority from the reign of Queen Elizabeth until the Methodist revival of the 19th century: before the Wesleyan missions dissenters were very few in Cornwall. The county remained within the Diocese of Exeter until 1876 when the Anglican Diocese of Truro was created (the first Bishop was appointed in 1877). Roman Catholicism was virtually extinct in Cornwall after the 17th century except for a few families such as the Arundells of Lanherne. From the mid-19th century the church reestablished episcopal sees in England, one of these being at Plymouth. Since then immigration to Cornwall has brought more Roman Catholics into the population.
Physical geography.
Cornwall forms the tip of the south-west peninsula of the island of Great Britain, and is therefore exposed to the full force of the prevailing winds that blow in from the Atlantic Ocean. The coastline is composed mainly of resistant rocks that give rise in many places to impressive cliffs. Cornwall has a border with only one other county, Devon, which is formed almost entirely by the River Tamar and (to the north) by the Marsland Valley.
Coastal areas.
The north and south coasts have different characteristics. The north coast on the Celtic Sea, part of the Atlantic Ocean, is more exposed and therefore has a wilder nature. The prosaically named "High Cliff", between Boscastle and St Gennys, is the highest sheer-drop cliff in Cornwall at . However, there are also many extensive stretches of fine golden sand which form the beaches that are so important to the tourist industry, such as those at Bude, Polzeath, Watergate Bay, Perranporth, Porthtowan, Fistral Beach, Newquay, St Agnes, St Ives, and on the south coast Gyllyngvase beach in Falmouth and the large beach at Praa Sands further to the south west. There are two river estuaries on the north coast: Hayle Estuary and the estuary of the River Camel, which provides Padstow and Rock with a safe harbour. The seaside town of Newlyn is a popular holiday destination, as it is one of the last remaining traditional Cornish fishing ports, with views reaching over Mount's Bay.
The south coast, dubbed the "Cornish Riviera", is more sheltered and there are several broad estuaries offering safe anchorages, such as at Falmouth and Fowey. Beaches on the south coast usually consist of coarser sand and shingle, interspersed with rocky sections of wave-cut platform. Also on the south coast, the picturesque fishing village of Polperro, at the mouth of the Pol River, and the fishing port of Looe on the River Looe are both popular with tourists.
Inland areas.
The interior of the county consists of a roughly east-west spine of infertile and exposed upland, with a series of granite intrusions, such as Bodmin Moor, which contains the highest land within Cornwall. From east to west, and with approximately descending altitude, these are Bodmin Moor, Hensbarrow north of St Austell, Carnmenellis to the south of Camborne, and the Penwith or Land's End peninsula. These intrusions are the central part of the granite outcrops that form the exposed parts of the Cornubian batholith of south-west Britain, which also includes Dartmoor to the east in Devon and the Isles of Scilly to the west, the latter now being partially submerged.
The intrusion of the granite into the surrounding sedimentary rocks gave rise to extensive metamorphism and mineralisation, and this led to Cornwall being one of the most important mining areas in Europe until the early 20th century. It is thought tin was mined here as early as the Bronze Age, and copper, lead, zinc and silver have all been mined in Cornwall. Alteration of the granite also gave rise to extensive deposits of China Clay, especially in the area to the north of St Austell, and the extraction of this remains an important industry.
The uplands are surrounded by more fertile, mainly pastoral farmland. Near the south coast, deep wooded valleys provide sheltered conditions for flora that like shade and a moist, mild climate. These areas lie mainly on Devonian sandstone and slate. The north east of Cornwall lies on Carboniferous rocks known as the Culm Measures. In places these have been subjected to severe folding, as can be seen on the north coast near Crackington Haven and in several other locations.
The Lizard Peninsula.
The geology of the Lizard peninsula is unusual, in that it is mainland Britain's only example of an ophiolite, a section of oceanic crust now found on land. Much of the peninsula consists of the dark green and red Precambrian serpentinite, which forms spectacular cliffs, notably at Kynance Cove, and carved and polished serpentine ornaments are sold in local gift shops. This ultramafic rock also forms a very infertile soil which covers the flat and marshy heaths of the interior of the peninsula. This is home to rare plants, such as the Cornish Heath, which has been adopted as the county flower.
Ecology.
Cornwall has varied habitats including terrestrial and marine ecosystems. One noted species in decline locally is the Reindeer lichen, which species has been made a priority for protection under the national UK Biodiversity Action Plan.
Botanists divide Cornwall and Scilly into two vice-counties: West (1) and East (2). The standard flora is by F. H. Davey "Flora of Cornwall" (1909). Davey was assisted by A. O. Hume and he thanks Hume, his companion on excursions in Cornwall and Devon, and for help in the compilation of that Flora, publication of which was financed by him.
Climate.
Cornwall has a temperate Oceanic climate (Köppen climate classification: Cfb) and has the mildest and sunniest climate in the United Kingdom, as a result of its southerly latitude and the influence of the Gulf Stream. The average annual temperature in Cornwall ranges from on the Isles of Scilly to in the central uplands. Winters are amongst the warmest in the country due to the southerly latitude and moderating effects of the warm ocean currents, and frost and snow are very rare at the coast and are also rare in the central upland areas. Summers are however not as warm as in other parts of southern England. The surrounding sea and its southwesterly position mean that Cornwall's weather can be relatively changeable.
Cornwall is one of the sunniest areas in the UK, with over 1541 hours of sunshine per year, with the highest average of 7.6 hours of sunshine per day in July. The moist, mild air coming from the south west brings higher amounts of rainfall than in eastern Great Britain, at per year, however not as much as in more northern areas of the west coast. The Isles of Scilly, for example, where there are on average less than 2 days of air frost per year, is the only area in the UK to be in the USDA Hardiness zone 10. In Scilly there is on average less than 1 day of air temperature exceeding 30 °C per year and it is in the AHS Heat Zone 1. Extreme temperatures in Cornwall are particularly rare; however, extreme weather in the form of storms and floods is common.
Politics and administration.
Local politics.
With the exception of the Isles of Scilly, Cornwall is governed by a unitary authority, Cornwall Council, based in Truro. The Crown Court is based at the Courts of Justice in Truro. Magistrates' Courts are found in Truro (but at a different location to the Crown Court), Bodmin, Penzance and Liskeard.
The Isles of Scilly form part of the ceremonial county of Cornwall and have, at times, been served by the same county administration. Since 1890 they have been administered by their own unitary authority, the Council of the Isles of Scilly. They are grouped with Cornwall for other administrative purposes, such as the National Health Service and Devon and Cornwall Police.
Before reorganisation on 1 April 2009, council functions throughout the rest of Cornwall were organised on a two-tier basis, with a county council and district councils for its six districts, Caradon, Carrick, Kerrier, North Cornwall, Penwith, and Restormel. While projected to streamline services, cut red tape and save around £17 million a year, the reorganisation was met with wide opposition, with a poll in 2008 giving a result of 89% disapproval from Cornish residents.
The first elections for the unitary authority were held on 4 June 2009. The council has 123 seats; the largest party (in 2015) is the Liberal Democrats with 37 seats (37), followed by the Independents, 36 seats (23), Conservative Party with 31 seats (46) Labour Party 8 seats (1), UKIP 6 seats (0), Mebyon Kernow with 4 seats (6), Greens 1 seat, (0). (Number of seats prior to 2013 election in brackets.)
Before the creation of the unitary council, the former county council had 82 seats, the majority of which were held by the Liberal Democrats, elected at the 2005 county council elections. The six former districts had a total of 249 council seats, and the groups with greatest numbers of councillors were Liberal Democrats, Conservatives, and Independents.
Parliament and national politics.
Following a review by the Boundary Commission for England taking effect at the 2010 general election, Cornwall is divided into six county constituencies to elect MPs to the House of Commons of the United Kingdom.
Before the 2010 boundary changes Cornwall had five constituencies all of which were won by Liberal Democrats in the 2005 general election. At the 2010 general election Liberal Democrat candidates won three constituencies and Conservative candidates won three constituencies ("see also 2010 United Kingdom general election result in Cornwall"). At the 2015 general election all six Cornish seats were won by Conservative candidates.
Until 1832, Cornwall had 44 MPs – more than any other county – reflecting the importance of tin to the Crown. Most of the increase in numbers of MPs came between 1529 and 1584 after which there was no change until 1832.
Self-rule movement.
Cornish nationalists have organised into two political parties: Mebyon Kernow, formed in 1951, and the Cornish Nationalist Party. In addition to the political parties, there are various interest groups such as the Revived Cornish Stannary Parliament and the Celtic League. The Cornish Constitutional Convention was formed in 2000 as a cross-party organisation including representatives from the private, public and voluntary sectors to campaign for the creation of a Cornish Assembly, along the lines of the National Assembly for Wales, Northern Ireland Assembly and the Scottish Parliament. Between 5 March 2000 and December 2001, the campaign collected the signatures of 41,650 Cornish residents endorsing the call for a devolved assembly, along with 8,896 signatories from outside Cornwall. The resulting petition was presented to the Prime Minister, Tony Blair. The Liberal Democrats recognise Cornwall's claims for greater autonomy, as do the Liberal Party.
An additional political issue is the recognition of the Cornish people as a minority.
Cornish national identity.
Cornwall is recognised by several organisations, including the Cornish nationalist party Mebyon Kernow, the Celtic League and the International Celtic Congress, as one of the six Celtic nations, alongside Brittany, Ireland, the Isle of Man, Scotland and Wales. Alongside Asturias and Galicia, Cornwall is also recognised as one of the eight Celtic nations by the Isle of Man Government and the Welsh Government. Cornwall is represented, as one of the Celtic nations, at the "Festival Interceltique de Lorient", an annual celebration of Celtic culture held in Brittany.
Cornwall Council consider Cornwall's unique cultural heritage and distinctiveness to be one of the area's major assets. They see Cornwall's language, landscape, Celtic identity, political history, patterns of settlement, maritime tradition, industrial heritage, and non-conformist tradition, to be among the features making up its "distinctive" culture. However, it is uncertain how many of the people living in Cornwall consider themselves to be Cornish; results from different surveys (including the national census) have varied. In the 2001 census, 7 percent of people in Cornwall identified themselves as Cornish, rather than British or English. However, activists have argued that this underestimated the true number as there was no explicit "Cornish" option included in the official census form. Subsequent surveys have suggested that as many as 44 percent identify as Cornish. Many people in Cornwall say that this issue would be resolved if a Cornish option became available on the census. The question and content recommendations for the 2011 Census provided an explanation of the process of selecting an ethnic identity which is relevant to the understanding of the often quoted figure of 37,000 who claim Cornish identity.
On 24 April 2014 it was announced that Cornish people would be granted minority status under the European Framework Convention for the Protection of National Minorities.
Settlements and communication.
Cornwall's only city, and the home of the council headquarters, is Truro. Nearby Falmouth is notable as a port. St Just in Penwith is the westernmost town in England, though the same claim has been made for Penzance, which is larger. St Ives and Padstow are today small vessel ports with a major tourism and leisure sector in their economies. Newquay on the north coast is famous for its beaches and is a popular surfing destination, as is Bude further north. St Austell is the county's largest town and more populous than the capital Truro; it was the centre of the china clay industry in Cornwall. Redruth and Camborne form the largest urban area in Cornwall, and both towns were significant as centres of the global tin mining industry in the 19th century (nearby copper mines were also very productive during that period).
Cornwall borders the county of Devon at the River Tamar. Major road links between Cornwall and the rest of Great Britain are the A38 which crosses the Tamar at Plymouth via the Tamar Bridge and the town of Saltash, the A39 road (Atlantic Highway) from Barnstaple, passing through North Cornwall to end in Falmouth, and the A30 which crosses the border south of Launceston crosses Bodmin Moor and connects Bodmin and Truro. Torpoint Ferry links Plymouth with Torpoint on the opposite side of the Hamoaze. A rail bridge, the Royal Albert Bridge, built by Isambard Kingdom Brunel (1859) provides the only other major transport link. The major city of Plymouth, a large urban centre closest to east Cornwall is an important location for services such as hospitals, department stores, road and rail transport, and cultural venues.
Newquay Cornwall International Airport provides an airlink to the rest of the UK, Ireland and Europe.
Cardiff and Swansea, across the Bristol Channel, have at some times in the past been connected to Cornwall by ferry, but these do not operate currently.
The Isles of Scilly are served by ferry (from Penzance) and by aeroplane, having its own airport — St Mary's Airport. There are regular flights between St Mary's and Land's End Airport, near St Just, and Newquay Airport; during the summer season, a service also exist between St Mary's and Exeter International Airport, in Devon.
Flag.
Saint Piran's Flag is regarded by many as the national flag of Cornwall, and an emblem of the Cornish people; and by others as the county flag. The banner of Saint Piran is a white cross on a black background (in terms of heraldry 'sable, a cross argent'). Saint Piran is supposed to have adopted these two colours from seeing the white tin in the black coals and ashes during his supposed discovery of tin. Davies Gilbert in 1826 described it as anciently the flag of St Piran and the banner of Cornwall, and another history of 1880 said that: "The white cross of St. Piran was the ancient banner of the Cornish people." The Cornish flag is an exact reverse of the former Breton national flag (black cross) and is known by the same name "Kroaz Du".
There are also claims that the patron saint of Cornwall is Saint Michael or Saint Petroc, but Saint Piran is by far the most popular of the three and his emblem is internationally recognised as the flag of Cornwall. St Piran's Day (5 March) is celebrated by the Cornish diaspora around the world.
Heraldry.
For the heraldry of Cornwall see:
Economy.
Cornwall is one of the poorest parts of the United Kingdom in terms of per capita GDP and average household incomes. At the same time, parts of the county, especially on the coast, have high house prices, driven up by demand from relatively wealthy retired people and second-home owners. The GVA per head was 65% of the UK average for 2004. The GDP per head for Cornwall and the Isles of Scilly was 79.2% of the EU-27 average for 2004, the UK per head average was 123.0%. In 2011, the latest available figures, Cornwall's (including the Isles of Scilly) measure of wealth was 64% of the European average per capita.
Historically mining of tin (and later also of copper) was important in the Cornish economy. The first reference to this appears to be by Pytheas: "see above". Julius Caesar was the last classical writer to mention the tin trade, which appears to have declined during the Roman occupation. The tin trade revived in the Middle Ages and its importance to the Kings of England resulted in certain privileges being granted to the tinners; the Cornish Rebellion of 1497 is attributed to grievances of the tin miners. In the mid-19th century, however, the tin trade again fell into decline. Other primary industries that have declined since the 1960s include china clay production, fishing and farming.
Today, the Cornish economy depends heavily on its tourist industry, which makes up around a quarter of the economy. The official measures of deprivation and poverty at district and 'sub-ward' level show that there is great variation in poverty and prosperity in Cornwall with some areas among the poorest in England and others among the top half in prosperity. For example, the ranking of 32,482 sub-wards in England in the index of multiple deprivation (2006) ranged from 819th (part of Penzance East) to 30,899th (part of Saltash Burraton in Caradon), where the lower number represents the greater deprivation.
Cornwall is one of two UK areas designated as 'less developed regions' which qualify for Cohesion 
Policy grants from the European Union. It was granted Objective 1 status by the European Commission for 2000 to 2006, followed by further rounds of funding known as 'Convergence Funding' from 2007 to 2013 and 'Growth Programme' for 2014 to 2020.
Tourism.
Tourism is estimated to contribute up to 24% of Cornwall's gross domestic product. In 2011 Tourism brought £1.85 billion into the Cornish economy. Cornwall's unique culture, spectacular landscape and mild climate make it a popular tourist destination, despite being somewhat distant from the United Kingdom's main centres of population. Surrounded on three sides by the English Channel and Celtic Sea, Cornwall has many miles of beaches and cliffs; the South West Coast Path follows a complete circuit of both coasts. Other tourist attractions include moorland, country gardens, museums, historic and prehistoric sites, and wooded valleys. Five million tourists visit Cornwall each year, mostly drawn from within the UK. Visitors to Cornwall are served by airports at Newquay and Exeter, whilst private jets, charters and helicopters are also served by Perranporth airfield; nightsleeper and daily rail services run between Cornwall, London and other regions of the UK. Cornwall has a tourism-based seasonal economy.
Newquay and Porthtowan are popular destinations for surfers. In recent years, the Eden Project near St Austell has been a major financial success, drawing one in eight of Cornwall's visitors.
Internet.
Cornwall is the landing point for one of the world's fastest high-speed transatlantic fibre optic cables, making Cornwall an important hub within Europe's Internet infrastructure. The Superfast Cornwall project completed in 2015, and saw 95% of Cornish houses and businesses connected to a fibre-based broadband network, with over 90% of properties able to connect with speeds above 24Mbit/s.
Other industries.
Other industries are fishing, although this has been significantly re-structured by EU fishing policies (the Southwest Handline Fishermen's Association has started to revive the fishing industry), and agriculture, which has also declined significantly. Mining of tin and copper was also an industry, but today the derelict mine workings survive only as a World Heritage Site. However, the Camborne School of Mines, which was relocated to Penryn in 2004, is still a world centre of excellence in the field of mining and applied geology and the grant of World Heritage status has attracted funding for conservation and heritage tourism. China clay extraction has also been an important industry in the St Austell area, but this sector has been in decline, and this, coupled with increased mechanisation, has led to a decrease in employment in this sector, although the industry still employs around 2,133 people in Cornwall, and generates over £80 Million to the local economy
Demographics.
Cornwall's population was 537,400 at the last census, with a population density of 144 people per square kilometre, ranking it 40th and 41st respectively among the 47 counties of England. Cornwall's population was 95.7% White British and has a relatively high level of population growth. At 11.2% in the 1980s and 5.3% in the 1990s, it had the fifth-highest population growth rate of the English counties. The natural change has been a small population decline, and the population increase is due to inward migration into Cornwall. According to the 1991 census, the population was 469,800.
Cornwall has a relatively high retired population, with 22.9% of pensionable age, compared with 20.3% for the United Kingdom as a whole. This may be due to a combination of Cornwall's rural and coastal geography increasing its popularity as a retirement location, and outward migration of younger residents to more economically diverse areas.
Education system.
Cornwall has a comprehensive education system, with 31 state and eight independent secondary schools. There are three further education colleges: Penwith College (a former sixth form college), Cornwall College (occupying the former home of the Camborne School of Mines) and Truro College. The Isles of Scilly only has one school while the former Restormel district has the highest school population, and school year sizes are around 200, with none above 270.
Higher education is provided by Falmouth University, the University of Exeter (including Camborne School of Mines), the Combined Universities in Cornwall, and by Truro College, Penwith College (which combined in 2008 to make Truro and Penwith College) and Cornwall College.
Languages and dialects.
English is the main language used in Cornwall, although the revived Cornish language may be seen on road signs and is spoken fluently by a small minority of people.
Cornish language.
The Cornish language is closely related to the other Brythonic languages of Welsh and Breton, and less so to the Goidelic languages of Irish, Scots Gaelic and Manx. The language continued to function visibly as a community language in parts of Cornwall until the late 18th century, and it was claimed in 2011 that the last native speaker did not die until 1914.
There has been a revival of the language since Henry Jenner's "Handbook of the Cornish Language" was published in 1904. A study in 2000 suggested that there were around 300 people who spoke Cornish fluently. Cornish, however, had no legal status in the UK until 2002. Nevertheless, the language is taught in about twelve primary schools, and occasionally used in religious and civic ceremonies. In 2002 Cornish was officially recognised as a UK minority language and in 2005 it received limited Government funding. A Standard Written Form was agreed in 2008.
Several Cornish mining words are used in English language mining terminology, such as costean, gunnies, vug, kibbal, gossan and kieve.
In the 2010–15 Parliament of the United Kingdom, four Cornish MPs, Andrew George, MP for St Ives, Dan Rogerson, MP for North Cornwall, Steve Gilbert, MP for St Austell and Newquay, and Sarah Newton, MP for Truro and Falmouth repeated their Parliamentary oaths in Cornish.
Culture.
Visual arts.
Since the 19th century, Cornwall, with its unspoilt maritime scenery and strong light, has sustained a vibrant visual art scene of international renown. Artistic activity within Cornwall was initially centred on the art-colony of Newlyn, most active at the turn of the 20th century. This Newlyn School is associated with the names of Stanhope Forbes, Elizabeth Forbes, Norman Garstin and Lamorna Birch. Modernist writers such as D. H. Lawrence and Virginia Woolf lived in Cornwall between the wars, and Ben Nicholson, the painter, having visited in the 1920s came to live in St Ives with his then wife, the sculptor Barbara Hepworth, at the outbreak of the second world war. They were later joined by the Russian emigrant Naum Gabo, and other artists. These included Peter Lanyon, Terry Frost, Patrick Heron, Bryan Wynter and Roger Hilton. St Ives also houses the Leach Pottery, where Bernard Leach, and his followers championed Japanese inspired studio pottery. Much of this modernist work can be seen in Tate St Ives. The Newlyn Society and Penwith Society of Arts continue to be active, and contemporary visual art is documented in a dedicated online journal.
Music and festivals.
Cornwall has a full and vibrant folk music tradition which has survived into the present and is well known for its unusual folk survivals such as Mummers Plays, the Furry Dance in Helston played by the famous Helston Town Band, and Obby Oss in Padstow.
Newlyn is home to a food and music festival which hosts live music, cooking demonstrations, and displays of locally caught fish.
As in other former mining districts of Britain, male voice choirs and Brass Bands, e.g. "Brass on the Grass" concerts during the summer at Constantine, are still very popular in Cornwall: Cornwall also has around 40 brass bands, including the six-times National Champions of Great Britain, Camborne Youth Band, and the bands of Lanner and St Dennis.
Cornish players are regular participants in inter-Celtic festivals, and Cornwall itself has several lively inter-Celtic festivals such as Perranporth's Lowender Peran folk festival.
On a more modern note, contemporary musician Richard D. James (also known as Aphex Twin) grew up in Cornwall, as did Luke Vibert and Alex Parks, winner of Fame Academy 2003. Roger Taylor, the drummer from the band Queen was also raised in the county, and currently lives not far from Falmouth. The American singer/songwriter Tori Amos now resides predominantly in North Cornwall not far from Bude with her family. The lutenist, lutarist, composer and festival director Ben Salfield lives in Truro.
Literature.
Cornwall's rich heritage and dramatic landscape have inspired writers since the 19th century.
Fiction.
Sir Arthur Quiller-Couch, author of many novels and works of literary criticism, lived in Fowey: his novels are mainly set in Cornwall. Daphne du Maurier lived at Menabilly near Fowey and many of her novels had Cornish settings, including "Rebecca", "Jamaica Inn", "Frenchman's Creek", "My Cousin Rachel", and "The House on the Strand". She is also noted for writing "Vanishing Cornwall". Cornwall provided the inspiration for "The Birds", one of her terrifying series of short stories, made famous as a film by Alfred Hitchcock. 
Medieval Cornwall is the setting of the trilogy by Monica Furlong, "Wise Child", "Juniper", and "Colman", as well as part of Charles Kingsley's "Hereward the Wake".
Conan Doyle's "The Adventure of the Devil's Foot" featuring Sherlock Holmes is set in Cornwall. Winston Graham's series "Poldark", Kate Tremayne's Adam Loveday series, Susan Cooper's novels "Over Sea, Under Stone" and "Greenwitch", and Mary Wesley's "The Camomile Lawn" are all set in Cornwall. Writing under the pseudonym of Alexander Kent, Douglas Reeman sets parts of his Richard Bolitho and Adam Bolitho series in the Cornwall of the late 18th and the early 19th centuries, particularly in Falmouth.
Hammond Innes's novel, "The Killer Mine"; Charles de Lint's novel "The Little Country"; and Chapters 24 and 25 of J. K. Rowling's "Harry Potter and the Deathly Hallows" take place in Cornwall (the Harry Potter story at Shell Cottage, which is on the beach outside the fictional village of Tinworth in Cornwall).
David Cornwell, who writes espionage novels under the name John le Carré, lives and writes in Cornwall. Nobel Prize-winning novelist William Golding was born in St Columb Minor in 1911, and returned to live near Truro from 1985 until his death in 1993. D. H. Lawrence spent a short time living in Cornwall. Rosamunde Pilcher grew up in Cornwall, and several of her books take place there.
Poetry.
The late Poet Laureate Sir John Betjeman was famously fond of Cornwall and it featured prominently in his poetry. He is buried in the churchyard at St Enodoc's Church, Trebetherick.
Charles Causley, the poet, was born in Launceston and is perhaps the best known of Cornish poets. Jack Clemo and the scholar A. L. Rowse were also notable Cornishmen known for their poetry; The Rev. R. S. Hawker of Morwenstow wrote some poetry which was very popular in the Victorian period. The Scottish poet W. S. Graham lived in West Cornwall from 1944 until his death in 1986.
The poet Laurence Binyon wrote "For the Fallen" (first published in 1914) while sitting on the cliffs between Pentire Point and The Rumps and a stone plaque was erected in 2001 to commemorate the fact. The plaque bears the inscription "FOR THE FALLEN / Composed on these cliffs, 1914". The plaque also bears below this the fourth stanza (sometimes referred to as "The Ode") of the poem:
Other literary works.
Cornwall produced a substantial number of passion plays such as the Ordinalia during the Middle Ages. Many are still extant, and provide valuable information about the Cornish language. See also Cornish literature
Prolific writer Colin Wilson, best known for his debut work "The Outsider" (1956) and for "The Mind Parasites" (1967), lives in Gorran Haven, a small village on the southern Cornish coast. The writer D. M. Thomas was born in Redruth but lived and worked in Australia and the United States before returning to his native Cornwall. He has written novels, poetry, and other works, including translations from Russian.
Thomas Hardy's drama "The Queen of Cornwall" (1923) is a version of the Tristan story; the second act of Richard Wagner's opera "Tristan und Isolde" takes place in Cornwall, as do Gilbert and Sullivan's operettas "The Pirates of Penzance" and "Ruddigore". A level of "", a game dealing with Arthurian Legend, takes place in Cornwall at a tacky museum above King Arthur's tomb.
The fairy tale Jack the Giant Killer takes place in Cornwall.
Sports and games.
With its comparatively small, and largely rural population, major contribution by the Cornish to national sport in the United Kingdom has been limited, with the county's greatest successes coming in fencing. In 2014, half of the men's GB team fenced for Truro Fencing Club, and 3 Truro fencers appeared at the 2012 Olympics. Truro, all of the towns and some villages have football clubs belonging to the Cornwall County Football Association, and the Cornwall County Cricket Club plays as one of the minor counties of English cricket. Viewed as an "important identifier of ethnic affiliation", rugby union has become a sport strongly tied to notions of Cornishness. and since the 20th century, rugby union in Cornwall has emerged as one of the most popular spectator and team sports in Cornwall (perhaps the most popular), with professional Cornish rugby footballers being described as a "formidable force", "naturally independent, both in thought and deed, yet paradoxically staunch English patriots whose top players have represented England with pride and passion". In 1985, sports journalist Alan Gibson made a direct connection between love of rugby in Cornwall and the ancient parish games of hurling and wrestling that existed for centuries before rugby officially began. Among Cornwall's native sports are a distinctive form of Celtic wrestling related to Breton wrestling, and Cornish hurling, a kind of mediaeval football played with a silver ball (distinct from Irish Hurling). Cornish Wrestling is Cornwall's oldest sport and as Cornwall's native tradition it has travelled the world to places like Victoria, Australia and Grass Valley, California following the miners and gold rushes. Cornish hurling now takes place at St. Columb Major, St Ives, and less frequently at Bodmin.
Surfing and other water sports.
Due to its long coastline, various maritime sports are popular in Cornwall, notably sailing and surfing. International events in both are held in Cornwall. Cornwall hosted the Inter-Celtic Watersports Festival in 2006. Surfing in particular is very popular, as locations such as Bude and Newquay offer some of the best surf in the UK. Pilot gig rowing has been popular for many years and the World championships takes place annually on the Isles of Scilly. On 2 September 2007, 300 surfers at Polzeath beach set a new world record for the highest number of surfers riding the same wave as part of the Global Surf Challenge and part of a project called Earthwave to raise awareness about global warming.
Cuisine.
Cornwall has a strong culinary heritage. Surrounded on three sides by the sea amid fertile fishing grounds, Cornwall naturally has fresh seafood readily available; Newlyn is the largest fishing port in the UK by value of fish landed, and is known for its wide range of restaurants. Television chef Rick Stein has long operated a fish restaurant in Padstow for this reason, and Jamie Oliver chose to open his second restaurant, Fifteen, in Watergate Bay near Newquay. MasterChef host and founder of Smiths of Smithfield, John Torode, in 2007 purchased Seiners in Perranporth. One famous local fish dish is Stargazy pie, a fish-based pie in which the heads of the fish stick through the piecrust, as though "star-gazing". The pie is cooked as part of traditional celebrations for Tom Bawcock's Eve, but is not generally eaten at any other time.
Cornwall is perhaps best known though for its pasties, a savoury dish made with pastry. Today's pasties usually contain a filling of beef steak, onion, potato and swede with salt and white pepper, but historically pasties had a variety of different fillings. "Turmut, 'tates and mate" (i.e. "Turnip, potatoes and meat", turnip being the Cornish and Scottish term for swede, itself an abbreviation of 'Swedish Turnip', the British term for rutabaga) describes a filling once very common. For instance, the licky pasty contained mostly leeks, and the herb pasty contained watercress, parsley, and shallots. Pasties are often locally referred to as "oggies". Historically, pasties were also often made with sweet fillings such as jam, apple and blackberry, plums or cherries.
The wet climate and relatively poor soil of Cornwall make it unsuitable for growing many arable crops. However, it is ideal for growing the rich grass required for dairying, leading to the production of Cornwall's other famous export, clotted cream. This forms the basis for many local specialities including Cornish fudge and Cornish ice cream. Cornish clotted cream has Protected Geographical Status under EU law, and cannot be made anywhere else. Its principal manufacturer is A. E. Rodda & Son of Scorrier.
Local cakes and desserts include Saffron cake, Cornish heavy ("hevva") cake, Cornish fairings biscuits, figgy 'obbin, Cream tea and whortleberry pie.
There are also many types of beers brewed in Cornwall – those produced by Sharp's Brewery, Skinner's Brewery and St Austell Brewery are the best-known – including stouts, ales and other beer types. There is some small scale production of wine, mead and cider.

</doc>
<doc id="5649" url="https://en.wikipedia.org/wiki?curid=5649" title="Constitutional monarchy">
Constitutional monarchy

A constitutional monarchy, limited monarchy or parliamentary monarchy (in its most limited form, a crowned republic) is a monarchy in which governing powers of the monarch are restricted by a constitution. Constitutional monarchy differs from absolute monarchy, in which a monarch holds absolute power.
A constitutional monarchy may refer to a system in which the monarch acts as a non-party political head of state under the constitution, whether written or unwritten. While most monarchs may hold formal reserve powers and the government may officially take place in the monarch's name, they do not set public policy or choose political leaders. Political scientist Vernon Bogdanor, paraphrasing Thomas Macaulay, has defined a constitutional monarch as "a sovereign who reigns but does not rule". In addition to acting as a visible symbol of national unity, a constitutional monarch may hold formal powers such as dissolving parliament or giving Royal Assent to legislation. However, the exercise of such powers is generally a formality rather than an opportunity for the sovereign to enact personal political preference. In "The English Constitution", British political theorist Walter Bagehot identified three main political rights which a constitutional monarch could freely exercise: the right to be consulted, the right to encourage, and the right to warn. Some constitutional monarchs, however, still retain significant power and influence and play an important political role.
The United Kingdom and fifteen of its former colonies are constitutional monarchies with a Westminster system of government. Three states – Malaysia, Cambodia and the Holy See – employ true elective monarchies, where the ruler is periodically selected by a small electoral college of the aristocracy.
History.
The oldest constitutional monarchy dating back to ancient times was that of the Hittites. They were an ancient Anatolian people that lived during the Bronze Age whose king or queen had to share their authority with an assembly, called "Panku", equivalent to a modern-day deliberative assembly of a legislature. These were scattered noble families that worked as representatives of their subjects in an adjutant or subaltern federal-type landscape.
The country to move from an absolute monarchy to a constitutional monarchy was Bhutan, between 2007 and 2008 "(see Politics of Bhutan, Constitution of Bhutan and Bhutanese democracy)".
Constitutional and absolute monarchy.
In the Kingdom of England, the Glorious Revolution of 1688 led to a constitutional monarchy restricted by laws such as the Bill of Rights 1689 and the Act of Settlement 1701, although limits on the power of the monarch ('a limited monarchy') are much older than that (see Magna Carta). At the same time, in Scotland the Convention of Estates enacted the Claim of Right Act 1689, which placed similar limits on the Scottish monarchy. From the Hanoverian accession to the throne of Great Britain, monarchs saw their powers pass further to their ministers, and Royal neutrality in politics became cemented early in the reign of Queen Victoria (although she had her personal favourites) following enlargements to the franchise. Today, the role of the British monarch is by convention effectively ceremonial. Instead, the British Parliament and the Government – chiefly in the office of Prime Minister – exercise their powers under 'Royal (or Crown) Prerogative': on behalf of the monarch and through powers still formally possessed by the Monarch.
No person may accept significant public office without swearing an oath of allegiance to the Queen. With few exceptions, the monarch is bound by constitutional convention to act on the advice of the Government.
Constitutional monarchy occurred first in continental Europe, briefly in the early years of the French Revolution, but much more widely afterwards. Napoleon Bonaparte is considered the first monarch proclaiming "himself" as an embodiment of the nation, rather than as a divinely-appointed ruler; this interpretation of monarchy is germane to continental constitutional monarchies. G.W.F. Hegel, in his "Elements of the Philosophy of Right" (1820), gave it a philosophical justification that concurred with evolving contemporary political theory and the Protestant Christian view of natural law. Hegel's forecast of a constitutional monarch with very limited powers whose function is to embody the national character and provide constitutional continuity in times of emergency was reflected in the development of constitutional monarchies in Europe and Japan. His forecast of the form of government suitable to the modern world may be seen as prophetic: the largely ceremonial offices of president in some modern parliamentary republics in Europe and Israel can be perceived as elected or appointed versions of Hegel's constitutional monarch; the Russian and French presidents, with their stronger powers, may also be regarded in Hegelian terms as wielding powers suitable to the embodiment of the national will.
Modern constitutional monarchy.
As originally conceived, a constitutional monarch was head of the executive branch and quite a powerful figure, even though his or her power was limited by the constitution and the elected parliament. Some of the framers of the US Constitution may have envisaged the president as an elected constitutional monarch, as the term was then understood, following Montesquieu's account of the separation of powers.
The present-day concept of a constitutional monarchy developed in the United Kingdom, where the democratically elected parliaments, and their leader, the prime minister, exercise power, with the monarchs having ceded power and remaining as a titular position. In many cases the monarchs, while still at the very top of the political and social hierarchy, were given the status of "servants of the people" to reflect the new, egalitarian position. In the course of France's July Monarchy, Louis-Philippe I was styled "King of the French" rather than "King of France".
Following the Unification of Germany, Otto von Bismarck rejected the British model. In the constitutional monarchy established under the Constitution of the German Empire which Bismarck inspired, the Kaiser retained considerable actual executive power, while the Imperial Chancellor needed no parliamentary vote of confidence and ruled solely by the imperial mandate. However this model of constitutional monarchy was discredited and abolished following Germany's defeat in the First World War. Later, Fascist Italy could also be considered as a constitutional monarchy, in that there was a king as the titular head of state while actual power was held by Benito Mussolini under a constitution. This eventually discredited the Italian monarchy and led to its abolition in 1946. After the Second World War, surviving European monarchies almost invariably adopted some variant of the constitutional monarchy model originally developed in Britain.
Nowadays a parliamentary democracy that is a constitutional monarchy is considered to differ from one that is a republic only in detail rather than in substance. In both cases, the titular head of state—monarch or president—serves the traditional role of embodying and representing the nation, while the government is carried on by a cabinet composed predominantly of elected Members of Parliament.
However, three important factors distinguish monarchies such as the United Kingdom from systems where greater power might otherwise rest with Parliament. These are: the Royal Prerogative under which the monarch may exercise power under certain very limited circumstances; Sovereign Immunity under which the monarch may "do no wrong" under the law because the responsible government is instead deemed accountable; and the monarch may not be subject to the same taxation or property use restrictions as most citizens. Other privileges may be nominal or ceremonial (e.g., where the executive, judiciary, police or armed forces act on the authority of or owe allegiance to the Crown).
Today slightly more than a quarter of constitutional monarchies are Western European countries, including the United Kingdom, the Netherlands, Belgium, Norway, Denmark, Spain, Luxembourg, Monaco, Liechtenstein and Sweden. However, the two most populous constitutional monarchies in the world are in Asia: Japan and Thailand. In these countries the prime minister holds the day-to-day powers of governance, while the monarch retains residual (but not always insignificant) powers. The powers of the monarch differ between countries. In Denmark and in Belgium, for example, the Monarch formally appoints a representative to preside over the creation of a coalition government following a parliamentary election, while in Norway the King chairs special meetings of the cabinet.
In nearly all cases, the monarch is still the nominal chief executive, but is bound by convention to act on the advice of the Cabinet. Only a few monarchies (most notably Japan and Sweden) have amended their constitutions so that the monarch is no longer even the nominal chief executive.
There are sixteen constitutional monarchies under Queen Elizabeth II, which are known as Commonwealth realms. Unlike some of their continental European counterparts, the Monarch and her Governors-General in the Commonwealth realms hold significant "reserve" or "prerogative" powers, to be wielded in times of extreme emergency or constitutional crises, usually to uphold parliamentary government. An instance of a Governor-General exercising such power occurred during the 1975 Australian constitutional crisis, when the Australian Prime Minister, Gough Whitlam, was dismissed by the Governor-General. The Australian senate had threatened to block the Government's budget by refusing to pass the necessary appropriation bills. On November 11, 1975, Whitlam intended to call a half-Senate election in an attempt to break the deadlock. When he sought the Governor-General's approval of the election, the Governor-General instead dismissed him as Prime Minister, and shortly thereafter installed leader of the opposition Malcolm Fraser in his place. Acting quickly before all parliamentarians became aware of the change of government, Fraser and his allies secured passage of the appropriation bills, and the Governor-General dissolved Parliament for a double dissolution election. Fraser and his government were returned with a massive majority. This led to much speculation among Whitlam's supporters as to whether this use of the Governor-General's reserve powers was appropriate, and whether Australia should become a republic. Among supporters of constitutional monarchy, however, the experience confirmed the value of the monarchy as a source of checks and balances against elected politicians who might seek powers in excess of those conferred by the constitution, and ultimately as a safeguard against dictatorship.
In Thailand's constitutional monarchy, the monarch is recognized as the Head of State, Head of the Armed Forces, Upholder of the Buddhist Religion, and Defender of the Faith. The current King, Bhumibol Adulyadej, is the longest reigning current monarch in the world and in all of Thailand's history. Bhumibol has reigned through several political changes in the Thai government. He has played an influential role in each incident, often acting as mediator between disputing political opponents. (See Bhumibol's role in Thai Politics.) Among the powers retained by the monarch under the constitution, lèse majesté protects the image of the monarch and enables him to play a role in politics. It carries strict criminal penalties for violators. Generally, the Thai people are reverent of Bhumibol. Much of his social influence arises from this reverence and from the socio-economic improvement efforts undertaken by the royal family.
In both the United Kingdom and elsewhere, a frequent debate centres on when it is appropriate for a monarch to use his or her political powers. When a monarch does act, political controversy can often ensue, partially because the neutrality of the crown is seen to be compromised in favour of a partisan goal, while some political scientists champion the idea of an "interventionist monarch" as a check against possible illegal action by politicians. For instance, the monarch of the United Kingdom can theoretically exercise an absolute veto over legislation by withholding Royal Assent. However, no monarch has done so since 1708, and it is widely believed that this and many of the monarch's other political powers are lapsed powers.
There are currently 44 monarchies, and most of them are constitutional monarchies.

</doc>
<doc id="5653" url="https://en.wikipedia.org/wiki?curid=5653" title="Clarke's three laws">
Clarke's three laws

British science fiction writer Arthur C. Clarke formulated three prediction-related adages that are known as Clarke's three laws.
Formulation.
The three laws are sometimes formulated as follows:
Origins.
Clarke's first law was proposed by Clarke in the essay "Hazards of Prophecy: The Failure of Imagination", in "Profiles of the Future" (1962).
The second law is offered as a simple observation in the same essay. Its status as Clarke's second law was conferred by others. In a 1973 revision of "Profiles of the Future", Clarke acknowledged the second law and proposed the third. "As three laws were good enough for Newton, I have modestly decided to stop there".
The third law is the best known and most widely cited, and appears in Clarke's 1973 revision of "Hazards of Prophecy: The Failure of Imagination". It echoes a statement in a 1942 story by Leigh Brackett: "Witchcraft to the ignorant, … simple science to the learned". An earlier example of this sentiment may be found in "Wild Talents" (1932) by the author Charles Fort, where he makes the statement: "...a performance that may some day be considered understandable, but that, in these primitive times, so transcends what is said to be the known that it is what I mean by magic." Even earlier, Rider Haggard's novel "" (1886) expresses the sentiment multiple times, such as in chapter 17: "Fear not, my Holly, I shall use no magic. Have I not told thee that there is no such thing as magic, though there is such a thing as understanding and applying the forces which are in Nature?"
Proposed fourth law.
A fourth law has been proposed for the canon, despite Clarke's declared intention of not going one better than Newton. Geoff Holder quotes: "For every expert, there is an equal and opposite expert."
Variants of the third law.
The third law has inspired many snowclones and other variations:
Rubin is referring to an earlier work of his: 
</ref> (referring to artificial intelligence)
A of the third law is
The third law has been:

</doc>
<doc id="5654" url="https://en.wikipedia.org/wiki?curid=5654" title="Caspar David Friedrich">
Caspar David Friedrich

[[File:Caspar David Friedrich - Wanderer above the sea of fog.jpg|thumb|"Wanderer above the Sea of Fog" (1818). 94.8 × 74.8 cm, Kunsthalle Hamburg. This well-known and especially Romantic masterpiece was described by the historian John Lewis Gaddis as leaving a contradictory impression, "suggesting at once mastery over a landscape and the insignificance of the individual within it. We see no face, so it's impossible to know whether the prospect facing the young man is exhilarating, or terrifying, or both."]]
Caspar David Friedrich (5 September 1774 – 7 May 1840) was a 19th-century German Romantic landscape painter, generally considered the most important German artist of his generation. He is best known for his mid-period allegorical landscapes which typically feature contemplative figures silhouetted against night skies, morning mists, barren trees or Gothic ruins. His primary interest as an artist was the contemplation of nature, and his often symbolic and anti-classical work seeks to convey a subjective, emotional response to the natural world. Friedrich's paintings characteristically set a human presence in diminished perspective amid expansive landscapes, reducing the figures to a scale that, according to the art historian Christopher John Murray, directs "the viewer's gaze towards their metaphysical dimension".
Friedrich was born in the Pomeranian town of Greifswald at the Baltic Sea, where he began his studies in art as a young man. He studied in Copenhagen until 1798, before settling in Dresden. He came of age during a period when, across Europe, a growing disillusionment with materialistic society was giving rise to a new appreciation of spirituality. This shift in ideals was often expressed through a reevaluation of the natural world, as artists such as Friedrich, J. M. W. Turner (1775–1851) and John Constable (1776–1837) sought to depict nature as a "divine creation, to be set against the artifice of human civilization".
Friedrich's work brought him renown early in his career, and contemporaries such as the French sculptor David d'Angers (1788–1856) spoke of him as a man who had discovered "the tragedy of landscape". Nevertheless, his work fell from favour during his later years, and he died in obscurity, and in the words of the art historian Philip Miller, "half mad". As Germany moved towards modernisation in the late 19th century, a new sense of urgency characterised its art, and Friedrich's contemplative depictions of stillness came to be seen as the products of a bygone age. The early 20th century brought a renewed appreciation of his work, beginning in 1906 with an exhibition of thirty-two of his paintings and sculptures in Berlin. By the 1920s his paintings had been discovered by the Expressionists, and in the 1930s and early 1940s Surrealists and Existentialists frequently drew ideas from his work. The rise of Nazism in the early 1930s again saw a resurgence in Friedrich's popularity, but this was followed by a sharp decline as his paintings were, by association with the Nazi movement, interpreted as having a nationalistic aspect. It was not until the late 1970s that Friedrich regained his reputation as an icon of the German Romantic movement and a painter of international importance.
Life.
Early years and family.
Caspar David Friedrich was born on 5 September 1774, in Greifswald, Swedish Pomerania, on the Baltic coast of Germany. The sixth of ten children, he was brought up in the strict Lutheran creed of his father Adolf Gottlieb Friedrich, a candle-maker and soap boiler. Records of the family's financial circumstances are contradictory; while some sources indicate the children were privately tutored, others record that they were raised in relative poverty. Caspar David was familiar with death from an early age. His mother, Sophie Dorothea Bechly, died in 1781 when he was just seven. A year later, his sister Elisabeth died, while a second sister, Maria, succumbed to typhus in 1791. Arguably the greatest tragedy of his childhood was the 1787 death of his brother Johann Christoffer: at the age of thirteen, Caspar David witnessed his younger brother fall through the ice of a frozen lake and drown. Some accounts suggest that Johann Christoffer perished while trying to rescue Caspar David, who was also in danger on the ice.
Friedrich began his formal study of art in 1790 as a private student of artist Johann Gottfried Quistorp at the University of Greifswald in his home city, at which the art department is now named "Caspar-David-Friedrich-Institut" in his honour. Quistorp took his students on outdoor drawing excursions; as a result, Friedrich was encouraged to sketch from life at an early age. Through Quistorp, Friedrich met and was subsequently influenced by the theologian Ludwig Gotthard Kosegarten, who taught that nature was a revelation of God. Quistorp introduced Friedrich to the work of the German 17th-century artist Adam Elsheimer, whose works often included religious subjects dominated by landscape, and nocturnal subjects. During this period he also studied literature and aesthetics with Swedish professor Thomas Thorild. Four years later Friedrich entered the prestigious Academy of Copenhagen, where he began his education by making copies of casts from antique sculptures before proceeding to drawing from life. Living in Copenhagen afforded the young painter access to the Royal Picture Gallery's collection of 17th-century Dutch landscape painting. At the Academy he studied under teachers such as Christian August Lorentzen and the landscape painter Jens Juel. These artists were inspired by the "Sturm und Drang" movement and represented a midpoint between the dramatic intensity and expressive manner of the budding Romantic aesthetic and the waning neo-classical ideal. Mood was paramount, and influence was drawn from such sources as the Icelandic legend of Edda, the poems of Ossian and Norse mythology.
Friedrich settled permanently in Dresden in 1798. During this early period, he experimented in printmaking with etchings and designs for woodcuts which his furniture-maker brother cut. By 1804 he had produced 18 etchings and four woodcuts; they were apparently made in small numbers and only distributed to friends. Despite these forays into other media, he gravitated toward working primarily with ink, watercolour and sepias. With the exception of a few early pieces, such as "" (1797), he did not work extensively with oils until his reputation was more established. Landscapes were his preferred subject, inspired by frequent trips, beginning in 1801, to the Baltic coast, Bohemia, the Krkonoše and the Harz Mountains. Mostly based on the landscapes of northern Germany, his paintings depict woods, hills, harbors, morning mists and other light effects based on a close observation of nature. These works were modeled on sketches and studies of scenic spots, such as the cliffs on Rügen, the surroundings of Dresden and the river Elbe. He executed his studies almost exclusively in pencil, even providing topographical information, yet the subtle atmospheric effects characteristic of Friedrich's mid-period paintings were rendered from memory. These effects took their strength from the depiction of light, and of the illumination of sun and moon on clouds and water: optical phenomena peculiar to the Baltic coast that had never before been painted with such an emphasis.
Move to Dresden.
Friedrich established his reputation as an artist when he won a prize in 1805 at the Weimar competition organised by the writer, poet, and dramatist Johann Wolfgang von Goethe. At the time, the Weimar competition tended to draw mediocre and now long-forgotten artists presenting derivative mixtures of neo-classical and pseudo-Greek styles. The poor quality of the entries began to prove damaging to Goethe's reputation, so when Friedrich entered two sepia drawings—"Procession at Dawn" and "Fisher-Folk by the Sea"—the poet responded enthusiastically and wrote, "We must praise the artist's resourcefulness in this picture fairly. The drawing is well done, the procession is ingenious and appropriate... his treatment combines a great deal of firmness, diligence and neatness... the ingenious watercolour... is also worthy of praise."
Friedrich completed the first of his major paintings in 1807, at the age of 34. "The Cross in the Mountains", today known as the "Tetschen Altar" (Galerie Neue Meister, Dresden), is an altarpiece panel commissioned by the Countess of Thun for her family's chapel in Tetschen, Bohemia. It was to be one of the few commissions the artist received. The altar panel depicts a "Gipfelkreuz," or a gilded cross, in profile at the top of a mountain, alone, and surrounded by German and Austrian pine trees. The cross reaches the highest point in the pictorial plane but is presented from an oblique and a distant viewpoint. Nature dominates the scene and for the first time in Christian art, an altarpiece showcases a landscape. According to the art historian Linda Siegel, the design of the altarpiece is the "logical climax of many earlier drawings of his which depicted a cross in nature's world."
The work was first exhibited on Christmas Day, 1808. Although it was generally coldly received, it was nevertheless Friedrich's first painting to receive wide publicity. The artist's friends publicly defended the work, while art critic Basilius von Ramdohr published a lengthy article rejecting Friedrich's use of landscape in such a context; he wrote that it would be "a veritable presumption, if landscape painting were to sneak into the church and creep onto the altar". Ramdohr fundamentally challenged the concept that pure landscape painting could convey explicit meaning. Friedrich responded with a programme describing his intentions. In his 1809 commentary on the painting, he compared the rays of the evening sun to the light of the Holy Father. The sinking of the sun suggests that the era when God revealed himself directly to man has passed. This statement marked the only time Friedrich recorded a detailed interpretation of his own work.
Friedrich was elected a member of the Berlin Academy in 1810 following the purchase of two of his paintings by the Prussian Crown Prince. Yet in 1816, he sought to distance himself from Prussian authority, and that June applied for Saxon citizenship. The move was unexpected by his friends, as the Saxon government of the time was pro-French, while Friedrich's paintings to date were seen as generally patriotic and distinctly anti-French. Nevertheless, with the aid of his Dresden-based friend Graf Vitzthum von Eckstädt, Friedrich attained not only citizenship, but in 1818, a place in the Saxon Academy as a member with a yearly dividend of 150 thalers. Although he hoped to receive a full Professorship, it was never awarded him as, according to the German Library of Information, "it was felt that his painting was too personal, his point of view too individual to serve as a fruitful example to students." Politics too may have played a role in the stalling of his career: Friedrich's decidedly Germanic choice of subject and costuming frequently clashed with the prevailing pro-French attitudes of the time.
Marriage.
On 21 January 1818, Friedrich married Caroline Bommer, the twenty-five-year-old daughter of a dyer from Dresden. The couple had three children, with their first, Emma, arriving in 1820. Physiologist and painter Carl Gustav Carus notes in his biographical essays that marriage did not impact significantly on either Friedrich's life or personality, yet his canvasses from this period, including "Chalk Cliffs on Rügen"—painted after his honeymoon—display a new sense of levity, while his palette is brighter and less austere. Human figures appear with increasing frequency in the paintings of this period, which Siegel interprets as a reflection that "the importance of human life, particularly his family, now occupies his thoughts more and more, and his friends, his wife, and his townspeople appear as frequent subjects in his art."
Around this time, the artist found support from two sources in Russia. In 1820, Grand Duke Nikolai Pavlovich, at the behest of his wife Alexandra Feodorovna, visited Friedrich's studio and returned to Saint Petersburg with a number of his paintings. The exchange marked the beginning of a patronage that continued for many years. Not long thereafter, the poet Vasily Zhukovsky, tutor to Alexander II, met Friedrich in 1821 and found in him a kindred spirit. For decades Zhukovsky helped Friedrich both by purchasing his work himself and by recommending his art to the royal family; his assistance toward the end of Friedrich's career proved invaluable to the ailing and impoverished artist. Zhukovsky remarked that his friend's paintings "please us by their precision, each of them awakening a memory in our mind."
Friedrich was acquainted with Philipp Otto Runge (1777–1810), another leading German painter of the Romantic period. He was also a friend of Georg Friedrich Kersting (1785–1847), who painted him at work in his unadorned studio, and of the Norwegian painter Johan Christian Clausen Dahl (1788–1857). Dahl was close to Friedrich during the artist's final years, and he expressed dismay that to the art-buying public, Friedrich's pictures were only "curiosities". While the poet Zhukovsky appreciated Friedrich's psychological themes, Dahl praised the descriptive quality of Friedrich's landscapes, commenting that "artists and connoisseurs saw in Friedrich's art only a kind of mystic, because they themselves were only looking out for the mystic... They did not see Friedrich's faithful and conscientious study of nature in everything he represented".
During this period Friedrich frequently sketched memorial monuments and sculptures for mausoleums, reflecting his obsession with death and the afterlife; he even created designs for some of the funerary art in Dresden's cemeteries. Some of these works were lost in the fire that destroyed Munich's Glass Palace (1931) and later in the 1945 bombing of Dresden.
Later life and death.
Friedrich's reputation steadily declined over the final fifteen years of his life. As the ideals of early Romanticism passed from fashion, he came to be viewed as an eccentric and melancholy character, out of touch with the times. Gradually his patrons fell away. By 1820, he was living as a recluse and was described by friends as the "most solitary of the solitary". Towards the end of his life he lived in relative poverty and was increasingly dependent on the charity of friends. He became isolated and spent long periods of the day and night walking alone through woods and fields, often beginning his strolls before sunrise.
In June 1835, Friedrich suffered his first stroke, which left him with minor limb paralysis and greatly reduced his ability to paint. As a result, he was unable to work in oil; instead he was limited to watercolour, sepia and reworking older compositions.
Although his vision remained strong, he had lost the full strength of his hand. Yet he was able to produce a final 'black painting', "Seashore by Moonlight" (1835–36), described by Vaughan as the "darkest of all his shorelines, in which richness of tonality compensates for the lack of his former finesse".
Symbols of death appeared in his other work from this period. Soon after his stroke, the Russian royal family purchased a number of his earlier works, and the proceeds allowed him to travel to Teplitz—in today's Czech Republic—to recover.
During the mid-1830s, Friedrich began a series of portraits and he returned to observing himself in nature. As the art historian William Vaughan has observed, however, "He can see himself as a man greatly changed. He is no longer the upright, supportive figure that appeared in "Two Men Contemplating the Moon" in 1819. He is old and stiff... he moves with a stoop".
By 1838, he was capable only of working in a small format. He and his family were living in poverty and grew increasingly dependent for support on the charity of friends.
Friedrich died in Dresden on 7 May 1840, and was buried in Dresden's Trinitatis-Friedhof (Trinity Cemetery) east of the city centre (the entrance to which he had painted some 15 years earlier). The simple flat gravestone lies north-west of the central roundel within the main avenue.
By the time of his death, his reputation and fame were waning, and his passing was little noticed within the artistic community. His artwork had certainly been acknowledged during his lifetime, but not widely. While the close study of landscape and an emphasis on the spiritual elements of nature were commonplace in contemporary art, his work was too original and personal to be well understood. By 1838, his work no longer sold or received attention from critics; the Romantic movement had been moving away from the early idealism that the artist had helped found.
After his death, Carl Gustav Carus wrote a series of articles which paid tribute to Friedrich's transformation of the conventions of landscape painting. However, Carus' articles placed Friedrich firmly in his time, and did not place the artist within a continuing tradition. Only one of his paintings had been reproduced as a print, and that was produced in very few copies.
Themes.
Landscape and the sublime.
The visualisation and portrayal of landscape in an entirely new manner was Friedrich's key innovation. He sought not just to explore the blissful enjoyment of a beautiful view, as in the classic conception, but rather to examine an instant of sublimity, a reunion with the spiritual self through the contemplation of nature. Friedrich was instrumental in transforming landscape in art from a backdrop subordinated to human drama to a self-contained emotive subject. Friedrich's paintings commonly employed the "Rückenfigur"—a person seen from behind, contemplating the view. The viewer is encouraged to place himself in the position of the "Rückenfigur", by which means he experiences the sublime potential of nature, understanding that the scene is as perceived and idealised by a human. Friedrich created the notion of a landscape full of romantic feeling—"die romantische Stimmungslandschaft". His art details a wide range of geographical features, such as rock coasts, forests, and mountain scenes. He often used the landscape to express religious themes. During his time, most of the best-known paintings were viewed as expressions of a religious mysticism.
Friedrich said, "The artist should paint not only what he sees before him, but also what he sees within him. If, however, he sees nothing within him, then he should also refrain from painting that which he sees before him. Otherwise, his pictures will be like those folding screens behind which one expects to find only the sick or the dead." Expansive skies, storms, mist, forests, ruins and crosses bearing witness to the presence of God are frequent elements in Friedrich's landscapes. Though death finds symbolic expression in boats that move away from shore—a Charon-like motif—and in the poplar tree, it is referenced more directly in paintings like "The Abbey in the Oakwood" (1808–10), in which monks carry a coffin past an open grave, toward a cross, and through the portal of a church in ruins.
He was one of the first artists to portray winter landscapes in which the land is rendered as stark and dead. Friedrich's winter scenes are solemn and still—according to the art historian Hermann Beenken, Friedrich painted winter scenes in which "no man has yet set his foot. The theme of nearly all the older winter pictures had been less winter itself than life in winter. In the 16th and 17th centuries, it was thought impossible to leave out such motifs as the crowd of skaters, the wanderer... It was Friedrich who first felt the wholly detached and distinctive features of a natural life. Instead of many tones, he sought the one; and so, in his landscape, he subordinated the composite chord into one single basic note".
Bare oak trees and tree stumps, such as those in "Raven Tree" (c. 1822), "" (c. 1833), and "Willow Bush under a Setting Sun" (c. 1835), are recurring elements of Friedrich's paintings, symbolizing death. Countering the sense of despair are Friedrich's symbols for redemption: the cross and the clearing sky promise eternal life, and the slender moon suggests hope and the growing closeness of Christ. In his paintings of the sea, anchors often appear on the shore, also indicating a spiritual hope. German literature scholar Alice Kuzniar finds in Friedrich's painting a temporality—an evocation of the passage of time—that is rarely highlighted in the visual arts. For example, in "The Abbey in the Oakwood", the movement of the monks away from the open grave and toward the cross and the horizon imparts Friedrich's message that the final destination of man's life lies beyond the grave.
With dawn and dusk constituting prominent themes of his landscapes, Friedrich's own later years were characterized by a growing pessimism. His work becomes darker, revealing a fearsome monumentality. "The Wreck of the Hope"—also known as "The Polar Sea" or "The Sea of Ice" (1823–24)—perhaps best summarizes Friedrich's ideas and aims at this point, though in such a radical way that the painting was not well received. Completed in 1824, it depicted a grim subject, a shipwreck in the Arctic Ocean; "the image he produced, with its grinding slabs of travertine-colored floe ice chewing up a wooden ship, goes beyond documentary into allegory: the frail bark of human aspiration crushed by the world's immense and glacial indifference."
Friedrich's written commentary on aesthetics was limited to a collection of aphorisms set down in 1830, in which he explained the need for the artist to match natural observation with an introspective scrutiny of his own personality. His best-known remark advises the artist to "close your bodily eye so that you may see your picture first with the spiritual eye. Then bring to the light of day that which you have seen in the darkness so that it may react upon others from the outside inwards." He rejected the overreaching portrayals of nature in its "totality", as found in the work of contemporary painters like Adrian Ludwig Richter (1803–84) and Joseph Anton Koch (1768–1839).
Loneliness and death.
Both Friedrich's life and art have at times been perceived by some to have been marked with an overwhelming sense of loneliness. Art historians and some of his contemporaries attribute such interpretations to the losses suffered during his youth to the bleak outlook of his adulthood, while Friedrich's pale and withdrawn appearance helped reinforce the popular notion of the "taciturn man from the North".
Friedrich suffered depressive episodes in 1799, 1803–1805, c.1813, in 1816 and between 1824 and 1826. There are noticeable thematic shifts in the works he produced during these episodes, which see the emergence of such motifs and symbols as vultures, owls, graveyards and ruins. From 1826 these motifs became a permanent feature of his output, while his use of color became more dark and muted. Carus wrote in 1929 that Friedrich "is surrounded by a thick, gloomy cloud of spiritual uncertainty", though the noted art historian and curator Hubertus Gassner disagrees with such notions, seeing in Friedrich's work a positive and life-affirming subtext inspired by Freemasonry and religion.
Germanic folklore.
Reflecting Friedrich's patriotism and resentment during the 1813 French occupation of the dominion of Pomerania, motifs from German folklore became increasingly prominent in his work. An anti-French German nationalist, Friedrich used motifs from his native landscape to celebrate Germanic culture, customs and mythology. He was impressed by the anti-Napoleonic poetry of Ernst Moritz Arndt and Theodor Körner, and the patriotic literature of Adam Müller and Heinrich von Kleist. Moved by the deaths of three friends killed in battle against France, as well as by Kleist's 1808 drama "Die Hermannsschlacht", Friedrich undertook a number of paintings in which he intended to convey political symbols solely by means of the landscape—a first in the history of art.
In ' (1812), a dilapidated monument inscribed "Arminius" invokes the Germanic chieftain, a symbol of nationalism, while the four tombs of fallen heroes are slightly ajar, freeing their spirits for eternity. Two French soldiers appear as small figures before a cave, lower and deep in a grotto surrounded by rock, as if farther from heaven. A second political painting, ' (c. 1813), depicts a lost French soldier dwarfed by a dense forest, while on a tree stump a raven is perched—a prophet of doom, symbolizing the anticipated defeat of France.
Legacy.
Influence.
Alongside other Romantic painters, Friedrich helped position landscape painting as a major genre within Western art. Of his contemporaries, Friedrich's style most influenced the painting of Johan Christian Dahl (1788–1857). Among later generations, Arnold Böcklin (1827–1901) was strongly influenced by his work, and the substantial presence of Friedrich's works in Russian collections influenced many Russian painters, in particular Arkhip Kuindzhi (c. 1842–1910) and Ivan Shishkin (1832–98). Friedrich's spirituality anticipated American painters such as Albert Pinkham Ryder (1847–1917), Ralph Blakelock (1847–1919), the painters of the Hudson River School and the New England Luminists.
At the turn of the 20th century, Friedrich was rediscovered by the Norwegian art historian Andreas Aubert (1851–1913), whose writing initiated modern Friedrich scholarship, and by the Symbolist painters, who valued his visionary and allegorical landscapes. The Norwegian Symbolist Edvard Munch (1863–1944) would have seen Friedrich's work during a visit to Berlin in the 1880s. Munch's 1899 print "The Lonely Ones" echoes Friedrich's "Rückenfigur (back figure)", although in Munch's work the focus has shifted away from the broad landscape and toward the sense of dislocation between the two melancholy figures in the foreground.
Friedrich's landscapes exercised a strong influence on the work of German artist Max Ernst (1891–1976), and as a result other Surrealists came to view Friedrich as a precursor to their movement. In 1934, the Belgian painter René Magritte (1898–1967) paid tribute in his work "The Human Condition", which directly echoes motifs from Friedrich's art in its questioning of perception and the role of the viewer. A few years later, the Surrealist journal "Minotaure" featured Friedrich in a 1939 article by critic Marie Landsberger, thereby exposing his work to a far wider circle of artists. The influence of "The Wreck of Hope" (or "The Sea of Ice") is evident in the 1940–41 painting "Totes Meer" by Paul Nash (1889–1946), a fervent admirer of Ernst. Friedrich's work has been cited as an inspiration by other major 20th-century artists, including Mark Rothko (1903–70), Gerhard Richter (b. 1932), Gotthard Graubner and Anselm Kiefer (b. 1945). Friedrich's Romantic paintings have also been singled out by writer Samuel Beckett (1906–89), who, standing before "Man and Woman Contemplating the Moon", said "This was the source of "Waiting for Godot", you know."
In his 1961 article "The Abstract Sublime", originally published in ARTnews, the art historian Robert Rosenblum drew comparisons between the Romantic landscape paintings of both Friedrich and Turner with the Abstract Expressionist paintings of Mark Rothko. Rosenblum specifically describes Friedrich's 1809 painting "The Monk by the Sea", Turner's "The Evening Star" and Rothko's 1954 "Light, Earth and Blue" as revealing affinities of vision and feeling. According to Rosenblum, "Rothko, like Friedrich and Turner, places us on the threshold of those shapeless infinities discussed by the aestheticians of the Sublime. The tiny monk in the Friedrich and the fisher in the Turner establish a poignant contrast between the infinite vastness of a pantheistic God and the infinite smallness of His creatures. In the abstract language of Rothko, such literal detail—a bridge of empathy between the real spectator and the presentation of a transcendental landscape—is no longer necessary; we ourselves are the monk before the sea, standing silently and contemplatively before these huge and soundless pictures as if we were looking at a sunset or a moonlit night."
Critical opinion.
Until 1890, and especially after his friends had died, Friedrich's work lay in near-oblivion for decades. Yet, by 1890, the symbolism in his work began to ring true with the artistic mood of the day, especially in central Europe. However, despite a renewed interest and an acknowledgment of his originality, his lack of regard for "painterly effect" and thinly rendered surfaces jarred with the theories of the time.
During the 1930s, Friedrich's work was used in the promotion of Nazi ideology, which attempted to fit the Romantic artist within the nationalistic "Blut und Boden". It took decades for Friedrich's reputation to recover from this association with Nazism. His reliance on symbolism and the fact that his work fell outside the narrow definitions of modernism contributed to his fall from favour. In 1949, art historian Kenneth Clark wrote that Friedrich "worked in the frigid technique of his time, which could hardly inspire a school of modern painting", and suggested that the artist was trying to express in painting what is best left to poetry. Clark's dismissal of Friedrich reflected the damage the artist's reputation sustained during the late 1930s.
Friedrich's reputation suffered further damage when his imagery was adopted by a number of Hollywood directors, such as Walt Disney, built on the work of such German cinema masters as Fritz Lang and F. W. Murnau, within the horror and fantasy genres. His rehabilitation was slow, but enhanced through the writings of such critics and scholars as Werner Hofmann, Helmut Börsch-Supan and Sigrid Hinz, who successfully rejected and rebutted the political associations ascribed to his work, and placed it within a purely art-historical context. By the 1970s, he was again being exhibited in major galleries across the world, as he found favour with a new generation of critics and art historians.
Today, his international reputation is well established. He is a national icon in his native Germany, and highly regarded by art historians and art connoisseurs across the Western World. He is generally viewed as a figure of great psychological complexity, and according to Vaughan, "a believer who struggled with doubt, a celebrator of beauty haunted by darkness. In the end, he transcends interpretation, reaching across cultures through the compelling appeal of his imagery. He has truly emerged as a butterfly—hopefully one that will never again disappear from our sight".
Work.
Friedrich was a prolific artist who produced more than 500 attributed works. In line with the Romantic ideals of his time, he intended his paintings to function as pure aesthetic statements, so he was cautious that the titles given to his work were not overly descriptive or evocative. It is likely that some of today's more literal titles, such as "The Stages of Life", were not given by the artist himself, but were instead adopted during one of the revivals of interest in Friedrich. Complications arise when dating Friedrich's work, in part because he often did not directly name or date his canvases. He kept a carefully detailed notebook on his output, however, which has been used by scholars to tie paintings to their completion dates.

</doc>
<doc id="5655" url="https://en.wikipedia.org/wiki?curid=5655" title="Courtney Love">
Courtney Love

Courtney Michelle Love (born Courtney Michelle Harrison, July 9, 1964) is an American musician, actress, and visual artist. Prolific in the punk and grunge scenes of the 1990s, Love became a fixture in alternative music as the frontwoman of Hole, drawing public attention for her uninhibited stage presence and confrontational lyrics, as well as her highly publicized personal life following her marriage to Kurt Cobain.
The daughter of psychotherapist Linda Carroll and Hank Harrison, Love had a nomadic early life; she mainly grew up in Oregon and California, where she was in a series of short-lived bands before landing roles in films by British director Alex Cox. After forming Hole in 1989, she received substantial attention from underground rock press for the group's debut album, produced by Kim Gordon, while the group's second release, "Live Through This" (1994), lent her a more high-profile renown, receiving critical accolades and going multi-platinum. In 1995, she returned to acting, earning a Golden Globe Award nomination for her performance in Miloš Forman's "The People vs. Larry Flynt" (1996). Shortly after, Hole's third release, "Celebrity Skin" (1998), earned Love recognition as a mainstream musician and was nominated for multiple Grammy Awards.
Love continued to work as an actress, appearing in big-budget pictures such as "Man on the Moon" (1999) and "Trapped" (2002), and released her first solo album, "America's Sweetheart", in 2004. In 2010, she released "Nobody's Daughter" as Hole, with a reformed band. Love debuted a new solo single in early 2014, and also saw a return to acting in multiple TV series, including "Empire".
Early life.
Love was born Courtney Michelle Harrison on July 9, 1964, in San Francisco. Her mother, Linda Carroll (née Risi), was employed at the UC San Francisco Hospital, and her father, Hank Harrison, was a publisher and road manager for the Grateful Dead. Love's godfather is the founding Grateful Dead bassist Phil Lesh. Love's mother, who was adopted as a child, was later revealed to be the biological daughter of novelist Paula Fox. Love's great-grandmother was screenwriter Elsie Fox, and Love learned in 2002 that, through marriage, her great uncle was actor Douglas Fairbanks. Love is of Cuban, Welsh, Irish, German, and English descent.
Love spent her early years in the Haight-Ashbury district of San Francisco until her parents' 1969 divorce, after which her father's custody was withdrawn when her mother alleged that he had fed LSD to Love as a toddler. Love described her parents' household as being full of "hairy, wangly-ass hippies running around naked." According to sources, Love's mother, who was studying to be a psychologist, had Love in therapy by the age of two. In 1970, her mother moved the family to the rural community of Marcola, Oregon where the family lived on a commune along the Mohawk River, while her mother completed her degree at the University of Oregon. Love was legally adopted by her then-stepfather, Frank Rodriguez, with whom her mother had Love's two half-sisters, Jaimee and Nicole, and adopted a brother Joshua, at three years old, from an African American family; another half-brother died in infancy of a heart defect when Love was ten. Love attended elementary school in Eugene, where she struggled academically and had trouble making friends. At age nine, she was diagnosed with mild autism.
In 1972, Love's mother divorced Rodriguez, remarried, and moved the family to New Zealand; there, she enrolled Love at Nelson College for Girls, from which Love was eventually expelled. Love's mother then sent her back to the United States, where she was raised in Portland, Oregon by her former stepfather and other family friends; during this time, her mother gave birth to two of Love's other half-brothers, Tobias and Daniel. At age fourteen, Love was arrested for shoplifting a T-shirt and was sent to Hillcrest Correctional Facility. She spent the following several years in foster care before becoming legally emancipated at age sixteen. She supported herself by working illegally as a stripper, adopting the last name "Love" to conceal her identity, which she came to use thereafter. She also worked various odd jobs, including picking berries at a farm in Troutdale, Oregon, and as a disc jockey. During this time, she enrolled at Portland State University, studying English and philosophy. Love has said that she "didn't have a lot of social skills," and that she learned them while frequenting gay clubs in Portland.
In 1981, Love was granted a small trust fund that had been left by her adoptive grandparents, which she used to travel to Dublin, Ireland, where her biological father was living. While there, she audited courses at Trinity College, studying theology for two semesters. She would later receive honorary patronage from Trinity's University Philosophical Society in 2010. In the United Kingdom, she became acquainted with musician Julian Cope and his band, The Teardrop Explodes, in Liverpool and briefly lived in his house. "They kind of took me in," she recalled. "I was sort of a mascot; I would get them coffee or tea during rehearsal." In Cope's autobiography, "Head-On", Love is referred to as "the adolescent." After leaving the United Kingdom, Love returned to the United States, and later worked as an erotic dancer in Taiwan and Japan.
Career.
1981–1987: Early projects.
Love initially began several music projects in the 1980s, first forming Sugar Babylon (later Sugar Babydoll) in Portland with friends Ursula Wehr and Robin Barbur. In 1982, Love attended a Faith No More concert in San Francisco, and convinced the members to let her join as a singer. The group recorded material with Love as a vocalist, but, according to the band's keyboardist Roddy Bottum, wanted a "male energy," and Love was subsequently kicked out of the band; she and Bottum, however, maintained a friendship in the years after. Love later formed the Pagan Babies with friend Kat Bjelland, whom she met at the Satyricon club in Portland in 1984: "The best thing that ever happened to me in a way, was Kat," Love said. Love asked Bjelland to start a band with her as a guitarist, and the two moved to San Francisco in June 1985, where they recruited Love's friend, bassist Jennifer Finch, and drummer Janis Tanaka. According to Bjelland, " didn't play an instrument at the time" aside from keyboards, so Bjelland would transpose Love's musical ideas on guitar for her. The group played several house shows and recorded one 4-track demo before disbanding in late 1985. Following Pagan Babies, Love moved to Minneapolis where Bjelland had formed the group Babes in Toyland, and briefly worked as a concert promoter before returning to California.
Deciding to shift her focus to acting, Love enrolled at the San Francisco Art Institute, where she studied film with George Kuchar and was featured in one of his short films, titled "Club Vatican". In 1985, she submitted an audition tape for the role of Nancy Spungen in the Sid Vicious biopic "Sid and Nancy" (1986), and was given a minor supporting role by director Alex Cox. After filming "Sid and Nancy" in New York City, Love worked at a peep show in Times Square and squatted at the ABC No Rio social center and Pyramid Club in the East Village. The same year, Cox cast her in a leading role in his following film, "Straight to Hell" (1987), a spaghetti western starring Joe Strummer and Grace Jones, which was filmed in Spain in 1986. The film caught the attention of Andy Warhol, who featured Love in an episode of "Andy Warhol's Fifteen Minutes" with Robbie Nevil in a segment titled "C'est la Vie." She also had a part in the 1988 Ramones music video for "I Wanna Be Sedated", appearing as a bride among dozens of party guests.
In 1988, Love aborted her acting career and left New York, returning to the west coast, citing the "celebutante" fame she'd attained as the central reason. "I hated it," she recalled. "It was misery itself." She returned to stripping in the small town of McMinnville, Oregon, where she was recognized by customers at the bar; this prompted Love to go into isolation, and she relocated to Anchorage, Alaska: "I decided to move to Alaska because I needed to get my shit together and learn how to work," Love said in retrospect. "So I went on this sort of vision quest. I got rid of all my earthly possessions. I had my bad little strip clothes and some big sweaters, and I moved into a trailer with a bunch of other strippers."
1988–1991: Beginnings of Hole.
At the end of 1988, Love taught herself to play guitar and relocated to Los Angeles, where she placed an ad in a local music zine, reading: "I want to start a band. My influences are Big Black, Sonic Youth, and Fleetwood Mac." Love recruited lead guitarist Eric Erlandson; Lisa Roberts, her neighbor, as bassist; and drummer Caroline Rue, whom she met at a Gwar concert. Love named the band Hole after a line from Euripedes' "Medea", as well as a conversation she had had with her mother, in which she told her that she "couldn't walk around with a hole inside herself."
Love continued to work at strip clubs in the band's formative stages, saving money to purchase a backline and a touring van, and rehearsed at a studio in Hollywood that was loaned to her by the Red Hot Chili Peppers. Hole played their first show in November 1989 at Raji's, a rock club in central Hollywood. The band's debut single, "Retard Girl", was issued in April 1990 through the Long Beach indie label Sympathy for the Record Industry, and was given air-time by Rodney Bingenheimer's local station, KROQ. That fall, the band appeared on the cover of "Flipside", a Los Angeles-based punk fanzine. In the spring of 1991, the band released their second single, "Dicknail", through Sub Pop Records.
With no wave, noise rock and grindcore bands being major influences on Love, Hole's first studio album, "Pretty on the Inside", captured a particularly abrasive sound and contained disturbing lyrics, described by "Q" magazine as "confrontational genuinely uninhibited." The record was released in September 1991 on Caroline Records, produced by Kim Gordon of Sonic Youth, with assistant production from Gumball's Don Fleming. Though Love would later say it was "unlistenable" and "[unmelodic," the album received generally positive critical reception from indie and punk rock critics and was labeled one of the twenty best albums of the year by "Spin" magazine. It also gained a following in the United Kingdom, charting at 59 on the UK Albums Chart, and its lead single, "Teenage Whore", entered the country's indie chart at number one. The underlying feminist slant of the album's songs led many to mistakenly tag the band as being part of the riot grrl movement, a movement that Love did not associate with. In support of the record, the band toured in Europe headlining with Mudhoney, and opening in the United States for The Smashing Pumpkins. They also performed at the Whisky a Go Go opening for Sonic Youth, and at CBGB in New York City. Love designed and distributed flyers promoting the shows, which included cutouts of women and young girls, as well as scattered lyrics and quotes from poems.
1992–1995: Breakthrough.
After the release of "Pretty on the Inside", Love began dating Kurt Cobain and became pregnant. During Love's pregnancy, Hole recorded a cover of "Over the Edge" for a Wipers tribute album, and recorded their fourth single, "Beautiful Son," which was released in April 1993. Love and Cobain married in February 1992, and after the birth of the couple's daughter, Frances Bean Cobain, relocated to Carnation, Washington, and then to Seattle. On September 8, 1993, Love and Cobain made their only public performance together at the Rock Against Rape benefit in Hollywood, performing two duets, both acoustic versions, of "Pennyroyal Tea" and "Where Did You Sleep Last Night." Love also performed electric versions of two new Hole songs, "Doll Parts" and "Miss World", both of which were written for the band's upcoming second release. 
In October 1993, Hole recorded their second album, titled "Live Through This", in Atlanta, Georgia. The album featured a new lineup, with bassist Kristen Pfaff and drummer Patty Schemel. "Live Through This" was released on Geffen's subsidiary DGC label in April 1994, four days after Love's husband, Cobain, was found dead of a self-inflicted shotgun wound in their home. Two months later, in June 1994, bassist Kristen Pfaff died of a heroin overdose, and Love recruited Melissa Auf der Maur for the band's impending tour. Throughout the months preceding the tour, Love was rarely seen in public, spending time at her Seattle home, or visiting the Namgyal Buddhist Monastery in New York.
"Live Through This" was a commercial and critical success, hitting platinum sales in April 1995 and receiving numerous critical accolades. The success of the record combined with Cobain's suicide resulted in a high level of publicity for Love, and she was featured on Barbara Walters' "10 Most Fascinating People" in 1995. At Hole's performance on August 26, 1994 at the Reading Festival— Love's first public performance following her husband's death— she appeared onstage, tear-drenched, with outstretched arms, mimicking crucifixion. John Peel wrote in "The Guardian" that Love's disheveled appearance "would have drawn whistles of astonishment in Bedlam," and that her performance "verged on the heroic ... Love steered her band through a set which dared you to pity either her recent history or that of the band ... the band teetered on the edge of chaos, generating a tension which I cannot remember having felt before from any stage." The band performed a series of riotous concerts during the tour, with Love frequently appearing hysterical onstage, flashing crowds, stage diving, and getting into fights with audience members. In retrospect, Love said she "couldn't remember much" of the shows as she was using drugs heavily at the time.
In February 1995, Hole performed a well-reviewed acoustic set on "MTV Unplugged" at the Brooklyn Academy of Music, and continued to tour late into the year, concluding their world tour with an appearance at the 1995 "MTV Video Music Awards", where they were nominated for Best Alternative Video for "Doll Parts".
1996–2000: Acting and mainstream success.
After Hole's world tour concluded in 1996, Love made a return to acting, first in small roles in "Basquiat" and "Feeling Minnesota" (1996), before landing the co-starring role of Larry Flynt's wife, Althea, in Miloš Forman's critically acclaimed 1996 film "The People vs. Larry Flynt". Despite Columbia Pictures' reluctance to hire Love due to her troubled past, she received critical acclaim for her performance in the film after its release in December 1996, earning a Golden Globe nomination for Best Actress, and a New York Film Critics Circle Award for Best Supporting Actress. Roger Ebert called her work in the film "quite a performance; Love proves she is not a rock star pretending to act, but a true actress." She won several other awards from various film critic associations for the film, and consequently adopted a more polished public image; during this time, she also became involved in fashion and modeled for Versace advertisements.
In late 1997, Hole released a compilation album, "My Body, the Hand Grenade", as well as an EP titled "The First Session" which consisted of the band's earliest recordings. In September 1998, Hole released their third studio album, "Celebrity Skin", which marked something of a transformation for Love, featuring a stark power pop sound as opposed to the group's earlier punk rock influences. Love divulged her ambition of making an album where "art meets commerce ... there are no compromises made, it has commercial appeal, and it sticks to original vision." She said she was influenced by Neil Young, Fleetwood Mac, and My Bloody Valentine when writing the album. "Celebrity Skin" was well received by critics; "Rolling Stone" called the album "accessible, fiery and intimate—often at the same time ... a basic guitar record that's anything but basic." "Celebrity Skin" went on to go multi-platinum, and topped "Best of Year" lists at "Spin" and the "Village Voice". The album garnered the band their only No. 1 hit single on the Modern Rock Tracks chart with the title track "Celebrity Skin." The band promoted the album through MTV performances and at the 1998 Billboard Music Awards. Hole toured with Marilyn Manson on the Beautiful Monsters Tour in 1999, but dropped out of the tour nine dates in after a dispute over production costs between Love and Manson; Hole resumed touring with Imperial Teen.
Before the release and promotion of "Celebrity Skin", Love and Fender designed a low-priced Squier brand guitar, called Vista Venus. The instrument featured a shape inspired by Mercury, Stratocaster, and Rickenbacker's solidbodies and had a single-coil and a humbucker pickup, and was available in 6-string and 12-string versions. In an early 1999 interview, Love said about the Venus: "I wanted a guitar that sounded really warm and pop, but which required just one box to go dirty ... And something that could also be your first band guitar. I didn't want it all teched out. I wanted it real simple, with just one pickup switch." In 1999, Love was awarded an Orville H. Gibson award for Best Female Rock Guitarist. During this time, she also landed a role opposite Jim Carrey in the Andy Kaufman biopic "Man on the Moon" (1999), which was followed with a role as William S. Burroughs's wife Joan Vollmer in "Beat" (2000) alongside Kiefer Sutherland.
After touring for "Celebrity Skin" finished, Auf der Maur left the band to tour with the Smashing Pumpkins; Hole's touring drummer Samantha Maloney left soon after. Love and Erlandson released the single "Be A Man"—an outtake from the "Celebrity Skin" sessions—for the soundtrack of the Oliver Stone film "Any Given Sunday" (1999). The group became dormant in the following two years, and Love starred in several more films, including in "Julie Johnson" (2001) as Lili Taylor's lesbian lover, for which she won an Outstanding Actress award at L.A.'s Outfest, and in the thriller "Trapped" (2002), alongside Kevin Bacon and Charlize Theron. In May 2002, Hole officially announced their breakup amid continuing litigation with Universal Music Group over their record contract.
2001–2011: Solo work, Hole revival.
With Hole in disarray, Love began a "punk rock femme supergroup" called Bastard during autumn 2001, enlisting Schemel, Veruca Salt co-frontwoman Louise Post, and bassist Gina Crosley. Though a demo was completed, the project never reached fruition.
In 2002, Love began composing an album with Linda Perry, titled "America's Sweetheart", also reuniting with drummer Patty Schemel. Love signed with Virgin Records to release it, and initially recorded it in France, but was forced by the label to re-record the entire album in the summer of 2003. "America's Sweetheart" was released in February 2004, and received mixed reviews from critics. Charles Aaron of "Spin" called it a "jaw-dropping act of artistic will and a fiery, proper follow-up to 1994's "Live Through This"" and awarded it eight out of ten stars, while "The Village Voice" said: "is willing to act out the dream of every teenage brat who ever wanted to have a glamorous, high-profile hissyfit, and she turns those egocentric nervous breakdowns into art. Sure, the art becomes less compelling when you've been pulling the same stunts for a decade. But, honestly, is there anybody out there who fucks up better?" The album sold less than 100,000 copies. Love has publicly expressed her regret over the record several times, calling it "a crap record" and reasoning that her drug issues at the time were to blame. Shortly after the record was released, Love told Kurt Loder on TRL: "I cannot exist as a solo artist. It's a joke." Love also collaborated on a manga comic titled "Princess Ai", illustrated by Misaho Kujiradou and Ai Yazawa, which was released in July 2004.
In 2006, Love released a memoir, titled "", and started recording what was going to be her second solo album, "How Dirty Girls Get Clean", collaborating again with Perry and Billy Corgan in the writing and recording. Love had written several songs, including an anti-cocaine song titled "Loser Dust", during her time in rehab in 2005. She told "Billboard": "My hand-eye coordination was so bad the drug use, I didn't even know chords anymore. It was like my fingers were frozen. And I wasn't allowed to make noise rehab ... I never thought I would work again." Some tracks and demos from the album (initially planned for release in 2008) were leaked on the internet in 2006, and a documentary entitled "The Return of Courtney Love", detailing the making of the album, aired on the British television network in the fall of that year. A rough acoustic version of "Never Go Hungry Again", recorded during an interview for "The Times" in November, was also released. Incomplete audio clips of the song "Samantha", originating from an interview with NPR, were also distributed on the internet in 2007.
On June 17, 2009, "NME" reported that Hole would be reuniting. Former Hole guitarist Erlandson stated in "Spin" magazine that contractually no reunion could take place without his involvement; therefore "Nobody's Daughter" would remain Love's solo record, as opposed to a "Hole" record. Love responded to Erlandson's comments in a Twitter post, claiming "he's out of his mind, Hole is my band, my name, and my Trademark". "Nobody's Daughter" was released worldwide as a Hole album on April 27, 2010. For the new line-up, Love recruited guitarist Micko Larkin, Shawn Dailey (bass guitar), and Stu Fisher (drums, percussion). "Nobody's Daughter" featured a great deal of material written and recorded for Love's aborted solo album, "How Dirty Girls Get Clean", including "Pacific Coast Highway", "Letter to God", "Samantha", and "Never Go Hungry", although they were re-produced with Larkin. The first single from "Nobody's Daughter" was "Skinny Little Bitch", which became a hit on alternative rock radio in early March 2010.
The album received mixed reviews. Robert Sheffield of "Rolling Stone" gave the album three out of five stars, saying that Love "worked hard on these songs, instead of just babbling a bunch of druggy bullshit and assuming people would buy it, the way she did on her 2004 flop, "America's Sweetheart"." Sal Cinquemani of "Slant Magazine" also gave the album three out of five stars, saying "It's Marianne Faithfull's substance-ravaged voice that comes to mind most often while listening to songs like "Honey" and "For Once in Your Life." The latter track is, in fact, one of Love's most raw and vulnerable vocal performances to date ... the song offers a rare glimpse into the mind of a woman who, for the last 15 years, has been as famous for being a rock star as she's been for being a victim." The album's subject matter was largely centered on Love's tumultuous life between 2003 and 2007, and featured a polished folk-rock sound with significantly more acoustic work than previous Hole albums. Love and the band toured internationally from 2010 into late 2012 promoting the record, after which she dropped the Hole name and returned to a solo career.
2012–present: Career expansion.
In May 2012, Love debuted an art collection at Fred Torres Collaborations in New York titled ""And She's Not Even Pretty"", which contained over forty drawings and paintings by Love composed in ink, colored pencil, pastels, and watercolors. She then collaborated with Michael Stipe on the track "Rio Grande" for Johnny Depp's sea shanty album "" and also contributed guest vocals and co-wrote a track on Fall Out Boy's album, "Save Rock and Roll" (2013).
After solo performances in December 2012 and January 2013, Love appeared in advertisements for Yves Saint Laurent alongside Kim Gordon and Ariel Pink. Love completed a solo tour of North America in the summer of 2013, which had initially been conceived to promote Love's new album; however, due to the impending release of new material, it was dubbed a "greatest hits" tour. Love told "Billboard" at the time that she had recorded eight songs in the studio. "songs are not my usual (style)," Love said. "I don't have any Fleetwood Mac references on it. Usually I always have a Fleetwood Mac reference as well as having, like, Big Black references. These are very unique songs that sort of magically happened."
On April 22, 2014, Love debuted the song "You Know My Name" on BBC Radio 6 to promote her tour of the United Kingdom. It was released as a double A-side single with the song "Wedding Day" on May 4, 2014 on her own label Cherry Forever Records via Kobalt Label Services. The tracks were produced by Michael Beinhorn, and feature Tommy Lee on drums. In an interview with BBC, Love revealed that she and former Hole guitarist Eric Erlandson had reconciled, and had been rehearsing new material together, along with former bassist Melissa Auf der Maur and drummer Patty Schemel, though did not confirm a reunion of the band. On May 1, 2014, in an interview with "Pitchfork", Love commented further on the possibility of Hole reuniting, saying:
"I'm not going to commit to it happening, because we want an element of surprise. There's a lot of i's to be dotted and t's to be crossed."
In July 2014, Love landed a minor role on the final season of the FX series "Sons of Anarchy", which began filming in the summer, and also appeared in a recurring role on the series "Revenge". Love also landed a leading role in "Kansas City Choir Boy", a collaborative "pop opera" which showed at the Manhattan arts center Here during their annual "Prototype" festival in January 2015. She was cast in Lee Daniels' network series "Empire" in a recurring guest role as Elle Dallas. The track "Walk Out on Me" featuring Love was included on the "" album, which debuted at number 1 on the Billboard 200. Love's contribution to the album was critically acclaimed. "The Guardian" praised the track, saying: "The idea of Courtney Love singing a ballad with a group of gospel singers seems faintly terrifying... the reality is brilliant. Love’s voice fits the careworn lyrics, effortlessly summoning the kind of ravaged darkness that Lana Del Rey nearly ruptures herself trying to conjure up."
In the spring of 2015, Love joined Lana Del Rey on her Endless Summer Tour with eight show performances. During her tenure on Del Rey's tour, Love debuted a new single, "Miss Narcissist," released on Wavves' independent label Ghost Ramp. She also was cast in a supporting role in James Franco's film "The Long Home", based on William Gay's novel of the same name, marking her first film role in over ten years. In January 2016, Love released a clothing line in collaboration with Sophia Amoruso titled "Love, Courtney," featuring eighteen pieces reflecting Love's style over the course of her career.
Musical style.
Songwriting and lyrics.
"Spin"s October 1991 review of Hole's first album noted Love's layering of harsh and abrasive riffs buried more sophisticated musical arrangements. In 1998, Love stated that Hole had "always been a pop band. We always had a subtext of pop. I always talked about it, if you go back ... what'll sound like some weird Sonic Youth tuning back then to you was sounding like the Raspberries to me, in my demented pop framework".
Love writes from a female's point of view, and her lyrics have been described as "literate and mordant" and noted by scholars for "articulating a third-wave feminist consciousness." She has repeatedly stated that lyrics are the most important component of songwriting for her, saying: "I want it to look just as good on the page as it would if it was in a poetry book". A great deal of her songwriting has been diaristic in nature. Her later work was more lyrically introspective. "Celebrity Skin" and "America's Sweetheart" deal with celebrity life, Hollywood, and drug addiction, while continuing Love's interest in vanity and body image. "Nobody's Daughter" was lyrically reflective of Love's past relationships and her struggle to sobriety, with the majority of its lyrics written while she was in rehab in 2006. Poetry has often been a major influence on her writing; Love said she had "always wanted to be a poet, but there was no money in it." She has named the works of T.S. Eliot and Charles Baudelaire as influential.
Voice and instruments.
Love possesses a contralto vocal range, and her vocal style has been described as "raw and distinctive." She has referred to herself as "a shit guitar player", further commenting: "I can write a song, but I have trouble with the execution of it". According to Love, she never wanted to be a singer, but rather aspired to be a skilled guitarist: "I'm such a lazy bastard though that I never did that", Love said. "I was always the only person with the nerve to sing, and so I got stuck with it". She has been regularly noted by critics for her husky vocals as well as her "banshee[-like]" screaming abilities. Her vocals have been compared to those of Johnny Rotten, and David Fricke of "Rolling Stone" described them as "lung-busting" and "a corrosive, lunatic wail". Upon the release of Hole's 2010 album, "Nobody's Daughter", Amanda Petrusich of "Pitchfork" compared Love's raspy, unpolished vocals to those of Bob Dylan.
Love has often played Fender guitars throughout her career, including a Jaguar and a vintage 1965 Jazzmaster, the latter of which was purchased by the Hard Rock Cafe and is on display in New York City. Love is seen playing her Jazzmaster in the music video for "Miss World." Earlier in Hole's career, between 1989 and 1991, Love primarily played a Rickenbacker 425 because she "preferred the 3/4 neck", but she destroyed the guitar onstage at a 1991 concert opening for The Smashing Pumpkins. She also often played a guitar made by Mercury, an obscure company that manufactured custom guitars, which she purchased in 1992. Fender's Vista Venus, designed by Love in 1998, was partially inspired by Rickenbacker guitars as well as her Mercury. Love's setup has included Fender tube gear, Matchless, Ampeg, Silvertone and a solid-state 1976 Randall Commander. During her 2010 and more recent tours, Love has played a Rickenbacker 360 onstage.
Influences.
Love has been candid about her diverse musical influences, the earliest being Patti Smith and the Pretenders, whom she discovered while in juvenile hall. As a teenager, she named Flipper, Kate Bush, Soft Cell, Lou Reed, and Dead Kennedys among her favorite artists, as well as several new wave and post-punk bands, such as Echo and the Bunnymen, The Smiths, The Teardrop Explodes, Bauhaus, and Joy Division. While in Ireland at age fifteen, she saw The Virgin Prunes perform live in Dublin, and said the experience "framed her career." Her varying genre interests were illustrated in a 1991 interview with "Flipside", in which she stated: "There's a part of me that wants to have a grindcore band and another that wants to have a Raspberries-type pop band". Love also embraced the influence of experimental artists and punk rock groups, including Sonic Youth, Swans, Big Black, Diamanda Galás, the Germs, and The Stooges. While writing "Celebrity Skin", Love was mainly influenced by Neil Young and My Bloody Valentine. She also cited her contemporary PJ Harvey as an influence, saying, "The one rock star that makes me know I'm shit is Polly Harvey. I'm nothing next to the purity that she experiences."
Personal life.
Relationships and marriages.
She was briefly married to James Moreland (vocalist of The Leaving Trains) in 1989 for several months, but has said that Moreland was a transvestite and that their marriage was "a joke", ending in an annulment filed by Love. After forming Hole in 1989, Love and bandmate Eric Erlandson had a romantic relationship for over a year, and she also briefly dated Billy Corgan in 1991, with whom she has maintained a volatile friendship over the years.
Her most documented romantic relationship was with Kurt Cobain. It is uncertain when they first met; according to Love, she first met Cobain at a Dharma Bums show in Portland where she was doing a spoken word performance. According to Michael Azerrad, the two met at the Satyricon nightclub in Portland in 1989, though Cobain biographer Charles Cross stated the date was actually February 12, 1990, and that Cobain playfully wrestled Love to the floor after she commented to him in passing that he looked like Dave Pirner of Soul Asylum. Love's bandmate Eric Erlandson stated that both he and Love were formally introduced to Cobain in a parking lot after a Butthole Surfers concert at the Hollywood Palladium in 1991. The two later became reacquainted through Jennifer Finch, one of Love's longtime friends and former bandmates.
Love and Cobain began dating in the fall of 1991, and were married on Waikiki Beach in Honolulu, Hawaii, on February 24, 1992. Love wore a satin and lace dress once owned by actress Frances Farmer, and Cobain wore green pajamas. Six months later, on August 18, the couple's only child, a daughter, Frances Bean Cobain, was born. In April 1994, Cobain committed suicide in their Seattle home while Love was in rehab in Los Angeles. During their marriage, and after Cobain's death, Love became something of a hate-figure among some of Cobain's fans. In reflecting on their relationship, Love said: "I think that it looked like it was headed for doom, but it didn't feel like it was headed for doom on a daily basis. We went mountain biking; we would go camping. We were damn normal." After his cremation, Love divided portions of Cobain's ashes, some of which she kept in a teddy bear and in an urn. Another portion of his ashes was taken by Love to the Namgyal Buddhist Monastery in Ithaca, New York in 1994, where they were ceremonially blessed by Buddhist monks and mixed into clay which was made into memorial sculptures.
Between 1996 and 1999, Love dated her "The People vs. Larry Flynt" co-star Edward Norton, and was also linked to comedian Steve Coogan in the early 2000s.
Health.
Love has struggled with substance abuse problems throughout her life. She took numerous opiates in her early adult years, and tried cocaine at age 19. She became addicted to heroin in the early 1990s, and was infamously thrust into the media spotlight in 1992 when "Vanity Fair" published an article by journalist Lynn Hirschberg which stated that Love was addicted to heroin during her pregnancy; this resulted in the custody of Love and Cobain's newborn daughter, Frances, being temporarily withdrawn in a Los Angeles County court and placed with Love's sister. Love claimed she was misquoted in the piece, and asserted that she had immediately quit using the drug during her first trimester after she discovered she was pregnant. Love quit using heroin in 1996 at the insistence of director Miloš Forman when she landed a leading role in "The People vs. Larry Flynt". Love was ordered to take multiple urine tests under the supervision of Columbia Pictures while filming the movie, and passed all of them. On July 9, 2004, Love's 40th birthday, she attempted to commit suicide at her Manhattan apartment, and was taken to Bellevue Hospital, allegedly incoherent, and put on a 72-hour watch. According to police, she was believed to be a potential "danger to herself," but was deemed mentally sound and released to a rehab facility two days later. In 2005 and 2006, after making several public appearances clearly intoxicated (namely on the "Late Show with David Letterman" and the "Comedy Central Roast" of Pamela Anderson) and suffering drug-related arrests and probation violations, Love was sentenced to six months in lockdown rehab due to struggles with prescription drugs and cocaine. She has stated she has been sober since 2007, and in May 2011, confirmed her sobriety.
Legal troubles.
Love's legal troubles have been widely publicized throughout her career. On July 4, 1995, at the Lollapalooza Festival in George, Washington, Love punched musician Kathleen Hanna in the face after alleging she had made a joke about her daughter. Love was charged with assault, to which she pled guilty, and was sentenced to anger management classes. The same year, she was arrested in Melbourne after disrupting a Qantas Airlines flight after getting into an argument with a stewardess. In 1998, Love punched "Los Angeles Times" journalist Belissa Cohen in the face after she attempted to take a photo of her at a party, though the suit was settled out of court for an undisclosed sum.
Several years later, in February 2003, Love was banned from Virgin Airlines by founder Richard Branson after being arrested at Heathrow Airport for disrupting a flight. The same year, in October, in the midst of what Love would later admit was a serious cocaine and prescription drug addiction, she was arrested in Los Angeles after breaking several windows of her producer and then-boyfriend James Barber's home, and was charged with being under the influence of a controlled substance. The ordeal resulted in custody of daughter Frances Bean being withdrawn from Love. In 2004, Love was arrested in New York City for allegedly striking a fan with a microphone stand at a concert.
In 2009, fashion designer Dawn Simorangkir brought a libel suit against Love concerning a defamatory post Love made on her Twitter account, which was settled for $450,000. Six years later, Simorangkir filed another lawsuit against Love for further defamatory Twitter posts, and Love paid a further $350,000 in recompense. A similar suit was brought against Love by her former attorney Rhonda Holmes in 2014, who also accused Love of online defamation, seeking $8 million in damages. It was the first case of alleged Twitter-based libel in U.S. history to make it to trial. The jury, however, found in Love's favor.
Religious and political views.
Love has practiced several religions, including Catholicism, Episcopalianism and New Age religions, but has said that Buddhism is the "most transcendent" path for her. She has studied and practiced both Tibetan and Nichiren Buddhism since 1989, and is a member of the Soka Gakkai International, a global lay Buddhist organization.
Love is a Democrat. In 2000, she gave a speech at the Million Mom March to advocate stricter gun control laws in the United States, calling the country's gun laws "nihilistic and barbaric", and urging stringent registration of guns, licensing of gun owners, and thorough evaluation of legal and mental health records.
In 2000, Love also publicly advocated for reform of the record industry in a personal letter published by "Salon". In the letter, Love stated: "It's not piracy when kids swap music over the Internet using Napster or Gnutella or Freenet or iMesh or beaming their CDs into a My.MP3.com or MyPlay.com music locker. It's piracy when those guys that run those companies make side deals with the cartel lawyers and label heads so that they can be "the labels' friend," and not the artists." In a subsequent interview with Carrie Fisher, Love divulged that she was interested in starting a union for recording artists, and also discussed race relations in the music industry, advocating for record companies to "put money back into the black community white people have been stealing from for years." She also cited Limp Bizkit's Fred Durst as an example of "a white guy [getting to express a black man's rage with all the privileges of a white guy." Love has also consistently advocated for LGBT rights, and identifies as a feminist.
Philanthropy.
In 1993, Love and husband Kurt Cobain performed an acoustic set together at the Rock Against Rape benefit in Los Angeles, which raised awareness and provided resources for victims of sexual abuse. Love has also contributed to amfAR's AIDS research benefits and held live musical performances at their events. In 2009, Love performed a benefit concert for the RED Campaign at Carnegie Hall alongside Laurie Anderson, Rufus Wainwright, and Scarlett Johansson, with proceeds going to AIDS research. In May 2011, she attended Mariska Hargitay's Joyful Heart Foundation event for victims of child abuse, rape, and domestic violence, donating six of husband Kurt Cobain's personal vinyl records for auction.
Love has also participated with LGBT youth charities, specifically with the Los Angeles Gay and Lesbian Center, where she has taken part in performances at the center's "An Evening with Women" events. The proceeds of the event help provide food and shelter for homeless youth; services for seniors; legal assistance; domestic violence services; health and mental health services, and cultural arts programs. Love participated with Linda Perry for the event again in 2012, relating her experiences as a nomadic teenager and having to live on the street: This really resonates with me, I was a kid from Oregon, and I came to Hollywood like a lot of people do, and you know, what happens is that we end up on the street ... and if you're gay, or lesbian, or transgendered— the more "outside" you are, the more screwed you are in a lot of ways ... Seven thousand kids in Los Angeles a year go out on the street, and forty percent of those kids are gay, lesbian, or transgendered. They come out to their parents, and become homeless. [The charity helps them get sent to the right foster care, they can get medical help, food, clothing ... and for whatever reason, I don't really know why, but gay men have a lot of foundations, I've played many of them— but the lesbian side of it doesn't have as much money and/or donors, so we're excited that this has grown to cover women and women's affairs.
Cultural impact.
Love had a significant impact on female-fronted alternative acts and performers, particularly the Riot grrrl movement, with Hole's first album, "Pretty on the Inside". She has been cited as particularly influential to young female instrumentalists, once infamously proclaiming: "I want every girl in the world to pick up a guitar and start screaming." "I strap on that motherfucking guitar and you cannot fuck with me. That's my feeling," she said. In "The Electric Guitar: A History of an American Icon", it is noted that, "Time" deemed Hole's "Live Through This" to be supplemented by "primal guitar riffs and high-IQ lyrics." Having sold over 3 million records in the United States alone, Hole became one of the most successful rock bands of all time fronted by a woman. In 2015, the "Phoenix New Times" declared Love the no. 1 greatest female rock star of all time, saying: "To build a perfect rock star, there are several crucial ingredients: musical talent, physical attractiveness, tumultuous relationships, substance abuse, and public meltdowns, just to name a few. These days, Love seems to have rebounded from her epic tailspin and has leveled out in a slightly more normal manner, but there's no doubt that her life to date is the type of story people wouldn't believe in a novel or a movie."
Among the alternative musicians who have cited Love as an influence are: Scout Niblett; Brody Dalle of The Distillers; Dee Dee Penny of Dum Dum Girls; and Nine Black Alps. Contemporary female pop artists Lana Del Rey, Avril Lavigne, Tove Lo, and Sky Ferreira have also cited Love as an influence. Love has frequently been recognized as the most high-profile contributor of feminist music during the 1990s, and for "subverting mainstream expectations of how a woman should look, act, and sound." According to music journalist Maria Raha, "Whether you love Courtney [Love or hate her, Hole was the highest-profile female-fronted band of the '90s to openly and directly sing about feminism." She has also been noted as a gay icon since the mid-1990s, and has jokingly referred to her fanbase as consisting of "females, gay guys, and a few advanced, evolved heterosexual men".
Love's aesthetic image, particularly in the early 1990s, also became influential, and was dubbed "kinderwhore" by critics and media. The subversive fashion mainly consisted of thrift shop babydoll dresses accompanied by smeared makeup and red lipstick; MTV reporter Kurt Loder described Love as looking like "a debauched rag doll" onstage. Love later said she had been influenced by the fashion of Chrissy Amphlett of the Divinyls.
The artist Barbara Kruger used one of Love's quotes on her NYC bus project. Indie pop punk band The Muffs named their second album, "Blonder and Blonder" (1995) after a quote by Love, and a recording of Love talking about a stolen dress appears as the track "Love" on the band's 2000 compilation album "Hamburger". There is also a band named after her.

</doc>
<doc id="5657" url="https://en.wikipedia.org/wiki?curid=5657" title="Cow (disambiguation)">
Cow (disambiguation)

Cow is the nickname for cattle, and the name of adult female cattle.
Cow, cows or COW may also refer to:

</doc>
<doc id="5658" url="https://en.wikipedia.org/wiki?curid=5658" title="Cannibalism">
Cannibalism

Cannibalism is the act or practice of humans eating the flesh or internal organs of other human beings. A person who practices cannibalism is called a cannibal. The expression "cannibalism" has been extended into zoology to mean one individual of a species consuming all or part of another individual of the same species as food, including sexual cannibalism.
The Island Carib people of the Lesser Antilles, from whom the word cannibalism derives, acquired a long-standing reputation as cannibals following the recording of their legends in the 17th century. Some controversy exists over the accuracy of these legends and the prevalence of actual cannibalism in the culture. Cannibalism was widespread in the past among humans in many parts of the world, continuing into the 19th century in some isolated South Pacific cultures, and to the present day in parts of tropical Africa. Cannibalism was practiced in New Guinea and in parts of the Solomon Islands, and flesh markets existed in some parts of Melanesia. Fiji was once known as the "Cannibal Isles". Cannibalism has been well documented around the world, from Fiji to the Amazon Basin to the Congo to Māori New Zealand. Neanderthals are believed to have practiced cannibalism, and Neanderthals may have been eaten by anatomically modern humans.
Cannibalism has recently been both practiced and fiercely condemned in several wars, especially in Liberia and Congo. It is still practiced in Papua New Gunea as of 2012 for cultic reasons and in ritual and in war in various Melanesian tribes. Cannibalism has been said to test the bounds of cultural relativism as it challenges anthropologists "to define what is or is not beyond the pale of acceptable human behavior."
Cannibalism has been occasionally practiced as a last resort by people suffering from famine, including in modern times. Famous examples include the ill-fated Westward expedition of the Donner Party (1846-7) and, more recently, the crash of Uruguayan Air Force Flight 571 (1972), after which some survivors ate the bodies of dead passengers. Also, some mentally ill people obsess about eating others and actually do so, such as Jeffrey Dahmer and Albert Fish. There is resistance to formally labeling cannibalism as a mental disorder.
Etymology.
Cannibalism derives from "Caníbales", the Spanish name for the Caribs, a West Indies tribe that formerly practiced cannibalism, from Spanish "canibal" or "caribal", "a savage". It is also called "anthropophagy".
Reasons.
In some societies, especially tribal societies, cannibalism is a cultural norm. Consumption of a person from within the same community is called endocannibalism; ritual cannibalism of the recently deceased can be part of the grieving process or a way of guiding the souls of the dead into the bodies of living descendants. 
Exocannibalism is the consumption of a person from outside the community, usually as a celebration of victory against a rival tribe. Both types of cannibalism can also be fueled by the belief that eating a person's flesh or internal organs will endow the cannibal with some of the characteristics of the deceased.
In most parts of the world, cannibalism is not a societal norm, but is sometimes resorted to in situations of extreme necessity. The survivors of the shipwrecks of the "Essex" and "Méduse" in the 19th century are said to have engaged in cannibalism, as did the members of Franklin's lost expedition and the Donner Party. Such cases generally involve necro-cannibalism (eating the corpse of someone who is already dead) as opposed to homicidal cannibalism (killing someone for food). In English law, the latter is always considered a crime, even in the most trying circumstances. The case of "R v Dudley and Stephens", in which two men were found guilty of murder for killing and eating a cabin boy while adrift at sea in a lifeboat, set the precedent that necessity is no defence to a charge of murder.
There are numerous examples of murderers consuming their victims, often deriving some degree of sexual satisfaction from the act of cannibalism. Notable examples include Albert Fish, Issei Sagawa and Jeffrey Dahmer. These individuals are usually considered to be mentally ill, although the compulsion to eat human flesh is not formally listed as a mental disorder in the "Diagnostic and Statistical Manual of Mental Disorders" (DSM). Cases of autophagia, or self-cannibalism, have also been reported.
In pre-modern medicine, the explanation given by the now-discredited theory of humorism for cannibalism was that it came about within a black acrimonious humour, which, being lodged in the linings of the ventricle, produced the voracity for human flesh.
Medical aspects.
A well-known case of mortuary cannibalism is that of the Fore tribe in New Guinea, which resulted in the spread of the prion disease kuru. Although the Fore's mortuary cannibalism was well documented, the practice had ceased before the cause of the disease was recognized. However, some scholars argue that although post-mortem dismemberment was the practice during funeral rites, cannibalism was not. Marvin Harris theorizes that it happened during a famine period coincident with the arrival of Europeans and was rationalized as a religious rite.
In 2003, a publication in "Science" received a large amount of press attention when it suggested that early humans may have practiced extensive cannibalism. According to this research, genetic markers commonly found in modern humans worldwide suggest that today many people carry a gene that evolved as protection against the brain diseases that can be spread by consuming human brain tissue. A 2006 reanalysis of the data questioned this hypothesis, as it claimed to have found a data collection bias, which led to an erroneous conclusion. This claimed bias came from incidents of cannibalism used in the analysis not being due to local cultures, but having been carried out by explorers, stranded seafarers or escaped convicts. The original authors published a subsequent paper in 2008 defending their conclusions.
Myths, legends and folklore.
Cannibalism features in the folklore and legends of many cultures and is most often attributed to evil characters or as extreme retribution for some wrong. Examples include the witch in "Hansel and Gretel", Lamia of Greek mythology and Baba Yaga of Slavic folklore.
A number of stories in Greek mythology involve cannibalism, in particular cannibalism of close family members, e.g., the stories of Thyestes, Tereus and especially Cronus, who was Saturn in the Roman pantheon. The story of Tantalus also parallels this.
The wendigo is a creature appearing in the legends of the Algonquian people. It is thought of variously as a malevolent cannibalistic spirit that could possess humans or a monster that humans could physically transform into. Those who indulged in cannibalism were at particular risk, and the legend appears to have reinforced this practice as taboo.
As used to demonize colonized peoples and indigenous groups.
William Arens, author of "The Man-Eating Myth: Anthropology and Anthropophagy", questions the credibility of reports of cannibalism and argues that the description by one group of people of another people as cannibals is a consistent and demonstrable ideological and rhetorical device to establish perceived cultural superiority. Arens bases his thesis on a detailed analysis of numerous "classic" cases of cultural cannibalism cited by explorers, missionaries, and anthropologists. He asserted that many were steeped in racism, unsubstantiated, or based on second-hand or hearsay evidence.
Often, these accusations helped characterize indigenous peoples as "uncivilized" and "primitive". Furthermore, these assertions promoted the use of military force as a means of "civilizing" and "pacifying" the "savages". An example of the sensationalism of cannibalism and its connection to imperialism was in the Japan's 1874 expedition to Taiwan . As Eskildsen describes, there was an exaggeration of cannibalism by Taiwanese aboriginals in Japan's popular media such as newspapers and illustrations at the time. 
History.
Among modern humans, cannibalism has been practiced by various groups. It was practiced by humans in Prehistoric Europe, South America, among Iroquoian peoples in North America, Maori in New Zealand, the Solomon Islands, parts of West Africa and Central Africa, some of the islands of Polynesia, New Guinea, Sumatra, and Fiji. Evidence of cannibalism has been found in ruins associated with the Anasazi culture of the Southwestern United States as well (at Cowboy Wash in Colorado).
Pre-history.
There is evidence, both archaeological and genetic, that cannibalism has been practiced for tens of thousands of years. Human bones that have been "de-fleshed" by other humans go back 600,000 years. The oldest "Homo sapiens" bones (from Ethiopia) show signs of this as well. Some anthropologists, such as Tim White, suggest that ritual cannibalism was common in human societies prior to the beginning of the Upper Paleolithic period. This theory is based on the large amount of "butchered human" bones found in Neanderthal and other Lower/Middle Paleolithic sites. Cannibalism in the Lower and Middle Paleolithic may have occurred because of food shortages. It has been also suggested that removing dead bodies through ritual cannibalism might been a means of predator control, aiming to eliminate predators' and scavengers' access to hominid (and early human) bodies. Jim Corbett proposed that after major epidemics, when human corpses are easily accessible to predators, there are more cases of man-eating leopards, so removing dead bodies through ritual cannibalism (before the cultural traditions of burying and burning bodies appeared in human history) might have had practical reasons for hominids and early humans to control predation.
In Gough's Cave, England, remains of human bones and skulls, around 15,000 years old, suggest that cannibalism took place amongst the people living in or visiting the cave, and that they may have used human skulls as drinking vessels.
Researchers have found physical evidence of cannibalism in ancient times. In 2001, archaeologists at the University of Bristol found evidence of Iron Age cannibalism in Gloucestershire. Cannibalism was practiced as recently as 2000 years ago in Great Britain. In Germany, Emil Carthaus and Dr. Bruno Bernhard have observed 1,891 signs of cannibalism in the caves at the Hönne (1000 – 700 BC)
Early history.
Cannibalism is mentioned many times in early history and literature. Cannibalism was reported by Flavius Josephus during the siege of Jerusalem by Rome in 70 AD, and according to Appian, the population of Numantia during the Roman Siege of Numantia in the 2nd century BC was reduced to cannibalism and suicide.
St. Jerome, in his letter "Against Jovinianus", discusses how people come to their present condition as a result of their heritage, and then lists several examples of peoples and their customs. In the list, he mentions that he has heard that Atticoti eat human flesh and that Massagetae and "Derbices" (a people on the borders of India) kill and eat old people. 
Herodotus in ""The Histories"" (450s to the 420s BC) claimed, that after eleven days' voyage up the Borysthenes (Dnieper River in Europe) a desolated land extended for a long way, and later the country of the Man-eaters (other than Scythians) was located, and beyond it again a desolated area extended where no men lived.
Middle Ages.
Reports of cannibalism were recorded during the First Crusade, as Crusaders were alleged to have fed on the bodies of their dead opponents following the Siege of Ma'arrat al-Numan. Amin Maalouf also alleges further cannibalism incidents on the march to Jerusalem, and to the efforts made to delete mention of these from western history. During Europe's Great Famine of 1315–1317 there were many reports of cannibalism among the starving populations. In North Africa, as in Europe, there are references to cannibalism as a last resort in times of famine.
The Moroccan Muslim explorer Ibn Batutta reported that one African king advised him that nearby people were cannibals (although this may have been a prank played on Ibn Batutta by the king to fluster his guest). However Batutta reported that Arabs and Christians were safe, as their flesh was "unripe" and would cause the eater to fall ill.
For a brief time in Europe, an unusual form of cannibalism occurred when thousands of Egyptian mummies preserved in bitumen were ground up and sold as medicine. The practice developed into a wide-scale business which flourished until the late 16th century. This "fad" ended because the mummies were revealed actually to be recently killed slaves. Two centuries ago, mummies were still believed to have medicinal properties against bleeding, and were sold as pharmaceuticals in powdered form (see human mummy confection and mummia).
In China during the Tang dynasty, cannibalism was supposedly resorted to by rebel forces early in the period (who were said to raid neighboring areas for victims to eat), as well as both soldiers and civilians besieged during the rebellion of An Lushan. Eating an enemy's heart and liver was also claimed to be a feature of both official punishments and private vengeance. References to cannibalizing the enemy has also been seen in poetry written in the Song dynasty (for example, in "Man Jiang Hong"), although the cannibalizing is perhaps poetic symbolism, expressing hatred towards the enemy.
While there is universal agreement that some Mesoamerican people practiced human sacrifice, there is a lack of scholarly consensus as to whether cannibalism in pre-Columbian America was widespread. At one extreme, anthropologist Marvin Harris, author of "Cannibals and Kings", has suggested that the flesh of the victims was a part of an aristocratic diet as a reward, since the Aztec diet was lacking in proteins. While most pre-Columbian historians believe that there was ritual cannibalism related to human sacrifices, they do not support Harris's thesis that human flesh was ever a significant portion of the Aztec diet. Others have hypothesized that cannibalism was part of a blood revenge in war.
Early modern era.
Physical evidence was found recently for cannibalism in the Jamestown Colony in 1609, which is also documented in written records of the colony.
European explorers and colonizers brought home many stories of cannibalism practiced by the native peoples they encountered. The friar Diego de Landa reported about Yucatán instances, and there have been similar reports by Purchas from Popayán, Colombia, and from the Marquesas Islands of Polynesia, where human flesh was called ""long pig"". According to Hans Egede, when the Inuit killed a woman accused of witchcraft, they ate a portion of her heart. It is recorded about the natives of the captaincy of Sergipe in Brazil: "They eat human flesh when they can get it, and if a woman miscarries devour the abortive immediately. If she goes her time out, she herself cuts the navel-string with a shell, which she boils along with the secondine, and eats them both."
The 1913 "Handbook of Indians of Canada" (reprinting 1907 material from the Bureau of American Ethnology), claims that North American natives practicing cannibalism included "... the Montagnais, and some of the tribes of Maine; the Algonkin, Armouchiquois, Iroquois, and Micmac; farther west the Assiniboine, Cree, Foxes, Chippewa, Miami, Ottawa, Kickapoo, Illinois, Sioux, and Winnebago; in the South the people who built the mounds in Florida, and the Tonkawa, Attacapa, Karankawa, Caddo, and Comanche (?); in the Northwest and West, portions of the continent, the Thlingchadinneh and other Athapascan tribes, the Tlingit, Heiltsuk, Kwakiutl, Tsimshian, Nootka, Siksika, some of the Californian tribes, and the Ute. There is also a tradition of the practice among the Hopi, and mentions of the custom among other tribes of New Mexico and Arizona. The Mohawk, and the Attacapa, Tonkawa, and other Texas tribes were known to their neighbours as 'man-eaters.'" The forms of cannibalism described included both resorting to human flesh during famines and ritual cannibalism, the latter usually consisting of eating a small portion of an enemy warrior.
As with most lurid tales of native cannibalism, these stories are treated with a great deal of scrutiny, as accusations of cannibalism were often used as justifications for the subjugation or destruction of "savages". However, there were several well-documented cultures that engaged in regular eating of the dead, such as New Zealand's Māori. In an 1809 incident known as the Boyd massacre, about 66 passengers and crew of the "Boyd" were killed and eaten by Māori on the Whangaroa peninsula, Northland. Cannibalism was already a regular practice in Māori wars. In another instance, on July 11, 1821 warriors from the Ngapuhi tribe killed 2,000 enemies and remained on the battlefield "eating the vanquished until they were driven off by the smell of decaying bodies". Māori warriors fighting the New Zealand government in Titokowaru's War in New Zealand's North Island in 1868–69 revived ancient rites of cannibalism as part of the radical Hauhau movement of the Pai Marire religion.
Other islands in the Pacific were home to cultures that allowed cannibalism to some degree. In parts of Melanesia, cannibalism was still practiced in the early 20th century, for a variety of reasons—including retaliation, to insult an enemy people, or to absorb the dead person's qualities. One tribal chief, Ratu Udre Udre in Rakiraki, Fiji, is said to have consumed 872 people and to have made a pile of stones to record his achievement. Fiji was nicknamed the "Cannibal Isles" by European sailors, who avoided disembarking there. The dense population of Marquesas Islands, Polynesia, was concentrated in the narrow valleys, and consisted of warring tribes, who sometimes practiced cannibalism on their enemies. W. D. Rubinstein wrote:
This period of time was also rife with instances of explorers and seafarers resorting to cannibalism for survival. The survivors of the sinking of the French ship "Méduse" in 1816 resorted to cannibalism after four days adrift on a raft and their plight was made famous by Théodore Géricault's painting Raft of the Medusa. After the sinking of the "Essex" of Nantucket by a whale on 20 November 1820 (an important source event for Herman Melville's "Moby-Dick"), the survivors, in three small boats, resorted, by common consent, to cannibalism in order for some to survive. Sir John Franklin's lost polar expedition is another example of cannibalism out of desperation. On land, the Donner Party found itself stranded by snow in a high mountain pass in California without adequate supplies during the Mexican–American War, leading to several instances of cannibalism. Another notorious cannibal was mountain man Boone Helm, who was known as "The Kentucky Cannibal" for eating several of his fellow travelers, from 1850 until his eventual hanging in 1864.
The case of "R v. Dudley and Stephens" (1884) 14 QBD 273 (QB) is an English case which dealt with four crew members of an English yacht, the "Mignonette", who were cast away in a storm some from the Cape of Good Hope. After several days, one of the crew, a seventeen-year-old cabin boy, fell unconscious due to a combination of the famine and drinking seawater. The others (one possibly objecting) decided then to kill him and eat him. They were picked up four days later. Two of the three survivors were found guilty of murder. A significant outcome of this case was that necessity was determined to be no defence against a charge of murder.
American consul James W. Davidson described in his 1903 book, "The Island of Formosa", how the Chinese in Taiwan ate and traded in the flesh of Taiwanese aboriginals.
Roger Casement, writing to a consular colleague in Lisbon on August 3, 1903 from Lake Mantumba in the Congo Free State, said: "The people round here are all cannibals. You never saw such a weird looking lot in your life. There are also dwarfs (called Batwas) in the forest who are even worse cannibals than the taller human environment. They eat man flesh raw! It's a fact." Casement then added how assailants would "bring down a dwarf on the way home, for the marital cooking pot ... The Dwarfs, as I say, dispense with cooking pots and eat and drink their human prey fresh cut on the battlefield while the blood is still warm and running. These are not fairy tales my dear Cowper but actual gruesome reality in the heart of this poor, benighted savage land."
World War II.
Many instances of cannibalism by necessity were recorded during World War II. For example, during the 872-day Siege of Leningrad, reports of cannibalism began to appear in the winter of 1941–1942, after all birds, rats and pets were eaten by survivors. Leningrad police even formed a special division to combat cannibalism.
Some 2.8 million Soviet POWs died in Nazi custody in less than eight months of 1941–42. According to the USHMM, by the winter of 1941, "starvation and disease resulted in mass death of unimaginable proportions". This deliberate starvation led to many incidents of cannibalism.
Following the Soviet victory at Stalingrad it was found that some German soldiers in the besieged city, cut off from supplies, resorted to cannibalism. Later, following the German surrender in January 1943, roughly 100,000 German soldiers were taken prisoner of war (POW). Almost all of them were sent to POW camps in Siberia or Central Asia where, due to being chronically underfed by their Soviet captors, many resorted to cannibalism. Fewer than 5,000 of the prisoners taken at Stalingrad survived captivity.
The Australian War Crimes Section of the Tokyo tribunal, led by prosecutor William Webb (the future Judge-in-Chief), collected numerous written reports and testimonies that documented Japanese soldiers' acts of cannibalism among their own troops, on enemy dead, and on Allied prisoners of war in many parts of the Greater East Asia Co-Prosperity Sphere. In September 1942, Japanese daily rations on New Guinea consisted of 800 grams of rice and tinned meat. However, by December, this had fallen to 50 grams. According to historian Yuki Tanaka, "cannibalism was often a systematic activity conducted by whole squads and under the command of officers".
In some cases, flesh was cut from living people. An Indian POW, Lance Naik Hatam Ali (later a citizen of Pakistan), testified that in New Guinea: "the Japanese started selecting prisoners and every day one prisoner was taken out and killed and eaten by the soldiers. I personally saw this happen and about 100 prisoners were eaten at this place by the Japanese. The remainder of us were taken to another spot away where 10 prisoners died of sickness. At this place, the Japanese again started selecting prisoners to eat. Those selected were taken to a hut where their flesh was cut from their bodies while they were alive and they were thrown into a ditch where they later died."
Another well-documented case occurred in Chichi-jima in February 1945, when Japanese soldiers killed and consumed five American airmen. This case was investigated in 1947 in a war crimes trial, and of 30 Japanese soldiers prosecuted, five (Maj. Matoba, Gen. Tachibana, Adm. Mori, Capt. Yoshii, and Dr. Teraki) were found guilty and hanged. In his book "", James Bradley details several instances of cannibalism of World War II Allied prisoners by their Japanese captors. The author claims that this included not only ritual cannibalization of the livers of freshly killed prisoners, but also the cannibalization-for-sustenance of living prisoners over the course of several days, amputating limbs only as needed to keep the meat fresh.
New Guinea.
The Korowai tribe of south-eastern Papua could be one of the last surviving tribes in the world engaging in cannibalism. A local cannibal cult killed and ate victims as late as 2012.
Africa.
During the 1892–1894 war between the Congo Free State and the Swahili-Arab city-states of Nyangwe and Kasongo in Eastern Congo, there were reports of widespread cannibalization of the bodies of defeated Arab combatants by the Batetela allies of Belgian commander Francis Dhanis. The Batetela, "like most of their neighbors were inveterate cannibals." According to Dhanis' medical officer, Captain Hinde, their town of Ngandu had "at least 2,000 polished human skulls" as a "solid white pavement in front" of its gates, with human skulls crowning every post of the stockade.
In April 1892, 10,000 of the Batetela, under the command of Gongo Lutete, joined forces with Dhanis in a campaign against the Swahili-Arab leaders Sefu and Mohara. After one early skirmish in the campaign, Hinde "noticed that the bodies of both the killed and wounded had vanished." When fighting broke out again, Hinde saw his Batetela allies drop human arms, legs and heads on the road. One young Belgian officer wrote home: "Happily Gongo's men ate them up a few hours. It's horrible but exceedingly useful and hygenic ... I should have been horrified at the idea in Europe! But it seems quite natural to me here. Don't show this letter to anyone indiscreet." After the massacre at Nyangwe, Lutete "hid himself in his quarters, appalled by the sight of thousands of men smoking human hands and human chops on their camp fires, enough to feed his army for many days."
In the 1980s, Médecins Sans Frontières, the international medical charity, supplied photographic and other documentary evidence of ritualized cannibal feasts among the participants in Liberia's internecine strife to representatives of Amnesty International who were on a fact-finding mission to the neighboring state of Guinea. However, Amnesty International declined to publicize this material; the Secretary-General of the organization, Pierre Sane, said at the time in an internal communication that "what they do with the bodies after human rights violations are committed is not part of our mandate or concern". The existence of cannibalism on a wide scale in Liberia was subsequently verified.
The self-declared Emperor of the Central African Empire, Jean-Bédel Bokassa (Emperor Bokassa I), was tried on October 24, 1986 for several cases of cannibalism although he was never convicted. Between April 17, and April 19, 1979 a number of elementary school students were arrested after they had protested against wearing the expensive, government-required school uniforms. Around 100 were killed. Bokassa is said to have participated in the massacre, beating some of the children to death with his cane and allegedly ate some of his victims.
Further reports of cannibalism were reported against the Seleka Muslim minority during the ongoing Central African Republic conflict.
Cannibalism has been reported in several recent African conflicts, including the Second Congo War, and the civil wars in Liberia and Sierra Leone. A UN human rights expert reported in July 2007 that sexual atrocities against Congolese women go "far beyond rape" and include sexual slavery, forced incest, and cannibalism. This may be done in desperation, as during peacetime cannibalism is much less frequent; at other times, it is consciously directed at certain groups believed to be relatively helpless, such as Congo Pygmies, even considered subhuman by some other Congolese. It is also reported by some that witch doctors sometimes use the body parts of children in their medicine. In the 1970s the Ugandan dictator Idi Amin was reputed to practice cannibalism.
In Uganda, the Lord's Resistance Army have been accused of routinely engaging in ritual or magical cannibalism.
North Korea.
Reports of widespread cannibalism began to emerge from North Korea during the famine of the 1990s and subsequent ongoing starvation. Kim Jong-il was reported to have ordered a crackdown on cannibalism in 1996. Chinese travellers reported in 1998 that cannibalism had occurred. Three people in North Korea were reported to have been executed for selling or eating human flesh in 2006. Further reports of cannibalism emerged in early 2013, including reports of a man executed for killing his two children for food.
There are competing claims about how widespread cannibalism was in North Korea. While refugees reported that it was widespread Barbara Demick in her 2010 book "" wrote that it did not seem to be.
China.
Cannibalism is documented to have occurred in China during the Great Leap Forward, when rural China was hit hard by drought and famine.
During Mao's Cultural Revolution, local governments' documents revealed hundreds of incidents of cannibalism for ideological reasons. Public events for cannibalism were organised by local Communist Party officials, and people took part together to prove their revolutionary passion.
Modern era.
Further instances include cannibalism as ritual practice, in times of drought, famine and other destitution, as well as those being criminal acts and war crimes throughout the 20th century, and also 21st century.
In West Africa, the Leopard Society was a secret society active into the mid-1900s and one that practiced cannibalism. Centered in Sierra Leone, Liberia and Ivory Coast, the "Leopard men" would dress in leopard skins, waylaying travelers with sharp claw-like weapons in the form of leopards' claws and teeth. The victims' flesh would be cut from their bodies and distributed to members of the society.
As in some other Papuan societies, the Urapmin people engaged in cannibalism in war. Notably, the Urapmin also had a system of food taboo wherein dogs could not be eaten and had to be kept from breathing on food, unlike humans who could be eaten and with whom food could be shared.
The Aghoris are Indian ascetics who believe that eating human flesh confers spiritual and physical benefits, such as prevention of aging. They claim to only eat those who have voluntarily willed their body to the sect upon their death, although an Indian TV crew witnessed one Aghori feasting on a corpse discovered floating in the Ganges, and a member of the Dom caste reports that Aghoris often take bodies from the cremation "ghat" (or funeral pyre).
During the 1930s, multiple acts of cannibalism were reported from Ukraine and Russia's Volga, South Siberian and Kuban regions during the Soviet famine of 1932–1933.
Survival was a moral as well as a physical struggle. A woman doctor wrote to a friend in June 1933 that she had not yet become a cannibal, but was "not sure that I shall not be one by the time my letter reaches you." The good people died first. Those who refused to steal or to prostitute themselves died. Those who gave food to others died. Those who refused to eat corpses died. Those who refused to kill their fellow man died. ... At least 2,505 people were sentenced for cannibalism in the years 1932 and 1933 in Ukraine, though the actual number of cases was certainly much higher.
Prior to 1931, "New York Times" reporter William Buehler Seabrook, allegedly in the interests of research, obtained from a hospital intern at the Sorbonne a chunk of human meat from the body of a healthy human killed in an accident, then cooked and ate it. He reported, "It was like good, fully developed veal, not young, but not yet beef. It was very definitely like that, and it was not like any other meat I had ever tasted. It was so nearly like good, fully developed veal that I think no person with a palate of ordinary, normal sensitiveness could distinguish it from veal. It was mild, good meat with no other sharply defined or highly characteristic taste such as for instance, goat, high game, and pork have. The steak was slightly tougher than prime veal, a little stringy, but not too tough or stringy to be agreeably edible. The roast, from which I cut and ate a central slice, was tender, and in color, texture, smell as well as taste, strengthened my certainty that of all the meats we habitually know, veal is the one meat to which this meat is accurately comparable."
In his book, "The Gulag Archipelago", Soviet writer Aleksandr Solzhenitsyn described cases of cannibalism in 20th-century USSR. Of the famine in Povolzhie (1921–1922) he wrote: "That horrible famine was up to cannibalism, up to consuming children by their own parents — the famine, which Russia had never known even in Time of Troubles 1601–1603 ..." 
He said of the Siege of Leningrad (1941–1944): "Those who consumed human flesh, or dealt with the human liver trading from dissecting rooms ... were accounted as the political criminals ..." And of the building of Northern Railway Labor Camp ("Sevzheldorlag") Solzhenitsyn reports, "An ordinary hard working political prisoner almost could not survive at that penal camp. In the camp Sevzheldorlag (chief: colonel Klyuchkin) in 1946–47 there were many cases of cannibalism: they cut human bodies, cooked and ate."
During the dekulakization process in the USSR in the 1920s and 1930s, many deportees were forced to eat one another by genocidal Soviet authorities, e.g. on the Nazino island or during Holodomor.
The Soviet journalist Yevgenia Ginzburg was a former long-term political prisoner who spent time in the Soviet prisons, Gulag camps and settlements from 1938 to 1955. She described in her memoir, "Harsh Route" (or "Steep Route"), of a case which she was directly involved in during the late 1940s, after she had been moved to the prisoners' hospital.
...The chief warder shows me the black smoked pot, filled with some food: 'I need your medical expertise regarding this meat.' I look into the pot, and hardly hold vomiting. The fibres of that meat are very small, and don't resemble me anything I have seen before. The skin on some pieces bristles with black hair (...) A former smith from Poltava, Kulesh worked together with Centurashvili. At this time, Centurashvili was only one month away from being discharged from the camp (...) And suddenly he surprisingly disappeared. The wardens looked around the hills, stated Kulesh's evidence, that last time Kulesh had seen his workmate near the fireplace, Kulesh went out to work and Centurashvili left to warm himself more; but when Kulesh returned to the fireplace, Centurashvili had vanished; who knows, maybe he got frozen somewhere in snow, he was a weak guy (...) The wardens searched for two more days, and then assumed that it was an escape case, though they wondered why, since his imprisonment period was almost over (...) The crime was there. Approaching the fireplace, Kulesh killed Centurashvili with an axe, burned his clothes, then dismembered him and hid the pieces in snow, in different places, putting specific marks on each burial place. ... Just yesterday, one body part was found under two crossed logs.
When Uruguayan Air Force Flight 571 crashed into the Andes on October 13, 1972, the survivors resorted to eating the deceased during their 72 days in the mountains. Their story was later recounted in the books ' and ' as well as the film "Alive", by Frank Marshall, and the documentaries ' (1993) and ' (2008).
Cannibalism was reported by the journalist Neil Davis during the South East Asian wars of the 1960s and 1970s. Davis reported that Cambodian troops ritually ate portions of the slain enemy, typically the liver. However he, and many refugees, also report that cannibalism was practiced non-ritually when there was no food to be found. This usually occurred when towns and villages were under Khmer Rouge control, and food was strictly rationed, leading to widespread starvation. Any civilian caught participating in cannibalism would have been immediately executed.
On July 23, 1988, Rick Gibson ate the flesh of another person in public. Because England does not have a specific law against cannibalism, he legally ate a canapé of donated human tonsils in Walthamstow High Street, London. A year later, on April 15, 1989, he publicly ate a slice of human testicle in Lewisham High Street, London. When he tried to eat another slice of human testicle at the Pitt International Galleries in Vancouver on July 14, 1989, the Vancouver police confiscated the testicle hors d'œuvre. However, the charge of publicly exhibiting a disgusting object was dropped and he finally ate the piece of human testicle on the steps of the Vancouver court house on September 22, 1989.

</doc>
<doc id="5659" url="https://en.wikipedia.org/wiki?curid=5659" title="Chemical element">
Chemical element

A chemical element or element is a species of atoms having the same number of protons in their atomic nuclei (i.e. the same atomic number, "Z"). There are 118 elements that have been identified, of which the first 94 occur naturally on Earth with the remaining 24 being synthetic elements. There are 80 elements that have at least one stable isotope and 38 that have exclusively radioactive isotopes, which decay over time into other elements. Iron is the most abundant element (by mass) making up the Earth, while oxygen is the most common element in the crust of the earth.
Chemical elements constitute all of the ordinary matter of the universe. However astronomical observations suggest that ordinary observable matter is only approximately 15% of the matter in the universe: the remainder is dark matter, the composition of which is unknown, but it is not composed of chemical elements.
The two lightest elements, hydrogen and helium were mostly formed in the Big Bang and are the most common elements in the universe. The next three elements (lithium, beryllium and boron) were formed mostly by cosmic ray spallation, and are thus more rare than those that follow. Formation of elements with from six to twenty six protons occurred and continues to occur in main sequence stars via stellar nucleosynthesis. The high abundance of oxygen, silicon, and iron on Earth reflects their common production in such stars. Elements with greater than twenty-six protons are formed by supernova nucleosynthesis in supernovae, which, when they explode, blast these elements far into space as planetary nebulae, where they may become incorporated into planets when they are formed.
The term "element" is used for a kind of atoms with a given number of protons (regardless of whether they are or they are not ionized or chemically bonded, e.g. hydrogen in water) as well as for a pure chemical substance consisting of a single element (e.g. hydrogen gas). For the second meaning, the terms "elementary substance" and "simple substance" have been suggested, but they have not gained much acceptance in the English-language chemical literature, whereas in some other languages their equivalent is widely used (e.g. French "corps simple", Russian "простое вещество"). One element can form multiple substances different by their structure; they are called allotropes of the element.
When different elements are chemically combined, with the atoms held together by chemical bonds, they form chemical compounds. Only a minority of elements are found uncombined as relatively pure minerals. Among the more common of such "native elements" are copper, silver, gold, carbon (as coal, graphite, or diamonds), and sulfur. All but a few of the most inert elements, such as noble gases and noble metals, are usually found on Earth in chemically combined form, as chemical compounds. While about 32 of the chemical elements occur on Earth in native uncombined forms, most of these occur as mixtures. For example, atmospheric air is primarily a mixture of nitrogen, oxygen, and argon, and native solid elements occur in alloys, such as that of iron and nickel.
The history of the discovery and use of the elements began with primitive human societies that found native elements like carbon, sulfur, copper and gold. Later civilizations extracted elemental copper, tin, lead and iron from their ores by smelting, using charcoal. Alchemists and chemists subsequently identified many more, with almost all of the naturally-occurring elements becoming known by 1900.
The properties of the chemical elements are summarized on the periodic table, which organizes the elements by increasing atomic number into rows ("periods") in which the columns ("groups") share recurring ("periodic") physical and chemical properties. Save for unstable radioactive elements with short half-lives, all of the elements are available industrially, most of them in high degrees of purity.
Description.
The lightest chemical elements are hydrogen and helium, both created by Big Bang nucleosynthesis during the first 20 minutes of the universe in a ratio of around 3:1 by mass (or 12:1 by number of atoms), along with tiny traces of the next two elements, lithium and beryllium. Almost all other elements found in nature were made by various natural methods of nucleosynthesis. On Earth, small amounts of new atoms are naturally produced in nucleogenic reactions, or in cosmogenic processes, such as cosmic ray spallation. New atoms are also naturally produced on Earth as radiogenic daughter isotopes of ongoing radioactive decay processes such as alpha decay, beta decay, spontaneous fission, cluster decay, and other rarer modes of decay.
Of the 94 naturally occurring elements, those with atomic numbers 1 through 82 each have at least one stable isotope, (except for technetium, element 43 and promethium, element 61, which have no stable isotopes). Isotopes considered stable are those for which no radioactive decay has yet been observed. Elements with atomic numbers 83 through 94 are unstable to the point that radioactive decay of all isotopes can be detected. Some of these elements, notably bismuth (atomic number 83), thorium (atomic number 90), uranium (atomic number 92) and plutonium (atomic number 94), have one or more isotopes with half-lives long enough to survive as remnants of the explosive stellar nucleosynthesis that produced the heavy elements before the formation of our solar system. For example, at over 1.9 years, over a billion times longer than the current estimated age of the universe, bismuth-209 (atomic number 83) has the longest known alpha decay half-life of any naturally occurring element. The very heaviest elements (those beyond plutonium, element 94) undergo radioactive decay with half-lives so short that they do not occur in nature and must be synthesized.
As of 2010, there are 118 known elements (in this context, "known" means observed well enough, even from just a few decay products, to have been differentiated from other elements). Of these 118 elements, 94 occur naturally on Earth. Six of these occur in extreme trace quantities: technetium, atomic number 43; promethium, number 61; astatine, number 85; francium, number 87; neptunium, number 93; and plutonium, number 94. These 94 elements have been detected in the universe at large, in the spectra of stars and also supernovae, where short-lived radioactive elements are newly being made. The first 94 elements have been detected directly on Earth as primordial nuclides present from the formation of the solar system, or as naturally-occurring fission or transmutation products of uranium and thorium.
The remaining 20 heavier elements, not found today either on Earth or in astronomical spectra, have been produced artificially: these are all radioactive, with very short half-lives; if any atoms of these elements were present at the formation of Earth, they are extremely likely, to the point of certainty, to have already decayed, and if present in novae, have been in quantities too small to have been noted. Technetium was the first purportedly non-naturally occurring element synthesized, in 1937, although trace amounts of technetium have since been found in nature (and also the element may have been discovered naturally in 1925). This pattern of artificial production and later natural discovery has been repeated with several other radioactive naturally-occurring rare elements.
Lists of the elements are available by name, by symbol, by atomic number, by density, by melting point, and by boiling point as well as ionization energies of the elements. The nuclides of stable and radioactive elements are also available as a list of nuclides, sorted by length of half-life for those that are unstable. One of the most convenient, and certainly the most traditional presentation of the elements, is in the form of the periodic table, which groups together elements with similar chemical properties (and usually also similar electronic structures).
Atomic number.
The atomic number of an element is equal to the number of protons in each atom, and defines the element. For example, all carbon atoms contain 6 protons in their atomic nucleus; so the atomic number of carbon is 6. Carbon atoms may have different numbers of neutrons; atoms of the same element having different numbers of neutrons are known as isotopes of the element.
The number of protons in the atomic nucleus also determines its electric charge, which in turn determines the number of electrons of the atom in its non-ionized state. The electrons are placed into atomic orbitals that determine the atom's various chemical properties. The number of neutrons in a nucleus usually has very little effect on an element's chemical properties (except in the case of hydrogen and deuterium). Thus, all carbon isotopes have nearly identical chemical properties because they all have six protons and six electrons, even though carbon atoms may, for example, have 6 or 8 neutrons. That is why the atomic number, rather than mass number or atomic weight, is considered the identifying characteristic of a chemical element.
The symbol for atomic number is "Z".
Isotopes.
Isotopes are atoms of the same element (that is, with the same number of protons in their atomic nucleus), but having "different" numbers of neutrons. Most (66 of 94) naturally occurring elements have more than one stable isotope. Thus, for example, there are three main isotopes of carbon. All carbon atoms have 6 protons in the nucleus, but they can have either 6, 7, or 8 neutrons. Since the mass numbers of these are 12, 13 and 14 respectively, the three isotopes of carbon are known as carbon-12, carbon-13, and carbon-14, often abbreviated to 12C, 13C, and 14C. Carbon in everyday life and in chemistry is a mixture of 12C (about 98.9%), 13C (about 1.1%) and about 1 atom per trillion of 14C.
Except in the case of the isotopes of hydrogen (which differ greatly from each other in relative mass—enough to cause chemical effects), the isotopes of a given element are chemically nearly indistinguishable.
All of the elements have some isotopes that are radioactive (radioisotopes), although not all of these radioisotopes occur naturally. The radioisotopes typically decay into other elements upon radiating an alpha or beta particle. If an element has isotopes that are not radioactive, these are termed "stable" isotopes. All of the known stable isotopes occur naturally (see primordial isotope). The many radioisotopes that are not found in nature have been characterized after being artificially made. Certain elements have no stable isotopes and are composed "only" of radioactive isotopes: specifically the elements without any stable isotopes are technetium (atomic number 43), promethium (atomic number 61), and all observed elements with atomic numbers greater than 82.
Of the 80 elements with at least one stable isotope, 26 have only one single stable isotope. The mean number of stable isotopes for the 80 stable elements is 3.1 stable isotopes per element. The largest number of stable isotopes that occur for a single element is 10 (for tin, element 50).
Isotopic mass and atomic mass.
The mass number of an element, "A", is the number of nucleons (protons and neutrons) in the atomic nucleus. Different isotopes of a given element are distinguished by their mass numbers, which are conventionally written as a superscript on the left hand side of the atomic symbol (e.g., 238U). The mass number is always a simple whole number and has units of "nucleons." An example of a referral to a mass number is "magnesium-24," which is an atom with 24 nucleons (12 protons and 12 neutrons).
Whereas the mass number simply counts the total number of neutrons and protons and is thus a natural (or whole) number, the atomic mass of a single atom is a real number for the mass of a particular isotope of the element, the unit being u. In general, when expressed in u it differs in value slightly from the mass number for a given nuclide (or isotope) since the mass of the protons and neutrons is not exactly 1 u, since the electrons contribute a lesser share to the atomic mass as neutron number exceeds proton number, and (finally) because of the nuclear binding energy. For example, the atomic mass of chlorine-35 to five significant digits is 34.969 u and that of chlorine-37 is 36.966 u. However, the atomic mass in u of each isotope is quite close to its simple mass number (always within 1%). The only isotope whose atomic mass is exactly a natural number is 12C, which by definition has a mass of exactly 12, because u is defined as 1/12 of the mass of a free neutral carbon-12 atom in the ground state.
The relative atomic mass (historically and commonly also called "atomic weight") of an element is the "average" of the atomic masses of all the chemical element's isotopes as found in a particular environment, weighted by isotopic abundance, relative to the atomic mass unit (u). This number may be a fraction that is "not" close to a whole number, due to the averaging process. For example, the relative atomic mass of chlorine is 35.453 u, which differs greatly from a whole number due to being made of an average of 76% chlorine-35 and 24% chlorine-37. Whenever a relative atomic mass value differs by more than 1% from a whole number, it is due to this averaging effect resulting from significant amounts of more than one isotope being naturally present in the sample of the element in question.
Chemically pure and isotopically pure.
Chemists and nuclear scientists have different definitions of a "pure element". In chemistry, a pure element means a substance whose atoms all (or in practice almost all) have the same atomic number, or number of protons. Nuclear scientists, however, define a pure element as one that consists of only one stable isotope.
For example, a copper wire is 99.99% chemically pure if 99.99% of its atoms are copper, with 29 protons each. However it is not isotopically pure since ordinary copper consists of two stable isotopes, 69% 63Cu and 31% 65Cu, with different numbers of neutrons.
Allotropes.
Atoms of chemically pure elements may bond to each other chemically in more than one way, allowing the pure element to exist in multiple structures (spatial arrangements of atoms), known as allotropes, which differ in their properties. For example, carbon can be found as diamond, which has a tetrahedral structure around each carbon atom; graphite, which has layers of carbon atoms with a hexagonal structure stacked on top of each other; graphene, which is a single layer of graphite that is very strong; fullerenes, which have nearly spherical shapes; and carbon nanotubes, which are tubes with a hexagonal structure (even these may differ from each other in electrical properties). The ability of an element to exist in one of many structural forms is known as 'allotropy'.
The standard state, also known as reference state, of an element is defined as its thermodynamically most stable state at 1 bar at a given temperature (typically at 298.15 K). In thermochemistry, an element is defined to have an enthalpy of formation of zero in its standard state. For example, the reference state for carbon is graphite, because the structure of graphite is more stable than that of the other allotropes.
Properties.
Several kinds of descriptive categorizations can be applied broadly to the elements, including consideration of their general physical and chemical properties, their states of matter under familiar conditions, their melting and boiling points, their densities, their crystal structures as solids, and their origins.
General properties.
Several terms are commonly used to characterize the general physical and chemical properties of the chemical elements. A first distinction is between metals, which readily conduct electricity, nonmetals, which do not, and a small group, (the "metalloids"), having intermediate properties and often behaving as semiconductors.
A more refined classification is often shown in colored presentations of the periodic table. This system restricts the terms "metal" and "nonmetal" to only certain of the more broadly defined metals and nonmetals, adding additional terms for certain sets of the more broadly viewed metals and nonmetals. The version of this classification used in the periodic tables presented here includes: actinides, alkali metals, alkaline earth metals, halogens, lanthanides, transition metals, post-transition metals; metalloids, noble gases, polyatomic nonmetals, diatomic nonmetals, and transition metals. In this system, the alkali metals, alkaline earth metals, and transition metals, as well as the lanthanides and the actinides, are special groups of the metals viewed in a broader sense. Similarly, the polyatomic nonmetals, diatomic nonmetals and the noble gases are nonmetals viewed in the broader sense. In some presentations, the halogens are not distinguished, with astatine identified as a metalloid and the others identified as nonmetals.
States of matter.
Another commonly used basic distinction among the elements is their state of matter (phase), whether solid, liquid, or gas, at a selected standard temperature and pressure (STP). Most of the elements are solids at conventional temperatures and atmospheric pressure, while several are gases. Only bromine and mercury are liquids at 0 degrees Celsius (32 degrees Fahrenheit) and normal atmospheric pressure; caesium and gallium are solids at that temperature, but melt at 28.4 °C (83.2 °F) and 29.8 °C (85.6 °F), respectively.
Melting and boiling points.
Melting and boiling points, typically expressed in degrees Celsius at a pressure of one atmosphere, are commonly used in characterizing the various elements. While known for most elements, either or both of these measurements is still undetermined for some of the radioactive elements available in only tiny quantities. Since helium remains a liquid even at absolute zero at atmospheric pressure, it has only a boiling point, and not a melting point, in conventional presentations.
Densities.
The density at a selected standard temperature and pressure (STP) is frequently used in characterizing the elements. Density is often expressed in grams per cubic centimeter (g/cm3). Since several elements are gases at commonly encountered temperatures, their densities are usually stated for their gaseous forms; when liquefied or solidified, the gaseous elements have densities similar to those of the other elements.
When an element has allotropes with different densities, one representative allotrope is typically selected in summary presentations, while densities for each allotrope can be stated where more detail is provided. For example, the three familiar allotropes of carbon (amorphous carbon, graphite, and diamond) have densities of 1.8–2.1, 2.267, and 3.515 g/cm3, respectively.
Crystal structures.
The elements studied to date as solid samples have eight kinds of crystal structures: cubic, body-centered cubic, face-centered cubic, hexagonal, monoclinic, orthorhombic, rhombohedral, and tetragonal. For some of the synthetically produced transuranic elements, available samples have been too small to determine crystal structures.
Occurrence and origin on Earth.
Chemical elements may also be categorized by their origin on Earth, with the first 94 considered naturally occurring, while those with atomic numbers beyond 94 have only been produced artificially as the synthetic products of man-made nuclear reactions.
Of the 94 naturally occurring elements, 84 are considered primordial and either stable or weakly radioactive. The remaining 10 naturally occurring elements possess half lives too short for them to have been present at the beginning of the Solar System, and are therefore considered transient elements. Of these 10 transient elements, 5 (polonium, radon, radium, actinium, and protactinium) are relatively common decay products of thorium, uranium, and plutonium. The remaining 5 transient elements (technetium, promethium, astatine, francium, and neptunium) occur only rarely, as products of rare decay modes or nuclear reaction processes involving uranium or other heavy elements.
Elements with atomic numbers 1 through 40 are all stable, while those with atomic numbers 41 through 82 (except technetium and promethium) are metastable. The half-lives of these metastable "theoretical radionuclides" are so long (at least 100 million times longer than the estimated age of the universe) that their radioactive decay has yet to be detected by experiment. Elements with atomic numbers 83 through 94 are unstable to the point that their radioactive decay can be detected. Four of these elements, bismuth (element 83), thorium (element 90), uranium (element 92), and plutonium (element 94), have one or more isotopes with half-lives long enough to survive as remnants of the explosive stellar nucleosynthesis that produced the heavy elements before the formation of our solar system. For example, at over 1.9 years, over a billion times longer than the current estimated age of the universe, bismuth-209 has the longest known alpha decay half-life of any naturally occurring element. The very heaviest elements (those beyond plutonium, element 94) undergo radioactive decay with short half-lives and do not occur in nature.
The periodic table.
The properties of the chemical elements are often summarized using the periodic table, which powerfully and elegantly organizes the elements by increasing atomic number into rows ("periods") in which the columns ("groups") share recurring ("periodic") physical and chemical properties. The current standard table contains 118 confirmed elements as of 10 April 2010.
Although earlier precursors to this presentation exist, its invention is generally credited to the Russian chemist Dmitri Mendeleev in 1869, who intended the table to illustrate recurring trends in the properties of the elements. The layout of the table has been refined and extended over time as new elements have been discovered and new theoretical models have been developed to explain chemical behavior.
Use of the periodic table is now ubiquitous within the academic discipline of chemistry, providing an extremely useful framework to classify, systematize and compare all the many different forms of chemical behavior. The table has also found wide application in physics, geology, biology, materials science, engineering, agriculture, medicine, nutrition, environmental health, and astronomy. Its principles are especially important in chemical engineering.
Nomenclature and symbols.
The various chemical elements are formally identified by their unique atomic numbers, by their accepted names, and by their symbols.
Atomic numbers.
The known elements have atomic numbers from 1 through 118, conventionally presented as Arabic numerals. Since the elements can be uniquely sequenced by atomic number, conventionally from lowest to highest (as in a periodic table), sets of elements are sometimes specified by such notation as "through", "beyond", or "from ... through", as in "through iron", "beyond uranium", or "from lanthanum through lutetium". The terms "light" and "heavy" are sometimes also used informally to indicate relative atomic numbers (not densities), as in "lighter than carbon" or "heavier than lead", although technically the weight or mass of atoms of an element (their atomic weights or atomic masses) do not always increase monotonically with their atomic numbers.
Element names.
The naming of various substances now known as elements precedes the atomic theory of matter, as names were given locally by various cultures to various minerals, metals, compounds, alloys, mixtures, and other materials, although at the time it was not known which chemicals were elements and which compounds. As they were identified as elements, the existing names for anciently-known elements (e.g., gold, mercury, iron) were kept in most countries. National differences emerged over the names of elements either for convenience, linguistic niceties, or nationalism. For a few illustrative examples: German speakers use "Wasserstoff" (water substance) for "hydrogen", "Sauerstoff" (acid substance) for "oxygen" and "Stickstoff" (smothering substance) for "nitrogen", while English and some romance languages use "sodium" for "natrium" and "potassium" for "kalium", and the French, Italians, Greeks, Portuguese and Poles prefer "azote/azot/azoto" (from roots meaning "no life") for "nitrogen".
For purposes of international communication and trade, the official names of the chemical elements both ancient and more recently recognized are decided by the International Union of Pure and Applied Chemistry (IUPAC), which has decided on a sort of international English language, drawing on traditional English names even when an element's chemical symbol is based on a Latin or other traditional word, for example adopting "gold" rather than "aurum" as the name for the 79th element (Au). IUPAC prefers the British spellings "aluminium" and "caesium" over the U.S. spellings "aluminum" and "cesium", and the U.S. "sulfur" over the British "sulphur". However, elements that are practical to sell in bulk in many countries often still have locally used national names, and countries whose national language does not use the Latin alphabet are likely to use the IUPAC element names.
According to IUPAC, chemical elements are not proper nouns in English; consequently, the full name of an element is not routinely capitalized in English, even if derived from a proper noun, as in californium and einsteinium. Isotope names of chemical elements are also uncapitalized if written out, "e.g.," carbon-12 or uranium-235. Chemical element "symbols" (such as Cf for californium and Es for einsteinium), are always capitalized (see below).
In the second half of the twentieth century, physics laboratories became able to produce nuclei of chemical elements with half-lives too short for an appreciable amount of them to exist at any time. These are also named by IUPAC, which generally adopts the name chosen by the discoverer. This practice can lead to the controversial question of which research group actually discovered an element, a question that delayed the naming of elements with atomic number of 104 and higher for a considerable amount of time. (See element naming controversy).
Precursors of such controversies involved the nationalistic namings of elements in the late 19th century. For example, "lutetium" was named in reference to Paris, France. The Germans were reluctant to relinquish naming rights to the French, often calling it "cassiopeium". Similarly, the British discoverer of "niobium" originally named it "columbium," in reference to the New World. It was used extensively as such by American publications prior to the international standardization.
Chemical symbols.
Specific chemical elements.
Before chemistry became a science, alchemists had designed arcane symbols for both metals and common compounds. These were however used as abbreviations in diagrams or procedures; there was no concept of atoms combining to form molecules. With his advances in the atomic theory of matter, John Dalton devised his own simpler symbols, based on circles, to depict molecules.
The current system of chemical notation was invented by Berzelius. In this typographical system, chemical symbols are not mere abbreviations—though each consists of letters of the Latin alphabet. They are intended as universal symbols for people of all languages and alphabets.
The first of these symbols were intended to be fully universal. Since Latin was the common language of science at that time, they were abbreviations based on the Latin names of metals. Cu comes from Cuprum, Fe comes from Ferrum, Ag from Argentum. The symbols were not followed by a period (full stop) as with abbreviations. Later chemical elements were also assigned unique chemical symbols, based on the name of the element, but not necessarily in English. For example, sodium has the chemical symbol 'Na' after the Latin "natrium". The same applies to "W" (wolfram) for tungsten, "Fe" (ferrum) for iron, "Hg" (hydrargyrum) for mercury, "Sn" (stannum) for tin, "K" (kalium) for potassium, "Au" (aurum) for gold, "Ag" (argentum) for silver, "Pb" (plumbum) for lead, "Cu" (cuprum) for copper, and "Sb" (stibium) for antimony.
Chemical symbols are understood internationally when element names might require translation. There have sometimes been differences in the past. For example, Germans in the past have used "J" (for the alternate name Jod) for iodine, but now use "I" and "Iod".
The first letter of a chemical symbol is always capitalized, as in the preceding examples, and the subsequent letters, if any, are always lower case (small letters). Thus, the symbols for californium or einsteinium are Cf and Es.
General chemical symbols.
There are also symbols in chemical equations for groups of chemical elements, for example in comparative formulas. These are often a single capital letter, and the letters are reserved and not used for names of specific elements. For example, an "X" indicates a variable group (usually a halogen) in a class of compounds, while "R" is a radical, meaning a compound structure such as a hydrocarbon chain. The letter "Q" is reserved for "heat" in a chemical reaction. "Y" is also often used as a general chemical symbol, although it is also the symbol of yttrium. "Z" is also frequently used as a general variable group. "E" is used in organic chemistry to denote an electron-withdrawing group. "L" is used to represent a general ligand in inorganic and organometallic chemistry. "M" is also often used in place of a general metal.
At least two additional, two-letter generic chemical symbols are also in informal usage, "Ln" for any lanthanide element and "An" for any actinide element. "Rg" was formerly used for any rare gas element, but the group of rare gases has now been renamed noble gases and the symbol "Rg" has now been assigned to the element roentgenium.
Isotope symbols.
Isotopes are distinguished by the atomic mass number (total protons and neutrons) for a particular isotope of an element, with this number combined with the pertinent element's symbol. IUPAC prefers that isotope symbols be written in superscript notation when practical, for example 12C and 235U. However, other notations, such as carbon-12 and uranium-235, or C-12 and U-235, are also used.
As a special case, the three naturally occurring isotopes of the element hydrogen are often specified as H for 1H (protium), D for 2H (deuterium), and T for 3H (tritium). This convention is easier to use in chemical equations, replacing the need to write out the mass number for each atom. For example, the formula for heavy water may be written D2O instead of 2H2O.
Origin of the elements.
Only about 4% of the total mass of the universe is made of atoms or ions, and thus represented by chemical elements. This fraction is about 15% of the total matter, with the remainder of the matter (85%) being dark matter. The nature of dark matter is unknown, but it is not composed of atoms of chemical elements because it contains no protons, neutrons, or electrons. (The remaining non-matter part of the mass of the universe is composed of the even more mysterious dark energy).
The universe's 94 naturally occurring chemical elements are thought to have been produced by at least four cosmic processes. Most of the hydrogen and helium in the universe was produced primordially in the first few minutes of the Big Bang. Three recurrently occurring later processes are thought to have produced the remaining elements. Stellar nucleosynthesis, an ongoing process, produces all elements from carbon through iron in atomic number, but little lithium, beryllium, or boron. Elements heavier in atomic number than iron, as heavy as uranium and plutonium, are produced by explosive nucleosynthesis in supernovas and other cataclysmic cosmic events. Cosmic ray spallation (fragmentation) of carbon, nitrogen, and oxygen is important to the production of lithium, beryllium and boron.
During the early phases of the Big Bang, nucleosynthesis of hydrogen nuclei resulted in the production of hydrogen-1 (protium, 1H) and helium-4 (4He), as well as a smaller amount of deuterium (2H) and very minuscule amounts (on the order of 10−10) of lithium and beryllium. Even smaller amounts of boron may have been produced in the Big Bang, since it has been observed in some very old stars, while carbon has not. It is generally agreed that no heavier elements than boron were produced in the Big Bang. As a result, the primordial abundance of atoms (or ions) consisted of roughly 75% 1H, 25% 4He, and 0.01% deuterium, with only tiny traces of lithium, beryllium, and perhaps boron. Subsequent enrichment of galactic halos occurred due to stellar nucleosynthesis and supernova nucleosynthesis. However, the element abundance in intergalactic space can still closely resemble primordial conditions, unless it has been enriched by some means.
On Earth (and elsewhere), trace amounts of various elements continue to be produced from other elements as products of natural transmutation processes. These include some produced by cosmic rays or other nuclear reactions (see cosmogenic and nucleogenic nuclides), and others produced as decay products of long-lived primordial nuclides. For example, trace (but detectable) amounts of carbon-14 (14C) are continually produced in the atmosphere by cosmic rays impacting nitrogen atoms, and argon-40 (40Ar) is continually produced by the decay of primordially occurring but unstable potassium-40 (40K). Also, three primordially occurring but radioactive actinides, thorium, uranium, and plutonium, decay through a series of recurrently produced but unstable radioactive elements such as radium and radon, which are transiently present in any sample of these metals or their ores or compounds. Three other radioactive elements, technetium, promethium, and neptunium, occur only incidentally in natural materials, produced as individual atoms by natural fission of the nuclei of various heavy elements or in other rare nuclear processes.
Human technology has produced various additional elements beyond these first 94, with those through atomic number 118 now known.
Abundance.
The following graph (note log scale) shows the abundance of elements in our solar system. The table shows the twelve most common elements in our galaxy (estimated spectroscopically), as measured in parts per million, by mass. Nearby galaxies that have evolved along similar lines have a corresponding enrichment of elements heavier than hydrogen and helium. The more distant galaxies are being viewed as they appeared in the past, so their abundances of elements appear closer to the primordial mixture. As physical laws and processes appear common throughout the visible universe, however, scientist expect that these galaxies evolved elements in similar abundance.
The abundance of elements in the Solar System is in keeping with their origin from nucleosynthesis in the Big Bang and a number of progenitor supernova stars. Very abundant hydrogen and helium are products of the Big Bang, but the next three elements are rare since they had little time to form in the Big Bang and are not made in stars (they are, however, produced in small quantities by the breakup of heavier elements in interstellar dust, as a result of impact by cosmic rays). Beginning with carbon, elements are produced in stars by buildup from alpha particles (helium nuclei), resulting in an alternatingly larger abundance of elements with even atomic numbers (these are also more stable). In general, such elements up to iron are made in large stars in the process of becoming supernovas. Iron-56 is particularly common, since it is the most stable element that can easily be made from alpha particles (being a product of decay of radioactive nickel-56, ultimately made from 14 helium nuclei). Elements heavier than iron are made in energy-absorbing processes in large stars, and their abundance in the universe (and on Earth) generally decreases with their atomic number.
The abundance of the chemical elements on Earth varies from air to crust to ocean, and in various types of life. The abundance of elements in Earth's crust differs from that in the Solar system (as seen in the Sun and heavy planets like Jupiter) mainly in selective loss of the very lightest elements (hydrogen and helium) and also volatile neon, carbon (as hydrocarbons), nitrogen and sulfur, as a result of solar heating in the early formation of the solar system. Oxygen, the most abundant Earth element by mass, is retained on Earth by combination with silicon. Aluminum at 8% by mass is more common in the Earth's crust than in the universe and solar system, but the composition of the far more bulky mantle, which has magnesium and iron in place of aluminum (which occurs there only at 2% of mass) more closely mirrors the elemental composition of the solar system, save for the noted loss of volatile elements to space, and loss of iron which has migrated to the Earth's core.
The composition of the human body, by contrast, more closely follows the composition of seawater—save that the human body has additional stores of carbon and nitrogen necessary to form the proteins and nucleic acids, together with phosphorus in the nucleic acids and energy transfer molecule adenosine triphosphate (ATP) that occurs in the cells of all living organisms. Certain kinds of organisms require particular additional elements, for example the magnesium in chlorophyll in green plants, the calcium in mollusc shells, or the iron in the hemoglobin in vertebrate animals' red blood cells.
History.
Evolving definitions.
The concept of an "element" as an undivisible substance has developed through three major historical phases: Classical definitions (such as those of the ancient Greeks), chemical definitions, and atomic definitions.
Classical definitions.
Ancient philosophy posited a set of classical elements to explain observed patterns in nature. These "elements" originally referred to "earth", "water", "air" and "fire" rather than the chemical elements of modern science.
The term 'elements' ("stoicheia") was first used by the Greek philosopher Plato in about 360 BCE in his dialogue Timaeus, which includes a discussion of the composition of inorganic and organic bodies and is a speculative treatise on chemistry. Plato believed the elements introduced a century earlier by Empedocles were composed of small polyhedral forms: tetrahedron (fire), octahedron (air), icosahedron (water), and cube (earth).
Aristotle, c. 350 BCE, also used the term "stoicheia" and added a fifth element called aether, which formed the heavens. Aristotle defined an element as:
Chemical definitions.
In 1661, Robert Boyle proposed his theory of corpuscularism which favoured the analysis of matter as constituted by irreducible units of matter (atoms) and, choosing to side with neither Aristotle's view of the four elements nor Paracelsus' view of three fundamental elements, left open the question of the number of elements. The first modern list of chemical elements was given in Antoine Lavoisier's 1789 "Elements of Chemistry", which contained thirty-three elements, including light and caloric. By 1818, Jöns Jakob Berzelius had determined atomic weights for forty-five of the forty-nine then-accepted elements. Dmitri Mendeleev had sixty-six elements in his periodic table of 1869.
From Boyle until the early 20th century, an element was defined as a pure substance that could not be decomposed into any simpler substance. Put another way, a chemical element cannot be transformed into other chemical elements by chemical processes. Elements during this time were generally distinguished by their atomic weights, a property measurable with fair accuracy by available analytical techniques.
Atomic definitions.
The 1913 discovery by English physicist Henry Moseley that the nuclear charge is the physical basis for an atom's atomic number, further refined when the nature of protons and neutrons became appreciated, eventually led to the current definition of an element based on atomic number (number of protons per atomic nucleus). The use of atomic numbers, rather than atomic weights, to distinguish elements has greater predictive value (since these numbers are integers), and also resolves some ambiguities in the chemistry-based view due to varying properties of isotopes and allotropes within the same element. Currently, IUPAC defines an element to exist if it has isotopes with a lifetime longer than the 10−14 seconds it takes the nucleus to form an electronic cloud.
By 1914, seventy-two elements were known, all naturally occurring. The remaining naturally occurring elements were discovered or isolated in subsequent decades, and various additional elements have also been produced synthetically, with much of that work pioneered by Glenn T. Seaborg. In 1955, element 101 was discovered and named mendelevium in honor of D.I. Mendeleev, the first to arrange the elements in a periodic manner. Most recently, the synthesis of element 118 was reported in October 2006, and the synthesis of element 117 was reported in April 2010.
Discovery and recognition of various elements.
Ten materials familiar to various prehistoric cultures are now known to be chemical elements: Carbon, copper, gold, iron, lead, mercury, silver, sulfur, tin, and zinc. Three additional materials now accepted as elements, arsenic, antimony, and bismuth, were recognized as distinct substances prior to 1500 AD. Phosphorus, cobalt, and platinum were isolated before 1750.
Most of the remaining naturally occurring chemical elements were identified and characterized by 1900, including:
Elements isolated or produced since 1900 include:
Recently discovered elements.
The first transuranium element (element with atomic number greater than 92) discovered was neptunium in 1940. Since 1999 claims for the discovery of new elements have been considered by the IUPAC/IUPAP Joint Working Party. As of January 2016, all 118 elements have been confirmed as discovered by IUPAC. The discovery of element 112 was acknowledged in 2009, and the name "copernicium" and the atomic symbol "Cn" were suggested for it. The name and symbol were officially endorsed by IUPAC on 19 February 2010. The heaviest element that is believed to have been synthesized to date is element 118, ununoctium, on 9 October 2006, by the Flerov Laboratory of Nuclear Reactions in Dubna, Russia. Element 117 was the latest element claimed to be discovered, in 2009. IUPAC officially recognized flerovium and livermorium, elements 114 and 116, in June 2011 and approved their names in May 2012. In December 2015, IUPAC further recognized elements 113, 115, 117 and 118, and the official names of those elements will be decided thereafter.
List of the 118 known chemical elements.
The following sortable table includes the 118 known chemical elements, with the names linking to the "Wikipedia" articles on each.

</doc>
<doc id="5661" url="https://en.wikipedia.org/wiki?curid=5661" title="Centime">
Centime

Centime (from ) is French for "cent", and is used in English as the name of the fraction currency in several Francophone countries (including Switzerland, Algeria, Belgium, Morocco and France).
In France the usage of "centime" goes back to the introduction of the decimal monetary system under Napoleon. This system aimed at replacing non-decimal fractions of older coins. A five-centime coin was known as a "sou", i.e. a solidus or shilling.
In Francophone Canada 1/100 of a Canadian Dollar is officially known as a "cent" (pronounced /sɛnt/) in both English and French. However, in practice, the female form of "cent", "cenne" (pronounced /sɛn/) has completely replaced the official "cent". Spoken and written use of the official masculine form of "cent" in Francophone Canada is exceptionally uncommon.
In the Canadian French vernacular "sou", "sou noir" (noir is the singular masculine form of the word black in French), "cenne", and "cenne noire" (noire is the singular feminine form of the word black in French) are all widely known, used, and accepted monikers when referring to either 1/100 of a Canadian Dollar or the 1¢ coin (colloquially known as a "penny" in North American English).
Subdivision of euro: cent or centime?
In the European community "cent" is the official name for one hundredth of a euro. However, in French-speaking countries the word "centime " is the preferred term. Indeed, the Superior Council of the French language of Belgium recommended in 2001 the use of "centime", since "cent" is also the French word for "hundred". An analogous decision was published in the "Journal officiel" in France (December 2, 1997).
In Morocco, dirhams are divided into 100 "centime"s and one may find prices in the country quoted in "centime"s rather than in dirhams. Sometimes "centime"s are known as francs or in former Spanish areas, pesetas.
Usage.
A centime is one-hundredth of the following basic monetary units:

</doc>
<doc id="5662" url="https://en.wikipedia.org/wiki?curid=5662" title="Calendar year">
Calendar year

Generally speaking, a calendar year begins on the New Year's Day of the given calendar system and ends on the day before the following New Year's Day, and thus consists of a whole number of days. To reconcile the calendar year with the astronomical cycle (which has a fractional number of days) certain years contain extra days.
The Gregorian year, which is in use in most of the world, begins on January 1 and ends on December 31. It has a length of 365 days in an ordinary year, with 8,760 hours, 525,600 minutes, and 31,536,000 seconds; but 366 days in a leap year, with 8,784 hours, 527,040 minutes, and 31,622,400 seconds. With 97 leap years every 400 years, the year has an average length of 365.2425 days. Other formula-based calendars can have lengths which are further out of step with the solar cycle: for example, the Julian calendar has an average length of 365.25 days, and the Hebrew calendar has an average length of 365.2468 days.
The astronomer's mean tropical year which is averaged over equinoxes and solstices is currently 365.24219 days, slightly shorter than the average length of the year in most calendars, but the astronomer's value changes over time, so William Herschel's suggested correction to the Gregorian calendar may become unnecessary by the year 4000.
Quarters.
The calendar year can be divided into 4 quarters, often abbreviated Q1, Q2, Q3 and Q4.

</doc>
<doc id="5663" url="https://en.wikipedia.org/wiki?curid=5663" title="CFA franc">
CFA franc

The CFA franc (in French: "franc CFA" , or colloquially "franc") is the name of two currencies used in Africa which are guaranteed by the French treasury. The two CFA franc currencies are the West African CFA franc and the Central African CFA franc. Although theoretically separate, the two CFA franc currencies are effectively interchangeable.
Both CFA francs currently have a fixed exchange rate to the euro: 100 CFA francs = 1 former French (nouveau) franc = 0.152449 euro; or 1 euro = 655.957 CFA francs exactly.
Although Central African CFA francs and West African CFA francs have always been at parity and have therefore always had the same monetary value against other currencies, they are in principle separate currencies. They could theoretically have different values from any moment if one of the two CFA monetary authorities, or France, decided it. Therefore, West African CFA coins and banknotes are "theoretically" not accepted in countries using Central African CFA francs, and vice versa. However, in practice, the permanent parity of the two CFA franc currencies is widely assumed.
Usage.
CFA francs are used in fourteen countries: twelve formerly French-ruled nations in West and Central Africa, as well as in Guinea-Bissau (a former Portuguese colony) and in Equatorial Guinea (a former Spanish colony). These fourteen countries have a combined population of 147.5 million people (as of 2013), and a combined GDP of US$166.6 billion (as of 2012). The ISO currency codes are XAF for the Central African CFA franc and XOF for the West African CFA franc.
Criticism.
The currency has been criticized for making economic planning for the developing countries of French West-Africa all but impossible since the CFA's value is pegged to the euro (whose monetary policy is set by the European Central Bank). Others disagree and argue that the CFA "helps stabilize the national currencies of Franc Zone member-countries and greatly facilitates the flow of exports and imports between France and the member-countries." The European Union's own assessment of the CFA's link to the euro, carried out in 2008, noted that "benefits from economic integration within each of the two monetary unions of the CFA franc zone, and even more so between them, remained remarkably low" but that "the peg to the French franc and, since 1999, to the euro as exchange rate anchor is usually found to have had favourable effects in the region in terms of macroeconomic stability.
Name.
Between 1945 and 1958, CFA stood for ' ("French colonies of Africa"); then for ' ("French Community of Africa") between 1958 (establishment of the French Fifth Republic) and the independence of these African countries at the beginning of the 1960s. Since independence, CFA is taken to mean "" (African Financial Community), but in actual use, the term can have two meanings (see Institutions below).
History.
Creation.
The CFA franc was created on 26 December 1945, along with the CFP franc. The reason for their creation was the weakness of the French franc immediately after World War II. When France ratified the Bretton Woods Agreement in December 1945, the French franc was devalued in order to set a fixed exchange rate with the US dollar. New currencies were created in the French colonies to spare them the strong devaluation, thereby facilitating exports to France. French officials presented the decision as an act of generosity. René Pleven, the French minister of finance, was quoted as saying:
Exchange rate.
The CFA franc was created with a fixed exchange rate versus the French franc. This exchange rate was changed only twice: in 1948 and in 1994.
Exchange rate:
The 1960 and 1999 events were merely changes in the currency in use in France: the relative value of the CFA franc versus the French franc/euro changed only in 1948 and 1994.
The value of the CFA franc has been widely criticized as being too high, which many economists believe favours the urban elite of the African countries, who can buy imported manufactured goods cheaply at the expense of farmers who cannot easily export agricultural products. The devaluation of 1994 was an attempt to reduce these imbalances.
Changes in countries using the franc.
Over time, the number of countries and territories using the CFA franc has changed as some countries began introducing their own separate currencies. A couple of nations in West Africa have also chosen to adopt the CFA franc since its introduction, despite the fact that they were never French colonies.
European Monetary Union.
In 1998, in anticipation of Economic and Monetary Union of the European Union, the Council of the European Union addressed the monetary agreements France has with the CFA Zone and Comoros and ruled that:
Institutions.
Strictly speaking, there are actually two different currencies called the CFA franc: the West African CFA franc (ISO 4217 currency code XOF), and the Central Africa CFA franc (ISO 4217 currency code XAF). They are distinguished in French by the meaning of the abbreviation CFA. These two CFA francs have the same exchange rate with the euro (1 euro = 655.957 XOF = 655.957 XAF), and they are both guaranteed by the French treasury (), but the West African CFA franc cannot be used in Central African countries, and the Central Africa CFA franc cannot be used in West African countries.
West African.
The West African CFA franc (XOF) is just known in French as the , where CFA stands for "Communauté financière d'Afrique" ("Financial Community of Africa") or ("African Financial Community"). It is issued by the BCEAO (, i.e., "Central Bank of the West African States"), located in Dakar, Senegal, for the eight countries of the UEMOA (, i.e., "West African Economic and Monetary Union"):
These eight countries have a combined population of 102.5 million people (as of 2013), and a combined GDP of US$78.4 billion (as of 2012).
Central African.
The Central Africa CFA franc (XAF) is known in French as the , where CFA stands for ("Financial Cooperation in Central Africa"). It is issued by the BEAC (, i.e., "Bank of the Central African States"), located in Yaoundé, Cameroon, for the six countries of the CEMAC (, i.e., "Economic and Monetary Community of Central Africa"):
These six countries have a combined population of 45.0 million people (as of 2013), and a combined GDP of US$88.2 billion (as of 2012).
In 1975, Central African CFA banknotes were issued with an obverse unique to each participating country, and common reverse, in a fashion similar to euro coins.
Equatorial Guinea, the only former Spanish colony in the zone, adopted the CFA in 1984.
See also.
General:

</doc>
<doc id="5664" url="https://en.wikipedia.org/wiki?curid=5664" title="Consciousness">
Consciousness

Consciousness is the state or quality of awareness, or, of being aware of an external object or something within oneself. It has been defined as: sentience, awareness, subjectivity, the ability to experience or to feel, wakefulness, having a sense of selfhood, and the executive control system of the mind. Despite the difficulty in definition, many philosophers believe that there is a broadly shared underlying intuition about what consciousness is. As Max Velmans and Susan Schneider wrote in "The Blackwell Companion to Consciousness": "Anything that we are aware of at a given moment forms part of our consciousness, making conscious experience at once the most familiar and most mysterious aspect of our lives."
Western philosophers, since the time of Descartes and Locke, have struggled to comprehend the nature of consciousness and pin down its essential properties. Issues of concern in the philosophy of consciousness include whether the concept is fundamentally coherent; whether consciousness can ever be explained mechanistically; whether non-human consciousness exists and if so how can it be recognized; how consciousness relates to language; whether consciousness can be understood in a way that does not require a dualistic distinction between mental and physical states or properties; and whether it may ever be possible for computing machines like computers or robots to be conscious, a topic studied in the field of artificial intelligence.
Thanks to recent developments in technology, consciousness has become a significant topic of research in psychology, neuropsychology and neuroscience within the past few decades. The primary focus is on understanding what it means biologically and psychologically for information to be present in consciousness—that is, on determining the neural and psychological correlates of consciousness. The majority of experimental studies assess consciousness by asking human subjects for a verbal report of their experiences (e.g., "tell me if you notice anything when I do this"). Issues of interest include phenomena such as subliminal perception, blindsight, denial of impairment, and altered states of consciousness produced by alcohol and other drugs, or spiritual or meditative techniques.
In medicine, consciousness is assessed by observing a patient's arousal and responsiveness, and can be seen as a continuum of states ranging from full alertness and comprehension, through disorientation, delirium, loss of meaningful communication, and finally loss of movement in response to painful stimuli. Issues of practical concern include how the presence of consciousness can be assessed in severely ill, comatose, or anesthetized people, and how to treat conditions in which consciousness is impaired or disrupted.
Etymology and early history.
The origin of the modern concept of consciousness is often attributed to John Locke's "Essay Concerning Human Understanding", published in 1690. Locke defined consciousness as "the perception of what passes in a man's own mind". His essay influenced the 18th-century view of consciousness, and his definition appeared in Samuel Johnson's celebrated "Dictionary" (1755).
"Consciousness" (French: "conscience") is also defined in the 1753 volume of Diderot and d'Alembert's Encyclopédie, as "the opinion or internal feeling that we ourselves have from what we do." 
The earliest English language uses of "conscious" and "consciousness" date back, however, to the 1500s. The English word "conscious" originally derived from the Latin "conscius" ("con-" "together" and "scio" "to know"), but the Latin word did not have the same meaning as our word—it meant "knowing with", in other words "having joint or common knowledge with another". There were, however, many occurrences in Latin writings of the phrase "conscius sibi", which translates literally as "knowing with oneself", or in other words "sharing knowledge with oneself about something". This phrase had the figurative meaning of "knowing that one knows", as the modern English word "conscious" does. In its earliest uses in the 1500s, the English word "conscious" retained the meaning of the Latin "conscius". For example, Thomas Hobbes in "Leviathan" wrote: "Where two, or more men, know of one and the same fact, they are said to be Conscious of it one to another." The Latin phrase "conscius sibi", whose meaning was more closely related to the current concept of consciousness, was rendered in English as "conscious to oneself" or "conscious unto oneself". For example, Archbishop Ussher wrote in 1613 of "being so conscious unto myself of my great weakness". Locke's definition from 1690 illustrates that a gradual shift in meaning had taken place.
A related word was "conscientia", which primarily means moral conscience. In the literal sense, "conscientia" means knowledge-with, that is, shared knowledge. The word first appears in Latin juridical texts by writers such as Cicero. Here, "conscientia" is the knowledge that a witness has of the deed of someone else. René Descartes (1596–1650) is generally taken to be the first philosopher to use "conscientia" in a way that does not fit this traditional meaning. Descartes used "conscientia" the way modern speakers would use "conscience". In "Search after Truth" ("", Amsterdam 1701) he says "conscience or internal testimony" ("conscientiâ, vel interno testimonio").
In the dictionary.
The dictionary meaning of the word "consciousness" extends through several centuries and associated cognate meanings which have ranged from formal definitions to somewhat more skeptical definitions. One formal definition indicating the range of these cognate meanings is given in "Webster's Third New International Dictionary" stating that "consciousness" is: "(1) "a." awareness or perception of an inward psychological or spiritual fact: intuitively perceived knowledge of something in one's inner self. "b." inward awareness of an external object, state, or fact. "c." concerned awareness: INTEREST, CONCERN -- often used with an attributive noun. (2): the state or activity that is characterized by sensation, emotion, volition, or thought: mind in the broadest possible sense: something in nature that is distinguished from the physical. (3): the totality in psychology of sensations, perceptions, ideas, attitudes and feelings of which an individual or a group is aware at any given time or within a particular time span -- compare STREAM OF CONSCIOUSNESS."
Philosophy of mind.
The philosophy of mind has given rise to many stances regarding consciousness. The "Routledge Encyclopedia of Philosophy" in 1998 defines consciousness as follows:
In a more skeptical definition of "consciousness", Stuart Sutherland has exemplified some of the difficulties in fully ascertaining all of its cognate meanings in his entry for the 1989 version of the "Macmillan Dictionary of Psychology":
Most writers on the philosophy of consciousness have been concerned to defend a particular point of view, and have organized their material accordingly. For surveys, the most common approach is to follow a historical path by associating stances with the philosophers who are most strongly associated with them, for example Descartes, Locke, Kant, etc. An alternative is to organize philosophical stances according to basic issues.
The coherence of the concept.
Philosophers and non-philosophers differ in their intuitions about what consciousness is. While most people have a strong intuition for the existence of what they refer to as consciousness, skeptics argue that this intuition is false, either because the concept of consciousness is intrinsically incoherent, or because our intuitions about it are based in illusions. Gilbert Ryle, for example, argued that traditional understanding of consciousness depends on a Cartesian dualist outlook that improperly distinguishes between mind and body, or between mind and world. He proposed that we speak not of minds, bodies, and the world, but of individuals, or persons, acting in the world. Thus, by speaking of "consciousness" we end up misleading ourselves by thinking that there is any sort of thing as consciousness separated from behavioral and linguistic understandings. More generally, many philosophers and scientists have been unhappy about the difficulty of producing a definition that does not involve circularity or fuzziness.
Types of consciousness.
Many philosophers have argued that consciousness is a unitary concept that is understood intuitively by the majority of people in spite of the difficulty in defining it. Others, though, have argued that the level of disagreement about the meaning of the word indicates that it either means different things to different people (for instance, the objective versus subjective aspects of consciousness), or else is an umbrella term encompassing a variety of distinct meanings with no simple element in common.
Ned Block proposed a distinction between two types of consciousness that he called "phenomenal" (P-consciousness) and "access" (A-consciousness). P-consciousness, according to Block, is simply raw experience: it is moving, colored forms, sounds, sensations, emotions and feelings with our bodies and responses at the center. These experiences, considered independently of any impact on behavior, are called "qualia". A-consciousness, on the other hand, is the phenomenon whereby information in our minds is accessible for verbal report, reasoning, and the control of behavior. So, when we perceive, information about what we perceive is access conscious; when we introspect, information about our thoughts is access conscious; when we remember, information about the past is access conscious, and so on. Although some philosophers, such as Daniel Dennett, have disputed the validity of this distinction, others have broadly accepted it. David Chalmers has argued that A-consciousness can in principle be understood in mechanistic terms, but that understanding P-consciousness is much more challenging: he calls this the "hard problem of consciousness".
Some philosophers believe that Block's two types of consciousness are not the end of the story. William Lycan, for example, argued in his book "Consciousness and Experience" that at least eight clearly distinct types of consciousness can be identified (organism consciousness; control consciousness; consciousness "of"; state/event consciousness; reportability; introspective consciousness; subjective consciousness; self-consciousness)—and that even this list omits several more obscure forms.
There is also debate in whether or not a-consciousness and p-consciousness always co-exist or if they can exist separately. Although p-consciousness without a-consciousness is more widely accepted, there have been some hypothetical examples of A without P. Block for instance suggests the case of a “zombie” that is computationally identical to a person but without any subjectivity. However, he remains somewhat skeptical concluding "I don’t know whether there are any actual cases of A-consciousness without P-consciousness, but I hope I have illustrated their conceptual possibility." 
Mind–body problem.
Mental processes (such as consciousness) and physical processes (such as brain events) seem to be correlated: but what is the basis of this connection and correlation between what seem to be two very different kinds of processes?
The first influential philosopher to discuss this question specifically was Descartes, and the answer he gave is known as Cartesian dualism. Descartes proposed that consciousness resides within an immaterial domain he called res cogitans (the realm of thought), in contrast to the domain of material things, which he called res extensa (the realm of extension). He suggested that the interaction between these two domains occurs inside the brain, perhaps in a small midline structure called the pineal gland.
Although it is widely accepted that Descartes explained the problem cogently, few later philosophers have been happy with his solution, and his ideas about the pineal gland have especially been ridiculed. However, no alternative solution has gained general acceptance. Proposed solutions can be divided broadly into two categories: dualist solutions that maintain Descartes' rigid distinction between the realm of consciousness and the realm of matter but give different answers for how the two realms relate to each other; and monist solutions that maintain that there is really only one realm of being, of which consciousness and matter are both aspects. Each of these categories itself contains numerous variants. The two main types of dualism are substance dualism (which holds that the mind is formed of a distinct type of substance not governed by the laws of physics) and property dualism (which holds that the laws of physics are universally valid but cannot be used to explain the mind). The three main types of monism are physicalism (which holds that the mind consists of matter organized in a particular way), idealism (which holds that only thought or experience truly exists, and matter is merely an illusion), and neutral monism (which holds that both mind and matter are aspects of a distinct essence that is itself identical to neither of them). There are also, however, a large number of idiosyncratic theories that cannot cleanly be assigned to any of these schools of thought.
Since the dawn of Newtonian science with its vision of simple mechanical principles governing the entire universe, some philosophers have been tempted by the idea that consciousness could be explained in purely physical terms. The first influential writer to propose such an idea explicitly was Julien Offray de La Mettrie, in his book "Man a Machine" ("L'homme machine"). His arguments, however, were very abstract. The most influential modern physical theories of consciousness are based on psychology and neuroscience. Theories proposed by neuroscientists such as Gerald Edelman and Antonio Damasio, and by philosophers such as Daniel Dennett, seek to explain consciousness in terms of neural events occurring within the brain. Many other neuroscientists, such as Christof Koch, have explored the neural basis of consciousness without attempting to frame all-encompassing global theories. At the same time, computer scientists working in the field of artificial intelligence have pursued the goal of creating digital computer programs that can simulate or embody consciousness.
A few theoretical physicists have argued that classical physics is intrinsically incapable of explaining the holistic aspects of consciousness, but that quantum theory may provide the missing ingredients. Several theorists have therefore proposed quantum mind (QM) theories of consciousness. Notable theories falling into this category include the holonomic brain theory of Karl Pribram and David Bohm, and the Orch-OR theory formulated by Stuart Hameroff and Roger Penrose. Some of these QM theories offer descriptions of phenomenal consciousness, as well as QM interpretations of access consciousness. None of the quantum mechanical theories has been confirmed by experiment. Recent publications by G. Guerreshi, J. Cia, S. Popescu, and H. Briegel could falsify proposals such as those of Hameroff, which rely on quantum entanglement in protein. At the present time many scientists and philosophers consider the arguments for an important role of quantum phenomena to be unconvincing.
Apart from the general question of the "hard problem" of consciousness, roughly speaking, the question of how mental experience arises from a physical basis, a more specialized question is how to square the subjective notion that we are in control of our decisions (at least in some small measure) with the customary view of causality that subsequent events are caused by prior events. The topic of free will is the philosophical and scientific examination of this conundrum.
Problem of other minds.
Many philosophers consider experience to be the essence of consciousness, and believe that experience can only fully be known from the inside, subjectively. But if consciousness is subjective and not visible from the outside, why do the vast majority of people believe that other people are conscious, but rocks and trees are not? This is called the problem of other minds. It is particularly acute for people who believe in the possibility of philosophical zombies, that is, people who think it is possible in principle to have an entity that is physically indistinguishable from a human being and behaves like a human being in every way but nevertheless lacks consciousness. Related issues have also been studied extensively by Greg Littmann of the University of Illinois. and Colin Allen a professor at Indiana University regarding the literature and research studying artificial intelligence in androids.
The most commonly given answer is that we attribute consciousness to other people because we see that they resemble us in appearance and behavior; we reason that if they look like us and act like us, they must be like us in other ways, including having experiences of the sort that we do. There are, however, a variety of problems with that explanation. For one thing, it seems to violate the principle of parsimony, by postulating an invisible entity that is not necessary to explain what we observe. Some philosophers, such as Daniel Dennett in an essay titled "The Unimagined Preposterousness of Zombies", argue that people who give this explanation do not really understand what they are saying. More broadly, philosophers who do not accept the possibility of zombies generally believe that consciousness is reflected in behavior (including verbal behavior), and that we attribute consciousness on the basis of behavior. A more straightforward way of saying this is that we attribute experiences to people because of what they can "do", including the fact that they can tell us about their experiences.
Animal consciousness.
The topic of animal consciousness is beset by a number of difficulties. It poses the problem of other minds in an especially severe form, because non-human animals, lacking the ability to express human language, cannot tell us about their experiences. Also, it is difficult to reason objectively about the question, because a denial that an animal is conscious is often taken to imply that it does not feel, its life has no value, and that harming it is not morally wrong. Descartes, for example, has sometimes been blamed for mistreatment of animals due to the fact that he believed only humans have a non-physical mind. Most people have a strong intuition that some animals, such as cats and dogs, are conscious, while others, such as insects, are not; but the sources of this intuition are not obvious, and are often based on personal interactions with pets and other animals they have observed.
Philosophers who consider subjective experience the essence of consciousness also generally believe, as a correlate, that the existence and nature of animal consciousness can never rigorously be known. Thomas Nagel spelled out this point of view in an influential essay titled "What Is it Like to Be a Bat?". He said that an organism is conscious "if and only if there is something that it is like to be that organism — something it is like "for" the organism"; and he argued that no matter how much we know about an animal's brain and behavior, we can never really put ourselves into the mind of the animal and experience its world in the way it does itself. Other thinkers, such as Douglas Hofstadter, dismiss this argument as incoherent. Several psychologists and ethologists have argued for the existence of animal consciousness by describing a range of behaviors that appear to show animals holding beliefs about things they cannot directly perceive — Donald Griffin's 2001 book "Animal Minds" reviews a substantial portion of the evidence.
On July 7, 2012, eminent scientists from different branches of neuroscience gathered at the University of Cambridge to celebrate the Francis Crick Memorial Conference, which deals with consciousness in humans and pre-linguistic consciousness in nonhuman animals. After the conference, they signed in the presence of Stephen Hawking, the 'Cambridge Declaration on Consciousness', which summarizes the most important findings of the survey:
"We decided to reach a consensus and make a statement directed to the public that is not scientific. It's obvious to everyone in this room that animals have consciousness, but it is not obvious to the rest of the world. It is not obvious to the rest of the Western world or the Far East. It is not obvious to the society."
"Convergent evidence indicates that non-human animals [...], including all mammals and birds, and other creatures, [...] have the necessary neural substrates of consciousness and the capacity to exhibit intentional behaviors."
Artifact consciousness.
The idea of an artifact made conscious is an ancient theme of mythology, appearing for example in the Greek myth of Pygmalion, who carved a statue that was magically brought to life, and in medieval Jewish stories of the Golem, a magically animated homunculus built of clay. However, the possibility of actually constructing a conscious machine was probably first discussed by Ada Lovelace, in a set of notes written in 1842 about the Analytical Engine invented by Charles Babbage, a precursor (never built) to modern electronic computers. Lovelace was essentially dismissive of the idea that a machine such as the Analytical Engine could think in a humanlike way. She wrote:
One of the most influential contributions to this question was an essay written in 1950 by pioneering computer scientist Alan Turing, titled "Computing Machinery and Intelligence". Turing disavowed any interest in terminology, saying that even "Can machines think?" is too loaded with spurious connotations to be meaningful; but he proposed to replace all such questions with a specific operational test, which has become known as the Turing test. To pass the test, a computer must be able to imitate a human well enough to fool interrogators. In his essay Turing discussed a variety of possible objections, and presented a counterargument to each of them. The Turing test is commonly cited in discussions of artificial intelligence as a proposed criterion for machine consciousness; it has provoked a great deal of philosophical debate. For example, Daniel Dennett and Douglas Hofstadter argue that anything capable of passing the Turing test is necessarily conscious, while David Chalmers argues that a philosophical zombie could pass the test, yet fail to be conscious. A third group of scholars have argued that with technological growth once machines begin to display any substantial signs of human-like behavior then the dichotomy (of human consciousness compared to human-like consciousness) becomes passé and issues of machine autonomy begin to prevail even as observed in its nascent form within contemporary industry and technology.
In a lively exchange over what has come to be referred to as "the Chinese room argument", John Searle sought to refute the claim of proponents of what he calls "strong artificial intelligence (AI)" that a computer program can be conscious, though he does agree with advocates of "weak AI" that computer programs can be formatted to "simulate" conscious states. His own view is that consciousness has subjective, first-person causal powers by being essentially intentional due simply to the way human brains function biologically; conscious persons can perform computations, but consciousness is not inherently computational the way computer programs are. To make a Turing machine that speaks Chinese, Searle imagines a room with one monolingual English speaker (Searle himself, in fact), a book that designates a combination of Chinese symbols to be output paired with Chinese symbol input, and boxes filled with Chinese symbols. In this case, the English speaker is acting as a computer and the rulebook as a program. Searle argues that with such a machine, he would be able to process the inputs to outputs perfectly without having any understanding of Chinese, nor having any idea what the questions and answers could possibly mean. If the experiment were done in English, since Searle knows English, he would be able to take questions and give answers without any algorithms for English questions, and he would be effectively aware of what was being said and the purposes it might serve. Searle would pass the Turing test of answering the questions in both languages, but he is only conscious of what he is doing when he speaks English. Another way of putting the argument is to say that computer programs can pass the Turing test for processing the syntax of a language, but that the syntax cannot lead to semantic meaning in the way strong AI advocates hoped.
In the literature concerning artificial intelligence, Searle's essay has been second only to Turing's in the volume of debate it has generated. Searle himself was vague about what extra ingredients it would take to make a machine conscious: all he proposed was that what was needed was "causal powers" of the sort that the brain has and that computers lack. But other thinkers sympathetic to his basic argument have suggested that the necessary (though perhaps still not sufficient) extra conditions may include the ability to pass not just the verbal version of the Turing test, but the robotic version, which requires grounding the robot's words in the robot's sensorimotor capacity to categorize and interact with the things in the world that its words are about, Turing-indistinguishably from a real person. Turing-scale robotics is an empirical branch of research on embodied cognition and situated cognition.
Scientific study.
For many decades, consciousness as a research topic was avoided by the majority of mainstream scientists, because of a general feeling that a phenomenon defined in subjective terms could not properly be studied using objective experimental methods. In 1975 George Mandler published an influential psychological study which distinguished between slow, serial, and limited conscious processes and fast, parallel and extensive unconscious ones. Starting in the 1980s, an expanding community of neuroscientists and psychologists have associated themselves with a field called "Consciousness Studies", giving rise to a stream of experimental work published in books, journals such as "Consciousness and Cognition", "Frontiers in Consciousness Research", and the "Journal of Consciousness Studies", along with regular conferences organized by groups such as the Association for the Scientific Study of Consciousness.
Modern medical and psychological investigations into consciousness are based on psychological experiments (including, for example, the investigation of priming effects using subliminal stimuli), and on case studies of alterations in consciousness produced by trauma, illness, or drugs. Broadly viewed, scientific approaches are based on two core concepts. The first identifies the content of consciousness with the experiences that are reported by human subjects; the second makes use of the concept of consciousness that has been developed by neurologists and other medical professionals who deal with patients whose behavior is impaired. In either case, the ultimate goals are to develop techniques for assessing consciousness objectively in humans as well as other animals, and to understand the neural and psychological mechanisms that underlie it.
Measurement.
Experimental research on consciousness presents special difficulties, due to the lack of a universally accepted operational definition. In the majority of experiments that are specifically about consciousness, the subjects are human, and the criterion used is verbal report: in other words, subjects are asked to describe their experiences, and their descriptions are treated as observations of the contents of consciousness. For example, subjects who stare continuously at a Necker cube usually report that they experience it "flipping" between two 3D configurations, even though the stimulus itself remains the same. The objective is to understand the relationship between the conscious awareness of stimuli (as indicated by verbal report) and the effects the stimuli have on brain activity and behavior. In several paradigms, such as the technique of response priming, the behavior of subjects is clearly influenced by stimuli for which they report no awareness.
Verbal report is widely considered to be the most reliable indicator of consciousness, but it raises a number of issues. For one thing, if verbal reports are treated as observations, akin to observations in other branches of science, then the possibility arises that they may contain errors—but it is difficult to make sense of the idea that subjects could be wrong about their own experiences, and even more difficult to see how such an error could be detected. Daniel Dennett has argued for an approach he calls heterophenomenology, which means treating verbal reports as stories that may or may not be true, but his ideas about how to do this have not been widely adopted. Another issue with verbal report as a criterion is that it restricts the field of study to humans who have language: this approach cannot be used to study consciousness in other species, pre-linguistic children, or people with types of brain damage that impair language. As a third issue, philosophers who dispute the validity of the Turing test may feel that it is possible, at least in principle, for verbal report to be dissociated from consciousness entirely: a philosophical zombie may give detailed verbal reports of awareness in the absence of any genuine awareness.
Although verbal report is in practice the "gold standard" for ascribing consciousness, it is not the only possible criterion. In medicine, consciousness is assessed as a combination of verbal behavior, arousal, brain activity and purposeful movement. The last three of these can be used as indicators of consciousness when verbal behavior is absent. The scientific literature regarding the neural bases of arousal and purposeful movement is very extensive. Their reliability as indicators of consciousness is disputed, however, due to numerous studies showing that alert human subjects can be induced to behave purposefully in a variety of ways in spite of reporting a complete lack of awareness. Studies of the neuroscience of free will have also shown that the experiences that people report when they behave purposefully sometimes do not correspond to their actual behaviors or to the patterns of electrical activity recorded from their brains.
Another approach applies specifically to the study of self-awareness, that is, the ability to distinguish oneself from others. In the 1970s Gordon Gallup developed an operational test for self-awareness, known as the mirror test. The test examines whether animals are able to differentiate between seeing themselves in a mirror versus seeing other animals. The classic example involves placing a spot of coloring on the skin or fur near the individual's forehead and seeing if they attempt to remove it or at least touch the spot, thus indicating that they recognize that the individual they are seeing in the mirror is themselves. Humans (older than 18 months) and other great apes, bottlenose dolphins, killer whales, pigeons, European magpies and elephants have all been observed to pass this test.
Neural correlates.
A major part of the scientific literature on consciousness consists of studies that examine the relationship between the experiences reported by subjects and the activity that simultaneously takes place in their brains—that is, studies of the neural correlates of consciousness. The hope is to find that activity in a particular part of the brain, or a particular pattern of global brain activity, which will be strongly predictive of conscious awareness. Several brain imaging techniques, such as EEG and fMRI, have been used for physical measures of brain activity in these studies.
Another idea that has drawn attention for several decades is that consciousness is associated with high-frequency (gamma band) oscillations in brain activity. This idea arose from proposals in the 1980s, by Christof von der Malsburg and Wolf Singer, that gamma oscillations could solve the so-called binding problem, by linking information represented in different parts of the brain into a unified experience. Rodolfo Llinás, for example, proposed that consciousness results from recurrent thalamo-cortical resonance where the specific thalamocortical systems (content) and the non-specific (centromedial thalamus) thalamocortical systems (context) interact in the gamma band frequency via synchronous oscillations.
A number of studies have shown that activity in primary sensory areas of the brain is not sufficient to produce consciousness: it is possible for subjects to report a lack of awareness even when areas such as the primary visual cortex show clear electrical responses to a stimulus. Higher brain areas are seen as more promising, especially the prefrontal cortex, which is involved in a range of higher cognitive functions collectively known as executive functions. There is substantial evidence that a "top-down" flow of neural activity (i.e., activity propagating from the frontal cortex to sensory areas) is more predictive of conscious awareness than a "bottom-up" flow of activity. The prefrontal cortex is not the only candidate area, however: studies by Nikos Logothetis and his colleagues have shown, for example, that visually responsive neurons in parts of the temporal lobe reflect the visual perception in the situation when conflicting visual images are presented to different eyes (i.e., bistable percepts during binocular rivalry).
Modulation of neural responses may correlate with phenomenal experiences. In contrast to the raw electrical responses that do not correlate with consciousness, the modulation of these responses by other stimuli correlates surprisingly well with an important aspect of consciousness: namely with the phenomenal experience of stimulus intensity (brightness, contrast). In the research group of Danko Nikolić it has been shown that some of the changes in the subjectively perceived brightness correlated with the modulation of firing rates while others correlated with the modulation of neural synchrony. An fMRI investigation suggested that these findings were strictly limited to the primary visual areas. This indicates that, in the primary visual areas, changes in firing rates and synchrony can be considered as neural correlates of qualia—at least for some type of qualia.
In 2011, Graziano and Kastner proposed the "attention schema" theory of awareness. In that theory, specific cortical areas, notably in the superior temporal sulcus and the temporo-parietal junction, are used to build the construct of awareness and attribute it to other people. The same cortical machinery is also used to attribute awareness to oneself. Damage to these cortical regions can lead to deficits in consciousness such as hemispatial neglect. In the attention schema theory, the value of explaining the feature of awareness and attributing it to a person is to gain a useful predictive model of that person's attentional processing. Attention is a style of information processing in which a brain focuses its resources on a limited set of interrelated signals. Awareness, in this theory, is a useful, simplified schema that represents attentional states. To be aware of X is explained by constructing a model of one's attentional focus on X.
In the 2013, the "perturbational complexity index" (PCI) was proposed, a measure of the algorithmic complexity of the electrophysiological response of the cortex to transcranial magnetic stimulation. This measure was shown to be higher in individuals that are awake, in REM sleep or in a locked-in state than in those who are in deep sleep or in a vegetative state, making it potentially useful as a quantitative assessment of consciousness states.
Assuming that not only humans but even some non-mammalian species are conscious, a number of evolutionary approaches to the problem of neural correlates of consciousness open up. For example, assuming that birds are conscious — a common assumption among neuroscientists and ethologists due to the extensive cognitive repertoire of birds — there are comparative neuroanatomical ways to validate some of the principal, currently competing, mammalian consciousness–brain theories. The rationale for such a comparative study is that the avian brain deviates structurally from the mammalian brain. So how similar are they? What homologues can be identified? The general conclusion from the study by Butler, et al., is that some of the major theories for the mammalian brain also appear to be valid for the avian brain. The structures assumed to be critical for consciousness in mammalian brains have homologous counterparts in avian brains. Thus the main portions of the theories of Crick and Koch, Edelman and Tononi, and Cotterill seem to be compatible with the assumption that birds are conscious. Edelman also differentiates between what he calls primary consciousness (which is a trait shared by humans and non-human animals) and higher-order consciousness as it appears in humans alone along with human language capacity. Certain aspects of the three theories, however, seem less easy to apply to the hypothesis of avian consciousness. For instance, the suggestion by Crick and Koch that layer 5 neurons of the mammalian brain have a special role, seems difficult to apply to the avian brain, since the avian homologues have a different morphology. Likewise, the theory of Eccles seems incompatible, since a structural homologue/analogue to the dendron has not been found in avian brains. The assumption of an avian consciousness also brings the reptilian brain into focus. The reason is the structural continuity between avian and reptilian brains, meaning that the phylogenetic origin of consciousness may be earlier than suggested by many leading neuroscientists.
Joaquin Fuster of UCLA has advocated the position of the importance of the prefrontal cortex in humans, along with the areas of Wernicke and Broca, as being of particular importance to the development of human language capacities neuro-anatomically necessary for the emergence of higher-order consciousness in humans.
Biological function and evolution.
Opinions are divided as to where in biological evolution consciousness emerged and about whether or not consciousness has any survival value. It has been argued that consciousness emerged (i) exclusively with the first humans, (ii) exclusively with the first mammals, (iii) independently in mammals and birds, or (iv) with the first reptiles. Other authors date the origins of consciousness to the first animals with nervous systems or early vertebrates in the Cambrian over 500 million years ago. Donald Griffin suggests in his book "Animal Minds" a gradual evolution of consciousness. Each of these scenarios raises the question of the possible survival value of consciousness.
Thomas Henry Huxley defends in an essay titled "On the Hypothesis that Animals are Automata, and its History" an epiphenomenalist theory of consciousness according to which consciousness is a causally inert effect of neural activity — “as the steam-whistle which accompanies the work of a locomotive engine is without influence upon its machinery”. To this William James objects in his essay "Are We Automata?" by stating an evolutionary argument for mind-brain interaction implying that if the preservation and development of consciousness in the biological evolution is a result of natural selection, it is plausible that consciousness has not only been influenced by neural processes, but has had a survival value itself; and it could only have had this if it had been efficacious. Karl Popper develops in the book "The Self and Its Brain" a similar evolutionary argument.
Regarding the primary function of conscious processing, a recurring idea in recent theories is that phenomenal states somehow integrate neural activities and information-processing that would otherwise be independent. This has been called the "integration consensus". Another example has been proposed by Gerald Edelman called dynamic core hypothesis which puts emphasis on reentrant connections that reciprocally link areas of the brain in a massively parallel manner. Edelman also stresses the importance of the evolutionary emergence of higher-order consciousness in humans from the historically older trait of primary consciousness which humans share with non-human animals (see "Neural correlates" section above). These theories of integrative function present solutions to two classic problems associated with consciousness: differentiation and unity. They show how our conscious experience can discriminate between a virtually unlimited number of different possible scenes and details (differentiation) because it integrates those details from our sensory systems, while the integrative nature of consciousness in this view easily explains how our experience can seem unified as one whole despite all of these individual parts. However, it remains unspecified which kinds of information are integrated in a conscious manner and which kinds can be integrated without consciousness. Nor is it explained what specific causal role conscious integration plays, nor why the same functionality cannot be achieved without consciousness. Obviously not all kinds of information are capable of being disseminated consciously (e.g., neural activity related to vegetative functions, reflexes, unconscious motor programs, low-level perceptual analyses, etc.) and many kinds of information can be disseminated and combined with other kinds without consciousness, as in intersensory interactions such as the ventriloquism effect. Hence it remains unclear why any of it is conscious. For a review of the differences between conscious and unconscious integrations, see the article of E. Morsella.
As noted earlier, even among writers who consider consciousness to be a well-defined thing, there is widespread dispute about which animals other than humans can be said to possess it. Edelman has described this distinction as that of humans possessing higher-order consciousness while sharing the trait of primary consciousness with non-human animals (see previous paragraph). Thus, any examination of the evolution of consciousness is faced with great difficulties. Nevertheless, some writers have argued that consciousness can be viewed from the standpoint of evolutionary biology as an adaptation in the sense of a trait that increases fitness. In his article "Evolution of consciousness", John Eccles argued that special anatomical and physical properties of the mammalian cerebral cortex gave rise to consciousness ("psychon ... linked to [a dendron through quantum physics"). Bernard Baars proposed that once in place, this "recursive" circuitry may have provided a basis for the subsequent development of many of the functions that consciousness facilitates in higher organisms. Peter Carruthers has put forth one such potential adaptive advantage gained by conscious creatures by suggesting that consciousness allows an individual to make distinctions between appearance and reality. This ability would enable a creature to recognize the likelihood that their perceptions are deceiving them (e.g. that water in the distance may be a mirage) and behave accordingly, and it could also facilitate the manipulation of others by recognizing how things appear to them for both cooperative and devious ends.
Other philosophers, however, have suggested that consciousness would not be necessary for any functional advantage in evolutionary processes. No one has given a causal explanation, they argue, of why it would not be possible for a functionally equivalent non-conscious organism (i.e., a philosophical zombie) to achieve the very same survival advantages as a conscious organism. If evolutionary processes are blind to the difference between function "F" being performed by conscious organism "O" and non-conscious organism "O*", it is unclear what adaptive advantage consciousness could provide. As a result, an exaptive explanation of consciousness has gained favor with some theorists that posit consciousness did not evolve as an adaptation but was an exaptation arising as a consequence of other developments such as increases in brain size or cortical rearrangement. Consciousness in this sense has been compared to the blind spot in the retina where it is not an adaption of the retina, but instead just a by-product of the way the retinal axons were wired. Several scholars including Pinker, Chomsky, Edelman, and Luria have indicated the importance of the emergence of human language as an important regulative mechanism of learning and memory in the context of the development of higher-order consciousness (see "Neural correlates" section above).
States of consciousness.
There are some brain states in which consciousness seems to be absent, including dreamless sleep, coma, and death. There are also a variety of circumstances that can change the relationship between the mind and the world in less drastic ways, producing what are known as altered states of consciousness. Some altered states occur naturally; others can be produced by drugs or brain damage. Altered states can be accompanied by changes in thinking, disturbances in the sense of time, feelings of loss of control, changes in emotional expression, alternations in body image and changes in meaning or significance.
The two most widely accepted altered states are sleep and dreaming. Although dream sleep and non-dream sleep appear very similar to an outside observer, each is associated with a distinct pattern of brain activity, metabolic activity, and eye movement; each is also associated with a distinct pattern of experience and cognition. During ordinary non-dream sleep, people who are awakened report only vague and sketchy thoughts, and their experiences do not cohere into a continuous narrative. During dream sleep, in contrast, people who are awakened report rich and detailed experiences in which events form a continuous progression, which may however be interrupted by bizarre or fantastic intrusions. Thought processes during the dream state frequently show a high level of irrationality. Both dream and non-dream states are associated with severe disruption of memory: it usually disappears in seconds during the non-dream state, and in minutes after awakening from a dream unless actively refreshed.
Research conducted on the effects of partial epileptic seizures on consciousness found that patients who suffer from partial epileptic seizures experience altered states of consciousness. In partial epileptic seizures, consciousness is impaired or lost while some aspects of consciousness, often automated behaviors, remain intact. Studies found that when measuring the qualitative features during partial epileptic seizures, patients exhibited an increase in arousal and became absorbed in the experience of the seizure, followed by difficulty in focusing and shifting attention.
A variety of psychoactive drugs and alcohol have notable effects on consciousness. These range from a simple dulling of awareness produced by sedatives, to increases in the intensity of sensory qualities produced by stimulants, cannabis, empathogens–entactogens such as MDMA ("Ecstasy"), or most notably by the class of drugs known as psychedelics. LSD, mescaline, psilocybin, Dimethyltryptamine, and others in this group can produce major distortions of perception, including hallucinations; some users even describe their drug-induced experiences as mystical or spiritual in quality. The brain mechanisms underlying these effects are not as well understood as those induced by use of alcohol, but there is substantial evidence that alterations in the brain system that uses the chemical neurotransmitter serotonin play an essential role.
There has been some research into physiological changes in yogis and people who practise various techniques of meditation. Some research with brain waves during meditation has reported differences between those corresponding to ordinary relaxation and those corresponding to meditation. It has been disputed, however, whether there is enough evidence to count these as physiologically distinct states of consciousness.
The most extensive study of the characteristics of altered states of consciousness was made by psychologist Charles Tart in the 1960s and 1970s. Tart analyzed a state of consciousness as made up of a number of component processes, including exteroception (sensing the external world); interoception (sensing the body); input-processing (seeing meaning); emotions; memory; time sense; sense of identity; evaluation and cognitive processing; motor output; and interaction with the environment. Each of these, in his view, could be altered in multiple ways by drugs or other manipulations. The components that Tart identified have not, however, been validated by empirical studies. Research in this area has not yet reached firm conclusions, but a recent questionnaire-based study identified eleven significant factors contributing to drug-induced states of consciousness: experience of unity; spiritual experience; blissful state; insightfulness; disembodiment; impaired control and cognition; anxiety; complex imagery; elementary imagery; audio-visual synesthesia; and changed meaning of percepts.
Phenomenology.
Phenomenology is a method of inquiry that attempts to examine the structure of consciousness in its own right, putting aside problems regarding the relationship of consciousness to the physical world. This approach was first proposed by the philosopher Edmund Husserl, and later elaborated by other philosophers and scientists. Husserl's original concept gave rise to two distinct lines of inquiry, in philosophy and psychology. In philosophy, phenomenology has largely been devoted to fundamental metaphysical questions, such as the nature of intentionality (""aboutness""). In psychology, phenomenology largely has meant attempting to investigate consciousness using the method of introspection, which means looking into one's own mind and reporting what one observes. This method fell into disrepute in the early twentieth century because of grave doubts about its reliability, but has been rehabilitated to some degree, especially when used in combination with techniques for examining brain activity.
Introspectively, the world of conscious experience seems to have considerable structure. Immanuel Kant asserted that the world as we perceive it is organized according to a set of fundamental "intuitions", which include "object" (we perceive the world as a set of distinct things); "shape"; "quality" (color, warmth, etc.); "space" (distance, direction, and location); and "time". Some of these constructs, such as space and time, correspond to the way the world is structured by the laws of physics; for others the correspondence is not as clear. Understanding the physical basis of qualities, such as redness or pain, has been particularly challenging. David Chalmers has called this the "hard problem of consciousness". Some philosophers have argued that it is intrinsically unsolvable, because qualities (""qualia"") are ineffable; that is, they are "raw feels", incapable of being analyzed into component processes. Most psychologists and neuroscientists reject these arguments. For example, research on ideasthesia shows that qualia are organised into a semantic-like network. Nevertheless, it is clear that the relationship between a physical entity such as light and a perceptual quality such as color is extraordinarily complex and indirect, as demonstrated by a variety of optical illusions such as neon color spreading.
In neuroscience, a great deal of effort has gone into investigating how the perceived world of conscious awareness is constructed inside the brain. The process is generally thought to involve two primary mechanisms: (1) hierarchical processing of sensory inputs, and (2) memory. Signals arising from sensory organs are transmitted to the brain and then processed in a series of stages, which extract multiple types of information from the raw input. In the visual system, for example, sensory signals from the eyes are transmitted to the thalamus and then to the primary visual cortex; inside the cerebral cortex they are sent to areas that extract features such as three-dimensional structure, shape, color, and motion. Memory comes into play in at least two ways. First, it allows sensory information to be evaluated in the context of previous experience. Second, and even more importantly, working memory allows information to be integrated over time so that it can generate a stable representation of the world—Gerald Edelman expressed this point vividly by titling one of his books about consciousness "The Remembered Present". In computational neuroscience, Bayesian approaches to brain function have been used to understand both the evaluation of sensory information in light of previous experience, and the integration of information over time. Bayesian models of the brain are probabilistic inference models, in which the brain takes advantage of prior knowledge to interpret uncertain sensory inputs in order to formulate a conscious percept; Bayesian models have successfully predicted many perceptual phenomena in vision and the nonvisual senses.
Despite the large amount of information available, many important aspects of perception remain mysterious. A great deal is known about low-level signal processing in sensory systems, but the ways by which sensory systems interact with each other, with "executive" systems in the frontal cortex, and with the language system are very incompletely understood. At a deeper level, there are still basic conceptual issues that remain unresolved. Many scientists have found it difficult to reconcile the fact that information is distributed across multiple brain areas with the apparent unity of consciousness: this is one aspect of the so-called "binding problem". There are also some scientists who have expressed grave reservations about the idea that the brain forms representations of the outside world at all: influential members of this group include psychologist J. J. Gibson and roboticist Rodney Brooks, who both argued in favor of "intelligence without representation".
Medical aspects.
The medical approach to consciousness is practically oriented. It derives from a need to treat people whose brain function has been impaired as a result of disease, brain damage, toxins, or drugs. In medicine, conceptual distinctions are considered useful to the degree that they can help to guide treatments. Whereas the philosophical approach to consciousness focuses on its fundamental nature and its contents, the medical approach focuses on the "amount" of consciousness a person has: in medicine, consciousness is assessed as a "level" ranging from coma and brain death at the low end, to full alertness and purposeful responsiveness at the high end.
Consciousness is of concern to patients and physicians, especially neurologists and anesthesiologists. Patients may suffer from disorders of consciousness, or may need to be anesthetized for a surgical procedure. Physicians may perform consciousness-related interventions such as instructing the patient to sleep, administering general anesthesia, or inducing medical coma. Also, bioethicists may be concerned with the ethical implications of consciousness in medical cases of patients such as Karen Ann Quinlan, while neuroscientists may study patients with impaired consciousness in hopes of gaining information about how the brain works.
Assessment.
In medicine, consciousness is examined using a set of procedures known as neuropsychological assessment. There are two commonly used methods for assessing the level of consciousness of a patient: a simple procedure that requires minimal training, and a more complex procedure that requires substantial expertise. The simple procedure begins by asking whether the patient is able to move and react to physical stimuli. If so, the next question is whether the patient can respond in a meaningful way to questions and commands. If so, the patient is asked for name, current location, and current day and time. A patient who can answer all of these questions is said to be "alert and oriented times four" (sometimes denoted "A&Ox4" on a medical chart), and is usually considered fully conscious.
The more complex procedure is known as a neurological examination, and is usually carried out by a neurologist in a hospital setting. A formal neurological examination runs through a precisely delineated series of tests, beginning with tests for basic sensorimotor reflexes, and culminating with tests for sophisticated use of language. The outcome may be summarized using the Glasgow Coma Scale, which yields a number in the range 3—15, with a score of 3 indicating brain death (the lowest defined level of consciousness), and 15 indicating full consciousness. The Glasgow Coma Scale has three subscales, measuring the "best motor response" (ranging from "no motor response" to "obeys commands"), the "best eye response" (ranging from "no eye opening" to "eyes opening spontaneously") and the "best verbal response" (ranging from "no verbal response" to "fully oriented"). There is also a simpler pediatric version of the scale, for children too young to be able to use language.
In 2013, an experimental procedure was developed to measure degrees of consciousness, the procedure involving stimulating the brain with a magnetic pulse, measuring resulting waves of electrical activity, and developing a consciousness score based on the complexity of the brain activity.
Disorders of consciousness.
Medical conditions that inhibit consciousness are considered disorders of consciousness. This category generally includes minimally conscious state and persistent vegetative state, but sometimes also includes the less severe locked-in syndrome and more severe chronic coma. Differential diagnosis of these disorders is an active area of biomedical research. Finally, brain death results in an irreversible disruption of consciousness. While other conditions may cause a moderate deterioration (e.g., dementia and delirium) or transient interruption (e.g., grand mal and petit mal seizures) of consciousness, they are not included in this category.
Anosognosia.
One of the most striking disorders of consciousness goes by the name anosognosia, a Greek-derived term meaning "unawareness of disease". This is a condition in which patients are disabled in some way, most commonly as a result of a stroke, but either misunderstand the nature of the problem or deny that there is anything wrong with them. The most frequently occurring form is seen in people who have experienced a stroke damaging the parietal lobe in the right hemisphere of the brain, giving rise to a syndrome known as hemispatial neglect, characterized by an inability to direct action or attention toward objects located to the right with respect to their bodies. Patients with hemispatial neglect are often paralyzed on the right side of the body, but sometimes deny being unable to move. When questioned about the obvious problem, the patient may avoid giving a direct answer, or may give an explanation that doesn't make sense. Patients with hemispatial neglect may also fail to recognize paralyzed parts of their bodies: one frequently mentioned case is of a man who repeatedly tried to throw his own paralyzed right leg out of the bed he was lying in, and when asked what he was doing, complained that somebody had put a dead leg into the bed with him. An even more striking type of anosognosia is Anton–Babinski syndrome, a rarely occurring condition in which patients become blind but claim to be able to see normally, and persist in this claim in spite of all evidence to the contrary.
Stream of consciousness.
William James is usually credited with popularizing the idea that human consciousness flows like a stream, in his "Principles of Psychology" of 1890. According to James, the "stream of thought" is governed by five characteristics: "(1) Every thought tends to be part of a personal consciousness. (2) Within each personal consciousness thought is always changing. (3) Within each personal consciousness thought is sensibly continuous. (4) It always appears to deal with objects independent of itself. (5) It is interested in some parts of these objects to the exclusion of others". A similar concept appears in Buddhist philosophy, expressed by the Sanskrit term "Citta-saṃtāna", which is usually translated as mindstream or "mental continuum". In the Buddhist view, though, the "mindstream" is viewed primarily as a source of noise that distracts attention from a changeless underlying reality.
Narrative form.
In the west, the primary impact of the idea has been on literature rather than science: stream of consciousness as a narrative mode means writing in a way that attempts to portray the moment-to-moment thoughts and experiences of a character. This technique perhaps had its beginnings in the monologues of Shakespeare's plays, and reached its fullest development in the novels of James Joyce and Virginia Woolf, although it has also been used by many other noted writers.
Here for example is a passage from Joyce's Ulysses about the thoughts of Molly Bloom:
Spiritual approaches.
To most philosophers, the word "consciousness" connotes the relationship between the mind and the world. To writers on spiritual or religious topics, it frequently connotes the relationship between the mind and God, or the relationship between the mind and deeper truths that are thought to be more fundamental than the physical world. Krishna consciousness, for example, is a term used to mean an intimate linkage between the mind of a worshipper and the god Krishna. The mystical psychiatrist Richard Maurice Bucke distinguished between three types of consciousness: "Simple Consciousness", awareness of the body, possessed by many animals; "Self Consciousness", awareness of being aware, possessed only by humans; and "Cosmic Consciousness", awareness of the life and order of the universe, possessed only by humans who are enlightened. Many more examples could be given. The most thorough account of the spiritual approach may be Ken Wilber's book "The Spectrum of Consciousness", a comparison of western and eastern ways of thinking about the mind. Wilber described consciousness as a spectrum with ordinary awareness at one end, and more profound types of awareness at higher levels.

</doc>
<doc id="5665" url="https://en.wikipedia.org/wiki?curid=5665" title="Currency">
Currency

A currency (from , "in circulation", from ) in the most specific use of the word refers to money in any form when in actual use or circulation as a medium of exchange, especially circulating banknotes and coins. A more general definition is that a currency is a "system of money" (monetary units) in common use, especially in a nation. Under this definition, British pounds, U.S. dollars, and European euros are examples of currency. These various currencies are recognized stores of value, and are traded between nations in foreign exchange markets, which determine the relative values of the different currencies. Currencies in this sense are defined by governments, and each type has limited boundaries of acceptance.
Other definitions of the term "currency" are discussed in their respective synonymous articles banknote, coin, and money. The latter definition, pertaining to the currency systems of nations, is the topic of this article. Currencies can be classified into two monetary systems: fiat money and commodity money, depending on what guarantees the value (the economy at large vs. the government's physical metal reserves). Some currencies are legal tender in certain political jurisdictions, which means they cannot be refused as payment for debt. Others are simply traded for their economic value. Digital currency has arisen with the popularity of computers and the Internet.
History.
Early currency.
Currency evolved from two basic innovations, both of which had occurred by 2000 BC. Originally money was a form of receipt, representing grain stored in temple granaries in Sumer in ancient Mesopotamia, then Ancient Egypt.
In this first stage of currency, metals were used as symbols to represent value stored in the form of commodities. This formed the basis of trade in the Fertile Crescent for over 1500 years. However, the collapse of the Near Eastern trading system pointed to a flaw: in an era where there was no place that was safe to store value, the value of a circulating medium could only be as sound as the forces that defended that store. Trade could only reach as far as the credibility of that military. By the late Bronze Age, however, a series of treaties had established safe passage for merchants around the Eastern Mediterranean, spreading from Minoan Crete and Mycenae in the northwest to Elam and Bahrain in the southeast. It is not known what was used as a currency for these exchanges, but it is thought that ox-hide shaped ingots of copper, produced in Cyprus, may have functioned as a currency.
It is thought that the increase in piracy and raiding associated with the Bronze Age collapse, possibly produced by the Peoples of the Sea, brought the trading system of oxhide ingots to an end. It was only with the recovery of Phoenician trade in the 10th and 9th centuries BC that saw a return to prosperity, and the appearance of real coinage, possibly first in Anatolia with Croesus of Lydia and subsequently with the Greeks and Persians. In Africa many forms of value store have been used, including beads, ingots, ivory, various forms of weapons, livestock, the manilla currency, and ochre and other earth oxides. The manilla rings of West Africa were one of the currencies used from the 15th century onwards to sell slaves. African currency is still notable for its variety, and in many places various forms of barter still apply.
Coinage.
These factors led to the metal itself being the store of value: first silver, then both silver and gold, and at one point also bronze. Now we have copper coins and other non-precious metals as coins. Metals were mined, weighed, and stamped into coins. This was to assure the individual taking the coin that he was getting a certain known weight of precious metal. Coins could be counterfeited, but they also created a new unit of account, which helped lead to banking. Archimedes' principle provided the next link: coins could now be easily tested for their fine weight of metal, and thus the value of a coin could be determined, even if it had been shaved, debased or otherwise tampered with (see Numismatics).
Most major economies using coinage had three tiers of coins: copper, silver and gold. Gold coins were used for large purchases, payment of the military and backing of state activities. Silver coins were used for midsized transactions, and as a unit of account for taxes, dues, contracts and fealty, while copper coins were used for everyday transactions. This system had been used in ancient India since the time of the Mahajanapadas. In Europe, this system worked through the medieval period because there was virtually no new gold, silver or copper introduced through mining or conquest. Thus the overall ratios of the three coinages remained roughly equivalent.
Paper money.
In premodern China, the need for credit and for a medium of exchange that was less physically cumbersome than large numbers of copper coins led to the introduction of paper money, i.e. banknotes. Their introduction was a gradual process which lasted from the late Tang dynasty (618–907) into the Song dynasty (960–1279). It began as a means for merchants to exchange heavy coinage for receipts of deposit issued as promissory notes by wholesalers' shops. These notes were valid for temporary use in a small regional territory. In the 10th century, the Song dynasty government began to circulate these notes amongst the traders in its monopolized salt industry. The Song government granted several shops the right to issue banknotes, and in the early 12th century the government finally took over these shops to produce state-issued currency. Yet the banknotes issued were still only locally and temporarily valid: it was not until the mid 13th century that a standard and uniform government issue of paper money became an acceptable nationwide currency. The already widespread methods of woodblock printing and then Pi Sheng's movable type printing by the 11th century were the impetus for the mass production of paper money in premodern China.
At around the same time in the medieval Islamic world, a vigorous monetary economy was created during the 7th–12th centuries on the basis of the expanding levels of circulation of a stable high-value currency (the dinar). Innovations introduced by Muslim economists, traders and merchants include the earliest uses of credit, cheques, promissory notes, savings accounts, transactional accounts, loaning, trusts, exchange rates, the transfer of credit and debt, and banking institutions for loans and deposits.
In Europe, paper money was first introduced on a regular basis in Sweden in 1661 (although Washington Irving records an earlier emergency use of it, by the Spanish in a siege during the Conquest of Granada). As Sweden was rich in copper, its low value necessitated extraordinarily big coins, often weighing several kilograms. 
The advantages of paper currency were numerous: it reduced the need to transport gold and silver, which was risky; it facilitated loans of gold or silver at interest, since the underlying specie (gold or silver) never left the possession of the lender until someone else redeemed the note; and it allowed a division of currency into credit and specie backed forms. It enabled the sale of stock in joint stock companies, and the redemption of those shares in paper.
But there were also disadvantages. First, since a note has no intrinsic value, there was nothing to stop issuing authorities from printing more notes than they had specie to back them with. Second, because it increased the money supply, it increased inflationary pressures, a fact observed by David Hume in the 18th century. Thus paper money would often lead to an inflationary bubble, which could collapse if people began demanding hard money, causing the demand for paper notes to fall to zero. The printing of paper money was also associated with wars, and financing of wars, and therefore regarded as part of maintaining a standing army. For these reasons, paper currency was held in suspicion and hostility in Europe and America. It was also addictive, since the speculative profits of trade and capital creation were quite large. Major nations established mints to print money and mint coins, and branches of their treasury to collect taxes and hold gold and silver stock.
At that time, both silver and gold were considered legal tender, and accepted by governments for taxes. However, the instability in the ratio between the two grew over the course of the 19th century, with the increases both in supply of these metals, particularly silver, and in trade. The parallel use of both metals is called bimetallism, and the attempt to create a bimetallic standard where both gold and silver backed currency remained in circulation occupied the efforts of inflationists. Governments at this point could use currency as an instrument of policy, printing paper currency such as the United States Greenback, to pay for military expenditures. They could also set the terms at which they would redeem notes for specie, by limiting the amount of purchase, or the minimum amount that could be redeemed.
By 1900, most of the industrializing nations were on some form of gold standard, with paper notes and silver coins constituting the circulating medium. Private banks and governments across the world followed Gresham's Law: keeping the gold and silver they received, but paying out in notes. This did not happen all around the world at the same time, but occurred sporadically, generally in times of war or financial crisis, beginning in the early part of the 20th century and continuing across the world until the late 20th century, when the regime of floating fiat currencies came into force. One of the last countries to break away from the gold standard was the United States in 1971. As of , no country has an enforceable gold standard or silver standard currency system.
Banknote era.
A banknote (more commonly known as a bill in the United States and Canada) is a type of currency, and commonly used as legal tender in many jurisdictions. With coins, banknotes make up the cash form of all money. Banknotes are mostly paper, but Australia's Commonwealth Scientific and Industrial Research Organisation developed the world's first polymer currency in the 1980s that went into circulation on the nation's bicentenary in 1988. Now used in some 22 countries (over 40 if counting commemorative issues), polymer currency dramatically improves the life span of banknotes and prevents counterfeiting.
Modern currencies.
Currency use is based on the concept of lex monetae; that a sovereign state decides which currency it shall use. Currently, the International Organization for Standardization has introduced a three-letter system of codes (ISO 4217) to define currency (as opposed to simple names or currency signs), in order to remove the confusion that there are dozens of currencies called the dollar and many called the franc. Even the pound is used in nearly a dozen different countries; most of these are tied to the Pound Sterling, while the remainder have varying values. In general, the three-letter code uses the ISO 3166-1 country code for the first two letters and the first letter of the name of the currency (D for dollar, for instance) as the third letter. United States currency, for instance is globally referred to as USD.
The International Monetary Fund uses a variant system when referring to national currencies.
Alternative currencies.
Distinct from centrally controlled government-issued currencies, private decentralized trust networks support alternative currencies such as Bitcoin, Litecoin, Peercoin or Dogecoin, as well as branded currencies, for example 'obligation' based stores of value, such as quasi-regulated BarterCard, Loyalty Points (Credit Cards, Airlines) or Game-Credits (MMO games) that are based on reputation of commercial products, or highly regulated 'asset backed' 'alternative currencies' such as mobile-money schemes like MPESA (called E-Money Issuance).
Currency may be Internet-based and digital, for instance, Bitcoin and not tied to any specific country, or the IMF's SDR that is based on a basket of currencies (and assets held).
Control and production.
In most cases, a central bank has a monopoly right to issue of coins and banknotes (fiat money) for its own area of circulation (a country or group of countries); it regulates the production of currency by banks (credit) through monetary policy.
An exchange rate is the price at which two currencies can be exchanged against each other. This is used for trade between the two currency zones. Exchange rates can be classified as either floating or fixed. In the former, day-to-day movements in exchange rates are determined by the market; in the latter, governments intervene in the market to buy or sell their currency to balance supply and demand at a fixed exchange rate.
In cases where a country has control of its own currency, that control is exercised either by a central bank or by a Ministry of Finance. The institution that has control of monetary policy is referred to as the monetary authority. Monetary authorities have varying degrees of autonomy from the governments that create them. In the United States, the Federal Reserve System operates without direct oversight by the legislative or executive branches. A monetary authority is created and supported by its sponsoring government, so independence can be reduced by the legislative or executive authority that creates it.
Several countries can use the same name for their own separate currencies (for example, "dollar" in Australia, Canada and the United States). By contrast, several countries can also use the same currency (for example, the euro or the CFA franc), or one country can declare the currency of another country to be legal tender. For example, Panama and El Salvador have declared U.S. currency to be legal tender, and from 1791 to 1857, Spanish silver coins were legal tender in the United States. At various times countries have either re-stamped foreign coins, or used currency board issuing one note of currency for each note of a foreign government held, as Ecuador currently does.
Each currency typically has a main currency unit (the dollar, for example, or the euro) and a fractional unit, often defined as of the main unit: 100 cents = 1 dollar, 100 centimes = 1 franc, 100 pence = 1 pound, although units of or occasionally also occur. Some currencies do not have any smaller units at all, such as the Icelandic króna.
Mauritania and Madagascar are the only remaining countries that do not use the decimal system; instead, the Mauritanian ouguiya is in theory divided into 5 khoums, while the Malagasy ariary is theoretically divided into 5 iraimbilanja. In these countries, words like "dollar" or "pound" "were simply names for given weights of gold." Due to inflation khoums and iraimbilanja have in practice fallen into disuse. (See non-decimal currencies for other historic currencies with non-decimal divisions.)
Currency convertibility.
Convertibility of a currency determines the ability of an individual, corporate or government to convert its local currency to another currency or vice versa with or without central bank/government intervention. Based on the above restrictions or free and readily conversion features, currencies are classified as:
Local currencies.
In economics, a local currency is a currency not backed by a national government, and intended to trade only in a small area. Advocates such as Jane Jacobs argue that this enables an economically depressed region to pull itself up, by giving the people living there a medium of exchange that they can use to exchange services and locally produced goods (in a broader sense, this is the original purpose of all money). Opponents of this concept argue that local currency creates a barrier which can interfere with economies of scale and comparative advantage, and that in some cases they can serve as a means of tax evasion.
Local currencies can also come into being when there is economic turmoil involving the national currency. An example of this is the Argentinian economic crisis of 2002 in which IOUs issued by local governments quickly took on some of the characteristics of local currencies.
One of the best examples of a local currency is the original LETS currency, founded on Vancouver Island in the early 1980s. In 1982, the Canadian Central Bank’s lending rates ran up to 14% which drove chartered bank lending rates as high as 19%. The resulting currency and credit scarcity left island residents with few options other than to create a local currency.
List of Major World Payments Currencies.
The following table are estimates for 15 most frequently used currencies in World Payments from 2012 to 2015 by SWIFT. 
See also.
Related concepts
Accounting units
Lists

</doc>
<doc id="5666" url="https://en.wikipedia.org/wiki?curid=5666" title="Central bank">
Central bank

A central bank, reserve bank, or monetary authority is an institution that manages a state's currency, money supply, and interest rates. Central banks also usually oversee the commercial banking system of their respective countries. In contrast to a commercial bank, a central bank possesses a monopoly on increasing the monetary base in the state, and usually also prints the national currency, which usually serves as the state's legal tender.
The primary function of a central bank is to control the nation's money supply (monetary policy), through active duties such as managing interest rates, setting the reserve requirement, and acting as a lender of last resort to the banking sector during times of bank insolvency or financial crisis. Central banks usually also have supervisory powers, intended to prevent bank runs and to reduce the risk that commercial banks and other financial institutions engage in reckless or fraudulent behavior. Central banks in most developed nations are institutionally designed to be independent from political interference. Still, limited control by the executive and legislative bodies usually exists.
History.
Prior to the 17th century most money was commodity money, typically gold or silver. However, promises to pay were widely circulated and accepted as value at least five hundred years earlier in both Europe and Asia. The Song dynasty was the first to issue generally circulating paper currency, while the Yuan Dynasty was the first to use notes as the predominant circulating medium. In 1455, in an effort to control inflation, the succeeding Ming Dynasty ended the use of paper money and closed much of Chinese trade. The medieval European Knights Templar ran an early prototype of a central banking system, as their promises to pay were widely respected, and many regard their activities as having laid the basis for the modern banking system.
As the first public bank to "offer accounts not directly convertible to coin", the Bank of Amsterdam established in 1609 is considered to be the precursor to modern central banks. The central bank of Sweden ("Sveriges Riksbank" or simply "Riksbanken") was founded in Stockholm from the remains of the failed bank Stockholms Banco in 1664 and answered to the parliament ("Riksdag of the Estates"). One role of the Swedish central bank was lending money to the government.
Bank of England.
In the Kingdom of England in the 1690s, public funds were in short supply and were needed to finance the ongoing Nine Years' War with France. The credit of William III's government was so low in London that it was impossible for it to borrow the £1,200,000 (at 8 percent) that the government wanted. In order to induce subscription to the loan, the subscribers were to be incorporated by the name of the Governor and Company of the Bank of England. The bank was given exclusive possession of the government's balances, and was the only limited-liability corporation allowed to issue banknotes. The lenders would give the government cash (bullion) and also issue notes against the government bonds, which can be lent again. The £1.2M was raised in 12 days; half of this was used to rebuild the Navy.
The establishment of the Bank of England, the model on which most modern central banks have been based, was devised by Charles Montagu, 1st Earl of Halifax, in 1694, as had been proposed by the banker William Paterson three years before, but had not been acted upon. He proposed a loan of £1.2M to the government; in return the subscribers would be incorporated as "The Governor and Company of the Bank of England" with long-term banking privileges including the issue of notes. The Royal Charter was granted on 27 July through the passage of the Tonnage Act 1694.
Although some would point to the 1694 establishment Bank of England as the origin of central banking, it did not have the functions as a modern central bank then, namely, to regulate the value of the national currency, to finance the government, to be the sole authorised distributor of banknotes, and to function as a 'lender of last resort' to banks suffering a liquidity crisis. The modern central bank evolved slowly through the 18th and 19th centuries to reach its current form.
Although the Bank was originally a private institution, by the end of the 18th century it was increasingly being regarded as a public authority with civic responsibility toward the upkeep of a healthy financial system. The currency crisis of 1797, caused by panicked depositors withdrawing from the Bank led to the government suspending convertibility of notes into specie payment. The bank was soon accused by the bullionists of causing the exchange rate to fall from over issuing banknotes, a charge which the Bank denied. Nevertheless, it was clear that the Bank was being treated as an organ of the state. 
Henry Thornton, a merchant banker and monetary theorist has been described as the father of the modern central bank. An opponent of the real bills doctrine, he was a defender of the bullionist position and a significant figure in monetary theory. Thornton's process of monetary expansion anticipated the theories of Knut Wicksell regarding the "cumulative process which restates the Quantity Theory in a theoretically coherent form". As a response 1797 currency crisis, Thornton wrote in 1802 "An Enquiry into the Nature and Effects of the Paper Credit of Great Britain", in which he argued that the increase in paper credit did not cause the crisis. The book also gives a detailed account of the British monetary system as well as a detailed examination of the ways in which the Bank of England should act to counteract fluctuations in the value of the pound.
Until the mid-nineteenth century, commercial banks were able to issue their own banknotes, and notes issued by provincial banking companies were commonly in circulation. Many consider the origins of the central bank to lie with the passage of the Bank Charter Act of 1844. Under this law, authorisation to issue new banknotes was restricted to the Bank of England. At the same time, the Bank of England was restricted to issue new banknotes only if they were 100% backed by gold or up to £14 million in government debt. The Act served to restrict the supply of new notes reaching circulation, and gave the Bank of England an effective monopoly on the printing of new notes.
The Bank accepted the role of 'lender of last resort' in the 1870s after criticism of its lacklustre response to the Overend-Gurney crisis. The journalist Walter Bagehot wrote on the subject in "", in which he advocated for the Bank to officially become a lender of last resort during a credit crunch, sometimes referred to as "Bagehot's dictum". Paul Tucker phrased the dictum in 2009 as follows:
Spread around the world.
Central banks were established in many European countries during the 19th century. The War of the Second Coalition led to the creation of the Banque de France in 1800, in an effort to improve the public financing of the war.
Although central banks today are generally associated with fiat money, the 19th and early 20th centuries central banks in most of Europe and Japan developed under the international gold standard, elsewhere free banking or currency boards were more usual at this time. Problems with collapses of banks during downturns, however, led to wider support for central banks in those nations which did not as yet possess them, most notably in Australia.
On 23 December 1913 the U.S. Congress created the US Federal Reserve through the passing of The Federal Reserve Act in the Senate and its signing by President Woodrow Wilson on the same day. Australia established its first central bank in 1920, Colombia in 1923, Mexico and Chile in 1925 and Canada and New Zealand in the aftermath of the Great Depression in 1934. By 1935, the only significant independent nation that did not possess a central bank was Brazil, which subsequently developed a precursor thereto in 1945 and the present Central Bank of Brazil twenty years later. After gaining independence, African and Asian countries also established central banks or monetary unions.
The People's Bank of China evolved its role as a central bank starting in about 1979 with the introduction of market reforms, which accelerated in 1989 when the country adopted a generally capitalist approach to its export economy. Evolving further partly in response to the European Central Bank, the People's Bank of China had by 2000 become a modern central bank. The most recent bank model was introduced together with the euro, and involves coordination of the European national banks, which continue to manage their respective economies separately in all respects other than currency exchange and base interest rates.
Naming of central banks.
There is no standard terminology for the name of a central bank, but many countries use the "Bank of Country" form — for example: Bank of England (which, despite its name, is in fact the central bank of the United Kingdom as a whole. The name's lack of representation of the entire United Kingdom ('Bank of Britain', for example) can be owed to the fact that its establishment occurred when the Kingdoms of England, Scotland and Ireland were separate entities (at least in name), and therefore pre-dates the merger of the Kingdoms of England and Scotland, the Kingdom of Ireland's absorption into the Union and the formation of the present day United Kingdom), Bank of Canada, Bank of Mexico. Some are styled "national" banks, such as the National Bank of Ukraine, although the term national bank is also used for private commercial banks in some countries. In other cases, central banks may incorporate the word "Central" (for example, European Central Bank, Central Bank of Ireland, Central Bank of Brazil). The word "Reserve" is also often included, such as the Reserve Bank of India, Reserve Bank of Australia, Reserve Bank of New Zealand, the South African Reserve Bank, and U.S. Federal Reserve System. Other central banks are known as monetary authorities such as the Monetary Authority of Singapore, Maldives Monetary Authority and Cayman Islands Monetary Authority. There is an instance where native language was used to name the central bank, such as Bangko Sentral ng Pilipinas. Many countries have state-owned banks or other quasi-government entities that have entirely separate functions, such as financing imports and exports.
In some countries, particularly in formerly Communist countries, the term national bank may be used to indicate both the monetary authority and the leading banking entity, such as the Soviet Union's Gosbank (state bank). In other countries, the term national bank may be used to indicate that the central bank's goals are broader than monetary stability, such as full employment, industrial development, or other goals. Some state-owned commercial banks have names suggestive of central banks, even if they are not: examples are the Bank of India and the Central Bank of India.
The chief executive of a central bank is usually known as the Governor, President or Chairman.
21st Century.
After the financial crisis of 2007-2008 central banks led change, but as of 2015 their ability to boost economic growth has stalled. Central banks debate whether they should experiment with new measures like negative interest rates or direct financing of government, "lean even more on politicians to do more". Andrew Haldane from the Bank of England said "central bankers may need to accept that their good old days – of adjusting interest rates to boost employment or contain inflation – may be gone for good". The European Central Bank and The Bank of Japan whose economies are in or close to deflation, continue quantitative easing buying securities to encourage more lending.
Activities and responsibilities.
Functions of a central bank may include:
Monetary policy.
Central banks implement a country's chosen monetary policy. 
Currency issuance.
At the most basic level, monetary policy involves establishing what form of currency the country may have, whether a fiat currency, gold-backed currency (disallowed for countries in the International Monetary Fund), currency board or a currency union. When a country has its own national currency, this involves the issue of some form of standardized currency, which is essentially a form of promissory note: a promise to exchange the note for "money" under certain circumstances. Historically, this was often a promise to exchange the money for precious metals in some fixed amount. Now, when many currencies are fiat money, the "promise to pay" consists of the promise to accept that currency to pay for taxes.
A central bank may use another country's currency either directly in a currency union, or indirectly on a currency board. In the latter case, exemplified by the Bulgarian National Bank, Hong Kong and Latvia, the local currency is backed at a fixed rate by the central bank's holdings of a foreign currency.
Similar to commercial banks, central banks hold assets (government bonds, foreign exchange, gold, and other financial assets) and incur liabilities (currency outstanding). Central banks create money by issuing interest-free currency notes and selling them to the public (government) in exchange for interest-bearing assets such as government bonds. When a central bank wishes to purchase more bonds than their respective national governments make available, they may purchase private bonds or assets denominated in foreign currencies.
The European Central Bank remits its interest income to the central banks of the member countries of the European Union. The US Federal Reserve remits all its profits to the U.S. Treasury. This income, derived from the power to issue currency, is referred to as seigniorage, and usually belongs to the national government. The state-sanctioned power to create currency is called the Right of Issuance. Throughout history there have been disagreements over this power, since whoever controls the creation of currency controls the seigniorage income.
The expression "monetary policy" may also refer more narrowly to the interest-rate targets and other active measures undertaken by the monetary authority.
Goals.
High employment.
Frictional unemployment is the time period between jobs when a worker is searching for, or transitioning from one job to another. Unemployment beyond frictional unemployment is classified as unintended unemployment.
For example, structural unemployment is a form of unemployment resulting from a mismatch between demand in the labour market and the skills and locations of the workers seeking employment. Macroeconomic policy generally aims to reduce unintended unemployment.
Keynes labeled any jobs that would be created by a rise in wage-goods (i.e., a decrease in real-wages) as involuntary unemployment:
Price stability.
Inflation is defined either as the devaluation of a currency or equivalently the rise of prices relative to a currency.
Since inflation lowers real wages, Keynesians view inflation as the solution to involuntary unemployment. However, "unanticipated" inflation leads to lender losses as the real interest rate will be lower than expected. Thus, Keynesian monetary policy aims for a steady rate of inflation. A publication from the Austrian School, "The Case Against the Fed", argues that the efforts of the central banks to control inflation have been counterproductive.
Economic growth.
Economic growth can be enhanced by investment in capital, such as more or better machinery. A low interest rate implies that firms can loan money to invest in their capital stock and pay less interest for it. Lowering the interest is therefore considered to encourage economic growth and is often used to alleviate times of low economic growth. On the other hand, raising the interest rate is often used in times of high economic growth as a contra-cyclical device to keep the economy from overheating and avoid market bubbles.
Further goals of monetary policy are stability of interest rates, of the financial market, and of the foreign exchange market.
Goals frequently cannot be separated from each other and often conflict. Costs must therefore be carefully weighed before policy implementation.
Policy instruments.
The main monetary policy instruments available to central banks are open market operation, bank reserve requirement, interest rate policy, re-lending and re-discount (including using the term repurchase market), and credit policy (often coordinated with trade policy). While capital adequacy is important, it is defined and regulated by the Bank for International Settlements, and central banks in practice generally do not apply stricter rules.
To enable open market operations, a central bank must hold foreign exchange reserves (usually in the form of government bonds) and official gold reserves. It will often have some influence over any official or mandated exchange rates: Some exchange rates are managed, some are market based (free float) and many are somewhere in between ("managed float" or "dirty float").
Interest rates.
By far the most visible and obvious power of many modern central banks is to influence market interest rates; contrary to popular belief, they rarely "set" rates to a fixed number. Although the mechanism differs from country to country, most use a similar mechanism based on a central bank's ability to create as much fiat money as required.
The mechanism to move the market towards a 'target rate' (whichever specific rate is used) is generally to lend money or borrow money in theoretically unlimited quantities, until the targeted market rate is sufficiently close to the target. Central banks may do so by lending money to and borrowing money from (taking deposits from) a limited number of qualified banks, or by purchasing and selling bonds. As an example of how this functions, the Bank of Canada sets a target overnight rate, and a band of plus or minus 0.25%. Qualified banks borrow from each other within this band, but never above or below, because the central bank will always lend to them at the top of the band, and take deposits at the bottom of the band; in principle, the capacity to borrow and lend at the extremes of the band are unlimited. Other central banks use similar mechanisms.
The target rates are generally short-term rates. The actual rate that borrowers and lenders receive on the market will depend on (perceived) credit risk, maturity and other factors. For example, a central bank might set a target rate for overnight lending of 4.5%, but rates for (equivalent risk) five-year bonds might be 5%, 4.75%, or, in cases of inverted yield curves, even below the short-term rate. Many central banks have one primary "headline" rate that is quoted as the "central bank rate". In practice, they will have other tools and rates that are used, but only one that is rigorously targeted and enforced.
"The rate at which the central bank lends money can indeed be chosen at will by the central bank; this is the rate that makes the financial headlines." – Henry C.K. Liu. Liu explains further that "the U.S. central-bank lending rate is known as the Fed funds rate. The Fed sets a target for the Fed funds rate, which its Open Market Committee tries to match by lending or borrowing in the money market ... a fiat money system set by command of the central bank. The Fed is the head of the central-bank because the U.S. dollar is the key reserve currency for international trade. The global money market is a USA dollar market. All other currencies markets revolve around the U.S. dollar market." Accordingly, the U.S. situation is not typical of central banks in general.
Typically a central bank controls certain types of short-term interest rates. These influence the stock- and bond markets as well as mortgage and other interest rates. The European Central Bank for example announces its interest rate at the meeting of its Governing Council; in the case of the U.S. Federal Reserve, the Federal Reserve Board of Governors. Both the Federal Reserve and the ECB are composed of one or more central bodies that are responsible for the main decisions about interest rates and the size and type of open market operations, and several branches to execute its policies. In the case of the Federal Reserve, they are the local Federal Reserve Banks; for the ECB they are the national central banks.
A typical central bank has several interest rates or monetary policy tools it can set to influence markets.
These rates directly affect the rates in the money market, the market for short term loans.
Open market operations.
Through open market operations, a central bank influences the money supply in an economy. Each time it buys securities (such as a government bond or treasury bill), it in effect creates money. The central bank exchanges money for the security, increasing the money supply while lowering the supply of the specific security. Conversely, selling of securities by the central bank reduces the money supply.
Open market operations usually take the form of:
All of these interventions can also influence the foreign exchange market and thus the exchange rate. For example, the People's Bank of China and the Bank of Japan have on occasion bought several hundred billions of U.S. Treasuries, presumably in order to stop the decline of the U.S. dollar versus the renminbi and the yen.
Capital requirements.
All banks are required to hold a certain percentage of their assets as capital, a rate which may be established by the central bank or the banking supervisor. For international banks, including the 55 member central banks of the Bank for International Settlements, the threshold is 8% (see the Basel Capital Accords) of risk-adjusted assets, whereby certain assets (such as government bonds) are considered to have lower risk and are either partially or fully excluded from total assets for the purposes of calculating capital adequacy. Partly due to concerns about asset inflation and repurchase agreements, capital requirements may be considered more effective than reserve requirements in preventing indefinite lending: when at the threshold, a bank cannot extend another loan without acquiring further capital on its balance sheet.
Reserve requirements.
Historically, bank reserves have formed only a small fraction of deposits, a system called fractional reserve banking. Banks would hold only a small percentage of their assets in the form of cash reserves as insurance against bank runs. Over time this process has been regulated and insured by central banks. Such legal reserve requirements were introduced in the 19th century as an attempt to reduce the risk of banks overextending themselves and suffering from bank runs, as this could lead to knock-on effects on other overextended banks. "See also money multiplier."
As the early 20th century gold standard was undermined by inflation and the late 20th century fiat dollar hegemony evolved, and as banks proliferated and engaged in more complex transactions and were able to profit from dealings globally on a moment's notice, these practices became mandatory, if only to ensure that there was some limit on the ballooning of money supply. Such limits have become harder to enforce. The People's Bank of China retains (and uses) more powers over reserves because the yuan that it manages is a non-convertible currency.
Loan activity by banks plays a fundamental role in determining the money supply. The central-bank money after aggregate settlement – "final money" – can take only one of two forms:
The currency component of the money supply is far smaller than the deposit component. Currency, bank reserves and institutional loan agreements together make up the monetary base, called M1, M2 and M3. The Federal Reserve Bank stopped publishing M3 and counting it as part of the money supply in 2006.
Exchange requirements.
To influence the money supply, some central banks may require that some or all foreign exchange receipts (generally from exports) be exchanged for the local currency. The rate that is used to purchase local currency may be market-based or arbitrarily set by the bank. This tool is generally used in countries with non-convertible currencies or partially convertible currencies. The recipient of the local currency may be allowed to freely dispose of the funds, required to hold the funds with the central bank for some period of time, or allowed to use the funds subject to certain restrictions. In other cases, the ability to hold or use the foreign exchange may be otherwise limited.
In this method, money supply is increased by the central bank when it purchases the foreign currency by issuing (selling) the local currency. The central bank may subsequently reduce the money supply by various means, including selling bonds or foreign exchange interventions.
Margin requirements and other tools.
In some countries, central banks may have other tools that work indirectly to limit lending practices and otherwise restrict or regulate capital markets. For example, a central bank may regulate margin lending, whereby individuals or companies may borrow against pledged securities. The margin requirement establishes a minimum ratio of the value of the securities to the amount borrowed.
Central banks often have requirements for the quality of assets that may be held by financial institutions; these requirements may act as a limit on the amount of risk and leverage created by the financial system. These requirements may be direct, such as requiring certain assets to bear certain minimum credit ratings, or indirect, by the central bank lending to counterparties only when security of a certain quality is pledged as collateral.
Limits on policy effects.
Although the perception by the public may be that the "central bank" controls some or all interest rates and currency rates, economic theory (and substantial empirical evidence) shows that it is impossible to do both at once in an open economy. Robert Mundell's "impossible trinity" is the most famous formulation of these limited powers, and postulates that it is impossible to target monetary policy (broadly, interest rates), the exchange rate (through a fixed rate) and maintain free capital movement. Since most Western economies are now considered "open" with free capital movement, this essentially means that central banks may target interest rates or exchange rates with credibility, but not both at once.
In the most famous case of policy failure, Black Wednesday, George Soros arbitraged the pound sterling's relationship to the ECU and (after making $2 billion himself and forcing the UK to spend over $8bn defending the pound) forced it to abandon its policy. Since then he has been a harsh critic of clumsy bank policies and argued that no one should be able to do what he did.
The most complex relationships are those between the yuan and the US dollar, and between the euro and its neighbours. The situation in Cuba is so exceptional as to require the Cuban peso to be dealt with simply as an exception, since the United States forbids direct trade with Cuba. US dollars were ubiquitous in Cuba's economy after its legalization in 1991, but were officially removed from circulation in 2004 and replaced by the convertible peso.
Banking supervision and other activities.
In some countries a central bank, through its subsidiaries, controls and monitors the banking sector. In other countries banking supervision is carried out by a government department such as the UK Treasury, or by an independent government agency, for example, UK's Financial Conduct Authority. It examines the banks' balance sheets and behaviour and policies toward consumers. Apart from refinancing, it also provides banks with services such as transfer of funds, bank notes and coins or foreign currency. Thus it is often described as the "bank of banks".
Many countries will monitor and control the banking sector through several different agencies and for different purposes. the Bank regulation in the United States for example is highly fragmented with 3 federal agencies, the Federal Deposit Insurance Corporation, the Federal Reserve Board, or Office of the Comptroller of the Currency and numerous others on the state and the private level. There is usually significant cooperation between the agencies. For example, money center banks, deposit-taking institutions, and other types of financial institutions may be subject to different (and occasionally overlapping) regulation. Some types of banking regulation may be delegated to other levels of government, such as state or provincial governments.
Any cartel of banks is particularly closely watched and controlled. Most countries control bank mergers and are wary of concentration in this industry due to the danger of groupthink and runaway lending bubbles based on a single point of failure, the credit culture of the few large banks.
Independence.
Governments generally have some degree of influence over even "independent" central banks; the aim of independence is primarily to prevent short-term interference. For example, the Board of Governors of the U.S. Federal Reserve are nominated by the President of the U.S. and confirmed by the Senate, publishes verbatim transcripts, and balance sheets are audited by the Government Accountability Office.
In the 2000s there has been a trend towards increasing the independence of central banks as a way of improving long-term economic performance. While a large volume of economic research has been done to define the relationship between central bank independence and economic performance, the results are ambiguous.
Advocates of central bank independence argue that a central bank which is too susceptible to political direction or pressure may encourage economic cycles ("boom and bust"), as politicians may be tempted to boost economic activity in advance of an election, to the detriment of the long-term health of the economy and the country. In this context, independence is usually defined as the central bank's operational and management independence from the government.
The literature on central bank independence has defined a number of types of independence.
It is argued that an independent central bank can run a more credible monetary policy, making market expectations more responsive to signals from the central bank. Both the Bank of England (1997) and the European Central Bank have been made independent and follow a set of published inflation targets so that markets know what to expect. Even the People's Bank of China has been accorded great latitude, though in the People's Republic of China the official role of the bank remains that of a national bank rather than a central bank, underlined by the official refusal to "unpeg" the yuan or to revalue it "under pressure". The People's Bank of China's independence can thus be read more as independence from the USA, which rules the financial markets, rather than from the Communist Party of China which rules the country. The fact that the Communist Party is not elected also relieves the pressure to please people, increasing its independence.
International organizations such as the World Bank, the Bank for International Settlements (BIS) and the International Monetary Fund (IMF) strongly support central bank independence. 

</doc>
<doc id="5667" url="https://en.wikipedia.org/wiki?curid=5667" title="Chlorine">
Chlorine

Chlorine is a chemical element with symbol Cl and atomic number 17. It has a relative atomic mass of about 35.5. Chlorine is in the halogen group (17) and is the second lightest halogen, following fluorine. The element is a yellow-green gas under standard conditions, where it forms diatomic molecules. Chlorine has the highest electron affinity and the third highest electronegativity of all the reactive elements. For this reason, chlorine is a strong oxidizing agent. Free chlorine is rare on Earth, and is usually a result of direct or indirect oxidation by oxygen.
The most common compound of chlorine, sodium chloride (common salt), has been known since ancient times. Around 1630, chlorine gas was first synthesized in a chemical reaction, but not recognized as a fundamentally important substance. Carl Wilhelm Scheele wrote a description of chlorine gas in 1774, supposing it to be an oxide of a new element. In 1809, chemists suggested that the gas might be a pure element, and this was confirmed by Sir Humphry Davy in 1810, who named it from .
Nearly all chlorine in the Earth's crust occurs as chloride in various ionic compounds, including table salt. It is the second most abundant halogen and 21st most abundant chemical element in Earth's crust. Elemental chlorine is commercially produced from brine by electrolysis. The high oxidizing potential of elemental chlorine led to the development of commercial bleaches and disinfectants, and a reagent for many processes in the chemical industry. Chlorine is used in the manufacture of a wide range of consumer products, about two-thirds of them organic chemicals such as polyvinyl chloride, and many intermediates for production of plastics and other end products which do not contain the element. As a common disinfectant, elemental chlorine and chlorine-generating compounds are used more directly in swimming pools to keep them clean and sanitary.
In the form of chloride ions, chlorine is necessary to all known species of life. Other types of chlorine compounds are rare in living organisms, and artificially produced chlorinated organics range from inert to toxic. In the upper atmosphere, chlorine-containing organic molecules such as chlorofluorocarbons have been implicated in ozone depletion. Small quantities of elemental chlorine are generated by oxidation of chloride to hypochlorite in neutrophils as part of the immune response against bacteria. 
Elemental chlorine at high concentrations is extremely dangerous and poisonous for all living organisms, and was used in World War I as the first gaseous chemical warfare agent.
Characteristics.
Physical characteristics of chlorine and its compounds.
At standard temperature and pressure, two chlorine atoms form the diatomic molecule Cl2. This is a yellow-green gas that has a distinctive strong odor, familiar to most people from common household bleach. The bonding between the two atoms is relatively weak (only 242.580 ± 0.004 kJ/mol), which makes the Cl2 molecule highly reactive. The boiling point at standard pressure is around −34 ˚C, but it can be liquefied at room temperature with pressures above 740 kPa (107 psi).
Elemental chlorine is yellow-green, but the chloride ion, in common with other halide ions, has no color in either minerals or solutions (example, table salt). As with other halogens, chlorine atoms impart no color to organic chlorides when they replace hydrogen atoms organic compounds, such as tetrachloromethane. The melting point and density of these compounds is increased by substitution of chlorine in place of hydrogen. Compounds of chlorine with other halogens are colored, as are many chlorine oxides.
Chemical characteristics.
Along with fluorine, bromine, iodine, and astatine, chlorine is a member of the halogen series that forms the group 17 (formerly VII, VIIA, or VIIB) of the periodic table. Chlorine combines with almost all other elements to produce compounds that are usually called chlorides. Chlorine gas reacts with most organic compounds, and sluggishly supports the combustion of hydrocarbons.
Compounds.
Chlorine exists in all odd numbered oxidation states from −1 to +7, as well as the elemental state and +4 in chlorine dioxide, with respective oxidation states of 0 and 4+ (see table below, and also structures in chlorite). Chlorine typically has a −1 oxidation state in compounds, except for compounds containing fluorine, oxygen, and nitrogen, all of which are even more electronegative than chlorine. Progressing through the states, hydrochloric acid can be oxidized using manganese dioxide, or hydrogen chloride gas oxidized catalytically by air to form elemental chlorine gas.
Hydrolysis of free chlorine or disproportionation in water.
At 25 °C and atmospheric pressure, one liter of water dissolves 3.26 g or 1.125 L of gaseous chlorine. Chlorine dissolves in water to produce dissolved chlorine (Cl2), and by reversible reaction, hydrochloric acid, and hypochlorous acid:
The conversion to the right is called disproportionation because the ingredient chlorine both increases and decreases in formal oxidation state. The solubility of chlorine in water is increased if the water contains dissolved alkali hydroxide, and in this way, chlorine bleach is produced.
Chlorine gas exists only in a neutral or acidic solution.
Chlorides.
Chlorine combines with almost all elements to produce chlorides. Compounds with oxygen, nitrogen, xenon, and krypton are known, but are not formed by direct reaction of the elements. Chloride is one of the most common anions in nature. Hydrogen chloride and its aqueous solution, hydrochloric acid, are commercially produced by the megaton annually as valued intermediates and as undesirable pollutants.
Chlorine oxides.
Chlorine forms a variety of oxides: chlorine dioxide (ClO2), dichlorine monoxide (Cl2O), dichlorine hexoxide (Cl2O6), dichlorine heptoxide (Cl2O7). The anionic derivatives of these same oxides are also well known including hypochlorite (ClO−), chlorite (), chlorate (), and perchlorate (). The acid forms of these anions are hypochlorous acid (HOCl), chlorous acid (HClO2), chloric acid (HClO3) and perchloric acid (HClO4), respectively. The chloroxy cation chloryl (ClO2+) is known and has the same structure as chlorite but with a positive charge and chlorine in the +5 oxidation state. The compound "chlorine trioxide" does not occur, but is found in gas form as the dimeric dichlorine hexoxide (Cl2O6) with a +6 oxidation state. This compound in liquid or solid form disproportionates to a mixture of +5 and +7 oxidation states, occurring as the ionic compound chloryl perchlorate, .
In hot concentrated alkali solution hypochlorite disproportionates:
Sodium chlorate and potassium chlorate can be crystallized from solutions formed by the above reactions. If their crystals are heated to a high temperature, they undergo a further, final disproportionation:
This same progression from chloride to perchlorate can be accomplished by electrolysis. The anode reaction progression is:
Each step is accompanied at the cathode by
Interhalogen compounds.
Chlorine forms a variety of interhalogen compounds, such as the chlorine fluorides, chlorine monofluoride (), chlorine trifluoride (), chlorine pentafluoride (). Chlorides of bromine and iodine are also known. Chlorine oxidizes bromide and iodide salts to bromine and iodine, respectively, but cannot oxidize fluoride salts to fluorine.
Organochlorine compounds.
Chlorine often imparts desired properties to an organic compound, in part owing to the relative inertness of the C-Cl bond. Organic chloride compounds tend to be less reactive in nucleophilic substitution reactions than the corresponding bromide or iodide derivatives, but they tend to be cheaper. Chlorine is used extensively in polymers and in organic chemistry.
Several general approaches exist for the formation of C-Cl bonds. Like the other halogens, chlorine undergoes electrophilic addition reactions. Notable is the chlorination of alkenes and aromatic compounds with a Lewis acid catalyst. Industries often avoid using chlorine directly, preferring cheaper reagents such as hydrogen chloride, as deployed in hydrohalogenation and oxychlorination. The organochlorine compound produced on the largest scale is polyvinyl chloride, 23 billion kilograms of which were produced in 2000.
Occurrence.
Essentially no chlorine was created in the Big Bang. Chlorine in the universe is created and distributed through the interstellar medium from nucleosynthesis in supernovae, via the r-process. This chlorine provides the supply found in the Solar System.
In meteorites and on Earth, chlorine is found primarily as the chloride ion in minerals. In the Earth's crust, chlorine is present at average concentrations of about 126 parts per million, predominantly in such minerals as "halite" (sodium chloride), "sylvite" (potassium chloride), and "carnallite" (potassium magnesium chloride hexahydrate).
Chloride is a component of the salt deposited in the earth and dissolved in the oceans — about 1.9% of the mass of seawater is chloride ions. Higher concentrations of chloride are found in the Dead Sea and in underground brine deposits and evaporites. Most chloride salts are soluble in water; therefore, chloride-containing minerals are usually found in abundance only in dry climates or deep underground.
More than 2000 naturally occurring organic chlorine compounds are known.
Isotopes.
Chlorine has a wide range of isotopes. The two stable isotopes are 35Cl (75.77%) and 37Cl (24.23%). Together they give chlorine a relative atomic mass of 35.4527 g/mol. The half-integer value for chlorine's weight caused some confusion in the early days of chemistry, when chemists were working on the postulate that atoms were composed of units of hydrogen (see Proust's law) and the existence of isotopes was unsuspected.
Radioactive 36Cl exists in the environment in a ratio of about 7x10−13 to 1 with stable isotopes. 36Cl is produced in the atmosphere by spallation of 36Ar from collisions of cosmic ray protons. In the subsurface environment, 36Cl is generated primarily as a result of neutron capture by 35Cl or muon capture by 40Ca. 36Cl decays to 36S and to 36Ar, with a combined half-life of 308,000 years. The half-life of this isotope makes it suitable for geologic dating in the range of 60,000 to 1 million years. Large amounts of 36Cl were produced by irradiation of seawater during atmospheric detonations of nuclear weapons between 1952 and 1958. The residence time of 36Cl in the atmosphere is about 1 week. Thus, as an event marker of 1950s water in soil and groundwater, 36Cl is also useful for dating waters less than 50 years before the present. 36Cl has seen other uses in the geological sciences, including dating ice and sediments.
History.
The most common compound of chlorine, sodium chloride, has been known since ancient times; archaeologists have found evidence that rock salt was used as early as 3000 BC and brine as early as 6000 BC. Around 1630, chlorine was recognized as a gas by the Flemish chemist and physician Jan Baptist van Helmont.
Elemental chlorine was first prepared and studied in 1774 by Swedish chemist Carl Wilhelm Scheele, and, therefore, he is credited for its discovery. He called it ""dephlogisticated muriatic acid air"" since it is a gas (then called "airs") and it came from hydrochloric acid (then known as "muriatic acid"). He failed to establish chlorine as an element, mistakenly thinking that it was the oxide obtained from the hydrochloric acid (see phlogiston theory). He named the new element within this oxide as "muriaticum". Regardless of what he thought, Scheele did isolate chlorine by reacting MnO2 (as the mineral pyrolusite) with HCl:
Scheele observed several of the properties of chlorine: the bleaching effect on litmus, the deadly effect on insects, the yellow green color, and the smell similar to aqua regia.
At the time, common chemical theory was: any acid is a compound that contains oxygen (still sounding in the German and Dutch names of oxygen: "sauerstoff" or "zuurstof", both translating into English as "acid substance"), so a number of chemists, including Claude Berthollet, suggested that Scheele's "dephlogisticated muriatic acid air" must be a combination of oxygen and the yet undiscovered element, "muriaticum".
In 1809, Joseph Louis Gay-Lussac and Louis-Jacques Thénard tried to decompose "dephlogisticated muriatic acid air" by reacting it with charcoal to release the free element "muriaticum" (and carbon dioxide). They did not succeed and published a report in which they considered the possibility that "dephlogisticated muriatic acid air" is an element, but were not convinced.
In 1810, Sir Humphry Davy tried the same experiment again, and concluded that it is an element, and not a compound. He named this new element as chlorine, from the Greek word χλωρος ("chlōros"), meaning green-yellow. The name "halogen", meaning "salt producer", was originally used for chlorine in 1811 by Johann Salomo Christoph Schweigger. This term was later used as a generic term to describe all the elements in the chlorine family (fluorine, bromine, iodine), after a suggestion by Jöns Jakob Berzelius in 1842. In 1823, Michael Faraday liquefied chlorine for the first time, and demonstrated that what was then known as "solid chlorine" had a structure of chlorine hydrate (Cl2·H2O).
Bleaching and disinfection.
Chlorine gas was first used by French chemist Claude Berthollet to bleach textiles in 1785. Modern bleaches resulted from further work by Berthollet, who first produced sodium hypochlorite in 1789 in his laboratory in the town of Javel (now part of Paris, France), by passing chlorine gas through a solution of sodium carbonate. The resulting liquid, known as ""Eau de Javel"" ("Javel water"), was a weak solution of sodium hypochlorite. This process was not very efficient, and alternative production methods were sought. Scottish chemist and industrialist Charles Tennant first produced a solution of calcium hypochlorite ("chlorinated lime"), then solid calcium hypochlorite (bleaching powder). These compounds produced low levels of elemental chlorine, and could be more efficiently transported than sodium hypochlorite, which remained as dilute solutions because when purified to eliminate water, it became a dangerously powerful and unstable oxidizer. Near the end of the nineteenth century, E. S. Smith patented a method of sodium hypochlorite production involving electrolysis of brine to produce sodium hydroxide and chlorine gas, which then mixed to form sodium hypochlorite. This is known as the chloralkali process, first introduced on an industrial scale in 1892, and now the source of most elemental chlorine and sodium hydroxide. A related low-temperature electrolysis reaction, the Hooker process, is now used to produce bleach and sodium hypochlorite.
Elemental chlorine solutions dissolved in chemically basic water (sodium and calcium hypochlorite) were first used as anti-putrifaction agents and disinfectants in the 1820s, in France, long before the establishment of the germ theory of disease. This work is mainly due to Antoine-Germain Labarraque, who adapted Berthollet's "Javel water" bleach and other chlorine preparations (for a more complete history, see below). Elemental chlorine has since served a continuous function in topical antisepsis (wound irrigation solutions and the like) and public sanitation, particularly in swimming and drinking water.
Chlorine gas was first used as a weapon on April 22, 1915, at Ypres by the German Army. The effect of this weapon on the allies was disastrous because the existing gas masks were difficult to deploy and had not been broadly distributed.
Historically important chlorine compounds.
In 1826, silver chloride was used to produce photographic images for the first time. Chloroform was first used as an anesthetic in 1847.
Polyvinyl chloride (PVC) was invented in 1912, initially without a purpose.
Production.
In industry, elemental chlorine is usually produced by the electrolysis of sodium chloride dissolved in water. This method, the chloralkali process industrialized in 1892, now provides most industrial chlorine gas. Along with chlorine, the method yields hydrogen gas and sodium hydroxide, which is the most valuable product. The process proceeds according to the following chemical equation:
The electrolysis of chloride solutions all proceed according to the following equations:
Overall process: 2 NaCl (or KCl) + 2 H2O → Cl2 + H2 + 2 NaOH (or KOH)
In diaphragm cell electrolysis, an asbestos (or polymer-fiber) diaphragm separates a cathode and an anode, preventing the chlorine forming at the anode from re-mixing with the sodium hydroxide and the hydrogen formed at the cathode. The salt solution (brine) is continuously fed to the anode compartment and flows through the diaphragm to the cathode compartment, where the caustic alkali is produced and the brine is partially depleted. Diaphragm methods produce dilute and slightly impure alkali, but they are not burdened with the problem of mercury disposal and they are more energy efficient. 
Membrane cell electrolysis employs permeable membrane as an ion exchanger. Saturated sodium (or potassium) chloride solution is passed through the anode compartment, leaving at a lower concentration. This method is more efficient than the diaphragm cell and produces very pure sodium (or potassium) hydroxide at about 32% concentration, but requires very pure brine.
Oxidation of HCl.
In the Deacon Process, hydrogen chloride recovered from the production of organochlorine compounds is recovered as chlorine. The process relies on oxidation using oxygen:
The reaction requires a catalyst. As introduced by Deacon, early catalysts were based on copper. Commercial processes, such as the Mitsui MT-Chlorine Process, have switched to chromium and ruthenium-based catalysts.
Laboratory methods.
Small batches of chlorine gas are prepared in the laboratory by combining hydrochloric acid and manganese dioxide. In a second method, a strong acid such as sulfuric acid or hydrochloric acid is reacted with sodium hypochlorite solution to release chlorine gas. In a third method, one of the same strong acids is reacted with sodium chlorate to produce chlorine gas and chlorine dioxide gas.
Applications.
Production of industrial and consumer products.
Chlorine is primarily used in the production of a wide range of industrial and consumer products and materials, including plastics, solvents for dry cleaning and metal degreasing, textiles, agrochemicals, pharmaceuticals, insecticides, dyestuffs, and household cleaning products.
Many important industrial products are produced via organochlorine intermediates. Examples include polycarbonates, polyurethanes, silicones, polytetrafluoroethylene, carboxymethyl cellulose, and propylene oxide. Like the other halogens, chlorine participates in free-radical substitution reactions with hydrogen-containing organic compounds. When applied to organic substrates, reaction is often—but not invariably—non-regioselective, and, hence, may result in a mixture of isomeric products. It is often difficult to control the degree of substitution as well, so multiple substitutions are common. If the different reaction products are easily separated (by distillation or the like), substitutive free-radical chlorination (in some cases accompanied by concurrent thermal dehydrochlorination) may be a useful synthetic route. Industrial examples of this are the production of methyl chloride, methylene chloride, chloroform, and carbon tetrachloride from methane, allyl chloride from propylene, and trichloroethylene, and tetrachloroethylene from 1,2-dichloroethane.
Quantitatively, of all elemental chlorine produced, about 63% is used in the manufacture of organic compounds, and 18% in the manufacture of inorganic chlorine compounds. About 15,000 chlorine compounds are used commercially. The remaining 19% of chlorine produced is used for bleaches and disinfection products. The most significant of organic compounds in terms of production volume are 1,2-dichloroethane and vinyl chloride, intermediates in the production of PVC. Other particularly important organochlorines are methyl chloride, methylene chloride, chloroform, vinylidene chloride, trichloroethylene, perchloroethylene, allyl chloride, epichlorohydrin, chlorobenzene, dichlorobenzenes, and trichlorobenzenes. The major inorganic compounds include HCl, Cl2O, HOCl, NaClO3, chlorinated isocyanurates, AlCl3, SiCl4, SnCl4, PCl3, PCl5, POCl3, AsCl3, SbCl3, SbCl5, BiCl3, S2Cl2, SCl2, SOCI2, ClF3, ICl, ICl3, TiCl3, TiCl4, MoCl5, FeCl3, ZnCl2, etc.
In the past, pulp bleaching was often done with elemental chlorine, producing organochlorine pollution; today, that process is prohibited by environmental laws. Chlorine is used either in chlorine dioxide and sodium hypochlorite stages in elemental chlorine free (ECF) bleaching, or not at all (total chlorine free or TCF bleaching).
Sanitation, disinfection, and antisepsis.
Combating putrefaction.
In France (as elsewhere), animal intestines were processed to make musical instrument strings, Goldbeater's skin and other products. This was done in "gut factories" ("boyauderies"), and it was an odiferous and unhealthy process. In or about 1820, the "Société d'encouragement pour l'industrie nationale" offered a prize for the discovery of a method, chemical or mechanical, for separating the peritoneal membrane of animal intestines without putrefaction. It was won by Antoine-Germain Labarraque, a 44-year-old French chemist and pharmacist who had discovered that Berthollet's chlorinated bleaching solutions (""Eau de Javel"") not only destroyed the smell of putrefaction of animal tissue decomposition, but also retarded the decomposition process itself.
Labarraque's research resulted in the use of chlorides and hypochlorites of lime (calcium hypochlorite) and of sodium (sodium hypochlorite) in the "boyauderies." The same chemicals were found to be useful in the routine disinfection and deodorization of latrines, sewers, markets, abattoirs, anatomical theatres and morgues. They were also used with success in hospitals, lazarets, prisons, infirmaries (both on land and at sea), magnaneries, stables, cattle-sheds, etc.; and for exhumations, embalming, during outbreaks of epidemic illness, fever, and blackleg in cattle.
Against infection and contagion.
Labarraque's chlorinated lime and soda solutions have been advocated since 1828 to prevent infection (called "contagious infection", presumed to be transmitted by "miasmas"), and to treat putrefaction of existing wounds, including septic wounds. In this 1828 work, Labarraque recommended that doctors breathe chlorine, wash their hands in chlorinated lime, and even sprinkle chlorinated lime about the patients' beds in cases of "contagious infection". In 1828, the contagion of infections was well known, even though the agency of the microbe was not realized or discovered until more than half a century later.
During the Paris cholera outbreak of 1832, large quantities of so-called "chloride of lime" were used to disinfect the capital. This was not simply modern calcium chloride, but contained chlorine gas dissolved in lime-water (dilute calcium hydroxide) to form calcium hypochlorite (chlorinated lime). Labarraque's discovery helped to remove the terrible stench of decay from hospitals and dissecting rooms, and, by doing so, effectively deodorised the Latin Quarter of Paris. These "putrid miasmas" were thought by many to be responsible for the spread of "contagion" and "infection" – both words used before the germ theory of infection. Chloride of lime was used for destroying odors and "putrid matter". One source has claimed chloride of lime was used by Dr. John Snow to disinfect water from the cholera-contaminated well that was feeding the Broad Street pump in 1854 London, though three other reputable sources that described the famous Broad Street pump cholera epidemic do not mention Snow performing any disinfection of water from that well. One reference makes it clear that chloride of lime was used to disinfect the offal and filth in the streets surrounding the Broad Street pump—a common practice in mid-nineteenth century England.
Semmelweis and experiments with antisepsis.
Perhaps the most famous application of Labarraque's chlorine and chemical base solutions was in 1847, when Ignaz Semmelweis used (first) chlorine-water (simply chlorine dissolved in pure water), then cheaper chlorinated lime solutions, to deodorize the hands of Austrian doctors, which Semmelweis noticed still carried the stench of decomposition from the dissection rooms to the patient examination rooms. Semmelweis, still long before the germ theory of disease, had theorized that "cadaveric particles" were somehow transmitting decay from fresh medical cadavers to living patients, and he used the well-known "Labarraque's solutions" as the only known method to remove the smell of decay and tissue decomposition (which he found that soap did not). The solutions proved to be far more effective germicide antiseptics than soap (Semmelweis was also aware of their greater efficacy, but not the reason), and this resulted in Semmelweis's (later) celebrated success in stopping the transmission of childbed fever ("puerperal fever") in the maternity wards of Vienna General Hospital in Austria in 1847.
Much later, during World War I in 1916, a standardized and diluted modification of Labarraque's solution containing hypochlorite (0.5%) and boric acid as an acidic stabilizer, was developed by Henry Drysdale Dakin (who gave full credit to Labarraque's prior work in this area). Called Dakin's solution, the method of wound irrigation with chlorinated solutions allowed antiseptic treatment of a wide variety of open wounds, long before the modern antibiotic era. A modified version of this solution continues to be employed in wound irrigation in modern times, where it remains effective against bacteria that are resistant to multiple antibiotics (see Century Pharmaceuticals).
Public sanitation.
By 1918, the US Department of Treasury called for all drinking water to be disinfected with chlorine. Chlorine is presently an important chemical for water purification (such as in water treatment plants), in disinfectants, and in bleach. Chlorine in water is more than three times as effective as a disinfectant against "Escherichia coli" than an equivalent concentration of bromine, and is more than six times more effective than an equivalent concentration of iodine.
Chlorine is usually used (in the form of hypochlorous acid) to kill bacteria and other microbes in drinking water supplies and public swimming pools. In most private swimming pools, chlorine itself is not used, but rather sodium hypochlorite, formed from chlorine and sodium hydroxide, or solid tablets of chlorinated isocyanurates. The drawback of using chlorine in swimming pools is that the chlorine reacts with the proteins in human hair and skin (see Hypochlorous acid), and becomes chemically bonded. Even small water supplies are now routinely chlorinated.
It is often impractical to store and use poisonous chlorine gas for water treatment, so alternative methods of adding chlorine are used. These include hypochlorite solutions, which gradually release chlorine into the water, and compounds like sodium dichloro-s-triazinetrione (dihydrate or anhydrous), sometimes referred to as "dichlor", and trichloro-s-triazinetrione, sometimes referred to as "trichlor". These compounds are stable while solid and may be used in powdered, granular, or tablet form. When added in small amounts to pool water or industrial water systems, the chlorine atoms hydrolyze from the rest of the molecule forming hypochlorous acid (HOCl), which acts as a general biocide, killing germs, micro-organisms, algae, and so on.
Use as a weapon.
World War I.
Chlorine gas, also known as bertholite, was first used as a weapon in World War I by Germany on April 22, 1915 in the Second Battle of Ypres. As described by the soldiers, it had the distinctive smell of a mixture of pepper and pineapple. It also tasted metallic and stung the back of the throat and chest. Chlorine reacts with water in the mucosa of the lungs to form hydrochloric acid, an destructive irritant that can be lethal. Human respiratory systems can be protected from chlorine gas by gas masks with activated charcoal or other filters, which makes chlorine gas much less lethal than other chemical weapons. It was pioneered by a German scientist later to be a Nobel laureate, Fritz Haber of the Kaiser Wilhelm Institute in Berlin, in collaboration with the German chemical conglomerate IG Farben, which developed methods for discharging chlorine gas against an entrenched enemy. Some say that Haber's role in the development of chlorine as a deadly weapon drove his wife, Clara Immerwahr, to suicide. After its first use, both sides in the conflict used chlorine as a chemical weapon, but it was soon replaced by the more deadly phosgene and mustard gas. Theodore Gray wrote in his book "The Elements: A Visual Exploration of Every Atom in the Universe," "Chlorine was used as a poison gas during the grueling trench-warfare phase. Soldiers would position a line of gas cylinders at the front lines, wait for the wind to shift towards the enemy, then open the valves and run like hell. This practice---sometimes overseen personally by Fritz Haber, a man whose positive contributions to humanity are listed under nitrogen (7)—was slowly phased out as experience showed that roughly equal numbers of soldiers on both sides died regardless of who set off the gas."
Iraq War.
Chlorine gas was also used by insurgents against the local population and coalition forces in the Iraq War in the form of chlorine bombs. On March 17, 2007, for example, three chlorine-filled trucks were detonated in the Anbar province killing two and sickening over 350. Other chlorine bomb attacks resulted in higher death tolls, with more than 30 deaths on two separate occasions. Most of the deaths were caused by the force of the explosions rather than the effects of chlorine, since the toxic gas is readily dispersed and diluted in the atmosphere by the blast. The Iraqi authorities have tightened security for elemental chlorine, which is essential for providing safe drinking water to the population.
Syrian Civil War.
There have been allegations of chlorine gas attacks during the Syrian Civil War such as the 2014 Kafr Zita chemical attack.
Islamic State of Iraq and the Levant (ISIL/ISIS).
On October 24, 2014, it was reported that the Islamic State of Iraq and the Levant had used chlorine gas in the town of Duluiyah, Iraq.
Laboratory analysis of clothing and soil samples confirmed the use of chlorine gas against Kurdish Peshmerga Forces in a vehicle-borne improvised explosive device attack on January 23, 2015 at the Highway 47 Kiske Junction near Mosul.
Health effects and hazards.
Chlorine is a toxic gas that attacks the respiratory system, eyes, and skin. Because it is denser than air, it tends to accumulate at the bottom of poorly ventilated spaces. Chlorine gas is a strong oxidizer, which may react with flammable materials.
Chlorine is detectable with measuring devices in concentrations as low as 0.2 parts per million (ppm), and by smell at 3 ppm. Coughing and vomiting may occur at 30 ppm and lung damage at 60 ppm. About 1000 ppm can be fatal after a few deep breaths of the gas. The IDLH (immediately dangerous to life and health) concentration is 10 ppm. Breathing lower concentrations can aggravate the respiratory system and exposure to the gas can irritate the eyes. The toxicity of chlorine comes from its oxidizing power. When chlorine is inhaled at concentrations above 30 ppm, it reacts with water and cells, which change it into hydrochloric acid (HCl) and hypochlorous acid (HClO).
When used at specified levels for water disinfection, the reaction of chlorine with water is not a major concern for human health. Other materials present in the water may generate disinfection by-products that are associated with negative effects on human health.
In the United States, the Occupational Safety and Health Administration (OSHA) has set the permissible exposure limit for elemental chlorine at 1 ppm, or 3 mg/m3. The National Institute for Occupational Safety and Health has designated a recommended exposure limit of 0.5 ppm over 15 minutes.
In the home, accidents occur when hypochlorite bleach solutions come into contact with certain acidic drain-cleaners. Hyperchlorite bleach (a popular laundry additive) combined with amonia (another popular laundry additive) produces chloramines, another toxic group of chemicals.
Chlorine-induced cracking in structural materials.
Chlorine is widely used for purifying water, especially potable water supplies and water used in swimming pools. Several catastrophic collapses of swimming pool ceilings have occurred from chlorine induced stress corrosion cracking of stainless steel suspension rods. Some polymers are also sensitive to attack, including acetal resin and polybutene. Both materials were used in hot and cold water domestic plumbing, and stress corrosion cracking caused widespread failures in the USA in the 1980s and 1990s. The picture on the right shows a fractured acetal joint in a water supply system. The cracks started at injection molding defects in the joint and slowly grew until finally triggered. The fracture surface shows iron and calcium salts that were deposited in the leaking joint from the water supply before failure.
Chlorine-iron fire.
The element iron can combine with chlorine at high temperatures in a strong exothermic reaction, creating a "chlorine-iron fire". Chlorine-iron fires are a risk in chemical process plants, where much of the pipework used to carry chlorine gas is made of steel.
Organochlorine compounds as pollutants.
Some organochlorine compounds, either as industrial by-products or commercial end-products, are serious pollutants that persist in the environment, including some chlorinated pesticides and chlorofluorocarbons. Chlorine is added to pesticides and pharmaceuticals to make the molecules more resistant to enzymatic degradation by bacteria, insects, and mammals, but this property has the effect of reducing biodegradability and prolonging the residence time of these compounds in the environment. In this respect, chlorinated organics resemble fluorinated organics.

</doc>
<doc id="5668" url="https://en.wikipedia.org/wiki?curid=5668" title="Calcium">
Calcium

Calcium is a chemical element with symbol Ca and atomic number 20. Calcium is a soft gray alkaline earth metal, fifth-most-abundant element by mass in the Earth's crust. The ion Ca2+ is also the fifth-most-abundant dissolved ion in seawater by both molarity and mass, after sodium, chloride, magnesium, and sulfate. Free calcium metal is too reactive to occur in nature. Calcium is produced in supernova nucleosynthesis.
Calcium is essential for living organisms, in particular in cell physiology, where movement of the calcium ion into and out of the cytoplasm functions as a signal for many cellular processes. As a major material used in mineralization of bone, teeth and shells, calcium is the most abundant metal by mass in many animals.
Notable characteristics.
In chemical terms, calcium is reactive and soft for a metal; though harder than lead, it can be cut with a knife with difficulty. It is a silvery metallic element that must be extracted by electrolysis from a fused salt like calcium chloride. Once produced, it rapidly forms a gray-white coating of calcium oxide and calcium nitride for reacting with the oxygen and nitrogen in the air when exposed to it. In bulk form (typically as chips or "turnings"), the metal is somewhat difficult to ignite, more so even than magnesium chips; but, when lit, the metal burns in air with a brilliant high-intensity orange-red light. Calcium metal reacts with water, generating hydrogen gas at a rate rapid enough to be noticeable, but not fast enough at room temperature to generate much heat, making it useful for generating hydrogen. In powdered form, however, the reaction with water is extremely rapid, as the increased surface area of the powder accelerates the reaction with the water. Part of the reason for the slowness of the calcium–water reaction is a result of the metal being partly protected by insoluble white calcium hydroxide; in water solutions of acids, where this salt is soluble, calcium reacts vigorously.
With a density of 1.54 g/cm3, calcium is the lightest of the alkaline earth metals; magnesium (specific gravity 1.74) and beryllium (1.84) are denser though lighter in atomic mass. From strontium onward, the alkali earth metals become denser with increasing atomic mass.Calcium has two allotropes.
Calcium has a higher electrical resistivity than copper or aluminium, yet weight-for-weight, due to its much lower density, it is a better conductor than either. Its use as such in terrestrial applications is usually limited by its high reactivity with air; however, it has potential for use as wiring in off-world applications.
Calcium salts are colorless from any contribution of the calcium, and ionic solutions of calcium (Ca2+) are colorless as well. As with magnesium salts and other alkaline earth metal salts, calcium salts are often quite soluble in water. Notable exceptions include calcium hydroxide, calcium sulfate (unusual for sulfate salts), calcium carbonate and tricalcium phosphate. With the exception of calcium sulfate, even the insoluble calcium salts listed are in general more soluble than the transition metal counterparts. When in solution, the calcium ion varies remarkably to the human taste, being reported as mildly salty, sour, "mineral-like" or even "soothing." It is apparent that many animals can taste, or develop a taste, for calcium, and use this sense to detect the mineral in salt licks or other sources. In human nutrition, soluble calcium salts may be added to tart juices without much effect to the average palate.
Calcium is the fifth-most-abundant element by mass in the human body, where it is an important cellular ionic messenger with many functions. Calcium also serves as a structural element in bone. It is the relatively high-atomic-number calcium in the skeleton that causes bone to be radio-opaque. Of the human body's solid components after drying and burning of organics (as for example, after cremation), about a third of the total "mineral" mass remaining is the approximately one kilogram of calcium that composes the average skeleton (the remainder being mostly phosphorus and oxygen).
H and K lines.
Visible spectra of many stars, including the Sun, exhibit strong emission lines of singly ionized calcium. Prominent among these are the H-line at 3968.5 Å and the K line at 3933.7 Å of singly ionized calcium, or Ca II. When observing the Sun, or stars with low temperatures, the prominence of the H and K lines in the visible spectra can be an indication of strong magnetic activity in the chromosphere. Measurement of periodic variations of these active regions can also be used to deduce the rotation periods of these stars.
Compounds.
Calcium, combined with phosphate, forming hydroxylapatite, is the mineral portion of human and animal bones and teeth. The mineral portion of some corals can also be transformed into hydroxylapatite.
Calcium forms a family of ionic compounds known as lime; they are all alkaline white powders (at STP). Calcium carbonate (CaCO3) is one of the common compounds of calcium and occurs naturally as limestone and chalk. When heated at high temperature (above 825 °C), it forms calcium oxide (CaO), also known as quicklime. When added to water (H2O), quicklime vigorously reacts (hence its name) to form calcium hydroxide (Ca(OH)2). Also known as slaked lime, this substance is an inexpensive base material used throughout the chemical industry. When mixed with sand, it hardens into a mortar and is turned into plaster by carbon dioxide uptake. Mixed with other compounds, lime forms an important part of Portland cement.
When water percolates through limestone or other soluble carbonate-containing rocks, it partially dissolves the rock and causes cave formation with their characteristic stalactites and stalagmites, and also forms hard water. Other important calcium compounds are calcium nitrate, calcium sulfide, calcium chloride, calcium carbide, calcium cyanamide and calcium hypochlorite.
A few calcium compounds where calcium is in the oxidation state +1 have also been investigated recently. Charlotte Froese Fischer predicted that a Ca− ion would be stable; this ion was discovered experimentally in 1987.
Isotopes.
Calcium has five stable isotopes (40Ca, 42Ca, 43Ca, 44Ca and 46Ca), plus one more isotope (48Ca) that has such a long half-life that for all practical purposes it can also be considered stable. The 20% range in relative mass among naturally occurring calcium isotopes is greater than for any other element except hydrogen and helium. Calcium also has a cosmogenic isotope, radioactive 41Ca, which has a half-life of 103,000 years. Unlike cosmogenic isotopes that are produced in the atmosphere, 41Ca is produced by neutron activation of 40Ca. Most of its production is in the upper metre or so of the soil column, where the cosmogenic neutron flux is still sufficiently strong. 41Ca has received much attention in stellar studies because it decays to 41K, a critical indicator of solar-system anomalies.
Ninety-seven percent of naturally occurring calcium is in the form of 40Ca. 40Ca is one of the daughter products of 40K decay, along with 40Ar. While K–Ar dating has been used extensively in the geological sciences, the prevalence of 40Ca in nature has impeded its use in dating. Techniques using mass spectrometry and a double spike isotope dilution have been used for K-Ca age dating.
The most abundant isotope, 40Ca, has a nucleus of 20 protons and 20 neutrons. This is the heaviest stable isotope of any element that has equal numbers of protons and neutrons. In supernova explosions, calcium is formed from the reaction of carbon with various numbers of alpha particles (helium nuclei), until the most common calcium isotope (containing 10 helium nuclei) has been synthesized.
Isotope fractionation.
As with the isotopes of other elements, a variety of processes fractionate, or alter the relative abundance of, calcium isotopes. The best studied of these processes is the mass-dependent fractionation of calcium isotopes that accompanies the precipitation of calcium minerals, such as calcite, aragonite and apatite, from solution. Isotopically light calcium is preferentially incorporated into minerals, leaving the solution from which the mineral precipitated enriched in isotopically heavy calcium. At room temperature the magnitude of this fractionation is roughly 0.25‰ (0.025%) per atomic mass unit (AMU). Mass-dependent differences in calcium isotope composition conventionally are expressed the ratio of two isotopes (usually 44Ca/40Ca) in a sample compared to the same ratio in a standard reference material. 44Ca/40Ca varies by about 1% among common earth materials.
Calcium isotope fractionation during mineral formation has led to several applications of calcium isotopes. In particular, the 1997 observation by Skulan and DePaolo that calcium minerals are isotopically lighter than the solutions from which the minerals precipitate is the basis of analogous applications in medicine and in paleooceanography. In animals with skeletons mineralized with calcium the calcium isotopic composition of soft tissues reflects the relative rate of formation and dissolution of skeletal mineral. In humans changes in the calcium isotopic composition of urine have been shown to be related to changes in bone mineral balance. When the rate of bone formation exceeds the rate of bone resorption, soft tissue 44Ca/40Ca rises. Soft tissue 44Ca/40Ca falls when bone resorption exceeds bone formation. Because of this relationship, calcium isotopic measurements of urine or blood may be useful in the early detection of metabolic bone diseases like osteoporosis.
A similar system exists in the ocean, where seawater 44Ca/40Ca tends to rise when the rate of removal of Ca2+ from seawater by mineral precipitation exceeds the input of new calcium into the ocean, and fall when calcium input exceeds mineral precipitation. It follows that rising 44Ca/40Ca corresponds to falling seawater Ca2+ concentration, and falling 44Ca/40Ca corresponds to rising seawater Ca2+ concentration. In 1997 Skulan and DePaolo presented the first evidence of change in seawater 44Ca/40Ca over geologic time, along with a theoretical explanation of these changes. More recent papers have confirmed this observation, demonstrating that seawater Ca2+ concentration is not constant, and that the ocean probably never is in “steady state” with respect to its calcium input and output. This has important climatological implications, as the marine calcium cycle is closely tied to the carbon cycle (see below).
Geochemical cycling.
Calcium provides an important link between tectonics, climate and the carbon cycle. In the simplest terms, uplift of mountains exposes Ca-bearing rocks to chemical weathering and releases Ca2+ into surface water. This Ca2+ eventually is transported to the ocean where it reacts with dissolved CO2 to form limestone. Some of this limestone settles to the sea floor where it is incorporated into new rocks. Dissolved CO2, along with carbonate and bicarbonate ions, are referred to as dissolved inorganic carbon (DIC).
The actual reaction is more complicated and involves the bicarbonate ion (HCO3−) that forms when CO2 reacts with water at seawater pH:
Note that at ocean pH most of the CO2 produced in this reaction is immediately converted back into . The reaction results in a net transport of one molecule of CO2 from the ocean/atmosphere into the lithosphere.
The result is that each Ca2+ ion released by chemical weathering ultimately removes one CO2 molecule from the surficial system (atmosphere, ocean, soils and living organisms), storing it in carbonate rocks where it is likely to stay for hundreds of millions of years. The weathering of calcium from rocks thus scrubs CO2 from the ocean and atmosphere, exerting a strong long-term effect on climate.
Analogous cycles involving magnesium, and to a much smaller extent strontium and barium, have the same effect.
As the weathering of limestone (CaCO3) liberates equimolar amounts of Ca2+ and CO2, it has no net effect on the CO2 content of the atmosphere and ocean. The weathering of silicate rocks like granite, on the other hand, is a net CO2 sink because it produces abundant Ca2+ but very little CO2.
History.
Lime as building material was used since prehistoric times going as far back as 7000 to 14000 BC. Significant statues made from lime plaster date back into the 7 millennia BC were found in 'Ain Ghazal. The first dated lime kiln dates back to 2500 BC and was found in Khafajah mesopotamia. Calcium (from Latin , genitive "calcis", meaning "lime") was known as early as the first century when the Ancient Romans prepared lime as calcium oxide. Literature dating back to 975 AD notes that plaster of paris (calcium sulfate), is useful for setting broken bones. It was not isolated until 1808 in England when Sir Humphry Davy electrolyzed a mixture of lime and mercuric oxide. Calcium metal was not available in large scale until the beginning of the 20th century.
Occurrence.
Calcium is not naturally found in its elemental state. Calcium occurs most commonly in sedimentary rocks in the minerals calcite, dolomite and gypsum. It also occurs in igneous and metamorphic rocks chiefly in the silicate minerals: plagioclases, amphiboles, pyroxenes and garnets.
Applications.
Calcium is used
Nutrition.
Calcium is an important component of a healthy diet and a mineral necessary for life. The National Osteoporosis Foundation says, "Calcium plays an important role in building stronger, denser bones early in life and keeping bones strong and healthy later in life." Approximately 99 percent of the body's calcium is stored in the bones and teeth. The rest of the calcium in the body has other important uses, such as some exocytosis, especially neurotransmitter release, and muscle contraction. Intracellular calcium overload may lead some kind of cells to oxidative stress and apoptosis, and thus producing several diseases. In the electrical conduction system of the heart, calcium replaces sodium as the mineral that depolarizes the cell, proliferating the action potential. In cardiac muscle, sodium influx commences an action potential, but during potassium efflux, the cardiac myocyte experiences calcium influx, prolonging the action potential and creating a plateau phase of dynamic equilibrium. Long-term calcium deficiency can lead to rickets and poor blood clotting and in case of a menopausal woman, it can lead to osteoporosis, in which the bone deteriorates and there is an increased risk of fractures. While a lifelong deficit can affect bone and tooth formation, over-retention can cause hypercalcemia (elevated levels of calcium in the blood), impaired kidney function and decreased absorption of other minerals. Several sources suggest a correlation between high calcium intake (2000 mg per day, or twice the U.S. recommended daily allowance, equivalent to six or more glasses of milk per day) and prostate cancer. Vitamin D is needed to absorb calcium.
Dairy products, such as milk and cheese, are a well-known source of calcium. Some individuals are allergic to dairy products and even more people, in particular those of non Indo-European descent, are lactose-intolerant, leaving them unable to consume non-fermented dairy products in quantities larger than about half a liter per serving. Others, such as vegans, avoid dairy products for ethical and health reasons.
Many good vegetable sources of calcium exist, including seaweeds such as kelp, wakame, and hijiki; nuts and seeds like almonds, hazelnuts, sesame, and pistachio; blackstrap molasses; beans (especially soy beans); figs; quinoa; okra; rutabaga; broccoli; dandelion leaves; and kale. In addition, several foods and drinks, such as orange juice, soy milk, tofu, breakfast cereals, and breads are often fortified with calcium.
Numerous vegetables, notably spinach, chard, and rhubarb have a high calcium content, but they may also contain varying amounts of oxalic acid that binds calcium and reduces its absorption.
The same problem may to a degree affect the absorption of calcium from amaranth, collard greens, and chicory greens.
This process may also be related to the generation of calcium oxalate.
An overlooked source of calcium is eggshell, which can be ground into a powder and mixed into food or a glass of water.
The calcium content of most foods can be found in the USDA National Nutrient Database.
Dietary supplements.
Calcium supplements are used to prevent and to treat calcium deficiencies. Office of Dietary Supplements (National Institutes of Health) recommends that no more than 600 mg of supplement should be taken at a time because the percent of calcium absorbed decreases as the amount of calcium in the supplement increases. It is therefore recommended to spread doses throughout the day. Recommended daily calcium intake for adults ranges from 1000 to 1300 mg. Calcium supplements may have side effects such as bloating and constipation in some people. It is suggested that taking the supplements with food may aid in nullifying these side effects.
Vitamin D is added to some calcium supplements. Proper vitamin D status is important because vitamin D is converted to a hormone in the body, which then induces the synthesis of intestinal proteins responsible for calcium absorption.
Bone health.
Calcium supplementation is generally not necessary for maintaining bone mineral density, and carries risks that outweigh any benefits. Calcium intake is not significantly associated with hip fracture risk in either men nor women. The U.S. Preventive Service Task Force therefore recommends against daily supplementation of calcium or Vitamin D.
Cardiovascular impact.
A study investigating the effects of personal calcium supplement use on cardiovascular risk in the Women’s Health Initiative Calcium/Vitamin D Supplementation Study (WHI CaD Study) found a modestly increased risk of cardiovascular events, particularly myocardial infarction in postmenopausal women. A broad recommendation of calcium/vitamin D supplements is therefore not warranted. In contrast, the authors of a 2013 literature review concluded that the benefits of calcium supplementation, such as on bone health, appear to outweigh any risk calcium supplementation may theoretically pose to the cardiovascular health.
Cancer.
Overall, there is no strong evidence calcium supplementation helps prevent cancer in people: some studies suggest it might decrease the risk, but others suggest it might increase the risk. The National Cancer Institute, part of the National Institutes of Health, does not recommend the use of calcium supplements to prevent any type of cancer, due to the lack of evidence supporting its use for this purpose.
There is weak evidence calcium supplementation might have a preventative effect against developing colorectal adenomatous polyps, but the evidence is insufficient to recommend such supplementation as a beneficial practice.
Cell signaling.
The release of calcium ions from the sarcoplasmic reticulum into the cytoplasm is an essential intracellular signal, important in many cellular functions and processes, including muscle contraction, neuronal transmission as in an excitatory synapse, cellular motility (including the movement of flagella and cilia), fertilisation, cell growth or proliferation, learning and memory as with synaptic plasticity, and secretion of saliva.
Calcium signalling can be studied by loading a cell's cytoplasm with a calcium-sensitive fluorescent dye such as Fura-2. Many of these dyes were developed by Roger Y. Tsien.
Hazards and toxicity.
Compared with other metals, the calcium ion and most calcium compounds have low toxicity. This is not surprising given the very high natural abundance of calcium compounds in the environment and in organisms.
Calcium poses few serious environmental problems.
High calcium intakes or high calcium absorption were previously thought to contribute to the development of kidney stones. However, a high calcium intake has been associated with a lower risk for kidney stones in more recent research.
Acute calcium poisoning is rare, and difficult to achieve unless calcium compounds are administered intravenously. For example, the oral median lethal dose (LD50) for rats for calcium carbonate and calcium chloride are 6.45 and 1.4 g/kg, respectively.
Calcium metal is hazardous because of its sometimes-violent reactions with water and acids. Calcium metal is found in some drain cleaners, where it functions to generate heat and calcium hydroxide that saponifies the fats and liquefies the proteins (e.g., hair) that block drains. When swallowed calcium metal has the same effect on the mouth, esophagus and stomach, and can be fatal.
Excessive consumption of calcium carbonate antacids/dietary supplements (such as Tums) over a period of weeks or months can cause milk-alkali syndrome, with symptoms ranging from hypercalcemia to potentially fatal renal failure. What constitutes “excessive” consumption is not well known and, it is presumed, varies a great deal from person to person. Persons consuming more than 10 grams/day of CaCO3 (=4 g Ca) are at risk of developing milk-alkali syndrome, but the condition has been reported in at least one person consuming only 2.5 grams/day of CaCO3 (=1 g Ca), an amount usually considered moderate and safe.
Oral calcium supplements diminish the absorption of thyroxine when taken within four to six hours of each other. Thus, people taking both calcium and thyroxine run the risk of inadequate thyroid hormone replacement and thence hypothyroidism if they take them simultaneously or near-simultaneously.
Although some studies have suggested that excessive intake of calcium in the diet or as supplements could be associated with increased cardiovascular mortality, other studies found no risk, leading a review to conclude that any risk could only be ascertained with specific further research.

</doc>
<doc id="5669" url="https://en.wikipedia.org/wiki?curid=5669" title="Chromium">
Chromium

Chromium is a chemical element with symbol Cr and atomic number 24. It is the first element in Group 6. It is a steely-grey, lustrous, hard and brittle metal which takes a high polish, resists tarnishing, and has a high melting point. The name of the element is derived from the Greek word χρῶμα, "chrōma", meaning color, because many of its compounds are intensely colored.
Chromium oxide was used by the Chinese in the Qin dynasty over 2,000 years ago to coat metal weapons found with the Terracotta Army. Chromium was discovered as an element after it came to the attention of the Western world in the red crystalline mineral crocoite (lead(II) chromate), discovered in 1761 and initially used as a pigment. Louis Nicolas Vauquelin first isolated chromium metal from this mineral in 1797. Since Vauquelin's first production of metallic chromium, small amounts of native (free) chromium metal have been discovered in rare minerals, but these are not used commercially. Instead, nearly all chromium is commercially extracted from the single commercially viable ore chromite, which is iron chromium oxide (FeCr2O4). Chromite is also now the chief source of chromium for chromium pigments.
Ferrochromium alloy is commercially produced from chromite by silicothermic or aluminothermic reactions; and chromium metal by roasting and leaching processes followed by reduction with carbon and then aluminium. Chromium metal has proven of high value due to its high corrosion resistance and hardness. A major development was the discovery that steel could be made highly resistant to corrosion and discoloration by adding metallic chromium to form stainless steel. This application, along with chrome plating (electroplating with chromium) currently comprise 85% of the commercial use for the element, with applications for chromium compounds forming the remainder.
Trivalent chromium (Cr(III)) ion is possibly required in trace amounts for sugar and lipid metabolism, although the issue remains in debate. Whilst chromium metal and Cr(III) ions are not considered toxic, hexavalent chromium (Cr(VI)) is toxic and carcinogenic. Abandoned chromium production sites often require environmental cleanup.
Characteristics.
Physical.
Chromium is remarkable for its magnetic properties: it is the only elemental solid which shows antiferromagnetic ordering at room temperature (and below). Above 38 °C, it transforms into a paramagnetic state.
Passivation.
Chromium metal left standing in air is passivated by oxygen, forming a thin protective oxide surface layer. This layer is a spinel structure only a few atoms thick. It is very dense, and prevents the diffusion of oxygen into the underlying material. This barrier is in contrast to iron or plain carbon steels, where the oxygen migrates into the underlying material and causes rusting. The passivation can be enhanced by short contact with oxidizing acids like nitric acid. Passivated chromium is stable against acids. The opposite effect can be achieved by treatment with a strong reducing agent that destroys the protective oxide layer on the metal. Chromium metal treated in this way readily dissolves in weak acids.
Chromium, unlike metals such as iron and nickel, does not suffer from hydrogen embrittlement. However, it does suffer from nitrogen embrittlement, reacting with nitrogen from air and forming brittle nitrides at the high temperatures necessary to work the metal parts.
Occurrence.
Chromium is the 22nd most abundant element in Earth's crust with an average concentration of 100 ppm. Chromium compounds are found in the environment, due to erosion of chromium-containing rocks and can be distributed by volcanic eruptions. The concentrations range in soil is between 1 and 300 mg/kg, in sea water 5 to 800 µg/liter, and in rivers and lakes 26 µg/liter to 5.2 mg/liter.
Chromium is mined as chromite (FeCr2O4) ore. About two-fifths of the chromite ores and concentrates in the world are produced in South Africa, while Kazakhstan, India, Russia, and Turkey are also substantial producers. Untapped chromite deposits are plentiful, but geographically concentrated in Kazakhstan and southern Africa.
Although rare, deposits of native chromium exist. The Udachnaya Pipe in Russia produces samples of the native metal. This mine is a kimberlite pipe, rich in diamonds, and the reducing environment helped produce both elemental chromium and diamond.
The relation between Cr(III) and Cr(VI) strongly depends on pH and oxidative properties of the location, but in most cases, the Cr(III) is the dominating species, although in some areas the ground water can contain up to 39 µg/liter of total chromium of which 30 µg/liter is present as Cr(VI).
Isotopes.
Naturally occurring chromium is composed of three stable isotopes; 52Cr, 53Cr and 54Cr with 52Cr being the most abundant (83.789% natural abundance). 19 radioisotopes have been characterized with the most stable being 50Cr with a half-life of (more than) 1.8 years, and 51Cr with a half-life of 27.7 days. All of the remaining radioactive isotopes have half-lives that are less than 24 hours and the majority of these have half-lives that are less than 1 minute. This element also has 2 meta states.
53Cr is the radiogenic decay product of 53Mn (half-life = 3.74 million years). Chromium isotopic contents are typically combined with manganese isotopic contents and have found application in isotope geology. Mn-Cr isotope ratios reinforce the evidence from 26Al and 107Pd for the early history of the solar system. Variations in 53Cr/52Cr and Mn/Cr ratios from several meteorites indicate an initial 53Mn/55Mn ratio that suggests Mn-Cr isotopic composition must result from in-situ decay of 53Mn in differentiated planetary bodies. Hence 53Cr provides additional evidence for nucleosynthetic processes immediately before coalescence of the solar system.
The isotopes of chromium range in atomic mass from 43 u (43Cr) to 67 u (67Cr). The primary decay mode before the most abundant stable isotope, 52Cr, is electron capture and the primary mode after is beta decay. 53Cr has been posited as a proxy for atmospheric oxygen concentration.
Compounds.
Chromium is a member of the transition metals, in group 6. Chromium(0) has an electronic configuration of 4s13d5, owing to the lower energy of the high spin configuration. Chromium exhibits a wide range of possible oxidation states, where the +3 state is most stable energetically; the +3 and +6 states are most commonly observed in chromium compounds, whereas the +1, +4 and +5 states are rare.
The following is the Pourbaix diagram for chromium in pure water, perchloric acid or sodium hydroxide:
Chromium(III).
A large number of chromium(III) compounds are known. Chromium(III) can be obtained by dissolving elemental chromium in acids like hydrochloric acid or sulfuric acid. The ion has a similar radius (63 pm) to the ion (radius 50 pm), so they can replace each other in some compounds, such as in chrome alum and alum. When a trace amount of replaces in corundum (aluminium oxide, Al2O3), depending on the amount, either pink sapphire or the red-colored ruby is formed.
Chromium(III) ions tend to form octahedral complexes. The colors of these complexes is determined by the ligands attached to the Cr center. The commercially available chromium(III) chloride hydrate is the dark green complex Closely related compounds have different colors: pale green [CrCl(H2O)5Cl2 and the violet [Cr(H2O)6]Cl3. If water-free green chromium(III) chloride is dissolved in water then the green solution turns violet after some time, due to the substitution of water by chloride in the inner coordination sphere. This kind of reaction is also observed with solutions of chrome alum and other water-soluble chromium(III) salts.
Chromium(III) hydroxide (Cr(OH)3) is amphoteric, dissolving in acidic solutions to form [Cr(H2O)6]3+, and in basic solutions to form . It is dehydrated by heating to form the green chromium(III) oxide (Cr2O3), which is the stable oxide with a crystal structure identical to that of corundum.
Chromium(VI).
Chromium(VI) compounds are powerful oxidants at low or neutral pH. Most important are chromate anion () and dichromate (Cr2O72−) anions, which exist in equilibrium:
Chromium(VI) halides are known also and include the hexafluoride CrF6 and chromyl chloride ().
Sodium chromate is produced industrially by the oxidative roasting of chromite ore with calcium or sodium carbonate. The dominant species is therefore, by the law of mass action, determined by the pH of the solution. The change in equilibrium is visible by a change from yellow (chromate) to orange (dichromate), such as when an acid is added to a neutral solution of potassium chromate. At yet lower pH values, further condensation to more complex oxyanions of chromium is possible.
Both the chromate and dichromate anions are strong oxidizing reagents at low pH:
They are, however, only moderately oxidizing at high pH:
Chromium(VI) compounds in solution can be detected by adding an acidic hydrogen peroxide solution. The unstable dark blue chromium(VI) peroxide (CrO5) is formed, which can be stabilized as an ether adduct .
Chromic acid has the hypothetical formula . It is a vaguely described chemical, despite many well-defined chromates and dichromates being known. The dark red chromium(VI) oxide , the acid anhydride of chromic acid, is sold industrially as "chromic acid". It can be produced by mixing sulfuric acid with dichromate, and is a strong oxidizing agent.
Chromium(V) and chromium(IV).
The oxidation state +5 is only realized in few compounds but are intermediates in many reactions involving oxidations by chromate. The only binary compound is the volatile chromium(V) fluoride (CrF5). This red solid has a melting point of 30 °C and a boiling point of 117 °C. It can be synthesized by treating chromium metal with fluorine at 400 °C and 200 bar pressure. The peroxochromate(V) is another example of the +5 oxidation state. Potassium peroxochromate (K3[Cr(O2)4]) is made by reacting potassium chromate with hydrogen peroxide at low temperatures. This red brown compound is stable at room temperature but decomposes spontaneously at 150–170 °C.
Compounds of chromium(IV) (in the +4 oxidation state) are slightly more common than those of chromium(V). The tetrahalides, CrF4, CrCl4, and CrBr4, can be produced by treating the trihalides () with the corresponding halogen at elevated temperatures. Such compounds are susceptible to disproportionation reactions and are not stable in water.
Chromium(II).
Many chromium(II) compounds are known, including the water-stable chromium(II) chloride, , which can be made by reduction of chromium(III) chloride with zinc. The resulting bright blue solution is only stable at neutral pH. Many chromous carboxylates are also known, most famously, the red chromous acetate (Cr2(O2CCH3)4), which features a quadruple bond.
Chromium(I).
Most Cr(I) compounds are obtained by oxidation of electron-rich, octahedral Cr(0) complexes. Other Cr(I) complexes contain cyclopentadienyl ligands. As verified by X-ray diffraction, a Cr-Cr quintuple bond (length 183.51(4)  pm) has also been described. Extremely bulky monodentate ligands stabilize this compound by shielding the quintuple bond from further reactions.
Chromium(0).
Many chromium(0) compounds are known. Most are derivatives of chromium hexacarbonyl or bis(benzene)chromium.
History.
Weapons found in burial pits dating from the late 3rd century B.C. Qin Dynasty of the Terracotta Army near Xi'an, China have been analyzed by archaeologists. Although buried more than 2,000 years ago, the ancient bronze tips of crossbow bolts and swords found at the site showed unexpectedly little corrosion, possibly because the bronze was deliberately coated with a thin layer of chromium oxide. However, this oxide layer was not chromium metal or chrome plating as we know it.
Chromium minerals as pigments came to the attention of the west in the 18th century. On 26 July 1761, Johann Gottlob Lehmann found an orange-red mineral in the Beryozovskoye mines in the Ural Mountains which he named "Siberian red lead". Though misidentified as a lead compound with selenium and iron components, the mineral was in fact crocoite ("lead chromate") with a formula of PbCrO4.
In 1770, Peter Simon Pallas visited the same site as Lehmann and found a red lead mineral that had useful properties as a pigment in paints. The use of Siberian red lead as a paint pigment then developed rapidly. A bright yellow pigment made from crocoite also became fashionable.
In 1797, Louis Nicolas Vauquelin received samples of crocoite ore. He produced chromium trioxide (CrO3) by mixing crocoite with hydrochloric acid. In 1798, Vauquelin discovered that he could isolate metallic chromium by heating the oxide in a charcoal oven, making him the discoverer of the element. Vauquelin was also able to detect traces of chromium in precious gemstones, such as ruby or emerald.
During the 1800s, chromium was primarily used as a component of paints and in tanning salts. At first, crocoite from Russia was the main source, but in 1827, a larger chromite deposit was discovered near Baltimore, United States. This made the United States the largest producer of chromium products till 1848 when large deposits of chromite were found near Bursa, Turkey.
Chromium is also known for its luster when polished. It is used as a protective and decorative coating on car parts, plumbing fixtures, furniture parts and many other items, usually applied by electroplating. Chromium was used for electroplating as early as 1848, but this use only became widespread with the development of an improved process in 1924.
Production.
Approximately 28.8 million metric tons (Mt) of marketable chromite ore were produced in 2013, and converted into 7.5 Mt of ferrochromium. According to John F. Papp, writing for the USGS, "Ferrochromium is the leading end use of chromite ore, stainless steel is the leading end use of ferrochromium."
The largest producers of chromium ore in 2013 have been South Africa (48%), Kazakhstan (13%), Turkey (11%), India (10%) with several other countries producing the rest of about 18% of the world production.
The two main products of chromium ore refining are ferrochromium and metallic chromium. For those products the ore smelter process differs considerably. For the production of ferrochromium, the chromite ore (FeCr2O4) is reduced in large scale in electric arc furnace or in smaller smelters with either aluminium or silicon in an aluminothermic reaction.
For the production of pure chromium, the iron must be separated from the chromium in a two step roasting and leaching process. The chromite ore is heated with a mixture of calcium carbonate and sodium carbonate in the presence of air. The chromium is oxidized to the hexavalent form, while the iron forms the stable Fe2O3. The subsequent leaching at higher elevated temperatures dissolves the chromates and leaves the insoluble iron oxide. The chromate is converted by sulfuric acid into the dichromate.
The dichromate is converted to the chromium(III) oxide by reduction with carbon and then reduced in an aluminothermic reaction to chromium.
Applications.
Metal alloys now account for 85% of the use of chromium. The remainder is used in the chemical industry and refractory and foundry industries.
Metallurgy.
The strengthening effect of forming stable metal carbides at the grain boundaries and the strong increase in corrosion resistance made chromium an important alloying material for steel. The high-speed tool steels contain between 3 and 5% chromium. Stainless steel, the main corrosion-proof metal alloy, is formed when chromium is added to iron in sufficient concentrations, usually above 11%. For its formation, ferrochromium is added to the molten iron. Also nickel-based alloys increase in strength due to the formation of discrete, stable metal carbide particles at the grain boundaries. For example, Inconel 718 contains 18.6% chromium. Because of the excellent high-temperature properties of these nickel superalloys, they are used in jet engines and gas turbines in lieu of common structural materials.
The relative high hardness and corrosion resistance of unalloyed chromium makes it a good surface coating, being still the most "popular" metal coating with unparalleled combined durability. A thin layer of chromium is deposited on pretreated metallic surfaces by electroplating techniques. There are two deposition methods: Thin, below 1 µm thickness, layers are deposited by chrome plating, and are used for decorative surfaces. If wear-resistant surfaces are needed then thicker chromium layers are deposited. Both methods normally use acidic chromate or dichromate solutions. To prevent the energy-consuming change in oxidation state, the use of chromium(III) sulfate is under development, but for most applications, the established process is used.
In the chromate conversion coating process, the strong oxidative properties of chromates are used to deposit a protective oxide layer on metals like aluminium, zinc and cadmium. This passivation and the self-healing properties by the chromate stored in the chromate conversion coating, which is able to migrate to local defects, are the benefits of this coating method. Because of environmental and health regulations on chromates, alternative coating methods are under development.
Chromic acid anodizing (or Type I anodizing) of aluminium is another electrochemical process, which does not lead to the deposition of chromium, but uses chromic acid as electrolyte in the solution. During anodization, an oxide layer is formed on the aluminium. The use of chromic acid, instead of the normally used sulfuric acid, leads to a slight difference of these oxide layers.
The high toxicity of Cr(VI) compounds, used in the established chromium electroplating process, and the strengthening of safety and environmental regulations demand a search for substitutes for chromium or at least a change to less toxic chromium(III) compounds.
Dye and pigment.
The mineral crocoite (lead chromate PbCrO4) was used as a yellow pigment shortly after its discovery. After a synthesis method became available starting from the more abundant chromite, chrome yellow was, together with cadmium yellow, one of the most used yellow pigments. The pigment does not photodegrade, but it tends to darken due to the formation of chromium(III) oxide. It has a strong color, and was used for school buses in the US and for Postal Service (for example Deutsche Post) in Europe. The use of chrome yellow declined due to environmental and safety concerns and was replaced by organic pigments or alternatives free from lead and chromium. Other pigments based on chromium are, for example, the bright red pigment chrome red, which is a basic lead chromate (PbCrO4·Pb(OH)2). A very important chromate pigment, which was used widely in metal primer formulations, was zinc chromate, now replaced by zinc phosphate. A wash primer was formulated to replace the dangerous practice of pretreating aluminium aircraft bodies with a phosphoric acid solution. This used zinc tetroxychromate dispersed in a solution of polyvinyl butyral. An 8% solution of phosphoric acid in solvent was added just before application. It was found that an easily oxidized alcohol was an essential ingredient. A thin layer of about 10–15 µm was applied, which turned from yellow to dark green when it was cured. There is still a question as to the correct mechanism. Chrome green is a mixture of Prussian blue and chrome yellow, while the chrome oxide green is chromium(III) oxide.
Chromium oxides are also used as a green color in glassmaking and as a glaze in ceramics. Green chromium oxide is extremely light-fast and as such is used in cladding coatings. It is also the main ingredient in infrared reflecting paints, used by the armed forces, to paint vehicles, to give them the same IR reflectance as green leaves.
Synthetic ruby and the first laser.
Natural rubies are corundum (aluminum oxide) crystals that are colored red (the rarest type) due to chromium (III) ions (other colors of corundum gems are termed sapphires). A red-colored artificial ruby may also be achieved by doping chromium(III) into artificial corundum crystals, thus making chromium a requirement for making synthetic rubies. Such a synthetic ruby crystal was the basis for the first laser, produced in 1960, which relied on stimulated emission of light from the chromium atoms in such a crystal.
Wood preservative.
Because of their toxicity, chromium(VI) salts are used for the preservation of wood. For example, chromated copper arsenate (CCA) is used in timber treatment to protect wood from decay fungi, wood-attacking insects, including termites, and marine borers. The formulations contain chromium based on the oxide CrO3 between 35.3% and 65.5%. In the United States, 65,300 metric tons of CCA solution were used in 1996.
Tanning.
Chromium(III) salts, especially chrome alum and chromium(III) sulfate, are used in the tanning of leather. The chromium(III) stabilizes the leather by cross linking the collagen fibers. Chromium tanned leather can contain between 4 and 5% of chromium, which is tightly bound to the proteins. Although the form of chromium used for tanning is not the toxic hexavalent variety, there remains interest in management of chromium in the tanning industry such as recovery and reuse, direct/indirect recycling, use of less chromium or "chrome-less" tanning are practiced to better manage chromium in tanning.
Refractory material.
The high heat resistivity and high melting point makes chromite and chromium(III) oxide a material for high temperature refractory applications, like blast furnaces, cement kilns, molds for the firing of bricks and as foundry sands for the casting of metals. In these applications, the refractory materials are made from mixtures of chromite and magnesite. The use is declining because of the environmental regulations due to the possibility of the formation of chromium(VI). 
Catalysts.
Several chromium compounds are used as catalysts for processing hydrocarbons. For example, the Phillips catalyst, prepared from chromium oxides, is used for the production of about half the world's polyethylene. Fe-Cr mixed oxides are employed as high-temperature catalysts for the water gas shift reaction. Copper chromite is a useful hydrogenation catalyst.
Biological role.
Recently, a paradigm shift has occurred in terms of the status of trivalent chromium (Cr(III) or Cr3+). It was first proposed to be an essential element in the late 1950s and accepted as a trace element in the 1980s. However, scientific studies have continued to fail to produce convincing evidence for this status. Trivalent chromium occurs in trace amounts in foods and waters, and appears to be benign. In contrast, hexavalent chromium (Cr(VI) or Cr6+) is very toxic and mutagenic when inhaled. Cr(VI) has not been established as a carcinogen when in solution, although it may cause allergic contact dermatitis (ACD).
Chromium deficiency, involving a lack of Cr(III) in the body, or perhaps some complex of it, such as glucose tolerance factor is controversial, or is at least extremely rare. Chromium has no verified biological role and has been classified by some as "not" essential for mammals. However, other reviews have regarded it as an essential trace element in humans. Studies suggest that the biologically active form of chromium (III) is an oligopeptide called Low-molecular-weight chromium-binding substance (LMWCr), which might play a role in the insulin signaling pathway.
Chromium deficiency has been attributed to only three people on long-term parenteral nutrition, which is when a patient is fed a liquid diet through intravenous drips for long periods of time.
Although no biological role for chromium has ever been demonstrated, dietary supplements for chromium include chromium(III) picolinate, chromium(III) polynicotinate, and related materials. The benefit of those supplements is questioned by some studies. The use of chromium-containing dietary supplements is controversial, owing to the absence of any verified biological role, the expense of these supplements, and the complex effects of their use. The popular dietary supplement chromium picolinate complex generates chromosome damage in hamster cells (due to the picolinate ligand). In the United States the dietary guidelines for daily chromium uptake were lowered in 2001 from 50–200 µg for an adult to 35 µg (adult male) and to 25 µg (adult female). In 2014 the European Food Safety Authority published a report stating that the intake of chromium(III) has no beneficial effect on healthy people, thus the Panel removed chromium from the list of nutrients and essential elements.
No comprehensive, reliable database of chromium content of food currently exists. Data reported prior to 1980 is unreliable due to analytical error. Chromium content of food varies widely due to differences in soil mineral content, growing season, plant cultivar, and contamination during processing. In addition, large amounts of chromium (and nickel) leach into food cooked in stainless steel.
Precautions.
Water-insoluble chromium(III) compounds and chromium metal are not considered a health hazard, while the toxicity and carcinogenic properties of chromium(VI) have been known for a long time. Because of the specific transport mechanisms, only limited amounts of chromium(III) enter the cells. Several "in vitro" studies indicated that high concentrations of chromium(III) in the cell can lead to DNA damage. Acute oral toxicity ranges between 1.5 and 3.3 mg/kg. The proposed beneficial effects of chromium(III) and the use as dietary supplements yielded some controversial results, but recent reviews suggest that moderate uptake of chromium(III) through dietary supplements poses no risk. In the US, the Occupational Safety and Health Administration (OSHA) has designated a permissible exposure limit (PEL) in the workplace as a time-weighted average (TWA) of 1 mg/m3. The National Institute for Occupational Safety and Health (NIOSH) has set a recommended exposure limit (REL) of 0.5 mg/m3, time-weighted average. The IDLH (immediately dangerous to life and health) value is 250 mg/m3.
Cr(VI).
The acute oral toxicity for chromium(VI) ranges between 50 and 150 µg/kg. In the body, chromium(VI) is reduced by several mechanisms to chromium(III) already in the blood before it enters the cells. The chromium(III) is excreted from the body, whereas the chromate ion is transferred into the cell by a transport mechanism, by which also sulfate and phosphate ions enter the cell. The acute toxicity of chromium(VI) is due to its strong oxidational properties. After it reaches the blood stream, it damages the kidneys, the liver and blood cells through oxidation reactions. Hemolysis, renal and liver failure are the results of these damages. Aggressive dialysis can improve the situation.
The carcinogenity of chromate dust is known for a long time, and in 1890 the first publication described the elevated cancer risk of workers in a chromate dye company. Three mechanisms have been proposed to describe the genotoxicity of chromium(VI). The first mechanism includes highly reactive hydroxyl radicals and other reactive radicals which are by products of the reduction of chromium(VI) to chromium(III). The second process includes the direct binding of chromium(V), produced by reduction in the cell, and chromium(IV) compounds to the DNA. The last mechanism attributed the genotoxicity to the binding to the DNA of the end product of the chromium(III) reduction.
Chromium salts (chromates) are also the cause of allergic reactions in some people. Chromates are often used to manufacture, amongst other things, leather products, paints, cement, mortar and anti-corrosives. Contact with products containing chromates can lead to allergic contact dermatitis and irritant dermatitis, resulting in ulceration of the skin, sometimes referred to as "chrome ulcers". This condition is often found in workers that have been exposed to strong chromate solutions in electroplating, tanning and chrome-producing manufacturers.
Environmental issues.
As chromium compounds were used in dyes and paints and the tanning of leather, these compounds are often found in soil and groundwater at abandoned industrial sites, now needing environmental cleanup and remediation per the treatment of brownfield land. Primer paint containing hexavalent chromium is still widely used for aerospace and automobile refinishing applications.
In 2010, the Environmental Working Group studied the drinking water in 35 American cities, the first nationwide analysis measuring the presence of the chemical in U.S. water systems. The study found measurable hexavalent chromium in the tap water of 31 of the cities sampled, with Norman, Oklahoma, at the top of list; 25 cities had levels that exceeded California's proposed limit. Concentrations of Cr(VI) in US municipal drinking water supplies reported by EWG are within likely, natural background levels for the areas tested and not necessarily indicative of industrial pollution, as asserted by EWG. This factor was not taken into consideration in their report.

</doc>
<doc id="5671" url="https://en.wikipedia.org/wiki?curid=5671" title="Cymbal">
Cymbal

Cymbals are a common percussion instrument. Cymbals consist of thin, normally round plates of various alloys. The majority of cymbals are of indefinite pitch, although small disc-shaped cymbals based on ancient designs sound a definite note (see: crotales). Cymbals are used in many ensembles ranging from the orchestra, percussion ensembles, jazz bands, heavy metal bands, and marching groups. Drum kits usually incorporate at least a crash, ride or crash/ride, and a pair of hi-hat cymbals.
Etymology and names.
The word cymbal is derived from the Latin "cymbalum", which is the latinisation of the Greek word "kymbalon", "cymbal", which in turn derives from "kymbē", "cup, bowl".
In orchestral scores, cymbals may be indicated by the French "cymbales"; German "Becken", "Schellbecken", "Tellern", or "Tschinellen"; Italian "piatti" or "cinelli"; and Spanish "platillos". Many of these derive from the word for plates.
History.
Cymbals have existed since ancient times. Representations of cymbals may be found in reliefs and paintings from Hittite Anatolia, Larsa, Babylon, Assyria, ancient Egypt, ancient Greece, and ancient Rome. References to cymbals also appear throughout the Bible. Cymbals may have been introduced to China from Central Asia in the 3rd or 4th century AD.
Cymbals were employed by Turkish janissaries in the 14th century or earlier. By the 17th century, such cymbals were used in European music, and more commonly played in military bands and orchestras by the mid 18th century. Since the 19th century, some composers have called for larger roles for cymbals in musical works, and a variety of cymbal shapes, techniques, and hardware have been developed in response.
Anatomy.
The anatomy of the cymbal plays a large part in the sound it creates. A hole is drilled in the center of the cymbal and it is used to either mount the cymbal on a stand or straps (for hand playing). The bell, dome, or cup is the raised section immediately surrounding the hole. The bell produces a higher "pinging" pitch than the rest of the cymbal. The bow is the rest of the surface surrounding the bell. The bow is sometimes described in two areas: the ride and crash area. The ride area is the thicker section closer to the bell while the crash area is the thinner tapering section near the edge. The edge or rim is the immediate circumference of the cymbal.
Cymbals are measured by their diameter often in inches or centimeters. The size of the cymbal affects its sound, larger cymbals usually being louder and having longer sustain. The weight describes how thick the cymbal is. Cymbal weights are important to the sound they produce and how they play. Heavier cymbals have a louder volume, more cut, and better stick articulation (when using drum sticks). Thin cymbals have a fuller sound, lower pitch, and faster response.
The profile of the cymbal is the vertical distance of the bow from the bottom of the bell to the cymbal edge (higher profile cymbals are more bowl shaped). The profile affects the pitch of the cymbal: higher profile cymbals have higher pitch.
Types.
Orchestral cymbals.
Cymbals offer a composer nearly endless amounts of color and effect. Their unique timbre allows them to project even against a full orchestra and through the heaviest of orchestrations and enhance articulation and nearly any dynamic. Cymbals have been utilized historically to suggest frenzy, fury or bacchanalian revels, as seen in the Venus music in Wagner's "Tannhäuser", Grieg's "Peer Gynt suite", and Osmin's aria "O wie will ich triumphieren" from Mozart's "Die Entführung aus dem Serail".
Crash cymbals.
Orchestral crash cymbals are traditionally used in pairs, each one having a strap set in the bell of the cymbal by which they are held. Such a pair is always known as crash cymbals or plates.
The sound can be obtained by rubbing their edges together in a sliding movement for a "sizzle", striking them against each other in what is called a "crash", tapping the edge of one against the body of the other in what is called a "tap-crash", scraping the edge of one from the inside of the bell to the edge for a "scrape" or "zischen," or shutting the cymbals together and choking the sound in what is called a "hi-hat chick" or crush. A skilled player can obtain an enormous dynamic range from such a pair of cymbals. For example, in Beethoven's ninth symphony, the percussionist is employed to first play cymbals at pianissimo, adding a touch of colour rather than loud crash.
Crash cymbals are usually damped by pressing them against the player's body. A composer may write "laissez vibrer", "Let vibrate" (usually abbreviated l.v.), "secco" (dry), or equivalent indications on the score; more usually, the player must judge exactly when to damp the cymbals based on the written duration of clash and the context in which it occurs.
Crash cymbals have traditionally been accompanied by the bass drum playing an identical part. This combination, played loudly, is an effective way to accentuate a note since the two instruments together contribute to both very low and very high frequency ranges and provide a satisfying "crash-bang-wallop". In older music the composer sometimes provided just one part for this pair of instruments, writing "senza piatti" or "piatti soli" () if only one of the instruments is needed. This came from the common practice of only having one percussionist play both instruments, using one cymbal mounted to the shell of the bass drum itself. The player would crash the cymbals with his left hand and use a mallet to strike the bass drum in his right. This method is often employed today in pit orchestras and is called for specifically by composers who desire a certain effect. Stravinsky calls for this in his ballet Petrushka and Mahler calls for this in his Titan Symphony.
The modern convention is for the instruments to have independent parts. However, in kit drumming, a cymbal crash is still most often accompanied by a simultaneous kick to the bass drum, which provides both a musical effect and a support to the crash stroke.
Hi hats.
Crash cymbals evolved into the low-sock and from this to the modern hi-hat. Even in a modern drum kit, they remain paired with the bass drum as the two instruments which are played with the player's feet. However, hi-hat cymbals tend to be heavy with little taper, more similar to a ride cymbal than to a clash cymbal as found in a drum kit, and perform a ride rather than a crash function.
Suspended cymbal.
Another use of cymbals is the suspended cymbal. This instrument takes its name from the traditional method of suspending the cymbal by means of a leather strap or rope, thus allowing the cymbal to vibrate as freely as possible for maximum musical effect. Early jazz drumming pioneers borrowed this style of cymbal mounting during the early 1900s and later drummers further developed this instrument into the mounted horizontal or nearly horizontally mounted "crash" cymbals of a modern drum kit, However, most modern drum kits do not employ a leather strap suspension system. Many modern drum kits use a mount with felt or otherwise dampening fabric to act as a barrier to hold the cymbals between metal clamps: thus forming the modern day ride cymbal.
Suspended cymbals can be played with yarn, sponge or cord wrapped mallets. The first known instance of using a sponge-headed mallet on a cymbal is the final chord of Hector Berlioz' Symphonie Fantastique. Composers sometimes specifically request other types of mallets like felt mallets or timpani beaters for different attack and sustain qualities.
Suspended cymbals can produce bright and slicing tones when forcefully struck, and give an eerie transparent "windy" sound when played quietly. A tremolo, or roll (played with two mallets alternately striking on opposing sides of the cymbal) can build in volume from almost inaudible to an overwhelming climax in a satisfyingly smooth manner (as in Humperdink's Mother Goose Suite).
The edge of a suspended cymbal may be hit with shoulder of a drum stick to obtain a sound somewhat akin to that of a pair of clash cymbals. Other methods of playing include scraping a coin or a triangle beater rapidly across the ridges on the top of the cymbal, giving a "zing" sound (as some players do in the fourth movement of Dvořák's Symphony No. 9). Other effects that can be used include drawing a cello or bass bow across the edge of the cymbal for a sound not unlike squealing car brakes.
Ancient cymbals.
Ancient cymbals or tuned cymbals are much more rarely called for. Their timbre is entirely different, more like that of small hand-bells or of the notes of the keyed harmonica. They are not struck full against each other, but by one of their edges, and the note given in by them is higher in proportion as they are thicker and smaller. Berlioz's "Romeo and Juliet" calls for two pairs of cymbals, modelled on some old Pompeian instruments no larger than the hand (some are no larger than a crown piece), and tuned to F and B flat. The modern instruments descended from this line are the crotales.
List of cymbal types.
Cymbal types include:

</doc>
<doc id="5672" url="https://en.wikipedia.org/wiki?curid=5672" title="Cadmium">
Cadmium

Cadmium is a chemical element with symbol Cd and atomic number 48. This soft, bluish-white metal is chemically similar to the two other stable metals in group 12, zinc and mercury. Like zinc, it prefers oxidation state +2 in most of its compounds and like mercury it shows a low melting point compared to transition metals. Cadmium and its congeners are not always considered transition metals, in that they do not have partly filled d or f electron shells in the elemental or common oxidation states. The average concentration of cadmium in Earth's crust is between 0.1 and 0.5 parts per million (ppm). It was discovered in 1817 simultaneously by Stromeyer and Hermann, both in Germany, as an impurity in zinc carbonate.
Cadmium occurs as a minor component in most zinc ores and therefore is a byproduct of zinc production. Cadmium was used for a long time as a corrosion-resistant plating on steel and cadmium compounds are used as red, orange and yellow pigments, to colour glass and to stabilize plastic. The use of cadmium is generally decreasing due to its toxicity (it is specifically listed in the European Restriction of Hazardous Substances) and the replacement of nickel-cadmium batteries with nickel-metal hydride and lithium-ion batteries. One of its few new uses is in cadmium telluride solar panels. 
Although cadmium has no known biological function in higher organisms, a cadmium-dependent carbonic anhydrase has been found in marine diatoms.
Characteristics.
Physical properties.
Cadmium is a soft, malleable, ductile, bluish-white divalent metal. It is similar in many respects to zinc but forms complex compounds. Unlike most other metals, cadmium is resistant to corrosion and as a result it is used as a protective layer when deposited on other metals. As a bulk metal, cadmium is insoluble in water and is not flammable; however, in its powdered form it may burn and release toxic fumes.
Chemical properties.
Although cadmium usually has an oxidation state of +2, it also exists in the +1 state. Cadmium and its congeners are not always considered transition metals, in that they do not have partly filled d or f electron shells in the elemental or common oxidation states. Cadmium burns in air to form brown amorphous cadmium oxide (CdO); the crystalline form of this compound is a dark red which changes color when heated, similar to zinc oxide. Hydrochloric acid, sulfuric acid and nitric acid dissolve cadmium by forming cadmium chloride (CdCl2), cadmium sulfate (CdSO4), or cadmium nitrate (Cd(NO3)2). The oxidation state +1 can be reached by dissolving cadmium in a mixture of cadmium chloride and aluminium chloride, forming the Cd22+ cation, which is similar to the Hg22+ cation in mercury(I) chloride.
The structures of many cadmium complexes with nucleobases, amino acids and vitamins have been determined.
Isotopes.
Naturally occurring cadmium is composed of 8 isotopes. Two of them are radioactive, and three are expected to decay but have not been experimentally confirmed to do so. The two natural radioactive isotopes are 113Cd (beta decay, half-life is 7.7 × 1015 years) and 116Cd (two-neutrino double beta decay, half-life is 2.9 × 1019 years). The other three are 106Cd, 108Cd (both double electron capture), and 114Cd (double beta decay); only lower limits on their half-life times have been set. At least three isotopes – 110Cd, 111Cd, and 112Cd – are stable. Among the isotopes that do not occur naturally, the most long-lived are 109Cd with a half-life of 462.6 days, and 115Cd with a half-life of 53.46 hours. All of the remaining radioactive isotopes have half-lives that are less than 2.5 hours, and the majority of these have half-lives that are less than 5 minutes. Cadmium has 8 known meta states, with the most stable being 113mCd (t1/2 = 14.1 years), 115mCd (t1/2 = 44.6 days), and 117mCd (t1/2 = 3.36 hours).
The known isotopes of cadmium range in atomic mass from 94.950 u (95Cd) to 131.946 u (132Cd). For isotopes lighter than 112 u, the primary decay mode is electron capture and the dominant decay product is element 47 (silver). Heavier isotopes decay mostly through beta emission producing element 49 (indium).
One isotope of cadmium, 113Cd, absorbs neutrons with very high probability if they have an energy below the "cadmium cut-off" and transmits them otherwise. The cadmium cut-off is about 0.5 eV. Neutrons with energy below the cut-off are deemed slow neutrons, distinguishing them from intermediate and fast neutrons.
Cadmium is created via the long s-process in low-medium mass stars with masses of 0.6 to 10 solar masses, which lasts thousands of years. It requires a silver atom to capture a neutron and then undergo beta decay.
History.
Cadmium (Latin "cadmia", Greek "καδμεία" meaning "calamine", a cadmium-bearing mixture of minerals, which was named after the Greek mythological character Κάδμος, Cadmus, the founder of Thebes) was discovered simultaneously in 1817 by Friedrich Stromeyer and Karl Samuel Leberecht Hermann, both in Germany, as an impurity in zinc carbonate. Stromeyer found the new element as an impurity in zinc carbonate (calamine), and, for 100 years, Germany remained the only important producer of the metal. The metal was named after the Latin word for calamine, because it was found in this zinc compound. Stromeyer noted that some impure samples of calamine changed color when heated but pure calamine did not. He was persistent in studying these results and eventually isolated cadmium metal by roasting and reduction of the sulfide. The possibility to use cadmium yellow as pigment was recognized in the 1840s but the lack of cadmium limited this application.
Even though cadmium and its compounds may be toxic in certain forms and concentrations, the British Pharmaceutical Codex from 1907 states that cadmium iodide was used as a medication to treat "enlarged joints, scrofulous glands, and chilblains".
In 1907, the International Astronomical Union defined the international ångström in terms of a red cadmium spectral line (1 wavelength = 6438.46963 Å). This was adopted by the 7th General Conference on Weights and Measures in 1927. In 1960, the definitions of both the metre and ångström were changed to use krypton.
After the industrial scale production of cadmium started in the 1930s and 1940s, the major application of cadmium was the coating of iron and steel to prevent corrosion; in 1944, 62% and in 1956, 59% of the cadmium in the United States was for coating. In 1956, 24% of the cadmium used within the United States was used for the second application, which was for red, orange and yellow pigments based on sulfides and selenides of cadmium. The stabilizing effect of cadmium-containing chemicals like the carboxylates cadmium laurate and cadmium stearate on PVC led to an increased use of those compounds in the 1970s and 1980s. The use of cadmium in applications such as pigments, coatings, stabilizers and alloys declined due to environmental and health regulations in the 1980s and 1990s; in 2006, only 7% of total cadmium consumption was used for plating and coating and only 10% was used for pigments.
The decrease in consumption in other applications was made up by a growing demand of cadmium in nickel-cadmium batteries, which accounted for 81% of the cadmium consumption in the United States in 2006.
Occurrence.
Cadmium makes up about 0.1 ppm of Earth's crust. Compared with the more abundant 65 ppm zinc, cadmium is rare. No significant deposits of cadmium-containing ores are known. Greenockite (CdS), the only cadmium mineral of importance, is nearly always associated with sphalerite (ZnS). This association is caused by the geochemical similarity between zinc and cadmium which makes geological separation unlikely. As a consequence, cadmium is produced mainly as a byproduct from mining, smelting, and refining sulfidic ores of zinc, and, to a lesser degree, lead and copper. Small amounts of cadmium, about 10% of consumption, are produced from secondary sources, mainly from dust generated by recycling iron and steel scrap. Production in the United States began in 1907, but it was not until after World War I that cadmium came into wide use. One place where metallic cadmium can be found is the Vilyuy River basin in Siberia.
Rocks mined to produce phosphate fertilizers contain varying amounts of cadmium, leading to a cadmium concentration of up to 300 mg/kg in the produced phosphate fertilizers and thus in the high cadmium content in agricultural soils. Coal can contain significant amounts of cadmium, which ends up mostly in the flue dust.
Production.
The British Geological Survey reports that in 2001, China was the top producer of cadmium, producing almost one-sixth of the world share, closely followed by South Korea and Japan.
Cadmium is a common impurity in zinc ores, and it is most often isolated during the production of zinc. Some zinc ores concentrates from sulfidic zinc ores contain up to 1.4% of cadmium. In the 1970s, the output of cadmium was 6.5 pounds per ton of zinc. Zinc sulfide ores are roasted in the presence of oxygen, converting the zinc sulfide to the oxide. Zinc metal is produced either by smelting the oxide with carbon or by electrolysis in sulfuric acid. Cadmium is isolated from the zinc metal by vacuum distillation if the zinc is smelted, or cadmium sulfate is precipitated out of the electrolysis solution.
Applications.
Cadmium has many common industrial uses as it is a key component in battery production, is present in cadmium pigments, coatings, and is commonly used in electroplating.
Batteries.
In 2009, 86% of cadmium was used in batteries, predominantly in rechargeable nickel-cadmium batteries. Nickel-cadmium cells have a nominal cell potential of 1.2 V. The cell consists of a positive nickel hydroxide electrode and a negative cadmium electrode plate separated by an alkaline electrolyte (potassium hydroxide). The European Union set the allowed use of cadmium in electronics in 2004 to limits of 0.01%, with several exceptions, but reduced the allowed content of cadmium in batteries to 0.002%.
Electroplating.
Cadmium electroplating, consuming 6% of the global production, can be found in the aircraft industry due to the ability to resist corrosion when applied to steel components. This coating is passivated by the usage of chromate salts. A limitation of cadmium plating is hydrogen embrittlement of high-strength steels caused by the electroplating process. Therefore, steel parts heat-treated to tensile strength above 1300 MPa (200 ksi) should be coated by an alternative method (such as special low-embrittlement cadmium electroplating processes or physical vapor deposition). In addition, titanium embrittlement caused by cadmium-plated tool residues resulted in banishment of these tools (along with routine tool testing programs to detect any cadmium contamination) from the A-12/SR-71 and U-2 programs, and subsequent aircraft programs using titanium.
Nuclear fission.
Cadmium is used in the control rods of nuclear reactors, acting as a very effective "neutron poison" to control neutron flux in nuclear fission. When cadmium rods are inserted in the core of a nuclear reactor, cadmium absorbs neurons preventing them from creating additional fission events, thus controlling the amount of reactivity. The pressurized water reactor designed by Westinghouse Electric Company uses an alloy consisting of 80% silver, 15% indium, and 5% cadmium.
Compounds.
Cadmium oxide is used in black and white television phosphors and in the blue and green phosphors for color television picture tubes. Cadmium sulfide (CdS) is used as a photoconductive surface coating for photocopier drums.
In paint pigments, cadmium forms various salts, with CdS being the most common. This sulfide is used as a yellow pigment. Cadmium selenide can be used as red pigment, commonly called "cadmium red". To painters who work with the pigment, cadmium yellows, oranges, and reds are the most brilliant and long-lasting colors to use. In fact, during production, these colors are significantly toned down before they are ground with oils and binders, or blended into watercolors, gouaches, acrylics, and other paint and pigment formulations. Because these pigments are potentially toxic, it is recommended to use a barrier cream on the hands to prevent absorption through the skin when working with them even though the amount of cadmium absorbed into the body through the skin is usually reported to be less than 1%.
In PVC, cadmium was used as heat, light, and weathering stabilizers. Currently, cadmium stabilizers have been completely replaced with barium-zinc, calcium-zinc and organo-tin stabilizers. Cadmium is used in many kinds of solder and bearing alloys, due to a low coefficient of friction and fatigue resistance. It is also found in some of the lowest-melting alloys, such as Wood's metal.
Laboratory uses.
Helium–cadmium lasers are a common source of blue-ultraviolet laser light. They operate at either 325 or 422 nm and are used in fluorescence microscopes and various laboratory experiments. Cadmium selenide quantum dots emit bright luminescence under UV excitation (He-Cd laser, for example). The color of this luminescence can be green, yellow or red depending on the particle size. Colloidal solutions of those particles are used for imaging of biological tissues and solutions with a fluorescence microscope.
Cadmium is a component of some compound semiconductors, such as cadmium sulfide, cadmium selenide, and cadmium telluride, which can be used for light detection or solar cells. HgCdTe is sensitive to infrared light and therefore may be utilized as an infrared detector or switch for example in remote control devices.
In molecular biology, cadmium is used to block voltage-dependent calcium channels from fluxing calcium ions, as well as in hypoxia research to stimulate proteasome-dependent degradation of Hif-1α.
Cadmium-selective sensors.
Cadmium-selective sensors based on the fluorophore BODIPY have been developed for imaging and sensing of cadmium in cells.
Biological role.
Cadmium has no known useful role in higher organisms, but a cadmium-dependent carbonic anhydrase has been found in some marine diatoms. The diatoms live in environments with very low zinc concentrations and cadmium performs the function normally carried out by zinc in other anhydrases. The discovery was made using X-ray absorption fluorescence spectroscopy (XAFS).
The highest concentration of cadmium has been found to be absorbed in the kidneys of humans, and up to about 30 mg of cadmium is commonly inhaled throughout childhood and adolescence. Cadmium can be used to block calcium channels in chicken neurons.
Analytical methods for the determination of cadmium in biological samples have been reviewed.
Environment.
The biogeochemistry of cadmium and its release to the environment has been the subject of review, as has the speciation of cadmium in the environment.
Safety.
The bioinorganic aspects of cadmium toxicity have been reviewed.
The most dangerous form of occupational exposure to cadmium is inhalation of fine dust and fumes, or ingestion of highly soluble cadmium compounds. Inhalation of cadmium-containing fumes can result initially in metal fume fever but may progress to chemical pneumonitis, pulmonary edema, and death.
Cadmium is also an environmental hazard. Human exposures to environmental cadmium are primarily the result of fossil fuel combustion, phosphate fertilizers, natural sources, iron and steel production, cement production and related activities, nonferrous metals production, and municipal solid waste incineration. Bread, root crops, and vegetables also contribute to the cadmium in modern populations. There have been a few instances of general population toxicity as the result of long-term exposure to cadmium in contaminated food and water, and research is ongoing regarding the estrogen mimicry that may induce breast cancer. In the decades leading up to World War II, mining operations contaminated the Jinzū River in Japan with cadmium and traces of other toxic metals. As a consequence, cadmium accumulated in the rice crops growing along the riverbanks downstream of the mines. Some members of the local agricultural communities consuming the contaminated rice developed itai-itai disease and renal abnormalities, including proteinuria and glucosuria.
The victims of this poisoning were almost exclusively post-menopausal women with low iron and other mineral body stores. Similar general population cadmium exposures in other parts of the world have not resulted in the same health problems because the populations maintained sufficient iron and other mineral levels. Thus, although cadmium is a major factor in the itai-itai disease in Japan, most researchers have concluded that it was one of several factors. Cadmium is one of six substances banned by the European Union's Restriction on Hazardous Substances (RoHS) directive, which bans certain hazardous substances in electrical and electronic equipment but allows for certain exemptions and exclusions from the scope of the law.
The International Agency for Research on Cancer has classified cadmium and cadmium compounds as carcinogenic to humans. Although occupational exposure to cadmium is linked to lung and prostate cancer, there is still a substantial controversy about the carcinogenicity of cadmium in low, environmental exposure. Recent data from epidemiological studies suggest that intake of cadmium through diet associates to higher risk of endometrial, breast and prostate cancer as well as to osteoporosis in humans. A recent study has demonstrated that endometrial tissue is characterized by higher levels of cadmium in current and former smoking females.
Although some epidemiological studies show a significant correlation between cadmium exposure and occurrence of disease conditions in human populations, a causative role for cadmium as the factor behind these effects remains yet to be shown. In order to prove a causative role, it will be important to define the molecular mechanisms through which cadmium in low exposure can cause adverse health effects. One hypothesis is that cadmium works as an endocrine disruptor because some experimental studies have shown that it can interact with different hormonal signaling pathways. For example, cadmium can bind to the estrogen receptor alpha, and affect signal transduction along the estrogen and MAPK signaling pathways at low doses.
Tobacco smoking is the most important single source of cadmium exposure in the general population. It has been estimated that about 10% of the cadmium content of a cigarette is inhaled through smoking. The absorption of cadmium from the lungs is much more effective than that from the gut, and as much as 50% of the cadmium inhaled via cigarette smoke may be absorbed.
On average, smokers have 4–5 times higher blood cadmium concentrations and 2–3 times higher kidney cadmium concentrations than non-smokers. Despite the high cadmium content in cigarette smoke, there seems to be little exposure to cadmium from passive smoking. No significant effect on blood cadmium concentrations has been detected in children exposed to environmental tobacco smoke.
In the non-smoking part of the population food is the biggest source of exposure to cadmium. High quantities of cadmium can be found for example in crustaceans, molluscs, offals, and algal products. However, due to the higher consumption the most significant contributors to the dietary cadmium exposure are grains, vegetables, and starchy roots and tubers.
Cadmium exposure is a risk factor associated with early atherosclerosis and hypertension, which can both lead to cardiovascular disease.
Regulations.
Due to the adverse effects on the environment and human health, the supply and use of cadmium is restricted in Europe under the REACH Regulation.
The EFSA Panel on Contaminants in the Food Chain has set 2.5 μg/kg body weight as a tolerable weekly intake for humans. In comparison, the Joint FAO/WHO Expert Committee on Food Additives has established 7 μg/kg bw as a provisional tolerable weekly intake level.
The Occupational Safety and Health Administration (OSHA) has set a permissible exposure limit (PEL) for cadmium at a time-weighted average (TWA) of 0.005 ppm. The National Institute for Occupational Safety and Health (NIOSH) has not set a recommended exposure limit (REL) and has designated it as a known human carcinogen. The IDLH (immediately dangerous to life and health) level for cadmium is 9 mg/m3.
Product recalls.
In May 2006, a sale of the seats from Arsenal F.C.'s old stadium, Highbury in London, England was cancelled after the seats were discovered to contain trace amounts of cadmium. Reports of high levels of cadmium use in children's jewelry in 2010 led to a US Consumer Product Safety Commission investigation. The U.S. CPSC issued specific recall notices for cadmium content applying to jewelry sold by Claire's and Wal-Mart stores. In June 2010, McDonald's voluntarily recalled more than 12 million promotional "Shrek Forever After 3D" Collectable Drinking Glasses owing to concerns over cadmium levels in paint pigments used on the glassware. The glasses were manufactured by Arc International, of Millville, NJ, USA.

</doc>
<doc id="5675" url="https://en.wikipedia.org/wiki?curid=5675" title="Curium">
Curium

Curium is a transuranic radioactive chemical element with symbol Cm and atomic number 96. This element of the actinide series was named after Marie and Pierre Curie – both were known for their research on radioactivity. Curium was first intentionally produced and identified in July 1944 by the group of Glenn T. Seaborg at the University of California, Berkeley. The discovery was kept secret and only released to the public in November 1945. Most curium is produced by bombarding uranium or plutonium with neutrons in nuclear reactors – one tonne of spent nuclear fuel contains about 20 grams of curium.
Curium is a hard, dense, silvery metal with a relatively high melting point and boiling point for an actinide. Whereas it is paramagnetic at ambient conditions, it becomes antiferromagnetic upon cooling, and other magnetic transitions are also observed for many curium compounds. In compounds, curium usually exhibits valence +3 and sometimes +4, and the +3 valence is predominant in solutions. Curium readily oxidizes, and its oxides are a dominant form of this element. It forms strongly fluorescent complexes with various organic compounds, but there is no evidence of its incorporation into bacteria and archaea. When introduced into the human body, curium accumulates in the bones, lungs and liver, where it promotes cancer.
All known isotopes of curium are radioactive and have a small critical mass for a sustained nuclear chain reaction. They predominantly emit α-particles, and the heat released in this process can potentially produce electricity in radioisotope thermoelectric generators. This application is hindered by the scarcity, high cost and radioactivity of curium isotopes. Curium is used in production of heavier actinides and of the 238Pu radionuclide for power sources in artificial pacemakers. It served as the α-source in the alpha particle X-ray spectrometers installed on several space probes, including the Sojourner, Spirit, Opportunity and Curiosity Mars rovers and the Philae lander on comet 67P/Churyumov-Gerasimenko, to analyze the composition and structure of the surface.
History.
Although curium had likely been produced in previous nuclear experiments, it was first intentionally synthesized, isolated and identified in 1944, at the University of California, Berkeley, by Glenn T. Seaborg, Ralph A. James, and Albert Ghiorso. In their experiments, they used a cyclotron.
Curium was chemically identified at the Metallurgical Laboratory (now Argonne National Laboratory) at the University of Chicago. It was the third transuranium element to be discovered even though it is the fourth in the series – the lighter element americium was unknown at the time.
The sample was prepared as follows: first plutonium nitrate solution was coated on a platinum foil of about 0.5 cm2 area, the solution was evaporated and the residue was converted into plutonium(IV) oxide (PuO2) by annealing. Following cyclotron irradiation of the oxide, the coating was dissolved with nitric acid and then precipitated as the hydroxide using concentrated aqueous ammonia solution. The residue was dissolved in perchloric acid, and further separation was carried out by ion exchange to yield a certain isotope of curium. The separation of curium and americium was so painstaking that the Berkeley group initially called those elements "pandemonium" (from Greek for "all demons" or "hell") and "delirium" (from Latin for "madness").
The curium-242 isotope was produced in July–August 1944 by bombarding 239Pu with α-particles to produce curium with the release of a neutron:
Curium-242 was unambiguously identified by the characteristic energy of the α-particles emitted during the decay:
The half-life of this alpha decay was first measured as 150 days and then corrected to 162.8 days.
Another isotope 240Cm was produced in a similar reaction in March 1945:
The half-life of the 240Cm α-decay was correctly determined as 26.7 days.
The discovery of curium, as well as americium, in 1944 was closely related to the Manhattan Project, the results were confidential and declassified only in 1945. Seaborg leaked the synthesis of the elements 95 and 96 on the U.S. radio show for children, the Quiz Kids, five days before the official presentation at an American Chemical Society meeting on November 11, 1945, when one of the listeners asked whether any new transuranium element beside plutonium and neptunium had been discovered during the war. The discovery of curium (242Cm and 240Cm), their production and compounds were later patented listing only Seaborg as the inventor.
The new element was named after Marie Skłodowska-Curie and her husband Pierre Curie who are noted for discovering radium and for their work in radioactivity. It followed the example of gadolinium, a lanthanide element above curium in the periodic table, which was named after the explorer of the rare earth elements Johan Gadolin:
The first curium samples were barely visible, and were identified by their radioactivity. Louis Werner and Isadore Perlman created the first substantial sample of 30 µg curium-242 hydroxide at the University of California in 1947 by bombarding americium-241 with neutrons. Macroscopic amounts of curium fluoride were obtained in 1950 by W. W. T. Crane, J. C. Wallmann and B. B. Cunningham. Its magnetic susceptibility was very close to that of GdF3 providing the first experimental evidence for the +3 valence of curium in its compounds. Curium metal was produced only in 1951 by reduction of curium fluoride with barium.
Characteristics.
Physical.
A synthetic, radioactive element, curium is a hard dense metal with silvery-white appearance and physical and chemical properties resembling those of gadolinium. Its melting point of 1340 °C is significantly higher than that of the previous transuranic elements neptunium (637 °C), plutonium (639 °C) and americium (1173 °C). In comparison, gadolinium melts at 1312 °C. The boiling point of curium is 3110 °C. With a density of 13.52 g/cm3, curium is significantly lighter than neptunium (20.45 g/cm3) and plutonium (19.8 g/cm3), but is heavier than most other metals. Between two crystalline forms of curium, the α-Cm is more stable at ambient conditions. It has a hexagonal symmetry, space group P63/mmc, lattice parameters "a" = 365 pm and "c" = 1182 pm, and four formula units per unit cell. The crystal consists of a double-hexagonal close packing with the layer sequence ABAC and so is isotypic with α-lanthanum. At pressures above 23 GPa, at room temperature, α-Cm transforms into β-Cm, which has a face-centered cubic symmetry, space group Fmm and the lattice constant "a" = 493 pm. Upon further compression to 43 GPa, curium transforms to an orthorhombic γ-Cm structure similar to that of α-uranium, with no further transitions observed up to 52 GPa. These three curium phases are also referred to as Cm I, II and III.
Curium has peculiar magnetic properties. Whereas its neighbor element americium shows no deviation from Curie-Weiss paramagnetism in the entire temperature range, α-Cm transforms to an antiferromagnetic state upon cooling to 65–52 K, and β-Cm exhibits a ferrimagnetic transition at about 205 K. Meanwhile, curium pnictides show ferromagnetic transitions upon cooling: 244CmN and 244CmAs at 109 K, 248CmP at 73 K and 248CmSb at 162 K. Similarly, the lanthanide analogue of curium, gadolinium, as well as its pnictides also show magnetic transitions upon cooling, but the transition character is somewhat different: Gd and GdN become ferromagnetic, and GdP, GdAs and GdSb show antiferromagnetic ordering.
In accordance with magnetic data, electrical resistivity of curium increases with temperature – about twice between 4 and 60 K – and then remains nearly constant up to room temperature. There is a significant increase in resistivity over time (about 10 µΩ·cm/h) due to self-damage of the crystal lattice by alpha radiation. This makes uncertain the absolute resistivity value for curium (about 125 µΩ·cm). The resistivity of curium is similar to that of gadolinium and of the actinides plutonium and neptunium, but is significantly higher than that of americium, uranium, polonium and thorium.
Under ultraviolet illumination, curium(III) ions exhibit strong and stable yellow-orange fluorescence with a maximum in the range about 590–640 nm depending on their environment. The fluorescence originates from the transitions from the first excited state 6D7/2 and the ground state 8S7/2. Analysis of this fluorescence allows monitoring interactions between Cm(III) ions in organic and inorganic complexes.
Chemical.
Curium ions in solution almost exclusively assume the oxidation state of +3, which is the most stable oxidation state for curium. The +4 oxidation state is observed mainly in a few solid phases, such as CmO2 and CmF4. Aqueous curium(IV) is only known in the presence of strong oxidizers such as potassium persulfate, and is easily reduced to curium(III) by radiolysis and even by water. The chemical behavior of curium is different from the actinides thorium and uranium, and is similar to that of americium and many lanthanides. In aqueous solution, the Cm3+ ion is colorless to pale green, and Cm4+ ion is pale yellow. The optical absorption of Cm3+ ions contains three sharp peaks at 375.4, 381.2 and 396.5 nanometers and their strength can be directly converted into the concentration of the ions. The +6 oxidation state has only been reported once in solution in 1978, as the curyl ion (): this was prepared from the beta decay of americium-242 in the americium(V) ion . Failure to obtain Cm(VI) from oxidation of Cm(III) and Cm(IV) may be due to the high Cm4+/Cm3+ ionization potential and the instability of Cm(V).
Curium ions are hard Lewis acids and thus form most stable complexes with hard bases. The bonding is mostly ionic, with a small covalent component. Curium in its complexes commonly exhibits a 9-fold coordination environment, within a tricapped trigonal prismatic geometry.
Isotopes.
About 20 radioisotopes and 7 nuclear isomers between 233Cm and 252Cm are known for curium, and no stable isotopes. The longest half-lives have been reported for 247Cm (15.6 million years) and 248Cm (348,000 years). Other long-lived isotopes are 245Cm (half-life 8500 years), 250Cm (8,300 years) and 246Cm (4,760 years). Curium-250 is unusual in that it predominantly (about 86%) decays via spontaneous fission. The most commonly used curium isotopes are 242Cm and 244Cm with the half-lives of 162.8 days and 18.1 years, respectively.
All isotopes between 242Cm and 248Cm, as well as 250Cm, undergo a self-sustaining nuclear chain reaction and thus in principle can act as a nuclear fuel in a reactor. As in most transuranic elements, the nuclear fission cross section is especially high for the odd-mass curium isotopes243Cm, 245Cm and 247Cm. These can be used in thermal-neutron reactors, whereas a mixture of curium isotopes is only suitable for fast breeder reactors since the even-mass isotopes are not fissile in a thermal reactor and accumulate as burn-up increases. The mixed-oxide (MOX) fuel, which is to be used in power reactors, should contain little or no curium because the neutron activation of 248Cm will create californium. This is strong neutron emitter, and would pollute the back end of the fuel cycle and increase the dose to reactor personnel. Hence, if the minor actinides are to be used as fuel in a thermal neutron reactor, the curium should be excluded from the fuel or placed in special fuel rods where it is the only actinide present.
The table to the right lists the critical masses for curium isotopes for a sphere, without a moderator and reflector. With a metal reflector (30 cm of steel), the critical masses of the odd isotopes are about 3–4 kg. When using water (thickness ~20–30 cm) as the reflector, the critical mass can be as small as 59 gram for 245Cm, 155 gram for 243Cm and 1550 gram for 247Cm. There is a significant uncertainty in these critical mass values. Whereas it is usually of the order 20%, the values for 242Cm and 246Cm were listed as large as 371 kg and 70.1 kg, respectively, by some research groups.
Currently, curium is not used as a nuclear fuel owing to its low availability and high price. 245Cm and 247Cm have a very small critical mass and therefore could be used in portable nuclear weapons, but none have been reported thus far. Curium-243 is not suitable for this purpose because of its short half-life and strong α emission which would result in excessive heat. Curium-247 would be highly suitable, having a half-life 647 times that of plutonium-239.
Occurrence.
The longest-lived isotope of curium, 247Cm, has a half-life of 15.6 million years. Therefore, any primordial curium, that is curium present on the Earth during its formation, should have decayed by now. Curium is produced artificially, in small quantities for research purposes. Furthermore, it occurs in spent nuclear fuel. Curium is present in nature in certain areas used for the atmospheric nuclear weapons tests, which were conducted between 1945 and 1980. So the analysis of the debris at the testing site of the first U.S. hydrogen bomb, Ivy Mike, (1 November 1952, Enewetak Atoll), beside einsteinium, fermium, plutonium and americium also revealed isotopes of berkelium, californium and curium, in particular 245Cm, 246Cm and smaller quantities of 247Cm, 248Cm and 249Cm. For reasons of military secrecy, this result was published only in 1956.
Atmospheric curium compounds are poorly soluble in common solvents and mostly adhere to soil particles. Soil analysis revealed about 4,000 times higher concentration of curium at the sandy soil particles than in water present in the soil pores. An even higher ratio of about 18,000 was measured in loam soils.
The transuranic elements from americium to fermium, including curium, occurred naturally in the natural nuclear fission reactor at Oklo, but no longer do so.
Synthesis.
Isotope preparation.
Curium is produced in small quantities in nuclear reactors, and by now only kilograms of it have been accumulated for the 242Cm and 244Cm and grams or even milligrams for heavier isotopes. This explains the high price of curium, which has been quoted at 160–185 USD per milligram, with a more recent estimate at 2,000 USD/g for 242Cm and 170 USD/g for 244Cm. In nuclear reactors, curium is formed from 238U in a series of nuclear reactions. In the first chain, 238U captures a neutron and converts into 239U, which via β− decay transforms into 239Np and 239Pu.
Further neutron capture followed by β−-decay produces the 241Am isotope of americium which further converts into 242Cm:
For research purposes, curium is obtained by irradiating not uranium but plutonium, which is available in large amounts from spent nuclear fuel. Much higher neutron flux is used for the irradiation that results in a different reaction chain and formation of 244Cm:
Curium-244 decays into 240Pu by emission of alpha particle, but it also absorbs neutrons resulting in a small amount of heavier curium isotopes. Among those, 247Cm and 248Cm are popular in scientific research because of their long half-lives. However, the production rate of 247Cm in thermal neutron reactors is relatively low because of it is prone to undergo fission induced by thermal neutrons. Synthesis of 250Cm via neutron absorption is also rather unlikely because of the short half-life of the intermediate product 249Cm (64 min), which converts by β− decay to the berkelium isotope 249Bk.
254Cf. For this however, the production rate is low as 254Cf decays mainly by spontaneous fission, and only slightly by emission of α-particles into 250Cm. -->
The above cascade of (n,γ) reactions produces a mixture of different curium isotopes. Their post-synthesis separation is cumbersome, and therefore a selective synthesis is desired. Curium-248 is favored for research purposes because of its long half-life. The most efficient preparation method of this isotope is via α-decay of the californium isotope 252Cf, which is available in relatively large quantities due to its long half-life (2.65 years). About 35–50 mg of 248Cm is being produced by this method every year. The associated reaction produces 248Cm with isotopic purity of 97%.
Another interesting for research isotope 245Cm can be obtained from the α-decay of 249Cf, and the latter isotope is produced in minute quantities from the β−-decay of the berkelium isotope 249Bk.
Metal preparation.
Most synthesis routines yield a mixture of different actinide isotopes as oxides, from which a certain isotope of curium needs to be separated. An example procedure could be to dissolve spent reactor fuel (e.g. MOX fuel) in nitric acid, and remove the bulk of the uranium and plutonium using a PUREX (Plutonium – URanium EXtraction) type extraction with tributyl phosphate in a hydrocarbon. The lanthanides and the remaining actinides are then separated from the aqueous residue (raffinate) by a diamide-based extraction to give, after stripping, a mixture of trivalent actinides and lanthanides. A curium compound is then selectively extracted using multi-step chromatographic and centrifugation techniques with an appropriate reagent. "Bis"-triazinyl bipyridine complex has been recently proposed as such reagent which is highly selective to curium. Separation of curium from a very similar americium can also be achieved by treating a slurry of their hydroxides in aqueous sodium bicarbonate with ozone at elevated temperature. Both americium and curium are present in solutions mostly in the +3 valence state; whereas americium oxidizes to soluble Am(IV) complexes, curium remains unchanged and can thus be isolated by repeated centrifugation.
Metallic curium is obtained by reduction of its compounds. Initially, curium(III) fluoride was used for this purpose. The reaction was conducted in the environment free from water and oxygen, in the apparatus made of tantalum and tungsten, using elemental barium or lithium as reducing agents.
Another possibility is the reduction of curium(IV) oxide using a magnesium-zinc alloy in a melt of magnesium chloride and magnesium fluoride.
Compounds and reactions.
Oxides.
Curium readily reacts with oxygen forming mostly Cm2O3 and CmO2 oxides, but the divalent oxide CmO is also known. Black CmO2 can be obtained by burning curium oxalate (Cm2(C2O4)3), nitrate (Cm(NO3)3) or hydroxide in pure oxygen. Upon heating to 600–650 °C in vacuum (about 0.01 Pa), it transforms into the whitish Cm2O3:
Alternatively, Cm2O3 can be obtained by reducing CmO2 with molecular hydrogen:
Furthermore, a number of ternary oxides of the type M(II)CmO3 are known, where M stands for a divalent metal, such as barium.
Thermal oxidation of trace quantities of curium hydride (CmH2–3) has been reported to produce a volatile form of CmO2 and the volatile trioxide CmO3, one of the two known examples of the very rare +6 state for curium. Another observed species was reported to behave similarly to plutonium tetroxide and was tentatively characterized as CmO4, with curium in the extremely rare +8 state; however, new experiments seem to indicate that CmO4 does not exist.
Halides.
The colorless curium(III) fluoride (CmF3) can be produced by introducing fluoride ions into curium(III)-containing solutions. The brown tetravalent curium(IV) fluoride (CmF4) on the other hand is only obtained by reacting curium(III) fluoride with molecular fluorine:
A series of ternary fluorides are known of the form A7Cm6F31, where A stands for alkali metal.
The colorless curium(III) chloride (CmCl3) is produced in the reaction of curium(III) hydroxide (Cm(OH)3) with anhydrous hydrogen chloride gas. It can further be converted into other halides, such as curium(III) bromide (colorless to light green) and curium(III) iodide (colorless), by reacting it with the ammonia salt of the corresponding halide at elevated temperature of about 400–450 °C:
An alternative procedure is heating curium oxide to about 600 °C with the corresponding acid (such as hydrobromic for curium bromide). Vapor phase hydrolysis of curium(III) chloride results in curium oxychloride:
Chalcogenides and pnictides.
Sulfides, selenides and tellurides of curium have been obtained by treating curium with gaseous sulfur, selenium or tellurium in vacuum at elevated temperature. The pnictides of curium of the type CmX are known for the elements nitrogen, phosphorus, arsenic and antimony. They can be prepared by reacting either curium(III) hydride (CmH3) or metallic curium with these elements at elevated temperatures.
Organocurium compounds and biological aspects.
Organometallic complexes analogous to uranocene are known also for other actinides, such as thorium, protactinium, neptunium, plutonium and americium. Molecular orbital theory predicts a stable "curocene" complex (η8-C8H8)2Cm, but it has not been reported experimentally yet.
Formation of the complexes of the type Cm(n-C3H7-BTP)3, where BTP stands for 2,6-di(1,2,4-triazin-3-yl)pyridine, in solutions containing n-C3H7-BTP and Cm3+ ions has been confirmed by EXAFS. Some of these BTP-type complexes selectively interact with curium and therefore are useful in its selective separation from lanthanides and another actinides. Dissolved Cm3+ ions bind with many organic compounds, such as hydroxamic acid, urea, fluorescein and adenosine triphosphate. Many of these compounds are related to biological activity of various microorganisms. The resulting complexes exhibit strong yellow-orange emission under UV light excitation, which is convenient not only for their detection, but also for studying the interactions between the Cm3+ ion and the ligands via changes in the half-life (of the order ~0.1 ms) and spectrum of the fluorescence.
Curium has no biological significance. There are a few reports on biosorption of Cm3+ by bacteria and archaea, however no evidence for incorporation of curium into them.
Applications.
Radionuclides.
Curium is one of the most radioactive isolable elements. Its two most common isotopes 242Cm and 244Cm are strong alpha emitters (energy 6 MeV); they have relatively short half-lives of 162.8 days and 18.1 years, and produce as much as 120 W/g and 3 W/g of thermal energy, respectively. Therefore, curium can be used in its common oxide form in radioisotope thermoelectric generators like those in spacecraft. This application has been studied for the 244Cm isotope, while 242Cm was abandoned due to its prohibitive price of around 2000 USD/g. Curium-243 with a ~30 year half-life and good energy yield of ~1.6 W/g could make for a suitable fuel, but it produces significant amounts of harmful gamma and beta radiation from radioactive decay products. Though as an α-emitter, 244Cm requires a much thinner radiation protection shielding, it has a high spontaneous fission rate, and thus the neutron and gamma radiation rate are relatively strong. As compared to a competing thermoelectric generator isotope such as 238Pu, 244Cm emits a 500 time greater fluence of neutrons, and its higher gamma emission requires a shield that is 20 times thicker — about 2 inches of lead for a 1 kW source, as compared to 0.1 in for 238Pu. Therefore, this application of curium is currently considered impractical.
A more promising application of 242Cm is to produce 238Pu, a more suitable radioisotope for thermoelectric generators such as in cardiac pacemakers. The alternative routes to 238Pu use the (n,γ) reaction of 237Np, or the deuteron bombardment of uranium, which both always produce 236Pu as an undesired by-product — since the latter decays to 232U with strong gamma emission. Curium is also a common starting material for the production of higher transuranic elements and transactinides. Thus, bombardment of 248Cm with oxygen (18O) or magnesium (26Mg) yielded certain isotopes of seaborgium (265Sg) and hassium (269Hs and 270Hs). Californium was discovered when a microgram-sized target of curium-242 was irradiated with 35 MeV alpha particles using the cyclotron at Berkeley:
Only about 5,000 atoms of californium were produced in this experiment.
X-ray spectrometer.
The most practical application of 244Cm — though rather limited in total volume — is as α-particle source in the alpha particle X-ray spectrometers (APXS). These instruments were installed on the Sojourner, Mars, Mars 96, Mars Exploration Rovers and Philae comet lander, as well as the Mars Science Laboratory to analyze the composition and structure of the rocks on the surface of planet Mars. APXS was also used in the Surveyor 5–7 moon probes but with a 242Cm source.
An elaborated APXS setup is equipped with a sensor head containing six curium sources having the total radioactive decay rate of several tens of millicuries (roughly a gigabecquerel). The sources are collimated on the sample, and the energy spectra of the alpha particles and protons scattered from the sample are analyzed (the proton analysis is implemented only in some spectrometers). These spectra contain quantitative information on all major elements in the samples except for hydrogen, helium and lithium.
Safety.
Owing to its high radioactivity, curium and its compounds must be handled in appropriate laboratories under special arrangements. Whereas curium itself mostly emits α-particles which are absorbed by thin layers of common materials, some of its decay products emit significant fractions of beta and gamma radiation, which require a more elaborate protection. If consumed, curium is excreted within a few days and only 0.05% is absorbed in the blood. From there, about 45% goes to the liver, 45% to the bones, and the remaining 10% is excreted. In the bone, curium accumulates on the inside of the interfaces to the bone marrow and does not significantly redistribute with time; its radiation destroys bone marrow and thus stops red blood cell creation. The biological half-life of curium is about 20 years in the liver and 50 years in the bones. Curium is absorbed in the body much more strongly via inhalation, and the allowed total dose of 244Cm in soluble form is 0.3 μC. Intravenous injection of 242Cm and 244Cm containing solutions to rats increased the incidence of bone tumor, and inhalation promoted pulmonary and liver cancer.
Curium isotopes are inevitably present in spent nuclear fuel with a concentration of about 20 g/tonne. Among them, the 245Cm–248Cm isotopes have decay times of thousands of years and need to be removed to neutralize the fuel for disposal. The associated procedure involves several steps, where curium is first separated and then converted by neutron bombardment in special reactors to short-lived nuclides. This procedure, nuclear transmutation, while well documented for other elements, is still being developed for curium.

</doc>
<doc id="5676" url="https://en.wikipedia.org/wiki?curid=5676" title="Californium">
Californium

Californium is a radioactive metallic chemical element with symbol Cf and atomic number 98. The element was first made in 1950 at the University of California Radiation Laboratory in Berkeley, by bombarding curium with alpha particles (helium-4 ions). It is an actinide element, the sixth transuranium element to be synthesized, and has the second-highest atomic mass of all the elements that have been produced in amounts large enough to see with the unaided eye (after einsteinium). The element was named after the university and the state of California.
Two crystalline forms exist for californium under normal pressure: one above and one below . A third form exists at high pressure. Californium slowly tarnishes in air at room temperature. Compounds of californium are dominated by a chemical form of the element, designated californium(III), that can participate in three chemical bonds. The most stable of californium's twenty known isotopes is californium-251, which has a half-life of 898 years. This short half-life means the element is not found in significant quantities in the Earth's crust. Californium-252, with a half-life of about 2.64 years, is the most common isotope used and is produced at the Oak Ridge National Laboratory in the United States and the Research Institute of Atomic Reactors in Russia.
Californium is one of the few transuranium elements that have practical applications. Most of these applications exploit the property of certain isotopes of californium to emit neutrons. For example, californium can be used to help start up nuclear reactors, and it is employed as a source of neutrons when studying materials with neutron diffraction and neutron spectroscopy. Californium can also be used in nuclear synthesis of higher mass elements; ununoctium (element 118) was synthesized by bombarding californium-249 atoms with calcium-48 ions. Users of californium must take into account radiological concerns and the element's ability to disrupt the formation of red blood cells by bioaccumulating in skeletal tissue.
Characteristics.
Physical properties.
Californium is a silvery white actinide metal with a melting point of and an estimated boiling point of . The pure metal is malleable and is easily cut with a razor blade. Californium metal starts to vaporize above when exposed to a vacuum. Below 51 K (−220 °C) californium metal is either ferromagnetic or ferrimagnetic (it acts like a magnet), between 48 and 66 K it is antiferromagnetic (an intermediate state), and above it is paramagnetic (external magnetic fields can make it magnetic). It forms alloys with lanthanide metals but little is known about them.
The element has two crystalline forms under 1 standard atmosphere of pressure: A double-hexagonal close-packed form dubbed alpha (α) and a face-centered cubic form designated beta (β). The α form exists below with a density of 15.10 g/cm3 and the β form exists above 900 °C with a density of 8.74 g/cm3. At 48 GPa of pressure the β form changes into an orthorhombic crystal system due to delocalization of the atom's 5f electrons, which frees them to bond.
The bulk modulus of a material is a measure of its resistance to uniform pressure. Californium's bulk modulus is 50 ± 5 GPa, which is similar to trivalent lanthanide metals but smaller than more familiar metals, such as aluminium (70 GPa).
Chemical properties and compounds.
Californium exhibits valences of 4, 3, or 2; indicating the number of chemical bonds one atom of this element can form. Its chemical properties are predicted to be similar to other primarily 3+ valence actinide elements and the element dysprosium, which is the lanthanide above californium in the periodic table. The element slowly tarnishes in air at room temperature, with the rate increasing when moisture is added. Californium reacts when heated with hydrogen, nitrogen, or a chalcogen (oxygen family element); reactions with dry hydrogen and aqueous mineral acids are rapid. 
Californium is only water-soluble as the californium(III) cation. Attempts to reduce or oxidize the +3 ion in solution have failed. The element forms a water-soluble chloride, nitrate, perchlorate, and sulfate and is precipitated as a fluoride, oxalate, or hydroxide. Californium is the heaviest actinide to exhibit covalent properties, as is observed in the californium borate.
Isotopes.
Twenty radioisotopes of californium have been characterized, the most stable being californium-251 with a half-life of 898 years, californium-249 with a half-life of 351 years, californium-250 with a half-life of 13.08 years, and californium-252 with a half-life of 2.645 years. All the remaining isotopes have half-lives shorter than a year, and the majority of these have half-lives shorter than 20 minutes. The isotopes of californium range in mass number from 237 to 256.
Californium-249 is formed from the beta decay of berkelium-249, and most other californium isotopes are made by subjecting berkelium to intense neutron radiation in a nuclear reactor. Although californium-251 has the longest half-life, its production yield is only 10% due to its tendency to collect neutrons (high neutron capture) and its tendency to interact with other particles (high neutron cross-section).
Californium-252 is a very strong neutron emitter, which makes it extremely radioactive and harmful. Californium-252 undergoes alpha decay (the loss of two protons and two neutrons) 96.9% of the time to form curium-248 while the remaining 3.1% of decays are spontaneous fission. One microgram (µg) of californium-252 emits 2.3 million neutrons per second, an average of 3.7 neutrons per spontaneous fission. Most of the other isotopes of californium decay to isotopes of curium (atomic number 96) via alpha decay.
History.
Californium was first synthesized at the University of California Radiation Laboratory in Berkeley, by the physics researchers Stanley G. Thompson, Kenneth Street, Jr., Albert Ghiorso, and Glenn T. Seaborg on or about February 9, 1950. It was the sixth transuranium element to be discovered; the team announced its discovery on March 17, 1950.
To produce californium, a microgram-sized target of curium-242 () was bombarded with 35 MeV-alpha particles () in the cyclotron at Berkeley, which produced californium-245 () plus one free neutron ().
Only about 5,000 atoms of californium were produced in this experiment, and these atoms had a half-life of 44 minutes.
The discoverers named the new element after the university and the state. This was a break from the convention used for elements 95 to 97, which drew inspiration from how the elements directly above them in the periodic table were named. However, the element directly above element 98 in the periodic table, dysprosium, has a name that simply means "hard to get at" so the researchers decided to set aside the informal naming convention. They added that "the best we can do is to point out ... searchers a century ago found it difficult to get to California."
Weighable quantities of californium were first produced by the irradiation of plutonium targets at the Materials Testing Reactor at the National Reactor Testing Station in eastern Idaho; and these findings were reported in 1954. The high spontaneous fission rate of californium-252 was observed in these samples. The first experiment with californium in concentrated form occurred in 1958. The isotopes californium-249 to californium-252 were isolated that same year from a sample of plutonium-239 that had been irradiated with neutrons in a nuclear reactor for five years. Two years later, in 1960, Burris Cunningham and James Wallman of the Lawrence Radiation Laboratory of the University of California created the first californium compounds—californium trichloride, californium oxychloride, and californium oxide—by treating californium with steam and hydrochloric acid.
The High Flux Isotope Reactor (HFIR) at the Oak Ridge National Laboratory (ORNL) in Oak Ridge, Tennessee, started producing small batches of californium in the 1960s. By 1995, the HFIR nominally produced of californium annually. Plutonium supplied by the United Kingdom to the United States under the 1958 US-UK Mutual Defence Agreement was used for californium production.
The Atomic Energy Commission sold californium-252 to industrial and academic customers in the early 1970s for $10 per microgram and an average of of californium-252 were shipped each year from 1970 to 1990. Californium metal was first prepared in 1974 by Haire and Baybarz who reduced californium(III) oxide with lanthanum metal to obtain microgram amounts of sub-micrometer thick films.
Occurrence.
Traces of californium can be found near facilities that use the element in mineral prospecting and in medical treatments. The element is fairly insoluble in water, but it adheres well to ordinary soil; and concentrations of it in the soil can be 500 times higher than in the water surrounding the soil particles.
Fallout from atmospheric nuclear testing prior to 1980 contributed a small amount of californium to the environment. Californium isotopes with mass numbers 249, 252, 253, and 254 have been observed in the radioactive dust collected from the air after a nuclear explosion. Californium is not a major radionuclide at United States Department of Energy legacy sites since it was not produced in large quantities.
Californium was once believed to be produced in supernovas, as their decay matches the 60 day half-life of 254Cf. However, subsequent studies failed to demonstrate any californium spectra, and supernova light curves are now thought to follow the decay of nickel-56.
The transuranic elements from americium to fermium, including californium, occurred naturally in the natural nuclear fission reactor at Oklo, but no longer do so.
Production.
Californium is produced in nuclear reactors and particle accelerators. Californium-250 is made by bombarding berkelium-249 () with neutrons, forming berkelium-250 () via neutron capture (n,γ) which, in turn, quickly beta decays (β−) to californium-250 () in the following reaction:
Bombardment of californium-250 with neutrons produces californium-251 and californium-252.
Prolonged irradiation of americium, curium, and plutonium with neutrons produces milligram amounts of californium-252 and microgram amounts of californium-249. As of 2006, curium isotopes 244 to 248 are irradiated by neutrons in special reactors to produce primarily californium-252 with lesser amounts of isotopes 249 to 255.
Microgram quantities of californium-252 are available for commercial use through the U.S. Nuclear Regulatory Commission. Only two sites produce californium-252 – the Oak Ridge National Laboratory in the United States, and the Research Institute of Atomic Reactors in Dimitrovgrad, Russia. As of 2003, the two sites produce 0.25 grams and 0.025 grams of californium-252 per year, respectively.
Three californium isotopes with significant half-lives are produced, requiring a total of 15 neutron captures by uranium-238 without nuclear fission or alpha decay occurring during the process. Californium-253 is at the end of a production chain that starts with uranium-238, includes several isotopes of plutonium, americium, curium, berkelium, and the californium isotopes 249 to 253 (see diagram).
Applications.
Californium-252 has a number of specialized applications as a strong neutron emitter, and each microgram of fresh californium produces 139 million neutrons per minute. This property makes californium useful as a neutron startup source for some nuclear reactors and as a portable (non-reactor based) neutron source for neutron activation analysis to detect trace amounts of elements in samples. Neutrons from californium are employed as a treatment of certain cervical and brain cancers where other radiation therapy is ineffective. It has been used in educational applications since 1969 when the Georgia Institute of Technology received a loan of 119 µg of californium-252 from the Savannah River Plant. It is also used with online elemental coal analyzers and bulk material analyzers in the coal and cement industries.
Neutron penetration into materials makes californium useful in detection instruments such as fuel rod scanners; neutron radiography of aircraft and weapons components to detect corrosion, bad welds, cracks and trapped moisture; and in portable metal detectors. Neutron moisture gauges use californium-252 to find water and petroleum layers in oil wells, as a portable neutron source for gold and silver prospecting for on-the-spot analysis, and to detect ground water movement. The major uses of californium-252 in 1982 were, in order of use, reactor start-up (48.3%), fuel rod scanning (25.3%), and activation analysis (19.4%). By 1994 most californium-252 was used in neutron radiography (77.4%), with fuel rod scanning (12.1%) and reactor start-up (6.9%) as important but distant secondary uses.
Californium-251 has a very small calculated critical mass (about ), high lethality, and a relatively short period of toxic environmental irradiation. The low critical mass of californium led to some exaggerated claims about possible uses for the element.
In October 2006, researchers announced that three atoms of ununoctium (element 118) had been identified at the Joint Institute for Nuclear Research in Dubna, Russia, as the product of bombardment of californium-249 with calcium-48, making it the heaviest element ever synthesized. The target for this experiment contained about 10 mg of californium-249 deposited on a titanium foil of 32 cm2 area. Californium has also been used to produce other transuranium elements; for example, element 103 (later named lawrencium) was first synthesized in 1961 by bombarding californium with boron nuclei.
Precautions.
Californium that bioaccumulates in skeletal tissue releases radiation that disrupts the body's ability to form red blood cells. The element plays no natural biological role in any organism due to its intense radioactivity and low concentration in the environment.
Californium can enter the body from ingesting contaminated food or drinks or by breathing air with suspended particles of the element. Once in the body, only 0.05% of the californium will reach the bloodstream. About 65% of that californium will be deposited in the skeleton, 25% in the liver, and the rest in other organs, or excreted, mainly in urine. Half of the californium deposited in the skeleton and liver are gone in 50 and 20 years, respectively. Californium in the skeleton adheres to bone surfaces before slowly migrating throughout the bone.
The element is most dangerous if taken into the body. In addition, californium-249 and californium-251 can cause tissue damage externally, through gamma ray emission. Ionizing radiation emitted by californium on bone and in the liver can cause cancer.

</doc>
