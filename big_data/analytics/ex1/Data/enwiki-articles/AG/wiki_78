<doc id="45802" url="https://en.wikipedia.org/wiki?curid=45802" title="Social capital">
Social capital

<onlyinclude>
Social capital is a form of economic and cultural capital in which social networks are central, transactions are marked by reciprocity, trust, and cooperation, and market agents produce goods and services not mainly for themselves, but for a common good.
The term generally refers to (a) resources, and the value of these resources, both tangible (public spaces, private property) and intangible ("actors", "human capital", people), (b) the relationships among these resources, and (c) the impact that these relationships have on the resources involved in each relationship, and on larger groups. It is generally seen as a form of capital that produces public goods for a common good.
Social capital has been used to explain the improved performance of diverse groups, the growth of entrepreneurial firms, superior managerial performance, enhanced supply chain relations, the value derived from strategic alliances, and the evolution of communities.
</onlyinclude>
During the 1990s and 2000s the concept has become increasingly popular in a wide range of social science disciplines and also in politics.
Background.
The term "social capital" was in intermittent use from about 1890, before becoming widely used in the late 1990s.
In the first half of the 19th century, Alexis de Tocqueville had observations about American life that seemed to outline and define social capital. He observed that Americans were prone to meeting at as many gatherings as possible to discuss all possible issues of state, economics, or the world that could be witnessed. The high levels of transparency caused greater participation from the people and thus allowed for democracy to work better. The French writers highlighted also that the level of social participation (social capital) in American society was directly linked to the equality of conditions (Ferragina, 2010; 2012; 2013).
L. J. Hanifan's 1916 article regarding local support for rural schools is one of the first occurrences of the term "social capital" in reference to social cohesion and personal investment in the community. In defining the concept, Hanifan contrasts social capital with material goods by defining it as:
I do not refer to real estate, or to personal property or to cold cash, but rather to that in life which tends to make these tangible substances count for most in the daily lives of people, namely, goodwill, fellowship, mutual sympathy and social intercourse among a group of individuals and families who make up a social unit… If he may come into contact with his neighbour, and they with other neighbours, there will be an accumulation of social capital, which may immediately satisfy his social needs and which may bear a social potentiality sufficient to the substantial improvement of living conditions in the whole community. The community as a whole will benefit by the cooperation of all its parts, while the individual will find in his associations the advantages of the help, the sympathy, and the fellowship of his neighbours (pp. 130-131).
John Dewey used the term in his monograph entitled "School and Society" in 1900, but he offered no definition of it.
Jane Jacobs used the term early in the 1960's. Although she did not explicitly define the term "social capital", her usage referred to the value of networks. Political scientist Robert Salisbury advanced the term as a critical component of interest group formation in his 1969 article "An Exchange Theory of Interest Groups" in the "Midwest Journal of Political Science". Sociologist Pierre Bourdieu used the term in 1972 in his "Outline of a Theory of Practice", and clarified the term some years later in contrast to cultural, economic, and symbolic capital. Sociologists James Coleman, Barry Wellman and Scot Wortley adopted Glenn Loury's 1977 definition in developing and popularising the concept. In the late 1990s the concept gained popularity, serving as the focus of a World Bank research programme and the subject of several mainstream books, including Robert Putnam's "Bowling Alone" and Putnam and Lewis Feldstein's "Better Together".
The concept that underlies social capital has a much longer history; thinkers exploring the relation between associational life and democracy were using similar concepts regularly by the 19th century, drawing on the work of earlier writers such as James Madison ("The Federalist Papers") and Alexis de Tocqueville ("Democracy in America") to integrate concepts of social cohesion and connectedness into the pluralist tradition in American political science. John Dewey may have made the first direct mainstream use of "social capital" in "The School and Society" in 1899, though he did not offer a definition.
The power of "community governance" has been stressed by many philosophers from antiquity to the 18th century, from Aristotle to Thomas Aquinas and Edmund Burke (Bowles and Gintis, 2002). This vision was strongly criticised at the end of the 18th century, with the development of the idea of "Homo Economicus" and subsequently with "rational choice theory". Such a set of theories became dominant in the last centuries, but many thinkers questioned the complicated relationship between "modern society" and the importance of "old institutions", in particular family and traditional communities (Ferragina, 2010:75). The debate of community versus modernization of society and individualism has been the most discussed topic among the fathers of sociology (Tönnies, 1887; Durkheim, 1893; Simmel, 1905; Weber, 1946). They were convinced that industrialisation and urbanization were transforming social relationship in an irreversible way. They observed a breakdown of traditional bonds and the progressive development of anomie and alienation in society (Wilmott, 1986).
After Tönnies' and Weber's works, reflection on social links in modern society continued with interesting contributions in the 1950s and in the 1960s, in particular "mass society theory" (Bell, 1962; Nisbet, 1969; Stein, 1960; Whyte, 1956). They proposed themes similar to those of the founding fathers, with a more pessimistic emphasis on the development of society (Ferragina, 2010: 76). In the words of Stein (1960:1): “The price for maintaining a society that encourages cultural differentiation and experimentation is unquestionably the acceptance of a certain amount of disorganization on both the individual and social level.” All these reflections contributed remarkably to the development of the social capital concept in the following decades.
The appearance of the modern social capital conceptualization is a new way to look at this debate, keeping together the importance of community to build generalized trust and the same time, the importance of individual free choice, in order to create a more cohesive society (Ferragina, 2010; Ferragina, 2012 It is for this reason that social capital generated so much interest in the academic and political world (Rose, 2000).
Evaluating social capital.
Pierre Bourdieu's work tends to show how social capital can be used practically to produce or reproduce inequality, demonstrating for instance how people gain access to powerful positions through the direct and indirect employment of social connections. Robert Putnam has used the concept in a much more positive light: though he was at first careful to argue that social capital was a neutral term, stating “whether or not shared are praiseworthy is, of course, entirely another matter”, his work on American society tends to frame social capital as a producer of "civic engagement" and also a broad societal measure of communal health. He also transforms social capital from a resource possessed by individuals to an attribute of collectives, focusing on norms and trust as producers of social capital to the exclusion of networks.
Mahyar Arefi identifies consensus building as a direct positive indicator of social capital. Consensus implies “shared interest” and agreement among various actors and stakeholders to induce collective action. Collective action is thus an indicator of increased social capital.
Edwards and Foley, as editors of a special edition of the "American Behavioural Scientist" on "Social Capital, Civil Society and Contemporary Democracy", raised two key issues in the study of social capital. First, social capital is not equally available to all, in much the same way that other forms of capital are differently available. Geographic and social isolation limit access to this resource. Second, not all social capital is created equally. The value of a specific source of social capital depends in no small part on the socio-economic position of the source with society. On top of this, Portes has identified four negative consequences of social capital: exclusion of outsiders; excess claims on group members; restrictions on individual freedom; and downward levelling norms.
An interesting distinction of social organization is that between bonding and bridging ties, which complicates the neo-Tocquevillean view of social capital.
Varshney studied the correlation between the presence of interethnic networks (bridging) versus intra-ethnic ones (bonding) on ethnic violence in India.
He argues that interethnic networks are agents of peace because they build bridges and manage tensions, by noting that if communities are organized only along intra-ethnic lines and the interconnections with other communities are very weak or even nonexistent, then ethnic violence is quite likely.
Three main implications of intercommunal ties explain their worth:
This is a useful distinction; nevertheless its implication on social capital can only be accepted if one espouses the functionalist understanding of the latter concept. Indeed, it can be argued that interethnic, as well as intra-ethnic networks can serve various purposes, either increasing or diminishing social capital. In fact, Varshney himself notes that intraethnic policing (equivalent to the “self-policing” mechanism proposed by Fearon and Laitin) may lead to the same result as interethnic engagement.
Social capital is often linked to the success of democracy and political involvement. Robert D. Putnam, in his book "Bowling Alone" makes the argument that social capital is linked to the recent decline in American political participation. Putnam's theoretical framework has been firstly applied to the South of Italy (Putnam, 1993). This framework has been rediscussed by considering simultaneously the condition of European regions and specifically Southern Italy (Ferragina, 2012; Ferragina, 2013).
Definitions, Forms, and Measurement.
Social capital lends itself to multiple definitions, interpretations, and uses. Thomas Sander defines it as "the collective value of all social networks (who people know), and the inclinations that arise from these networks to do things for each other (norms of reciprocity)." Social capital, in this view, emphasizes "specific benefits that flow from the trust, reciprocity, information, and cooperation associated with social networks". It "creates value for the people who are connected, and for bystanders as well." Meanwhile, negative norms of reciprocity serve as disincentives for detrimental and violent behaviors.
David Halpern argues that the popularity of social capital for policymakers is linked to the concept's duality, coming because "it has a hard nosed economic feel while restating the importance of the social." For researchers, the term is popular partly due to the broad range of outcomes it can explain; the multiplicity of uses for social capital has led to a multiplicity of definitions. Social capital has been used at various times to explain superior managerial performance, the growth of entrepreneurial firms, improved performance of functionally diverse groups, the value derived from strategic alliances, and enhanced supply chain relations.
'A resource that actors derive from specific social structures and then use to pursue their interests; it is created by changes in the relationship among actors'; (Baker 1990, p. 619).
Early attempts to define social capital focused on the degree to which social capital as a resource should be used for public good or for the benefit of individuals. Putnam suggested that social capital would facilitate co-operation and mutually supportive relations in communities and nations and would therefore be a valuable means of combating many of the social disorders inherent in modern societies, for example crime. In contrast to those focusing on the individual benefit derived from the web of social relationships and ties individual actors find themselves in, attribute social capital to increased personal access to information and skill sets and enhanced power. According to this view, individuals could use social capital to further their own career prospects, rather than for the good of organisations.
In "The Forms of Capital" Pierre Bourdieu distinguishes between three forms of capital: economic capital, cultural capital and social capital. He defines social capital as "the aggregate of the actual or potential resources which are linked to possession of a durable network of more or less institutionalized relationships of mutual acquaintance and recognition." His treatment of the concept is instrumental, focusing on the advantages to possessors of social capital and the "“deliberate construction of sociability for the purpose of creating this resource.”" Quite contrary to Putnam's positive view of social capital, Bourdieu employs the concept to demonstrate a mechanism for the generational reproduction of inequality. Bourdieu thus points out that the wealthy and powerful use their "old boys network" or other social capital to maintain advantages for themselves, their social class, and their children.
James Coleman defined social capital functionally as “a variety of entities with two elements in common: they all consist of some aspect of social structure, and they facilitate certain actions of actors...within the structure”—that is, social capital is anything that facilitates individual or collective action, generated by networks of relationships, reciprocity, trust, and social norms. In Coleman's conception, social capital is a neutral resource that facilitates any manner of action, but whether society is better off as a result depends entirely on the individual uses to which it is put.
According to Robert Putnam, social capital "connections among individuals - social networks and the norms of reciprocity and trustworthiness that arise from them." According to Putnam and his followers, social capital is a key component to building and maintaining democracy. Putnam says that social capital is declining in the United States. This is seen in lower levels of trust in government and lower levels of civic participation. Putnam also says that television and urban sprawl have had a significant role in making America far less 'connected'. Putnam believes that social capital can be measured by the amount of trust and "reciprocity" in a community or between individuals.
Putnam also suggests that a root cause of the decline in social capital is women's entry the workforce, which could correlate with time restraints that inhibit civic organizational involvement like parent-teacher associations. Technological transformation of leisure (e.g., television) is another cause of declining social capital, as stated by Putnam. This offered a reference point from which several studies assessed social capital measurements by how media is engaged strategically to build social capital (
Nan Lin's concept of social capital has a more individualistic approach: "Investment in social relations with expected returns in the marketplace." This may subsume the concepts of some others such as Bourdieu, Flap and Eriksson.
Newton (1997) considered social capital as subjective phenomenon formed by values and attitudes which influence interactions.
In "Social Capital and Development: The Coming Agenda," Francis Fukuyama points out that there isn't an agreed definition of social capital, so he explains it as "shared norms or values that promote social cooperation, instantiated in actual social relationships" (Fukuyama, 27), and uses this definition throughout this paper. He argues that social capital is a necessary precondition for successful development, but a strong rule of law and basic political institutions are necessary to build social capital. He believes that a strong social capital is necessary for a strong democracy and strong economic growth. Familism is a major problem of trust because it fosters a two-tiered moral system, in which a person must favor the opinions of family members. Fukuyama believes that bridging social capital (a phrase used by Putnam in "Bowling Alone"), is essential for a strong social capital because a broader radius of trust will enable connections across borders of all sorts and serve as a basis for organizations. Although he points out many problems and possible solutions in his paper, he does admit that there is still much to be done to build a strong social capital.
Nahapiet and Ghoshal in their examination of the role of social capital in the creation of intellectual capital, suggest that social capital should be considered in terms of three clusters: structural, relational, and cognitive. Carlos García Timón describes that the structural dimensions of social capital relate to an individual ability to make weak and strong ties to others within a system. This dimension focuses on the advantages derived from the configuration of an actor's, either individual or collective, network. The differences between weak and strong ties are explained by Granovetter. The relational dimension focuses on the character of the connection between individuals. This is best characterized through trust of others and their cooperation and the identification an individual has within a network. Hazleton and Kennan added a third angle, that of communication. Communication is needed to access and use social capital through exchanging information, identifying problems and solutions, and managing conflict. According to Boisot and Boland and Tenkasi, meaningful communication requires at least some sharing context between the parties to such exchange. The cognitive dimension focusses on the shared meaning and understanding that individuals or groups have with one another.
A number of scholars have raised concerns about lack of precise definition of social capital. Portes, for example, noted that the term has become so widely used, including in mainstream media, that "the point is approaching at which social capital comes to be applied to so many events and in so many different contexts as to lose any distinct meaning." Robison, Schmid, and Siles reviewed various definitions of social capital and concluded that many did not satisfy the formal requirement of a definition. They noted that definitions must be of the form A=B while many definition of social capital described what it can be used to achieve, where it resides, how it can be created, and what it can transform. In addition, they argue that many proposed definition of social capital fail to satisfy the requirements of capital. They propose that social capital be defined as "sympathy". The object of another's sympathy has social capital. Those who have sympathy for others provide social capital. One of the main advantages of having social capital is that it provides access to resources on preferential terms. Their definition of sympathy follows that used by Adam Smith, the title of his first chapter in the "Theory of Moral Sentiments."
A network-based conception can also be used for characterizing the social capital of collectivities (such as organizations or business clusters).
Roots.
Social capital: a new name from an old idea.
The modern emergence of social capital concept renewed the academic interest for an old debate in social science: the relationship between trust, social networks and the development of modern industrial society. Social Capital Theory gained importance through the integration of classical sociological theory with the description of an intangible form of capital. In this way the classical definition of capital has been overcome allowing researchers to tackle issues in a new manner (Ferragina, 2010:73).
Through the social capital concept researchers have tried to propose a synthesis between the value contained in the communitarian approaches and individualism professed by the 'rational choice theory.' Social capital can only be generated collectively thanks to the presence of communities and social networks, but individuals and groups can use it at the same time. Individuals can exploit social capital of their networks to achieve private objectives and groups can use it to enforce a certain set of norms or behaviors. In this sense, social capital is generated collectively but it can also be used individually, bridging the dichotomized approach 'communitarianism' versus 'individualism' (Ferragina, 2010:75).
Definitional issues.
The term "capital" is used by analogy with other forms of economic capital, as social capital is argued to have similar (although less measurable) benefits. However, the analogy with capital is misleading to the extent that, unlike traditional forms of capital, social capital is not depleted by use; in fact it is depleted by non-use ("use it or lose it"). In this respect, it is similar to the now well-established economic concept of human capital.
Social capital is also distinguished from the economic theory social capitalism. Social capitalism as a theory challenges the idea that socialism and capitalism are mutually exclusive. Social capitalism posits that a strong social support network for the poor enhances capital output. By decreasing poverty, capital market participation is enlarged.
Sub-types.
In "Bowling Alone: The Collapse and Revival of American Community" (Simon & Schuster, 2000), Harvard political scientist Robert D. Putnam wrote: "Henry Ward Beecher's advice a century ago to 'multiply picnics' is not entirely ridiculous today. We should do this, ironically, not because it will be good for America — though it will be — but because it will be good for us." This quote is illustrative of the use of social capital within neo-liberal discourse to divert attention away from economic inequality as the source of social problems.
Daniel P. Aldrich, Associate Professor at Purdue University, describes three mechanisms of social capital. Aldrich defines the three differences as bonding, bridging, and linking social capital. Bonding capital are the relationships a person has with friends and family, making it also the strongest form of social capital. Bridging capital is the relationship between friends of friends, making its strength secondary to bonding capital. Linking capital is the relationship between a person and a government official or other elected leader. Aldrich also applies the ideas of social capital to the fundamental principles of disaster recovery, and discusses factors that either aid or impede recovery, such as extent of damage, population density, quality of government and aid. He primarily examines Japanese recovery following the 2011 Fukishima nuclear meltdown in his book "Building Resilience: Social Capital in Post-Disaster Recovery."
Putnam speaks of two main components of the concept: "bonding social capital" and "bridging social capital", the creation of which Putnam credits to Ross Gittell and Avis Vidal. Bonding refers to the value assigned to social networks between homogeneous groups of people and Bridging refers to that of social networks between socially heterogeneous groups. Typical examples are that criminal gangs create bonding social capital, while choirs and bowling clubs (hence the title, as Putnam lamented their decline) create bridging social capital. Bridging social capital is argued to have a host of other benefits for societies, governments, individuals, and communities; Putnam likes to note that joining an organization cuts in half an individual's chance of dying within the next year.
The distinction is useful in highlighting how social capital may not always be beneficial for society as a whole (though it is always an asset for those individuals and groups involved). Horizontal networks of individual citizens and groups that enhance community productivity and cohesion are said to be positive social capital assets whereas self-serving exclusive gangs and hierarchical patronage systems that operate at cross purposes to societal interests can be thought of as negative social capital burdens on society.
Social capital development on the internet via social networking websites such as Facebook or Myspace tends to be bridging capital according to one study, though "virtual" social capital is a new area of research.
There are two other sub-sources of social capital. These are consummatory, or a behavior that is made up of actions that fulfill a basis of doing what is inherent, and instrumental, or behavior that is taught through ones surroundings over time. Two examples of consummatory social capital are value interjection and solidarity. Value interjection pertains to a person or community that fulfills obligations such as paying bills on time, philanthropy, and following the rules of society. People that live their life this way feel that these are norms of society and are able to live their lives free of worry for their credit, children, and receive charity if needed. Coleman goes on to say that when people live in this way and benefit from this type of social capital, individuals in the society are able to rest assured that their belongings and family will be safe.
The other form of consummatory social capital, solidarity, dates back to the writings of Karl Marx, a German philosopher and political economist from the 19th century. The main focus of the study of Karl Marx was the working class of the Industrial Revolution. Marx analyzed the reasons these workers supported each other for the benefit of the group. He held that this support was an adaptation to the immediate time as opposed to a trait that was installed in them throughout their youth. As another example, Coleman states that this type of social capital is the type that brings individuals to stand up for what they believe in, and even die for it, in the face of adversity.
The second of these two other sub-sources of social capital is that of instrumental social capital. The basis of the category of social capital is that an individual who donates his or her resources not because he is seeking direct repayment from the recipient, but because they are part of the same social structure. By his or her donation, the individual might not see a direct repayment, but, most commonly, they will be held by the society in greater honor. The best example of this, and the one that Portes mentions, is the donation of a scholarship to a member of the same ethnic group. The donor is not freely giving up his resources to be directly repaid by the recipient, but, as stated above, the honor of the community. With this in mind, the recipient might not know the benefactor personally, but he or she prospers on the sole factor that he or she is a member of the same social group.
Measurement.
There is no widely held consensus on how to measure social capital, which has become a debate in itself: why refer to this phenomenon as 'capital' if there is no true way to measure it? While one can usually intuitively sense the level/amount of social capital present in a given relationship (regardless of type or scale), quantitative measuring has proven somewhat complicated. This has resulted in different metrics for different functions. In measuring political social capital, it is common to take the sum of society’s membership of its groups. Groups with higher membership (such as political parties) contribute more to the amount of capital than groups with lower membership, although many groups with low membership (such as communities) still add up to be significant. While it may seem that this is limited by population, this need not be the case as people join multiple groups. In a study done by Yankee City, a community of 17,000 people was found to have over 22,000 different groups.
Many studies measure social capital by asking the question: “do you trust the others?” Other researches analyse the participation in voluntary associations or civic activities.
Knack and Keefer (1996) measured econometrically correlations between confidence and civic cooperation norms, with economic growth in a big group of countries. They found that confidence and civic cooperation have a great impact in economic growth, and that in less polarized societies in terms of inequality and ethnic differences, social capital is bigger.
Narayan and Pritchet (1997) researched the associativity degree and economic performance in rural homes of Tanzania. They saw that even in high poverty indexes, families with higher levels of incomes had more participation in collective organizations. The social capital they accumulated because of this participation had individual benefits for them, and created collective benefits through different routes, for example: their agricultural practices were better than those of the families without participation (they had more information about agrochemicals, fertilizers and seeds); they had more information about the market; they were prepared to take more risks, because being part of a social network made them feel more protected; they had an influence on the improvement of public services, showing a bigger level of participation in schools; they cooperated more in the municipality level.
The level of cohesion of a group also affects its social capital. However, there is no one quantitative way of determining the level of cohesiveness, but rather a collection of social network models that researchers have used over the decades to operationalize social capital. One of the dominant methods is Ronald Burt's constraint measure, which taps into the role of tie strength and group cohesion. Another network-based model is network transitivity.
How a group relates to the rest of society also affects social capital, but in a different manner. Strong internal ties can in some cases weaken the group’s perceived capital in the eyes of the general public, as in cases where the group is geared towards crime, distrust, intolerance, violence or hatred towards others. The Ku Klux Klan and the Mafia are examples of these kinds of organizations.
Sociologists Carl L. Bankston and Min Zhou have argued that one of the reasons social capital is so difficult to measure is that it is neither an individual-level nor a group-level phenomenon, but one that emerges across levels of analysis as individuals participate in groups. They argue that the metaphor of "capital" may be misleading because unlike financial capital, which is a resource held by an individual, the benefits of forms of social organization are not held by actors, but are results of the participation of actors in advantageously organized groups.
To expand upon the methodological potential of measuring online and offline social bonding, as it relates to social capital, offers a matrix of social capital measures that distinguishes social bridging as a form of less emotionally tethered relationships compared to bonding. Bonding and bridging sub-scales are proposed, which have been adopted by over 300 scholarly articles. Lin, Peng, Kim, Kim & LaRose (2012) offer a noteworthy application of the scale by measuring international residents originating from locations outside of the United States. The study found that social media platforms like Facebook provide an opportunity for increased social capital, but mostly for extroverts. However, less introverted social media users could engage social media and build social capital by connecting with Americans before arriving and then maintaining old relationships from home upon arriving to the states. The ultimate outcome of the study indicates that social capital is measurable and is a concept that may be operationalized to understand strategies for coping with cross-cultural immersion through online engagement.
Recently, Foschi and Lauriola presented a measure of sociability as a proxy of social capital. The authors demonstrated that facets of sociability can mediate between general personality traits and measures of civic involvement and political participation, as predictors of social capital, in a holistic model of political behavior.
Integrating history and socio-economic analysis.
Beyond Putnam.
Robert Putnam's work contributed to shape the discussion of the importance of social capital. His conclusions have been praised but also criticised. Criticism has mainly focused on:
Ferragina (2012; 2013) integrated the insights of these two criticisms and proposed a cross-regional analysis of 85 European regions, linking together the socio-economic and the historic- institutional analyses to explore the determinants of social capital. He argued that to investigate the determinants of social capital, one has to integrate the synchronic and the diachronic perspectives under the guidance of a methodological framework able to put these two approaches in continuity.
The sleeping social capital theory.
Putnam’s work, nourished by doctrines like the "end of history" (Fukuyama 1992) was largely deterministic, and proposed the dismissal of more articulated historical interpretations. This determinism has reduced Southern Italian history as being a negative path to modernity; only the Italian regions that experienced the development of medieval towns during the twelfth and thirteenth centuries have got high levels of social capital today, the others ‘are condemned’ by the prevalence of the authoritarian rule of the Normans more than 800 years ago.
However, from a purely historical perspective, the medieval town is not unanimously considered to be a symbol of freedom, creation of horizontal ties and embryo of democratic life. In Making Democracy Work, Putnam disregarded the division within municipal towns and their dearth of civic participation and considered only the experience of few areas in North Central Italy, ignoring the existence of important towns in the South.
To this more complicated historical picture, Ferragina (2012) added the result of a regression model, which indicated that social capital in the South of Italy and Wallonia should be much lower than currently detected according to their socio-economic condition. He unfolded Putnam’s theory by undertaking a comparative analysis between these two deviant cases and two regular cases located in the same country, namely Flanders and the North of Italy. The historical legacy does not have a negative effect on the present lack of social capital in Wallonia and the South of Italy, but the potentially positive effect of the historical legacy is currently curtailed by the poor socio-economic conditions, notably by the high level of income inequality and the low level of labour market participation. This historical interpretation is driven by the comparison with Flanders and the North East of Italy.
The value of the historical legacy for present socio-economic development is similar to the ‘appropriable social capital’ theorized by Coleman (1990) at the individual level. Using the example of the Korean students, Coleman argued that the construction of a secret network of people (at a time in which the appreciation for the authoritarian government was rapidly declining among the population) as a means of organizing the democratic revolt was the result of a process of socialization that took place during their childhood (with the involvement in the local churches).
The relation between historical evolutions and the socio-economic variables has similar characteristics at the macro level. Only after reaching a sufficient level of labour market activity and income redistribution (this is comparable to the growing unpopularity of the authoritarian government) can the memory of historical events of social engagement become fully appropriable by the population (this is comparable to the participation in the local churches during childhood), leading to the development of innovative forms of social participation (this is comparable to the construction of the secret circles that enhanced the democratic revolt). This process increases social capital even further if socio-economic development is matched by the revival of the unique historical legacy of the area. The reconstruction of this unique past can rapidly become a source of pride for the entire area, contributing in turn to an increasing intra-regional solidarity, and with it enhancement of social networks and social trust.
The Flemish case (and also to a lesser extent that of the North East of Italy) illustrates this process well. The socio-economic improvements that took place in the nineteenth century were matched by the revival of the glorious Flemish traditions of the thirteenth and fourteenth century. The increase of social capital generated by the reduction of income inequality and the increasing participation in the labour market due to the economic development was multiplied by the reconstruction of Flemish identity and pride. This pride and self- confidence has, in turn, increased the feeling of solidarity within the region and contributed to generate a level of social capital, which is hardly explicable by the single socio-economic predictors.
Ferragina suggests that, in the divergent cases, the value of the historical legacy is affected by the poor present socio- economic conditions. Social capital sleeps, not because of the absence of certain clearly defined historical steps as suggested by Putnam, but because socio- economic underdevelopment profoundly depressed the self-pride of Southern Italians and Walloons.
The biased and simplistic interpretations of Southern Italian and Walloon history will be discarded only when their socio-economic conditions reach a sufficient level, enacting a cycle similar to Flanders and the North East of Italy. Stronger redistribution, an increase of labour market participation accompanied by a simultaneous process of ‘reinvention of the past’ could enhance a positive cycle of social capital increase in both areas. The historical legacy in these two areas should not be seen as the root of the present lack of social capital but as a potential element for improvement. Important moments of social engagement also existed in the history of these two areas; the imagery of Walloons and Southern Italians should be nourished by these almost forgotten examples of collective history (i.e. the Fasci Siciliani in the south of Italy) rather than the prevailing idea that the historical legacy of these areas is simply an original sin, a burden to carry through the process of modernization.
Social capital motives.
Robison and colleagues measured the relative importance of selfishness and four social capital motives using resource allocation data collected in hypothetical surveys and non-hypothetical experiments. The selfishness motive assumes that an agent's allocation of a scarce resource is independent of his relationships with others. This motive is sometimes referred to as the selfishness of preference assumption in neoclassical economics. Social capital motives assume that agents’ allocation of a scarce resource may be influenced by their social capital or sympathetic relationships with others which may produce socio-emotional goods that satisfy socio-emotional needs for validation and belonging. The first social capital motive seeks for validation by acting consistently with the values of one’s ideal self. The second social capital motive seeks to be validated by others by winning their approval. The third social capital motive seeks to belong. Recognizing that one may not be able to influence the sympathy of others, persons seeking to belong may act to increase their own sympathy for others and the organizations or institutions they represent. The fourth social capital motive recognizes that our sympathy or social capital for another person will motivate us to act in their interest. In doing so we satisfy our own needs for validation and belonging. Empirical results reject the hypothesis often implied in economics that we are 95% selfish.
Requiem for a theory? A double-edged sword relationship with neoliberalism.
The social capital concept has influenced academic literature and public debate through the specter of social disintegration: would anybody disagree with the fact that we need healthy communities and civic engagement to protect our democracies? Ferragina and Arrigoni have argued that the popularity of this theory is rooted in the connection made with neoliberalism by James Coleman (1990) and Robert Putnam (1993). They contend that social capital theory has become an analytical tool to avoid the debate on the effects of neoliberal policies on civic engagement (Ferragina and Arrigoni 2016: 9 ).
More specifically, by elaborating the most popular version of social capital theory, Putnam (1993) revitalised Tocqueville’s seminal work on American democracy, showing that ‘the health of liberal democracy’ depends upon social engagement. However, in linking social capital, neoliberalism, and rational choice theory, Putnam did not consider that the intensity of social engagement in a society tends to be strictly related to the level of economic inequality (Ferragina, 2010, 2012) and other structural factors (Costa and Kahn, 2003), such as the universal nature of the welfare state (Rothstein, 2008). Hence, by arguing that the disadvantaged need more social capital to insure themselves against the odds of a competitive world, Putnam implicitly suggests that being powerless is a result of not having enough capital rather than a structural problem of society (Ferragina and Arrigoni 2016). 
However, in a period during which neoliberal governance is showing many drawbacks and the marked incapacity to deliver economic growth (Piketty, 2014), it is possible that to strengthen secondary groups and social engagement, more equality and greater levels of solidarity are needed (as classically argued by Tocqueville, see Ferragina, 2010).
There is a tension between the individualisation of social risks pursued by several political parties and the call to create social capital: it is becoming harder to blame the individual for collective problems. Prior to the start of the economic crisis in 2008, the tension between rising economic inequality and the demand to strengthen civic engagement was undermined by neoliberalism’s capacity to sustain a certain level of economic growth. One might claim this capacity contributed to a transposition of social capital theory within public discourse. The limitations of finance as the central engine of economic growth, the material hardships fostered by the crisis, and the austerity measures implemented by governments in response to these challenges are critically undermining the legitimacy of neoliberal policies (Ferragina and Arrigoni 2016: 10).
Relation with civil society.
A number of authors give definitions of civil society that refer to voluntary associations and organisations outside the market and state. This definition is very close to that of the third sector, which consists of "private organisations that are formed and sustained by groups of people acting voluntarily and without seeking personal profit to provide benefits for themselves or for others". According to such authors as Walzer, Alessandrini, Newtown, Stolle and Rochon, Foley and Edwards, and Walters, it is through civil society, or more accurately, the third sector, that individuals are able to establish and maintain relational networks. These voluntary associations also connect people with each other, build trust and reciprocity through informal, loosely structured associations, and consolidate society through altruism without obligation. It is "this range of activities, services and associations produced by... civil society" that constitutes the sources of social capital.
If civil society, then, is taken to be synonymous with the third sector then the question it seems is not 'how important is social capital to the production of a civil society?' but 'how important is civil society to the production of social capital?'. Not only have the authors above documented how civil society produces sources of social capital, but in Lyons work "Third Sector", social capital does not appear in any guise under either the factors that enable or those that stimulate the growth of the third sector, and Onyx describes how social capital depends on an already functioning community.
The idea that creating social capital (i.e., creating networks) will strengthen civil society underlies current Australian social policy aimed at bridging deepening social divisions. The goal is to reintegrate those marginalised from the rewards of the economic system into "the community". However, according to Onyx (2000), while the explicit aim of this policy is inclusion, its effects are exclusionary.
Foley and Edwards believe that "political systems... are important determinants of both the character of civil society and of the uses to which whatever social capital exists might be put". Alessandrini agrees, saying, "in Australia in particular, neo-liberalism has been recast as economic rationalism and identified by several theorists and commentators as a danger to society at large because of the use to which they are putting social capital to work".
The resurgence of interest in social capital as a remedy for the cause of today’s social problems draws directly on the assumption that these problems lie in the weakening of civil society. However this ignores the arguments of many theorists who believe that social capital leads to exclusion rather than to a stronger civil society. In international development, Ben Fine and John Harriss have been heavily critical of the inappropriate adoption of social capital as a supposed panacea (promoting civil society organisations and NGOs, for example, as agents of development) for the inequalities generated by neo liberal economic development. This leads to controversy as to the role of state institutions in the promotion of social capital.
An abundance of social capital is seen as being almost a necessary condition for modern liberal democracy. A low level of social capital leads to an excessively rigid and unresponsive political system and high levels of corruption, in the political system and in the region as a whole. Formal public institutions require social capital in order to function properly, and while it is possible to have too much social capital (resulting in rapid changes and excessive regulation), it is decidedly worse to have too little.
Kathleen Dowley and Brian Silver published an article entitled "Social Capital, Ethnicity and Support for Democracy in the Post-Communist States". This article found that in post-communist states, higher levels of social capital did not equate to higher levels of democracy. However, higher levels of social capital led to higher support for democracy.
A number of intellectuals in developing countries have argued that the idea of social capital, particularly when connected to certain ideas about civil society, is deeply implicated in contemporary modes of donor and NGO driven imperialism and that it functions, primarily, to blame the poor for their condition.
The concept of social capital in a Chinese social context has been closely linked with the concept of "guanxi".
An interesting attempt to measure social capital spearheaded by Corporate Alliance in the English speaking market segment of the United States of America and Xentrum through the Latin American Chamber of Commerce in Utah on the Spanish speaking population of the same country, involves the quantity, quality and strength of an individual social capital. With the assistance of software applications and web-based relationship-oriented systems such as LinkedIn, these kinds of organizations are expected to provide its members with a way to keep track of the "number" of their relationships, meetings designed to boost the "strength" of each relationship using group dynamics, executive retreats and networking events as well as training in how to reach out to higher circles of "influential" people.
Social capital and women's engagement with politics.
There are many factors that drive volume towards the ballot box, including education, employment, civil skills, and time. Careful evaluation of these fundamental factors often suggests that women do not vote at similar levels as men. However the gap between women and men voter turnout is diminishing and in some cases women are becoming more prevalent at the ballot box than their male counterparts. Recent research on social capital is now serving as an explanation for this change.
Social capital offers a wealth of resources and networks that facilitate political engagement. Since social capital is readily available no matter the type of community, it is able to override more traditional queues for political engagement; e.g.: education, employment, civil skills, etc.
There are unique ways in which women organize. These differences from men make social capital more personable and impressionable to women audiences thus creating a stronger presence in regards to political engagement. A few examples of these characteristics are:
The often informal nature of female social capital allows women to politicize apolitical environments without conforming to masculine standards, thus keeping this activity at a low public profile. These differences are hard to recognize within the discourse of political engagement and may explain why social capital has not been considered as a tool for female political engagement until as of late.
Effects on health.
A growing body of research has found that the presence of social capital through social networks and communities has a protective quality on health. Social capital affects health risk behavior in the sense that individuals who are embedded in a network or community rich in support, social trust, information, and norms, have resources that help achieve health goals. For example, a person who is sick with cancer may receive information, money, or moral support he or she needs to endure treatment and recover. Social capital also encourages social trust and membership. These factors can discourage individuals from engaging in risky health behaviors such as smoking and binge drinking. Furthermore, neighbourhood social capital may also aid in buffering health inequities amongst children and adolescents.
Inversely, a lack of social capital can impair health. For example, results from a survey given to 13- to 18-year-old students in Sweden showed that low social capital and low social trust are associated with higher rates of psychosomatic symptoms, musculoskeletal pain, and depression. Additionally, negative social capital can detract from health. Although there are only a few studies that assess social capital in criminalized populations, there is information that suggests that social capital does have a negative effect in broken communities. Deviant behavior is encouraged by deviant peers via favorable definitions and learning opportunities provided by network-based norms. However, in these same communities, an adjustment of norms (i.e. deviant peers being replaced by positive role models) can pose a positive effect.
Effects of the Internet.
Similar to watching the news and keeping abreast of current events, the use of the Internet can relate to an individual's level of social capital. In one study, informational uses of the Internet correlated positively with an individual's production of social capital, and social-recreational uses were negatively correlated (higher levels of these uses correlated with lower levels of social capital). An example supporting the former argument is the contribution of Peter Maranci's blog (Charlie on the Commuter Line) to address the train problems in Massachusetts. He created it after an incident where a lady passed out during a train ride due to the congestion in the train and help was delayed because of the congestion in the train and the inefficiency of the train conductor. His blog exposed the poor conditions of train stations, overcrowding train rides and inefficiency of the train conductor which eventually influenced changes within the transit system. Another perspective holds that the rapid growth of social networking sites such as Facebook and Myspace suggests that individuals are creating a virtual-network consisting of both bonding and bridging social capital. Unlike face to face interaction, people can instantly connect with others in a targeted fashion by placing specific parameters with internet use. This means that individuals can selectively connect with others based on ascertained interests, and backgrounds. Facebook is currently the most popular social networking site and touts many advantages to its users including serving as a "social lubricant" for individuals who otherwise have difficulties forming and maintaining both strong and weak ties with others.
This argument continues, although the preponderance of evidence shows a positive association between social capital and the internet. Critics of virtual communities believe that the Internet replaces our strong bonds with online "weak-ties" or with socially empty interactions with the technology itself. Others fear that the Internet can create a world of "narcissism of similarity," where sociability is reduced to interactions between those that are similar in terms of ideology, race, or gender. A few articles suggest that technologically based interactions has a negative relationship with social capital by displacing time spent engaging in geographical/ in-person social activities. However, the consensus of research shows that the more time people spend online the more in-person contact they have, thus positively enhancing social capital.
Recent research, conducted 2006, also shows that Internet users often have wider networks than those who uses internet irregularly or not at all. When not considering family and work contacts, Internet users actually tend to have contact with a higher number of friends and relatives. This is supported by another study that shows that internet users and non-internet users do feel equally close to the same number of people; also the internet users maintain relationships with 20% more people that they “feel somewhat close” to.
Other research shows that younger people use the Internet as a supplemental medium for communication, rather than letting the Internet communication replace face-to-face contact. This supports the view that Internet communication does not hinder development of social capital and does not make people feel lonelier than before.
Ellison, Steinfield & Lampe (2007) suggest social capital exercised online is a result of relationships formed offline; whereby, bridging capital is enabled through a "maintenance" of relationships. Among respondents of this study, social capital built exclusively online creates weaker ties. A distinction of social bonding is offered by Ellison et al., 2007, suggesting bonds, or strong ties, are possible through social media, but less likely.
Effects on educational achievement.
Coleman and Hoffer collected quantitative data of 28,000 students in total 1,015 public, Catholic and other private high schools in America from the 7 years' period from 1980 to 1987. It was found from this longitudinal research that social capital in students' families and communities attributed to the much lower dropout rates in Catholic schools compared with the higher rates in public.
Teachman et al. further develop the family structure indicator suggested by Coleman. They criticise Coleman, who used only the number of parents present in the family, neglected the unseen effect of more discrete dimensions such as stepparents' and different types of single-parent families. They take into account of a detailed counting of family structure, not only with two biological parents or stepparent families, but also with types of single-parent families with each other (mother-only, father-only, never-married, and other). They also contribute to the literature by measuring parent-child interaction by the indicators of how often parents and children discuss school-related activities.
Morgan and Sorensen directly challenge Coleman for his lacking of an explicit mechanism to explain why Catholic schools students perform better than public school students on standardised tests of achievement. Researching students in Catholic schools and public schools again, they propose two comparable models of social capital effect on mathematic learning. One is on Catholic schools as norm-enforcing schools whereas another is on public schools as horizon-expanding schools. It is found that while social capital can bring about positive effect of maintaining an encompassing functional community in norm-enforcing schools, it also brings about the negative consequence of excessive monitoring. Creativity and exceptional achievement would be repressed as a result. Whereas in horizon expanding school, social closure is found to be negative for student's mathematic achievement. These schools explore a different type of social capital, such as information about opportunities in the extended social networks of parents and other adults. The consequence is that more learning is fostered than norm-enforcing Catholic school students. In sum, Morgan and Sorensen's (1999) study implies that social capital is contextualised, one kind of social capital may be positive in this setting but is not necessarily still positive in another setting.
In the setting of education through Kilpatrick et al., (2010) state, ‘... social capital is a useful lens for analysing lifelong learning and its relationship to community development’. Social capital is particularly important in terms of education. Also the importance of education with ‘...schools being designed to create “functioning community”- forging tighter links between parents and the school’ (Coleman &Hoffer, 1987) linking that without this interaction, the social capital in this area is disadvantaged and demonstrates that social capital plays a major role in education.
Without social capital in the area of education, teachers and parents that play a responsibility in a students learning, the significant impacts on their child’s academic learning can rely on these factors. With focus on parents contributing to their child’s academic progress as well as being influenced by social capital in education. Without the contribution by the parent in their child’s education, gives parents less opportunity and participation in the student’s life. As Tedin et al. (2010) state ‘...one of the most important factors in promoting student success is the active involvement of parents in a child’s education.’’ With parents also involved in activities and meetings the school conducts, the more involved parents are with other parents and the staff members. Thus parent involvement contributes to social capital with becoming more involved in the school community and participating makes the school a sustainable and easy to run community.
In their journal article "Beyond social capital: Spatial dynamics of collective efficacy for children", Sampson et al. stress the normative or goal-directed dimension of social capital. They claim, "resources or networks alone (e.g. voluntary associations, friendship ties, organisational density) are neutral--- they may or may not be effective mechanism for achieving intended effect"
Marjoribanks and Kwok conducted a survey in Hong Kong secondary schools with 387 fourteen-year-old students with an aim to analyse female and male adolescents differential educational achievement by using social capital as the main analytic tool. In that research, social capital is approved of its different effects upon different genders. In his thesis "New Arrival Students in Hong Kong: Adaptation and School Performance", Hei Hang Hayes Tang argues that adaptation is a process of activation and accumulation of (cultural and social) capitals. The research findings show that supportive networks is the key determinant differentiating the divergent adaptation pathways. Supportive networks, as a form of social capital, is necessary for activating the cultural capital the newly arrived students possessed. The amount of accumulated capital is also relevant to further advancement in the ongoing adaptation process.
Min Zhou and Carl L. Bankston in their study of a Vietnamese community in New Orleans find that preserving traditional ethnic values enable immigrants to integrate socially and to maintain solidarity in an ethnic community. Ethnic solidarity is especially important in the context where immigrants just arrive in the host society. In her article "Social Capital in Chinatown", Zhou examines how the process of adaptation of young Chinese Americans is affected by tangible forms of social relations between the community, immigrant families, and the younger generations. Chinatown serves as the basis of social capital that facilitates the accommodation of immigrant children in the expected directions. Ethnic support provides impetus to academic success. Furthermore, maintenance of literacy in native language also provides a form of social capital that contributes positively to academic achievement. Stanton-Salazar and Dornbusch found that bilingual students were more likely to obtain the necessary forms of institutional support to advance their school performance and their life chances.
Putnam (2000) mentions in his book "Bowling Alone", "Child development is powerfully shaped by social capital" and continues "presence of social capital has been linked to various positive outcomes, particularly in education". According to his book, these positive outcomes are the result of parents' social capital in a community. In states where there is a high social capital, there is also a high education performance. The similarity of these states is that parents were more associated with their children's education. Teachers have reported that when the parents participate more in their children's education and school life, it lowers levels of misbehavior, such as bringing weapons to school, engaging in physical violence, unauthorized absence, and being generally apathetic about education. Borrowing Coleman's quotation from Putnam's book, Coleman once mentioned we cannot understate "the importance of the embeddedness of young persons in the enclaves of adults most proximate to them, first and most prominent the family and second, a surrounding community of adults".
In geography.
In order to understand social capital as a subject in geography, one must look at it in a sense of space, place, and territory. In its relationship, the tenets of geography relate to the ideas of social capital in the family, community, and in the use of social networks. The biggest advocate for seeing social capital as a geographical subject was American economist and political scientist Robert Putnam. His main argument for classifying social capital as a geographical concept is that the relationships of people is shaped and molded by the areas in which they live.
Putnam (1993) argued that the lack of social capital in the South of Italy was more the product of a peculiar historical and geographical development than the consequence of a set of contemporary socio-economic conditions. This idea has sparked a lengthy debate and received fierce criticism (Ferragina, 2010; Ferragina 2012: 3). There are many areas in which social capital can be defined by the theories and practices. Anthony Giddens developed a theory in 1984 in which he relates social structures and the actions that they produce. In his studies, he does not look at the individual participants of these structures, but how the structures and the social connections that stem from them are diffused over space. If this is the case, the continuous change in social structures could bring about a change in social capital, which can cause changes in community atmosphere. If an area is plagued by social organizations whose goals are to revolt against social norms, such as gangs, it can cause a negative social capital for the area causing those who disagreed with said organizations to relocate thus taking their positive social capital to a different space than the negative.
Another area where social capital can be seen as an area of study in geography is through the analysis of participation in volunteerism and its support of different governments. One area to look into with this is through those who participate in social organizations. People that participate are of different races, ages, and economic status. With these in mind, variances of the space in which these different demographics may vary, causing a difference in involvement among areas. Secondly, there are different social programs for different areas based on economic situation. A governmental organization would not place a welfare center in a wealthier neighborhood where it would have very limited support to the community, as it is not needed. Thirdly, social capital can be affected by the participation of individuals of a certain area based on the type of institutions that are placed there. Mohan supports this with the argument of J. Fox in his paper "Decentralization and Rural Development in Mexico", which states “structures of local governance in turn influence the capacity of grassroots communities to influence social investments." With this theory, if the involvement of a government in specific areas raises the involvement of individuals in social organizations and/or communities, this will in turn raise the social capital for that area. Since every area is different, the government takes that into consideration and will provide different areas with different institutions to fit their needs thus there will be different changes in social capital in different areas.
In leisure studies.
In the context of leisure studies, social capital is seen as the consequence of investment in and cultivation of social relationships allowing an individual access to resources that would otherwise be unavailable to him or her. The concept of social capital in relation to leisure is grounded in a perspective that emphasizes the interconnectedness rather than the separateness of human activity and human goals. There is a significant connection between leisure and democratic social capital. Specific forms of leisure activity contribute to the development of the social capital central to democracy and democratic citizenship. The more an individual participates in social activities, the more autonomy the individual experiences, which will help her or his individual abilities and skills to develop. The greater the accumulation of social capital a person experiences, may transfer to other leisure activities as well as personal social roles, relationships and in other roles within a social structure.
Negative social capital.
It has been noted that social capital may be not always be used for positive ends. An example of the complexities of the effects of social capital is violent or criminal gang activity that is encouraged through the strengthening of intra-group relationships (bonding social capital). The negative consequences of social capital are more often associated with "bonding" vis-à-vis "bridging".
Without "bridging" social capital, "bonding" groups can become isolated and disenfranchised from the rest of society and, most importantly, from groups with which bridging must occur in order to denote an "increase" in social capital. Bonding social capital is a necessary antecedent for the development of the more powerful form of bridging social capital. Bonding and bridging social capital can work together productively if in balance, or they may work against each other. As social capital bonds and stronger homogeneous groups form, the likelihood of bridging social capital is attenuated. Bonding social capital can also perpetuate sentiments of a certain group, allowing for the bonding of certain individuals together upon a common radical ideal. The strengthening of insular ties can lead to a variety of effects such as ethnic marginalization or social isolation. In extreme cases ethnic cleansing may result if the relationship between different groups is so strongly negative. In mild cases, it just isolates certain communities such as suburbs of cities because of the bonding social capital and the fact that people in these communities spend so much time away from places that build bridging social capital.
Social capital (in the institutional Robert Putnam sense) may also lead to bad outcomes if the political institution and democracy in a specific country is not strong enough and is therefore overpowered by the social capital groups. "Civil society and the collapse of the Weimar Republic" suggests that "it was weak political institutionalization rather than a weak civil society that was Germany’s main problem during the Wihelmine and Weimar eras." Because the political institutions were so weak people looked to other outlets. “Germans threw themselves into their clubs, voluntary associations, and professional organizations out of frustration with the failures of the national government and political parties, thereby helping to undermine the Weimar Republic and facilitate Hitler’s rise to power.” In this article about the fall of the Weimar Republic, the author makes the claim that Hitler rose to power so quickly because he was able to mobilize the groups towards one common goal. Even though German society was, at the time, a "joining" society these groups were fragmented and their members did not use the skills they learned in their club associations to better their society. They were very introverted in the Weimar Republic. Hitler was able to capitalize on this by uniting these highly bonded groups under the common cause of bringing Germany to the top of world politics. The former world order had been destroyed during World War I, and Hitler believed that Germany had the right and the will to become a dominant global power. Additionally, in his essay "A Criticism of Putnam’s Theory of Social Capital", Michael Shindler expands upon Berman's argument that Wiemar social clubs and similar associations in countries that did not develop democracy, were organized in such a way that they fostered a "we" instead of an "I" mentality among their members, by arguing that groups which possess cultures that stress solidarity over individuality, even ones that are "horizontally" structured and which were also common to pre-soviet eastern europe, will not engender democracy if they are politically aligned with non-democratic ideologies.
Later work by Putnam also suggests that social capital, and the associated growth of public trust are inhibited by immigration and rising racial diversity in communities. Putnam's study regarding the issue argued that in American areas with a lack of homogeneity, some individuals neither participated in bonding nor bridging social capital. In societies where immigration is high (USA) or where ethnic heterogeneity is high (Eastern Europe), it was found that citizens lacked in both kinds of social capital and were overall far less trusting of others than members of homogenous communities were found to be. Lack of homogeneity led to people withdrawing from even their closest groups and relationships, creating an atomized society as opposed to a cohesive community. These findings challenge previous beliefs that exposure to diversity strengthens social capital, either through bridging social gaps between ethnicities or strengthening in-group bonds. It is very important for policy makers to monitor the level of perceived socio-economic threat from immigrants because negative attitudes towards immigrants make integration difficult and affect social capital.
Social capital and reproduction of inequality.
James Coleman has indicated that social capital eventually led to the creation of human capital for the future generation. Human capital, a private resource, could be accessed through what the previous generation accumulated through social capital. Field suggested that such a process could lead to the very inequality social capital attempts to resolve. While Coleman viewed social capital as a relatively neutral resource, he did not deny the class reproduction that could result from accessing such capital, given that individuals worked toward their own benefit. Even though Coleman never truly addresses Bourdieu in his discussion, this coincides with Bourdieu's argument set forth in "Reproduction in Education, Society and Culture". Bourdieu and Coleman were fundamentally different at the theoretical level (as Bourdieu believed the actions of individuals were rarely ever conscious, but more so only a result of their habitus (see below) being enacted within a particular field, but this realization by both seems to undeniably connect their understanding of the more latent aspects of social capital.
According to Bourdieu, habitus refers to the social context within which a social actor is socialized. Thus, it is the social platform, itself, that equips one with the social reality they become accustomed to. Out of habitus comes field, the manner in which one integrates and displays his or her habitus. To this end, it is the social exchange and interaction between two or more social actors. To illustrate this, we assume that an individual wishes to better his place in society. He therefore accumulates social capital by involving himself in a social network, adhering to the norms of that group, allowing him to later access the resources (e.g. social relationships) gained over time. If, in the case of education, he uses these resources to better his educational outcomes, thereby enabling him to become socially mobile, he effectively has worked to reiterate and reproduce the stratification of society, as social capital has done little to alleviate the system as a whole. This may be one negative aspect of social capital, but seems to be an inevitable one in and of itself, as are all forms of capital.

</doc>
<doc id="45803" url="https://en.wikipedia.org/wiki?curid=45803" title="Individual capital">
Individual capital

Individual capital, the economic view of talent, comprises inalienable or personal traits of persons, tied to their bodies and available only through their own free will, such as skill, creativity, enterprise, courage, capacity for moral example, non-communicable wisdom, invention or empathy, non-transferable personal trust and leadership.
As recognized in theories of economics.
Individual talent & initiative was recognized as an intangible quality of persons in economics back to at least Adam Smith. He distinguished it (as "enterprise") from labour which can be coerced and is usually seen as strictly imitative (learned or transmitted, via such means as apprenticeship).
Marxist economics refers instead to "an individual's social capital - individuals are sources neither of creativity and innovation, nor management skill. A problem with that analysis is that it simply cannot explain the substitution problem and lack of demand that occurs when, for instance, an understudy takes on a leading role, or a second author takes over writing a popular book series. At the very least there must be some conditional, if not firm-specific then "class specific", special ability to command premiums for outstanding personal performance.
Neoclassical economics by contrast refers to "the individual in whom the human capital is ... embedded", which implies a strong association of the individual with the instructional capital they learn from, with little or no social capital influence. This is orthogonal to the Marxist view, but not necessarily opposed.
Human development theory reflects both distinctions: it sees labour as the yield of individual capital in the same way that neoclassical macro-economics sees financial capital as the yield of the looser idea of human capital. But the rest problem and social welfare function selection, as well as the subjective factors in behavioral finance, has led to a closer analysis of factors of production. In effect, the financial architecture is no longer trusted as an arbiter of the value of life as it was in neoclassical economics. Money is not seen as values-neutral, but as embodying a set of larger social choices about money supply rules, made by measuring well-being of whole populations.
Versus "human", "firm-specific", "individual social".
While conflated in many analyses with human capital, the latter term includes social capital (human relationships) and instructional capital (abstract texts and training materials and so on) that are not tied to any one person, do not die with them or leave employment with them, and therefore cannot be equated with talent alone. In intangibles measurement, value creation and value reporting metrics require all assets with such different characteristics to be categorized as different capital assets, so the more exact reference to the individual person is preferred.
Fusions of terminology are common. Sociological analysts refer to "individual-level elements of social capital" or "an individual's social capital" or just "individual social capital" while economic analysts often use the phrase firm-specific human capital. In either case the clearly includes individual capital but also some "activity-", "community-" or "firm-specific" social capital (community trust) and instructional capital (shareable knowledge or skills). This is easy to measure: its yield is your salary in your current job.
To the degree this is consistent if you take other work nearby, this opens the questions of what is "not" "firm-specific" and whether a nation is just a bigger "firm": Some analyses see political capital, or just "influence" or "trust of professionals" as a full style of capital of its own. Some ethicists, most clearly Jane Jacobs, see this as simple corruption. Nonetheless, corruption clearly has a cash value, involves some creativity to arrange, and is a decision factor. It is a skill like any other.
Versus "intellectual capital".
Perhaps because of this, not all theorists recognize individual capital as being as essential as labour, or distinct from social or political influence, or from instructional capacity. These theorists often refer to "intellectual capital", which more properly describes a debate or locus of complexity that arises when individuals take key instructional roles. Some refer to celebrity as another fusion, when individuals take key social roles.
However, a great many celebrities are clearly not "intellectual" achievers nor notable for any cognitive or analytic powers, e.g. Kim Kardashian, professional sports figures or other athletes. While they may through sheer exposure become involved in causes or controversies (as Paris Hilton did in the US presidential election, 2008) it's clearly not correct to label all individually unique talent or economic value as being an "intellectual" asset.
This failure to distinguish individual's objectively observed economic value (the power to promote or publicize products, draw attention to causes, etc.) from the "intellectual" powers is probably an elitist bias. Clearly, there are some individuals, including non-humans such as a racehorse, which have economic value unique to their individual body and being that cannot be captured or defined as an "intellectual" asset nor as a set of "social" relationships (because horses do not socialize in the sense humans do). Where slavery exists or has existed, there is clearly a value put on living bodies separate from their instructional or social selves.
Thus for analyzing historical or criminal economic activities, or even professional sports, the instructional capital vs. individual capital vs. social capital distinction is essential.
Investment.
Those who differentiate individual capital tend to see it as something that one can invest in, directly, and see growth, directly. For individual skill, even skill at a highly imitative enterprise, like sports or mastery of a musical instrument, this is very often quite measurable. Many enterprises, for instance, a music conservatory or circus school or creative writing coach, are clearly making a living on the identification and (somewhat) measurable enhancement of the individual.

</doc>
<doc id="45804" url="https://en.wikipedia.org/wiki?curid=45804" title="Human capital">
Human capital

Human capital is the stock of knowledge, habits, social and personality attributes, including creativity, embodied in the ability to perform labor so as to produce economic value.
Alternatively, "Human capital" is a collection of resources—all the knowledge, talents, skills, abilities, experience, intelligence, training, judgment, and wisdom possessed individually and collectively by individuals in a population. These resources are the total capacity of the people that represents a form of wealth which can be directed to accomplish the goals of the nation or state or a portion thereof.
It is an aggregate economic view of the human being acting within economies, which is an attempt to capture the social, biological, cultural and psychological complexity as they interact in explicit and/or economic transactions. Many theories explicitly connect investment in human capital development to education, and the role of human capital in economic development, productivity growth, and innovation has frequently been cited as a justification for government subsidies for education and job skills training.
"Human capital" has been and continues to be criticized in numerous ways. Michael Spence offers signaling theory as an alternative to human capital. Pierre Bourdieu offers a nuanced conceptual alternative to human capital that includes cultural capital, social capital, economic capital, and symbolic capital. These critiques, and other debates, suggest that "human capital" is a reified concept without sufficient explanatory power.
It was assumed in early economic theories, reflecting the context, i.e., the secondary sector of the economy was producing much more than the tertiary sector was able to produce at the time in most countries – to be a fungible resource, homogeneous, and easily interchangeable, and it was referred to simply as workforce or labor, one of three factors of production (the others being land, and assumed-interchangeable assets of money and physical equipment). Just as land became recognized as natural capital and an asset in itself, human factors of production were raised from this simple mechanistic analysis to human capital. In modern technical financial analysis, the term "balanced growth" refers to the goal of equal growth of both aggregate human capabilities and physical assets that produce goods and services.
The assumption that labour or workforces could be easily modelled in aggregate began to be challenged in 1950s when the tertiary sector, which demanded creativity, begun to produce more than the secondary sector was producing at the time in the most developed countries in the world. Accordingly, much more attention was paid to factors that led to success versus failure where human management was concerned. The role of leadership, talent, even celebrity was explored.
Today, most theories attempt to break down human capital into one or more components for analysis  – usually called "intangibles". Most commonly, social capital, the sum of social bonds and relationships, has come to be recognized, along with many synonyms such as goodwill or brand value or social cohesion or social resilience and related concepts like celebrity or fame, as distinct from the talent that an individual (such as an athlete has uniquely) has developed that cannot be passed on to others regardless of effort, and those aspects that can be transferred or taught: instructional capital. Less commonly, some analyses conflate good instructions for health with health itself, or good knowledge management habits or systems with the instructions they compile and manage, or the "intellectual capital" of teams – a reflection of their social and instructional capacities, with some assumptions about their individual uniqueness in the context in which they work. In general these analyses acknowledge that individual trained bodies, teachable ideas or skills, and social influence or persuasion power, are different.
Management accounting is often concerned with questions of how to model human beings as a capital asset. However it is broken down or defined,
human capital is vitally important for an organization's success (Crook et al., 2011); human capital increases through education and experience. Human capital is also important for the success of cities and regions: a 2012 study examined how the production of university degrees and R&D activities of educational institutions are related to the human capital of metropolitan areas in which they are located.
In 2010, the OECD (the Organization of Economic Co-operation and Development) encouraged the governments of advanced economies to embrace policies to increase innovation and knowledge in products and services as an economical path to continued prosperity. International policies also often address human capital flight, which is the loss of talented or trained persons from a country that invested in them, to another country which benefits from their arrival without investing in them.
Studies of structural unemployment have increasingly focused on a mismatch between the stock of job-specific human capital and the needs of employers. In other words, there is increasingly a recognition that human capital may be specific to particular jobs or tasks and not general and readily transferable. Recent work has attempted to improve the linkages between education and the needs of the labor market by linking labor market data to education loan pricing.
Background.
Adam Smith defined four types of fixed capital (which is characterized as that which affords a revenue or profit without circulating or changing masters). The four types were:
Smith defined human capital as follows:
“Fourthly, of the acquired and useful abilities of all the inhabitants or members of the society. The acquisition of such talents, by the maintenance of the acquirer during his education, study, or apprenticeship, always costs a real expense, which is a capital fixed and realized, as it were, in his person. Those talents, as they make a part of his fortune, so do they likewise that of the society to which he belongs. The improved dexterity of a workman may be considered in the same light as a machine or instrument of trade which facilitates and abridges labor, and which, though it costs a certain expense, repays that expense with a profit.”.
Therefore, Smith argued, the productive power of labor are both dependent on the division of labor:
The greatest improvement in the productive powers of labour, and the greater part of the skill, dexterity, and judgement with which it is any where directed, or applied, seem to have been the effects of the division of labour.
There is a complex relationship between the division of labor and human capital.
Etymology.
Arthur Lewis is said to have begun the field of development economics and consequently the idea of human capital when he wrote in 1954 "Economic Development with Unlimited Supplies of Labour." The term "human capital" was not used due to its negative undertones until it was first discussed by Arthur Cecil Pigou: <br><br>""There is such a thing as investment in human capital as well as investment in material capital. So soon as this is recognised, the distinction between economy in consumption and economy in investment becomes blurred. For, up to a point, consumption is investment in personal productive capacity. This is especially important in connection with children: to reduce unduly expenditure on their consumption may greatly lower their efficiency in after-life. Even for adults, after we have descended a certain distance along the scale of wealth, so that we are beyond the region of luxuries and "unnecessary" comforts, a check to personal consumption is also a check to investment.""
The use of the term in the modern neoclassical economic literature dates back to Jacob Mincer's article "Investment in Human Capital and Personal Income Distribution" in the "Journal of Political Economy" in 1958. Then Theodore Schultz who is also contributed to the development of the subject matter. The best-known application of the idea of "human capital" in economics is that of Mincer and Gary Becker of the "Chicago School" of economics. Becker's book entitled "Human Capital", published in 1964, became a standard reference for many years. In this view, human capital is similar to "physical means of production", e.g., factories and machines: one can invest in human capital (via education, training, medical treatment) and one's outputs depend partly on the rate of return on the human capital one owns. Thus, human capital is a means of production, into which additional investment yields additional output. Human capital is substitutable, but not transferable like land, labor, or fixed capital.
Some contemporary growth theories sees human capital as an important economic growth factor. Further research shows the relevance of education for the economic welfare of people.
Competence and capital.
The introduction is explained and justified by the unique characteristics of competence (often used only knowledge). Unlike physical labor (and the other factors of production), competence is:
Competence, ability, skills or knowledge?
Often the term "knowledge" is used. "Competence" is broader and includes cognitive ability ("intelligence") and further abilities like motoric and artistic abilities. "Skill" stands for narrow, domain-specific ability. The broader terms "competence" and "ability" are interchangeable.
Knowledge equity (= knowledge capital – knowledge liability) plus emotional equity (= emotional capital – emotional liability) equals goodwill or immaterial/intangible value of the company.
Intangible value of the company (goodwill) plus (material) equity equals the total value of the company.
Marxist analysis.
In some way, the idea of "human capital" is similar to Karl Marx's concept of labor power: he thought in capitalism workers sold their labor power in order to receive income (wages and salaries). But long before Mincer or Becker wrote, Marx pointed to "two disagreeably frustrating facts" with theories that equate wages or salaries with the interest on human capital.
An employer must be receiving a profit from his operations, so that workers must be producing what Marx (under the labor theory of value) perceived as surplus-value, i.e., doing work beyond that necessary to maintain their labor power. Though having "human capital" gives workers some benefits, they are still dependent on the owners of non-human wealth for their livelihood.
The term appears in Marx's article in the "New-York Daily Tribune" article "The Emancipation Question," January 17 and 22, 1859, although there the term is used to describe humans who act like a capital to the producers, rather than in the modern sense of "knowledge capital" endowed to or acquired by humans.
Neo-Marxist economists such as Bowles have argued that education does not lead to higher wages by increasing human capital, but rather by making workers more compliant and reliable in a corporate environment.
Importance.
The concept of Human capital has relatively more importance in labour-surplus countries. These countries are naturally endowed with more of labour due to high birth rate under the given climatic conditions. The surplus labour in these countries is the human resource available in more abundance than the tangible capital resource. This human resource can be transformed into Human capital with effective inputs of education, health and moral values. The transformation of raw human resource into highly productive human resource with these inputs is the process of human capital formation. The problem of scarcity of tangible capital in the labour surplus countries can be resolved by accelerating the rate of human capital formation with both private and public investment in education and health sectors of their National economies. The tangible financial capital is an effective instrument of promoting economic growth of the nation. The intangible human capital, on the other hand, is an instrument of promoting comprehensive development of the nation because human capital is directly related to human development, and when there is human development, the qualitative and quantitative progress of the nation is inevitable. This importance of human capital is explicit in the changed approach of United Nations towards comparative evaluation of economic development of different nations in the World economy. United Nations publishes Human Development Report on human development in different nations with the objective of evaluating the rate of human capital formation in these nations. The statistical indicator of estimating Human Development in each nation is Human Development Index (HDI). It is the combination of "Life Expectancy Index", "Education Index" and "Income Index". The Life expectancy index reveals the standard of health of the population in the country; education index reveals the educational standard and the literacy ratio of the population; and the income index reveals the standard of living of the population. If all these indices have the rising trend over a long period of time, it is reflected into rising trend in HDI. The Human Capital is developed by health, education and quality of Standard of living. Therefore, the components of HDI viz, Life Expectancy Index, Education Index and Income Index are directly related to Human Capital formation within the nation. HDI is indicator of positive correlation between human capital formation and economic development. If HDI increases, there is higher rate of human capital formation in response to higher standard of education and health. Similarly, if HDI increases, per capita income of the nation also increases. Implicitly, HDI reveals that higher the human capital formation due to good standard of health and education, higher is the per capita income of the nation. This process of human development is the strong foundation of a continuous process of economic development of the nation for a long period of time. This significance of the concept of Human capital in generating long-term economic development of the nation cannot be neglected. It is expected that the Macroeconomic policies of all the nations are focussed towards promotion of human development and subsequently economic development. Human Capital is the backbone of Human Development and economic development in every nation. Mahroum (2007) suggested that at the macro-level, human capital management is about three key capacities, the capacity to develop talent, the capacity to deploy talent, and the capacity to draw talent from elsewhere. Collectively, these three capacities form the backbone of any country's human capital competitiveness. Recent U.S. research shows that geographic regions that invest in the human capital and economic advancement of immigrants who are already living in their jurisdictions help boost their short- and long-term economic growth. There is also strong evidence that organizations that possess and cultivate their human capital outperform other organizations lacking human capital (Crook, Todd, Combs, Woehr, and Ketchen, 2011).
Cumulative growth.
Human capital is distinctly different from the tangible monetary capital due to the extraordinary characteristic of human capital to grow cumulatively over a long period of time. The growth of tangible monetary capital is not always linear due to the shocks of business cycles. During the period of prosperity, monetary capital grows at relatively higher rate while during the period of recession and depression, there is deceleration of monetary capital. On the other hand, human capital has uniformly rising rate of growth over a long period of time because the foundation of this human capital is laid down by the educational and health inputs. The current generation is qualitatively developed by the effective inputs of education and health. The future generation is more benefited by the advanced research in the field of education and health, undertaken by the current generation. Therefore, the educational and health inputs create more productive impacts upon the future generation and the future generation becomes superior to the current generation. In other words, the productive capacity of future generation increases more than that of current generation. Therefore, rate of human capital formation in the future generation happens to be more than the rate of human capital formation in the current generation. This is the cumulative growth of human capital formation generated by superior quality of manpower in the succeeding generation as compared to the preceding generation.
India.
In India, rate of human capital formation has consistently increased after Independence due to qualitative improvement in each generation. In the second decade of 21st century, the third generation of India's population is active in the workforce of India. This third generation is qualitatively most superior human resource in India. It has developed the service sector of India with the export of financial services, software services, tourism services and improved the Invisible balance of India's Balance of payments. The rapid growth of Indian economy in response to improvement in the service sector is an evidence of cumulative growth of Human Capital in India.
Criticism.
Some labor economists have criticized the Chicago-school theory, claiming that it tries to explain all differences in wages and salaries in terms of human capital. One of the leading alternatives, advanced by Michael Spence and Joseph Stiglitz, is "Signaling theory". According to signaling theory, education does not lead to increased human capital, but rather acts as a mechanism by which workers with superior innate abilities can signal those abilities to prospective employers and so gain above average wages.
The concept of human capital can be infinitely elastic, including unmeasurable variables such as personal character or connections with insiders (via family or fraternity). This theory has had a significant share of study in the field proving that wages can be higher for employees on aspects other than human capital. Some variables that have been identified in the literature of the past few decades include, gender and nativity wage differentials, discrimination in the work place, and socioeconomic status.
The prestige of a credential may be as important as the knowledge gained in determining the value of an education. This points to the existence of market imperfections such as non-competing groups and labor-market segmentation. In segmented labor markets, the "return on human capital" differs between comparably skilled labor-market groups or segments. An example of this is discrimination against minority or female employees.
Following Becker, the human capital literature often distinguishes between "specific" and "general" human capital. Specific human capital refers to skills or knowledge that is useful only to a single employer or industry, whereas general human capital (such as literacy) is useful to all employers. Economists view firm specific human capital as risky, since firm closure or industry decline lead to skills that cannot be transferred (the evidence on the quantitative importance of firm specific capital is unresolved).
Human capital is central to debates about welfare, education, health care, and retirement..
In 2004, "human capital" () was named the German Un-Word of the Year by a jury of linguistic scholars, who considered the term inappropriate and inhumane, as individuals would be degraded and their abilities classified according to economically relevant quantities.
"Human capital" is often confused with human development. The UN suggests "Human development denotes both the process of widening people's choices and improving their well-being". The UN Human Development indices suggest that human capital is merely a means to the end of human development: "Theories of human capital formation and human resource development view human beings as means to increased income and wealth rather than as ends. These theories are concerned with human beings as inputs to increasing production".
Mobility between nations.
Educated individuals often migrate from poor countries to rich countries seeking opportunity. This movement has positive effects for both countries: capital-rich countries gain an influx in labor, and labor rich countries receive capital when migrants remit money home. The loss of labor in the old country also increases the wage rate for those who do not emigrate, while the additional labor lowers wages in the new country. When workers migrate, their early care and education generally benefit the country where they move to work. And, when they have health problems or retire, their care and retirement pension will typically be paid in the new country.
African nations have invoked this argument with respect to slavery, other colonized peoples have invoked it with respect to the "brain drain" or "human capital flight" which occurs when the most talented individuals (those with the most individual capital) depart for education or opportunity to the colonizing country (historically, Britain and France and the U.S.). Even in Canada and other developed nations, the loss of human capital is considered a problem that can only be offset by further draws on the human capital of poorer nations via immigration. The economic impact of immigration to Canada is generally considered to be positive.
During the late 19th and early 20th centuries, human capital in the United States became considerably more valuable as the need for skilled labor came with newfound technological advancement. The 20th century is often revered as the "human capital century" by scholars such as Claudia Goldin. During this period a new mass movement toward secondary education paved the way for a transition to mass higher education. New techniques and processes required further education than the norm of primary schooling, which thus led to the creation of more formalized schooling across the nation. These advances produced a need for more skilled labor, which caused the wages of occupations that required more education to considerably diverge from the wages of ones that required less. This divergence created incentives for individuals to postpone entering the labor market in order to obtain more education. The “high school movement” had changed the educational system for youth in America. With minor state involvements, the high school movement started at the grass-roots level, particularly the communities with the most homogeneous populations. As a year in high school added more than ten percent to an individual’s income, post-elementary school enrollment and graduation rates increased significantly during the 20th century. The U.S. system of education was characterized for much of the 20th century by publicly funded mass secondary education that was open and forgiving, academic yet practical, secular, gender neutral, and funded by small, fiscally independent districts. This early insight into the need for education allowed for a significant jump in US productivity and economic prosperity, when compared to other world leaders at the time. It is suggested by several economists, that there is a positive correlation between high school enrollment rates and GDP per capita. Less developed countries have not established a set of institutions favoring equality and role of education for the masses and therefore have been incapable of investing in human capital stock necessary for technological growth.
The rights and freedom of individuals to travel and opportunity, despite some historical exceptions such as the Soviet bloc and its "Iron Curtain", seem to consistently transcend the countries in which they are educated. One must also remember that the ability to have mobility with regards to where people want to move and work is a part of their human capital. Being able to move from one area to the next is an ability and a benefit of having human capital. To restrict people from doing so would be to inherently lower their human capital.
This debate resembles, in form, that regarding natural capital.
Intangibility and portability.
Human capital is an intangible asset – it is not owned by the firm that employs it and is generally not fungible. Specifically, individuals arrive at 9am and leave at 5pm (in the conventional office model) taking most of their knowledge and relationships with them.
Human capital when viewed from a time perspective consumes time in one of key activities:
Despite the lack of formal ownership, firms can and do gain from high levels of training, in part because it creates a corporate culture or vocabulary teams use to create cohesion.
In recent economic writings the concept of firm-specific human capital, which includes those social relationships, individual instincts, and instructional details that are of value within one firm (but not in general), appears by way of explaining some labour mobility issues and such phenomena as golden handcuffs. Workers can be more valuable where they are simply for having acquired this knowledge, these skills and these instincts. Accordingly, the firm gains for their unwillingness to leave and market talents elsewhere.
Risk.
When human capital is assessed by activity based costing via time allocations it becomes possible to assess human capital risk. Human capital risks can be identified if HR processes in organizations are studied in detail. Human capital risk occurs when the organization operates below attainable operational excellence levels. For example, if a firm could reasonably reduce errors and rework (the Process component of human capital) from 10,000 hours per annum to 2,000 hours with attainable technology, the difference of 8,000 hours is human capital risk. When wage costs are applied to this difference (the 8,000 hours) it becomes possible to financially value human capital risk within an organizational perspective.
Risk accumulates in four primary categories:
Corporate finance.
In Corporate finance, human capital is one of the three primary components of Intellectual capital (which in addition to tangible assets comprise the entire value of a company). Human Capital is the value that the employees of a business provide through the application of skills, know-how and expertise. It is an organization’s combined human capability for solving business problems. Human Capital is inherent in people and cannot be owned by an organization. Therefore, Human Capital leaves an organization when people leave. Human Capital also encompasses how effectively an organization uses its people resources as measured by creativity and innovation. A company’s reputation as an employer affects the Human Capital it draws.
References.
aris 1966.

</doc>
<doc id="45805" url="https://en.wikipedia.org/wiki?curid=45805" title="Instructional capital">
Instructional capital

Instructional capital is a term used in educational administration after the 1960s, to reflect capital resulting from investment in producing learning materials.
Some have objected to this phrasing, which is an elaboration of referring to training as "human capital", either for the same reason that phrase is objectionable, or on the grounds that it implies that the human in which the knowledge is "invested" is a resource to be exploited.
Instructional capital can be used to guide or limit or restrict action by people (individual capital) or equipment (infrastructural capital) (if the learning materials are computer programs). It cannot generally make either individuals or infrastructure do what they are not trained or designed to do, but it can help prevent them from doing most stupid, destructive and dangerous things.
When people begin to trust instructions, they tend to associate social capital with them, as symbolized by a brand, flag or label. This usually opens up a possibility for those with power to start cheating and/or creating bad instructions that can no longer be trusted, but the good reputation of the brand, flag or label protects them from being caught for longer than would be the case without the symbol that is associated with good reputation.

</doc>
<doc id="45806" url="https://en.wikipedia.org/wiki?curid=45806" title="Garuda">
Garuda

The Garuda is a large bird-like creature, or humanoid bird that appears in both Hinduism and Buddhism. Garuda is the mount ("vahana") of the Lord Vishnu. Garuda is the Hindu name for the constellation Aquila. The brahminy kite and phoenix are considered to be the contemporary representations of Garuda. Indonesia adopts a more stylistic approach to the Garuda's depiction as its national symbol, where it depicts a Javanese eagle (being much larger than a kite).
About Garuda.
In Hinduism, is a Hindu divinity, usually the mount ("vahana") of the Lord Vishnu. is depicted as having the golden body of a strong man with a white face, red wings, and an eagle's beak and with a crown on his head. This ancient deity was said to be massive, large enough to block out the sun.
Garuda is known as the eternal sworn enemy of the Nāga serpent race and known for feeding exclusively on snakes, such behavior may have referred to the actual short-toed eagle of India. The image of Garuda is often used as the charm or amulet to protect the bearer from snake attack and its poison, since the king of birds is an implacable enemy and "devourer of serpent". Garudi Vidya is the mantra against snake poison to remove all kinds of evil.
His stature in Hindu religion can be gauged by the fact that a dependent Upanishad, the , and a Purana, the Garuda Purana, is devoted to him. Various names have been attributed to - Chirada, Gaganeshvara, Kamayusha, Kashyapi, Khageshvara, Nagantaka, Sitanana, Sudhahara, Suparna, Tarkshya, Vainateya, Vishnuratha and others. The Vedas provide the earliest reference of , though by the name of Śyena, where this mighty bird is said to have brought nectar to earth from heaven. The Puranas, which came into existence much later, mention as doing the same thing, which indicates that Śyena (Sanskrit for eagle) and are the same. One of the faces of Śrī Pañcamukha Hanuman is Mahavira . This face points towards the west. Worship of is believed to remove the effects of poisons from one's body. In Tamil Vaishnavism Garuda and Hanuman are known as "Periya Thiruvadi" and "Siriya Thiruvadi" respectively.
In the Bhagavad-Gita (Ch.10, Verse 30), in the middle of the battlefield "Kurukshetra", Krishna explaining his omnipresence, says - " as son of Vinata, I am in the form of Garuda, the king of the bird community (Garuda)" indicating the importance of Garuda.
Garuda wears the serpent Adisesha on his left small toenail and the serpent Gulika on his right cerebral cortex. The serpent Vasuki forms his sacred thread. The cobra Takshaka forms his belt on his hip. The snake Karkotaka is worn as his necklace. The snakes Padma and Mahapadma are his ear rings. The snake Shankachuda adorns his divine hair. He is flanked by his two wives ‘Rudra’ and ‘Sukeerthi’ or (Sukirthi). These are all invoked in Vedanta Desika's Garuda Panchashath and Garuda Dandaka compositions. Garuda flanked with his consorts 'Rudra' and 'Sukirthi' can be seen worshipped in an ancient Soumya Keshava temple in Bindiganavile (or Mayura puri in Sanskrit ) in Karnataka state of India.
Garuda Vyuha is worshiped in Tantra for Abhichara and to protect against Abhichara. However, the interesting thing is that Garuda is the Sankarshna form of the lord who during creation primarily possesses the knowledge aspect of the lord (among Vasudeva, Sankarshana, Pradyumna and Aniruddha forms). The important point is that Garuda represents the five vayus within us : prana, apana, vyana, udana, samana through his five forms Satya, Suparna, Garuda, Tarkshya, Vihageshwara. These five vayus through yoga can be controlled through Pranayama which can lead to Kundalini awakening leading to higher levels of consciousness.
Garuda plays an important role in Krishna Avatar in which Krishna and Satyabhama ride on Garuda to kill Narakasura. On another occasion, Lord Hari rides on Garuda to save the devotee elephant Gajendra. It is also said that Garuda's wings when flying will chant the Vedas.
With the position of Garuda's hands and palms, he is also called 'Kai Yendhi Perumal', in Tamil.
In the Mahabharata.
Birth and deeds.
The story of Garuda's birth and deeds is told in the first book of the great epic Mahabharata. According to the epic, when Garuda first burst forth from his egg, he appeared as a raging inferno equal to the cosmic conflagration that consumes the world at the end of every age. Frightened, the gods begged him for mercy. Garuda, hearing their plea, reduced himself in size and energy.
Garuda's father was the creator-rishi Kasyapa. He had two wives, Vinata and Kadru, who were daughters of Prajapathi Daksha. Kasyapa, on the pleadings of his wives, granted them their wishes; Vinata wished for two sons and Kadru wished for thousand snakes as her sons. Both laid eggs, while the thousand eggs of Kadru hatched early (after steaming the eggs to hatch) into snakes, the hatching of two eggs of Vinata did not take place for a long time. Impatient, Vinata broke open one egg, which was half formed with the upper half only as a human and was thus deformed. Her half formed son cursed her that she would be slave for her sister (she was her rival) for a long time by which time her second son would be born who would save her from his curse; her first son who flew away and came to prominence as Aruna, the red spectacle seen as the Sun rises in the morning, and as also charioteer of the Sun. The second egg hatched after a long time during which period Vinata was the servant of her sister as she had lost a bet with her. When the second egg hatched, a fully grown, shining and of mighty sized bird form emerged as Garuda, the king of birds. Garuda was thus born.
One day, Vinata entered into and lost a foolish bet, as a result of which she became enslaved to her sister. Resolving to release his mother from this state of bondage, Garuda approached the serpents and asked them what it would take to purchase her freedom. Their reply was that Garuda would have to bring them the elixir of immortality, also called amrita. It was a tall order. The amrita at that time found itself in the possession of the gods, who guarded it zealously, since it was the source of their immortality. They had ringed the elixir with a massive fire that covered the sky. They had blocked the way to the elixir with a fierce mechanical contraption of sharp rotating blades. And finally, they had stationed two gigantic poisonous snakes next to the elixir as deadly guardians.
Undaunted, Garuda hastened toward the abode of the gods intent on robbing them of their treasure. Knowing of his design, the gods met him in full battle-array. Garuda, however, defeated the entire host and scattered them in all directions. Taking the water of many rivers into his mouth, he extinguished the protective fire the gods had thrown up. Reducing his size, he crept past the rotating blades of their murderous machine. And finally, he mangled the two gigantic serpents they had posted as guards. Taking the elixir into his mouth without swallowing it, he launched again into the air and headed toward the eagerly waiting serpents. En route, he encountered Vishnu. Rather than fight, the two exchanged promises. Vishnu promised Garuda the gift of immortality even without drinking from the elixir, and Garuda promised to become Vishnu's mount. Flying onward, he met Indra the god of the sky. Another exchange of promises occurred. Garuda promised that once he had delivered the elixir, thus fulfilling the request of the serpents, he would make it possible for Indra to regain possession of the elixir and to take it back to the gods. Indra in turn promised Garuda the serpents as food.
At long last, Garuda alighted in front of the waiting serpents. Placing the elixir on the grass, and thereby liberating his mother Vinata from her servitude, he urged the serpents to perform their religious ablutions before consuming it. As they hurried off to do so, Indra swooped in to make off with the elixir. The serpents came back from their ablutions and saw the elixir gone but with small droplets of it on the grass. They tried to lick the droplets and thereby split their tongues in two. From then onwards, serpents have split tongues and shed their skin as a kind of immortality. From that day onward, Garuda was the ally of the gods and the trusty mount of Vishnu, as well as the implacable enemy of snakes, upon whom he preyed at every opportunity.
Descendants.
According to the Mahabharata, Garuda had six sons (Sumukha, Suvarna, Subala, Sunaama, Sunethra and Suvarchas) from whom were descended the race of birds. The members of this race were of great might and without compassion, subsisting as they did on their relatives the snakes. Vishnu was their protector.
As a symbol.
Throughout the Mahabharata, Garuda is invoked as a symbol of impetuous violent force, of speed, and of martial prowess. Powerful warriors advancing rapidly on doomed foes are likened to Garuda swooping down on a serpent. Defeated warriors are like snakes beaten down by Garuda. The field marshal Drona uses a military formation named after Garuda. Krishna even carries the image of Garuda on his banner.
In Buddhism.
In Buddhist mythology, the Garuda (Pāli: ') are enormous predatory birds with intelligence and social organization. Another name for the Garuda is ' (Pāli: ), meaning "well-winged, having good wings". Like the Naga, they combine the characteristics of animals and divine beings, and may be considered to be among the lowest devas.
The exact size of the Garuda is uncertain, but its wings are said to have a span of many miles. This may be a poetic exaggeration, but it is also said that when a Garuda's wings flap, they create hurricane-like winds that darken the sky and blow down houses. A human being is so small compared to a Garuda that a man can hide in the plumage of one without being noticed (Kākātī Jātaka, J.327). They are also capable of tearing up entire banyan trees from their roots and carrying them off.
Garudas are the great golden-winged Peng birds. They also have the ability to grow large or small, and to appear and disappear at will. Their wingspan is 330 yojanas (one yojana being 8 miles long). With one flap of its wings, a Peng bird dries up the waters of the sea so that it can gobble up all the exposed dragons. With another flap of its wings, it can level the mountains by moving them into the ocean.
There were also the four garuda-kings : Great-Power-Virtue Garuda-King, Great-Body Garuda-King, Great-Fulfillment Garuda-King, and Free-At-Will Garuda-King, each accompanied by hundreds of thousands of attendants.
The Garudas have kings and cities, and at least some of them have the magical power of changing into human form when they wish to have dealings with people. On some occasions Garuda kings have had romances with human women in this form. Their dwellings are in groves of the "simbalī", or silk-cotton tree.
The Garuda are enemies to the nāga, a race of intelligent serpent- or dragon-like beings, whom they hunt. The Garudas at one time caught the nāgas by seizing them by their heads; but the nāgas learned that by swallowing large stones, they could make themselves too heavy to be carried by the Garudas, wearing them out and killing them from exhaustion. This secret was divulged to one of the Garudas by the ascetic Karambiya, who taught him how to seize a nāga by the tail and force him to vomit up his stone (Pandara Jātaka, J.518).
The Garudas were among the beings appointed by Śakra to guard Mount Sumeru and the Trayastrimsa heaven from the attacks of the asuras.
In the Maha-samaya Sutta (Digha Nikaya 20), the Buddha is shown making temporary peace between the Nagas and the Garudas.
The Thai rendering of Garuda ( "Krut") as Vishnu vehicle and Garuda's quest for elixir was based on Indian legend of Garuda. It was told that Garuda overcame many heavenly beings in order to gain the ambrosia (amrita) elixir. No one was able to get the better of him, not even Narai (Vishnu). At last, a truce was called and an agreement was made to settle the rancor and smooth all the ruffled feathers. It was agreed that when Narai is in his heavenly palace, Garuda will be positioned in a superior status, atop the pillar above Narai's residence. However, whenever Narai wants to travel anywhere, Garuda must serve as his transport.
The Sanskrit word Garuda has been borrowed and modified in the languages of several countries. In Burmese, Garudas are called "galone" (). In Burmese astrology, the vehicle of the Sunday planet is the "galone". In the Kapampangan language of the Philippines, the native word for eagle is "galura". In Japanese a Garuda is called karura (however, the form "Garuda" ガルーダ is used in recent Japanese fiction - see below).
For the Mongols, the Garuda is called Khan Garuda or "Khangarid" (). Before and after each round of Mongolian wrestling, wrestlers perform the Garuda ritual, a stylised imitation of the "Khangarid" and a hawk.
In the Qing Dynasty fiction "The Story of Yue Fei" (1684), Garuda sits at the head of the Buddha's throne. But when a celestial bat (an embodiment of the Aquarius constellation) flatulates during the Buddha’s expounding of the Lotus Sutra, Garuda kills her and is exiled from paradise. He is later reborn as Song Dynasty General Yue Fei. The bat is reborn as Lady Wang, wife of the traitor Prime Minister Qin Hui, and is instrumental in formulating the "Eastern Window" plot that leads to Yue's eventual political execution. It is interesting to note "The Story of Yue Fei" plays on the legendary animosity between Garuda and the Nagas when the celestial bird-born Yue Fei defeats a magic serpent who transforms into the unearthly spear he uses throughout his military career. Literary critic C. T. Hsia explains the reason why Qian Cai, the book's author, linked Yue with Garuda is because of the homology in their Chinese names. Yue Fei's courtesy name is Pengju (鵬舉). A Peng (鵬) is a giant mythological bird likened to the Middle Eastern Roc. Garuda's Chinese name is Great Peng, the Golden-Winged Illumination King (大鵬金翅明王).
As a cultural and national symbol.
In India, Indonesia and the rest of Southeast Asia the eagle symbolism is represented by Garuda, a large mythical bird with eagle-like features that appears in both Hindu and Buddhist mythology as the vahana (vehicle) of the god Vishnu. Garuda became the national emblem of Thailand and Indonesia; Thailand's Garuda is rendered in a more traditional anthropomorphic mythical style, while that of Indonesia is rendered in heraldic style with traits similar to the real Javan hawk-eagle.
India.
India primarily uses Garuda as a martial motif:
Indonesia.
Indonesia uses the Garuda, called the Garuda Pancasila, as its national symbol, it is somewhat intertwined with the concept of the phoenix.
Thailand.
Thailand uses the Garuda () as its national symbol.

</doc>
<doc id="45807" url="https://en.wikipedia.org/wiki?curid=45807" title="Financial capital">
Financial capital

Financial capital is any economic resource measured in terms of money used by entrepreneurs and businesses to buy what they need to make their products or to provide their services to the sector of the economy upon which their operation is based, i.e. retail, corporate, investment banking, etc.
Three concepts of capital maintenance authorized in IFRS.
Financial capital or just capital/equity in finance, accounting and economics, is internal retained earnings generated by the entity or funds provided by lenders (and investors) to businesses to purchase real capital equipment or services for producing new goods/services. Real capital or economic capital comprises physical goods that assist in the production of other goods and services, e.g. shovels for gravediggers, sewing machines for tailors, or machinery and tooling for factories.
Financial capital generally refers to saved-up financial wealth, especially that used to start or maintain a business. A financial concept of capital is adopted by most entities in preparing their financial reports. Under a financial concept of capital, such as invested money or invested purchasing power, capital is synonymous with the net assets or equity of the entity. Under a physical concept of capital, such as operating capability, capital is regarded as the productive capacity of the entity based on, for example, units of output per day. Financial capital maintenance can be measured in either nominal monetary units or units of constant purchasing power. There are thus three concepts of capital maintenance in terms of International Financial Reporting Standards (IFRS): (1) Physical capital maintenance (2) Financial capital maintenance in nominal monetary units (3) Financial capital maintenance in units of constant purchasing power. Framework for the Preparation and Presentation of Financial Statements,
Financial capital is provided by lenders for a price: interest. Also see time value of money for a more detailed description of how financial capital may be analyzed.
Furthermore, financial capital, is any liquid medium or mechanism that represents wealth, or other styles of capital. It is, however, usually purchasing power in the form of money available for the production or purchasing of goods, etcetera. Capital can also be obtained by producing more than what is immediately required and saving the surplus.
Financial capital can also be in the form of purchasable items such as computers or books that can contribute directly or indirectly to obtaining various other types of capital.
Financial capital has been subcategorized by some academics as economic or "productive capital" necessary for operations, signaling capital which signals a company's financial strength to shareholders, and regulatory capital which fulfills capital requirements.
Fixed capital.
Fixed capital is money firms use to purchase assets that will remain permanently in the business and help it make a profit.
Working capital.
Firms use working capital to run their business. For example, money that they use to buy stock, pay expenses and finance credit.
Factors determining working capital requirements.
business policies
Instruments.
A contract regarding any combination of capital assets is called a financial instrument, and may serve as a 
Most indigenous forms of money (wampum, shells, tally sticks and such) and the modern fiat money is only a "symbolic" storage of value and not a real storage of value like commodity money.
Own and borrowed capital.
Capital contributed by the owner or entrepreneur of a business, and obtained, for example, by means of savings or inheritance, is known as own capital or equity, whereas that which is granted by another person or institution is called borrowed capital, and this must usually be paid back with interest. The ratio between debt and equity is named leverage. It has to be optimized as a high leverage can bring a higher profit but create solvency risk.
Borrowed capital.
This is capital which the business borrows from institutions or people, and includes debentures:
Own capital.
This is capital that owners of a business (shareholders and partners, for example) provide:
These have preference over the equity shares. This means the payments made to the shareholders are first paid to the preference shareholder(s) and then to the equity shareholders.
Issuing and trading.
Like money, financial instruments may be "backed" by state military fiat, credit (i.e. social capital held by banks and their depositors), or commodity resources. Governments generally closely control the supply of it and usually require some "reserve" be held by institutions granting credit. Trading between various national currency instruments is conducted on a money market. Such trading reveals differences in probability of debt collection or store of value function of that currency, as assigned by traders.
When in forms other than money, financial capital may be traded on bond markets or reinsurance markets with varying degrees of trust in the social capital (not just credits) of bond-issuers, insurers, and others who issue and trade in financial instruments. When payment is deferred on any such instrument, typically an interest rate is higher than the standard interest rates paid by banks, or charged by the central bank on its money. Often such instruments are called fixed-income instruments if they have reliable payment schedules associated with the uniform rate of interest. A variable-rate instrument, such as many consumer mortgages, will reflect the standard rate for deferred payment set by the central bank prime rate, increasing it by some fixed percentage. Other instruments, such as citizen entitlements, e.g. "U.S. Social Security", or other pensions, may be indexed to the rate of inflation, to provide a reliable value stream.
Trading in stock markets or commodity markets is actually trade in underlying assets which are not wholly financial in themselves, although they often move up and down in value in direct response to the trading in more purely financial derivatives. Typically commodity markets depend on politics that affect international trade, e.g. boycotts and embargoes, or factors that influence natural capital, e.g. weather that affects food crops. Meanwhile, stock markets are more influenced by trust in corporate leaders, i.e. individual capital, by consumers, i.e. social capital or "brand capital" (in some analyses), and internal organizational efficiency, i.e. instructional capital and infrastructural capital. Some enterprises issue instruments to specifically track one limited division or brand. "Financial futures", "Short selling" and "financial options" apply to these markets, and are typically pure financial bets on outcomes, rather than being a direct representation of any underlying asset.
Broadening the notion.
The relationship between financial capital, money, and all other styles of capital, especially human capital or labor, is assumed in central bank policy and regulations regarding instruments as above.
Such relationships and policies are characterized by a political economy - feudalist, socialist, capitalist, green, anarchist or otherwise. In effect, the means of money supply and other regulations on financial capital represent the economic sense of the value system of the society itself, as they determine the allocation of labor in that society.
So, for instance, rules for increasing or reducing the money supply based on perceived inflation, or on measuring well-being, reflect some such values, reflect the importance of using (all forms of) financial capital as a stable store of value. If this is very important, inflation control is key - any amount of money inflation reduces the value of financial capital with respect to all other types.
If, however, the medium of exchange function is more critical, new money may be more freely issued regardless of impact on either inflation or well-being.
Marxian perspectives.
It is common in Marxian theory to refer to the role of "Finance Capital" as the determining and ruling class interest in capitalist society, particularly in the latter stages.
Valuation.
Normally, a financial instrument is priced accordingly to the perception by capital market players of its expected return and risk.
Unit of account functions may come into question if valuations of complex financial instruments vary drastically based on timing. The "book value", "mark-to-market" and "mark-to-future" conventions are three different approaches to reconciling financial capital value units of account.
Economic role.
Socialism, capitalism, feudalism, anarchism, other civic theories take markedly different views of the role of financial capital in social life, and propose various political restrictions to deal with that.
Finance capitalism is the production of profit from the manipulation of financial capital. It is held in contrast to industrial capitalism, where profit is made from the manufacture of goods.

</doc>
<doc id="45809" url="https://en.wikipedia.org/wiki?curid=45809" title="Dijkstra's algorithm">
Dijkstra's algorithm

Dijkstra's algorithm is an algorithm for finding the shortest paths between nodes in a graph, which may represent, for example, road networks. It was conceived by computer scientist Edsger W. Dijkstra in 1956 and published three years later. 
The algorithm exists in many variants; Dijkstra's original variant found the shortest path between two nodes, but a more common variant fixes a single node as the "source" node and finds shortest paths from the source to all other nodes in the graph, producing a shortest-path tree.
For a given source node in the graph, the algorithm finds the shortest path between that node and every other. It can also be used for finding the shortest paths from a single node to a single destination node by stopping the algorithm once the shortest path to the destination node has been determined. For example, if the nodes of the graph represent cities and edge path costs represent driving distances between pairs of cities connected by a direct road, Dijkstra's algorithm can be used to find the shortest route between one city and all other cities. As a result, the shortest path algorithm is widely used in network routing protocols, most notably IS-IS and Open Shortest Path First (OSPF). It is also employed as a subroutine in other algorithms such as Johnson's.
Dijkstra's original algorithm does not use a min-priority queue and runs in time formula_1 (where formula_2 is the number of nodes). The idea of this algorithm is also given in . The implementation based on a min-priority queue implemented by a Fibonacci heap and running in formula_3 (where formula_4 is the number of edges) is due to .
This is asymptotically the fastest known single-source shortest-path algorithm for arbitrary directed graphs with unbounded non-negative weights. However, specialized cases (such as bounded/integer weights, directed acyclic graphs etc) can indeed be improved further as detailed in .
In some fields, artificial intelligence in particular, Dijkstra's algorithm or a variant of it is known as uniform-cost search and formulated as an instance of the more general idea of best-first search.
History.
Dijkstra thought about the shortest path problem when working at the Mathematical Center in Amsterdam in 1956 as a programmer to demonstrate capabilities of a new computer called ARMAC. His objective was to choose both a problem as well as an answer (that would be produced by computer) that non-computing people could understand. He designed the shortest path algorithm and later implemented it for ARMAC for a slightly simplified transportation map of 64 cities in the Netherlands (64, so that 6 bits would be sufficient to encode the city number). A year later, he came across another problem from hardware engineers working on the institute's next computer: minimize the amount of wire needed to connect the pins on the back panel of the machine. As a solution, he re-discovered the algorithm known as Prim's minimal spanning tree algorithm (known earlier to Jarník, and also rediscovered by Prim). Dijkstra published the algorithm in 1959, two years after Prim and 29 years after Jarník.
Algorithm.
Let the node at which we are starting be called the initial node. Let the distance of node "Y" be the distance from the initial node to "Y". Dijkstra's algorithm will assign some initial distance values and will try to improve them step by step.
Description.
Suppose you would like to find the "shortest path" between two intersections on a city map: a "starting point" and a "destination". Dijkstra's algorithm initially marks the distance (from the starting point) to every other intersection on the map with "infinity". This is done not to imply there is an infinite distance, but to note that those intersections have not yet been visited; some variants of this method simply leave the intersections' distances "unlabeled". Now, at each iteration, select the "current intersection". For the first iteration, the current intersection will be the starting point, and the distance to it (the intersection's label) will be "zero". For subsequent iterations (after the first), the current intersection will be the "closest unvisited intersection" to the starting point (this will be easy to find).
From the current intersection, "update" the distance to every unvisited intersection that is directly connected to it. This is done by determining the "sum" of the distance between an unvisited intersection and the value of the current intersection, and relabeling the unvisited intersection with this value (the sum), if it is less than its current value. In effect, the intersection is relabeled if the path to it through the current intersection is shorter than the previously known paths. To facilitate shortest path identification, in pencil, mark the road with an arrow pointing to the relabeled intersection if you label/relabel it, and erase all others pointing to it. After you have updated the distances to each neighboring intersection, mark the current intersection as "visited", and select the unvisited intersection with lowest distance (from the starting point) – or the lowest label—as the current intersection. Nodes marked as visited are labeled with the shortest path from the starting point to it and will not be revisited or returned to.
Continue this process of updating the neighboring intersections with the shortest distances, then marking the current intersection as visited and moving onto the closest unvisited intersection until you have marked the destination as visited. Once you have marked the destination as visited (as is the case with any visited intersection) you have determined the shortest path to it, from the starting point, and can "trace your way back, following the arrows in reverse"; in the algorithm's implementations, this is usually done (after the algorithm has reached the destination node) by following the nodes' parents from the destination node up to the starting node; that's why we keep also track of each node's parent.
This algorithm makes no attempt to direct "exploration" towards the destination as one might expect. Rather, the sole consideration in determining the next "current" intersection is its distance from the starting point. This algorithm therefore expands outward from the starting point, interactively considering every node that is closer in terms of shortest path distance until it reaches the destination. When understood in this way, it is clear how the algorithm necessarily finds the shortest path. However, it may also reveal one of the algorithm's weaknesses: its relative slowness in some topologies.
Pseudocode.
In the following algorithm, the code , searches for the vertex in the vertex set that has the least value. returns the length of the edge joining (i.e. the distance between) the two neighbor-nodes and . The variable on line 17 is the length of the path from the root node to the neighbor node if it were to go through . If this path is shorter than the current shortest path recorded for , that current path is replaced with this path. The array is populated with a pointer to the "next-hop" node on the source graph to get the shortest route to the source.
If we are only interested in a shortest path between vertices and , we can terminate the search after line 13 if .
Now we can read the shortest path from to by reverse iteration:
Now sequence is the list of vertices constituting one of the shortest paths from to , or the empty sequence if no path exists.
A more general problem would be to find all the shortest paths between and (there might be several different ones of the same length). Then instead of storing only a single node in each entry of we would store all nodes satisfying the relaxation condition. For example, if both and connect to and both of them lie on different shortest paths through (because the edge cost is the same in both cases), then we would add both and to . When the algorithm completes, data structure will actually describe a graph that is a subset of the original graph with some edges removed. Its key property will be that if the algorithm was run with some starting node, then every path from that node to any other node in the new graph will be the shortest path between those nodes in the original graph, and all paths of that length from the original graph will be present in the new graph. Then to actually find all these shortest paths between two given nodes we would use a path finding algorithm on the new graph, such as depth-first search.
Using a priority queue.
A min-priority queue is an abstract data type that provides 3 basic operations : , and . As mentioned earlier, using such a data structure can lead to faster computing times than using a basic queue. Notably, Fibonacci heap or Brodal queue offer optimal implementations for those 3 operations. As the algorithm is slightly different, we mention it here, in pseudo-code as well :
Instead of filling the priority queue with all nodes in the initialization phase, it is also possible to initialize it to contain only "source"; then, inside the block, the node must be inserted if not already in the queue (instead of performing a operation).
Other data structures can be used to achieve even faster computing times in practice.
Proof of correctness.
Proof is by induction on the number of visited nodes.
Invariant hypothesis: For each visited node , is the shortest distance from to ; and for each unvisited , is the shortest distance via visited nodes only from to (if such a path exists, otherwise infinity; note we do not assume is the actual shortest distance for un-visited nodes).
The base case is when there is just one visited node, namely the initial node , and the hypothesis is trivial.
Assume the hypothesis for "n-1" visited nodes. Now we choose an edge where has the least of any unvisited node and the edge is such that . 
After processing it will still be true that for each unvisited node , is the shortest distance from to using visited nodes only, since if there were a shorter path which doesn't visit we would have found it previously, and if there is a shorter path using we update it when processing .
Running time.
Bounds of the running time of Dijkstra's algorithm on a graph with edges formula_5 and vertices formula_6 can be expressed as a function of the number of edges, denoted formula_4, and the number of vertices, denoted formula_2, using big-O notation. How tight a bound is possible depends on the way the vertex set formula_9 is implemented. In the following, upper bounds can be simplified because formula_10 for any graph, but that simplification disregards the fact that in some problems, other upper bounds on formula_4 may hold.
For any implementation of the vertex set formula_9, the running time is in
where formula_14 and formula_15 are the complexities of the "decrease-key" and "extract-minimum" operations in formula_9, respectively. The simplest implementation of the Dijkstra's algorithm stores the vertex set formula_9 as an ordinary linked list or array, and extract-minimum is simply a linear search through all vertices in formula_9. In this case, the running time is formula_19.
For sparse graphs, that is, graphs with far fewer than formula_20 edges, Dijkstra's algorithm can be implemented more efficiently by storing the graph in the form of adjacency lists and using a self-balancing binary search tree, binary heap, pairing heap, or Fibonacci heap as a priority queue to implement extracting minimum efficiently. To perform decrease-key steps in a binary heap efficiently, it is necessary to use an auxiliary data structure that maps each vertex to its position in the heap, and to keep this structure up to date as the priority queue formula_9 changes. With a self-balancing binary search tree or binary heap, the algorithm requires
time in the worst case; for connected graphs this time bound can be simplified to formula_23. The Fibonacci heap improves this to
When using binary heaps, the average case time complexity is lower than the worst-case: assuming edge costs are drawn independently from a common probability distribution, the expected number of "decrease-key" operations is bounded by formula_25, giving a total running time of
Practical optimizations and infinite graphs.
In common presentations of Dijkstra's algorithm, initially all nodes are entered into the priority queue. This is, however, not necessary: the algorithm can start with a priority queue that contains only one item, and insert new items as they are discovered (instead of doing a decrease-key, check whether the key is in the queue; if it is, decrease its key, otherwise insert it). This variant has the same worst-case bounds as the common variant, but maintains a smaller priority queue in practice, speeding up the queue operations.
Moreover, not inserting all nodes in a graph makes it possible to extend the algorithm to find the shortest path from a single source to the closest of a set of target nodes on infinite graphs or those too large to represent in memory. The resulting algorithm is called "uniform-cost search" (UCS) in the artificial intelligence literature and can be expressed in pseudocode as
The complexity of this algorithm can be expressed in an alternative way for very large graphs: when is the length of the shortest path from the start node to any node satisfying the "goal" predicate, each edge has cost at least , and the number of neighbors per node is bounded by , then the algorithm's worst-case time and space complexity are both in .
Further optimizations of Dijkstra's algorithm for the single-target case include bidirectional variants, goal-directed variants such as the A* algorithm (see ), graph pruning to determine which nodes are likely to form the middle segment of shortest paths (reach-based routing), and hierarchical decompositions of the input graph that reduce routing to connecting and to their respective "transit nodes" followed by shortest-path computation between these transit nodes using a "highway".
Combinations of such techniques may be needed for optimal practical performance on specific problems.
Specialized variants.
When arc weights are integers and bounded by a constant "C", the usage of a special priority queue structure by Van Emde Boas et al.(1977) brings the complexity to formula_27. Another interesting implementation based on a combination of a new radix heap and the well-known Fibonacci heap runs in time formula_28 . Finally, the best algorithms in this special case are as follows. The algorithm given by runs in formula_29 time and the algorithm given by runs in formula_30 time.
Also, for directed acyclic graphs, it is possible to find shortest paths from a given starting vertex in linear formula_31 time, by processing the vertices in a topological order, and calculating the path length for each vertex to be the minimum length obtained via any of its incoming edges.
In the special case of integer weights and undirected graphs, the Dijkstra's algorithm can be completely countered with a linear formula_32 complexity algorithm, given by .
Picture behind Dijkstra's algorithm.
The idea behind Dijkstra's algorithm can be depicted like this:
Related problems and algorithms.
The functionality of Dijkstra's original algorithm can be extended with a variety of modifications. For example, sometimes it is desirable to present solutions which are less than mathematically optimal. To obtain a ranked list of less-than-optimal solutions, the optimal solution is first calculated. A single edge appearing in the optimal solution is removed from the graph, and the optimum solution to this new graph is calculated. Each edge of the original solution is suppressed in turn and a new shortest-path calculated. The secondary solutions are then ranked and presented after the first optimal solution.
Dijkstra's algorithm is usually the working principle behind link-state routing protocols, OSPF and IS-IS being the most common ones.
Unlike Dijkstra's algorithm, the Bellman–Ford algorithm can be used on graphs with negative edge weights, as long as the graph contains no negative cycle reachable from the source vertex "s". The presence of such cycles means there is no shortest path, since the total weight becomes lower each time the cycle is traversed. It is possible to adapt Dijkstra's algorithm to handle negative weight edges by combining it with the Bellman-Ford algorithm (to remove negative edges and detect negative cycles), such an algorithm is called Johnson's algorithm.
The A* algorithm is a generalization of Dijkstra's algorithm that cuts down on the size of the subgraph that must be explored, if additional information is available that provides a lower bound on the "distance" to the target. This approach can be viewed from the perspective of linear programming: there is a natural linear program for computing shortest paths, and solutions to its dual linear program are feasible if and only if they form a consistent heuristic (speaking roughly, since the sign conventions differ from place to place in the literature). This feasible dual / consistent heuristic defines a non-negative reduced cost and A* is essentially running Dijkstra's algorithm with these reduced costs. If the dual satisfies the weaker condition of admissibility, then A* is instead more akin to the Bellman–Ford algorithm.
The process that underlies Dijkstra's algorithm is similar to the greedy process used in Prim's algorithm. Prim's purpose is to find a minimum spanning tree that connects all nodes in the graph; Dijkstra is concerned with only two nodes. Prim's does not evaluate the total weight of the path from the starting node, only the individual path.
Breadth-first search can be viewed as a special-case of Dijkstra's algorithm on unweighted graphs, where the priority queue degenerates into a FIFO queue.
Fast marching method can be viewed as a continuous version of Dijkstra's algorithm which computes the geodesic distance on a triangle mesh.
Dynamic programming perspective.
From a dynamic programming point of view, Dijkstra's algorithm is a successive approximation scheme that solves the dynamic programming functional equation for the shortest path problem by the Reaching method.
In fact, Dijkstra's explanation of the logic behind the algorithm, namely
is a paraphrasing of Bellman's famous Principle of Optimality in the context of the shortest path problem.
In popular culture.
In the third season episode "Money For Nothing" (2007) of the television crime drama NUMB3RS, mathematics professor Charlie Eppes uses Dijkstra's algorithm to find the best escape routes out of Los Angeles for a hijacked truck that is carrying millions of dollars in cash and medical supplies and also two kidnapping victims.

</doc>
<doc id="45810" url="https://en.wikipedia.org/wiki?curid=45810" title="Subwoofer">
Subwoofer

A subwoofer (or sub) is a woofer, or a complete loudspeaker, which is dedicated to the reproduction of low-pitched audio frequencies known as bass. The typical frequency range for a subwoofer is about 20–60 Hz for consumer products, below 100 Hz for professional live sound, and below 80 Hz in THX-approved systems. Subwoofers are intended to augment the low frequency range of loudspeakers covering higher frequency bands.
Subwoofers are made up of one or more woofers mounted in a loudspeaker enclosure—often made of wood—capable of withstanding air pressure while resisting deformation. Subwoofer enclosures come in a variety of designs, including bass reflex (with a port or passive radiator in the enclosure), infinite baffle, horn-loaded, and bandpass designs, representing unique tradeoffs with respect to efficiency, bandwidth, size and cost. Passive subwoofers have a subwoofer driver and enclosure and they are powered by an external amplifier. Active subwoofers include a built-in amplifier.
The first subwoofers were developed in the 1960s to add bass response to home stereo systems. Subwoofers came into greater popular consciousness in the 1970s with the introduction of Sensurround in movies such as "Earthquake", which produced loud low-frequency sounds through large subwoofers. With the advent of the compact cassette and the compact disc in the 1980s, the easy reproduction of deep "and" loud bass was no longer limited by the ability of a phonograph record stylus to track a groove, and producers could add more low frequency content to recordings. As well, during the 1990s, DVDs were increasingly recorded with "surround sound" processes that included a Low-frequency effects (LFE) channel, which could be heard using the subwoofer in home theater systems. During the 1990s, subwoofers also became increasingly popular in home stereo systems, custom car audio installations, and in PA systems. By the 2000s, subwoofers became almost universal in sound reinforcement systems in nightclubs and concert venues.
History.
In September 1964 Raymon Dones, of El Cerrito, CA, received a US Patent (numbered US3150739) which was the first for a subwoofer specifically designed to omni-directionally augment the low frequency range of modern stereo systems. Able to reproduce distortion-free low frequencies down to 15 cycles per second, a specific objective of Dones’ invention was to provide portable sound enclosures which provide for high fidelity reproduction of-low frequency sound waves without giving an audible indication of the portion of the room from which they emanate. Dones' loudspeaker was marketed in the US under the trade name "The Octavium" from the early 60s to the mid-1970s. The Octavium was utilized by several recording artists of that era, most notably the Grateful Dead, bassist Monk Montgomery, bassist Nathan East, and the Pointer Sisters. The Octavium speaker and Dones' subwoofer technology was also utilized, in a few select theaters, to reproduce low pitch frequencies for the 1974 blockbuster movie "Earthquake". During the late 1960s Dones’ Octavium was favorably reviewed by audiophile publications including Hi-Fi News and Audio Magazine.
Another early subwoofer enclosure made for home and studio use was the separate bass speaker for the Servo Statik 1, by New Technology Enterprises. Designed as a prototype in 1966 by physicist Arnold Nudell and airline pilot Cary Christie in Nudell's garage, the design used a second winding around a custom Cerwin Vega 18-inch (45 cm) driver to provide servo control information to the amplifier, and it was offered for sale at $1795, some 40% more expensive than any other complete loudspeaker listed at "Stereo Review". In 1968, the two found outside investment and reorganized as Infinity. The subwoofer was reviewed positively in "Stereophile" magazine's Winter 1968 issue as the SS-1 by Infinity. The SS-1 was reviewed very highly in 1970 by "High Fidelity" magazine.
One of the first subwoofers was developed during the late 1960s by Ken Kreisel, the former president of the Miller & Kreisel Sound Corporation in Los Angeles. When Kreisel's business partner, Jonas Miller, who owned a high-end audio store in Los Angeles, told Kreisel that some purchasers of the store's high-end electrostatic speakers had complained about a lack of bass response in the electrostatics, Kreisel designed a powered woofer that would reproduce only those frequencies that were too low for the electrostatic speakers to convey. Infinity's full range electrostatic speaker system that was developed during the 1960s also used a woofer to cover the lower frequency range that its electrostatic arrays did not handle adequately.
The first use of a subwoofer in a recording session was in 1973 for mixing the Steely Dan album "Pretzel Logic" when recording engineer Roger Nichols arranged for Kreisel to bring a prototype of his subwoofer to Village Recorders. Further design modifications were made by Kreisel over the next ten years, and in the 1970s and 1980s by engineer John P. D'Arcy; record producer Daniel Levitin served as a consultant and "golden ears" for the design of the crossover network (used to partition the frequency spectrum so that the subwoofer would not attempt to reproduce frequencies too high for its effective range, and so that the main speakers would not need to handle frequencies too low for their effective range).
Subwoofers received a great deal of publicity in 1974 with the movie "Earthquake" which was released in Sensurround. Initially installed in 17 U.S. theaters, the Sensurround system used large subwoofers which were driven by racks of 500 watt amplifiers which were triggered by control tones printed on one of the audio tracks on the film. Four of the subwoofers were positioned in front of the audience under (or behind) the film screen and two more were placed together at the rear of the audience on a platform. Powerful noise energy in the range of 17 Hz to 120 Hz was generated at the level of 110–120 decibels of sound pressure level, abbreviated dB(SPL). The new low frequency entertainment method helped the film become a box office success. More Sensurround systems were assembled and installed. By 1976 there were almost 300 Sensurround systems leapfrogging through select theaters. Other films to use the effect include the WW II naval battle epic "Midway" in 1976 and "Rollercoaster" in 1977. Deep-Bass speakers were once an exotic commodity and are now much more popular with different sizes and capabilities of sound output.
For owners of 33 rpm LPs and 45 singles, loud "and" deep bass was limited by the ability of the phonograph record stylus to track the groove. Some hi-fi aficionados solved the problem by using reel-to-reel tape players which were capable of delivering accurate, naturally deep bass from acoustic sources, or synthetic bass not found in nature. With the popular introduction of the compact cassette and the CD, it became possible to add more low frequency content to recordings, and satisfy a larger number of consumers. Home subwoofers grew in popularity, as they were easy to add to existing multimedia speaker setups and they were easy to position or hide.
Construction and features.
Loudspeaker and enclosure design.
Subwoofers use speaker drivers (woofers) typically between 8" (20 cm) and 21" (53 cm) in diameter. Some uncommon subwoofers use larger drivers, and single prototype subwoofers as large as 60" (152 cm) have been fabricated. On the smaller end of the spectrum, subwoofer drivers as small as 4" (10 cm) may be used, depending on the design of the loudspeaker enclosure, the desired sound pressure level, the lowest frequency targeted and the level of permitted distortion. The most common subwoofer driver sizes used for sound reinforcement are 10", 12", 15" and 18" models (25 cm, 30 cm, 40 cm, and 45 cm respectively). The largest available sound reinforcement subwoofers, 21" (53 cm) drivers, are less commonly seen.
The efficiency of a speaker driver is given by:
Where the variables are Thiele/Small parameters. Deep low frequency extension is a common goal for a subwoofer and small box volumes are also considered desirable. Hoffman's Iron Laws therefore mandate low efficiency under those constraints, and indeed most subwoofers require considerable power, much more than other individual drivers.
So for the example of a sealed speaker box, the box volume to achieve a given Qts is proportional to Vas:
Therefore, a decrease in box volume and the same F3 will decrease the efficiency of the sub woofer. Similarly the F3 of a speaker is proportional to Fs:
As the efficiency is proportional to Fs3, small improvements in low frequency extension with the same driver and box volume will result in very significant reductions in efficiency. For these reasons, subwoofers are typically very inefficient at converting electrical energy into sound energy. This combination of factors accounts for the higher power output of subwoofer amplifiers, and the requirement for greater power handling for subwoofer drivers. Enclosure variations (e.g., bass reflex designs) are sometimes used for subwoofers to increase the efficiency of the driver/enclosure system, helping to reduce the amplifier power requirement.
Subwoofers have been designed using a number of enclosure approaches: bass reflex, acoustic suspension, infinite baffle, horn loaded, tapped horn, transmission line and bandpass. Each enclosure type has advantages and disadvantages in efficiency increase, bass extension, cabinet size, distortion, and cost. Subwoofers are typically constructed by mounting one or more woofers in a cabinet of medium-density fibreboard (MDF), oriented strand board (OSB), plywood, fiberglass, aluminum or other stiff materials. Because of the high air pressure they produce in the cabinet, subwoofer enclosures often require internal bracing to distribute the resulting forces.
The smallest subwoofers are typically those designed for desktop multimedia systems. The largest common subwoofer enclosures are those used for concert sound reinforcement systems or dance club sound systems. An example of a large concert subwoofer enclosure is the 1980s-era ElectroVoice MT-4 "Bass Cube" system, which used four 18" (45 cm) drivers. An example of a subwoofer that uses a bass horn is the Bassmaxx B-Two, which loads an 18" (45 cm) driver onto an long folded horn. Folded horn-type subwoofers can typically produce a deeper range with greater efficiency than the same driver in an enclosure that lacks a horn. Some experimental fixed-installation subwoofer horns have been constructed using brick and concrete to produce a very long horn that allows a very deep sub-bass extension.
Subwoofer output level can be increased by increasing cone surface area or by increasing cone excursion. Since large drivers require undesirably large cabinets, most subwoofer drivers have large excursions. Unfortunately, high excursion, at high power levels, tends to produce more distortion from inherent mechanical and magnetic effects in electro-dynamic drivers (the most common sort). The conflict between assorted goals can never be fully resolved; subwoofer designs are necessarily compromises. Hoffman's Iron Law (the efficiency of a woofer system is directly proportional to its cabinet volume and to the cube of its cutoff frequency) applies to subwoofers just as to all loudspeakers.
Frequency range and frequency response.
The frequency response specification of a speaker describes the range of frequencies or musical tones a speaker can reproduce, measured in hertz (Hz). The typical frequency range for a subwoofer is between 20–200 Hz. Professional concert sound system subwoofers typically operate below 100 Hz, and THX-approved systems operate below 80 Hz. Subwoofers vary in terms of the range of pitches that they can reproduce, depending on a number of factors such as the size of the cabinet and the construction and design of the enclosure and driver(s). Specifications of frequency response depend wholly for relevance on an accompanying amplitude value—measurements taken with a wider amplitude tolerance will give any loudspeaker a wider frequency response. For example, the JBL 4688 TCB Subwoofer System, a now-discontinued system which was designed for movie theaters, had a frequency response of 23–350 Hz when measured within a 10-decibel boundary (0 dB to -10 dB) and a narrower frequency response of 28–120 Hz when measured within a six-decibel boundary (±3 dB).
Subwoofers also vary in regard to the sound pressure levels achievable and the distortion levels they can produce over their range. Some subwoofers, such as "The Abyss" by MartinLogan for example can reproduce pitches down to around 18 Hz (which is about the pitch of the lowest rumbling notes on a huge pipe organ with -16 Hz-bass pipes) to 120 Hz (±3 dB). Nevertheless, even though the Abyss subwoofer can go down to 18 Hz, its lowest frequency and maximum SPL with a limit of 10% distortion is 35.5 Hz and 79.8 dB at 2 meters. This means that a person choosing a subwoofer needs to consider more than just the lowest pitch that the sub can reproduce.
Amplification.
'Active subwoofers' include their own dedicated amplifiers within the cabinet. Some also include user-adjustable equalization that allows boosted or reduced output at particular frequencies; these vary from a simple "boost" switch, to fully parametric equalizers meant for detailed speaker and room correction. Some such systems are even supplied with a calibrated microphone to measure the subwoofer's in-room response, so the automatic equalizer can correct the combination of subwoofer, subwoofer location, and room response to minimize effects of room modes and improve low frequency performance.
'Passive subwoofers' have a subwoofer driver and enclosure, but they do not include an amplifier. They sometimes incorporate internal passive crossovers, with the filter frequency determined at the factory. These are generally used with third-party power amplifiers, taking their inputs from active crossovers earlier in the signal chain. Inexpensive Home Theatre in a Box packages often come with a passive subwoofer cabinet that is amplified by the multi-channel amplifier. While few high-end home-theater systems use passive subwoofers, this format is still popular in the professional sound industry.
Equalization.
Equalization can be used to adjust the in-room response of a subwoofer system. Designers of active subwoofers sometimes include a degree of corrective equalization to compensate for known performance issues (e.g., a steeper than desired low end roll-off rate). In addition, many amplifiers include an adjustable low-pass filter, which prevents undesired higher frequencies from reaching the subwoofer driver. For example, if a listener's main speakers are usable down to 80 Hz, then the subwoofer filter can be set so the subwoofer only works below 80. Typical filters involve some overlap in frequency ranges; a steep filter is not generally desired for subwoofers. The crossover section may also include a high-pass "infrasonic" or "subsonic" filter which prevents the subwoofer driver from attempting to reproduce frequencies below its safe capabilities.
Some systems use parametric equalization in an attempt to correct for room frequency response irregularities. Equalization is often unable to achieve flat frequency response at all listening locations in part because of the resonance (i.e., standing wave) patterns at low frequencies in nearly all rooms. Careful positioning of the subwoofer within the room can also help flatten the frequency response. Multiple subwoofers can manage a flatter general response since they can often be arranged to excite room modes more evenly than a single subwoofer, allowing equalization to be more effective.
Phase control.
Changing the relative phase of the subwoofer with respect to the woofers in other speakers may or may not help to minimize unwanted destructive acoustic interference in the frequency region covered by both subwoofer and main speakers. It may not help at all frequencies, and may create further problems with frequency response, but is even so generally provided as an adjustment for subwoofer amplifiers. Phase control circuits may be a simple polarity reversal switch or a more complex continuously variable circuits.
Continuously variable phase control circuits are common in subwoofer amplifiers, and may be found in crossovers and as do-it-yourself electronics projects. Phase controls allow the listener to change the arrival time of the subwoofer sound waves relative to the same frequencies from the main speakers (i.e., at and around the crossover point to the subwoofer). A similar effect can be achieved with the delay control on many home theater receivers. The subwoofer phase control found on many subwoofer amplifiers is actually a polarity inversion switch. It allows users to reverse the polarity of the subwoofer relative to the audio signal it is being given. This type of control allows the subwoofer to either be in phase with the source signal, or 180 degrees out of phase.
The subwoofer phase can still be changed by moving the subwoofer closer to or further from the listening position, however this may not be always practical.
Servo subwoofers.
Some active subwoofers use a servo feedback mechanism based on cone movement which modifies the signal sent to the voice coil. The servo feedback signal is derived from a comparison of the input signal to the amplifier versus the actual motion of the cone. The usual source of the feedback signal is a few turns of voice coil attached to the cone or a microchip-based accelerometer placed on the cone itself. An advantage of a well-implemented servo subwoofer design is reduced distortion making smaller enclosure sizes possible. The primary disadvantages are cost and complexity.
Servo controlled subwoofers are not the same as Servodrive subwoofers whose primary mechanism of sound reproduction avoids the normal voice coil and magnet combination in favor of a high-speed belt-driven servomotor. The Servodrive design increases output power, reduces harmonic distortion and virtually eliminates the loss of loudspeaker output that results from an increase in voice coil impedance due to overheating of the voice coil (called "power compression".) This feature allows high power operation for extended periods of time. Intersonics was nominated for a TEC Award for its Servo Drive Loudspeaker (SDL) design in 1986 and for the Bass Tech 7 model in 1990.
Applications.
Home audio.
The use of a subwoofer augments the bass capability of the main speakers, and allows them to be smaller without sacrificing low frequency capability. A subwoofer does not necessarily provide superior bass performance in comparison to large conventional loudspeakers on ordinary music recordings due to the typical lack of very low frequency content on such sources. However, there are recordings with substantial low frequency content that most conventional loudspeakers are ill-equipped to handle without the help of a subwoofer, especially at high playback levels, such as music for pipe organs with 32' (9.75 meter) bass pipes (16 Hz), very large bass drums on symphony orchestra recordings and electronic music with extremely low synth bass parts, such as bass tests or bass songs.
Frequencies which are sufficiently low are not easily localized by humans, hence many stereo and multichannel audio systems feature only one subwoofer channel and a single subwoofer can be placed off-center without affecting the perceived sound stage, since the sound produced will be difficult to localize. The intention in a system with a subwoofer is often to use small main speakers (of which there are two for stereo and five or more for surround sound or movie tracks) and to hide the subwoofer elsewhere (e.g. behind furniture or under a table), or to augment an existing speaker to save it from having to handle woofer-destroying low frequencies at high levels. This effect is possible only if the subwoofer is restricted to quite low frequencies, usually taken to, say, 100 Hz and below—still less localization is possible if restricted to even lower maximum frequencies. Higher upper limits for the subwoofer (e.g., 125 Hz) are much more easily localized, making a single subwoofer impractical.
Some users add a subwoofer because high levels of low bass are desired, even beyond what is in the original recording, as in the case of house music enthusiasts. Thus, subwoofers may be part of a package that includes satellite speakers, may be purchased separately, or may be built into the same cabinet as a conventional speaker system. For instance, some floor standing tower speakers include a subwoofer driver in the lower portion of the same cabinet. Physical separation of subwoofer and "satellite" speakers not only allows placement in an inconspicuous location, but since sub-bass frequencies are particularly sensitive to room location (due to room resonances and reverberation 'modes'), the best position for the subwoofer is not likely to be where the "satellite" speakers are located.
For greatest efficiency and best coupling to the room's air volume, subwoofers can be placed in a corner of the room, far from large room openings, and closer to the listener. This is possible since low bass frequencies have a long wavelength; hence there is little difference between the information reaching a listener's left and right ears, and so they cannot be readily localized. All low frequency information is sent to the subwoofer. However, unless the sound tracks have been carefully mixed for a single subwoofer channel, it's possible to have some cancellation of low frequencies if bass information in one channel is out of phase with another.
The physically separate subwoofer/satellite arrangement has been popularized by multimedia speaker systems such as Bose Acoustimass Home Entertainment Systems, Polk Audio RM2008 Series and Klipsch Audio Technologies ProMedia, among many others. Low-cost "home theater in a box" systems advertise their integration and simplicity.
Particularly among low cost "Home Theater in a Box" systems and with "boom boxes", however, inclusion of a subwoofer may be little more than a marketing device. It is unlikely that a small woofer in an inexpensively-built compact plastic cabinet will have better bass performance than well-designed conventional (and typically larger) speakers in a plywood or MDF cabinet. Mere use of the term "subwoofer" is no guarantee of good or extended bass performance. Many multimedia "subwoofers" might better be termed "bass drivers" as they are too small to produce deep bass.
Further, poorly designed systems often leave everything below about 120 Hz (or even higher) to the subwoofer, meaning that the subwoofer handles frequencies which the ear can use for sound source localization, thus introducing an undesirable subwoofer "localization effect". This is usually due to poor crossover designs or choices (too high crossover point or insufficient crossover slope) used in many computer and home theater systems; localization also comes from port noise and from typically large amounts of harmonic distortion in the subwoofer design. Home subwoofers sold individually usually include crossover circuitry to assist integration into an existing system.
Car audio.
Automobiles are not well suited for the "hidden" subwoofer approach due to space limitations in the passenger compartments. It is not possible, in most circumstances, to fit such large drivers and enclosures into doors or dashboards, so subwoofers are installed in the trunk or back seat space. Some car audio enthusiasts compete to produce very high sound pressure levels in the confines of their vehicle's cabin; sometimes dangerously high. The "SPL wars" have drawn much attention to subwoofers in general, but subjective competitions in sound quality ("SQ") have not gained equivalent popularity. Top SPL cars are not able to play normal music, or perhaps even to drive normally as they are designed solely for competition. Many non-competition subwoofers are also capable of generating high levels in cars due to the small volume of a typical car interior. High sound levels can cause hearing loss and tinnitus if one is exposed to them for an extended period of time.
In the 2000s, several car audio manufacturers have produced subwoofers using non-circular shapes from manufacturers including Boston Acoustic, Kicker, Sony, Bazooka, and X-Tant. These shapes typically carry some sort of distortion penalties. In situations of limited mounting space they provide a greater cone area and assuming all other variables are constant, greater maximum output. An important factor in the "square sub vs round sub" argument is the effects of the enclosure used. In a sealed enclosure, the maximum displacement is determined by
formula_5
where
These are some of the Thiele/Small parameters which can either be measured or found with the driver specifications.
Cinema sound.
After the introduction of Sensurround, movie theater owners began installing permanent subwoofer systems. Dolby Stereo 70 mm Six Track was a six channel film sound format introduced in 1976 that used two subwoofer channels for stereo reproduction of low frequencies. In 1981, Altec introduced a dedicated cinema subwoofer model tuned to around 20 Hz: the 8182. Starting in 1983, THX certification of the cinema sound experience quantified the parameters of good audio for watching films, including requirements for subwoofer performance levels and enough isolation from outside sounds so that noise did not interfere with the listening experience. This helped provide guidelines for multiplex cinema owners who wanted to isolate each individual cinema from its neighbors, even as louder subwoofers were making isolation more difficult. Specific cinema subwoofer models appeared from JBL, Electro-Voice, Eastern Acoustic Works, Kintek, Meyer Sound Laboratories and BGW Systems in the early 1990s. In 1992, Dolby Digital's six-channel film sound format incorporated a single LFE channel, the "point one" in 5.1 surround sound.
Tom Horral, a Boston-based acoustician, blames complaints about modern movies being too loud on subwoofers. He says that before subwoofers made it possible to have loud, relatively undistorted bass, movie sound levels were limited by the distortion in less capable systems at low frequency and high levels.
Sound reinforcement.
Professional audio subwoofers used in rock concerts in stadiums, DJ performances at EDM festivals and similar events must be capable of very high output levels with low distortion. This is reflected in the design attention given in recent years to the subwoofer applications for sound reinforcement, public address systems, dance club systems and concert systems. Consumer applications (as in home use) are considerably less demanding due to much smaller listening space and lower playback levels. Subwoofers are now almost universal in professional sound applications such as live concert sound, churches, nightclubs, and theme parks. Movie theatres certified to the THX standard for playback always include high capability subwoofers. Some professional applications require subwoofers designed for very high sound levels, using multiple 12", 15", 18" or 21" drivers (30 cm, 40 cm, 45 cm, 53 cm respectively). Drivers as small as 10" (25 cm) are occasionally used, generally in horn loaded enclosures.
The number of subwoofer enclosures used in a concert depends on a number of factors, including the size of the venue, whether it is indoors or outdoors, the amount of low-frequency content in the band's sound, the desired volume of the concert, and the design and construction of the enclosures (e.g., direct-radiating versus horn-loaded. A small bar may use a single direct-radiating 15-inch (40 cm) sub cabinet. A large dance club may have a row of four or five twin 18-inch (45 cm) subwoofer cabinets, or more). In the largest stadium venues, there may be a very large number of subwoofer enclosures. For example, the 2009–2010 U2 360° Tour uses 24 Clair Brothers BT-218 subwoofers (a double 18" (45 cm) box) around the perimeter of the central circular stage, and 72 proprietary Clair Brothers cardioid S4 subwoofers placed underneath the ring-shaped "B" stage which encircles the central main stage.
The main speakers may be 'flown' from the ceiling of a venue on chain hoists, and 'flying points' (i.e., attachment points) are built into many professional loudspeaker enclosures. Subwoofers can be flown or stacked on the ground near the stage. There can be more than 50 double-18-inch (45 cm) cabinets in a typical concert system. Just as consumer subwoofer enclosures can be made of Medium-density fibreboard (MDF), Oriented strand board (OSB), plywood, plastic or other dense material, professional subwoofer enclosures can be built from the same materials. MDF is commonly used to construct subwoofers for permanent installations as its density is relatively high and weatherproofing is not a concern. Other permanent installation subwoofers have used very thick plywood: the Altec 8182 (1981) used 7-ply 28 mm birch-faced oak plywood. Touring subwoofers are typically built from 18–20 mm thick void-free Baltic birch (Betula pendula or Betula pubescens) plywood from Finland, Estonia or Russia; such plywood affords greater strength for frequently transported enclosures. Not naturally weatherproof, Baltic birch is coated with carpet, thick paint or spray-on truck bedliner to give the subwoofer enclosures greater durability.
Touring subwoofer cabinets are typically designed with features that facilitate moving the enclosure (e.g., wheels, a "towel bar" handle and recessed handles), a protective grill for the speaker (in direct radiating-style cabinets), metal or plastic protection for the cabinets to protect the finish as the cabinets are being slid one on top of another, and hardware to facilitate stacking the cabinets (e.g., interlocking corners) and for "flying" the cabinets from stage rigging.
In the 2000s, many small-to mid-size subwoofers designed for bands' live sound use and DJ applications are "powered subs"; that is, they have an integrated power amplifier. These models typically have a built-in crossover. Some models have a metal-reinforced hole in which a speaker stand can be mounted for full-range cabinets.
Full-range system.
In professional concert sound system design, subwoofers can be incorporated seamlessly with the main speakers into a stereo or mono full-range system by using an active crossover. Such a system receives its signal from the main mono or stereo mixing console mix bus and amplifies all frequencies together in the desired balance. If the main sound system is stereo, the subwoofers can also be in stereo. Otherwise, a mono subwoofer channel can be derived within the crossover from a stereo mix, depending on the crossover make and model.
Aux-fed subwoofers.
Instead of being incorporated into a full-range system, concert subwoofers can be supplied with their own signal from a separate mix bus on the mixing console; often one of the auxiliary sends ("aux" or "auxes") is used. This configuration is called "aux-fed subwoofers", and has been observed to significantly reduce low frequency "muddiness" that can build up in a concert sound system which has on stage a number of microphones each picking up low frequencies and each having different phase relationships of those low frequencies. The aux-fed subs method greatly reduces the number of sources feeding the subwoofers to include only those instruments that have desired low frequency information; sources such as kick drum, bass guitar, samplers and keys. This simplifies the signal sent to the subwoofers and makes for greater clarity and low punch. Aux-fed subs can even be stereo, if desired, using two auxiliary mix buses.
Directional bass.
In order to keep low frequency energy focused on the audience area and not on the stage, and to keep low frequencies from bothering people outside of the event space, a variety of techniques have been developed in concert sound to turn the naturally omnidirectional radiation of subwoofers into a more directional pattern. These techniques include setting up subwoofers in a vertical array; using combinations of delay and polarity inversion; and setting up a delay-shaded system.
Vertical array.
Stacking or rigging the subwoofers in a vertical array focuses the low frequencies forward to a greater or lesser extent depending on the physical length of the array. Longer arrays have a more directional effect at lower frequencies. The directionality is more pronounced in the vertical dimension, yielding a radiation pattern that is wide but not tall. This helps reduce the amount of low frequency sound bouncing off the ceiling indoors and assists in mitigating external noise complaints outdoors.
Rear delay array.
Another cardioid subwoofer array pattern can be used horizontally, one which takes few channels of processing and no change in required physical space. This method is often called "cardioid subwoofer array" or "CSA" even though the pattern of "all" directional subwoofer methods is cardioid. The CSA method reverses the enclosure orientation and inverts the polarity of one out of every three subwoofers across the front of the stage, and delays those enclosures for maximum cancellation of the target frequency on stage. Polarity inversion can be implemented electronically, by reversing the wiring polarity, or by physically positioning the enclosure to face rearward. This method reduces forward output relative to a tight-packed, flat-fronted array of subwoofers, but can solve problems of unwanted low frequency energy coming into microphones on stage. Compared to the end-fire array, this method has less on-axis energy but more even pattern control throughout the audience, and more predictable cancellation rearward. The effect spans a range of slightly more than one octave.
A second method of rear delay array combines end-fire topology with polarity reversal, using two subwoofers positioned front to back, the drivers spaced one-quarter wavelength apart, the rear enclosure inverted in polarity and delayed by a few milliseconds for maximum cancellation on stage of the target frequency. This method has the least output power directed toward the audience, compared to other directional methods.
End-fire array.
The end-fire subwoofer method, also called "forward steered arrays", places subwoofer drivers co-axially in one or more rows, using destructive interference to reduce emissions to the sides and rear. This can be done with separate subwoofer enclosures positioned front to back with a spacing between them of one-quarter wavelength of the target frequency, the frequency that is least wanted on stage or most desired in the audience. Each row is delayed beyond the first row by an amount related to the speed of sound in air; typically a few milliseconds. The arrival time of sound energy from all the subwoofers is near-simultaneous from the audience's perspective, but is canceled out to a large degree behind the subwoofers because of offset sound wave arrival times. Directionality of the target frequency can achieve as much as 25 dB rear attenuation, and the forward sound is coherently summed in line with the subwoofers. The positional technique of end-fire subwoofers came into widespread use in European live concert sound in 2006.
The end-fire array trades a few decibels of output power for directionality, so it requires more enclosures for the same output power as a tight-packed, flat-fronted array of enclosures. Sixteen enclosures in four rows were used in 2007 at one of the stages of the Ultra Music Festival, to reduce low frequency interference to neighboring stages. Because of the physical size of the end-fire array, few concert venues are able to implement it. The output pattern suffers from comb-filtering off-axis, but can be further shaped by adjusting the frequency response of each row of subwoofers.
Delay-shaded array.
A long line of subwoofers placed horizontally along the front edge of the stage can be delayed such that the center subs fire several milliseconds prior to the ones flanking them, which fire several milliseconds prior to "their" neighbors, continuing in this fashion until the last subwoofers are reached at the outside ends of the subwoofer row (beamforming). This method helps to counteract the extreme narrowing of horizontal dispersion pattern seen with a horizontal subwoofer array. Such delay shading can be used to virtually reshape a loudspeaker array.
Directional enclosure.
Some subwoofer enclosure designs rely on drivers facing to the sides or to the rear in order to achieve a degree of directionality. End-fire drivers can be positioned within a single enclosure that houses more than one driver.
Bass instrument amplification.
In rare cases, sound reinforcement subwoofer enclosures are also used for bass instrument amplification by electric bass players and synth bass players. For most bands and most small- to mid-size venues (e.g., nightclubs and bars), standard bass guitar speaker enclosures or keyboard amplifiers will provide sufficient sound pressure levels for onstage monitoring. Since a regular electric bass has a low "E" (41 Hz) as its lowest note, most standard bass guitar cabinets are only designed with a range that goes down to about 40 Hz. However, in some cases, performers wish to have extended sub-bass response that is not available from standard instrument speaker enclosures, so they use subwoofer cabinets. Just as some electric guitarists add huge stacks of guitar cabinets mainly for show, some bassists will add immense subwoofer cabinets with 18" woofers mainly for show, and the extension sub cabinets will be operated at a lower volume than the main bass cabinets.
Bass guitar players who may use subwoofer cabinets include performers who play with extended range basses that include a low "B" string (about 31 Hz); bassists who play in styles where a very powerful sub-bass response is an important part of the sound (e.g., funk, Latin, gospel, R & B, etc.); and/or bass players who perform in stadium-size venues or large outdoor venues. Keyboard players who use subwoofers for on-stage monitoring include electric organ players who use bass pedal keyboards (which go down to a low "C" which is about 33 Hz) and synth bass players who play rumbling sub-bass parts that go as low as 18 Hz. Of all of the keyboard instruments that are amplified onstage, synthesizers can produce some of the lowest pitches, because unlike a traditional electric piano or electric organ, which have as their lowest notes a low "A" and a low "C", respectively, a synth does not have a fixed lowest octave. A synth player can add lower octaves to a patch by pressing an "octave down" button, which can produce pitches that are at the limits of human hearing.
Several concert sound subwoofer manufacturers suggest that their subs can be used for bass instrument amplification. Meyer Sound suggests that its 650-R2 Concert Series Subwoofer, a enclosure with two 18-inch drivers (45 cm), can be used for bass instrument amplification. While performers who use concert sound subwoofers for onstage monitoring may like the powerful sub-bass sound that they get onstage, sound engineers may find the use of large subwoofers (e.g., two 18" drivers (45 cm)) for onstage instrument monitoring to be problematic, because it may interfere with the "Front of House" sub-bass sound.
Bass shakers.
Since subsonic bass is felt, sub-bass can be augmented using tactile transducers. Unlike a typical subwoofer driver, which produces audible vibrations, tactile transducers produce low-frequency vibrations that are designed to be felt by individuals who are touching the transducer or indirectly through a piece of furniture or a wooden floor. Tactile transducers have recently emerged as a device class, called variously "bass shakers", "butt shakers" and "throne shakers". They are attached to a seat, for instance a drummer's stool ("throne") or gamer's chair, car seat or home theater seating, and the vibrations of the driver are transmitted to the body then to the ear in a manner similar to bone conduction. They connect to an amplifier like a normal subwoofer. They can be attached to a large flat surface (for instance a floor or platform) to create a large low frequency conduction area, although the transmission of low frequencies through the feet is not as efficient as the seat.
The advantage of tactile transducers used for low frequencies is that they allow a listening environment that is not filled with loud low frequency waves. This helps the concert drummer to monitor his or her kick drum performance without "polluting" the stage with powerful low frequency waves from a 15" (40 cm) subwoofer monitor. By not having a subwoofer monitor, a bass shaker also enables a drummer to lower the sound pressure levels that he is exposed to during a performance. For home cinema or videogame use, bass shakers help the user avoid disturbing others in nearby apartments or rooms, because even powerful sound effects such as explosion sounds in a war videogame or the simulated rumbling of an earthquake in an adventure film will not be heard by others. However, some critics argue that the felt vibrations are disconnected from the auditory experience, and they claim that that music is less satisfying with the "butt shaker" than sound effects. As well, critics have claimed that the bass shaker itself can rattle during loud sound effects, which can distract the listener.
World record claims.
With varying measures upon which to base claims, several subwoofers have been said to be the world's largest, loudest or lowest.
Matterhorn.
The Matterhorn is a subwoofer model completed in March 2007 by Danley Sound Labs in Gainesville, Georgia after a U.S. military request for a loudspeaker that could project infrasonic waves over a distance. The Matterhorn was designed to reproduce a continuous sine wave from 15 to 20 Hz, and generate 94 dB at a distance of , and more than 140 dB for music playback measured at the horn mouth. It can generate a constant 15 Hz sine wave tone at 140 dB for 24 hours a day, seven days a week with extremely low harmonic distortion. The subwoofer has a flat frequency response from 15 to 80 Hz, and is down 3 dB at 12 Hz. It was built within an intermodal container long and square. The container doors swing open to reveal a tapped horn driven by 40 long-throw 15-inch (40 cm) MTX speaker drivers each powered by its own 1000-watt amplifier. The manufacturer claims that 53 13-ply 18 mm sheets of plywood were used in its construction, though one of the fabricators wrote that double-thickness 26-ply sheets were used for convenience.
A diesel generator is housed within the enclosure to supply electricity when external power is unavailable. Of the constant tone output capability, designer Tom Danley wrote that the "target 94 dB at 250 meters is not the essentially fictional 'burst' or 'peak SPL' nonsense in pro sound, or like the 'death burp' signal used in car sound contests." At the annual National Systems Contractors Association (NSCA) convention in March 2007, the Matterhorn was barred from making any loud demonstrations of its power because of concerns about damaging the building of the Orange County Convention Center. Instead, using only a single 20 amp electrical circuit for safety, visitors were allowed to step inside the horn of the subwoofer for an "acoustic massage" as the fractionally powered Matterhorn reproduced low level 10–15 Hz waves.
Royal Device custom installation.
Another subwoofer claimed to be the world's biggest is a custom installation in Italy made by Royal Device primarily of bricks, concrete and sound-deadening material consisting of two subwoofers embedded in the foundation of a listening room. The horn-loaded subwoofers each have a floor mouth that is , and a horn length that is , in a cavity under the floor of the listening room. Each subwoofer is driven by eight 18-inch subwoofer drivers with voice coils. The designers assert that the floor mouths of the horns are additionally loaded acoustically by a vertical wooden horn expansion and the room's ceiling to create a 10 Hz "full power" wave at the listening position.
Concept Design 60-inch.
A single diameter subwoofer driver was designed by Richard Clark and David Navone with the help of Dr. Eugene Patronis of the Georgia Institute of Technology. The driver was intended to break sound pressure level records when mounted in a road vehicle, calculated to be able to achieve more than 180 dBSPL. It was built in 1997, driven by DC motors connected to a rotary crankshaft somewhat like in a piston engine. The cone diameter was and was held in place with a surround. With a peak-to-peak stroke, it created a one-way air displacement of . It was capable of generating 5–20 Hz sine waves at various DC motor speeds—not as a response to audio signal—it could not play music. The driver was mounted in a stepvan owned by Tim Maynor but was too powerful for the amount of applied reinforcement and damaged the vehicle. MTX's Loyd Ivey helped underwrite the project and the driver was then called the MTX "Thunder 1000000" (one million).
Still unfinished, the vehicle was entered in an SPL competition in 1997 at which a complaint was lodged against the computer control of the DC motor. Instead of using the controller, two leads were touched together in the hope that the motor speed was set correctly. The drive shaft broke after one positive stroke which created an interior pressure wave of 162 dB. The Concept Design 60-inch was not shown in public after 1998.
MTX Jackhammer.
The heaviest production subwoofer intended for use in automobiles is the MTX Jackhammer by MTX Audio, which features a diameter cone. The Jackhammer has been known to take upwards of 6000 watts sent to a dual voice coil moving within a strontium ferrite magnet. The Jackhammer weighs in at and has an aluminum heat sink. The Jackhammer has been featured on the television show Pimp My Ride.

</doc>
<doc id="45811" url="https://en.wikipedia.org/wiki?curid=45811" title="Eponym">
Eponym

An eponym is a person, place, or thing for whom or for which something is named, or believed to be named. For example, Elizabeth I of England is the eponym of the Elizabethan era.
Many genericized trademarks such as aspirin, heroin and thermos are based on their original brand eponyms.
The adjectives derived from eponym, which include "eponymous" and "eponymic", similarly refers to being the person or thing after whom something is named, as "the "eponymous" founder of the Ford Motor Company" refers to founder's being Henry Ford. Recent usage, especially in the recorded-music industry, also allows "eponymous" to mean "named after its central character or creator".
History.
Periods have often been named after a ruler or other influential figure:
Trends
Specific types of incidents
Lists of eponyms.
By person's name
By category
Orthographic conventions.
Capitalized versus lowercase.
For examples, see the comparison table below.

</doc>
<doc id="45812" url="https://en.wikipedia.org/wiki?curid=45812" title="Pim Fortuyn">
Pim Fortuyn

Wilhelmus Simon Petrus Fortuijn, known as Pim Fortuyn (; 19 February 1948 – 6 May 2002) was a Dutch politician, civil servant, sociologist, author and professor who formed his own party, "Pim Fortuyn List" (Lijst Pim Fortuyn or LPF) in 2002.
Fortuyn provoked controversy with his outspoken views about multiculturalism, immigration and Islam in the Netherlands. He called Islam "a backward culture", and was quoted as saying that if it were legally possible, he would close the borders for Muslim immigrants. He was labelled a far-right populist by his opponents and in the media, but he fiercely rejected this label.
Fortuyn explicitly distanced himself from "far-right" politicians such as the Belgian Filip Dewinter, the Austrian Jörg Haider, or Frenchman Jean-Marie Le Pen whenever compared to them. While he compared his own politics to centre-right politicians such as Silvio Berlusconi of Italy, he also admired former Dutch Prime Minister Joop den Uyl, a social democrat, and Democratic U.S. president John F. Kennedy. Fortuyn also criticised the Polder model and the policies of the outgoing government of Wim Kok and repeatedly described himself and LPF's ideology as pragmatic and not populistic. Fortuyn was openly homosexual.
Fortuyn was assassinated during the 2002 Dutch national election campaign by Volkert van der Graaf. In court at his trial, Van der Graaf said he murdered Fortuyn to stop him from exploiting Muslims as "scapegoats" and targeting "the weak members of society" in seeking political power.
Early life and education.
Fortuyn was born on 19 February 1948 in Driehuis, as the third child to a Catholic family. In 1967 he began to study sociology at the University of Amsterdam but transferred after a few months to the Vrije Universiteit in Amsterdam. In 1971 he ended his study with the Academic degree Doctorandus. In 1981 he received a doctorate in sociology at the University of Groningen as a Doctor of Philosophy.
Career.
Fortuyn worked as a lecturer at the Nyenrode Business Universiteit and as an associate professor at the University of Groningen, where he taught Marxist sociology. He was a Marxist at the time. Later, he joined the Labour Party.
In 1989 Fortuyn became director of a government organisation administering student transport cards. In 1990 he moved to Rotterdam. From 1991 to 1995, he was an extraordinary professor at the Erasmus University Rotterdam, appointed to the Albeda-chair in "employment conditions in public service".
When his contract ended, he made a career of public speaking and writing books and press columns, gradually becoming involved in politics. Fortuyn was openly gay, and said in a 2002 interview that he was Catholic.
Political career.
In 1992 Fortuyn wrote "Aan het volk van Nederland" (To the people of the Netherlands), declaring he was the successor to the charismatic but controversial 18th-century Dutch politician Joan van der Capellen tot den Pol. A one-time communist and former member of the social-democratic Labour Party, Fortuyn was elected "lijsttrekker" of the newly formed Livable Netherlands party by a large majority on 26 November 2001, prior to the Dutch general election of 2002.
On 9 February 2002, he was interviewed by the "Volkskrant", a Dutch newspaper (see below). His statements were considered so controversial that the party dismissed him as "lijsttrekker" the next day. Fortuyn had said that he favoured putting an end to Muslim immigration, if possible. Having been rejected by Livable Netherlands, Fortuyn founded his own party LPF (Pim Fortuyn List) on 11 February 2002. Many Livable Netherlands supporters transferred their support to the new party.
Heading the list of the Livable Rotterdam party, a local issues party, he achieved a major victory in the Rotterdam district council elections in early March 2002. The new party won about 36% of the seats, making it the largest party in the council. For the first time since the Second World War, the Labour Party was out of power in Rotterdam.
Fortuyn's victory made him the subject of hundreds of interviews during the next three months, and he made many statements about his political ideology. In March he released his book "The Mess of Eight Purple Years" ("De puinhopen van acht jaar Paars"), which he used as his political agenda for the upcoming general election. Purple is the colour to indicate a coalition government consisting of left parties (red) and conservative-liberal parties (blue). The Netherlands had been governed by such a coalition for eight years at that time.
Assassination.
On 6 May 2002, at age 54, Fortuyn was assassinated in Hilversum, North Holland, by Volkert van der Graaf. The attack took place in a parking lot outside a radio studio where Fortuyn had just given an interview. This was nine days before the general election, for which he was running. The attacker was pursued by Hans Smolders, Fortuyn's driver, and was arrested by the police shortly afterward, still in possession of a handgun. Months later, Van der Graaf confessed in court to the first notable political assassination in the Netherlands since 1672 (excluding WW II events). He was sentenced to eighteen years in prison and automatically paroled in 2014, after having served twelve years.
The assassination shocked many residents of the Netherlands and highlighted the cultural clashes within the country. Various conspiracy theories arose after Pim Fortuyn's murder and deeply affected Dutch politics and society. Politicians from all parties suspended campaigning. After consultation with LPF, the government decided not to postpone the elections. As Dutch law did not permit modifying the ballots, Fortuyn became a posthumous candidate. The LPF made an unprecedented debut in the House of Representatives by winning 26 seats (17% of the 150 seats in the house). The LPF joined a cabinet with the Christian Democratic Appeal and the People's Party for Freedom and Democracy, but conflicts in the rudderless LPF quickly collapsed the cabinet, forcing new elections. By the following year, the party had lost support, winning only eight seats in the 2003 elections. It won no seats in the 2006 elections, by which time the Party for Freedom, led by Geert Wilders, had emerged as a successor.
During the last months of his life, Fortuyn had become closer to the Catholic Church. To the surprise of many commentators and Dutch TV hosts, Fortuyn insisted on Fr. Louis Berger, a parish priest from The Hague, accompanying him in some of his last TV appearances. According to the "New York Times", Berger had become his "friend and confessor" during the last weeks of his life.
Fortuyn was initially buried in Driehuis in the Netherlands. He was re-interred on 20 July 2002, at San Giorgio della Richinvelda, in the province of Pordenone in Italy, where he had owned a house.
Views on Islam and immigration.
When asked about his opposition to Muslim immigration, Fortuyn explained that, "I have no desire to go through the emancipation of women and homosexuals all over again." In August 2001, Fortuyn was quoted in the "Rotterdams Dagblad" newspaper saying, "I am also in favour of a cold war with Islam. I see Islam as an extraordinary threat, as a hostile religion." In the TV program, "Business class", Fortuyn said that Muslims in the Netherlands did not accept Dutch society. He appeared on the program several times. It was moderated by his friend Harry Mens. Since his death, commentators have suggested Fortuyn's words were interpreted rather harshly, if not wrongly. For instance, he said that Muslims in the Netherlands needed to accept living "together" with the Dutch, and that if this was unacceptable for them, then they were free to leave. His concluding words in the TV program were "...I want to live together with the Muslim people, but it takes two to tango."
On 9 February 2002, additional statements made by him were carried in the "Volkskrant". He said that the Netherlands, with a population of 16 million, had enough inhabitants, and the practice of allowing as many as 40,000 asylum-seekers into the country each year had to be stopped. (This figure was higher than the actual numbers, and immigrants were decreasing at the time.). He claimed that if he became part of the next government, he would pursue a restrictive immigration policy while also granting citizenship to a large group of illegal immigrants.
He said that he did not intend to "unload our Moroccan hooligans" onto the Moroccan King Hassan. Hassan had died three years earlier. He considered Article 7 of the constitution, which asserts freedom of speech, of more importance than Article 1, which forbids discrimination on the basis of religion, life principles, political inclination, race, or sexual preference. Fortuyn distanced himself from Hans Janmaat of the Centrum Democraten, who in the 1980s wanted to remove all foreigners from the country and was repeatedly convicted for discrimination and hate speech.
Fortuyn proposed that all people who already resided in the Netherlands would be able to stay, but he emphasized the need of the immigrants to adopt Dutch society's consensus on human rights as their own. He said "If it were legally possible, I'd say no more Muslims will get in here", claiming that the influx of Muslims would threaten freedoms in the liberal Dutch society. He thought Muslim culture had never undergone a process of modernisation and therefore still lacked acceptance of democracy and women's, gays', lesbians' and minorities' rights. He feared Muslims would try to replace the Dutch legal system with the shari'a law.
He said he was concerned about intolerance in the Muslim community. In a televised debate in 2002, "Fortuyn baited the Muslim cleric by flaunting his homosexuality. Finally the "imam" exploded, denouncing Fortuyn in strongly anti-homosexual terms. Fortuyn calmly turned to the camera and, addressing viewers directly, told them that this is the kind of Trojan horse of intolerance the Dutch are inviting into their society in the name of multiculturalism."
When asked by the Dutch newspaper "Volkskrant" whether he hated Islam, he replied:
I don't hate Islam. I consider it a backward culture. I have travelled much in the world. And wherever Islam rules, it's just terrible. All the hypocrisy. It's a bit like those old Reformed Protestants. The Reformed lie all the time. And why is that? Because they have standards and values that are so high that you can't humanly maintain them. You also see that in that Muslim culture. Then look at the Netherlands. In what country could an electoral leader of such a large movement as mine be openly homosexual? How wonderful that that's possible. That's something that one can be proud of. And I'd like to keep it that way, thank you very much.
Fortuyn used the word "achterlijk", literally meaning "backward", but commonly used as an insult in the sense of "retarded". After his use of "achterlijk" caused an uproar, Fortuyn said he had used the word with its literal meaning of "backward".
Fortuyn wrote "Against the Islamization of Our Culture" (1997) (in Dutch).
Fortuynism.
The ideology or political style that is derived from Pim Fortuyn, and in turn the LPF, is often called Fortuynism. Observers variously saw him as a political protest targeting the alleged elitism and bureaucratic style of the Dutch purple coalitions or as offering an appealing political style. The style was characterized variously as one "of openness, directness and clearness", populism or simply as charisma. Another school holds Fortuynism as a distinct ideology, with an alternative vision of society. Some argued that Fortuynism was not just "one" ideology, but contained liberalism, populism and nationalism.
During the 2002 campaign, Fortuyn was accused of being on the "extreme right", although others saw only certain similarities. While he employed anti-immigration rhetoric, he was neither a radical nationalist nor a defender of traditional authoritarian values. On the contrary, Fortuyn wanted to protect the socio-culturally liberal values of the Netherlands, women's rights and sexual minorities (he was openly homosexual himself), from the "backward" Islamic culture. He held liberal views favouring the drug policy of the Netherlands, same-sex marriage, euthanasia, and related positions.
The LPF also won support from some ethnic minorities; one of Fortuyn's closest associates was of Cape Verdean origin, and one of the party's MPs was a young woman of Turkish descent.
His ideology can be comprised in the following positions:
Criticism.
Fortuyn was compared with the politicians Jörg Haider and Jean-Marie Le Pen in the foreign press. These comparisons were often referred to by Dutch reporters and politicians. An explicit comparison with Le Pen was made by Ad Melkert, then "lijsttrekker" of the Labour Party, who said in Emmen on 24 April 2002: "If you flirt with Fortuyn, then in the Netherlands the same thing will happen as happened in France. There they woke up with Le Pen, soon we will wake up with Fortuyn."
On 5 May, the day before the assassination, Fortuyn debated with Melkert in a debate organized by the "Algemeen Dagblad" newspaper about demonization of himself. In it he said that he often had to tell journalists that the image created of him in the media was incorrect.
Columnist Jan Blokker wrote "After reading (...) I realized once again that Professor Pim may really be called the Jean-Marie Le Pen, the Filip Dewinter, the Jörg Haider and the new Hans Janmaat of the Netherlands."
Legacy.
Fortuyn changed the Dutch political landscape and political culture. The 2002 elections, only weeks after Fortuyn's death, were marked by large losses for the liberal People's Party for Freedom and Democracy and especially the social democratic Labour Party (whose parliamentary group was halved in size); both parties replaced their leaders shortly after their losses. The election winners were the Pim Fortuyn List, and the Christian democratic Christian Democratic Appeal. Some commentators think that Fortuyn's perceived martyrdom created greater support for the LPF, which seems likely given its quick later decline.
The Netherlands has made its asylum policy more strict. Some have objected to what they think is a harsher political and social climate, especially towards immigrants and Muslims.
Contemporary Dutch politics is more polarized than it has been in recent years, especially on the issues for which Fortuyn was best known. People debate the success of their multicultural society, and whether they need to better assimilate newcomers. The government's decision to expel numerous asylum seekers whose applications had failed was controversial. Fortuyn had advocated an amnesty for asylum seekers already residing in the Netherlands.
The coalition cabinet of Christian Democratic Appeal, Pim Fortuyn List and People's Party for Freedom and Democracy fell within three months, due to infighting within the LPF. In the following elections, the LPF was left with only 8 seats in parliament (out of 150) and was not included in the new government. Political commentators speculated that discontented voters might vote for a non-traditional party, if a viable alternative was at hand. In recent times the right-wing Party for Freedom, which has a strong stance on immigration and integration, won 9 (out of 150) seats in the 2006 elections and 24 in 2010.
In 2004, in a TV show, Fortuyn was chosen as De Grootste Nederlander ("Greatest Dutchman of all-time"), followed closely by William of Orange, the leader of the independence war that established the precursor to the present-day Netherlands. The election was not considered representative, as it was held by viewers' voting through the internet and by phoning in. Theo van Gogh had been murdered a few days before by a Muslim, which likely affected people's voting in the TV contest for Fortuyn. The program later revealed that William of Orange had received the most votes, but many could not be counted until after the official closing time of the television show (and the proclamation of the winner), due to technical problems. The official rules of the show said that votes counted before the end of the show would be decisive, but it was suggested that all votes correctly cast before the closing of the vote would be counted. Following the official rules, the outcome was not changed.
Fortuyn's political career and popularity suggested a change in the Dutch people's views of their society as tolerant with integrated multiple cultures. 
"First of all, one can conclude that criticism on political correctness and on the ideal of the multicultural society has broken through for real relatively late... In the end it was Pim Fortuyn, the electoral success of the LPF and namely the murder on Fortuyn which led to the definitive breakthrough." Although Fortuyn did not advocate segregation, he brought it up as a debatable issue.
Right-wing politicians gained power after Fortuyn's death, such as former Minister for Integration & Immigration Rita Verdonk and the prominent critic of Islam, Member of the House of Representatives Geert Wilders. These politicians often focused on the debate over cultural assimilation and integration.

</doc>
<doc id="45829" url="https://en.wikipedia.org/wiki?curid=45829" title="Structural engineering">
Structural engineering

Structural engineers are trained to understand and calculate the stability, strength and rigidity of built structures for buildings and nonbuilding structures, to develop designs and integrate their design with that of other designers, and to supervise construction of projects on site. They can also be involved in the design of machinery, medical equipment, vehicles etc. where structural integrity affects functioning and safety.
Structural engineering theory is based upon applied physical laws and empirical knowledge of the structural performance of different materials and geometries. Structural engineering design utilizes a number of relatively simple structural elements to build complex structural systems. Structural engineers are responsible for making creative and efficient use of funds, structural elements and materials to achieve these goals.
Structural Engineer (Professional).
Structural engineers are responsible for engineering design and analysis. Entry-level structural engineers may design the individual structural elements of a structure, for example the beams, columns, and floors of a building. More experienced engineers may be responsible for the structural design and integrity of an entire system, such as a building.
Structural engineers often specialize in particular fields, such as bridge engineering, building engineering, pipeline engineering, industrial structures, or special mechanical structures such as vehicles, ships or aircraft.
Structural engineering has existed since humans first started to construct their own structures. It became a more defined and formalised profession with the emergence of the architecture profession as distinct from the engineering profession during the industrial revolution in the late 19th century. Until then, the architect and the structural engineer were usually one and the same - the master builder. Only with the development of specialised knowledge of structural theories that emerged during the 19th and early 20th centuries did the professional structural engineer come into existence.
The role of a structural engineer today involves a significant understanding of both static and dynamic loading, and the structures that are available to resist them. The complexity of modern structures often requires a great deal of creativity from the engineer in order to ensure the structures support and resist the loads they are subjected to. A structural engineer will typically have a four or five year undergraduate degree, followed by a minimum of three years of professional practice before being considered fully qualified.
Structural engineers are licensed or accredited by different learned societies and regulatory bodies around the world (for example, the Institution of Structural Engineers in the UK). Depending on the degree course they have studied and/or the jurisdiction they are seeking licensure in, they may be accredited (or licensed) as just structural engineers, or as civil engineers, or as both civil and structural engineers.
Another international organisation is IABSE (International Association for Bridge and Structural Engineering). The aim of that association is to exchange knowledge and to advance the practice of structural engineering worldwide in the service of the profession and society.
History of Structural Engineering.
Structural engineering dates back to 2700 B.C.E. when the step pyramid for Pharaoh Djoser was built by Imhotep, the first engineer in history known by name. Pyramids were the most common major structures built by ancient civilizations because the structural form of a pyramid is inherently stable and can be almost infinitely scaled (as opposed to most other structural forms, which cannot be linearly increased in size in proportion to increased loads).
However, it's important to note that the structural stability of the pyramid is not primarily a result of its shape. The integrity of the pyramid is intact as long as each of the stones is able to support the weight of the stone above it. The limestone blocks were taken from a quarry near the build site. Since the compressive strength of limestone is anywhere from 30 to 250 MPa (MPa = Pa * 10^6), the blocks will not fail under compression. Therefore, the structural strength of the pyramid stems from the material properties of the stones from which it was built rather than the pyramid's geometry.
Throughout ancient and medieval history most architectural design and construction was carried out by artisans, such as stone masons and carpenters, rising to the role of master builder. No theory of structures existed, and understanding of how structures stood up was extremely limited, and based almost entirely on empirical evidence of 'what had worked before'. Knowledge was retained by guilds and seldom supplanted by advances. Structures were repetitive, and increases in scale were incremental.
No record exists of the first calculations of the strength of structural members or the behavior of structural material, but the profession of structural engineer only really took shape with the Industrial Revolution and the re-invention of concrete (see History of Concrete). The physical sciences underlying structural engineering began to be understood in the Renaissance and have since developed into computer-based applications pioneered in the 1970s.
Timeline.
Structural failure.
The history of structural engineering contains many collapses and failures. Sometimes this is due to obvious negligence, as in the case of the Pétionville school collapse, in which Rev. Fortin Augustin ""constructed the building all by himself, saying he didn't need an engineer as he had good knowledge of construction"" following a partial collapse of the three-story schoolhouse that sent neighbors fleeing. The final collapse killed 94 people, mostly children.
In other cases structural failures require careful study, and the results of these inquiries have resulted in improved practices and greater understanding of the science of structural engineering. Some such studies are the result of forensic engineering investigations where the original engineer seems to have done everything in accordance with the state of the profession and acceptable practice yet a failure still eventuated. A famous case of structural knowledge and practice being advanced in this manner can be found in a series of failures involving box girders which collapsed in Australia during the 1970s.
Specializations.
Building structures.
Structural building engineering includes all structural engineering related to the design of buildings. It is a branch of structural engineering closely affiliated with architecture.
Structural building engineering is primarily driven by the creative manipulation of materials and forms and the underlying mathematical and scientific ideas to achieve an end which fulfills its functional requirements and is structurally safe when subjected to all the loads it could reasonably be expected to experience. This is subtly different from architectural design, which is driven by the creative manipulation of materials and forms, mass, space, volume, texture and light to achieve an end which is aesthetic, functional and often artistic.
The architect is usually the lead designer on buildings, with a structural engineer employed as a sub-consultant. The degree to which each discipline actually leads the design depends heavily on the type of structure. Many structures are structurally simple and led by architecture, such as multi-storey office buildings and housing, while other structures, such as tensile structures, shells and gridshells are heavily dependent on their form for their strength, and the engineer may have a more significant influence on the form, and hence much of the aesthetic, than the architect.
The structural design for a building must ensure that the building is able to stand up safely, able to function without excessive deflections or movements which may cause fatigue of structural elements, cracking or failure of fixtures, fittings or partitions, or discomfort for occupants. It must account for movements and forces due to temperature, creep, cracking and imposed loads. It must also ensure that the design is practically buildable within acceptable manufacturing tolerances of the materials. It must allow the architecture to work, and the building services to fit within the building and function (air conditioning, ventilation, smoke extract, electrics, lighting etc.). The structural design of a modern building can be extremely complex, and often requires a large team to complete.
Structural engineering specialties for buildings include:
Earthquake engineering structures.
Earthquake engineering structures are those engineered to withstand earthquakes.
The main objectives of earthquake engineering are to understand the interaction of structures with the shaking ground, foresee the consequences of possible earthquakes, and design and construct the structures to perform during an earthquake.
Earthquake-proof structures are not necessarily extremely strong like the El Castillo pyramid at Chichen Itza shown above. In fact, many structures considered strong may in fact be stiff, which can result in poor seismic performance.
One important tool of earthquake engineering is base isolation, which allows the base of a structure to move freely with the ground.
Civil engineering structures.
Civil structural engineering includes all structural engineering related to the built environment. It includes:
The structural engineer is the lead designer on these structures, and often the sole designer. In the design of structures such as these, structural safety is of paramount importance (in the UK, designs for dams, nuclear power stations and bridges must be signed off by a chartered engineer).
Civil engineering structures are often subjected to very extreme forces, such as large variations in temperature, dynamic loads such as waves or traffic, or high pressures from water or compressed gases. They are also often constructed in corrosive environments, such as at sea, in industrial facilities or below ground.
Mechanical structures.
Principles of structural engineering are applied to variety of mechanical (moveable) structures. The design of static structures assumes they always have the same geometry (in fact, so-called static structures can move significantly, and structural engineering design must take this into account where necessary), but the design of moveable or moving structures must account for fatigue, variation in the method in which load is resisted and significant deflections of structures.
The forces which parts of a machine are subjected to can vary significantly, and can do so at a great rate. The forces which a boat or aircraft are subjected to vary enormously and will do so thousands of times over the structure's lifetime. The structural design must ensure that such structures are able to endure such loading for their entire design life without failing.
These works can require mechanical structural engineering:
Aerospace structures.
Aerospace structure types include launch vehicles, (Atlas, Delta, Titan), missiles (ALCM, Harpoon), Hypersonic vehicles (Space Shuttle), military aircraft (F-16, F-18) and commercial aircraft (Boeing 777, MD-11). Aerospace structures typically consist of thin plates with stiffeners for the external surfaces, bulkheads and frames to support the shape and fasteners such as welds, rivets, screws and bolts to hold the components together.
Nanoscale structures.
A nanostructure is an object of intermediate size between molecular and microscopic (micrometer-sized) structures. In describing nanostructures it is necessary to differentiate between the number of dimensions on the nanoscale. Nanotextured surfaces have one dimension on the nanoscale, i.e., only the thickness of the surface of an object is between 0.1 and 100 nm. Nanotubes have two dimensions on the nanoscale, i.e., the diameter of the tube is between 0.1 and 100 nm; its length could be much greater. Finally, spherical nanoparticles have three dimensions on the nanoscale, i.e., the particle is between 0.1 and 100 nm in each spatial dimension. The terms nanoparticles and ultrafine particles (UFP) often are used synonymously although UFP can reach into the micrometre range. The term 'nanostructure' is often used when referring to magnetic technology.
Structural Engineering for Medical Science.
Medical equipment (also known as armamentarium) is designed to aid in the diagnosis, monitoring or treatment of medical conditions. There are several basic types: Diagnostic equipment includes medical imaging machines, used to aid in diagnosis ; equipment includes infusion pumps, medical lasers and LASIK surgical machines ; Medical monitors allow medical staff to measure a patient's medical state. Monitors may measure patient vital signs and other parameters including ECG, EEG, blood pressure, and dissolved gases in the blood ; Diagnostic Medical Equipment may also be used in the home for certain purposes, e.g. for the control of diabetes mellitus. A biomedical equipment technician (BMET) is a vital component of the healthcare delivery system. Employed primarily by hospitals, BMETs are the people responsible for maintaining a facility's medical equipment.
Structural elements.
Any structure is essentially made up of only a small number of different types of elements:
Many of these elements can be classified according to form (straight, plane / curve) and dimensionality (one-dimensional / two-dimensional):
Columns.
Columns are elements that carry only axial force - compression - or both axial force and bending (which is technically called a beam-column but practically, just a column). The design of a column must check the axial capacity of the element, and the buckling capacity.
The buckling capacity is the capacity of the element to withstand the propensity to buckle. Its capacity depends upon its geometry, material, and the effective length of the column, which depends upon the restraint conditions at the top and bottom of the column. The effective length is formula_1 where formula_2 is the real length of the column and K is the factor dependent on the restraint conditions.
The capacity of a column to carry axial load depends on the degree of bending it is subjected to, and vice versa. This is represented on an interaction chart and is a complex non-linear relationship.
Beams.
A beam may be defined as an element in which one dimension is much greater than the other two and the applied loads are usually normal to the main axis of the element. Beams and columns are called line elements and are often represented by simple lines in structural modeling.
Beams are elements which carry pure bending only. Bending causes one part of the section of a beam (divided along its length) to go into compression and the other part into tension. The compression part must be designed to resist buckling and crushing, while the tension part must be able to adequately resist the tension.
Trusses.
A truss is a structure comprising two types of structural elements; compression members and tension members (i.e. struts and ties). Most trusses use gusset plates to connect intersecting elements. Gusset plates are relatively flexible and minimize bending moments at the connections, thus allowing the truss members to carry primarily tension or compression.
Trusses are usually utilised in large-span structures, where it would be uneconomical to use solid beams.
Plates.
Plates carry bending in two directions. A concrete flat slab is an example of a plate. Plates are understood by using continuum mechanics, but due to the complexity involved they are most often designed using a codified empirical approach, or computer analysis.
They can also be designed with yield line theory, where an assumed collapse mechanism is analysed to give an upper bound on the collapse load (see Plasticity). This technique is used in practice but because the method provides an upper-bound, i.e. an unsafe prediction of the collapse load, for poorly conceived collapse mechanisms great care is needed to ensure that the assumed collapse mechanism is realistic.
Shells.
Shells derive their strength from their form, and carry forces in compression in two directions. A dome is an example of a shell. They can be designed by making a hanging-chain model, which will act as a catenary in pure tension, and inverting the form to achieve pure compression.
Arches.
Arches carry forces in compression in one direction only, which is why it is appropriate to build arches out of masonry. They are designed by ensuring that the line of thrust of the force remains within the depth of the arch. It is mainly used to increase the bountifulness of any structure.
Catenaries.
Catenaries derive their strength from their form, and carry transverse forces in pure tension by deflecting (just as a tightrope will sag when someone walks on it). They are almost always cable or fabric structures. A fabric structure acts as a catenary in two directions.
Structural engineering theory.
Structural engineering depends upon a detailed knowledge of applied mechanics, materials science and applied mathematics to understand and predict how structures support and resist self-weight and imposed loads. To apply the knowledge successfully a structural engineer generally requires detailed knowledge of relevant empirical and theoretical design codes, the techniques of structural analysis, as well as some knowledge of the corrosion resistance of the materials and structures, especially when those structures are exposed to the external environment. Since the 1990s, specialist software has become available to aid in the design of structures, with the functionality to assist in the drawing, analyzing and designing of structures with maximum precision; examples include AutoCAD, StaadPro, ETABS, Prokon, Revit Structure etc. Such software may also take into consideration environmental loads, such as from earthquakes and winds.
Materials.
Structural engineering depends on the knowledge of materials and their properties, in order to understand how different materials support and resist loads.
Common structural materials are:

</doc>
<doc id="45831" url="https://en.wikipedia.org/wiki?curid=45831" title="Tetanus">
Tetanus

Tetanus, also known as lockjaw, is an infection characterized by muscle spasms. --> In the most common type the spasms begin in the jaw and then progress to the rest of the body. --> These spasms usually last a few minutes each time and occur frequently for three to four weeks. Spasms may be so severe that bone fractures may occur. Other symptoms may include fever, sweating, headache, trouble swallowing, high blood pressure, and a fast heart rate. Onset of symptoms is typically three to twenty-one days following infection. --> It may take months to recover. --> About 10% of those infected die.
Tetanus is caused by an infection with the bacterium "Clostridium tetani", which is commonly found in soil, dust and manure. The bacteria generally enter through a break in the skin such as a cut or puncture wound by a contaminated object. They produce toxins that interfere with muscle contractions, resulting in the typical symptoms. Diagnosis is based on the presenting signs and symptoms. --> The disease does not spread between people.
Infection can be prevented by proper immunization with the tetanus vaccine. --> In those who have a significant wound and less than three doses of the vaccine both immunization and tetanus immune globulin are recommended. --> In those who are infected tetanus immune globulin or, if it is not available, intravenous immunoglobulin (IVIG) is used. --> The wound should be cleaned and any dead tissue should be removed. Muscle relaxants may be used to control spasms. --> Mechanical ventilation may be required if a person's breathing is affected.
Tetanus occurs in all parts of the world but is most frequent in hot and wet climates where the soil contains a lot of organic matter. In 2013 it caused about 59,000 deaths – down from 356,000 in 1990. Description of the disease by Hippocrates exists from at least as far back as the 5th century BC. --> The cause of the disease was determined in 1884 by Antonio Carle and Giorgio Rattone at the University of Turin, with a vaccine being developed in 1924.
Signs and symptoms.
Tetanus often begins with mild spasms in the jaw muscles—also known as lockjaw or trismus. The spasms can also affect the facial muscles resulting in an appearance called risus sardonicus. Chest, neck, back, abdominal muscles, and buttocks may be affected. Back muscle spasms often cause arching, called opisthotonos. Sometimes the spasms affect muscles that help with breathing, which can lead to breathing problems.
Prolonged muscular action causes sudden, powerful, and painful contractions of muscle groups, which is called "tetany". These episodes can cause fractures and muscle tears. Other symptoms include drooling, excessive sweating, fever, hand or foot spasms, irritability, difficulty swallowing, suffocation, heart attack, breathing problems, irregular heartbeat, and uncontrolled urination or defecation.
Even with treatment, about 10% of people who contract tetanus die. The mortality rate is higher in unvaccinated people and people over 60 years of age.
Incubation period.
The incubation period of tetanus may be up to several months, but is usually about eight days. In general, the farther the injury site is from the central nervous system, the longer the incubation period. The shorter the incubation period, the more severe the symptoms. In neonatal tetanus, symptoms usually appear from 4 to 14 days after birth, averaging about 7 days. On the basis of clinical findings, four different forms of tetanus have been described.
Generalized tetanus.
Generalized tetanus is the most common type of tetanus, representing about 80% of cases. The generalized form usually presents with a descending pattern. The first sign is trismus, or lockjaw, and the facial spasms called risus sardonicus, followed by stiffness of the neck, difficulty in swallowing, and rigidity of pectoral and calf muscles. Other symptoms include elevated temperature, sweating, elevated blood pressure, and episodic rapid heart rate. Spasms may occur frequently and last for several minutes with the body shaped into a characteristic form called opisthotonos. Spasms continue for up to four weeks, and complete recovery may take months.
Sympathetic overactivity (SOA) is common in severe tetanus and manifests as labile hypertension, tachycardia, dysrhythmia, peripheral vasculature constriction, profuse sweating, fever, increased carbon dioxide output, increased catecholamine excretion and late development of hypotension.
Death can occur within four days.
Neonatal tetanus.
Neonatal tetanus is a form of generalized tetanus that occurs in newborns, usually those born to mothers who themselves have not been vaccinated. If the mother has been vaccinated against tetanus, the infants acquire passive immunity and are thus protected. It usually occurs through infection of the unhealed umbilical stump, particularly when the stump is cut with a non-sterile instrument. As of 1998 neonatal tetanus was common in many developing countries and was responsible for about 14% (215,000) of all neonatal deaths. In 2010 the worldwide death toll was 58,000 newborns. As the result of a public health campaign, the death toll from neonatal tetanus was reduced by 90% between 1990 and 2010, and by 2013 the disease had been largely eliminated from all but 25 countries. Neonatal tetanus is rare in developed countries.
Local tetanus.
Local tetanus is an uncommon form of the disease, in which patients have persistent contraction of muscles in the same anatomic area as the injury. The contractions may persist for many weeks before gradually subsiding. Local tetanus is generally milder; only about 1% of cases are fatal, but it may precede the onset of generalized tetanus.
Cephalic tetanus.
Cephalic tetanus is the rarest form of the disease (0.9–3% of cases) and is limited to muscles and nerves in the head. It usually occurs after trauma to the head area, including skull fracture, laceration, eye injury, dental extraction, and otitis media, but it has been observed from injuries to other parts of the body. Paralysis of the facial nerve is most frequently implicated, which may cause lockjaw, facial palsy, or ptosis, but other cranial nerves can also be affected. Cephalic tetanus may progress to a more generalized form of the disease. Due to its rarity, clinicians may be unfamiliar with the clinical presentation and may not suspect tetanus as the illness. Treatment can be complicated as symptoms may be concurrent with the initial injury that caused the infection. Cephalic tetanus is more likely than other forms of tetanus to be fatal, with the progression to generalized tetanus carrying a 15–30% case fatality rate.
Cause.
Tetanus is caused by the tetanus bacterium "Clostridium tetani". Tetanus is often associated with rust, especially rusty nails. Objects that accumulate rust are often found outdoors, or in places that harbour anaerobic bacteria, but the rust itself does not cause tetanus nor does it contain more "C. tetani" bacteria. The rough surface of rusty metal merely provides a prime habitat for "C. tetani" endospores to reside in (due to its high surface area), and the nail affords a means to puncture skin and deliver endospores deep within the body at the site of the wound.
An endospore is a non-metabolizing survival structure that begins to metabolize and cause infection once in an adequate environment. Because "C. tetani" is an anaerobic bacterium, it and its endospores thrive in environments that lack oxygen. Hence, stepping on a nail (rusty or not) may result in a tetanus infection, as the low-oxygen (anaerobic) environment is caused by the oxidization of the same object that causes a puncture wound, delivering endospores to a suitable environment for growth.
Tetanus is an international health problem, as "C. tetani" spores are ubiquitous. The disease occurs almost exclusively in persons unvaccinated or inadequately immunized. It is more common in hot, damp climates with soil rich in organic matter. This is particularly true with manure-treated soils, as the spores are widely distributed in the intestines and feces of many animals such as horses, sheep, cattle, dogs, cats, rats, guinea pigs, and chickens. Spores can be introduced into the body through puncture wounds. In agricultural areas, a significant number of human adults may harbor the organism. The spores can also be found on skin surfaces and in contaminated heroin. Heroin users, particularly those that inject the drug subcutaneously, appear to be at high risk of contracting tetanus.
Pathophysiology.
Tetanus affects skeletal muscle, a type of striated muscle used in voluntary movement. The other type of striated muscle, cardiac, or heart muscle, cannot be tetanized because of its intrinsic electrical properties.
The tetanus toxin initially binds to peripheral nerve terminals. It is transported within the axon and across synaptic junctions until it reaches the central nervous system. There it becomes rapidly fixed to gangliosides at the presynaptic inhibitory motor nerve endings, and is taken up into the axon by endocytosis. The effect of the toxin is to block the release of inhibitory neurotransmitters glycine and gamma-aminobutyric acid (GABA) across the synaptic cleft, which is required to check the nervous impulse. If nervous impulses cannot be checked by normal inhibitory mechanisms, the generalized muscular spasms characteristic of tetanus are produced. The toxin appears to act by selective cleavage of a protein component of synaptic vesicles, synaptobrevin II, and this prevents the release of neurotransmitters by the cells.
Diagnosis.
There are currently no blood tests for diagnosing tetanus. The diagnosis is based on the presentation of tetanus symptoms and does not depend upon isolation of the bacterium, which is recovered from the wound in only 30% of cases and can be isolated from patients without tetanus. Laboratory identification of "C. tetani" can be demonstrated only by production of tetanospasmin in mice. Having recently experienced head trauma may indicate cephalic tetanus if no other diagnosis has been made.
The "spatula test" is a clinical test for tetanus that involves touching the posterior pharyngeal wall with a soft-tipped instrument and observing the effect. A positive test result is the involuntary contraction of the jaw (biting down on the "spatula") and a negative test result would normally be a gag reflex attempting to expel the foreign object. A short report in "The American Journal of Tropical Medicine and Hygiene" states that, in a patient research study, the spatula test had a high specificity (zero false-positive test results) and a high sensitivity (94% of infected patients produced a positive test).
Prevention.
Unlike many infectious diseases, recovery from naturally acquired tetanus does not usually result in immunity to tetanus. This is due to the extreme potency of the tetanospasmin toxin. Even a lethal dose of tetanospasmin is insufficient to provoke an immune response.
Tetanus can be prevented by vaccination with tetanus toxoid. The CDC recommends that adults receive a booster vaccine every ten years, and standard care practice in many places is to give the booster to any patient with a puncture wound who is uncertain of when he or she was last vaccinated, or if he or she has had fewer than three lifetime doses of the vaccine. The booster may not prevent a potentially fatal case of tetanus from the current wound, however, as it can take up to two weeks for tetanus antibodies to form.
In children under the age of seven, the tetanus vaccine is often administered as a combined vaccine, DPT/DTaP vaccine, which also includes vaccines against diphtheria and pertussis. For adults and children over seven, the Td vaccine (tetanus and diphtheria) or Tdap (tetanus, diphtheria, and acellular pertussis) is commonly used.
The World Health Organisation certifies countries as having eliminated maternal or neonatal tetanus. Certification requires at least two years of rates of less than 1 case per 1000 live births. In 1998 in Uganda, 3,433 tetanus cases were recorded in newborn babies; of these, 2,403 died. After a major public health effort, Uganda in 2011 was certified as having eliminated tetanus.
Post-exposure prophylaxis.
Tetanus toxoid can be given in case of a suspected exposure to tetanus. In such cases, it can be given with or without tetanus immunoglobulin (also called "tetanus antibodies" or "tetanus antitoxin"). It can be given as intravenous therapy or by intramuscular injection.
The guidelines for such events in the United States for non-pregnant people 11 years and older are as follows:
Treatment.
Mild tetanus.
Mild cases of tetanus can be treated with: 
Severe tetanus.
Severe cases will require admission to intensive care. In addition to the measures listed above for mild tetanus:
Drugs such as diazepam or other muscle relaxants can be given to control the muscle spasms. In extreme cases it may be necessary to paralyze the patient with curare-like drugs and use a mechanical ventilator.
In order to survive a tetanus infection, the maintenance of an airway and proper nutrition are required. An intake of 3,500 to 4,000 calories and at least 150 g of protein per day is often given in liquid form through a tube directly into the stomach (percutaneous endoscopic gastrostomy), or through a drip into a vein (parenteral nutrition). This high-caloric diet maintenance is required because of the increased metabolic strain brought on by the increased muscle activity. Full recovery takes 4 to 6 weeks because the body must regenerate destroyed nerve axon terminals.
Epidemiology.
[[File:Tetanus world map - DALY - WHO2004.svg|thumb|Disability-adjusted life year for tetanus per 100,000 inhabitants in 2004.
In 2013 it caused about 59,000 deaths – down from 356,000 in 1990. Tetanus – in particular, the neonatal form – remains a significant public health problem in non-industrialized countries with 59,000 newborns worldwide dying in 2008 as a result of neonatal tetanus. In the United States, from 2000 through 2007 an average of 31 cases were reported per year. Nearly all of the cases in the United States occur in unimmunized individuals or individuals who have allowed their inoculations to lapse.
History.
Tetanus was well known to ancient people who recognized the relationship between wounds and fatal muscle spasms. In 1884, Arthur Nicolaier isolated the strychnine-like toxin of tetanus from free-living, anaerobic soil bacteria. The etiology of the disease was further elucidated in 1884 by Antonio Carle and Giorgio Rattone, two pathologists of the University of Turin, who demonstrated the transmissibility of tetanus for the first time. They produced tetanus in rabbits by injecting pus from a patient with fatal tetanus into their sciatic nerves.
In 1891, "C. tetani" was isolated from a human victim by Kitasato Shibasaburō, who later showed that the organism could produce disease when injected into animals, and that the toxin could be neutralized by specific antibodies. In 1897, Edmond Nocard showed that tetanus antitoxin induced passive immunity in humans, and could be used for prophylaxis and treatment. Tetanus toxoid vaccine was developed by P. Descombey in 1924, and was widely used to prevent tetanus induced by battle wounds during World War II.
Etymology.
The word tetanus comes from the , which is further from the .

</doc>
<doc id="45832" url="https://en.wikipedia.org/wiki?curid=45832" title="Gyrocompass">
Gyrocompass

A gyrocompass is a type of non-magnetic compass which is based on a fast-spinning disc and rotation of the Earth (or another planetary body if used elsewhere in the universe) to automatically find geographical direction. Although one important component of a gyrocompass is a gyroscope, these are not the same devices; a gyrocompass is built to use the effect of gyroscopic precession, which is a distinctive aspect of the general gyroscopic effect. Gyrocompasses are widely used for navigation on ships, because they have two significant advantages over magnetic compasses:
Operation.
A gyroscope, not to be confused with gyrocompass, is a spinning wheel mounted on a gimbal so that the wheel's axis is free to orient itself in any way. When it is spun up to speed with its axis pointing in some direction, due to the law of conservation of angular momentum, such a wheel will normally maintain its original orientation to a fixed point in outer space (not to a fixed point on Earth). Since our planet rotates, it appears to a stationary observer on Earth that a gyroscope's axis is completing a full rotation once every 24 hours. Such a rotating gyroscope is used for navigation in some cases, for example on aircraft, where it is known as heading indicator, but cannot ordinarily be used for long-term marine navigation. The crucial additional ingredient needed to turn a gyroscope into a gyrocompass, so it would automatically position to true north, is some mechanism that results in an application of torque whenever the compass's axis is not pointing north.
One method uses friction to apply the needed torque: the gyroscope in a gyrocompass is not completely free to reorient itself; if for instance a device connected to the axis is immersed in a viscous fluid, then that fluid will resist reorientation of the axis. This friction force caused by the fluid results in a torque acting on the axis, causing the axis to turn in a direction orthogonal to the torque (that is, to precess) along a line of longitude. Once the axis points toward the celestial pole, it will appear to be stationary and won't experience any more frictional forces. This is because true north is the only direction for which the gyroscope can remain on the surface of the earth and not be required to change. This axis orientation is considered to be a point of minimum potential energy.
Another, more practical, method is to use weights to force the axis of the compass to remain horizontal (perpendicular to the direction of the center of the Earth), but otherwise allow it to rotate freely within the horizontal plane. In this case, gravity will apply a torque forcing the compass's axis toward true north. Because the weights will confine the compass's axis to be horizontal with respect to the Earth's surface, the axis can never align with the Earth's axis (except on the Equator) and must realign itself as the Earth rotates. But with respect to the Earth's surface, the compass will appear to be stationary and pointing along the Earth's surface toward the true North Pole.
Since the gyrocompass's north-seeking function depends on the rotation around the axis of the Earth that causes torque-induced gyroscopic precession, it will not orient itself correctly to true north if it is moved very fast in an east to west direction, thus negating the Earth's rotation. However, aircraft commonly use heading indicators or directional gyros, which are not gyrocompasses and do not position themselves to north via precession, but are periodically aligned manually to magnetic north.
Mathematical model of a gyrocompass.
We will consider here a gyrocompass, as a gyroscope which is free to rotate about one of its symmetry axis, and the whole rotating gyroscope is also free to rotate on the horizontal plane, about the local vertical, the zenith. Therefore there are two independent local rotations. In addition to these rotations we will also consider the rotation of the Earth about its North-South (NS) axis, and we will model the planet as a perfect sphere. We will neglect friction and the rotation of the Earth about the Sun.
In this case a non-rotating observer located at the center of the Earth can be approximated as being an inertial frame. We can set cartesian coordinates formula_1 for such an observer (that we will name as 1-O), and the barycenter of the gyroscope will be located at a distance formula_2 from the center of the Earth.
First time-dependent rotation.
Let us consider another (non-inertial) observer (the 2-O) located at the center of the Earth but rotating about the NS-axis by formula_3, then we set coordinates attached to the observer as
so that the unit formula_5 versor formula_6 is mapped to the point formula_7. For the 2-O the Earth is not moving so as the barycenter of the gyroscope. The rotation of 2-O, according to 1-O, is performed with angular velocity formula_8. We will suppose that the formula_9 axis denotes points with zero longitude.
Second and third fixed rotations.
We will now rotate about the formula_10 axis, so that the formula_11-axis will have the longitude of the barycenter. In this case we have
With the next rotation (about the axis formula_13 of an angle formula_14, the co-latitude) we will bring the formula_15 axis along the local zenith (formula_16-axis) of the barycenter. This can be achieved by the following orthogonal matrix (with unit determinant)
so that the formula_18 versor formula_19 is mapped to the point formula_20.
Constant translation.
We now choose another coordinate basis whose origin is located at the barycenter of the gyroscope. This can be performed by the following translation along the zenith axis
so that the origin of the new system, formula_22 is located at the point formula_23, and formula_2 is the radius of the Earth. Now the formula_25-axis points towards the south direction.
Fourth time-dependent rotation.
Now we rotate about the zenith formula_26-axis so that the new coordinate system is attached to the structure of the gyroscope, so that for an observer at rest in this coordinate system, the gyrocompass is only rotating about its own axis of symmetry. In this case we find
The axis of symmetry of the gyrocompass is now along the formula_28-axis.
Last time-dependent rotation.
The last rotation is a rotation on the axis of symmetry of the gyroscope as in
Dynamics of the system.
Since the gyroscope is not moving the height of its barycenter (and the origin of the coordinate system is located at this same point), its gravitational potential energy is constant. Therefore its Lagrangian formula_30 corresponds to its kinetic energy formula_31 only. We have
where formula_33 is the mass of the gyroscope, formula_34 is the squared inertial speed of the origin of the coordinates of the final coordinate system (i.e. the center of mass). This constant term does not affect the dynamics of the gyroscope and it can be neglected. On the other hand, the tensor of inertia is given by
and
Therefore we find
The Lagrangian can be rewritten as
where
is the part of the Lagrangian responsible for the dynamics of the system. Then, since formula_40, we find
Since the angular momentum formula_42 of the gyrocompass is given by formula_43, we see that the constant formula_44, is the component of the angular momentum about the axis of symmetry. Furthermore, we find the equation of motion for the variable formula_45 as
or
Particular case: the poles.
At the poles we find formula_48, and the equations of motion become
This simple solution implies that the gyroscope is uniformly rotating with constant angular velocity in both the vertical and symmetrical axis.
The general and physically relevant case.
Let us suppose, now that formula_50, and that formula_51, that is the axis of the gyroscope is approximately along the North-South line, and let us find the parameter space (if it exists), for which the system admits stable small oscillations about this same line. If this situation occurs, the gyroscope will be always approximately aligned along the North-South line, giving direction. In this case we find
Let us consider the case that
and, further, we allow for fast gyro-rotations, that is
Therefore, for fast spinning rotations, formula_55 implies formula_56. In this case, the equations of motion further simplify to
Therefore we find small oscillations about the North-South line, as formula_58, where the angular velocity of this harmonic motion of the axis of symmetry of the gyrocompass about the North-South line is given by
which corresponds to a period for the oscillations given by
Therefore formula_61 is proportional to the geometric mean of the Earth and spinning angular velocities. In order to have small oscillations we have required formula_62, so that the North is located along the right-hand-rule direction of the spinning axis, that is along the negative direction of the formula_63-axis, the axis of symmetry. As a side result, on measuring formula_64 (and knowing formula_65), one can deduce the local colatitude formula_66.
History.
The first, not yet practical, form of gyrocompass was patented in 1885 by Marinus Gerardus van den Bos. Usable gyrocompass was invented in 1906 in Germany by Hermann Anschütz-Kaempfe, and after successful tests in 1908 became widely used in German Imperial Navy.
The gyrocompass was an important invention for nautical navigation because it allowed accurate determination of a vessel’s location at all times regardless of the vessel’s motion, the weather and the amount of steel used in the construction of the ship. In the United States, Elmer Ambrose Sperry produced a workable gyrocompass system (1908: patent #1,242,065), and founded the Sperry Gyroscope Company. The unit was adopted by the U.S. Navy (1911), and played a major role in World War I. The Navy also began using Sperry's "Metal Mike": the first gyroscope-guided autopilot steering system. In the following decades, these and other Sperry devices were adopted by steamships such as the RMS Queen Mary, airplanes, and the warships of World War II. After his death in 1930, the Navy named the USS Sperry after him.
Meanwhile, in 1913, C. Plath (a Hamburg, Germany-based manufacturer of navigational equipment including sextants and magnetic compasses) developed the first gyrocompass to be installed on a commercial vessel. C. Plath sold many gyrocompasses to the Weems’ School for Navigation in Annapolis, MD, and soon the founders of each organization formed an alliance and became Weems & Plath.
Before the success of gyrocompass, several attempts had been made in Europe to use gyroscope instead. By 1880, William Thomson (lord Kelvin) tried to propose a gyrostat (tope) to the British Navy. In 1889, Arthur Krebs adapted an electric motor to the Dumoulin-Froment marine gyroscope, for the French Navy. That gave the "Gymnote" submarine the ability to keep a straight line while underwater for several hours, and it allowed her to force a naval block in 1890.
Errors.
A gyrocompass is subject to certain errors. These include streaming error, where rapid changes in course, speed and latitude cause deviation before the gyro can adjust itself. On most modern ships the GPS or other navigational aids feed data to the gyrocompass allowing a small computer to apply a correction.
Alternatively a design based on a strapdown architecture (including a triad of fibre optic gyroscope, ring laser gyroscopes or Hemispherical resonator gyroscope and a triad of accelerometers) will eliminate these errors, as they do not depend upon mechanical parts, to determinate rate of rotation.

</doc>
<doc id="45833" url="https://en.wikipedia.org/wiki?curid=45833" title="Palatino">
Palatino

Palatino is the name of an old-style serif typeface designed by Hermann Zapf, initially released in 1948 by the Linotype foundry.
Named after 16th century Italian master of calligraphy Giambattista Palatino, Palatino is based on the humanist types of the Italian Renaissance, which mirror the letters formed by a broad nib pen; this gives a calligraphic grace. Its capital 'Y' is in the unusual 'palm Y' style, inspired by the Greek letter upsilon. But where Renaissance faces tend to have delicate proportions such as a low x-height (short lower-case letters and longer ascenders and descenders), Palatino has larger proportions, increasing legibility. It is one of several related typefaces by Zapf, each showing influence of Italian Renaissance letter forms, although Zapf was unable to visit Italy until after he had finished the Palatino roman. The group includes Palatino, Sistina, Michaelangelo Titling, and Aldus, which take (loose) inspiration from printing types cut by Francesco Griffo c. 1495 in the print shop of Aldus Manutius. Paul Shaw has described Michaelangelo, Sistina, Aldus and Kompakt, an ultra-bold display design from 1952, as "Palatino's extended family". 
Palatino was particularly intended as a design for trade or 'jobbing' use, such as headings, advertisements and display printing, and was created with a solid, wide structure and wide apertures that could appear clearly on poor-quality paper, when read at a distance or printed at small sizes. However, it became popular for book body text use, overshadowing the narrower and lighter Aldus, which Zapf had designed for this role. It has been cited as one of the ten most used serif typefaces. Since Palatino was not originally designed for body text, some of its characters were intended to stand out with quirky, calligraphic design features, and Zapf later redesigned them with more sober alternates, which have become the norm on most digital versions. Zapf retained an interest in the design, and continued to collaborate on new versions into his eighties.
Linotype licensed Palatino to Adobe and Apple who incorporated it into the PostScript digital printing technology as a standard font. This guaranteed its importance in digital and desktop publishing and made it (or a variant of it) a preinstalled font on most computers. As with many popular fonts, knockoff designs and rereleases under different names are common.
Palatino family.
Palatino was originally created by Stempel, and became part of a family of typefaces by Zapf. 
Aldus.
Zapf also designed Aldus, which appeared in the D. Stempel AG catalog in 1954. It has a more condensed design lighter in colour, more suitable for the higher quality of book printing. Zapf wanted the design to be named Palatino Buch (Palatino Book), it was instead given a name as a separate family. 
It is named for the Venetian Renaissance printer Aldus Manutius, a decision which annoyed Zapf since it bears little direct resemblance to his typefaces.
Michaelangelo.
A set of titling capitals, based on Roman square capitals. The design has a 'U' with a foot serif at bottom right, a 'double-V'-style 'W' with four top terminals and a 'palm Y' similar to that on Palatino, inspired by the Greek letter upsilon. It was renamed "Palatino Titling" in the Palatino nova release (see below), since the rights to the name were held by Berthold who also publish a digitisation.
Sistina.
A slightly bolder set of titling capitals than Michaelangelo on the same basic structure. It was originally named 'Aurelia Titling' after the Roman road named Via Aurelia; Zapf would later use the name for another separate font. The Palatino nova version (see below) is renamed "Palatino Imperial" and has small capitals as a lower case. It was created following an artistically productive 1950 visit to Italy, which Zapf had been unable to visit before. Zapf was very interested in the quality of Italian art and lettering, and his sketches of stonecarving in Florence also inspired the humanist sans-serif Optima. 
Kompakt.
An ultra-bold display type, with a slight slope but roman rather than italic letter forms, somewhat similar to blackletter. Unlike Palatino, it is very unlike the style of roman type printing used during the Renaissance, which did not use bold type.
Zapf Renaissance Antiqua.
A separate interpretation by Zapf of the same Renaissance models, initially created for Scangraphic and later digitised and sold by Linotype along with Palatino. Its digitisation is based on the versions prepared by Scangraphic for display use, with tight spacing and striking contrasts in stroke weight. It is also notable for including a full set of swash caps, something not included on digital versions of Palatino.
Variants and similar typefaces.
Digitisations.
Palatino's early digitisation intended for PostScript use is very widely used or cloned. Later Palatino digitisations have different features and spacing. In 1999, Zapf revised Palatino for Linotype and Microsoft, called Palatino Linotype. The revised family incorporated extended Latin, Greek, and Cyrillic character sets. Linotype released a more complex redesign named Palatino nova, together with digitisations of some of Zapf's other Renaissance-inspired designs and Aldus. Zapf also created a matching Palatino Sans and Palatino Sans Informal design in 2006.
Palatino Linotype.
Palatino Linotype is the version of the Palatino family included with modern versions of Microsoft software. It incorporates extended Latin, Greek, Cyrillic characters, as well as currency signs, subscripts and superscripts, and fractions. The family includes roman and italic in text and bold weights. 
Palatino Linotype was notable as being the first western OpenType font that Microsoft shipped; Palatino Linotype was bundled with Windows 2000. The OpenType version showcased some (then new) alternate features features, including ligatures, true small caps, proportional and tabular figures, text figures and a variety of special alternate characters, such as the swash Capital Qu combination. This marks it out from earlier digitisations such as the OS X system version, which do not include ligatures such as "Th" and "Qu". On release it was one of the few fonts to incorporate an interrobang.
Palatino nova.
Palatino nova is a redesigned version of Palatino, by Hermann Zapf and Akira Kobayashi. This Palatino nova typeface family includes roman and italics in the light, text, medium, and bold weights, a new release of Aldus and versions of Michelangelo and Sistina under the name of "Palatino Titling" and "Palatino Imperial".
The font family was premiered on 2005-11-24, the same day as Hermann Zapf’s 87th birthday celebration. A new digitisation of Aldus named Aldus nova was created at the same time.
Palatino Sans.
Palatino Sans is a sans-serif design with stroke width modulation, resembling Zapf's classic design Optima but with a softer, more organic feel. Unlike the serifed counterpart, the Sans families do not have full Greek or Cyrillic characters. Reviewing it for "Typographica" on release, font designer Hrant Papazian commented:
The confluence of competence, freedom and kiai...evident in Palatino Sans is breathtaking. The sober organicity, the bravado of the raised ‘r’, the confident flair of the italic; all done before, but never in such a usable, contemporary whole.
Palatino Sans Informal.
Palatino Sans Informal incorporates informal characteristics to the Palatino Sans, such as asymmetrical A, K, N, W, X, Y, w.
Palatino Arabic.
It is a family designed by Lebanese designer Nadine Chahine and Hermann Zapf. The design is based on the Al-Ahram typeface designed by Zapf in 1956 but reworked and modified to fit the Palatino nova family. The design is Naskh in style but with a strong influence of Thuluth style.
This family only comes in 1 font, with the arabic characters based on Palatino nova Regular. It supports basic Latin, Arabic, Persian, and Urdu scripts. Chahine also created a version of Zapf's Zapfino.
Palatino eText (2013).
It is a family designed by Toshi Omagari of Monotype Imaging, optimised for on-screen use. It includes a larger x-height and wider spacing. It is the standard four-font family, with bolds and italics.
Palatino clones.
As one of the most iconic typefaces of the twentieth century, derivative designs based on Palatino were rapidly developed, taking advantage of the lack of practical copyright and the easy copying possible in the phototypesetting font market of the 1960s and 70s onwards. Many of these are almost indistinguishable from Palatino, and some even had Zapf's involvement as a consultant.
Palazzo Original.
Softmaker's clone of Palatino, Palazzo, is unique for being based on the original metal type of Palatino: as a result, it contains many design features not seen in the digital versions of Palatino endorsed by Zapf and most clones. These include a 'p' and 'q' without foot serif and no serif on the centre stroke of the 'E' and 'F', as well as a slightly more delicate design with a lower x-height. It has also been released as 'Marathon Serial'.
PostScript clones.
Most modern Palatino clones are set to match the spacing and design of the PostScript version of Palatino that was a standard font in early digital publishing. In the Bitstream font collection, the Palatino equivalent is called "Zapf Calligraphic."
URW++ sells its version as "URW Palladio L." A version of this font was later released by URW under a free and open source licence as part of the Ghostscript project to develop an open-source alternative to PostScript. As a result, it (or a derivative) is used by much open-source software such as R as a system font.
Book Antiqua.
One of the best-known Palatino PostScript clones is "Book Antiqua" (originally by Monotype), distributed with much Microsoft software. It is one of many clone PostScript typefaces distributed by Microsoft and Monotype around this time, including Arial (a clone of Helvetica), Century Gothic (ITC Avant Garde) and Bookman Old Style (ITC Bookman). Book Antiqua resembles Palatino extremely closely and is almost indistinguishable from the original apart from a few detail differences. 
In 1993, Zapf resigned from l'Association Typographique Internationale (ATypI) over what he viewed as its hypocritical attitude toward unauthorized copying by prominent ATypI members (namely Monotype). In the United States, the abstract design of a typeface is not protected by copyright, and can be imitated freely (unless the typeface is protected by a design patent, which is of much more limited duration and rarely applied for). Copyright protection is available for the representation of a typeface in software (a computer font), and the names of typefaces can be protected by trademark.
Microsoft has since licensed and distributes Linotype's version of Zapf's original design called "Palatino Linotype" in all versions of Windows since Windows 2000. During the Palatino Linotype development process, Zapf and Linotype requested that Microsoft cease to include Book Antiqua with Office, but Microsoft concluded that this was impossible as too many documents had already been created using it. A custom version of Book Antiqua was created by Monotype as a corporate font by the Parliament of the UK.
Free and open Source versions and derivatives.
The only legal free version of the typeface is "URW Palladio L." The open source community greatly extended the character sets of the fonts and releases new, updated versions under new names.
Awards.
Palatino Sans and Palatino Sans Informal won Type Directors Club Type Design Competition 2007 award under Type System / Superfamily category.
Palatino Arabic won 2008 Type Directors Club TDC2 2008 award under Text / Type Family category.

</doc>
<doc id="45834" url="https://en.wikipedia.org/wiki?curid=45834" title="Optima">
Optima

Optima is a humanist sans-serif typeface designed by Hermann Zapf and released between 1952 and 1955 for the D. Stempel AG foundry, Frankfurt, Germany.
Though classified as a sans-serif, Optima has a subtle swelling at the terminals suggesting a glyphic serif. Optima was inspired by Roman and Italian stonecarving.
Zapf intended Optima to be a typeface that could serve for both body text and titling. To prove its versatility, Zapf set his entire book "About Alphabets" in the regular weight. Zapf retained an interest in the design, collaborating on variants and expansions into his eighties.
Characteristics.
Optima’s design follows humanist lines; its capitals (like those of Palatino, Hans Eduard Meier’s Syntax and Carol Twombly's Trajan) are directly derived from the classic Roman monumental capital model, reflecting a reverence for Roman capitals as an ideal form.
Optima is an example of a modulated-stroke sans-serif, a design type where the strokes are variable in width. The design style has been intermittently popular since the late nineteenth century; Optima is one of the most lastingly popular examples of the genre. Optima was originally targeted by Stempel's Walter Cunz as a competitor to Ludwig & Mayer's Colonia design, which has not been digitised. Shaw also suggests the little-known 1948 design Romann Antiqua, as well as Stellar by Robert Hunter Middleton as predecessors, and notes the existence of Pascal by José Mendoza y Almeida (1962) as a design with a similar set of influences.
History.
Zapf began his design while visiting Italy in 1950 and examining inscriptions there, especially at the cemetery of the Basilica di Santa Croce in Florence; an early draft of the design was quickly sketched on a 1000 lira banknote.
In his book "About Alphabets", Zapf commented that his key aim, inspired by Roman alphabets, was the desire to avoid the monotony of all capital letters having a roughly square footprint, as he felt was true of some early sans-serif designs. Optima's 'E' and 'R' occupy a half-square, while the 'M' is splayed.
Zapf was inspired by combining the unserifed Italian letters with strong thick and thin contrast with the best features of the roman letters, which were all that were previously being used. Zapf began his designs for Optima in 1952, spending more than 6 years on the development of the typeface. Upon the suggestion of Monroe Wheeler of the Museum of Modern Art in New York, Zapf decided to adapt his typeface to be used as a book type. “He thereupon changed the proportions of the lowercase, and by means of photography, he tested the suitability of the design for continuous reading application.” Zapf designed the capital letters of Optima after the inscriptions on the Trajan Column (A.D. 113). Optima is the first German typeface not based on the standard baseline alignment that had been used up until this point in time. Zapf states “ This base line is too deep for a roman, as it was designed for the high x-height of the Fraktur and Textura letters. Thus, too many German types have ascenders which are too long and descenders which are too short. The proportions of Optima Roman are now in the Golden Section: lowercase x-height equalling the minor and ascenders-descenders the major. However, the curved lines of the stems of each letter result from technical considerations of type manufacturing rather than purely esthetic considerations.”
Optima was first manufactured as a foundry version in 1958 by Stempel of Frankfurt, and by Mergenthaler in America shortly thereafter. It was released to the public at an exhibition in Dusseldorf in that same year. If it had been up to Zapf, Optima would have been named New Roman, but the marketing staff insisted that it be named Optima.
Zapf wrote later in his life of his preference for Optima over all of his other typefaces, but he also mentioned “a father should not have a favorite among his daughters.”
Optima Greek (1973).
It is a Greek variant designed by Matthew Carter, based on sketches from Hermann Zapf. Digital version has not been produced.
Optima Classified (1976).
It is a variant designed by Matthew Carter, based closely on Optima Medium. Digital version has not been produced.
Optima nova (2002).
Optima nova is a redesign of the original font family, designed by Hermann Zapf and Linotype GmbH type director Akira Kobayashi. The new family contains 7 font weights, adding light, demi, and heavy font weights, but removing extra black weight. Medium weight is readjusted to between medium and bold weights in the old family scale. Glyph sets are expanded to include Adobe CE and Latin Extended characters, with light to bold weight fonts supporting proportional lining figures, old style figures, and small caps.
The initial and most common release of Optima, like many sans-serif fonts, has an oblique style instead of an italic: the shapes are merely tilted to the right. In Optima nova, this is replaced by a true italic. (In interviews, Zapf has said that this was his original goal from the beginning, but the need to release Optima quickly forced him to settle for an oblique.)
Even in roman fonts, letters such as Q, a, f are redesigned. The overall bounding boxes were widened in Optima nova.
Optima nova Condensed.
It is a condensed variant which consists of light to bold weights, but no italic fonts. The glyph set does not support proportional lining figures, old style figures, or small caps.
Optima nova Titling.
It is a titling capitals variant, which contains only capital letters, with restyled letterform. The glyph set is the same as Optima nova Condensed, but also includes extra ligatures.
In the tradition of hand lettering and lapidary inscription, the titling face shares similarities with the work of Zapf's friend Herb Lubalin, especially the exuberant ligatures (for which Lubalin's ITC Lubalin Graph and ITC Avant Garde are notable). Further influence of A.M. Cassandre and Rudolf Koch, whose work greatly inspired the young Zapf, can also be seen in Optima.
Optima Pro Cyrillic (2010).
In April 2010, Linotype announced the release of Cyrillic version of the original Optima family, in OpenType Pro font formats. Released fonts include Optima Pro Cyrillic Roman, Oblique, Bold, Bold Oblique.
Usages.
The typeface Optima is used for the Vietnam Veterans Memorial and was used by the 2008 John McCain presidential campaign. Optima is also used as the official branding typeface for Estée Lauder Companies, the University of Calgary, and Aston Martin. It is also used in the logo for banking company Desjardins.
Optima is used iconically for "Traveller", and "Diaspora" used it to pay homage to "Traveller".
Optima was used in the end credits of the 1973 horror movie "The Exorcist" as well as for the opening titles of its second sequel, "The Exorcist III".
Optima was used in the official logo and most publications associated with Expo 67 in Montreal.
Optima is used by the Mexican Social Security Institute especially in his UMF Family Medical Units.
Optima was used for lettering on Premier League kits from July 1997 until May 2007, when it was replaced by a different typeface.
Optima was used in the Taipei Metro.
Optima was used as the original fonts used on The Smiths original 7-inch single covers and their debut album.
Optima was used for the logo of American emo band Moss Icon, albeit slightly weathered.
Optima was used for the logo of Trans TV & Trans7 from December 2001 until December 2013.
Marks and Spencer used the font for its corporate logo and as the default on all internal correspondence from 2000 but since 2007 it is gradually being phased out on all signage and packaging as part of another re-branding exercise.
Optima was chosen as the font to be used for the names of those who lost their lives in the September 11 attacks, carved into bronze parapets, at the National September 11 Memorial & Museum, which is named "Reflecting Absence".
The Optima font is used in the logo of the Indian Premier League.
Optima is used in the LDS Church conferences.
Optima is used on the labels of wines from Ridge Vineyards.
Optima is used in the text of all the rules and guidance notes for the classification of ships published by Bureau Veritas.
Opinions on the design have been variable, perhaps because of its extensive use. Erik Spiekermann described it as "used on parking garages & hospitals across the USA. Tired & inappropriate. I don’t blame the typeface but the designers." He also commented "Optima is patronizing. It hasn't got the guts to be either a proper Sans or Serif, so it keeps all its options open and appeals to the middle...It suits everything and pleases nobody. Optima would indeed make a good president. Hermann the German Zapf is a fine calligrapher and has designed some pretty amazing typefaces that have been over- and badly used, which isn't his fault. But Optima shows too much of its origin: post-war Germany, the early 50s. With the country in ruins and not enough to eat, there was an understandable desire to go back to wholesome type that promised peace and harmony after 12 years of Hitler and 5 years of occupation. Optima is a well drawn face, at least in its original version. And you hardly see it in Germany. Not sure what that says about our politics."
Jonathan Hoefler commented that "after three decades signifying a very down-market notion of luxe, this particular sans serif has settled into being the font of choice for the hygiene aisle."
Derivatives.
As with many popular fonts, knockoff designs and rereleases under different names are common, some created by Zapf himself. These all tend to copy the original release, rather than the Optima nova design which represents Zapf's final thoughts on his design. In the Bitstream font collection, Zapf Humanist 601 is provided as an Optima clone. Other Optima clones include Optane from the WSI Fonts collection, Opulent by Rubicon Computer Labs Inc., Ottawa from Corel, CG Omega and Eterna. Freely available implementations include URW Classico (available with URW Font package from Ghostscript). Linux Biolinum is a libre font inspired by it. Zapf's Palatino Sans is a more informal typeface the same style, with a design reminiscent of brushstrokes or calligraphy.
In a memoir written for Linotype, Zapf commented "The name "Optima" was not my idea at all. It is for me too presumptious and was the invention of the sales people at Stempel."

</doc>
<doc id="45835" url="https://en.wikipedia.org/wiki?curid=45835" title="Hermann Zapf">
Hermann Zapf

Hermann Zapf (; November 8, 1918 – June 4, 2015) was a German typeface designer and calligrapher who lived in Darmstadt, Germany. He was married to the calligrapher and typeface designer Gudrun Zapf von Hesse. Typefaces he designed include Palatino, Optima and Zapfino.
Early life.
Zapf was born in Nuremberg during turbulent times marked by the German Revolution of 1918–1919 in Munich and Berlin, the end of World War I, the exile of Kaiser Wilhelm, and the establishment of Bavaria as a free state by Kurt Eisner. In addition, the Spanish flu pandemic took hold in Europe in 1918 and 1919. Two of Zapf's siblings died of the disease. Famine later struck Germany, and Zapf's mother was grateful to send him to school in 1925, where he received daily meals in a program organized by Herbert Hoover. In school, Zapf was mainly interested in technical subjects. One of his favorite books was the annual science journal "Das neue Universum" ("The New Universe"). He and his older brother experimented with electricity, building a crystal radio and an alarm system for his house. Even at this early age, Zapf was already getting involved with type, inventing cipher alphabets to exchange secret messages with his brother.
Zapf left school in 1933 with the ambition of pursuing a career in electrical engineering. However, his father had become unemployed and was in trouble with the newly established Third Reich, having been involved with trade unions, and was sent to the Dachau concentration camp for a short time.
Introduction to typography.
Under the new political regime, Zapf was not able to attend the Ohm Technical Institute in Nuremberg, and therefore he needed to find an apprenticeship. His teachers, aware of the new political difficulties, noticed Zapf's skill in drawing and suggested that he become a lithographer. Each company that interviewed him for an apprenticeship would ask him political questions, and every time he was interviewed, he was complimented on his work but was rejected. Ten months later, in 1934, he was interviewed by the last company in the telephone directory, and the company did not ask any political questions. They also complimented Zapf's work, but did not do lithography and did not need an apprentice lithographer. However, they allowed him to become a retoucher, and Zapf began his four-year apprenticeship in February 1934.
In 1935, Zapf attended an exhibition in Nuremberg in honor of the late typographer Rudolf Koch. This exhibition gave him his first interest in lettering. Zapf bought two books there, using them to teach himself calligraphy. He also studied examples of calligraphy in the Nuremberg city library. Soon, his master noticed his expertise in calligraphy, and Zapf's work shifted to retouching lettering and improving his colleagues' retouching.
Frankfurt.
A few days after finishing his apprenticeship, Zapf left for Frankfurt. He did not bear a journeyman's certificate and thus would not be able to get a work permit at another company in Nuremberg, as they would not have been able to check on his qualifications. Zapf went to the Werkstatt Haus zum Fürsteneck, a building run by Paul Koch, son of Rudolf Koch. He spent most of his time there working in typography and writing songbooks.
Through print historian Gustav Mori, Zapf came into contact with the type foundries D. Stempel, AG, and Linotype GmbH of Frankfurt. In 1938, he designed his first printed typeface for them, Gilgengart, a fraktur.
War service.
On April 1, 1939, Zapf was conscripted and sent to Pirmasens to help reinforce the Siegfried Line against France. As a consequence of hard labor, he developed heart trouble in a few weeks and was given a desk job, writing camp records and sports certificates in Fraktur.
World War II broke out in September, and Zapf's unit was to be taken into the Wehrmacht. However, because of his heart trouble, Zapf was not transferred to the Wehrmacht but was instead dismissed. On April 1, 1942, he was summoned again for the war effort. Zapf had been chosen for the Luftwaffe, but instead was sent to the artillery in Weimar. He did not perform well, confusing left and right during training and being too cautious and clumsy with his gun. His officers soon brought an unusually early end to his career in the artillery.
Zapf was sent back to the office and then to Jüterbog to train as a cartographer. After that, he went to Dijon and then Bordeaux, joining the staff of the First Army. In the cartography unit at Bordeaux, Zapf drew maps of Spain, especially the railway system, which could have been used to transport artillery had Francisco Franco not used narrow-gauge tracks to repair bridges after the Spanish Civil War. Zapf was happy in the cartography unit. His eyesight was so good that he could write letters 1 millimeter in height without using a magnifying glass, and this skill probably prevented him from being commissioned back into the army.
After the war had ended, Zapf was held by the French as a prisoner of war at a field hospital in Tübingen. He was treated with respect because of his artwork and, on accont of his poor health, was sent home only four weeks after the end of the war. He went back to Nuremberg, which had suffered great damage in air raids.
Postwar career.
Zapf taught calligraphy in Nuremberg in 1946. He returned to Frankfurt in 1947, where the type foundry Stempel offered him a position as artistic head of their printshop. They did not ask for qualifications, certificates, or references, but instead only required him to show them his sketchbooks from the war and a calligraphic piece he did in 1944 of Hans von Weber's "Junggesellentext".
One of Zapf's projects was the book "Feder und Stichel" ("Pen and Graver"), printed from metal plates designed by Zapf and cut by the punchcutter August Rosenberger during the war. It was printed at the Stempel printshop in 1949.
From 1948 to 1950, Zapf taught calligraphy at the Arts and Crafts School in Offenbach, giving lettering lessons twice a week to two classes of graphics students. In 1951 he married Gudrun von Hesse, who taught at the school of Städel in Frankfurt.
Most of Zapf's work as a graphic artist was in book design. He worked for various publishing houses, including Suhrkamp Verlag, Insel Verlag, Büchergilde Gutenberg, Hanser Verlag, Dr. Ludwig Reichert Verlag, and Verlag Philipp von Zabern.
Type design.
Zapf's career in type design spanned the three most recent stages of printing: hot metal composition, phototypesetting (also called cold type), and digital typesetting. His two most famous typefaces, Palatino and Optima, were designed in 1948 and 1952, respectively. Palatino was designed in conjunction with August Rosenberger, with careful attention to detail. It was named after the 16th-century Italian writing master Giambattista Palatino. It became better known after it became one of the core 35 PostScript fonts in 1984, bundled with virtually all PostScript devices from laser printers to imagesetters. Optima, a flared sans-serif, was released by Stempel in 1958. Zapf intended the design to bridge serifs and sans serifs and to be suitable for both headings and continuous passages of text.
Zapf's work reached into a range of genres. While Palatino and Optima are warm, organic designs inspired by Italian Renaissance calligraphy, printing and stonecarving, he also designed a number of serif text fonts, such as Melior, in the more austere, classical style, following the work of the great German neoclassical printer Justus Erich Walbaum. His sans serif series URW Grotesk was designed for newspaper use and presents a wide range of widths and weights, reminiscent of geometric sans serif fonts like Futura but in a more eccentric style. Several of his more geometric designs, like both of these, make use of superellipses, squarish designs incorporating a slight curve. Opinion on Zapf's later designs has not always been favourable: Maxim Zhukov remembered his contemporary Adrian Frutiger commenting, with reference to URW Grotesk, that Zapf was "not a sans-serif man" at a conference in 1990, and graphic designer Dan Margulis commented on a retrospective that "he participated in the 1980s trend toward faces with very large x-heights and tight letterfits; his major works in that genre, Zapf Book and Zapf International, have deservedly been forgotten... you would have to say that his historical standing will be based on the first ten years of his professional career."
Zapf's later releases for Linotype in the 1990s and 2000s, often created in collaboration with Akira Kobayashi, were radical reformations of his previous work, often removing compromises that had been necessary in the manufacture of metal type. In this period he created Palatino Sans, a more informal modulated sans serif than Optima.
Zapf's typefaces have been widely copied, usually but not always against his will. The best-known example may be Monotype's Book Antiqua, which was included in Microsoft Office and is often considered an imitation of Palatino. In 1993, Zapf resigned from ATypI (Association Typographique Internationale) over what he viewed as its hypocritical attitude toward unauthorized copying by prominent ATypI members. At a 1994 conference of the Raster Imaging and Digital Typography association in Darmstadt, Germany, a panel discussion on digital typefaces and designers' rights strongly criticized the alleged plagiarism of Zapf's Palatino, while several Microsoft attendees listened in the audience. In 1999, Microsoft worked with Zapf and Linotype to develop a new, authorized version of Palatino for Microsoft, called Palatino Linotype.
Sometimes, however, Zapf worked with a font maker to make new versions of his existing typefaces created for another company. For example, in the 1980s Zapf worked with Bitstream to make versions of many of his prior typefaces, including Palatino, Optima and Melior, all with "Zapf" in their new names.
Calligraphy.
Though his calligraphy is considered superb by calligraphers, Zapf did not work extensively as a calligrapher. His largest calligraphic project was the "Preamble to the United Nations Charter", written in four languages, commissioned by the Pierpont Morgan Library in 1960, for which he received $1000.
Computer typography.
Zapf worked on typography for computer programs from the 1960s onwards. His ideas were considered radical, not taken seriously in Germany, and rejected by the Darmstadt University of Technology, where he lectured from 1972 to 1981. Because he had no success in Germany, Zapf went to the United States, where he lectured about computerized typesetting, and was invited to speak at Harvard University in 1964. The University of Texas at Austin was also interested in Zapf and offered him a professorship, which he did not take, because his wife opposed a move to that state.
Because Zapf's plans for the United States had come to nothing, and because their house in Frankfurt had become too small, Zapf and his wife moved to Darmstadt in 1972.
In 1976, the Rochester Institute of Technology offered Zapf a professorship in typographic computer programming, the first of its kind in the world. He taught there from 1977 to 1987, flying between Darmstadt and Rochester. There he developed his ideas further, with the help of his connections in companies such as IBM and Xerox and his discussions with computer specialists at Rochester. A number of Zapf's students from this time at RIT went on to become influential type designers, including Kris Holmes and Charles Bigelow, who together created the Lucida type family. Other prominent students include the calligrapher and type designer Julian Waters and the book designer Jerry Kelly.
In 1977, Zapf and his friends Aaron Burns and Herb Lubalin founded Design Processing International, Inc., in New York and developed typographical computer software. It existed until 1986, when Lubalin died. Zapf and Burns founded Zapf, Burns & Company in 1987. Burns, also an expert in typeface design and typography, was in charge of marketing until his death in 1992. Shortly before, two of their employees had stolen Zapf's ideas and founded a company of their own.
Zapf knew that he could not run an American company from Darmstadt and did not want to move to New York. Instead, he used his experience to begin the development of a typesetting program, the "Hz-program", building on the hyphenation and justification system in TeX.
During financial problems and bankruptcy of URW (Type foundry, article in German) in the mid-1990s, Adobe Systems acquired the Hz patent(s) and later made some use of the concepts in their InDesign program.
Zapfino.
In 1983, Zapf completed the typeface "AMS Euler" with Donald Knuth and graduate students in Knuth's and Charles Bigelow's digital typography program at Stanford University, including students Dan Mills, Carol Twombly, and David Siegel and Knuth's computer science Ph.D. students Scott Kim and John Hobby, for the American Mathematical Society. Euler digital font production was eventually finished by Siegel as his M.S. thesis project in 1985. Euler is a typeface family for mathematical composition including Latin, fraktur and Greek letters. After Siegel finished his studies at Stanford and was interested in entering the field of typography, he told Zapf his idea of making a typeface with a large number of glyph variations and wanted to start with an example of Zapf's calligraphy, which had been reproduced in a publication of the Society of Typographic Arts in Chicago.
Zapf was concerned that this was the wrong way to go, and while he was interested in creating a complicated program, he was worried about starting something new. However, Zapf remembered a page of calligraphy from his sketchbook from 1944 and considered the possibility of making a typeface from it. He had tried to create a calligraphic typeface for Stempel in 1948, but hot metal composition placed too many limits on the freedom of swash characters. Such a pleasing result could only be achieved using modern digital technology, and so Zapf and Siegel began work on the complicated software necessary. Siegel also hired Gino Lee, a programmer from Boston, Massachusetts, to work on the project.
Unfortunately, just before the project was completed, Siegel wrote a letter to Zapf, saying that his girlfriend had left him and that he had lost all interest in anything. Thus Siegel abandoned the project and started a new life, working on bringing color to Macintosh computers and later becoming a website design expert.
The development of Zapfino had become seriously delayed until Zapf presented the project to Linotype. The company wwas prepared to complete it and reorganized the project. Zapf worked with Linotype to create four alphabets and various ornaments, flourishes, and other dingbats. Zapfino was released in 1998.
Later versions of Zapfino using the Apple Advanced Typography and OpenType technologies were able to make automatic ligatures and glyph substitutions (especially contextual ones, in which the nature of ligatures and substituted glyphs is determined by other glyphs nearby or even in different words), to more accurately reflect the fluid and dynamic nature of Zapf's calligraphy.
Death.
Zapf died on June 4, 2015, at the age of 96 in Darmstadt, Germany.
List of typefaces.
Zapf designed the following typefaces:
Appearances in film.
Zapf starred in the film "The Art of Hermann Zapf", produced in 1967 at Hallmark Cards in Kansas City, Missouri, and in Zapf's design studio in Dreieichenhain, Germany. He was also featured in the 2007 documentary "Helvetica," by Gary Hustwit.

</doc>
<doc id="45839" url="https://en.wikipedia.org/wiki?curid=45839" title="Drum machine">
Drum machine

A drum machine is an electronic musical instrument designed to imitate the sound of drums, cymbals, other percussion instruments, and often basslines. Drum machines are most commonly associated with electronic music genres such as house music, but are also used in many other genres. They are also used when session drummers are not available or if the production cannot afford the cost of a professional drummer. Also, many modern drum machines can also produce unique sounds plus it can allow the user to compose unique drum beats and patterns that might be difficult to perform with a human drummer. In the 2010s, most modern drum machines are sequencers with a sample playback (rompler) or synthesizer component that specializes in the reproduction of drum timbres.
History.
Early drum machines.
In 1930–32, the spectacularly innovative and hard to use "Rhythmicon" was realized by D-Rail at the request of Henry Cowell, who wanted an instrument which could play compositions with multiple rhythmic patterns, based on the overtone series, were far too hard to perform on existing keyboard instruments. The invention could produce sixteen different rhythms, each associated with a particular pitch, either individually or in any combination, including en masse, if desired. Received with considerable interest when it was publicly introduced in 1932, the Rhythmicon was soon set aside by Cowell and was virtually forgotten for decades. The next generation of rhythm machines played only pre-programmed rhythms such as mambo, tango, or bossa nova.
In 1957 Californian Harry Chamberlin constructed a tape loop-based drum machine called the "Chamberlin Rhythmate". It had 14 tape loops with a sliding head that allowed playback of different tracks on each piece of tape, or a blending between them. It contained a volume and a pitch/speed control and also had a separate amplifier with bass, treble, and volume controls, and an input jack for a guitar, microphone or other instrument. The tape loops were of real acoustic jazz drum kits playing different style beats, with some additions to tracks such as bongos, clave, castanets, etc.
In 1959 Wurlitzer released an electro-mechanical drum machine called the "Side Man", which was the first-ever commercially produced drum machine. The Side Man was intended as a percussive accompaniment for the Wurlitzer organ range. The Side Man offered a choice of 12 electronically generated, predefined rhythm patterns with variable tempos. The sound source was a series of vacuum tubes which created 10 preset electronic drum sounds. The drum sounds were 'sequenced' by a rotating wiper arm with contact brushes on it that swept around a phenolic panel with corresponding contacts arranged in a pattern of concentric circles across its face; these were spaced in certain patterns to generate parts of a particular rhythm. Combinations of these different sets of rhythms and drum sounds created popular rhythmic patterns of the day, e.g. waltzes, fox trots etc. These combinations were selected by a rotary knob on the top of the Side Man box. The tempo of the patterns was controlled by a slider that increased the speed of rotation of the wiper arm. The Side Man had a panel of 10 buttons for manually triggering drum sounds, and a remote player to control the machine while playing from an organ keyboard. The Side Man was housed in a mahogany cabinet that contained the sound-generating circuitry, amplifier and speaker.
In 1960, Raymond Scott constructed the "Rhythm Synthesizer" and, in 1963, a drum machine called "Bandito the Bongo Artist". Scott's machines were used for recording his album "Soothing Sounds for Baby" series (1964).
During the 1960s, implementation of rhythm machines were evolved into fully solid-state (transistorized) from early electro-mechanical with vacuum tubes, and also size were reduced to desktop size from earlier floor type. In the early 1960s, a home organ manufacturer, Gulbransen (later acquired by Fender) cooperated with an automatic musical equipment manufacturer Seeburg Corporation, and released early compact rhythm machines "Rhythm Prince" (PRP), although, at that time, these size were still as large as small guitar amp head, due to the use of bulky electro-mechanical pattern generators. Then in 1964, Seeburg invented a compact electronic rhythm pattern generator using "diode matrix" ( in 1967), — When this patent was filed in 1964-06-26, also , , and its sound circuits and were filed at the same time.
</ref> and fully transistorized electronic rhythm machine with pre-programmed patterns, "Select-A-Rhythm" (SAR1), was released. As the result of its robustness and enough compact size, these rhythm machines were gradually installed on the electronic organ as accompaniment of organists, and finally spread widely.
In the early-1960s, a nightclub owner in Tokyo, Tsutomu Katoh was consulted from Tadashi Osanai, a notable accordion player, about the rhythm machine he used for accompaniment in club, Wurlitzer Side Man. Osanai, a graduate of the Department of Mechanical Engineering at University of Tokyo, convinced Katoh to finance his efforts to build better one. In 1963, their new company Keio-Giken (later Korg) released their first rhythm machine, Donca-Matic DA-20 using vacuum tube circuit for sound and mechanical-wheel for rhythm patterns. It was a floor-type machine with built-in speaker and keyboard featuring the manual play, in addition to the multiple automatic rhythm patterns, and the price was comparable with the average annual income of Japanese at that time.
Then, their effort was focused on the improvement of reliability and performance, along with the size reduction and the cost down. Unstable vacuum tube circuit was replaced with reliable transistor circuit on Donca-Matic DC-11 in mid-1960s, and in 1966, bulky mechanical-wheel was also replaced with compact transistor circuit on Donca-Matic DE-20 and DE-11. In 1967, Mini Pops MP-2 was developed as an option of Yamaha Electone (electric organ), and Mini Pops was established as a series of the compact desktop rhythm machine. In the United States, Mini Pops MP-3, MP-7, etc. were sold under Univox brand by the distributor at that time, Unicord Corporation.
In 1965, Nippon Columbia filed a patent for an automatic rhythm instrument. It described it as an "automatic rhythm player which is simple but capable of electronically producing various rhythms in the characteristic tones of a drum, a piccolo and so on." It has some similarities to Seeburg's slightly earlier 1964 patent.
In 1967, Ace Tone founder Ikutaro Kakehashi (later founder of Roland Corporation) developed the preset rhythm-pattern generator using "diode matrix" circuit, which has some similarities to the earlier Seeburg and Nippon Columbia patents. Kakehashi's patent describes his device as a "plurality of inverting circuits and/or clipper circuits" which "are connected to a counting circuit to synthesize the output signal of the counting circuit" where the "synthesized output signal becomes a desired rhythm."
Ace Tone commercialized its preset rhythm machine, called the FR-1 Rhythm Ace, in 1967. It offered 16 preset patterns, and four buttons to manually play each instrument sound (cymbal, claves, cowbell and bass drum). The rhythm patterns could also be cascaded together by pushing multiple rhythm buttons simultaneously, and the possible combination of rhythm patterns were more than a hundred (on the later models of Rhythm Ace, the individual volumes of each instrument could be adjusted with the small knobs or faders). The FR-1 was adopted by the Hammond Organ Company for incorporation within their latest organ models. In the US, the units were also marketed under the Multivox brand by Peter Sorkin Music Company, and in the UK, marketed under the Bentley Rhythm Ace brand.
A number of other preset drum machines were released in the 1970s, but early examples of the use can be found on The United States of America's eponymous album from 1967–8. The first major pop song to use a drum machine was "Saved by the Bell" by Robin Gibb, which reached #2 in Britain in 1969. Drum machine tracks were also heavily used on the Sly & the Family Stone album "There's a Riot Goin' On", released in 1971. The German krautrock band Can also used a drum machine on their song "Peking O". The 1972 Timmy Thomas single "Why Can't We Live Together"/"Funky Me" featured a distinctive use of a drum machine and keyboard arrangement on both tracks. Another early example of electronic drums used by a rock group, is Obscured by Clouds by Pink Floyd, from early in 1972. The first album on which a drum machine produced all the percussion was Kingdom Come's "Journey", recorded in November 1972 using a Bentley Rhythm Ace. French singer-songwriter Léo Ferré mixed a drum machine with a symphonic orchestra in the song "Je t'aimais bien, tu sais..." in his album "L'Espoir", released in 1974. Osamu Kitajima's progressive psychedelic rock album "Benzaiten" (1974) also utilized drum machines, and one of the album's contributors, Haruomi Hosono, would later start the electronic music band Yellow Magic Orchestra (as "Yellow Magic Band") in 1977.
Drum sound synthesis.
A key difference between such early machines and more modern equipment is that they use sound synthesis rather than digital sampling in order to generate their sounds. For example, a snare drum or maraca sound would typically be created using a burst of white noise whereas a bass drum sound would be made using sine waves or other basic waveforms. This meant that while the resulting sound was not very close to that of the real instrument, each model tended to have a unique character. For this reason, many of these early machines have achieved a certain "cult status" and are now sought after by producers for use in production of modern electronic music, most notably the Roland TR-808.
Programmable drum machines.
In 1972, Eko released the ComputeRhythm (1972), which was the first programmable drum machine. It had a 6-row push-button matrix that allowed the user to enter a pattern manually. The user could also push punch cards with pre-programmed rhythms through a reader slot on the unit.
Another stand-alone drum machine released in 1975, the PAiA Programmable Drum Set was also one of the first programmable drum machines, and was sold as a kit with parts and instructions which the buyer would use to build the machine.
In 1975, Ace Tone released the Rhythm Producer FR-15 that enables the modification of the pre-programmed rhythm patterns. In 1978, Roland released the Roland CR-78 drum machine, a programmable rhythm machine with four memory storage for user patterns, and in 1979, a simpler version with four sounds, Boss DR-55 was released.
Digital sampling.
The Linn LM-1 Drum Computer (released in 1980 at $4,995) was the first drum machine to use digital samples. Only about 500 were ever made, but its effect on the music industry was extensive. Its distinctive sound almost defines 1980s pop, and it can be heard on hundreds of hit records from the era, including The Human League's Dare, Gary Numan's Dance, Devo's New Traditionalists, and Ric Ocasek's Beatitude. Prince bought one of the very first LM-1s and used it on nearly all of his most popular albums, including 1999 and Purple Rain.
Many of the drum sounds on the LM-1 were composed of two chips that were triggered at the same time, and each voice was individually tunable with individual outputs. Due to memory limitations, a crash cymbal sound was not available except as an expensive third-party modification. A cheaper version of the LM-1 was released in 1982 called the LinnDrum. Priced at $2,995, not all of its voices were tunable, but crash cymbal was included as a standard sound. Like its predecessor the LM-1, it featured swappable sound chips. The LinnDrum can be heard on records such as The Cars' "Heartbeat City" and Giorgio Moroder's soundtrack for the film "Scarface".
It was feared the LM-1 would put every session drummer in Los Angeles out of work and it caused many of L.A's top session drummers (Jeff Porcaro is one example) to purchase their own drum machines and learn to program them themselves in order to stay employed. Linn even marketed the LinnDrum specifically to drummers.
Following the success of the LM-1, Oberheim introduced the DMX, which also featured digitally sampled sounds and a "swing" feature similar to the one found on the Linn machines. It became very popular in its own right, becoming a staple of the nascent hip-hop scene.
Other manufacturers soon began to produce machines, e.g. the Sequential Circuits Drum-Traks and Tom, the E-mu Drumulator and the Yamaha RX11.
In the 1986, SpecDrum by Cheetah Marketing, an inexpensive 8-bit sampling drum external module for ZX Spectrum, was introduced. And its price was less than £30 when similar models cost around £250.
Roland TR-808 and TR-909 machines.
The famous Roland TR-808, a programmable drum machine, was also launched in 1980. At the time it was received with little fanfare, as it did not have digitally sampled sounds; drum machines using digital samples were more popular in the early 1980s. In time, however, the TR-808, along with its successor, the TR-909 (released in 1983), would become a fixture of the burgeoning underground dance, electro, house, techno, R&B and hip-hop genres, mainly because of its low cost (relative to that of the Linn machines) and the unique character of its analogue-generated sounds, which included five unique percussion sounds: “the hum kick, the ticky snare, the tishy hi-hats (open and closed) and the spacey cowbell.” It was first utilized by Yellow Magic Orchestra in the year of its release, after which it would gain further popularity with Marvin Gaye's "Sexual Healing" and Afrikaa Bambaataa's "Planet Rock" in 1982.
In a somewhat ironic twist it is the analogue-based Roland machines that have endured over time as the Linn sound became somewhat overused and dated by the end of the decade. The TR-808 and TR-909's beats have since been widely featured in pop music, and can be heard on countless recordings up to the present day. Because of its bass and long decay, the kick drum from the TR-808 has also featured as a bass line in various genres such as hip hop and drum and bass. Since the mid-1980s, the TR-808 and TR-909 have been used on more hit records than any other drum machine, and has thus attained an iconic status within the music industry.
MIDI breakthrough.
Because these early drum machines came out before the introduction of MIDI in 1983, they use a variety of methods of having their rhythms synchronized to other electronic devices. Some used a method of synchronization called DIN-sync, or Sync-24. Some of these machines also output analog CV/Gate voltages that could be used to synchronize or control analog synthesizers and other music equipment. The Oberheim DMX came with a feature allowing it to be synchronized to its proprietary Oberheim Parallel Buss interfacing system, developed prior to the introduction of MIDI.
By the year 2000, standalone drum machines became much less common, being partly supplanted by general-purpose hardware samplers controlled by sequencers (built-in or external), software-based sequencing and sampling and the use of loops, and music workstations with integrated sequencing and drum sounds. TR-808 and other digitized drum machine sounds can be found in archives on the Internet. However, traditional drum machines are still being made by companies such as Roland Corporation (under the name Boss), Zoom, Korg and Alesis, whose SR-16 drum machine has remained popular since it was introduced in 1991.
There are percussion-specific sound modules that can be triggered by pickups, trigger pads, or through MIDI. These are called drum modules; the Alesis D4 and Roland TD-8 are popular examples. Unless such a sound module also features a sequencer, it is, strictly speaking, not a drum machine.
Programming.
Programming of drum machines are varied by the products. On most products, it can be done in real time: the user creates drum patterns by pressing the trigger pads as though a drum kit were being played; or using step-sequencing: the pattern is built up over time by adding individual sounds at certain points by placing them, as with the TR-808 and TR-909, along a 16-step bar. For example, a generic 4-on-the-floor dance pattern could be made by placing a closed high hat on the 3rd, 7th, 11th, and 15th steps, then a kick drum on the 1st, 5th, 9th, and 13th steps, and a clap or snare on the 5th and 13th. This pattern could be varied in a multitude of ways to obtain fills, break-downs and other elements that the programmer sees fit, which in turn could be sequenced with song-sequence — essentially the drum machine plays back the programmed patterns from memory in an order the programmer has chosen. The machine will quantize entries that are slightly off-beat in order to make them exactly in time.
If the drum machine has MIDI connectivity, then one could program the drum machine with a computer or another MIDI device.
Comparison with live drumming.
While recordings in the 2010s are increasingly using drum machines, "...scientific studies show there are certain aspects of human-created rhythm that machines cannot replicate, or can only replicate poorly" such as the "feel" of human drumming and the ability of a human drummer to respond to changes in a song as it is being played live onstage. Human drummers also have the ability to make slight variations in their playing, such as playing "ahead of the beat" or "behind the beat" for sections of a song, in contrast to a drum machine which plays a pre-programmed rhythm. As well, human drummers play a "...tremendously wide variety of rhythmic variations" that drum machines cannot reproduce.
Labor costs.
Drum machines developed out of a need to create drum beats when a drum kit was not available. Increasingly, drum machines and drum programming are used by major record labels to undercut the costly expense of studio drummers.

</doc>
<doc id="45845" url="https://en.wikipedia.org/wiki?curid=45845" title="Voynich manuscript">
Voynich manuscript

The Voynich manuscript is an illustrated codex hand-written in an unknown writing system. The vellum on which it is written has been carbon-dated to the early 15th century (1404–1438), and it may have been composed in Northern Italy during the Italian Renaissance. The manuscript is named after Wilfrid Voynich, a Polish book dealer who purchased it in 1912.
Some of the pages are missing, with around 240 still remaining. The text is written from left to right, and most of the pages have illustrations or diagrams.
The Voynich manuscript has been studied by many professional and amateur cryptographers, including American and British codebreakers from both World War I and World War II. No one has yet succeeded in deciphering the text, and it has become a famous case in the history of cryptography. The mystery of the meaning and origin of the manuscript has excited the popular imagination, making the manuscript the subject of novels and speculation. None of the many hypotheses proposed over the last hundred years has yet been independently verified.
The Voynich manuscript was donated by Hans P. Kraus to Yale University's Beinecke Rare Book and Manuscript Library in 1969, where it is catalogued under call number MS 408.
Description.
Codicology.
The manuscript measures , with hundreds of vellum pages collected into eighteen quires. The total number of pages is around 240, but the exact number depends on how the manuscript's unusual foldouts are counted. The quires have been numbered from 1 to 20 in various locations, with numerals consistent with the 1400s, and the top righthand corner of each recto (righthand) page has been numbered from 1 to 116, with numerals of a later date. From the various numbering gaps in the quires and pages, it seems likely that in the past the manuscript had at least 272 pages in 20 quires, some of which were already missing when Wilfrid Voynich acquired the manuscript in 1912. There is strong evidence that many of the book's bifolios were reordered at various points in its history, and that the original page order may well have been quite different from what it is today.
The binding and covers are not original to the book, but date to during its possession by the Collegio Romano.
Every page in the manuscript contains text, mostly in an unknown script, but some have extraneous writing in Latin script. Many pages contain substantial drawings or charts which are colored with paint. Based on modern analysis, it has been determined that a quill pen and iron gall ink were used for the text and figure outlines; the colored paint was applied (somewhat crudely) to the figures, possibly at a later date.
Text.
The bulk of the text in the manuscript of 240 pages is written in an unknown script, running left to right. Most of the characters are composed of one or two simple pen strokes. While there is some dispute as to whether certain characters are distinct or not, a script of 20–25 characters would account for virtually all of the text; the exceptions are a few dozen rarer characters that occur only once or twice each. There is no obvious punctuation.
Much of the text is written in a single column in the body of a page, with a slightly ragged right margin and paragraph divisions, and sometimes with stars in the left margin. Other text occurs in charts or as labels associated with illustrations. There are no indications of any errors or corrections made at any place in the document. The ductus flows smoothly, giving the impression that the symbols were not enciphered, as there is no delay between characters as would normally be expected in written encoded text.
The text consists of over 170,000 characters, with spaces dividing the text into about 35,000 groups of varying length, usually referred to as "words". The structure of these words seems to follow phonological or orthographic laws of some sort, e.g., certain characters must appear in each word (like English vowels), some characters never follow others, some may be doubled or tripled but others may not, etc. The distribution of letters within words is also rather peculiar: some characters occur only at the beginning of a word, some only at the end, and some always in the middle section. Many researchers have commented upon the highly regular structure of the words.
Some words occur only in certain sections, or in only a few pages; others occur throughout the manuscript. There are very few repetitions among the thousand or so labels attached to the illustrations. There are practically no words with fewer than two letters or more than ten. There are instances where the same common word appears up to three times in a row. Words that differ by only one letter also repeat with unusual frequency, causing single-substitution alphabet decipherings to yield babble-like text. In 1962, Elizebeth Friedman described such attempts as "doomed to utter frustration".
Various transcription alphabets have been created to equate the Voynich characters with Latin characters in order to help with cryptanalysis, such as the European Voynich Alphabet. The first major one was created by cryptographer William F. Friedman in the 1940s, where each line of the manuscript was transcribed to an IBM punch card to make it machine readable.
Extraneous writing.
Only a few words in the manuscript are considered not to be written in the unknown script:
It is not known whether these bits of Latin script were part of the original text or were added later.
Illustrations.
Because the text cannot be read, the illustrations are conventionally used to divide most of the manuscript into six different sections. Each section is typified by illustrations with different styles and supposed subject matter, except for the last section, in which the only drawings are small stars in the margin. Following are the sections and their conventional names:
Purpose.
The overall impression given by the surviving leaves of the manuscript is that it was meant to serve as a pharmacopoeia or to address topics in medieval or early modern medicine. However, the puzzling details of illustrations have fueled many theories about the book's origins, the contents of its text, and the purpose for which it was intended.
The first section of the book is almost certainly herbal, but attempts to identify the plants, either with actual specimens or with the stylized drawings of contemporary herbals, have largely failed. Only a few of the plant drawings (such as a wild pansy and the maidenhair fern) can be identified with reasonable certainty. Those herbal pictures that match pharmacological sketches appear to be clean copies of these, except that missing parts were completed with improbable-looking details. In fact, many of the plant drawings in the herbal section seem to be composite: the roots of one species have been fastened to the leaves of another, with flowers from a third.
Hugh O'Neill believed that one illustration depicted a New World sunflower, which would help date the manuscript and open up intriguing possibilities for its origin; unfortunately the identification is only speculative.
The basins and tubes in the "biological" section are sometimes interpreted as implying a connection to alchemy, yet bear little obvious resemblance to the alchemical equipment of the period.
Astrological considerations frequently played a prominent role in herb gathering, bloodletting and other medical procedures common during the likeliest dates of the manuscript. However, apart from the obvious Zodiac symbols, and one diagram possibly showing the classical planets, interpretation remains speculative.
History.
Much of the early history of the book is unknown, though the text and illustrations are all characteristically European. In 2009, University of Arizona researchers performed radiocarbon dating on the manuscript's vellum. The result of that test put the date the manuscript was made between 1404 and 1438. In addition, the McCrone Research Institute in Chicago found that the paints in the manuscript were of materials to be expected from that period of European history. It has also been suggested that the McCrone Research Institute found that much of the ink was added not long after the creation of the parchment, but the official report contains no statement to this effect.
The earliest historical information about the manuscript comes from a letter found inside the cover—written in 1666 to accompany the manuscript when it was sent by Johannes Marcus to Athanasius Kircher—which claims that the book once belonged to Emperor Rudolf II (1552–1612), who paid 600 gold ducats (~2.07 kg gold) for it. The book was then given or lent to Jacobus Horcicky de Tepenecz (died 1622), the head of Rudolf's botanical gardens in Prague.
The next confirmed owner is Georg Baresch, an obscure alchemist also in Prague. Baresch was apparently just as puzzled as modern scientists about this "Sphynx" that had been "taking up space uselessly in his library" for many years. On learning that Athanasius Kircher, a Jesuit scholar from the Collegio Romano, had published a Coptic (Egyptian) dictionary and "deciphered" the Egyptian hieroglyphs, Baresch sent a sample copy of the script to Kircher in Rome (twice), asking for clues. His 1639 letter to Kircher is the earliest confirmed mention of the manuscript that has been found so far.
It is not known whether Kircher answered the request, but apparently, he was interested enough to try to acquire the book, which Baresch refused to yield. Upon Baresch's death, the manuscript passed to his friend Jan Marek Marci (1595–1667) (Johannes Marcus Marci), then rector of Charles University in Prague, who a few years later sent the book to Kircher, his longtime friend and correspondent. Marci's 1666 cover letter (written in Latin) was still with the manuscript when Voynich purchased it:
There are no records of the book for the next 200 years, but in all likelihood it was stored with the rest of Kircher's correspondence in the library of the Collegio Romano (now the Pontifical Gregorian University). It probably remained there until the troops of Victor Emmanuel II of Italy captured the city in 1870 and annexed the Papal States. The new Italian government decided to confiscate many properties of the Church, including the library of the Collegio. According to investigations by Xavier Ceccaldi and others, just before this happened, many books of the University's library were hastily transferred to the personal libraries of its faculty, which were exempt from confiscation. Kircher's correspondence was among those books—and so apparently was the Voynich manuscript, as it still bears the "ex libris" of Petrus Beckx, head of the Jesuit order and the University's Rector at the time.
Beckx's "private" library was moved to the Villa Mondragone, Frascati, a large country palace near Rome that had been bought by the Society of Jesus in 1866 and housed the headquarters of the Jesuits' Ghislieri College.
Around 1912, the Collegio Romano was short of money and decided to sell some of its holdings discreetly. Wilfrid Voynich acquired 30 manuscripts, among them the manuscript that now bears his name. He spent the next seven years attempting to interest scholars in deciphering the script while he worked to determine the origins of the manuscript.
In 1930, after Wilfrid's death, the manuscript was inherited by his widow, Ethel Voynich (known as the author of the novel "The Gadfly" and daughter of mathematician George Boole). She died in 1960 and left the manuscript to her close friend, Miss Anne Nill. In 1961, Nill sold the book to another antique book dealer, Hans P. Kraus. Unable to find a buyer, Kraus donated the manuscript to Yale University in 1969, where it was catalogued as "MS 408". In discussions, it is sometimes also referred to as "Beinecke MS 408".
Authorship hypotheses.
Many people have been proposed as possible authors of the Voynich manuscript.
Marci's 1666 cover letter to Kircher says that, according to his friend, the late Raphael Mnishovsky, the book had once been bought by Rudolf II, Holy Roman Emperor and King of Bohemia (1552–1612), for 600 ducats (66.42 troy ounce actual gold weight, or 2.07 kg). (Mnishovsky had died 22 years earlier, in 1644, and the deal must have occurred before Rudolf's abdication in 1611—at least 55 years before Marci's letter.) According to the letter, Mnishovsky (but not necessarily Rudolf) speculated that the author was the Franciscan friar and polymath Roger Bacon (1214–94). Even though Marci said that he was "suspending his judgment" about this claim, it was taken quite seriously by Wilfrid Voynich, who did his best to confirm it.
The assumption that Roger Bacon was the author led Voynich to conclude that the person who sold the manuscript to Rudolf could only have been John Dee (1527–1608), a mathematician and astrologer at the court of Queen Elizabeth I of England, known to have owned a large collection of Bacon's manuscripts. Dee and his "scrier" (mediumic assistant) Edward Kelley lived in Bohemia for several years, where they had hoped to sell their services to the emperor. However, this seems quite unlikely, because Dee's meticulously kept diaries do not mention that sale. If the Voynich manuscript author is not Bacon, a supposed connection to Dee is much weakened. Until the carbon dating of the manuscript to the 15th century, it was thought possible that Dee or Kelley may have written it and spread the rumor that it was originally a work of Bacon's in the hopes of later selling it.
Fabrication by Voynich.
Some suspected Voynich of having fabricated the manuscript himself. As an antique book dealer, he probably had the necessary knowledge and means, and a "lost book" by Roger Bacon would have been worth a fortune. Furthermore, Baresch's letter (and Marci's as well) only establish the existence of "a" manuscript, not that the Voynich manuscript is "the same one" spoken of there. In other words, these letters could possibly have been the motivation for Voynich to fabricate the manuscript (assuming he was aware of them), rather than as proofs authenticating it. However, many consider the expert internal dating of the manuscript and the recent discovery of Baresch's letter to Kircher as having eliminated this possibility.
Other theories.
Voynich was able, sometime before 1921, to read a name faintly written at the foot of the manuscript's first page: "Jacobj à Tepenece". This is taken to be a reference to Jakub Hořčický of Tepenec (1575–1622), also known by his Latin name Jacobus Sinapius. Rudolph II had ennobled him in 1607; appointed him his Imperial Distiller; and had made him both curator of his botanical gardens as well as one of his personal physicians. Voynich, and many other people after him, concluded from this that Jacobus owned the Voynich manuscript prior to Baresch, and drew a link to Rudolf's court from that, in confirmation of Mnishovsky's story.
Jacobus's name is still clearly visible under UV light: however, it does not match the copy of his signature in a document located by Jan Hurych in 2003. As a result, it has been suggested that the signature was added later, possibly even fraudulently by Voynich himself. Yet because the writing on page "f1r" might well have been an ownership mark added by a librarian at the time, the difference between the two signatures does not necessarily disprove Horczicky's ownership.
It has been noted that Baresch's letter bears some resemblance to a hoax that orientalist Andreas Mueller once played on Kircher. Mueller sent some unintelligible text to Kircher with a note explaining that it had come from Egypt, and asking Kircher for a translation: which Kircher, reportedly, produced at once. It has been speculated that these were both cryptographic tricks played on Kircher to make him look foolish: but the Voynich manuscript is on such a vastly different scale to a few signs in a letter that this seems somewhat out of scale for such an endeavor.
Raphael Mnishovsky, the friend of Marci who was the reputed source of Bacon's story, was himself a cryptographer (among many other things) and apparently invented a cipher that he claimed was uncrackable (ca. 1618). This has led to the speculation that Mnishovsky might have produced the Voynich manuscript as a practical demonstration of his cipher and made Baresch his unwitting test subject. Indeed, the disclaimer in the Voynich manuscript cover letter could mean that Marci suspected some kind of deception was at play. However, there is no definite evidence for this theory.
In his 2006 book, Nick Pelling proposed that the Voynich manuscript was written by the 15th century North Italian architect Antonio Averlino (also known as "Filarete"), a theory broadly consistent with the radiocarbon dating.
Richard SantaColoma has speculated that the Voynich Manuscript may be connected to Cornelis Drebbel, initially suggesting it was Drebbel's cipher notebook on microscopy and alchemy, and then later hypothesising it is a fictional "tie-in" to Francis Bacon's utopian novel "New Atlantis" in which some Drebbel-related items (submarine, perpetual clock) are said to appear.
Language hypotheses.
There are many hypotheses about the Voynich manuscript's "language":
Ciphers.
According to the "letter-based cipher" theory, the Voynich manuscript contains a meaningful text in some European language that was intentionally rendered obscure by mapping it to the Voynich manuscript "alphabet" through a cipher of some sort—an algorithm that operated on individual letters. This has been the working hypothesis for most twentieth-century deciphering attempts, including an informal team of NSA cryptographers led by William F. Friedman in the early 1950s.
The main argument for this theory is that the use of a strange alphabet by a European author is awkward to explain except as an attempt to hide information. Indeed, even Roger Bacon knew about ciphers, and the estimated date for the manuscript roughly coincides with the birth of cryptography in Europe as a relatively systematic discipline.
The counterargument is that almost all cipher systems consistent with that era fail to match what we see in the Voynich manuscript. For example, simple monoalphabetic ciphers can be excluded because the distribution of letter frequencies does not resemble that of any common language; while the small number of different letter-shapes used implies that we can rule out nomenclator ciphers and homophonic ciphers, because these typically employ larger cipher alphabets. Similarly, polyalphabetic ciphers, first invented by Alberti in the 1460s and including the later Vigenère cipher, usually yield ciphertexts where all cipher shapes occur with roughly equal probability, quite unlike the language-like letter distribution the Voynich Manuscript appears to have.
However, the presence of many tightly grouped shapes in the Voynich manuscript (such as "or", "ar", "ol", "al", "an", "ain", "aiin", "air", "aiir", "am", "ee", "eee", etc.) does suggest that its cipher system may make use of a "verbose cipher", where single letters in a plaintext get enciphered into groups of fake letters. For example, the first two lines of page f15v (seen above) contain "" and "", which strongly resemble how Roman numbers such as "CCC" or "XXXX" would look if verbosely enciphered. Yet, even though verbose encipherment is arguably the best match, it still falls well short of being able to explain all of the Voynich manuscript's odd textual properties.
It is also entirely possible that the encryption system started from a fundamentally simple cipher and then augmented it by adding nulls (meaningless symbols), homophones (duplicate symbols), transposition cipher (letter rearrangement), false word breaks, and so on.
Codes.
According to the "codebook cipher" theory, the Voynich manuscript "words" would actually be codes to be looked up in a "dictionary" or codebook. The main evidence for this theory is that the internal structure and length distribution of many words are similar to those of Roman numerals—which, at the time, would be a natural choice for the codes. However, book-based ciphers are viable only for short messages, because they are very cumbersome to write and to read.
Steganography.
This theory holds that the text of the Voynich manuscript is mostly meaningless, but contains meaningful information hidden in inconspicuous details—e.g., the second letter of every word, or the number of letters in each line. This technique, called steganography, is very old and was described by Johannes Trithemius in 1499. Though it has been speculated that the plain text was to be extracted by a Cardan grille of some sort, this seems somewhat unlikely because the words and letters are not arranged on anything like a regular grid. Still, steganographic claims are hard to prove or disprove, since stegotexts can be arbitrarily hard to find. An argument against steganography is that having a cipher-like cover text highlights the very "existence" of the secret message, which would be self-defeating: yet because the cover text no less resembles an unknown natural language, this argument is not hugely persuasive.
It has been suggested that the meaningful text could be encoded in the length or shape of certain pen strokes. There are indeed examples of steganography from about that time that use letter shape (italic vs. upright) to hide information. However, when examined at high magnification, the Voynich manuscript pen strokes seem quite natural, and substantially affected by the uneven surface of the vellum.
Natural language.
Statistical analysis of the text reveals patterns similar to those of natural languages. For instance, the word entropy (about 10 bits per word) is similar to that of English or Latin texts. In 2013, Diego Amancio "et al" argued that the Voynich manuscript "is mostly compatible with natural languages and incompatible with random texts".
The linguist Jacques Guy once suggested that the Voynich manuscript text could be some little-known natural language, written in the plain with an invented alphabet. The word structure is similar to that of many language families of East and Central Asia, mainly Sino-Tibetan (Chinese, Tibetan, and Burmese), Austroasiatic (Vietnamese, Khmer, etc.) and possibly Tai (Thai, Lao, etc.). In many of these languages, the words have only one syllable; and syllables have a rather rich structure, including tonal patterns.
This theory has some historical plausibility. While those languages generally had native scripts, these were notoriously difficult for Western visitors. This difficulty motivated the invention of several phonetic scripts, mostly with Latin letters but sometimes with invented alphabets. Although the known examples are much later than the Voynich manuscript, history records hundreds of explorers and missionaries who could have done it—even before Marco Polo's thirteenth century journey, but especially after Vasco da Gama sailed the sea route to the Orient in 1499.
The main argument for this theory is that it is consistent with all statistical properties of the Voynich manuscript text which have been tested so far, including doubled and tripled words (which have been found to occur in Chinese and Vietnamese texts at roughly the same frequency as in the Voynich manuscript). It also explains the apparent lack of numerals and Western syntactic features (such as articles and copulas), and the general inscrutability of the illustrations. Another possible hint is two large red symbols on the first page, which have been compared to a Chinese-style book title, inverted and badly copied. Also, the apparent division of the year into 360 days (rather than 365 days), in groups of 15 and starting with Pisces, are features of the Chinese agricultural calendar ("jie qi", 節氣). The main argument against the theory is the fact that no one (including scholars at the Chinese Academy of Sciences in Beijing) has been able to find any clear examples of Asian symbolism or Asian science in the illustrations.
In 1976, James R Child of the National Security Agency, a linguist of Indo-European languages, proposed that the manuscript was written in a "hitherto unknown North Germanic dialect". He identified in the manuscript a "skeletal syntax several elements of which are reminiscent of certain Germanic languages", while the content itself is expressed using "a great deal of obscurity".
In late 2003, Zbigniew Banasik of Poland proposed that the manuscript is plaintext written in the Manchu language and gave a proposed piecemeal translation of the first page of the manuscript.
In February 2014, Professor Stephen Bax of the University of Bedfordshire made public his research into using "bottom up" methodology to understand the manuscript. His method involves looking for and translating proper nouns, in association with relevant illustrations, in the context of other languages of the same time period. A paper he posted online offers tentative translation of 14 characters and 10 words. He suggests the text is a treatise on nature written in a natural language, rather than a code.
In 2014, Arthur O. Tucker and Rexford H. Talbert published a paper claiming a positive identification of 37 plants, 6 animals, and 1 mineral referenced in the manuscript to plant drawings in the Libellus de Medicinalibus Indorum Herbis or Badianus manuscript, a fifteenth century Aztec herbal. They argue that these were from Colonial New Spain and represented the Nahuatl language, and date the manuscript to between 1521 (the date of the Conquest) to ca. 1576, in contradiction of radiocarbon dating evidence of the vellum and many other elements of the manuscript. The analysis has been criticized by other Voynich Manuscript researchers, pointing out that—among other things—a skilled forger could construct plants that have a passing resemblance to existing plants that were heretofore undiscovered.
Constructed language.
The peculiar internal structure of Voynich manuscript words led William F. Friedman to conjecture that the text could be a constructed language. In 1950, Friedman asked the British army officer John Tiltman to analyze a few pages of the text, but Tiltman did not share this conclusion. In a paper in 1967, Brigadier Tiltman said, "After reading my report, Mr. Friedman disclosed to me his belief that the basis of the script was a very primitive form of synthetic universal language such as was developed in the form of a philosophical classification of ideas by Bishop Wilkins in 1667 and Dalgarno a little later. It was clear that the productions of these two men were much too systematic, and anything of the kind would have been almost instantly recognisable. My analysis seemed to me to reveal a cumbersome mixture of different kinds of substitution."
The concept of an artificial language is quite old, as attested by John Wilkins's "Philosophical Language" (1668), but still postdates the generally accepted origin of the Voynich manuscript by two centuries. In most known examples, categories are subdivided by adding suffixes; as a consequence, a text in a particular subject would have many words with similar prefixes—for example, all plant names would begin with similar letters, and likewise for all diseases, etc. This feature could then explain the repetitious nature of the Voynich text. However, no one has been able yet to assign a plausible meaning to any prefix or suffix in the Voynich manuscript.
Hoax.
The bizarre features of the Voynich manuscript text (such as the doubled and tripled words), and the suspicious contents of its illustrations support the idea that the manuscript is a hoax. In other words, if no one is able to extract meaning from the book, then perhaps this is because the document contains no meaningful content in the first place. Various hoax theories have been proposed over time.
In 2003, computer scientist Gordon Rugg showed that text with characteristics similar to the Voynich manuscript could have been produced using a table of word prefixes, stems, and suffixes, which would have been selected and combined by means of a perforated paper overlay. The latter device, known as a Cardan grille, was invented around 1550 as an encryption tool, more than 100 years after the estimated creation date of the Voynich manuscript. Some maintain that the similarity between the pseudo-texts generated in Gordon Rugg's experiments and the Voynich manuscript is superficial, and the grille method could be used to emulate any language to a certain degree.
In April 2007, a study by Austrian researcher Andreas Schinner published in "Cryptologia" supported the hoax hypothesis. Schinner showed that the statistical properties of the manuscript's text were more consistent with meaningless gibberish produced using a quasi-stochastic method such as the one described by Rugg, than with Latin and medieval German texts.
The argument for authenticity is that the manuscript appears too sophisticated to be a hoax. While hoaxes of the period tended to be quite crude, the Voynich manuscript exhibits many subtle characteristics which show up only after careful statistical analysis. The question then arises as to why the author would employ such a complex and laborious forging algorithm in the creation of a simple hoax, if no one in the expected audience (that is, the creator's contemporaries) could tell the difference. Marcelo Montemurro, a theoretical physicist from the University of Manchester who spent years analysing the linguistic patterns in the Voynich manuscript, found semantic networks such as content-bearing words occurring in a clustered pattern, and new words being used when there was a shift in topic. With this evidence, he believes it unlikely that these features were simply "incorporated" into the text to make a hoax more realistic, as most of the required academic knowledge of these structures did not exist at the time the Voynich manuscript was created. These fine touches require much more work than would have been necessary for a simple forgery, and some of the complexities are only visible with modern tools.
Glossolalia.
In their 2004 book, Gerry Kennedy and Rob Churchill hint at the possibility that the Voynich manuscript may be a case of glossolalia, channeling, or outsider art.
If this is true, then the author felt compelled to write large amounts of text in a manner which somehow resembles stream of consciousness, either because of voices heard, or because of an urge. While in glossolalia this often takes place in an invented language (usually made up of fragments of the author's own language), invented scripts for this purpose are rare. Kennedy and Churchill use Hildegard von Bingen's works to point out similarities between the illustrations she drew when she was suffering from severe bouts of migraine—which can induce a trance-like state prone to glossolalia—and the Voynich manuscript. Prominent features found in both are abundant "streams of stars", and the repetitive nature of the "nymphs" in the biological section.
The theory is virtually impossible to prove or disprove, short of deciphering the text; Kennedy and Churchill are themselves not convinced of the hypothesis, but consider it plausible. In the culminating chapter of their work, Kennedy states his belief that it is a hoax or forgery. Churchill acknowledges the possibility that the manuscript is a synthetic forgotten language (as advanced by Friedman), or a forgery, to be preeminent theories. However he concludes that if the manuscript is genuine, mental illness or delusion seems to have affected the author. after John Matthews Manly of the University of Chicago pointed out serious flaws in his theory. Each shorthand character was assumed to have multiple interpretations, with no reliable way to determine which was intended for any given case. Newbold's method also required rearranging letters at will until intelligible Latin was produced. These factors alone ensure the system enough flexibility that nearly anything at all could be discerned from the microscopic markings. Although evidence of micrography using the Hebrew language can be traced as far back as the ninth century, it is nowhere near as compact or complex as the shapes Newbold made out. Close study of the manuscript revealed the markings to be artifacts caused by the way ink cracks as it dries on rough vellum. Perceiving significance in these artifacts can be attributed to pareidolia. Thanks to Manly's thorough refutation, the micrography theory is now generally disregarded.
Joseph Martin Feely.
In 1943, Joseph Martin Feely published "Roger Bacon's Cipher: The Right Key Found", in which he claimed that the book was a scientific diary. Feely's method posited that the text was a highly abbreviated medieval Latin written with a simple substitution cipher. He also claimed that the writer of the manuscript was Roger Bacon.
Leonell C Strong.
Leonell C. Strong, a cancer research scientist and amateur cryptographer, believed that the solution to the Voynich manuscript was a "peculiar double system of arithmetical progressions of a multiple alphabet". Strong claimed that the plaintext revealed the Voynich manuscript to be written by the 16th-century English author Anthony Ascham, whose works include "A Little Herbal", published in 1550. The main argument against this theory is that its claimed offsetting cryptography runs counter to all the complex internal structures presented by the text.
Robert S Brumbaugh.
Robert Brumbaugh, a professor of medieval philosophy at Yale University, claimed that the manuscript was a forgery intended to fool Emperor Rudolf II into purchasing it. The text is Latin, but enciphered with a complex, two–step method.
John Stojko.
In 1978, John Stojko published "Letters to God's Eye" in which he claimed that the Voynich Manuscript was a series of letters written in vowelless Ukrainian. However, the date Stojko gives for the letters, the lack of relation between the text and the images, and the general looseness in the method of decryption all speak against his theory.
Leo Levitov.
Leo Levitov proposed in his 1987 book, "Solution of the Voynich Manuscript: A Liturgical Manual for the Endura Rite of the Cathari Heresy, the Cult of Isis", that the manuscript is a handbook for the Cathar rite of "Endura" written in a Flemish based creole. He further claimed that Catharism was a survival of the cult of Isis.
However, Levitov's decipherment has been refuted on several grounds, not least of being unhistorical. Levitov had a poor grasp on the history of the Cathar, and his depiction of "Endura" as an elaborate suicide ritual is at odds with surviving documents describing it as a fast. Likewise, there is no known link between Catharism and Isis.
Cultural impact.
Many books and articles have been written about the manuscript. The first facsimile edition was published in 2005, "Le Code Voynich": the whole manuscript published with a short presentation in French.
The manuscript has also inspired several works of fiction, including "The Book of Blood and Shadow" by Robin Wasserman, "Time Riders: The Doomsday Code" by Alex Scarrow, "Codex" by Lev Grossman, "PopCo" by Scarlett Thomas, "Prime" by Jeremy Robinson with Sean Ellis, "The Sword of Moses" (2013) by Dominic Selwood, "The Return of the Lloigor" by Colin Wilson, "Datura, or a delusion we all see" (Finnish version 2001) by Leena Krohn, "Assassin's Code" by Jonathan Maberry and "The Source" by Michael Cordy.
Between 1976 and 1978, Italian artist Luigi Serafini created the "Codex Seraphinianus" containing false writing and pictures of imaginary plants, in a style reminiscent of the Voynich manuscript.
Contemporary classical composer Hanspeter Kyburz's 1995 Chamber work "The Voynich Cipher Manuscript, for chorus & ensemble" is inspired by the manuscript.

</doc>
<doc id="45846" url="https://en.wikipedia.org/wiki?curid=45846" title="Ferrara">
Ferrara

Ferrara ( ) is a city and "comune" in Emilia-Romagna, northern Italy, capital city of the Province of Ferrara. It is situated north-northeast of Bologna, on the Po di Volano, a branch channel of the main stream of the Po River, located north. The town has broad streets and numerous palaces dating from the 14th and 15th centuries, when it hosted the court of the House of Este. For its beauty and cultural importance it has been qualified by UNESCO as World Heritage Site.
Modern times have brought a renewal of industrial activity. Ferrara is on the main rail line from Bologna to Padua and Venice, and has branches to Ravenna, Poggio Rusco (for Suzzara) and Codigoro.
History.
Origins.
Ferrara was probably settled by the inhabitants of the lagoons at the mouth of Po river; there are two early centers of settlement, one round the cathedral, the other, the "castrum bizantino", on the opposite shore, where the Primaro empties into the Volano channel. Ferrara appears first in a document of the Lombard king Desiderius of 753 AD, as a city forming part of the Exarchate of Ravenna. Desiderius pledged a Lombard "ducatus ferrariae" ("Duchy of Ferrara") in 757 to Pope Stephen II.
Obizzo II d'Este was proclaimed lifelong ruler of Ferrara five hundred years later. He also became seignior of nearby Modena in 1288 and of Reggio in 1289. In 1452 the Este rulers were created Dukes of Modena and Reggio, and in 1471 Ferrara also became a duchy.
In 1597, when Alfonso II died without heirs, the House of Este lost Ferrara to the Papal States.
Modern history.
Ferrara remained a part of the Papal States from 1598 to 1859, with an interruption during the Napoleonic period: in 1859 it became part of the Kingdom of Italy. A fortress was constructed by Pope Paul V on the site of the castle called "Castel Tedaldo", at the south-west angle of the town, that was occupied by an Austrian garrison from 1832 until 1859. All of the fortress was dismantled following the birth of the Kingdom of Italy and the bricks used for new constructions all over the town.
On August 23, 1944, the Ferrara synthetic rubber plant was a target of Strategic bombing during World War II.
Civic and secular landmarks.
The town is still surrounded by more than of ancient walls, mainly built in the 15th and 16th-centuries. Along with those of Lucca, they are the best preserved Renaissance walls in Italy.
The imposing brick Castello Estense sited in the very centre of the town is iconic of Ferrara. The castle, erected in 1385, is surrounded by a moat, with four massive bastions. The pavilions on the top of the towers date from the 16th-century refurbishment.
The City Hall, renovated in the 18th century, was the earlier residence of the Este family. Close by it is the former Cathedral of San Giorgio, The Romanesque lower part of the main façade and the side façades was completed first in 1135. According to a now lost inscription the church had been commissioned by Guglielmo I of Adelardi (d. 1146). The sculpture of the main portal was signed by a Nicholaus, mentioned in the lost inscription as the church's Romanesque architect. The upper part of the main façade, with arcades of pointed arches, dates from the 13th century. The recumbent lions guarding the entrance are copies of the originals, now in the narthex of the church. An elaborate 13th-century relief depicting the Last Judgement is found in the second story of the porch. The interior was restored in the baroque style in 1712. The campanile, in the Renaissance style, dates from 1451–1493, but the top storey was added at the end of the 16th century. The campanile is still incomplete, missing one additional storey and a conical top, as it can be seen from numerous historical prints and paintings on the subject.
Nearby is the University of Ferrara; the university library houses part of manuscript of the "Orlando furioso" and letters by Tasso. Its famous graduates include Nicolaus Copernicus (1503) and Paracelsus. The campus also shelters the University of Ferrara Botanic Garden.
Unlike other towns, Ferrara retains many early Quattrocento palaces, often retaining "terracotta" decorations, though most are comparatively small in size. Among them are those in the north quarter (especially the four at the intersection of its two main streets), which was added by Ercole I in 1492–1505, from the plans of Biagio Rossetti, and hence called the "Addizione Erculea".
Among the finest palaces is Palazzo dei Diamanti ("Diamond Palace"), named after the diamond points into which the façade's stone blocks are cut. The "palazzo" houses the National Picture Gallery, with a large collection of the school of Ferrara, which first rose to prominence in the latter half of the 15th century, with Cosimo Tura, Francesco Cossa and Ercole dei Roberti. Noted masters of the 16th-century School of Ferrara include Lorenzo Costa and Dosso Dossi, the most eminent of all, Girolamo da Carpi and Benvenuto Tisi (il Garofalo).
The Casa Romei is perhaps the best preserved Renaissance building in Ferrara. It was the residence of Giovanni Romei, related by marriage to Este family and likely the work of the court architect Pietro Bono Brasavola. The occupation of the palace by the nuns of the Corpus Domini order prevented its destruction. Much of the decoration in the inner rooms has been saved. There are fresco cycles in the Sala delle Sibille (Room of Sibyls), with its original "terracotta" fireplace bearing the coat of arms of Giovanni Romei, in the adjoining Saletta dei Profeti (Room of the Prophets), depicting allegories from the Bible and in other rooms, some of which were commissioned by cardinal Ippolito d'Este and painted by the school of Camillo and Cesare Filippi (16th century).
The Palazzo Schifanoia ("sans souci") was built in 1385 for Alberto V d'Este. The "palazzo" includes frescoes depicting the life of Borso d'Este, the signs of the zodiac and allegorical representations of the months. The vestibule was decorated with "stucco" mouldings by Domenico di Paris. The building also contains fine choir-books with miniatures and a collection of coins and Renaissance medals.
The City Historical Archives contain a relevant amount of historical documents, starting from 15th century. The "Diocesan Historical Archive" is more ancient, mentioned in documents in AD 955, and contains precious documents collected across the centuries by the clergy. Other sites include:
Churches, monasteries, and synagogue.
The Corpus Domini Monastery contains tombs of the House of Este, including Alfonso I, Alfonso II, Ercole I, Ercole II, as well as Lucrezia Borgia, Eleanor of Aragon, and many more.
The Ferrara Synagogue and Jewish Museum are located in the heart of the medieval centre, close to the cathedral and the Castello Estense. This street was part of the Jewish Quarter in which the Jews were separated from the rest of the population of Ferrara from 1627 to 1859.
Other sites include:
Demographics.
In 2007, there were 135,369 people residing in Ferrara, of whom 46.8% were male and 53.2% were female. Minors (children ages 18 and younger) totalled 12.28 percent of the population compared to pensioners who number 26.41%. This compares with the Italian average of 18.06% (minors) and 19.94% (pensioners). The average age of Ferrara residents is 49 compared to the Italian average of 42. In the five years between 2002 and 2007, the population of Ferrara grew by 2.28%, while Italy as a whole grew by 3.85%. The current birth rate of Ferrara is 7.02 births per 1,000 inhabitants compared to the Italian average of 9.45 births. Ferrara is known as being the oldest city with a population over 100,000, as well the city with lowest birth rate.
, 95.59% of the population was Italian. The largest immigrant group was other European nations (mostly from the Ukraine, and Albania: 2.59%) North Africa: 0.51%, and East Asia: 0.39%. The city is predominantly Roman Catholic, with small Orthodox Christian adherents. The historical Jewish community is still surviving.
Jewish community.
The Jewish community of Ferrara is the only one in Emilia Romagna with a continuous presence from the Middle Ages to the present day. It played an important role when Ferrara enjoyed its greatest splendor in the 15th and 16th century, with the duke Ercole I d'Este. The situation of the Jews deteriorated in 1598, when the Este dynasty moved to Modena and the city came under papal control. The Jewish settlement, located in three streets forming a triangle near the cathedral, became a ghetto in 1627. Apart from a few years under Napoleon and during the 1848 revolution, the ghetto lasted until Italian unification in 1859.
In 1799, the Jewish community saved the city from sacking by troops of the Holy Roman Empire. During the spring of 1799, the city had fallen into the hands of the Republic of France, which established a small garrison there. On 15 April, Lieutenant Field Marshal Johann von Klenau approached the fortress with a modest mixed force of Austrian cavalry, artillery and infantry augmented by Italian peasant rebels, commanded by Count Antonio Bardaniand and demanded its capitulation. The commander refused. Klenau blockaded the city, leaving a small group of artillery and troops to continue the siege. For the next three days, Klenau patrolled the countryside, capturing the surrounding strategic points of Lagoscuro, Borgoforte and the Mirandola fortress. The besieged garrison made several sorties from the Saint Paul's Gate, which were repulsed by the insurgent peasants. The French attempted two rescues of the beleaguered fortress: the first, on 24 April, when a force of 400 Modenese was repulsed at Mirandola. In the second, General Montrichard tried to raise the city-blockade by advancing with a force of 4,000. Finally, at the end of the month, a column led by Pierre-Augustin Hulin reached and relieved the fortress.
Klenau took possession of the town on 21 May, and garrisoned it with a light battalion. The Jewish residents of Ferrara paid 30,000 ducats to prevent the pillage of the city by Klenau's forces; this was used to pay the wages of Gardani's troops. Although Klenau held the town, the French still possessed the town's fortress. After making the standard request for surrender at 0800, which was refused, Klenau ordered a barrage from his mortars and howitzers. After two magazines caught fire, the commandant was summoned again to surrender; there was some delay, but a flag of truce was sent at 2100, and the capitulation was concluded at 0100 the next day. Upon taking possession of the fortress, Klenau found 75 new artillery pieces, plus ammunition and six months worth of provisions.
In 1938, Mussolini's fascist government instituted racial laws reintroducing segregation of Jews which lasted until the end of the Nazi occupation. During the Second World War, ninety-six of Ferrara's 300 Jews were deported to Nazi concentration and death camps; five survived. The Italian Jewish writer, Giorgio Bassani, was from Ferrara. His celebrated book, "The Garden of the Finzi-Continis", was published in Italian as Giardino del Finzi-Contini, 1962, by Giulio Einaudi editore s.p.a. It was made into a film by Vittorio de Sica in 1970.
During WWII, the Este Castle, adjacent to the Corso Roma, now known as the Corso Martiri della Libertà, was the site of an infamous massacre in 1943.
Culture.
Literature.
The Renaissance literary men and poets Torquato Tasso (author of "Jerusalem Delivered"), Ludovico Ariosto (author of the romantic epic poem "Orlando Furioso") and Matteo Maria Boiardo (author of the grandiose poem of chivalry and romance "Orlando Innamorato"), lived and worked at the court of Ferrara during the 15th and 16th century.
The "Ferrara Bible" was a 1553 publication of the Ladino version of the Tanach used by Sephardi Jews. It was paid for and made by Yom-Tob ben Levi Athias (the Spanish Marrano "Jerónimo de Vargas", as typographer) and Abraham ben Salomon Usque (the Portuguese Jew "Duarte Pinhel", as translator), and was dedicated to Ercole II d'Este. In the 20th century Ferrara was the home and workplace of writer Giorgio Bassani, well known for his novels that were often adapted for cinema ("The Garden of the Finzi-Continis", "Long Night in 1943"). In historical fiction, British author Sarah Dunant set her 2009 novel "Sacred Hearts" in a convent in Ferrara.
Painting.
During the Renaissance, the House of Este, well known for its partonage of the arts, welcomed a great number of artists, especially painters, that formed the so-called School of Ferrara. The astounding list of painters and artists includes the names of Andrea Mantegna, Vicino da Ferrara, Giovanni Bellini, Leon Battista Alberti, Pisanello, Piero della Francesca, Battista Dossi, Dosso Dossi, Cosmé Tura, Francesco del Cossa and Titian. During the 19th and 20th centuries, Ferrara hosted and inspired a number of important painters who grew fond of its eerie atmosphere: among them Giovanni Boldini, Filippo de Pisis and Giorgio de Chirico.
Religion.
Ferrara gave birth to Girolamo Savonarola, the famous medieval Dominican priest and leader of Florence from 1494 until his execution in 1498. He was known for his book burning, destruction of what he considered immoral art, and hostility to the Renaissance. He vehemently preached against the moral corruption of much of the clergy at the time, and his main opponent was Pope Alexander VI (Rodrigo Borgia).
Music.
The Ferrarese musician Girolamo Frescobaldi was one of the most important composers of keyboard music in the late Renaissance and early Baroque periods. His masterpiece "Fiori musicali" ("Musical Flowers") is a collection of liturgical organ music first published in 1635. It became the most famous of Frescobaldi's works and was studied centuries after his death by numerous composers, including Johann Sebastian Bach. Maurizio Moro (15??—16??) an Italian poet of the 16th century best known for madrigals is thought to have been born in Ferrara.
Cinema.
Ferrara is the birthplace and childhood home of the well-known Italian film director, Michelangelo Antonioni. The town of Ferrara was also the setting of the famous film "The Garden of the Finzi-Continis" by Vittorio De Sica in (1970), that tells the vicissitudes of a rich Jewish family during the dictatorship of Benito Mussolini and World War II. Furthermore, Wim Wenders and Michelangelo Antonioni's "Beyond the Clouds" in (1995) and Ermanno Olmi's "The Profession of Arms" in (2001), a film about the last days of Giovanni dalle Bande Nere, were also shot in Ferrara.
Festivals.
The Palio of St. George is a typical medieval festival held every last Sunday of May. The Buskers Festival is a non-competitive parade of the best street musicians in the world. In terms of tradition and dimension it is the most important festival in the world of this kind. Additionally, Ferrara is becoming the Italian capital of hot air balloons, thanks to the ten-day-long Ferrara Balloons Festival, the biggest celebration of balloons in Italy and one of the largest in Europe.
Sport.
Ferrara's local football team, Società Polisportiva Ars Et Labor 1907 is going to play in "Lega Pro Prima Divisione" (former Serie C1), which is the third highest football league in Italy. The local basketball team, Carife Ferrara, have been doing considerably better; they won the 2007-08 title in the second-level LegADue, thereby earning promotion to Serie A. The city is hosting the Ferrara Marathon since 1979.
Culinary tradition.
The cooking tradition of the town is characterized by many typical dishes that can be traced back to the Middle Ages and reveals in some instances the influence of the important Jewish community. The signature first course is cappellacci di zucca, a kind of ravioli with a filling of butternut squash, Parmigiano-Reggiano and flavored with nutmeg. It is served with a sauce of butter and sage. The traditional Christmas first dish is cappelletti, small meat-filled ravioli served in chicken broth or with a white sauce made from cream and, optionally, local truffles. A peculiar first dish is the pasticcio di maccheroni, a domed macaroni pie, consisting of a crust of sweet dough enclosing macaroni in a Béchamel sauce, studded with porcini mushrooms and ragù bolognese. The second course that is a must of the Christmas table is the Salama da sugo, a one-year-old dry salami made from a special selection of pork meat, spices and red wine. Seafood is an important part of the town tradition, due to the vicinity to the sea, and grilled or stewed eel from the river Po delta is especially appreciated. In the Ferrara's pantry you can also find a kosher salami, made of goose meat stuffed in goose neck skin. The Christmas traditional dessert is a chocolat pie, the pampepato, and the zuppa inglese. The clay terroir of the area, an alluvial plain created by the river Po, is not ideal for wine; a notable exception is the Vini del Bosco Eliceo (DOC), made from grapes cultivated on the sandy coast line. The typical bread, called coppia ferrarese, has been awarded the IGP (Protected Geographical Status) label .
Transport.
Ferrara railway station, opened in 1862, forms part of the Padua–Bologna railway. It is also a terminus of three secondary railways, linking Modena with Ravenna and Rimini, Suzzara, and Codigoro, respectively. The station is located at Piazzale della Stazione, at the northwestern edge of the city centre.
International relations.
Twin towns — sister cities.
Ferrara is twinned with:
Politics.
The last municipal elections was held on May 25, 2014, resulting in the election of Tiziano Tagliani (Democratic Party) as Mayor of the city of Ferrara.
The division of the 32 seats in the city council is as followed:

</doc>
<doc id="45847" url="https://en.wikipedia.org/wiki?curid=45847" title="Agritourism">
Agritourism

Agritourism or agrotourism, as it is defined most broadly, involves any agriculturally based operation or activity that brings visitors to a farm or ranch. Agritourism has different definitions in different parts of the world, and sometimes refers specifically to farm stays, as in Italy. Elsewhere, agritourism includes a wide variety of activities, including buying produce direct from a farm stand, navigating a corn maze, slopping hogs, picking fruit, feeding animals, or staying at a bed and breakfast (B&B) on a farm.
Agritourism is a form of niche tourism that is considered a growth industry in many parts of the world, including Australia, Canada, the United States, and the Philippines. Other terms associated with agritourism are "agritainment", "value added products", "farm direct marketing" and "sustainable agriculture".
Public awareness.
People have become more interested in how their food is produced. They want to meet farmers and processors and talk with them about what goes into food production. For many people who visit farms, especially children, the visit marks the first time they see the source of their food, be it a dairy cow, an ear of corn growing in a field, or an apple they can pick right off a tree.
Farmers and ranchers use this interest to develop traffic at their farm or ranch, and interest in the quality of their products, as well as awareness of their products.
Safety.
While revenue and education are often primary drivers for farmers to diversify their operations and invite guests onto their property, safety isn't always a top priority. Accidents involving tractors, wagon rides, trips, falls, and traffic occur at agritourism operations on a regular basis. 
Agritourism by country.
Italy.
The country-hotel scene has come on apace since 1960, when the Michelin guide to Italy listed not a single establishment in the Chianti area. But even after the boom in rural accommodation in the 1970s, 1980s and 1990s, the choice was still limited, by and large, to basic agriturismo farm-holiday places or rather stuffy country-house hotels. The past few years have seen the arrival of a handful of stylish luxury spa resorts, and some welcome mid-range options where guests benefit from a hands-on, personal approach. These mini-resorts - which include La Bandita (Tuscany), Prati Palai (Veneto - Lake Garda) and Hotelito Lupaia (Tuscany) - are proof that, even in Italy, it is possible to run a classy rural retreat and charge less than £250 a night for a standard double in mid-season.
Since 1985 agritourism in Italy is formally regulated by a state law, emended in 2006. The law states basic requirements to claim the title of "agriturismo", and delegates single regions to further regulate the matter.
India.
Since 2004 Agriculture Tourism is operational, it started in Baramati Agri Tourism Center under the guidance of Pandurang Taware who is known as Father of Agri Tourism Concept in India. He received the National Tourism Award from the President Of India, for the most innovative Tourism Product. Agri Tourism India (ATDC) is pioneer in the development and marketing of agri tourism concept in India. ATDC, as of 2014, has 218 affiliated farmers and operates agri tourism center in their respective villages in the state of Maharashtra.
Turkey.
In the province of Hatay, The village of Vakifli has a small eco and cultural tourism industry, as it is often touted as the last rural Armenian village in Turkey. The small village has a guest house where visitors can buy organic produce and see the life of the village. There is potential for ecotourism in the Aegean area of Western turkey as well, and is a growing industry there.
United States.
Agritourism is widespread in the United States. Agritourists can choose from a wide range of activities that include picking fruits and vegetables, riding horses, tasting honey, learning about wine and cheesemaking, or shopping in farm gift shops and farm stands for local and regional produce or hand-crafted gifts.
According to the USDA Cooperative State, Education and Extension Service, "Tourism is becoming increasingly important to the U.S. economy. A conservative estimate from the Federal Reserve Board in Kansas, based on 2000 data, shows that basic travel and tourism industries accounted for 3.6 percent of all U.S. employment. Even more telling, data from the Travel Industry Association of America indicate that 1 out of every 18 people in the U.S. has a job directly resulting from travel expenditures".
Through the Small Farm Center at the University of California, "Agricultural tourism or agritourism, is one alternative for improving the incomes and potential economic viability of small farms and rural communities. Some forms of agritourism enterprises are well developed in California, including fairs and festivals. Other possibilities still offer potential for development". The UC Small Farm Center has developed a California Agritourism Database that "provides visitors and potential entrepreneurs with information about existing agritourism locations throughout the state".
The publication "Promoting Tourism in Rural America" explains the need for planning and marketing a rural community and weighing the pros and cons of tourism. According to the publication, local citizen participation is helpful and should be included in starting any kind of a tourism program. Citizen participation in planning tourism can contribute to building a successful program that enhances the community. Additional websites that promote and publicize agritourism in the United States include Rural Bounty, founded by agritourism consultant Jane Eckert, Farm Stay U.S., a nationwide directory of farm stays, and The Farm Stay Project, a blog that profiles farm stays and tracks agritourism news.
Dude ranches.
Dude (or guest) ranches offer tourists the chance to work on cattle ranches, and sometimes participate in cattle drives. The fact sheet, "Promoting the Farm and Ranch Recreation Business", gives farmers and ranchers information on marketing and developing strategies to win tourism dollars. Dude ranches are common in the United States and Australian Outback.

</doc>
<doc id="45848" url="https://en.wikipedia.org/wiki?curid=45848" title="Deep Purple">
Deep Purple

Deep Purple are an English rock band formed in Hertford in 1968. They are considered to be among the pioneers of heavy metal and modern hard rock, although their musical approach changed over the years. Originally formed as a progressive rock band, the band shifted to a heavier sound in 1970. Deep Purple, together with Led Zeppelin and Black Sabbath, have been referred to as the "unholy trinity of British hard rock and heavy metal in the early to mid-seventies". They were listed in the 1975 "Guinness Book of World Records" as "the globe's loudest band" for a 1972 concert at London's Rainbow Theatre, and have sold over 100 million albums worldwide.
Deep Purple have had several line-up changes and an eight-year hiatus (1976–1984). The 1968–1976 line-ups are commonly labelled Mark I, II, III and IV. Their second and most commercially successful line-up featured Ian Gillan (vocals), Jon Lord (organ), Roger Glover (bass), Ian Paice (drums), and Ritchie Blackmore (guitar). This line-up was active from 1969 to 1973, and was revived from 1984 to 1989, and again from 1992 to 1993. The band achieved more modest success in the intervening periods between 1968 and 1969 with the line-up including Rod Evans (vocals) and Nick Simper (bass, backing vocals), between 1974 and 1976 (Tommy Bolin replacing Blackmore in 1975) with the line-up including David Coverdale (vocals) and Glenn Hughes (bass, vocals), and between 1989 and 1992 with the line-up including Joe Lynn Turner (vocals). The band's line-up (currently featuring Ian Gillan, and guitarist Steve Morse from 1994) has been much more stable in recent years, although organist Jon Lord's retirement from the band in 2002 (being succeeded by Don Airey) left Ian Paice as the only original Deep Purple member still in the band.
Deep Purple were ranked number 22 on VH1's "Greatest Artists of Hard Rock" programme and a poll on British radio station Planet Rock ranked them 5th among the "most influential bands ever". The band received the Legend Award at the 2008 World Music Awards. Having been nominated for induction into the Rock and Roll Hall of Fame in 2012 and 2013, Deep Purple were announced as inductees into the Hall of Fame in December 2015. They were officially inducted on 8 April 2016.
History.
Beginnings (1967–68).
In 1967 former Searchers drummer Chris Curtis contacted London businessman Tony Edwards, in the hope that he would manage a new group he was putting together, to be called Roundabout. Curtis' vision was a "supergroup" where the band members would get on and off, like a musical roundabout. Impressed with the plan, Edwards agreed to finance the venture with two business partners: John Coletta and Ron Hire, all of Hire-Edwards-Coletta (HEC) Enterprises.
The first recruit to the band was the classically trained Hammond organ player Jon Lord, Curtis' flatmate who had most notably played with the Artwoods (led by Art Wood, brother of future Rolling Stones guitarist Ronnie Wood, and featuring Keef Hartley). He was followed by guitarist Ritchie Blackmore, who was persuaded to return from Hamburg to audition for the new group. Blackmore was making a name for himself as a studio session guitarist, and had also been a member of the Outlaws, Screaming Lord Sutch, and Neil Christian. Curtis' erratic behaviour, including a sudden disinterest in the project he had started, forced HEC to dismiss him from Roundabout. But Lord and Blackmore were keen to continue, and carried on recruiting additional members, keeping Tony Edwards as their manager.
For the bass guitar, Lord suggested his old friend Nick Simper, with whom he had played in a band called The Flower Pot Men (formerly known as the Ivy League) back in 1967. Simper had previously been in Johnny Kidd and the Pirates and survived the car crash that killed Kidd. Simper had known Blackmore since the early 1960s when his first band, the Renegades, debuted around the same time as one of Blackmore's early bands, the Dominators. Bobby Woodman was the initial choice for the drums, but during the auditions for a singer, Rod Evans of the Maze came in with his drummer, Ian Paice. Blackmore had seen Paice on tour with the Maze in Germany in 1966, and had been impressed by the 18-year-old's drumming. While Woodman was out for cigarettes, Blackmore quickly arranged an audition for Paice. Both Paice and Evans won their respective jobs, and the line-up was complete.
The band began in earnest in March 1968 at Deeves Hall, a country house in South Mimms, Hertfordshire. The band would live, write and rehearse at Deeves Hall, which was fully kitted out with the latest Marshall amplification. After a brief tour of Denmark and Sweden in April, in which they were still billed as Roundabout, Blackmore suggested a new name: "Deep Purple", named after his grandmother's favourite song. The group had resolved to choose a name after everyone had posted one on a board in rehearsal. Second to Deep Purple was "Concrete God", which the band thought was too harsh to take on.
Early years (1968–70).
In May 1968, the band moved into Pye Studios in London's Marble Arch to record their debut album, "Shades of Deep Purple", which was released in July by American label Tetragammaton, and in September by UK label EMI. The group had success in North America with a cover of Joe South's "Hush", and by September 1968, the song had reached number 4 on the "Billboard" Hot 100 in the US and number 2 in the Canadian RPM charts, pushing the "Shades" LP up to No. 24 on Billboard's pop album charts. The following month, Deep Purple was booked to support Cream on their "Goodbye" tour.
The band's second album, "The Book of Taliesyn", was quickly recorded, then released in North America to coincide with the tour. The album included a cover of Neil Diamond's "Kentucky Woman", which cracked the Top 40 in both the US (#38 on the "Billboard" charts) and Canada (#21 on the "RPM" charts), though sales for the album were not as strong (#54 in US, #48 in Canada). "The Book of Taliesyn" would not be released in the band's home country until the following year, and like its predecessor, it failed to have much impact in the UK charts.
Early in 1969, the band recorded a single called "Emmaretta", named after Emmaretta Marks, then a cast member of the musical "Hair", whom Evans was trying to seduce. By March of that year, the band had completed recording for their third album, "Deep Purple". The album contained strings and woodwind on one track ("April"), showcasing Lord's classical antecedents such as Bach and Rimsky-Korsakov, and several other influences were in evidence, notably Vanilla Fudge. (Lord and Blackmore had even claimed the group wanted to be a "Vanilla Fudge clone".) This would be the last recording by the original line-up.
Deep Purple's troubled North American record label, Tetragrammaton, delayed production of the "Deep Purple" album until after the band's 1969 American tour ended. This, as well as lackluster promotion by the nearly broke label, caused the album to sell poorly, finishing well out of the "Billboard" Top 100. Soon after the third album's eventual release, Tetragrammaton went out of business, leaving the band with no money and an uncertain future. (Tetragrammaton's assets were assumed by Warner Bros. Records, who would release Deep Purple's records in the US throughout the 1970s.) During the 1969 American tour, Lord and Blackmore met with Paice to discuss their desire to take the band in a heavier direction. Feeling that Evans and Simper would not fit well with a heavy rock style, both were replaced that summer. Paice stated, "A change had to come. If they hadn't left, the band would have totally disintegrated." Both Simper and Blackmore noted that Rod Evans already had one foot out the door. Simper said that Evans had met a girl in Hollywood and had eyes on being an actor, while Blackmore explained, "Rod just wanted to go to America and live in America."
In search of a replacement vocalist, Blackmore set his own sights on 19-year-old singer Terry Reid. Though he found the offer "flattering", Reid was still bound by the exclusive recording contract with his producer Mickie Most and more interested in his solo career. Blackmore had no other choice but to look elsewhere. The band hunted down singer Ian Gillan from Episode Six, a band that had released several singles in the UK without achieving their big break for commercial success. Gillan had at one time been approached by Nick Simper when Deep Purple was first forming, but Gillan had reportedly told Simper that the Roundabout project would not go anywhere, while he felt Episode Six was poised to make it big. Six's drummer Mick Underwood – an old comrade of Blackmore's from his days in the Outlaws – introduced the band to Gillan and bassist Roger Glover. This effectively killed Episode Six and gave Underwood a guilt complex that lasted nearly a decade, until Gillan recruited him for his new post-Purple band in the late 1970s. According to Blackmore, Deep Purple was only interested in Gillan and not Glover, but Roger was retained on the advice of Ian Paice.
This created the Deep Purple Mark II line-up, whose first release was a Greenaway-Cook tune titled "Hallelujah". At the time of its recording, Nick Simper still thought he was in the band, and had called the studio to inquire about the recording dates for the song. It was then he found that the song had already been recorded with Glover on bass. The remaining original members of Deep Purple then instructed management to inform Simper that he had been officially replaced.
Despite television appearances to promote the "Hallelujah" single in the UK, the song flopped. Blackmore had told the British weekly music newspaper "Record Mirror" they "need to have a commercial record in Britain", and described the song as "an in-between sort of thing"—a median between what the band would normally make but with an added commercial motive.
The band gained some much-needed publicity in September 1969, with the "Concerto for Group and Orchestra", a three-movement epic composed by Lord as a solo project and performed by the band at the Royal Albert Hall in London with the Royal Philharmonic Orchestra, conducted by Malcolm Arnold. Together with "Days of Future Passed" by the Moody Blues and "Five Bridges" by the Nice, it was one of the first collaborations between a rock band and an orchestra. This live album became their first album with any kind of chart success in the UK. Gillan and Blackmore were less than happy at the band being tagged as "a group who played with orchestras", both feeling that the Concerto was a distraction that would get in the way of developing their desired hard-rocking style. Lord acknowledged that while the band members were not keen on the project going in, at the end of the performance "you could put the five smiles together, and it would have spanned the Thames." Lord would also write the "Gemini Suite", another orchestra/group collaboration in the same vein, for the band in late 1970. In 1975, Blackmore stated that he thought the "Concerto for Group and Orchestra" wasn't bad but the "Gemini Suite" was horrible and very disjointed. Roger Glover later claimed Jon Lord had appeared to be the leader of the band in the early years.
Breakthrough success (1970–73).
Shortly after the orchestral release, Deep Purple began a hectic touring and recording schedule that was to see little respite for the next three years. Their first studio album of this period, released in mid-1970, was "In Rock" (a name supported by the album's Mount Rushmore-inspired cover), which contained the then-concert staples "Speed King", "Into The Fire" and "Child in Time". The single "Black Night" finally put Deep Purple into the UK Top Ten. The interplay between Blackmore's guitar and Lord's distorted organ, coupled with Gillan's howling vocals and the rhythm section of Glover and Paice, now started to take on a unique identity that separated the band from its earlier albums. Along with Zeppelin's "Led Zeppelin II" and Sabbath's "Paranoid", "In Rock" codified the budding heavy metal genre.
On the album's development, Blackmore stated: "I got fed up with playing with classical orchestras, and thought, 'well, this is my turn.' Jon was into more classical. I thought, 'well you do that, I'll do rock.' And I said, 'If this fails, this record, I'll play with orchestras the rest of my life.'" "In Rock" performed well, especially in the UK where it reached number 4, while the "Black Night" single reached number 2 on the UK Singles Chart, and the band performed the song live on the BBC's "Top of the Pops". A second album, the creatively progressive "Fireball", was issued in the summer of 1971, reaching number 1 on the UK Albums Chart. The title track "Fireball" was released as a single, as was "Strange Kind of Woman", not from the album but recorded during the same sessions (although it replaced "Demon's Eye" on the US version of the album). "Strange Kind of Woman" became their second UK Top 10 single, reaching number 8.
Within weeks of "Fireball"'s release, the band were already performing songs planned for the next album. One song (which later became "Highway Star") was performed at the first gig of the "Fireball" tour, having been written on the bus to a show in Portsmouth, in answer to a journalist's question: "How do you go about writing songs?" Three months later, in December 1971, the band travelled to Switzerland to record "Machine Head". The album was due to be recorded at the Montreux Casino, using the Rolling Stones Mobile Studio, but a fire during a Frank Zappa and the Mothers of Invention gig, caused by a man firing a flare gun into the ceiling, burned down the Casino. This incident famously inspired the song "Smoke on the Water". The album was later recorded in a corridor at the nearby empty Grand Hotel.
Continuing from where both previous albums left off, "Machine Head" became one of the band's most famous albums. It became the band's second number 1 album in the UK, while re-establishing Deep Purple in North America, hitting number 7 in the US and number 1 in Canada. It included tracks that became live classics, such as "Highway Star", "Space Truckin'", "Lazy" and "Smoke on the Water", for which Deep Purple is most famous. Deep Purple continued to tour and record at a rate that would be rare thirty years on; when "Machine Head" was recorded, the group had only been together three and a half years, yet the album was their sixth.
Meanwhile, the band undertook four North America tours in 1972, and a Japan tour that led to a double-vinyl live release, "Made in Japan". Originally intended as a Japan-only record, its worldwide release saw the double LP become an instant hit. It remains one of rock music's most popular and highest selling live-concert recordings.
The classic Deep Purple Mark II line-up continued to work, and released the album "Who Do We Think We Are" in 1973. Featuring the hit single "Woman from Tokyo", the album hit number 4 in the UK charts and number 15 in the US charts while achieving gold record status faster than any Deep Purple album released up to that time. But internal tensions and exhaustion were more noticeable than ever. Following the successes of "Machine Head" and "Made in Japan", the addition of "Who Do We Think We Are" made them the top-selling artists of 1973 in the US.
New line-up, successes and struggles (1973–76).
Ian Gillan admitted in a 1984 interview that the band was pushed by management to complete the "Who Do We Think We Are" album on time and go on tour, although they badly needed a break. The bad feelings culminated in Gillan, followed by Glover, quitting the band after their second tour of Japan in the summer of 1973 over tensions with Blackmore. In interviews years later, Jon Lord called the departure of Gillan and Glover while the band was at its peak "the biggest shame in rock and roll; God knows what we would have become over the next three or four years."
The band first hired Midlands bassist/vocalist Glenn Hughes, formerly of Trapeze. According to Ian Paice, Glover had told him and Lord a few months before his official resignation that he wanted to leave the band, so they had already started to drop in on Trapeze shows. After acquiring Hughes, they debated continuing as a four-piece band, with Hughes as both bassist and lead vocalist. According to Hughes, he was persuaded to join under the guise that the band would be bringing in Paul Rodgers of Free as a co-lead vocalist, but by that time Rodgers had just started Bad Company. Instead, auditions were held for lead vocal replacements. They settled on David Coverdale, an unknown singer from Saltburn in Northeast England, primarily because Blackmore liked his masculine, blues-tinged voice.
This new line-up continued into 1974, and their spring tour included shows at Madison Square Garden, New York on 13 March, and Nassau Coliseum four days later. The band then headlined the famous California Jam festival at Ontario Motor Speedway located in Southern California on 6 April 1974. Attracting over 250,000 fans, the festival also included 1970s rock giants Black Sabbath, Eagles, Emerson, Lake & Palmer, Earth, Wind & Fire, Seals and Crofts, Rare Earth and Black Oak Arkansas. Portions of the show were telecast on ABC Television in the US, exposing the band to a wider audience. This line-up's first album, titled "Burn", was a highly successful release, reaching No. 3 in the UK, No. 9 in the US, and was followed by another world tour. The title track "Burn", which opens the album, was a conscious effort by the band to embrace the progressive rock movement that was popularised at the time by bands such as Yes, ELP, Genesis, Gentle Giant, etc. "Burn" was a complex arrangement which showcased all the band members' musical virtuosity and particularly Blackmore's classically influenced guitar prowess. The album also featured Hughes and Coverdale providing vocal harmonies and elements of funk and blues, respectively, to the band's music, a sound that was even more apparent on the late 1974 release "Stormbringer". Besides the title track, the "Stormbringer" album had a number of songs that received much radio play, such as "Lady Double Dealer", "The Gypsy" and "Soldier of Fortune", and the album reached No. 6 in the UK and No. 20 on the US "Billboard" charts. However, Blackmore publicly disliked the album and the funky soul elements, even calling it "shoeshine music". As a result, he left the band on 21 June 1975 to form his own band with Ronnie James Dio of Elf, called Ritchie Blackmore's Rainbow, later shortened to Rainbow after one album.
With Blackmore's departure, Deep Purple was left to fill one of the biggest band member vacancies in rock music. In spite of this, the rest of the band refused to stop, and announced a replacement for Blackmore: American Tommy Bolin. Before Bolin was recruited, Clem Clempson (Colosseum, Humble Pie), Zal Cleminson (The Sensational Alex Harvey Band), Mick Ronson (David Bowie & The Spiders From Mars) and Rory Gallagher were considered for the part.
There are at least two versions about the recruitment of Bolin: Coverdale claims to have been the one who suggested auditioning Bolin. "He walked in, thin as a rake, his hair coloured green, yellow and blue with feathers in it. Slinking along beside him was this stunning Hawaiian girl in a crochet dress with nothing on underneath. He plugged into four Marshall 100-watt stacks and...the job was his". But in an interview originally published by Melody Maker in June 1975, Bolin himself claimed that he came to the audition following a recommendation from Blackmore. Bolin had been a member of many now-forgotten late-1960s bands – Denny & The Triumphs, American Standard, and Zephyr, which released three albums from 1969 to 1972. Before Deep Purple, Bolin's best-known recordings were made as a session musician on Billy Cobham's 1973 jazz fusion album "Spectrum", and as lead guitarist on two post-Joe Walsh James Gang albums: "Bang" (1973) and "Miami" (1974). He had also jammed with such luminaries as Dr. John, Albert King, the Good Rats, Moxy and Alphonse Mouzon, and was busy working on his first solo album, "Teaser", when he accepted the invitation to join Deep Purple.
The resulting album, "Come Taste the Band", was released in October 1975, one month before Bolin's "Teaser" album. Despite mixed reviews and so-so sales (#19 in the UK charts and No. 43 in the US Billboard charts), the collection revitalised the band once again, bringing a new, extreme funk edge to their hard rock sound. Bolin's influence was crucial, and with encouragement from Hughes and Coverdale, the guitarist developed much of the album's material. Despite Bolin's talents, his personal problems with hard drugs began to manifest themselves. During the "Come Taste the Band" tour, many fans openly booed Tommy's inability to play solos like Ritchie Blackmore, not realising that the former was physically hampered by his addiction. After several below-par concert performances, the band was in danger.
Band split and solo projects (1976–84).
The end came on tour in England on 15 March 1976 at the Liverpool Empire Theatre. Coverdale reportedly walked off in tears and handed in his resignation, to which he was allegedly told there was no band left to quit. The decision to disband Deep Purple had been made some time before the last show by Lord and Paice (the last remaining original members), who hadn't told anyone else. The break-up was finally made public in July 1976, with then-manager Rob Cooksey issuing the simple statement: "the band will not record or perform together as Deep Purple again".
Later in the year, Bolin had just finished recording his second solo album, "Private Eyes", when, on 4 December 1976, tragedy struck. In a Miami hotel room, during a tour supporting Jeff Beck, Bolin was found unconscious by his girlfriend and bandmates. Unable to wake him, she hurriedly called paramedics, but it was too late. The official cause of death was multiple-drug intoxication. Bolin was 25 years old.
After the break-up, most of the past and present members of Deep Purple went on to have considerable success in a number of other bands, including Gillan, Whitesnake and Rainbow. There were, however, a number of promoter-led attempts to get the band to reform, especially with the revival of the hard rock market in the late 1970s and early 1980s. In 1980, a touring version of the band surfaced with Rod Evans as the only member who had ever been in Deep Purple, eventually ending in successful legal action from the legitimate Deep Purple camp over unauthorised use of the name. Evans was ordered to pay damages of US$672,000 for using the band name without permission.
Reformation, reunions and turmoil (1984–94).
In April 1984, eight years after the demise of Deep Purple, a full-scale (and legal) reunion took place with the "classic" early 1970s line-up of Gillan, Lord, Blackmore, Glover and Paice. The reformed band signed a worldwide deal with PolyGram, with Mercury Records releasing their albums in the US, and Polydor Records in the UK and other countries. The album "Perfect Strangers" was recorded in Vermont and released in October 1984. The album was commercially successful, reaching number 5 in the UK Albums Chart and number 17 on the "Billboard" 200 in the US. The album included the singles and concert staples "Knockin' At Your Back Door" and "Perfect Strangers". "Perfect Strangers" became the second Deep Purple studio album to go platinum in the US, following "Machine Head".
The reunion tour followed, starting in Australia and winding its way across the world to North America, then into Europe by the following summer. Financially, the tour was also a tremendous success. In the US, the 1985 tour out-grossed every other artist except Bruce Springsteen. The UK homecoming saw the band perform a concert at Knebworth on 22 June 1985 (with main support from the Scorpions; also on the bill were UFO and Meat Loaf), where the weather was bad (torrential rain and 6" of mud) in front of 80,000 fans. The gig was called the "Return of the Knebworth Fayre".
The Mark II line-up then released "The House of Blue Light" in 1987, which was followed by a world tour (interrupted after Blackmore broke a finger on stage while trying to catch his guitar after throwing it in the air) and another live album "Nobody's Perfect" (1988) which was culled from several shows on this tour, but still largely based on the by-now familiar "Made in Japan" set-list. In the UK a new version of "Hush" (with Gillan on lead vocals) was released to mark 20 years of the band. In 1989 Gillan was fired as his relations with Blackmore had again soured and their musical differences had diverged too far. Originally, the band intended to recruit Survivor frontman Jimi Jamison as Gillan's replacement, but this fell through due to complications with Jamison's record label. Eventually, after auditioning several high-profile candidates, including Brian Howe (White Spirit, Ted Nugent, Bad Company), Doug Pinnick (King's X), Australians Jimmy Barnes (Cold Chisel) and John Farnham (Little River Band), Terry Brock (Strangeways, Giant) and Norman "Kal" Swan (Tytan, Lion, Bad Moon Rising), former Rainbow vocalist Joe Lynn Turner was recruited into the band. This Mark V line-up recorded just one album, "Slaves & Masters" (1990) and toured in support. It achieved modest success and reached number 87 in the Billboard Charts in the US, but some fans criticised it as little more than a so-called "generic Foreigner wannabe" album.
With the tour complete, Turner was forced out, as Lord, Paice and Glover (and the record company) wanted Gillan back in the fold for the 25th anniversary. Blackmore grudgingly relented, after requesting and eventually receiving 250,000 dollars in his bank account and the classic line-up recorded "The Battle Rages On...". However, Gillan reworked much of the existing material which had been written with Turner for the album. As a result, Blackmore became infuriated at what he considered non-melodic elements. During an otherwise successful European tour, Blackmore walked out in 1993, for good, after a show on 17 November in Helsinki, Finland. Joe Satriani was drafted to complete the Japanese dates in December and stayed on for a European Summer tour in 1994. He was asked to join permanently, but his commitments to his contract with Epic Records prevented this. The band unanimously chose Dixie Dregs/Kansas guitarist Steve Morse to become Satriani's successor.
Revival with Steve Morse and longer tours (1994–present).
Morse's arrival revitalised the band creatively, and in 1996 a new album titled "Purpendicular" was released, showing a wide variety of musical styles, though it never made chart success on the Billboard 200 in the US. The Mark VII line-up then released a new live album "Live at The Olympia '96" in 1997. With a revamped set list to tour, Deep Purple enjoyed successful tours throughout the rest of the 1990s, releasing the harder-sounding "Abandon" in 1998, and touring with renewed enthusiasm. In 1999, Lord, with the help of a Dutch fan, who was also a musicologist and composer, Marco de Goeij, painstakingly recreated the "Concerto for Group and Orchestra", the original score having been lost. It was once again performed at the Royal Albert Hall in September 1999, this time with the London Symphony Orchestra conducted by Paul Mann. The concert also featured songs from each member's solo careers, as well as a short Deep Purple set, and the occasion was commemorated on the 2000 album "Live at the Royal Albert Hall". In 2001, the box set "The Soundboard Series" was released featuring concerts from the 2001 Australian Tour plus two from Tokyo, Japan.
Much of the next few years was spent on the road touring. The group continued forward until 2002, when founding member Lord (who, along with Paice, was the only member to be in all incarnations of the band) announced his amicable retirement from the band to pursue personal projects (especially orchestral work). Lord left his Hammond organ to his replacement, rock keyboard veteran Don Airey (Colosseum II, Rainbow, Ozzy Osbourne, Black Sabbath, Whitesnake), who had helped Deep Purple out when Lord's knee was injured in 2001. In 2003, Deep Purple released their first studio albums in five years ("Bananas") and began touring in support of the album. EMI Records refused a contract extension with Deep Purple, possibly because of lower than expected sales. Actually "In Concert with the London Symphony Orchestra" sold more than "Bananas". Most of the songs played in their live concerts consist of classic 1970s material. In July 2005, the band played at the Live 8 concert in Park Place (Barrie, Ontario) and, in October released their next album, "Rapture of the Deep", which was followed by the Rapture of the Deep tour. This Mark VIII line-up's two studio albums were produced by Michael Bradford.
In February 2007, Gillan asked fans not to buy a live album "Come Hell or High Water" being released by Sony BMG. This was a recording of their 1993 appearance at the NEC in Birmingham, England. Recordings of this show have previously been released without assistance from Gillan or any other members of the band, but he said: "It was one of the lowest points of my life – all of our lives, actually". In 2009, Ian Gillan said, "Record sales have been steadily declining, but people are prepared to pay a lot for concert tickets." In addition, Gillan stated "I don't think happiness comes with money." In 2011, Deep Purple did concert tours in 48 countries. The Songs That Built Rock Tour featured a 38-piece orchestra, and included a performance at London's O2 Arena. Until May 2011, the band members had disagreed about whether to make a new studio album, because it would not really make money any more. Roger Glover stated that Deep Purple should make a new studio album "even if it costs us money."
In early 2011, David Coverdale and Glenn Hughes told VH1 they would like to reunite with former Deep Purple Mark III line-up for the right opportunity, such as a benefit concert. The current band's chief sound engineer on nine years of tours, Moray McMillin, died in September 2011, aged 57.
After a lot of songwriting sessions in Europe, Deep Purple decided to record through the summer of 2012, and the band announced the release of their new studio album in 2013. Steve Morse announced to French magazine "Rock Hard" that the new studio album would be produced by the highly respected Bob Ezrin, who is known for his works with Alice Cooper, Kiss, and Pink Floyd. On 16 July 2012, the band's co-founding member and former organ player, Jon Lord, died in London, aged 71. In December 2012, Roger Glover revealed in an interview that the band has completed work on 14 songs for a new studio album, with 11 or 12 tracks set to appear on the final album to be released in 2013. On 26 February 2013, the title of the band's new album was announced as "Now What?!", which was recorded and mixed in Nashville, Tennessee.
Legacy.
Deep Purple are cited as one of the pioneers of hard rock and heavy metal, along with Led Zeppelin and Black Sabbath. The group have influenced a number of rock and metal bands including Metallica, Queen, Aerosmith, Van Halen, Alice in Chains, Pantera, Bon Jovi, Europe, Rush, Motörhead, and many New Wave of British Heavy Metal bands such as Iron Maiden, Judas Priest, and Def Leppard. Iron Maiden's bassist and primary songwriter, Steve Harris, states that his band's "heaviness" was inspired by "Black Sabbath and Deep Purple with a bit of Zeppelin thrown in."
In 2000, Deep Purple were ranked number 22 on VH1's "100 Greatest Artists of Hard Rock" programme. At the 2008 World Music Awards the band received the Legend Award. In 2011, they received the Innovator Award at the 2011 "Classic Rock" Awards in London. A "Rolling Stone" readers' poll in 2012 ranked "Made in Japan" the sixth best live album of all time. As part of the 40th anniversary celebrations of "Machine Head" (1972), "" was released on 25 September 2012. This tribute album features artists such as Iron Maiden, Metallica, Steve Vai, Carlos Santana, Chickenfoot consisting of former Van Halen members Sammy Hagar and Michael Anthony, guitarist Joe Satriani and Chad Smith of Red Hot Chili Peppers, the Flaming Lips, Black Label Society, Papa Roach vocalist Jacoby Shaddix, and the supergroup Kings of Chaos featuring Def Leppard vocalist Joe Elliott, Steve Stevens, and former Guns N' Roses members Duff McKagan and Matt Sorum.
Rock and Roll Hall of Fame.
Prior to October 2012, Deep Purple had not been nominated for induction into the Rock and Roll Hall of Fame (though they have been eligible since 1993), but were nominated for induction in 2012 and 2013. Despite ranking second in the public's vote on the Rock Hall fans' ballot, which had over half a million votes, they were not inducted by the Rock Hall committee. Kiss bassist Gene Simmons and Rush bassist Geddy Lee commented that Deep Purple should obviously be among the Rock and Roll Hall of Fame inductees. There have been criticisms in the past over Deep Purple not having been inducted. Toto guitarist Steve Lukather commented, "they put Patti Smith in there but not Deep Purple? What's the first song every kid learns how to play? ["Smoke on the Water"]...And they're not in the Rock and Roll Hall of Fame? ...the Rock and Roll Hall of Fame has lost its cool because of the glaring omissions." Former Guns N' Roses guitarist Slash expressed his surprise and disagreement for the non-induction of Deep Purple; "The list of people who haven't even been nominated is mind-boggling..big one for me is Deep Purple. How could you not induct Deep Purple?". Metallica band members James Hetfield, Lars Ulrich and Kirk Hammett have also lobbied for the band's induction. In an interview with "Rolling Stone" in April 2014, Ulrich pleaded: "I'm not gonna get into the politics or all that stuff, but I got two words to say: 'Deep Purple'. That's all I have to say: Deep Purple. Seriously, people, Deep Purple. Two simple words in the English language...'Deep Purple'! Did I say that already?" In 2015, Chris Jericho, former WWE wrestler and current vocalist of rock band Fozzy, stated: "that Deep Purple are not in it of Fame. It's bullshit. Obviously there's some politics against them from getting in there."
In response to these, a Hall of Fame chief executive said, "The definition of 'rock and roll' means different things to different people, but as broad as the classifications may be, they all share a common love of the music." Roger Glover remains ambivalent about induction and got an inside word from the Hall, “One of the jurors said, ‘You know, Deep Purple, they’re just one-hit wonders.’ How can you deal with that kind of Philistinism, you know?”. Ian Gillan also commented, "I've fought all my life against being institutionalised and I think you have to actively search these things out, in other words mingle with the right people, and we don't get invited to those kind of things." On 16 October 2013 Deep Purple were again announced as nominees for inclusion to the Hall, and once again they were not inducted.
In April 2015, Deep Purple topped the list in a "Rolling Stone" readers poll of acts that should be inducted into the Hall of Fame in 2016. In October 2015, the band were nominated for induction for the third time. In December 2015, the band were announced as 2016 inductees into the Hall of Fame, with the Hall stating: "Deep Purple's non-inclusion in the Hall is a gaping hole which must now be filled," adding that along with fellow inductees Led Zeppelin and Black Sabbath, the band make up "the Holy Trinity of hard rock and metal bands." The band was officially inducted on April 8, 2016. The Hall of Fame announced that the following members were included as inductees: Ian Paice, Jon Lord, Ritchie Blackmore, Roger Glover, Ian Gillan, Rod Evans, David Coverdale and Glenn Hughes. Excluded from induction were Nick Simper, Tommy Bolin, Joe Lynn Turner, Steve Morse and Don Airey.
Concert tours.
Deep Purple are considered to be one of the hardest touring bands in the world. From 1968 until today (with the exception of their 1976–1984 split) they continue to tour around the world. In 2007, the band received a special award for selling more than 150,000 tickets in France, with 40 dates in the country in 2007 alone. Also in 2007, Deep Purple's Rapture of the Deep Tour was voted number 6 concert tour of the year (in all music genres) by Planet Rock listeners. The Rolling Stones' A Bigger Bang Tour was voted number 5 and beat Purple's tour by only 1%. Deep Purple released a new live compilation DVD box, Around the World Live, in May 2008. In February 2008, the band made their first ever appearance at the Kremlin Palace in Moscow, Russia at the personal request of Dmitry Medvedev who at the time was considered a shoo-in for the seat of the Presidency of Russia. Prior to that, Deep Purple has toured Russia several times starting as early as 1996, but has not been considered to have played such a significant venue previously. The band was part of the entertainment for the FIS Nordic World Ski Championships 2009 in Liberec, Czech Republic.

</doc>
<doc id="45849" url="https://en.wikipedia.org/wiki?curid=45849" title="Chinese democracy movement">
Chinese democracy movement

The Chinese democracy movement (), abbreviated as Minyun () refers to a series of loosely organized political movements in the People's Republic of China against the continued one-party rule by the Communist Party. One such movement began during the Beijing Spring in 1978 and was taken up again in the Tiananmen Square protests of 1989. In the 1990s, Chinese democracy movements , in part due to a number of philosophical reasons, and are fragmented and not considered by most analysts to be a serious threat to power of the government, at least at present.
History.
The origin of the Chinese movement started in 1978, when the brief liberalization known as Beijing Spring occurred after the Cultural Revolution. The founding document of the movement is considered to be the Fifth Modernization manifesto by Wei Jingsheng, who was sentenced to fifteen years in prison for authoring the document. In it, Wei argued that political liberalization and the empowering of the laboring masses was essential for modernization, that the Communist Party was controlled by reactionaries, and that the people must struggle to overthrow the reactionaries via a long and possibly bloody fight.
Throughout the 1980s, these ideas increased in popularity among college educated Chinese. In response to the growing corruption, the economic dislocation, and the sense that reforms in the Soviet Union and Eastern Europe were leaving China behind, the Tiananmen Square protests erupted in 1989. These protests were put down by government troops on June 4, 1989. In response, a number of pro-democracy organizations were formed by overseas Chinese student activists, and there was considerable sympathy for the movement among Westerners, who formed the China Support Network (CSN).
While the CSN was initially a go-to organization for U.S. mainstream news media (MSM) to cite, CSN and MSM parted company in a dispute over the casualty count from the June 4 massacre. MSM originally reported 3,000 dead. On June 22, 1989, Agence France Press referred to "the Chinese army's assault on the demonstrators in and around Beijing's Tiananmen Square, an operation in which U.S. intelligence sources estimated "3,000 people were killed." That casualty count, originally reported as above, was subsequently changed by the news media. CAN reported that it was the interest of China's propaganda minister to reduce the casualty count by an order of magnitude, resulting in later reports that "hundreds" were killed at Tiananmen Square. In November, 1989 CSN editor James W. Hawkins MD wrote, "It appears as if Mr. Yuan Mu State Council spokesman has gotten his way and when we read reports on the AP wire we are told exactly what Mr. Mu ["sic"] wants us to read."
The rift between CSN and MSM plays into the history of the movement. In January, 2005 upon the death of ousted Communist Party General Secretary Zhao Ziyang, CSN raised its estimate to 3,001 dead in the Tiananmen crackdown. CSN proceeded to be critical of the MSM, and MSM proceeded to minimize, downplay, ignore, or underreport movement news and China's human rights abuse.
Current situation.
This could be in part the result of the Chinese government tightening its control over its people's freedom of speech, thus giving the appearance of disinterest, or as a result of the overall economical and social reforms China has undertaken in recent years. The difficulties that the Soviet Union had in converting to democracy and capitalism was used to validate the PRC's official position that slow gradual reform was a wise policy. Structurally, democracy promotion organizations in the United States such as the China Alliance for Democracy, the Federation for a Democratic China and the Independent Federation of Chinese Students and Scholars suffered from internal disputes and infighting. Much support was lost over the issue of Most Favored Nation trade status and China's entry into the World Trade Organization which was popular both within and outside of China, but which were opposed by 79% of the American people (in a poll published by Business Week) and the overseas democracy movement.
Censorship in Mainland China is very strict, including in the Internet. The new generation finds it difficult to obtain, or are unaware of, the truth regarding several important historical events which occurred before they were born.
A generation gap has begun to appear between older and younger students when people born after the Cultural Revolution began entering college campuses. These students perceived the older activists as more pro-American than pro-democracy, and thus they are far more supportive of the Communist Party. The younger students also tend to be more nationalistic. Internal disputes within the movement over such issues as China's most-favored nation status in US trade law crippled the movement; as did the perception by many within China that overseas dissidents such as Harry Wu and Wei Jingsheng were simply out of touch with the growing economic prosperity and decreasing political control within China.
Government response.
Ideologically, the government's first reaction to the democracy movement was an effort to focus on the personal behavior of individual dissidents and argue that they were tools of foreign powers. In the mid-1990s, the government began using more effective arguments which were influenced by Chinese Neo-Conservatism and Western authors such as Edmund Burke. The main argument was that China's main priority was economic growth, and economic growth required political stability. The democracy movement was flawed because it promoted radicalism and revolution which put the gains that China had made into jeopardy. In contrast to Wei's argument that democracy was essential to economic growth, the government argued that economic growth must come before political liberalization, comparable to what happened in the Asian Tigers.
With regard to political dissent engendered by the movement, the government has taken a three pronged approach. First, dissidents who are widely known in the West such as Wei Jingsheng, Fang Lizhi, and Wang Dan are deported. Although Chinese criminal law does not contain any provisions for exiling citizens, these deportations are conducted by giving the dissident a severe jail sentence and then granting medical parole. Second, the less well-known leaders of a dissident movement are identified and given severe jail sentences. Generally, the government targets a relatively small number of organizers who are crucial in coordinating a movement and who are then charged with endangering state security or revealing official secrets. Thirdly, the government attempts to address the grievances of possible supporters of the movement. This is intended to isolate the leadership of the movement, and prevent disconnected protests from combining into a general organized protest that can threaten the Communist hold on power.
Chinese socialist democracy.
Chinese Communist Party leaders assert there are already elements of democracy; they dubbed the term "Chinese socialist democracy" for what they describe as a participatory representative government.
For example, in a November 23, 2002 interview, the Chinese ambassador to Egypt, Liu Xiaoming, said:
I think what we are practicing today is Chinese socialist democracy, which is represented by the National People's Congress and a broad participation of the Chinese people. In fact, in today's China, the political participation at the grassroots level is much higher than any western country you can name of. We have grassroots level democracy demonstrated by village election. The turnout is 99 percent, i.e. 99% of villagers participating in this political process to elect their village leaders, comparing with only less than 50% of participation in election process in many western countries.
Modern democracy activism.
Many pro-democracy supporters noted that China has successfully overcome much of the challenges to democracy in China faced during the transition from a communist to a capitalist economy so there is no longer a need for prolonged political repression. They claim that pro-democracy forces would not necessarily stall economic growth after the transition, as the Communist Party states, and more importantly that the presence of democracy would help to check wasteful corruption and might achieve a more even distribution of wealth. Many believe that the Communist Party of China has no intention whatsoever of ever relinquishing power even if all their economic goals are ever achieved; it is said that China would have refused the WTO if the terms of entry were linked to a shift to a Western-style democracy.
Within China, most protest activity now is expressed in single-issue demonstrations, which are tolerated to a degree by the government. Some of the ideas of the movement have been incorporated in the Chinese liberal faction who tend to agree with neoconservatives that stability is important, but argue that political liberalization is essential to maintain stability. In contrast to democracy movement activists, most members of the liberal faction do not overtly call for the overthrow of the Communist Party nor do they deny the possibility of reform from within the Party. As a result, members of the liberal faction are generally enjoying more official tolerance than persons who identify themselves as members of the democracy movement. 

</doc>
<doc id="45851" url="https://en.wikipedia.org/wiki?curid=45851" title="Opeth">
Opeth

Opeth is a Swedish heavy metal band from Stockholm, formed in 1990. Though the group has been through several personnel changes, singer, guitarist, and songwriter Mikael Åkerfeldt has remained Opeth's only founding member and primary driving force throughout the years. Opeth has consistently incorporated progressive, folk, blues, classical and jazz influences into their usually lengthy compositions, as well as strong influences from death metal, especially in their early works. Many songs include acoustic guitar passages and strong dynamic shifts, as well as both death growls and clean vocals. Opeth rarely made live appearances supporting their first four albums; but since conducting their first world tour after the 2001 release of "Blackwater Park", they have led several major world tours.
Opeth has released eleven studio albums, three live DVDs, three live albums (two that are in conjunction with DVDs), and two boxsets. The band released its debut album "Orchid" in 1995. Although their eighth studio album, "Ghost Reveries", was quite popular in the United States, Opeth did not experience major American commercial success until the 2008 release of their ninth studio album, "Watershed", which peaked at No. 23 on the "Billboard" 200, and topped the Finnish albums chart in its first week of release. Opeth has sold over 1.5 million albums and DVDs worldwide, including 300,000 collective SoundScans of their albums "Blackwater Park", "Damnation" and "Deliverance" in the United States.
History.
Formation (1990–1993).
Opeth was formed as a death metal band in the autumn of 1990 in Stockholm, Sweden by vocalist David Isberg. Isberg asked former Eruption band member Mikael Åkerfeldt (just 16 years old at the time) to join Opeth as a bassist. When Åkerfeldt showed up to practice the day after Isberg invited him, it became clear that Isberg had not told the band members, including the band's current bassist, that Åkerfeldt would be joining. An ensuing argument led to all members but Isberg and Åkerfeldt leaving to form a new project. The band name was derived from the word "Opet," taken from the Wilbur Smith novel "The Sunbird". In this novel, Opet is the name of a fictional Phoenician city in South Africa whose name is translated as "City of the Moon".
Isberg and Åkerfeldt recruited drummer Anders Nordin, bassist Nick Döring, and guitarist Andreas Dimeo. Unsatisfied with Opeth's slow progress, Döring and Dimeo left the band after their first performance, and were replaced by guitarist Kim Pettersson and bassist Johan De Farfalla. After the next show, DeFarfalla left Opeth to spend time with his girlfriend in Germany, and was initially replaced by Mattias Ander, before Åkerfeldt's friend Peter Lindgren took on the role of bassist. Rhythm guitarist Kim Pettersson left following the band's next performance, and Lindgren switched to guitar, with the role of bassist falling to Stefan Guteklint. The following year, David Isberg left the band citing "creative differences".
Following Isberg's departure, Åkerfeldt took over vocal duties and he, Lindgren, and Nordin spent the next year writing and rehearsing new material. The group began to rely less on the blast beats and aggression typical of death metal, and incorporated acoustic guitars and guitar harmonies into their music; developing the core sound of Opeth. Bassist Guteklint was dismissed by the band after they signed their first record deal with Candlelight Records in 1994. Opeth initially employed former member DeFarfalla as a session bassist for their demo recordings, and he went on to join on a full-time basis following the release of Opeth's debut album, "Orchid", in 1995.
"Orchid", "Morningrise", and "My Arms, Your Hearse" (1994–1998).
Opeth recorded its debut album, "Orchid", with producer Dan Swanö in April 1994. Because of distribution problems with the newly formed Candlelight Records, the album was not released until May 15, 1995, and only in Europe. "Orchid" tested the boundaries of traditional death metal, featuring acoustic guitars, piano, and clean vocals. AllMusic called "Orchid" "brilliant," "startlingly unique," and "a far-beyond-epic prog/death monstrosity exuding equal parts beauty and brutality."
After a few live shows in the United Kingdom, Opeth returned to the studio in March 1996 to begin work on a second album, again produced by Dan Swanö. The album was named "Morningrise", and was released in Europe on June 24, 1996. With only five songs, but lasting 66 minutes; it features Opeth's longest song, the twenty-minute "Black Rose Immortal". "Morningrise" was a huge success, with Allmusic giving the album four stars. Opeth toured the UK in support of "Morningrise", followed by a 26-date Scandinavian tour with Cradle of Filth. While on tour, Opeth attracted the attention of Century Media Records, who signed the band and released the first two albums in the United States in 1997.
In 1997, after the tour, Åkerfeldt and Lindgren dismissed DeFarfalla for personal reasons, without the consent of Nordin. When Åkerfeldt informed Nordin, who was on a vacation in Brazil, Nordin left the band and remained in Brazil for personal reasons. Former "Eternal" members, drummer Martín López (formerly of Amon Amarth) and bassist Martín Méndez, responded to an ad at a music shop placed by Åkerfeldt. The Martíns were fans of the band and took the ads down themselves so no other musicians could apply for the job. Åkerfeldt and Lindgren did not want the Martíns to join at first, due to them already knowing each other; they felt that they wanted two strangers so that there wouldn't be two camps in the band, but eventually hired both. López made his debut with Opeth playing on a cover version of Iron Maiden's "Remember Tomorrow", which was included on the album "".
With a larger recording budget from Century Media, Opeth began work on its third album, with noted Swedish producer Fredrik Nordström, at Studio Fredman in August 1997. Although Opeth had Méndez, due to time constraints Åkerfeldt played bass on the album. "My Arms, Your Hearse" was released to critical acclaim on August 18, 1998.
"Still Life" and "Blackwater Park" (1999–2001).
In 1999, the ownership of Candlelight Records changed hands, with owner and friend of the band Lee Barrett leaving the company. Opeth signed with UK label Peaceville Records in Europe, which was distributed by Music for Nations. Opeth reserved time at Studio Fredman to begin work on its next album, but recording was postponed while the studio was relocated. Due to time constraints, the band was able to rehearse only twice before entering the studio. Delays with the album's artwork pushed the release back an additional month and "Still Life" was released on October 18, 1999. Due to problems with the band's new distribution network, the album was not released in the United States until February 2001. "Still Life" was the first album recorded with Méndez, and also the first Opeth album to bear any kind of caption on the front cover upon its initial release, including the band's logo. Allmusic called "Still Life" a "formidable splicing of harsh, often jagged guitar riffs with graceful melodies." As explained by Åkerfeldt, "Still Life" is a concept album: "The main character is kind of banished from his hometown because he hasn't got the same faith as the rest of the inhabitants there. The album pretty much starts off when he is returning after several years to hook up with his old 'babe.' The big bosses of the town know that he's back... A lot of bad things start happening."
Following a few live dates in Europe, Opeth returned to Studio Fredman to begin work on its next album, with Porcupine Tree's Steven Wilson producing. The band sought to recreate the recording experience of "Still Life", and again entered the studio with minimal rehearsals, and no lyrics written. "This time it was tough," Åkerfeldt said, "I feel pleasantly blown away by the immense result, though. It was indeed worth the effort." Wilson also pushed the band to expand its sound, incorporating new sounds and production techniques. "Steve guided us into the realms of 'strange' noises for guitars and voice," Åkerfeldt said.
Opeth released its fifth studio album, "Blackwater Park", on February 21, 2001. Allmusic called "Blackwater Park" "astounding, a work of breathtaking creative breadth," noting that the album "keeps with Opeth's tradition by transcending the limits of death/black metal and repeatedly shattering the foundations of conventional songwriting." In support of "Blackwater Park", Opeth embarked on its first world tour, headlined Europe for the first time, and made an appearance at the 2001 Wacken Open Air festival in Germany, playing to a crowd of 60,000.
"Deliverance" and "Damnation" (2002–2004).
Opeth returned to Sweden after touring in support of "Blackwater Park", and began writing for the next album. At first, Åkerfeldt had trouble putting together new material: "I wanted to write something heavier than we'd ever done, still I had all these great mellow parts and arrangements which I didn't want to go to waste." Jonas Renkse of Katatonia, a long-time friend of Åkerfeldt, suggested writing music for two separate albums—one heavy and one soft.
Excited at the prospect, Åkerfeldt agreed without consulting his band mates or record label. While his band mates liked the idea of recording two separate albums, Åkerfeldt had to convince the label: "I had to lie somewhat... saying that we could do this recording very soon, it won't cost more than a regular single album." With most of the material written, the band rehearsed just once before entering Nacksving Studios in 2002, and again with producer Steven Wilson in Studio Fredman. Under pressure to complete both albums simultaneously, Åkerfeldt said the recording process was "the toughest test of our history." After recording basic tracks, the band moved production to England to first mix the heavy album, "Deliverance", with Andy Sneap at Backstage Studios. ""Deliverance" was so poorly recorded, without any organisation whatsoever," Åkerfeldt claimed, that Sneap "is credited as a 'saviour' in the sleeve, as he surely saved much of the recording."
"Deliverance" was released on November 4, 2002, and debuted at number 19 on the US Top Independent Albums chart, marking the band's first US chart appearance. Allmusic stated, ""Deliverance" is altogether more subtle than any of its predecessors, approaching listeners with haunting nuances and masterful dynamics rather than overwhelming them with sheer mass and complexity."
Opeth performed a one-off concert in Stockholm, then returned to the UK to finish recording vocals for the second of the two albums, "Damnation", at Steven Wilson's No Man's Land Studios. Although Åkerfeldt believed the band could not finish both albums, Opeth completed "Deliverance" and "Damnation" in just seven weeks of studio time, which was the same amount spent on "Blackwater Park" alone. "Damnation" was released on April 14, 2003, and garnered the band its first appearance on the US "Billboard" 200 at number 192. The album also won the 2003 Swedish Grammy Award for Best Hard Rock Performance.
The band embarked on its biggest tour yet, playing nearly 200 shows in 2003 and 2004. Opeth performed three special shows in Europe with two song lists each—one acoustic set and one heavy set. The band recorded its first DVD, "Lamentations (Live at Shepherd's Bush Empire 2003)", at Shepherd's Bush Empire in London, England. The DVD features a two-hour performance, including the entire "Damnation" album, several songs from "Deliverance" and "Blackwater Park", and a one-hour documentary about the recording of "Deliverance" and "Damnation". "Lamentations" was certified Gold in Canada.
Opeth was scheduled to perform in Jordan without a crew due to the fear of terrorist attacks in the Middle East. Opeth's tour manager distributed 6,000 tickets for the concert, but before the band left for Jordan, drummer Lopez called Åkerfeldt stating he was having an anxiety attack and could not perform, forcing the band to cancel the show. In early 2004, Lopez was sent home from Canada after more anxiety attacks on tour. Opeth decided against cancelling the remainder of the tour, with Lopez's drum technician filling in for two concerts. Lopez promised that he would return to the tour as soon as he could, but two shows later Opeth asked Strapping Young Lad drummer Gene Hoglan to fill in. Lopez returned to Opeth for the Seattle show on the final leg of the "Deliverance" and "Damnation" tour. Per Wiberg also joined the band on tour to perform keyboards, after more than a year on tour.
"Ghost Reveries" (2005-2007).
Opeth returned home in 2004 to start writing new material for its eighth album, and by the end of the year, they had finished writing it. Opeth's European label, Music for Nations, closed its doors in 2005, and after negotiations with various labels, the band signed with Roadrunner Records. Åkerfeldt said the primary reason for signing with Roadrunner was the label's wide distribution, ensuring the album would be available at larger-chain retailers. When news leaked that the band was signed to Roadrunner, who predominantly worked with trend-oriented rock and metal, some fans accused the band of selling out. "To be honest," Åkerfeldt said, "that's such an insult after 15 years as a band and 8 records. I can't believe we haven't earned each and every Opeth fan's credibility after all these years. I mean, our songs are 10 minutes long!" The band rehearsed for three weeks before entering the studio, the first time the band rehearsed since the 1998 album, "My Arms, Your Hearse". During rehearsal, keyboardist Wiberg joined Opeth as a full-time member. Opeth recorded at Fascination Street Studios in Örebro, Sweden, from March 18 to June 1, 2005, and released the resulting "Ghost Reveries" on August 30, 2005, to critical acclaim and commercial success. The album debuted at number 64 in the US, and number nine in Sweden, higher than any previous Opeth release. Keith Bergman of Blabbermouth.net gave the album ten out of ten, one of only 21 albums to achieve a perfect rating from the site. Rod Smith of "Decibel" magazine called "Ghost Reveries" "achingly beautiful, sometimes unabashedly brutal, often a combination of both."
On May 12, 2006, Martin Lopez announced that he had officially parted ways with Opeth due to health problems, and was replaced by Martin Axenrot. Opeth toured on the main stage of Gigantour in 2006, alongside Megadeth. "Ghost Reveries" was re-released on October 31, 2006, with a bonus cover song (Deep Purple's "Soldier of Fortune"), a DVD featuring a 5.1 surround sound mix of the album and a documentary on the making of the record. A recording of Opeth's live performance at the Camden Roundhouse, in London, on November 9, 2006, was released as the double live album "The Roundhouse Tapes", which topped the Finnish DVD chart.
On May 17, 2007, Peter Lindgren announced he would be leaving Opeth after 16 years. "The decision has been the toughest I've ever made but it is the right one to make at this point in my life," Lindgren said. "I feel that I simply have lost some of the enthusiasm and inspiration needed to participate in a band that has grown from a few guys playing the music we love to a worldwide industry." Ex-Arch Enemy guitarist Fredrik Åkesson replaced Lindgren, as Åkerfeldt explained "Fredrik was the only name that popped up thinking about a replacement for Peter. In my opinion he's one of the top three guitar players out of Sweden. We all get along great as we've known each other for maybe four years and he already has the experience to take on the circus-like lifestyle we lead as members of Opeth."
"Watershed" and "In Live Concert at the Royal Albert Hall" (2008–2010).
Opeth entered Fascination Street Studios in November 2007 to record their ninth studio album, with Åkerfeldt producing. By January 2008, Opeth had recorded 13 songs, including three cover songs. The finished album, "Watershed", features seven tracks, with cover songs used as bonus tracks on different versions of the album. "Watershed" was released on June 3, 2008. Åkerfeldt described the songs on the album as "a bit more energetic". Opeth toured in support of "Watershed", including headlining the UK Defenders of the Faith tour with Arch Enemy, an appearance at Wacken Open Air, and the Progressive Nation tour with headliner Dream Theater. "Watershed" was Opeth's highest-charting album to date, debuting at number 23 on the US "Billboard" 200, on the Australian ARIA album charts at number seven and at number one on Finland's official album chart. Opeth went on a worldwide tour in support of "Watershed". From September to October, the band toured North America backed by High on Fire, Baroness, and Nachtmystium. They returned to tour Europe for the rest of the year with Cynic and The Ocean.
In 2010, Opeth wrote and recorded the new track, "The Throat of Winter", which appeared on the digital EP soundtrack of the video game, "God of War III". Åkerfeldt described the song as "odd" and "not very metal." To celebrate their 20th anniversary, Opeth performed a six-show, worldwide tour called "Evolution XX: An Opeth Anthology", from March 30 through April 9, 2010. "Blackwater Park" was performed in its entirety, along with several songs never before performed. The concert of April 5, 2010, at the Royal Albert Hall in London, England was filmed for a DVD and live album package titled "In Live Concert at the Royal Albert Hall". The set was released on September 21, 2010, in 2-DVD and 2-DVD/3-CD configurations. For the DVD the concert was split into two sets. The first set consists of the entire "Blackwater Park" album, while the second set contains one song from every album excluding "Blackwater Park", in chronological order representing the twenty years of "evolution" in their music. Åkerfeldt stated, "I can't believe it, but, fuck, we're celebrating 20 years. I've been in this band ever since I was 16. It's insane." A special edition of "Blackwater Park" was released in March 2010 to coincide with the tour.
"Heritage" (2011–2013).
In September 2010, Mikael Åkerfeldt stated that he was writing for a new Opeth album. The band announced on their website that they would start recording their tenth album on January 31, 2011, at the Atlantis/Metronome studios in Stockholm, once again with Jens Bogren (engineering) and Steven Wilson from Porcupine Tree as co-producer.
Shortly after mixing was complete on the new album in April 2011, Opeth announced that Per Wiberg was relieved of his duties in the band.
In the press statement, Mikael Åkerfeldt explained the decision, saying, "Mendez, Axe and Fredrik and I came to the decision that we should find a replacement for Per right after the recordings of the new album, and this came as no surprise to Per. He had, in turn, been thinking about leaving, so you could say it was a mutual decision. There's no bad blood, just a relationship that came to an end, and that's that."
Opeth's tenth album, "Heritage", was released on September 14, 2011, to generally favorable reviews. The album sold 19,000 copies in the United States in its first week of release and debuted at number 19 on the "Billboard 200" chart. "Heritage" debuted at number four in the band's native country of Sweden.
"Heritage" became the second Opeth album to not feature any death growls and had a much more progressive style than previous albums from the band, something that Åkerfeldt had been wanting to do for some time.
The first two songs Åkerfeldt wrote for "Heritage" were in the style of "Watershed". After hearing the songs for the first time, Martín Méndez told Åkerfeldt that he would be disappointed if the album continued in that direction. Relieved that Méndez was not interested in doing another conventional Opeth album, Åkerfeldt scrapped the two songs and started the writing process over in a different style. In the press release for "Heritage", Mikael Åkerfeldt revealed that he felt as though he had been building to write the album since he was 19 years old. In a review for Allmusic, Thom Jurek called "Heritage" the band's most adventurous album, describing the songs as "drenched in instrumental interludes, knotty key and chord changes, shifting time signatures, clean vocals, and a keyboard-heavy instrumentation that includes Mellotrons, Rhodes pianos, and Hammond organs".
Opeth supported "Heritage" with a tour that would last for over 200 tour dates. The tour was the band's first with new keyboardist, Joakim Svalberg, who played on the opening track of the album. During the tour, Opeth played with bands such as Katatonia, Pain of Salvation, Mastodon, Ghost and Anathema all over the world in countries such as the United States, Europe, Turkey, India, Japan, Greece, Israel, Latin America and Sweden. The tour concluded with "Melloboat 2013".
"Pale Communion" (2013–present).
On August 26, 2014, Opeth released its eleventh studio album, titled "Pale Communion". Åkerfeldt began working on new material as far back as August 2012. In January 2014 he stated, "We've been looking at the next album at Rockfield Studios in Wales where Queen recorded "Bohemian Rhapsody", but we haven't made a decision yet, but it will be an expensive album. There's a lot going on, lots of string arrangements that we haven't had in the past." Despite fearing that the band's new musical direction would split Opeth's fanbase, when asked if it will it be heavier or softer than "Heritage", Åkerfeldt said, "Maybe a little bit heavier, not death metal heavy, but hard rock/heavy metal heavy. There's also lots of progressive elements and acoustic guitars, but also more sinister-sounding riffs." Åkerfeldt also produced the new album which will include string instrumentation, something that he became interested in doing when working on "Storm Corrosion". The band members in Opeth felt rejuvenated after creating "Heritage" which resulted in closer relationships between them. 
"The Guardian" reviewed "Pale Communion" positively, calling it "strange, intricate prog-metal genius" somewhat flawed by Åkerfeldt's indulgent vocal stylings. The album saw Opeth's highest chart positions in the history of the band with "Pale Communion" debuting at number 19 in the US, number 3 in Sweden, and number 14 in the United Kingdom. It sold 13,000 copies in its first week of release in the US.
"Pale Communion" was supported with more touring from Opeth. In 2015, Opeth will play several concerts to celebrate the 25th anniversary of the band. At these special shows, the band will be playing two sets. The first set is 2005's "Ghost Reveries" as a ten-year anniversary celebration of the album. The second set will span the rest of Opeth's career to celebrate its 25th anniversary. Åkerfeldt expressed excitement for the concerts
Musical style and influence.
As Opeth's primary songwriter and lyricist, vocalist/guitarist Mikael Åkerfeldt heads the direction of Opeth's sound. He was influenced at a young age by the 1970s progressive rock bands Camel, P.F.M. and Gracious, and by heavy metal bands such as Iron Maiden, Slayer, Death, Black Sabbath, Deep Purple, Celtic Frost, King Diamond, Morbid Angel, Voivod, and most importantly Judas Priest. Åkerfeldt considers "Sad Wings of Destiny" the best metal album of all time, and notes that there was a time when he only listened to Judas Priest. Åkerfeldt sings "Here Come the Tears" by Judas Priest before most Opeth concerts while warming up. Åkerfeldt later discovered progressive rock and folk music, both of which had a profound impact on the sound of the band.
Opeth's distinct sound mixes death metal with progressive rock. Steve Huey of Allmusic refers to Opeth's "epic, progressive death metal style." Ryan Ogle of Blabbermouth described Opeth's sound as incorporating "the likes of folk, funk, blues, '70s rock, goth and a laundry list of other sonic oddities into their trademark progressive death style." In his review of Opeth's 2001 album "Blackwater Park", Allmusic's Eduardo Rivadavia wrote, "Tracks start and finish in seemingly arbitrary fashion, usually traversing ample musical terrain, including acoustic guitar and solo piano passages, ambient soundscapes, stoner rock grooves, and Eastern-tinged melodies—any of which are subject to savage punctuations of death metal fury at any given moment." Åkerfeldt commented on the diversity of Opeth's music:
I don't see the point of playing in a band and going just one way when you can do everything. It would be impossible for us to play just death metal; that is our roots, but we are now a mishmash of everything, and not purists to any form of music. It's impossible for us to do that, and quite frankly I would think of it as boring to be in a band that plays just metal music. We're not afraid to experiment, or to be caught with our pants down, so to speak. That's what keeps us going.
More recently, Opeth have abandoned their death metal sound resulting in a mellower progressive rock sound. When talking about "Heritage", guitarist Fredrik Åkesson stated:
In the beginning it took me a little while to get used to the new idea of the sound, not having any screaming vocals and stuff like that. But I think the album was necessary for us to do. Maybe the band wouldn't have continued if we hadn't done "Heritage". I think the old Opeth fans understand this album. There's always going to be some haters, but you can't be loved by everyone. Opeth has always been about not repeating ourself. A lot of people don't think "Heritage" is metal but I think it's metal to go somewhere people don't expect. It doesn't mean we're not embracing the past sound of Opeth.
Vocally, Åkerfeldt shifts between traditional death metal vocals for heavy sections, and clean, sometimes whispered or soft-spoken vocals over mellower passages. While his death growls were dominant on early releases, later efforts incorporate more clean vocals, with "Damnation", "Heritage" and "Pale Communion" featuring only clean singing. Rivadavia noted that "Åkerfeldt's vocals run the gamut from bowel-churning grunts to melodies of chilling beauty—depending on each movement section's mood."

</doc>
<doc id="45852" url="https://en.wikipedia.org/wiki?curid=45852" title="Charles Ponzi">
Charles Ponzi

Charles Ponzi (March 3, 1882 – January 18, 1949) was an Italian businessman and con artist in the U.S. and Canada. His aliases include "Charles Ponci", "Carlo" and "Charles P. Bianchi". Born and raised in Italy, he became known in the early 1920s as a swindler in North America for his money-making scheme. He promised clients a 50% profit within 45 days, or 100% profit within 90 days, by buying discounted postal reply coupons in other countries and redeeming them at face value in the United States as a form of arbitrage. In reality, Ponzi was paying early investors using the investments of later investors, a practice known as "robbing Peter to pay Paul." While this swindle predated Ponzi by several years, it became so identified with him that it now bears his name. His scheme ran for over a year before it collapsed, costing his "investors" $20 million.
Ponzi was probably inspired by the scheme of William F. Miller, a Brooklyn bookkeeper who in 1899 used the same scheme to take in $1 million. In addition, "The Man Who Broke the Bank at Monte Carlo", Charles Deville Wells, had operated a very similar scheme in France in 1910-11, when — under the alias ″Lucien Rivier″ — he had set up a phony bank, to the detriment of his 6,000 victims. 
Early life.
Charles Ponzi was born Carlo Pietro Giovanni Guglielmo Tebaldo Ponzi in Lugo, Italy, in 1882. He told "The New York Times" that he had come from a well-to-do family in Parma, Italy. He took a job as a postal worker early on, but soon was accepted into the University of Rome La Sapienza. His friends considered the university a "four-year vacation," and he was inclined to follow them around to bars, cafés, and the opera.
Arrival in the United States.
On November 15, 1903, he arrived in Boston aboard the S.S. "Vancouver". By his own account, Ponzi had $2.51 in his pocket, having gambled away the rest of his life savings during the voyage. "I landed in this country with $2.50 in cash and $1 million in hopes, and those hopes never left me," he later told "The New York Times". He quickly learned English and spent the next few years doing odd jobs along the East Coast, eventually taking a job as a dishwasher in a restaurant, where he slept on the floor. He managed to work his way up to the position of waiter, but was fired for shortchanging the customers and theft.
In 1907, Ponzi moved to Montreal and became an assistant teller in the newly opened "Banco Zarossi", a bank started by Luigi "Louis" Zarossi to service the influx of Italian immigrants arriving in the city. Zarossi paid 6% interest on bank deposits – double the going rate at the time – and was growing rapidly as a result. Ponzi eventually rose to bank manager. However, he found out that the bank was in serious financial trouble because of bad real estate loans, and that Zarossi was funding the interest payments not through profit on investments, but by using money deposited in newly opened accounts. The bank eventually failed and Zarossi fled to Mexico with a large portion of the bank's money.
Ponzi stayed in Montreal and, for some time, lived at Zarossi's house helping the man's abandoned family, while planning to return to the United States and start over. As Ponzi was penniless, this proved to be very difficult. Eventually he walked into the offices of a former Zarossi customer "Canadian Warehousing" and, finding no one there, wrote himself a check for $423.58 in a checkbook he found, forging the signature of a director of the company, Damien Fournier. 
Confronted by police who had taken note of his large expenditures just after the forged check was cashed, Ponzi held out his hands wrist up and said "I'm guilty." He ended up spending three years at "St. Vincent-de-Paul Federal Penitentiary", a bleak facility located on the outskirts of Montreal. Rather than inform his mother of this development, he posted her a letter stating that he had found a job as a "special assistant" to a prison warden.
After his release in 1911 he decided to return to the United States, but got involved in a scheme to smuggle Italian illegal immigrants across the border. He was caught and spent two years in Atlanta Prison. Here he became a translator for the warden, who was intercepting letters from mobster Ignazio "the Wolf" Lupo. Ponzi ended up befriending Lupo. It was another prisoner who became a true role model to Ponzi: Charles W. Morse. Morse, a wealthy Wall Street businessman and speculator, fooled doctors during medical exams, poisoning himself by eating soap shavings, toxins that left his body as quickly as the doctors left his bedside. Morse was soon released from prison. Ponzi completed his prison term following Morse's release, having an additional month added to his term due to his inability to pay a $500 fine.
Origin of the term "Ponzi scheme".
After Ponzi's release from prison, he made his way back to Boston. There he met Rose Maria Gnecco, a stenographer, to whom he proposed marriage. Though Ponzi did not tell Gnecco about his years in jail, his mother sent Gnecco a letter telling her of Ponzi's past. Nonetheless, she married him in 1918. For the next few months, he worked at a number of businesses, including his father-in-law's grocery, before hitting upon an idea to sell advertising in a large business listing to be sent to various businesses. Ponzi was unable to sell this idea to businesses, and his company failed soon after.
A few weeks later, Ponzi received a letter from a company in Spain asking about the catalog. Inside the envelope was an international reply coupon (IRC), something which he had never seen before. He asked about the IRC and found a weakness in the system which would, in theory, allow him to make money.
The purpose of the postal reply coupon was to allow someone in one country to send it to a correspondent in another country, who could use it to pay the postage of a reply. IRCs were priced at the cost of postage in the country of purchase, but could be exchanged for stamps to cover the cost of postage in the country where redeemed; if these values were different, there was a potential profit. Inflation after World War I had greatly decreased the cost of postage in Italy expressed in U.S. dollars, so that an IRC could be bought cheaply in Italy and exchanged for U.S. stamps of higher value, which could then be sold. Ponzi claimed that the net profit on these transactions, after expenses and exchange rates, was in excess of 400%. This was a form of arbitrage, or profiting by buying an asset at a lower price in one market and immediately selling it in a market where the price is higher, which is not illegal.
Seeing an opportunity, Ponzi quit his translator's job to set his scheme in motion. He borrowed money and sent it back to relatives in Italy with instructions to buy postal coupons and send them to him. However, when he tried to redeem them, he ran into an avalanche of red tape.
Undaunted, Ponzi went to several of his friends in Boston and promised that he would double their investment in 90 days. The great returns available from postal reply coupons, he explained to them, made such incredible profits easy. Some people invested and were paid off as promised, receiving $750 interest on initial investments of $1,250.
Soon afterward, Ponzi started his own company, the "Securities Exchange Company," to promote the scheme. He set up shop in the Niles Building on School Street. Word spread, and investments came in at an ever-increasing rate. Ponzi hired agents and paid them generous commissions for every dollar they brought in. By February 1920, Ponzi's total take was US$5,000, (approximately US$54,000 in 2010 dollars). By March, he had made $30,000 ($324,000 in 2010 terms). A frenzy was building, and Ponzi began to hire agents to take in money from all over New England and New Jersey. At that time, investors were being paid impressive rates, encouraging others to invest. By May 1920, he had made $420,000 ($4.53 million in 2010 terms).
He began depositing the money in the Hanover Trust Bank of Boston (a small bank on Hanover Street in the mostly Italian North End), in the hope that once his account was large enough he could impose his will on the bank or even be made its president; he bought a controlling interest in the bank through himself and several friends after depositing $3 million. By July 1920, he had made millions. People were mortgaging their homes and investing their life savings. Most did not take their profits, but reinvested.
Ponzi was bringing in cash at a fantastic rate, but the simplest financial analysis would have shown that the operation was running at a large loss. As long as money kept flowing in, existing investors could be paid with the new money. This was the only way Ponzi had to pay off those investors, as he made no effort to generate legitimate profits.
Ponzi lived luxuriously: he bought a mansion in Lexington, Massachusetts, and he maintained accounts in several banks across New England besides Hanover Trust. He also brought his mother from Italy in a first-class stateroom on an ocean liner. She died soon afterward. On July 31, 1920, Ponzi told Father Pasquale Di Milla the director of the Italian Children's Home in Jamaica Plain that he would donate $100,000 in honor of his mother.
Suspicion.
Ponzi's rapid rise naturally drew suspicion. When a Boston financial writer suggested there was no way Ponzi could legally deliver such high returns in a short period of time, Ponzi sued for libel and won $500,000 in damages. As libel law at the time placed the burden of proof on the writer and the paper, this effectively neutralized any serious probes into his dealings for some time.
Nonetheless, there were still signs of his eventual ruin. Joseph Daniels, a Boston furniture dealer who had given Ponzi furniture which he could not afford to pay for, sued Ponzi to cash in on the gold rush. The lawsuit was unsuccessful, but it did start people asking how Ponzi could have gone from being penniless to being a millionaire in so short a time. There was a run on the Securities Exchange Company, as some investors decided to pull out. Ponzi paid them and the run stopped. 
On July 24, 1920, the "Boston Post" printed a favorable article on Ponzi and his scheme that brought in investors faster than ever. At that time, Ponzi was making $250,000 a day. Ponzi's good fortune was increased by the fact that just below this favorable article, which seemed to imply that Ponzi was indeed returning 50% return on investment after only 45 days, was a bank advertisement that stated that the bank was paying 5% returns annually. The next business day after this article was published, Ponzi arrived at his office to find thousands of Bostonians waiting to give him their money.
Despite this reprieve, "Post" acting publisher Richard Grozier and city editor Eddie Dunn were suspicious and assigned investigative reporters to check Ponzi out. He was also under investigation by the Commonwealth of Massachusetts, and on the day the "Post" printed its article, Ponzi met with state officials. He managed to divert the officials from checking his books by offering to stop taking money during the investigation, a fortunate choice, as proper records were not being kept. Ponzi's offer temporarily calmed the suspicions of the state officials.
Collapse of the scheme.
By this time, Ponzi was seeking another deal to get him out of trouble, but time was running out. On July 26, the "Post" started a series of articles that asked hard questions about the operation of Ponzi's money machine. The "Post" contacted Clarence Barron, the financial journalist who headed Dow Jones & Company, to examine Ponzi's scheme. Barron observed that though Ponzi was offering fantastic returns on investments, Ponzi himself was not investing with his own company.
Barron then noted that to cover the investments made with the Securities Exchange Company, 160 million postal reply coupons would have to be in circulation. However, only about 27,000 actually were. The United States Post Office stated that postal reply coupons were not being bought in quantity at home or abroad. The gross profit margin in percent on buying and selling each IRC was colossal, but the overhead required to handle the purchase and redemption of these items, which were of extremely low cost and were sold individually, would have exceeded the gross profit. Barron noted that if Ponzi really was doing what he claimed to do, he would effectively be profiting at the expense of either the governments where he bought the coupons or the United States government. For this reason, Barron argued that even if Ponzi's operation was legitimate, it was immoral to take advantage of a government in this manner.
The stories caused a panic run on the Securities Exchange Company. Ponzi paid out $2 million in three days to a wild crowd outside his office. He canvassed the crowd, passed out coffee and donuts, and cheerfully told them they had nothing to worry about. Many changed their minds and left their money with him. However, this attracted the attention of Daniel Gallagher, the United States Attorney for the District of Massachusetts. Gallagher commissioned Edwin Pride to audit the Securities Exchange Company's books—an effort made difficult by the fact his bookkeeping system consisted merely of index cards with investors' names.
In the meantime, Ponzi had hired a publicity agent, William McMasters. However, McMasters quickly became suspicious of Ponzi's endless talk of postal reply coupons, as well as the ongoing investigation against him. He later described Ponzi as a "financial idiot" who did not seem to know how to add.
The denouement for Ponzi began in late July, when McMasters found several highly incriminating documents that indicated Ponzi was merely "robbing Peter to pay Paul." He went to his former employer with this information. Grozier offered him $5,000 for his story. On August 2, 1920, McMasters wrote an article for the "Post" declaring Ponzi hopelessly insolvent. The article claimed that while Ponzi claimed $7 million in liquid funds, he was actually at least $2 million in debt. With interest factored in, McMasters wrote, Ponzi was as much as $4.5 million in the red. The story touched off a massive run, and Ponzi paid off in one day. He then sped up plans to build a massive conglomerate that would engage in banking and import/export operations.
Trouble now came from an unexpected quarter—Massachusetts Bank Commissioner Joseph Allen. An initial investigation into Ponzi's banking practices found nothing illegal, but Allen was afraid that if major withdrawals exhausted Ponzi's reserves, it would bring Boston's banking system to its knees. Allen's suspicions were further aroused when he found out a large number of Ponzi-controlled accounts had received more than $250,000 in loans from Hanover Trust. This led Allen to speculate that Ponzi wasn't nearly as well-financed as he claimed, since he was getting large loans from the bank he effectively controlled. He ordered two bank examiners to keep an eye on Ponzi's accounts. 
On August 9, the examiners reported that enough investors had cashed their checks on Ponzi's main account there that it was almost certainly overdrawn. Allen then ordered Hanover Trust not to pay out any more checks from Ponzi's main account. He also orchestrated an involuntary bankruptcy filing by several small Ponzi investors. The move forced Massachusetts Attorney General J. Weston Allen to release a statement that there was little to support Ponzi's claims of large-scale dealings in postal coupons. State officials then invited Ponzi note holders to come to the Massachusetts State House to furnish their names and addresses for the purpose of the investigation. On the same day, Ponzi received a preview of Pride's audit, which revealed Ponzi was at least $7 million in debt.
On August 11, it all came crashing down for Ponzi. First, the "Post" came out with a front-page story about his activities in Montreal 13 years earlier—including his forgery conviction and his role at Zarossi's scandal-ridden bank. That afternoon, Bank Commissioner Allen seized Hanover Trust due to numerous irregularities. The commissioner thus inadvertently foiled Ponzi's plan to "borrow" funds from the bank vaults as a last resort in the event all other efforts to obtain funds failed.
By the morning of August 12, Ponzi knew he was at the end of his tether. He'd held a certificate of deposit at Hanover Trust that was worth $1.5 million, but that total had been reduced to $1 million after bank officials tapped into it to cover the overdraft. Even if he'd been able to convert it into cash, he would have had only $4 million in assets. Amid reports that he was about to be arrested any day, Ponzi surrendered to federal authorities that morning and accepted Pride's figures. He was charged with mail fraud for sending letters to his marks telling them their notes had matured. He was originally released on $25,000 bail and was immediately re-arrested on state charges of larceny, for which he posted an additional $10,000 bond. After the "Post" released the results of the audit, the bail bondsman feared Ponzi might flee the country and withdrew the bail for the federal charges. Attorney General Allen declared that if Ponzi managed to regain his freedom, the state would seek additional charges and seek a bail high enough to ensure Ponzi would stay in custody.
Magnitude of losses.
The news brought down five other banks in addition to Hanover Trust. His investors were practically wiped out, receiving less than 30 cents to the dollar. His investors lost about $20 million in 1920 dollars ($225 million in 2011 dollars): Charles Ponzi completely annihilated their finances. As a comparison, Bernard Madoff's similar scheme that collapsed in 2008 cost his investors about $18 billion, 53 times the losses of Ponzi's scheme without taking into account conversion from 1920 dollars to 2008 dollars.
Prison and later life.
In two federal indictments, Ponzi was charged with 86 counts of mail fraud, and faced a lifetime in jail. At the urging of his wife, Ponzi pleaded guilty on November 1, 1920, to a single count before Judge Clarence Hale, who declared before sentencing, "Here was a man with all the duties of seeking large money. He concocted a scheme which, on his counsel's admission, did defraud men and women. It will not do to have the world understand that such a scheme as that can be carried out ... without receiving substantial punishment." He was sentenced to five years in federal prison.
He was released after three and a half years and was almost immediately indicted on 22 Massachusetts state charges of larceny, which came as a surprise to Ponzi; he thought he had a deal calling for the state to drop any charges against him if he pleaded guilty to the federal charges. He sued, claiming that he would be facing double jeopardy if Massachusetts essentially retried him for the same offenses spelled out in the federal indictment. The case, "Ponzi v. Fessenden", made it all the way to the Supreme Court of the United States. On March 27, 1922, the Supreme Court ruled that federal plea bargains have no standing regarding state charges. It also ruled that Ponzi was not facing double jeopardy because Massachusetts was charging him with larceny while the federal government charged him with mail fraud, even though the charges implicated the same criminal operation.
In October 1922, he was tried on the first ten larceny counts. Since he was insolvent, Ponzi served as his own attorney and, being as persuasive as he had been with his duped investors, the jury found him not guilty on all charges. He was tried a second time on five of the remaining charges, and the jury deadlocked. Ponzi was found guilty at a third trial, and was sentenced to an additional seven to nine years in prison as "a common and notorious thief."
After word got out that Ponzi had never obtained American citizenship (despite having lived in the United States for most of the time since 1903), federal officials initiated efforts to have him deported as an undesirable alien in 1922.
Ponzi was released on bail as he appealed the state conviction, and fled to the Springfield neighborhood of Jacksonville, Florida, and launched the Charpon Land Syndicate ("Charpon" is an amalgam of his name), offering investors in September 1925 tiny tracts of land, some under water, and promising 200% returns in 60 days. In reality, it was a scam that sold swampland in Columbia County. Ponzi was indicted by a Duval County grand jury in February 1926 and charged with violating Florida trust and securities laws. A jury found him guilty on the securities charges, and the judge sentenced him to a year in the Florida State Prison. Ponzi appealed his conviction and was freed after posting a $1,500 bond.
Ponzi traveled to Tampa, where he shaved his head, grew a mustache, and tried to flee the country as a crewman on a merchant ship bound for Italy. The ship, however, made one last American port call; he was caught in New Orleans and sent back to Massachusetts to serve out his prison term. Ponzi served seven more years in prison.
In the meantime, government investigators tried to trace Ponzi's convoluted accounts to figure out how much money he had taken and where it had gone. They never managed to untangle it and could conclude only that millions had gone through his hands.
Ponzi was released in 1934. With the release came an immediate order to have him deported to Italy. He asked for a full pardon from Governor Joseph B. Ely. However, on July 13, Ely turned the appeal down. His charismatic confidence had faded, and when he left the prison gates, he was met by an angry crowd. He told reporters before he left, "I went looking for trouble, and I found it." On October 7, Ponzi was officially deported.
Rose stayed behind, and divorced him in 1937. She had not wanted to leave Boston, and Ponzi was in no position to support her in any event.
In Italy, Ponzi jumped from scheme to scheme, but little came of them. He eventually got a job in Brazil as an agent for Ala Littoria, the Italian state airline. During World War II, however, the airline's operation in the country was shut down after the British intelligence services intervened and Brazil sided with the Allies. During that time, Ponzi also wrote his autobiography. He was sentenced to 17 total years in prison for fraudulent scam.
Death.
Ponzi spent the last years of his life in poverty, working occasionally as a translator. His health suffered. A heart attack in 1941 left him considerably weakened. His eyesight began failing, and by 1948, he was almost completely blind. A brain hemorrhage paralyzed his right leg and arm. He died in a charity hospital in Rio de Janeiro, the Hospital São Francisco de Assis of Federal University of Rio de Janeiro on January 15, 1949.
Supported by his last and only friend who spoke English and had notions of Italian, the barber Francisco Nonato Nunes, Ponzi granted one last interview to an American reporter, telling him, "Even if they never got anything for it, it was cheap at that price. Without malice aforethought I had given them the best show that was ever staged in their territory since the landing of the Pilgrims! It was easily worth fifteen million
bucks to watch me put the thing over."

</doc>
<doc id="45854" url="https://en.wikipedia.org/wiki?curid=45854" title="Uffizi">
Uffizi

The Uffizi Gallery (, ) is a prominent art museum located adjacent to the Piazza della Signoria in central Florence, region of Tuscany, Italy.
History.
The building of Uffizi complex was begun by Giorgio Vasari in 1560 for Cosimo I de' Medici so as to accommodate the offices of the Florentine magistrates, hence the name , "offices". The construction was later continued by Alfonso Parigi and Bernardo Buontalenti and completed in 1581. The "cortile" (internal courtyard) is so long and narrow, and open to the Arno at its far end through a Doric screen that articulates the space without blocking it, that architectural historians treat it as the first regularized streetscape of Europe. Vasari, a painter and architect as well, emphasised its perspective length by the matching facades' continuous roof cornices, and unbroken cornices between storeys and the three continuous steps on which the palace-fronts stand. The niches in the piers that alternate with columns filled with sculptures of famous artists in the XIX century.
The Uffizi brought together under one roof the administrative offices, the Tribunal and the Archivio di Stato, the state archive. The project commissioned by Cosimo I de' Medici, Grand Duke of Tuscany planned to display prime art works of the Medici collections on the piano nobile; the plan was carried out by his son, Grand Duke Francesco I. He commissioned from the architect Buontalenti the design of the Tribuna degli Uffizi that collected a series of masterpieces in one room, and was a highly influential attraction of a Grand Tour.
Over the years, more sections of the palace were recruited to exhibit paintings and sculpture collected or commissioned by the Medici. According to Vasari, who was not only the architect of the Uffizi but also the author of "Lives of the Artists", published in 1550 and 1568, artists such as Leonardo da Vinci and Michelangelo gathered at the Uffizi "for beauty, for work and for recreation."
After the house of Medici was extinguished, the art treasures remained in Florence by terms of the famous "Patto di famiglia" negotiated by Anna Maria Luisa, the last Medici heiress; it formed one of the first modern museums. The gallery had been open to visitors by request since the sixteenth century, and in 1765 it was officially opened to the public.
Because of its huge collection, some of its works have in the past been transferred to other museums in Florence—for example, some famous statues to the Bargello. A project is currently underway to expand the museum's exhibition space in 2006 from some 6,000 metres² (64,000 ft²) to almost 13,000 metres² (139,000 ft²), allowing public viewing of many artworks that have usually been in storage.
On 27 May 1993, a car bomb exploded in Via dei Georgofili and damaged parts of the palace, killing five people. The most severe damage was to the Niobe room and classical sculptures and neoclassical interior of which have been restored, although its frescoes were damaged beyond repair. The identity of the bomber or bombers are unknown, although it was almost certainly attributable to the Sicilian Mafia who were engaged in a period of terrorism at that time.
Today, the Uffizi is one of the most popular tourist attractions of Florence. In high season (particularly in July), waiting times can be up to five hours.
In early August 2007, Florence was caught with a large rainstorm, and the Gallery was partially flooded, with water leaking through the ceiling, and the visitors had to be evacuated. There was a much more significant flood in 1966 which damaged most of the art collections in Florence severely, including the Uffizi.
Here is a selection from the painting collection:
The collection also contains some ancient sculptures, such as the "Arrotino" and the "Two Wrestlers".

</doc>
<doc id="45856" url="https://en.wikipedia.org/wiki?curid=45856" title="Monastery">
Monastery

A monastery is a building or complex of buildings comprising the domestic quarters and workplaces of monastics, whether monks or nuns, and whether living in communities or alone (hermits). A monastery generally includes a place reserved for prayer which may be a chapel, church or temple, and may also serve as an oratory.
Monasteries vary greatly in size, comprising a small dwelling accommodating only a hermit, or in the case of communities anything from a single building housing only one senior and two or three junior monks or nuns, to vast complexes and estates housing tens or hundreds. A monastery complex typically comprises a number of buildings which include a church, dormitory, cloister, refectory, library, and infirmary. Depending on the location, the monastic order and the occupation of its inhabitants, the complex may also include a wide range of buildings that facilitate self-sufficiency and service to the community. These may include a hospice, a school and a range of agricultural and manufacturing buildings such as a barn, a forge or a brewery.
In English usage, the term "monastery" is generally used to denote the buildings of a community of monks. In modern usage, "convent" tends to be applied only to institutions of female monastics (nuns), particularly communities of teaching or nursing religious sisters. Historically, a convent denoted a house of friars (reflecting the Latin), now more commonly called a "friary". Various religions may apply these terms in more specific ways.
Etymology.
The word "monastery" comes from the Greek word "μοναστήριον", neut. of "μοναστήριος" – "monasterios" from "μονάζειν" – "monazein" "to live alone" from the root "μόνος" – "monos" "alone" (originally all Christian monks were hermits); the suffix "-terion" denotes a "place for doing something". The earliest extant use of the term "monastērion" is by the 1st century AD Jewish philosopher Philo in "On The Contemplative Life," ch. III.
In England the word "monastery" was also applied to the habitation of a bishop and the cathedral clergy who lived apart from the lay community. Most cathedrals were not monasteries, and were served by canons secular, which were communal but not monastic. However some were run by monasteries orders, such as York Minster. Westminster Abbey was for a short time a cathedral, and was a Benedictine monastery until the Reformation, and its Chapter preserves elements of the Benedictine tradition. See the entry cathedral. They are also to be distinguished from collegiate churches, such as St George's Chapel, Windsor.
Terms.
In most of this article, the term "monastery" is used generically to refer to any of a number of types of religious community. In the Roman Catholic religion and to some extent in certain other branches of Buddhism, there is a somewhat more specific definition of the term and many related terms.
Buddhist monasteries are generally called vihara (Pali language). Viharas may be occupied by males or females, and in keeping with common English usage, a vihara populated by females may often be called a nunnery or a convent. However, vihara can also refer to a temple. In Tibetan Buddhism, monasteries are often called gompa. In Thailand, Laos and Cambodia, a monastery is called a "wat". In Burma, a monastery is called a "kyaung".
A monastery may be an abbey (i.e., under the rule of an abbot), or a priory (under the rule of a prior), or conceivably a hermitage (the dwelling of a hermit). It may be a community of men (monks) or of women (nuns). A charterhouse is any monastery belonging to the Carthusian order. In Eastern Christianity a very small monastic community can be called a skete, and a very large or important monastery can be given the dignity of a lavra.
The great communal life of a Christian monastery is called cenobitic, as opposed to the anchoretic (or anchoritic) life of an anchorite and the eremitic life of a hermit. There has also been, mostly under the Osmanli occupation of Greece and Cyprus, an "idiorrhythmic" lifestyle where monks come together but being able to own things individually and not being obliged to work for the common good.
In Hinduism monasteries are called matha, mandir, koil, or most commonly an ashram.
Jains use the Buddhist term vihara.
Monastic life.
In most religions the life inside monasteries is governed by community rules that stipulate the gender of the inhabitants and require them to remain celibate and own little or no personal property. The degree to which life inside a particular monastery is socially separate from the surrounding populace can also vary widely; some religious traditions mandate isolation for purposes of contemplation removed from the everyday world, in which case members of the monastic community may spend most of their time isolated even from each other. Others focus on interacting with the local communities to provide services, such as teaching, medical care, or evangelism. Some monastic communities are only occupied seasonally, depending both on the traditions involved and the local weather, and people may be part of a monastic community for periods ranging from a few days at a time to almost an entire lifetime.
The life within the walls of a monastery may be supported in several ways: by manufacturing and selling goods, often agricultural products, by donations or alms, by rental or investment incomes, and by funds from other organizations within the religion, which in the past formed the traditional support of monasteries. There has been a long tradition of Christian monasteries providing hospitable, charitable and hospital services. Monasteries have often been associated with the provision of education and the encouragement of scholarship and research, which has led to the establishment of schools and colleges and the association with universities. Christian monastic life has adapted to modern society by offering computer services, accounting services and management as well as modern hospital and educational administration.
Buddhism.
Buddhist monasteries, known as vihara, emerged sometime around the 4th century BC, from the practice of vassa, the retreat undertaken by Buddhist monks and nuns during the South Asian rainy season. To prevent wandering monks from disturbing new plant growth or becoming stranded in inclement weather, Buddhist monks and nuns were instructed to remain in a fixed location for the roughly three-month period typically beginning in mid-July. Outside of the "vassa" period, monks and nuns both lived a migratory existence, wandering from town to town begging for food. These early fixed "vassa" retreats were held in pavilions and parks that had been donated to the "sangha" by wealthy supporters. Over the years, the custom of staying on property held in common by the "sangha" as a whole during the "vassa" retreat evolved into a more cenobitic lifestyle, in which monks and nuns resided year round in monasteries.
In India, Buddhist monasteries gradually developed into centres of learning where philosophical principles were developed and debated; this tradition is currently preserved by monastic universities of Vajrayana Buddhists, as well as religious schools and universities founded by religious orders across the Buddhist world. In modern times, living a settled life in a monastery setting has become the most common lifestyle for Buddhist monks and nuns across the globe.
Whereas early monasteries are considered to have been held in common by the entire "sangha", in later years this tradition diverged in a number of countries. Despite "vinaya" prohibitions on possessing wealth, many monasteries became large land owners, much like monasteries in medieval Christian Europe. In China, peasant families worked monastic-owned land in exchange for paying a portion of their yearly crop to the resident monks in the monastery, just as they would to a feudal landlord. In Sri Lanka and Tibet, the ownership of a monastery often became vested in a single monk, who would often keep the property within the family by passing it on to a nephew who ordained as a monk. In Japan, where civil authorities permitted Buddhist monks to marry, being the head of a temple or monastery sometimes became a hereditary position, passed from father to son over many generations.
Forest monasteries – most commonly found in the Theravada traditions of Southeast Asia and Sri Lanka – are monasteries dedicated primarily to the study of Buddhist meditation, rather than scholarship or ceremonial duties. Forest monasteries often function like early Christian monasteries, with small groups of monks living an essentially hermit-like life gathered loosely around a respected elder teacher. While the wandering lifestyle practised by the Buddha and his disciples continues to be the ideal model for forest tradition monks in Thailand and elsewhere, practical concerns- including shrinking wilderness areas, lack of access to lay supporters, dangerous wildlife, and dangerous border conflicts- dictate that more and more 'meditation' monks live in monasteries, rather than wandering.
Tibetan Buddhist monasteries are sometimes known as lamaseries and the monks are sometimes (mistakenly) known as lamas. H. P. Blavatsky's Theosophical Society named its initial New York City meeting place "the Lamasery."
Some famous Buddhist monasteries include:
A further list of Buddhist monasteries is available at the list of Buddhist temples
Trends in Buddhist monasticism.
Some of the largest monasteries in the world are Buddhist. Drepung Monastery in Tibet housed around 10,000 monks prior to the Chinese invasion. Today, its relocated monastery in India houses around 8,000.
Christianity.
According to tradition, Christian monasticism began in Egypt with St. Anthony. Originally, all Christian monks were hermits seldom encountering other people. But because of the extreme difficulty of the solitary life, many monks failed, either returning to their previous lives, or becoming spiritually deluded.
A transitional form of monasticism was later created by Saint Amun in which "solitary" monks lived close enough to one another to offer mutual support as well as gathering together on Sundays for common services.
It was St. Pachomios who developed the idea of having monks live together and worship together under the same roof (Coenobitic Monasticism). Some attribute his mode of communal living to the barracks of the Roman Army in which Pachomios served as a young man. Soon the Egyptian desert blossomed with monasteries, especially around Nitria (Wadi El Natrun), which was called the "Holy City". Estimates are that upwards of 50,000 monks lived in this area at any one time.
Hermitism never died out though, but was reserved only for those advanced monks who had worked out their problems within a cenobitic monastery.
The idea caught on, and other places followed:
Western Medieval Europe.
The life of prayer and communal living was one of rigorous schedules and self-sacrifice. Prayer was their work, and the Office prayers took up much of a monk's waking hours – Matins, Lauds, Prime, Terce, daily Mass, Sext, None, Vespers, and Compline. In between prayers, monks were allowed to sit in the cloister and work on their projects of writing, copying, or decorating books. These would have been assigned based on a monk's abilities and interests. The non-scholastic types were assigned to physical labour of varying degrees.
The main meal of the day took place around noon, often taken at a refectory table, and consisted of the most simple and bland foods i.e., poached fish, boiled oats. While they ate, scripture would be read from a pulpit above them. Since no other words were allowed to be spoken, monks developed communicative gestures. Abbots and notable guests were honoured with a seat at the high table, while everyone else sat perpendicular to that in the order of seniority. This practice remained when some monasteries became universities after the first millennium, and can still be seen at Oxford University and Cambridge University.
Monasteries were important contributors to the surrounding community. They were centres of intellectual progression and education. They welcomed aspiring priests to come study and learn, allowing them even to challenge doctrine in dialogue with superiors. The earliest forms of musical notation are attributed to a monk named Notker of St Gall, and was spread to musicians throughout Europe by way of the interconnected monasteries. Since monasteries offered respite for weary pilgrim travellers, monks were obligated also to care for their injuries or emotional needs. Over time, lay people started to make pilgrimages "to" monasteries instead of just using them as a stop over. By this time, they had sizeable libraries that attracted learned tourists. Families would donate a son in return for blessings. During the plagues, monks helped to till the fields and provide food for the sick.
A Warming House is a common part of a medieval monastery, where monks went to warm themselves. It was often the only room in the monastery where a fire was lit.
Catholic religious orders.
A number of distinct monastic orders developed within Roman Catholicism:
While in English most mendicant Orders use the monastic terms of monastery or priory, in the Latin languages, the term used by the friars for their houses is convent, from the Latin "conventus", e.g., () or (), meaning "gathering place". The Franciscans rarely use the term "monastery" at present, preferring to call their house a "friary".
Orthodox Christianity.
In the Eastern Orthodox Church, both monks and nuns follow a similar ascetic discipline, and even their religious habit is the same (though nuns wear an extra veil, called the "apostolnik"). Unlike Roman Catholic monasticism, the Orthodox do not have separate religious orders, but a single monastic form throughout the Orthodox Church. Monastics, male or female, live away from the world, in order to pray for the world.
Monasteries vary from the very large to the very small. There are three types of monastic houses in the Orthodox Church:
One of the great centres of Orthodox monasticism is Mount Athos in Greece, which, like the Vatican State, is self-governing. It is located on an isolated peninsula approximately long and wide, and is administered by the heads of the 20 monasteries. Today the population of the Holy Mountain is around 2,200 men only and can only be visited by men with special permission granted by both the Greek government and the government of the Holy Mountain itself.
Oriental Orthodox churches.
The Oriental Orthodox churches, distinguished by their Miaphysite beliefs, consist of the Armenian Apostolic Church, the Coptic Orthodox Church of Alexandria (whose Patriarch, is considered first among equals for the following churches), and the Ethiopian Orthodox Church, the Eritrean Orthodox Church, the Indian Orthodox Church, and the Syriac Orthodox Church of Antioch. The now extinct Caucasian Albanian Church also fell under this group.
The monasteries of St. Macarius ("Deir Abu Makaria") and St. Anthony ("Deir Mar Antonios") are the oldest monasteries in the world and under the patronage of the Patriarch of the Coptic Orthodox Church.
Other Christian communities.
The last years of the 18th century marked in the Christian Church the beginnings of growth of monasticism among Protestant denominations. The center of this movement was in the United States and Canada beginning with the Shaker Church, which was founded in England and then moved to the United States. In the 19th century many of these monastic societies were founded as Utopian communities based on the monastic model in many cases. Aside from the Shakers, there were the Amanna, the Anabaptists, and others. Many did allow marriage but most had a policy of celibacy and communal life in which members shared all things communally and disavowed personal ownership.
In the 19th-century monasticism was revived in the Church of England, leading to the foundation of such institutions as the House of the Resurrection, Mirfield (Community of the Resurrection), Nashdom Abbey (Benedictine), Cleeve Priory (Community of the Glorious Ascension) and Ewell Monastery (Cistercian), Benedictine orders, Franciscan orders and the Orders of the Holy Cross, Order of St. Helena. Other Protestant Christian denominations also engage in monasticism, particularly Lutherans in Europe and North America. For example, the Benedictine order of the Holy Cross at St Augustine's House in Michigan is a Lutheran order of monks and there are Lutheran religious communities in Sweden and Germany. In the 1960s, experimental monastic groups were formed in which both men and women were members of the same house and also were permitted to be married and have children—these were operated on a communal form.
Trends in Christian monasticism.
The number of dedicated monastics in any religion has waxed and waned due to many factors. There have been Christian monasteries such as "The Cappadocian Caves" that used to shelter upwards of 5,000 monks, or St Pantelaimon's Monastery on the Mount Athos in Greece, which has held up to 3,000 monks. Today those numbers have dwindled and the entire population of the "Holy Mountain" may be 2,000.
Some Orthodox monastic leaders that are critical of monasteries that are too large, arguing that they become institutions and lose the intensity of spiritual training that can better be achieved when an elder has only 2 or 3 disciples. On the Mount Athos there are areas such as the Skete of St Anne, which could be considered as monastic entities but are small "Sketes" (monastic houses containing one elder and 2 or 3 disciples) who come together in one church for services.
There is a growing Christian neo-monasticism, particularly among evangelical Christians. Established upon at least some of the customary monastic principles, they have attracted many who seek to live in relationship with other, or who seek to live in an intentionally focused lifestyle, such as a focus upon simplicity or pacifism. Some include rites, noviciate periods in which a newly interested person can test out living and sharing of resources, while others are more pragmatic, providing a sense of family in addition to a place to live in.
Hinduism.
Advaita mathas.
From the times of the Vedas people following monastic ways of life have been in existence in the Indian sub-continent. In what is now called Hinduism, monks have existed for a long time, and with them, their respective monasteries, called mathas. Important among them are the chatur-amnaya mathas established by Adi Shankara which formed the nodal centres of under whose guidance the ancient Order of Advaitin monks were re-organised under ten names of the Dashanami Sampradaya. 
Sri Vaishnava mathas.
Ramanuja heralded a new era in the world Hinduism by reviving the lost faith in it and gave a firm doctrinal basis to the Vishishtadvaita philosophy which had existed since time immemorial. He ensured the establishment of a number of mathas of his Sri Vaishnava creed at different important centres of pilgrimage. 
Later on, other famous Sri Vaishnava theologians and religious heads established various important mathas such as
Nimbarka Vaishnava mathas.
Nimbarka Sampradaya of Nimbarkacharya is widely popular all over North, West and East India and has several important Mathas.
Madhva mathas.
Ashta matha (eight monasteries) of Udupi founded by Madhvacharya (Madhwa acharya) a dwaitha philosopher.
Sufism.
Islam prohibits monasticism, which is referred to in the Quran as "an invention". However, the term "Sufi" is applied to Muslim mystics who, as a means of achieving union with Allah, adopted ascetic practices including wearing a garment made of coarse wool called "sf". The term "Sufism" comes from "sf" meaning the person, who wears "sf". But in the course of time, Sufi has come to designate all Muslim believers in mystic union.
In the roots of Sufi philosophy there are influences of neoplatonist and other philosophies. Many of the practices of Orthodox Christian hermits and desert-dwellers were imitated in Sufism's growth in the center of the former-Christian lands of the Middle East. Ascetic practices within the Sufi philosophy were also associated with Buddhism. The notion of purification (cleaning one's soul from all evil things and trying to reach Nirvana and to become immortal in Nirvana) plays an important role in Buddhism. The same idea shows itself in the belief of "fanaa" (union with God) in Sufi philosophy.

</doc>
<doc id="45857" url="https://en.wikipedia.org/wiki?curid=45857" title="Hurwitz polynomial">
Hurwitz polynomial

In mathematics, a Hurwitz polynomial, named after Adolf Hurwitz, is a polynomial whose roots (zeros) are located in the left half-plane of the complex plane or on the imaginary axis, that is, the real part of every root is zero or negative. Such a polynomial must have coefficients that are positive real numbers. The term is sometimes restricted to polynomials whose roots have real parts that are strictly negative, excluding the axis (i.e., a Hurwitz stable polynomial).
A polynomial function "P"("s") of a complex variable "s" is said to be Hurwitz if the following conditions are satisfied:
Hurwitz polynomials are important in control systems theory, because they represent the characteristic equations of stable linear systems. Whether a polynomial is Hurwitz can be determined by solving the equation to find the roots, or from the coefficients without solving the equation by the Routh–Hurwitz stability criterion.
Examples.
A simple example of a Hurwitz polynomial is the following:
The only real solution is −1, as it factors to
In general, all second-degree polynomials with positive coefficients are Hurwitz.
This follows directly from the quadratic formula:
where, if the determinant "b^2-4ac" is less than zero, then the polynomial will have to complex-conjugate solutions with real part "-b/a", which is negative for positive "a" and "b".
If it is equal to zero, there will be to coinciding real solutions in "-b/a". Finally, if the determinant is greater than zero, there will be two real negative solutions,
because <math>\sqrt{b^2-4ac} for positive "a", "b" and "c".
Properties.
For a polynomial to be Hurwitz, it is necessary but not sufficient that all of its coefficients be positive (except for second-degree polynomials, which also imply sufficiency). A necessary and sufficient condition that a polynomial is Hurwitz is that it passes the Routh–Hurwitz stability criterion. A given polynomial can be efficiently tested to be Hurwitz or not by using the Routh continued fraction expansion technique.
The properties of Hurwitz polynomials are:

</doc>
