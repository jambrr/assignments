<doc id="46183" url="https://en.wikipedia.org/wiki?curid=46183" title="Butter">
Butter

Butter is a solid dairy product made by churning fresh or fermented cream or milk, to separate the butterfat from the buttermilk. It is generally used as a spread on plain or toasted bread products and a condiment on cooked vegetables, as well as in cooking, such as baking, sauce making, and pan frying. Butter consists of butterfat, milk proteins and water.
Most frequently made from cows' milk, butter can also be manufactured from the milk of other mammals, including sheep, goats, buffalo, and yaks. Salt such as dairy salt, flavorings and preservatives are sometimes added to butter. Rendering butter produces clarified butter or "ghee", which is almost entirely butterfat.
Butter is a water-in-oil emulsion resulting from an inversion of the cream; in a water-in-oil emulsion, the milk proteins are the emulsifiers. Butter remains a solid when refrigerated, but softens to a spreadable consistency at room temperature, and melts to a thin liquid consistency at 32–35 °C (90–95 °F). The density of butter is 911 g/L (0.950 lbs per US pint).
It generally has a pale yellow color, but varies from deep yellow to nearly white. Its unmodified color is dependent on the animals' feed and is commonly manipulated with food colorings in the commercial manufacturing process, most commonly annatto or carotene.
Etymology.
The word "butter" derives (via Germanic languages) from the Latin "butyrum", which is the latinisation of the Greek βούτυρον ("bouturon"). This may have been a construction meaning "cow-cheese", from βοῦς ("bous"), "ox, cow" + τυρός ("turos"), "cheese", but perhaps this is a false etymology of a Scythian word. Nevertheless, the earliest attested form of the second stem, "turos" ("cheese"), is the Mycenaean Greek "tu-ro", written in Linear B syllabic script. The root word persists in the name butyric acid, a compound found in rancid butter and dairy products such as Parmesan cheese.
In general use, the term "butter" refers to the spread dairy product when unqualified by other descriptors. The word commonly is used to describe puréed vegetable or seed and nut products such as peanut butter and almond butter. It is often applied to spread fruit products such as apple butter. Fats such as cocoa butter and shea butter that remain solid at room temperature are also known as "butters". In addition to the act of applying butter being called "to butter", non-dairy items that have a dairy butter consistency may use "butter" to call that consistency to mind, including food items such as maple butter and witch's butter and nonfood items such as baby bottom butter, hyena butter, and rock butter.
Production.
Unhomogenized milk and cream contain butterfat in microscopic globules. These globules are surrounded by membranes made of phospholipids (fatty acid emulsifiers) and proteins, which prevent the fat in milk from pooling together into a single mass. Butter is produced by agitating cream, which damages these membranes and allows the milk fats to conjoin, separating from the other parts of the cream. Variations in the production method will create butters with different consistencies, mostly due to the butterfat composition in the finished product. Butter contains fat in three separate forms: free butterfat, butterfat crystals, and undamaged fat globules. In the finished product, different proportions of these forms result in different consistencies within the butter; butters with many crystals are harder than butters dominated by free fats.
Churning produces small butter grains floating in the water-based portion of the cream. This watery liquid is called buttermilk—although the buttermilk most common today is instead a directly fermented skimmed milk. The buttermilk is drained off; sometimes more buttermilk is removed by rinsing the grains with water. Then the grains are "worked": pressed and kneaded together. When prepared manually, this is done using wooden boards called scotch hands. This consolidates the butter into a solid mass and breaks up embedded pockets of buttermilk or water into tiny droplets.
Commercial butter is about 80% butterfat and 15% water; traditionally made butter may have as little as 65% fat and 30% water. Butterfat is a mixture of triglyceride, a triester derived from glycerol and three of any of several fatty acid groups. Butter becomes rancid when these chains break down into smaller components, like butyric acid and diacetyl. The density of butter is 0.911 g/cm3 (0.527 oz/in3), about the same as ice.
In some countries, butter is given a grade before commercial distribution.
Types.
Before modern factory butter making, cream was usually collected from several milkings and was therefore several days old and somewhat fermented by the time it was made into butter. Butter made from a fermented cream is known as cultured butter. During fermentation, the cream naturally sours as bacteria convert milk sugars into lactic acid. The fermentation process produces additional aroma compounds, including diacetyl, which makes for a fuller-flavored and more "buttery" tasting product. Today, cultured butter is usually made from pasteurized cream whose fermentation is produced by the introduction of "Lactococcus" and "Leuconostoc" bacteria.
Another method for producing cultured butter, developed in the early 1970s, is to produce butter from fresh cream and then incorporate bacterial cultures and lactic acid. Using this method, the cultured butter flavor grows as the butter is aged in cold storage. For manufacturers, this method is more efficient, since aging the cream used to make butter takes significantly more space than simply storing the finished butter product. A method to make an artificial simulation of cultured butter is to add lactic acid and flavor compounds directly to the fresh-cream butter; while this more efficient process is claimed to simulate the taste of cultured butter, the product produced is not cultured but is instead flavored.
Dairy products are often pasteurized during production to kill pathogenic bacteria and other microbes. Butter made from pasteurized fresh cream is called sweet cream butter. Production of sweet cream butter first became common in the 19th century, with the development of refrigeration and the mechanical cream separator. Butter made from fresh or cultured unpasteurized cream is called raw cream butter. While butter made from pasteurized cream may keep for several months, raw cream butter has a shelf life of roughly ten days.
Throughout continental Europe, cultured butter is preferred, while sweet cream butter dominates in the United States and the United Kingdom. Cultured butter is sometimes labeled "European-style" butter in the United States, although cultured butter is made and sold by some, especially Amish, dairies. Commercial raw cream butter is virtually unheard-of in the United States. Raw cream butter is generally only found made at home by consumers who have purchased raw whole milk directly from dairy farmers, skimmed the cream themselves, and made butter with it. It is rare in Europe as well.
Several "spreadable" butters have been developed. These remain softer at colder temperatures and are therefore easier to use directly out of refrigeration. Some methods modify the makeup of the butter's fat through chemical manipulation of the finished product, some manipulate the cattle's feed, and some incorporate vegetable oil into the butter. "Whipped" butter, another product designed to be more spreadable, is aerated by incorporating nitrogen gas—normal air is not used to avoid oxidation and rancidity.
All categories of butter are sold in both salted and unsalted forms. Either granular salt or a strong brine are added to salted butter during processing. In addition to enhanced flavor, the addition of salt acts as a preservative. The amount of butterfat in the finished product is a vital aspect of production. In the United States, products sold as "butter" must contain at least 80% butterfat. In practice, most American butters contain slightly more than that, averaging around 81% butterfat. European butters generally have a higher ratio—up to 85%.
Clarified butter is butter with almost all of its water and milk solids removed, leaving almost-pure butterfat. Clarified butter is made by heating butter to its melting point and then allowing it to cool; after settling, the remaining components separate by density. At the top, whey proteins form a skin, which is removed. The resulting butterfat is then poured off from the mixture of water and casein proteins that settle to the bottom.
Ghee is clarified butter that has been heated to around 120 °C (250 °F) after the water evaporated, turning the milk solids brown. This process flavors the ghee, and also produces antioxidants that help protect it from rancidity. Because of this, ghee can keep for six to eight months under normal conditions.
Whey butter.
Cream may be separated (usually by a centrifugal separator) from whey instead of milk, as a byproduct of cheese-making. Whey butter may be made from whey cream. Whey cream and butter have a lower fat content and taste more salty, tangy and "cheesy". They are also cheaper than "sweet" cream and butter. The fat content of whey is low, so 1000 pounds of whey will typically give 3 pounds of butter.
European butters.
There are several butters produced in Europe with Protected geographical indications; these include:
History.
The earliest butter would have been from sheep or goat's milk; cattle are not thought to have been domesticated for another thousand years. An ancient method of butter making, still used today in parts of Africa and the Near East, involves a goat skin half filled with milk, and inflated with air before being sealed. The skin is then hung with ropes on a tripod of sticks, and rocked until the movement leads to the formation of butter.
In the Mediterranean climate, unclarified butter spoils quickly— unlike cheese, it is not a practical method of preserving the nutrients of milk. The ancient Greeks and Romans seemed to have considered butter a food fit more for the northern barbarians. A play by the Greek comic poet Anaxandrides refers to Thracians as "boutyrophagoi", "butter-eaters". In his "Natural History", Pliny the Elder calls butter "the most delicate of food among barbarous nations", and goes on to describe its medicinal properties. Later, the physician Galen also described butter as a medicinal agent only.
Historian and linguist Andrew Dalby says most references to butter in ancient Near Eastern texts should more correctly be translated as ghee. Ghee is mentioned in the Periplus of the Erythraean Sea as a typical trade article around the first century CE Arabian Sea, and Roman geographer Strabo describes it as a commodity of Arabia and Sudan. In India, ghee has been a symbol of purity and an offering to the gods—especially Agni, the Hindu god of fire—for more than 3000 years; references to ghee's sacred nature appear numerous times in the "Rigveda", circa 1500–1200 BCE. The tale of the child Krishna stealing butter remains a popular children's story in India today. Since India's prehistory, ghee has been both a staple food and used for ceremonial purposes, such as fueling holy lamps and funeral pyres.
Middle Ages.
In the cooler climates of northern Europe, people could store butter longer before it spoiled. Scandinavia has the oldest tradition in Europe of butter export trade, dating at least to the 12th century. After the fall of Rome and through much of the Middle Ages, butter was a common food across most of Europe—but had a low reputation, and so was consumed principally by peasants. Butter slowly became more accepted by the upper class, notably when the early 16th century Roman Catholic Church allowed its consumption during Lent. Bread and butter became common fare among the middle class, and the English, in particular, gained a reputation for their liberal use of melted butter as a sauce with meat and vegetables.
In antiquity, butter was used for fuel in lamps as a substitute for oil. The "Butter Tower" of Rouen Cathedral was erected in the early 16th century when Archbishop Georges d'Amboise authorized the burning of butter instead of oil, which was scarce at the time, during Lent.
Across northern Europe, butter was sometimes treated in a manner unheard-of today: it was packed into barrels (firkins) and buried in peat bogs, perhaps for years. Such "bog butter" would develop a strong flavor as it aged, but remain edible, in large part because of the unique cool, airless, antiseptic and acidic environment of a peat bog. Firkins of such buried butter are a common archaeological find in Ireland; the National Museum of Ireland – Archaeology has some containing "a grayish cheese-like substance, partially hardened, not much like butter, and quite free from putrefaction." The practice was most common in Ireland in the 11th–14th centuries; it ended entirely before the 19th century.
Industrialization.
Like Ireland, France became well known for its butter, particularly in Normandy and Brittany. By the 1860s, butter had become so in demand in France that Emperor Napoleon III offered prize money for an inexpensive substitute to supplement France's inadequate butter supplies. A French chemist claimed the prize with the invention of margarine in 1869. The first margarine was beef tallow flavored with milk and worked like butter; vegetable margarine followed after the development of hydrogenated oils around 1900.
Until the 19th century, the vast majority of butter was made by hand, on farms. The first butter factories appeared in the United States in the early 1860s, after the successful introduction of cheese factories a decade earlier. In the late 1870s, the centrifugal cream separator was introduced, marketed most successfully by Swedish engineer Carl Gustaf Patrik de Laval. This dramatically sped up the butter-making process by eliminating the slow step of letting cream naturally rise to the top of milk. Initially, whole milk was shipped to the butter factories, and the cream separation took place there. Soon, though, cream-separation technology became small and inexpensive enough to introduce an additional efficiency: the separation was accomplished on the farm, and the cream alone shipped to the factory. By 1900, more than half the butter produced in the United States was factory made; Europe followed suit shortly after.
In 1920, Otto Hunziker authored "The Butter Industry, Prepared for Factory, School and Laboratory", a well-known text in the industry that enjoyed at least three editions (1920, 1927, 1940). As part of the efforts of the American Dairy Science Association, Professor Hunziker and others published articles regarding: causes of tallowiness (an odor defect, distinct from rancidity, a taste defect); mottles (an aesthetic issue related to uneven color); introduced salts; the impact of creamery metals and liquids; and acidity measurement. These and other ADSA publications helped standardize practices internationally.
Butter also provided extra income to farm families. They used wood presses with carved decoration to press butter into pucks or small bricks to sell at nearby markets or general stores. The decoration identified the farm that produced the butter. This practice continued until production was mechanized and butter was produced in less decorative stick form. Today, butter presses remain in use for decorative purposes.
Per capita butter consumption declined in most western nations during the 20th century, in large part because of the rising popularity of margarine, which is less expensive and, until recent years, was perceived as being healthier. In the United States, margarine consumption overtook butter during the 1950s, and it is still the case today that more margarine than butter is eaten in the U.S. and the EU.
Size and shape of butter packaging.
Butter has traditionally been made into small, rectangular blocks by means of a pair of wooden butter paddles.
In the United States, butter is usually produced in 4-ounce sticks, wrapped in waxed or foiled paper and sold four to a one-pound carton. This practice is believed to have originated in 1907, when Swift and Company began packaging butter in this manner for mass distribution.
Due to historical differences in butter printers (machines that cut and package butter), these sticks are commonly produced in two different shapes:
Both sticks contain the same amount of butter, although most butter dishes are designed for Elgin-style butter sticks.
The stick's wrapper is usually marked off as eight tablespoons (); the actual volume of one stick is approximately nine tablespoons ().
Outside of the United States, butter is packaged and sold by weight only, not by volume (fluid measure) nor by unit (stick), but the package shape remains approximately the same. The wrapper is usually a foil and waxed-paper laminate (the waxed paper is now a siliconised substitute, but is still referred to in some places as parchment, from the wrapping used in past centuries; and the term 'parchment-wrapped' is still employed where the paper alone is used, without the foil laminate).
In the UK and Ireland, and in some other regions historically accustomed to using British measures, this was traditionally ½lb and 1 lb packs; since metrication, pack sizes have changed to similar metric sizes such as 250g and 500g. In cooking (recipes), butter is specified and measured by weight only (grams or ounces); although melted butter could be measured by fluid measure (centiliters or fluid ounces), this is rare.
In the remainder of the metricated world, butter is packed and sold in 250g and 500g packs (roughly equivalent to the ½lb and 1 lb measures) and measured for cooking in grams or kilograms.
Butter for commercial and industrial use is packaged in plastic buckets, tubs, or drums, in quantities and units suited to the local market.
Worldwide.
In 1997, India produced of butter, most of which was consumed domestically. Second in production was the United States (), followed by France (), Germany (), and New Zealand (). France ranks first in per capita butter consumption with 8 kg per capita per year. In terms of absolute consumption, Germany was second after India, using of butter in 1997, followed by France (), Russia (), and the United States (). New Zealand, Australia, and the Ukraine are among the few nations that export a significant percentage of the butter they produce.
Different varieties are found around the world. "Smen" is a spiced Moroccan clarified butter, buried in the ground and aged for months or years. Yak butter is a speciality in Tibet; "tsampa", barley flour mixed with yak butter, is a staple food. Butter tea is consumed in the Himalayan regions of Tibet, Bhutan, Nepal and India. It consists of tea served with intensely flavored—or "rancid"—yak butter and salt. In African and Asian developing nations, butter is traditionally made from sour milk rather than cream. It can take several hours of churning to produce workable butter grains from fermented milk.
Storage and cooking.
Normal butter softens to a spreadable consistency around 15 °C (60 °F), well above refrigerator temperatures. The "butter compartment" found in many refrigerators may be one of the warmer sections inside, but it still leaves butter quite hard. Until recently, many refrigerators sold in New Zealand featured a "butter conditioner", a compartment kept warmer than the rest of the refrigerator—but still cooler than room temperature—with a small heater. Keeping butter tightly wrapped delays rancidity, which is hastened by exposure to light or air, and also helps prevent it from picking up other odors. Wrapped butter has a shelf life of several months at refrigerator temperatures.
"French butter dishes" or "Acadian butter dishes" have a lid with a long interior lip, which sits in a container holding a small amount of water. Usually the dish holds just enough water to submerge the interior lip when the dish is closed. Butter is packed into the lid. The water acts as a seal to keep the butter fresh, and also keeps the butter from overheating in hot temperatures. This method lets butter sit on a countertop for several days without spoiling.
Once butter is softened, spices, herbs, or other flavoring agents can be mixed into it, producing what is called a "compound butter" or "composite butter" (sometimes also called "composed butter"). Compound butters can be used as spreads, or cooled, sliced, and placed onto hot food to melt into a sauce. Sweetened compound butters can be served with desserts; such hard sauces are often flavored with spirits.
Melted butter plays an important role in the preparation of sauces, most obviously in French cuisine. "Beurre noisette" (hazelnut butter) and "Beurre noir" (black butter) are sauces of melted butter cooked until the milk solids and sugars have turned golden or dark brown; they are often finished with an addition of vinegar or lemon juice. Hollandaise and béarnaise sauces are emulsions of egg yolk and melted butter; they are in essence mayonnaises made with butter instead of oil. Hollandaise and béarnaise sauces are stabilized with the powerful emulsifiers in the egg yolks, but butter itself contains enough emulsifiers—mostly remnants of the fat globule membranes—to form a stable emulsion on its own. "Beurre blanc" (white butter) is made by whisking butter into reduced vinegar or wine, forming an emulsion with the texture of thick cream. "Beurre monté" (prepared butter) is melted but still emulsified butter; it lends its name to the practice of "mounting" a sauce with butter: whisking cold butter into any water-based sauce at the end of cooking, giving the sauce a thicker body and a glossy shine—as well as a buttery taste.
In Poland, the butter lamb ("Baranek wielkanocny") is a traditional addition to the Easter Meal for many Polish Catholics. Butter is shaped into a lamb either by hand or in a lamb-shaped mould. Butter is also used to make edible decorations to garnish other dishes.
Butter is used for sautéing and frying, although its milk solids brown and burn above 150 °C (250 °F)—a rather low temperature for most applications. The smoke point of butterfat is around 200 °C (400 °F), so clarified butter or ghee is better suited to frying. Ghee has always been a common frying medium in India, where many avoid other animal fats for cultural or religious reasons.
Butter fills several roles in baking, where it is used in a similar manner as other solid fats like lard, suet, or shortening, but has a flavor that may better complement sweet baked goods. Many cookie doughs and some cake batters are leavened, at least in part, by creaming butter and sugar together, which introduces air bubbles into the butter. The tiny bubbles locked within the butter expand in the heat of baking and aerate the cookie or cake. Some cookies like shortbread may have no other source of moisture but the water in the butter. Pastries like pie dough incorporate pieces of solid fat into the dough, which become flat layers of fat when the dough is rolled out. During baking, the fat melts away, leaving a flaky texture. Butter, because of its flavor, is a common choice for the fat in such a dough, but it can be more difficult to work with than shortening because of its low melting point. Pastry makers often chill all their ingredients and utensils while working with a butter dough.
Butter also has many non-culinary, traditional uses, specific to certain cultures. For instance, in North America, applying butter to the handle of a door is a common prank on April Fools' Day.
Nutritional information.
As butter is essentially just the milk fat, it contains only traces of lactose, so moderate consumption of butter is not a problem for lactose intolerant people. People with milk allergies may still need to avoid butter, which contains enough of the allergy-causing proteins to cause reactions. Whole milk, butter and cream have high levels of saturated fat.
Butter is a good source of Vitamin A.

</doc>
<doc id="46184" url="https://en.wikipedia.org/wiki?curid=46184" title="The Star Beast">
The Star Beast

The Star Beast is a 1954 science fiction novel by Robert A. Heinlein about a high school senior who discovers that his extraterrestrial pet is more than it appears to be. The novel, somewhat abridged, was originally serialised in "The Magazine of Fantasy & Science Fiction" (May, June, July 1954) as "Star Lummox" and then published in hardcover as part of Scribner's series of Heinlein juveniles.
Plot summary.
An ancestor of John Thomas Stuart XI brought the alien, long-lived Lummox home from an interstellar voyage. The articulate, sentient pet he inherited from his late father has gradually grown from the size of a collie pup to a ridable behemoth—especially after consuming a used car. The childlike Lummox is perceived to be a neighborhood nuisance and, upon leaving the Stuart property one day, causes substantial property damage across the city of Westville. John's mother wants him to get rid of it, and a court orders it destroyed.
Desperate to save his pet, John Thomas considers selling Lummox to a zoo. He rapidly changes his mind and runs away from home, riding into the nearby wilderness on Lummox's back. His girlfriend Betty Sorenson joins him and suggests bringing the beast back into town and hiding it in a neighbor's greenhouse. However, it isn't easy to conceal such a large creature. Eventually, the court tries to have Lummox destroyed, but is unable to do so, much to Lummox's amusement.
Meanwhile, the Hroshii, an advanced, powerful and previously unknown alien race, appear and demand the return of their lost child...or else. A friendly alien diplomat of a third species intimates that the threat is not an empty one. Initially, no one associates Lummox with the newcomers, in part due to the size difference (Lummox was overfed). Lummox is identified as royalty, complicating the already-tense negotiations. It is discovered that, from her viewpoint, the young Lummox has been pursuing her only hobby and principal interest: the raising of John Thomases. She makes it clear that she intends to continue doing so. This gives the chief human negotiator the leverage he needs to establish diplomatic relations with the aliens, who normally do not hold regular relations with other species. At the request of Lummox, the recently married John and Betty accompany her back to her people as members of the human diplomatic mission.
Race.
Heinlein grew up in the era of racial segregation in the United States. This book was very much ahead of its time both in its explicit rejection of racism and in its inclusion of non-white protagonists. It was published in 1954 before the beginning of the US civil rights movement. The mere existence of non-white characters was a remarkable novelty. In this juvenile, the government official in charge of the negotiations with the Hroshii is a Mr. Kiku who is from Africa. Heinlein explicitly states his skin is "ebony black", and that Kiku is in an arranged marriage that is happy.
Critical response.
The noted science fiction author and critic Damon Knight wrote:
This is a novel that won't go bad on you. Many of science fiction's triumphs, even from as little as ten years ago, are unreadable today; they were shoddily put together, not meant for re-use. But Heinlein is durable. I've read this story twice, so far – once in the "Fantasy and Science Fiction" serialized version, once in hard covers – and expect to read it again, sooner or later, for pleasure. I don't know any higher praise.
Groff Conklin described the novel as "one of Heinlein's most enchanting tales." P. Schuyler Miller found "The Star Beast" to be "one of the best of 1954."
Editions.
All paperback editions and the Science Fiction Book Club hard cover edition omit page 148 of Chapter VIII, "The Sensible Thing to Do", which was in the Scribner's edition and the magazine serialization. In this chapter, John Thomas rereads the entries in his great-grandfather's diary of how Lummox was found. Of significance on the omitted page is that:
The diary skipped a couple of days; the "Trail Blazer" had made an emergency raise-ship and Assistant Powerman J. T. Stuart had been too busy to write. John Thomas knew why ... the negotiations opened so hopefully with the dominant race had failed ... no one knew why.
The rest of the page summarizes John Thomas' grandfather's family history, discussing the first John Thomas Stuart, who had retired as a sea captain. The history, as reprinted in the paperback and Science Fiction Book Club editions, then resumes with John Thomas Stuart, Junior.

</doc>
<doc id="46185" url="https://en.wikipedia.org/wiki?curid=46185" title="Mineral matter in plants">
Mineral matter in plants

Minerals are required by plants as part of their food, to form their structure. The firmness of straw for example, is due to the presence in it of silica, the principal constituent of sand and flints. Potassa, soda, lime, magnesia, and phosphoric acid are contained in plants, in different proportions. All of these they must obtain from the soil. The alkalies above named appear to be essential to the proper development of the higher vegetable forms. Some plants require them in one mode of combination, and some in another; and thus the soil that is very good for one, may be quite unfit for others. Firs and pines find enough to support them in barren, sandy soil.
The proportion of silicate of potash, (necessary for the firmness of wheat straw), does not vary perceptibly in the soil of grain fields, because what is removed by the reaper, is again replaced by decaying straw. But this is not the case with meadow-land. Hence you would never find a luxuriant crop of grass on sandy and limestone soils which contain little potash, evidently because one of the constituents indispensable to the growth of the plants is wanting. If a meadow be well manured, we remove, with the increased crop of grass, a greater quantity of potash than can, by a repetition of the same manure, be restored to it. So grass-land manured with gypsum soon ceases to feel its agency. But if the meadow be strewed from time to time with wood ashes, or soap-boilers' lye made from wood ashes, then the grass thrives as luxuriantly as before. The ashes are only a means of restoring the necessary potash for the grass stalks. So oats, barley, and rye may be made for once to grow upon a sandy heath, by mixing with the scanty soil the ashes of the heath-plants that grow upon it. Those ashes contain soda and potash, conveyed to the growing furze or gorse by rain-water. The soil of one district consists of sandstone; certain trees find in it a quantity of alkaline earths sufficient for their own sustenance. When felled, and burnt and sprinkled upon the soil, oats will grow and thrive that without such aid would not vegetate. 
The most decisive proof of the absurdity of the indiscriminate use of any strong manure was obtained at Bingen, a town on the Rhine, where the produce and development of vines were highly increased by manuring them with animal matters such as shavings of horn. After some years, the formation of the wood and leaves decreased perceptibly. Such manure had too much hastened the growth of the vines: in two or three years they had exhausted the potash in the formation of their fruit leaves and wood; so that none remained for the future crops, as shavings of horn contain no potash. Cow-dung would have been better, and is known to be better.
References.
"This text was taken from the "Household Cyclopedia" of 1881."

</doc>
<doc id="46187" url="https://en.wikipedia.org/wiki?curid=46187" title="Lobotomy">
Lobotomy

Lobotomy ( "lobe (of brain)"; τομή "tomē" "cut, slice") is a neurosurgical procedure, a form of psychosurgery, also known as a leukotomy or leucotomy (from the Greek λευκός "leukos" "clear, white" and "tomē"). It consists of cutting or scraping away most of the connections to and from the prefrontal cortex, the anterior part of the frontal lobes of the brain.
The procedure, controversial from its inception, was a mainstream procedure in some Western countries for more than two decades (prescribed for psychiatric and occasionally other conditions) despite general recognition of frequent and serious side effects. While some patients experienced symptomatic improvement with the operation, this was achieved at the cost of creating other impairments, and this balance between benefits and risks contributed to the controversial nature of the procedure. The originator of the procedure, Portuguese neurologist António Egas Moniz, shared the Nobel Prize for Physiology or Medicine of 1949 for the "discovery of the therapeutic value of leucotomy in certain psychoses", although the awarding of the prize has been subject to controversy.
The use of the procedure increased dramatically from the early 1940s and into the 1950s; by 1951, almost 20,000 lobotomies had been performed in the United States. Following the introduction of antipsychotic medications in the mid-1950s and under the influence of the anti-psychiatry movement, lobotomies were quickly and almost completely abandoned.
Effects.
The purpose of the operation was to reduce the symptoms of mental disorder, and it was recognized that this was accomplished at the expense of a person's personality and intellect. British psychiatrist Maurice Partridge, who conducted a follow-up study of 300 patients, said that the treatment achieved its effects by "reducing the complexity of psychic life". Following the operation, spontaneity, responsiveness, self-awareness and self-control were reduced. Activity was replaced by inertia, and people were left emotionally blunted and restricted in their intellectual range.
The consequences of the operation have been described as "mixed". Some patients died as a result of the operation and others later committed suicide. Some were left severely brain-damaged. Others were able to leave the hospital, or became more manageable within the hospital. A few people managed to return to responsible work, while at the other extreme people were left with severe and disabling impairments. Most people fell into an intermediate group, left with some improvement of their symptoms but also with emotional and intellectual deficits to which they made a better or worse adjustment. On average, there was a mortality rate of approximately 5 percent during the 1940s.
The lobotomy procedure could have severe negative effects on a patient's personality and ability to function independently. Lobotomy patients often show a marked reduction in initiative and inhibition. They may also exhibit difficulty putting themselves in the position of others because of decreased cognition and detachment from society.
Immediately following surgery, patients were often stuporous, confused, and incontinent. Some developed an enormous appetite and gained considerable weight. Seizures were another common complication of surgery. Emphasis was put on the training of patients in the weeks and months following surgery.
Freeman coined the term "surgically induced childhood" and used it constantly to refer the results of lobotomy. The operation left people with an "infantile personality"; a period of maturation would then, according to Freeman, lead to recovery. In an unpublished memoir he described how the "personality of the patient was changed in some way in the hope of rendering him more amenable to the social pressures under which he is supposed to exist." He described one 29-year-old woman as being, following lobotomy, a "smiling, lazy and satisfactory patient with the personality of an oyster" who couldn't remember Freeman's name and endlessly poured coffee from an empty pot. When her parents had difficulty dealing with her behaviour, Freeman advised a system of rewards (ice-cream) and punishment (smacks).
Russian psychiatrist Fedor Kondratev, of the Serbsky Center, said that thousands of the people with schizophrenia to whom the method was applied had completely lost the remnants of their mental health; their fate had been irrevocably broken.
History.
In the early 20th century, the number of patients residing in mental hospitals increased significantly while little in the way of effective medical treatment was available. Lobotomy was one of a series of radical and invasive physical therapies developed in Europe at this time that signaled a break with a psychiatric culture of therapeutic nihilism that had prevailed since the late nineteenth-century. The new "heroic" physical therapies devised during this experimental era, including malarial therapy for general paresis of the insane (1917), deep sleep therapy (1920), insulin shock therapy (1933), cardiazol shock therapy (1934), and electroconvulsive therapy (1938), helped to imbue the then therapeutically moribund and demoralised psychiatric profession with a renewed sense of optimism in the curability of insanity and the potency of their craft. The success of the shock therapies, despite the considerable risk they posed to patients, also helped to accommodate psychiatrists to ever more drastic forms of medical intervention, including lobotomy.
The clinician-historian Joel Braslow argues that from malarial therapy onward to lobotomy, physical psychiatric therapies "spiral closer and closer to the interior of the brain" with this organ increasingly taking "center stage as a source of disease and site of cure." For Roy Porter, once the doyen of medical history, the often violent and invasive psychiatric interventions developed during the 1930s and 1940s are indicative of both the well-intentioned desire of psychiatrists to find some medical means of alleviating the suffering of the vast number of patients then in psychiatric hospitals and also the relative lack of social power of those same patients to resist the increasingly radical and even reckless interventions of asylum doctors. Many doctors, patients and family members of the period believed that despite potentially catastrophic consequences, the results of lobotomy were seemingly positive in many instances or, at least they were deemed as such when measured next to the apparent alternative of long-term institutionalisation. Lobotomy has always been controversial, but for a period of the medical mainstream, it was even feted and regarded as a legitimate if desperate remedy for categories of patients who were otherwise regarded as hopeless. Today, lobotomy has become a disparaged procedure, a byword for medical barbarism and an exemplary instance of the medical trampling of patients' rights.
Early psychosurgery.
Prior to the 1930s, individual doctors had infrequently experimented with novel surgical operations on the brains of those deemed insane. Most notably in 1888, the Swiss psychiatrist, Gottlieb Burckhardt, initiated what is commonly considered the first systematic attempt at modern human psychosurgery. He operated on six chronic patients under his care at the Swiss Préfargier Asylum, removing sections of their cerebral cortex. Burckhardt's decision to operate was informed by three pervasive views on the nature of mental illness and its relationship to the brain. First, the belief that mental illness was organic in nature, and reflected an underlying brain pathology; next, that the nervous system was organized according to an associationist model comprising an input or afferent system (a sensory center), a connecting system where information processing took place (an association center), and an output or efferent system (a motor centre); and, finally, a modular conception of the brain whereby discrete mental faculties were connected to specific regions of the brain. Burckhardt's hypothesis was that by deliberately creating lesions in regions of the brain identified as association centres a transformation in behaviour might ensue. According to his model, those mentally ill might experience "excitations abnormal in quality, quantity and intensity" in the sensory regions of the brain and this abnormal stimulation would then be transmitted to the motor regions giving rise to mental pathology. He reasoned, however, that removing material from either of the sensory or motor zones could give rise to "grave functional disturbance". Instead, by targeting the association centres and creating a "ditch" around the motor region of the temporal lobe, he hoped to break their lines of communication and thus alleviate both mental symptoms and the experience of mental distress.
Intending to ameliorate symptoms in those with violent and intractable conditions rather than effect a cure, Burckhardt began operating on patients in December 1888, but both his surgical methods and instruments were crude and the results of the procedure were mixed at best. He operated on six patients in total and, according to his own assessment, two experienced no change, two patients became quieter, one patient experienced epileptic convulsions and died a few days after the operation, and one patient improved. Complications included motor weakness, epilepsy, sensory aphasia and "word deafness". Claiming a success rate of 50 percent, he presented the results at the Berlin Medical Congress and published a report, but the response from his medical peers was hostile and he did no further operations.
In 1912, two physicians based in Saint Petersburg, the leading Russian neurologist Vladimir Bekhterev and his younger Estonian colleague, the neurosurgeon Ludvig Puusepp, published a paper reviewing a range of surgical interventions that had been performed on the mentally ill. While generally treating these endeavours favourably, in their consideration of psychosurgery they reserved unremitting scorn for Burckhardt's surgical experiments of 1888 and opined that it was extraordinary that a trained medical doctor could undertake such an unsound procedure.
"We have quoted this data to show not only how groundless but also how dangerous these operations were. We are unable to explain how their author, holder of a degree in medicine, could bring himself to carry them out ..."
The authors neglected to mention, however, that in 1910 Puusepp himself had performed surgery on the brains of three mentally ill patients, sectioning the cortex between the frontal and parietal lobes. He had abandoned these attempts because of unsatisfactory results and this experience probably inspired the invective that was directed at Burckhardt in the 1912 article. By 1937, Puusepp, despite his earlier criticism of Burckhardt, was increasingly persuaded that psychosurgery could be a valid medical intervention for the mentally disturbed. In the late 1930s he worked closely with the neurosurgical team of the Racconigi Hospital near Turin to establish it as an early and influential centre for the adoption of leucotomy in Italy.
Development.
Leucotomy was first undertaken in 1935 under the direction of the Portuguese neurologist (and inventor of the term "psychosurgery") António Egas Moniz. First developing an interest in psychiatric conditions and their somatic treatment in the early 1930s, Moniz apparently conceived a new opportunity for recognition in the development of a surgical intervention on the brain as a treatment for mental illness.
Frontal lobes.
The source of inspiration for Moniz's decision to hazard psychosurgery has been clouded by contradictory statements made on the subject by Moniz and others both contemporaneously and retrospectively. The traditional narrative addresses the question of why Moniz targeted the frontal lobes by way of reference to the work of the Yale neuroscientist John Fulton and, most dramatically, to a presentation Fulton made with his junior colleague Carlyle Jacobsen at the Second International Congress of Neurology held in London in 1935. Fulton's primary area of research was on the cortical function of primates and he had established America's first primate neurophysiology laboratory at Yale in the early 1930s. At the 1935 Congress, with Moniz in attendance, Fulton and Jacobsen presented two chimpanzees, named Becky and Lucy who had had frontal lobectomies and subsequent changes in behaviour and intellectual function. According to Fulton's account of the congress, they explained that prior to surgery, both animals, and especially Becky, the more emotional of the two, exhibited "frustrational behaviour" – that is, have tantrums that could include rolling on the floor and defecating—if, because of their poor performance in a set experimental tasks, they were not rewarded. Following the surgical removal of their frontal lobes, the behaviour of both primates changed markedly and Becky was pacified to such a degree that Jacobsen apparently stated it was as if she had joined a "happiness cult". During the question and answer section of the paper, Moniz, it is alleged, "startled" Fulton by inquiring if this procedure might be extended to human subjects suffering from mental illness. Fulton stated that he replied that while possible in theory it was surely "too formidable" an intervention for use on humans.
That Moniz began his experiments with leucotomy just three months after the congress has reinforced the apparent cause and effect relationship between the Fulton and Jacobsen's presentation and the Portuguese neurologist's resolve to operate on the frontal lobes. As the author of this account Fulton, who has sometimes been claimed as the father of lobotomy, was later able to record that the technique had its true origination in his laboratory. Endorsing this version of events, in 1949, the Harvard neurologist Stanley Cobb remarked during his presidential address to the American Neurological Association that, "seldom in the history of medicine has a laboratory observation been so quickly and dramatically translated into a therapeutic procedure." Fulton's report, penned ten years after the events described, is, however, without corroboration in the historical record and bears little resemblance to an earlier unpublished account he wrote of the congress. In this previous narrative he mentioned an incidental, private exchange with Moniz, but it is likely that the official version of their public conversation he promulgated is without foundation. In fact, Moniz stated that he had conceived of the operation some time before his journey to London in 1935, having told in confidence his junior colleague, the young neurosurgeon Pedro Almeida Lima, as early as 1933 of his psychosurgical idea. The traditional account exaggerates the importance of Fulton and Jacobsen to Moniz's decision to initiate frontal lobe surgery, and omits the fact that a detailed body of neurological research that emerged at this time suggested to Moniz and other neurologists and neurosurgeons that surgery on this part of the brain might yield significant personality changes in the mentally ill.
As the frontal lobes had been the object of scientific inquiry and speculation since the late 19th century, Fulton's contribution, while it may have functioned as source of intellectual support, is of itself unnecessary and inadequate as an explanation of Moniz's resolution to operate on this section of the brain. Under an evolutionary and hierarchical model of brain development it had been hypothesized that those regions associated with more recent development, such as the mammalian brain and, most especially, the frontal lobes, were responsible for more complex cognitive functions. However, this theoretical formulation found little laboratory support, as 19th century experimentation found no significant change in animal behaviour following surgical removal or electrical stimulation of the frontal lobes. This picture of the so-called "silent lobe" changed in the period after World War I with the production of clinical reports of ex-servicemen who had suffered brain trauma. The refinement of neurosurgical techniques also facilitated increasing attempts to remove brain tumours, treat focal epilepsy in humans and led to more precise experimental neurosurgery in animal studies. Cases were reported where mental symptoms were alleviated following the surgical removal of diseased or damaged brain tissue. The accumulation of medical case studies on behavioural changes following damage to the frontal lobes led to the formulation of the concept of "Witzelsucht", which designated a neurological condition characterised by a certain hilarity and childishness in the afflicted. The picture of frontal lobe function that emerged from these studies was complicated by the observation that neurological deficits attendant on damage to a single lobe might be compensated for if the opposite lobe remained intact. In 1922, the Italian neurologist Leonardo Bianchi published a detailed report on the results of bilateral lobectomies in animals that supported the contention that the frontal lobes were both integral to intellectual function and that their removal led to the disintegration of the subject's personality. This work, while influential, was not without its critics due to deficiencies in experimental design.
The first bilateral lobectomy of a human subject was performed by the American neurosurgeon Walter Dandy in 1930. The neurologist Richard Brickner reported on this case in 1932, relating that the recipient, known as "Patient A", while experiencing a flattening of affect, had suffered no apparent decrease in intellectual function and seemed, at least to the casual observer, perfectly normal. Brickner concluded from this evidence that "the frontal lobes are not 'centers' for the intellect". These clinical results were replicated in a similar operation undertaken in 1934 by the neurosurgeon Roy Glenwood Spurling and reported on by the neuropsychiatrist Spafford Ackerly. By the mid-1930s, interest in the function of the frontal lobes reached a high-water mark. This was reflected in the 1935 neurological congress in London, which hosted as part of its deliberations, "a remarkable symposium ... on the functions of the frontal lobes." The panel was chaired by Henri Claude, a French neuropsychiatrist, who commenced the session by reviewing the state of research on the frontal lobes, and concluded that, "altering the frontal lobes profoundly modifies the personality of subjects". This parallel symposium contained numerous papers by neurologists, neurosurgeons and psychologists; amongst these was one by Brickner, which impressed Moniz greatly, that again detailed the case of "Patient A". Fulton and Jacobsen's paper, presented in another session of the conference on experimental physiology, was notable in linking animal and human studies on the function of the frontal lobes. Thus, at the time of the 1935 Congress, Moniz had available to him an increasing body of research on the role of the frontal lobes that extended well beyond the observations of Fulton and Jacobsen.
Nor was Moniz the only medical practitioner in the 1930s to have contemplated procedures directly targeting the frontal lobes. Although ultimately discounting brain surgery as carrying too much risk, physicians and neurologists such as William Mayo, Thierry de Martel, Richard Brickner, and Leo Davidoff had, prior to 1935, entertained the proposition. Inspired by Julius Wagner-Jauregg's development of malarial therapy for the treatment of general paresis of the insane, the French physician Maurice Ducosté reported in 1932 that he had injected 5 ml of malarial blood directly into the frontal lobes of over 100 paretic patients through holes drilled into the skull. He claimed that the injected paretics showed signs of "uncontestable mental and physical amelioration" and that the results for psychotic patients undergoing the procedure was also "encouraging". The experimental injection of fever inducing malarial blood into the frontal lobes was also replicated during the 1930s in the work of Ettore Mariotti and M. Sciutti in Italy and Ferdière Coulloudon in France. In Switzerland, almost simultaneously with the commencement of Moniz's leucotomy programme, the neurosurgeon François Ody had removed the entire right frontal lobe of a catatonic schizophrenic patient. In Romania, Ody's procedure was adopted by Dimitri Bagdasar and Constantinesco working out of the Central Hospital in Bucharest. Ody, who delayed publishing his own results for several years, later rebuked Moniz for claiming to have cured patients through leucotomy without waiting to determine if there had been a "lasting remission".
Neurological model.
The theoretical underpinnings of Moniz's psychosurgery were largely commensurate with the nineteenth century ones that had informed Burckhardt's decision to excise matter from the brains of his patients. Although in his later writings Moniz referenced both the neuron theory of Ramón y Cajal and the conditioned reflex of Ivan Pavlov, in essence he simply interpreted this new neurological research in terms of the old psychological theory of associationism. He differed significantly from Burckhardt, however in that he did not think there was any organic pathology in the brains of the mentally ill, but rather that their neural pathways were caught in fixed and destructive circuits leading to "predominant, obsessive ideas." As Moniz wrote in 1936:
For Moniz, "to cure these patients," it was necessary to "destroy the more or less fixed arrangements of cellular connections that exist in the brain, and particularly those which are related to the frontal lobes," thus removing their fixed pathological brain circuits. Moniz believed the brain would functionally adapt to such injury. A significant advantage of this approach was that, unlike the position adopted by Burckhardt, it was unfalsifiable according to the knowledge and technology of the time as the absence of a known correlation between physical brain pathology and mental illness could not disprove his thesis.
First leucotomies.
On 12 November 1935 at the Hospital Santa Marta in Lisbon, Moniz initiated the first of a series of operations on the brains of the mentally ill. The initial patients selected for the operation were provided by the medical director of Lisbon's Miguel Bombarda Mental Hospital, José de Matos Sobral Cid. As Moniz lacked training in neurosurgery and his hands were crippled from gout, the procedure was performed under general anaesthetic by Pedro Almeida Lima, who had previously assisted Moniz with his research on cerebral angiography. The intention was to remove some of the long fibres that connected the frontal lobes to other major brain centres. To this end, it was decided that Lima would trephine into the side of the skull and then inject ethanol into the "subcortical white matter of the prefrontal area" so as to destroy the connecting fibres, or association tracts, and create what Moniz termed a "frontal barrier". After the first operation was complete, Moniz considered it a success and, observing that the patient's depression had been relieved, he declared her "cured" although she was never, in fact, discharged from the mental hospital. Moniz and Lima persisted with this method of injecting alcohol into the frontal lobes for the next seven patients but, after having to inject some patients on numerous occasions to elicit what they considered a favourable result, they modified the means by which they would section the frontal lobes. For the ninth patient they introduced a surgical instrument called a leucotome; this was a cannula that was in length and in diameter. It had a retractable wire loop at one end that, when rotated, produced a diameter circular lesion in the white matter of the frontal lobe. Typically, six lesions were cut into each lobe, but, if they were dissatisfied by the results, Lima might perform several procedures, each producing multiple lesions in the left and right frontal lobes.
By the conclusion of this first run of leucotomies in February 1936, Moniz and Lima had operated on twenty patients with an average period of one week between each procedure; Moniz published his findings with great haste in March of the same year. The patients were aged between 27 and 62 years of age, twelve were female and eight were male. Nine of the patients were diagnosed as suffering from depression, six from schizophrenia, two from panic disorder, and one each from mania, catatonia and manic-depression with the most prominent symptoms being anxiety and agitation. The duration of the illness prior to the procedure varied from as little as four weeks to as much as 22 years, although all but four had been ill for at least one year. Patients were normally operated on the day they arrived at Moniz's clinic and returned within ten days to the Miguel Bombarda Mental Hospital. A perfunctory post-operative follow-up assessment took place anywhere from one to ten weeks following surgery. Complications were observed in each of the leucotomy patients and included: "increased temperature, vomiting, bladder and bowel incontinence, diarrhea, and ocular affections such as ptosis and nystagmus, as well as psychological effects such as apathy, akinesia, lethargy, timing and local disorientation, kleptomania, and abnormal sensations of hunger". Moniz asserted that these effects were transitory and, according to his published assessment, the outcome for these first twenty patients was that 35%, or seven cases, improved significantly, another 35% were somewhat improved and the remaining 30% (six cases) were unchanged. There were no deaths and he did not consider that any patients had deteriorated following leucotomy.
Reception.
Moniz rapidly disseminated his results through articles in the medical press and a monograph in 1936. Initially, however, the medical community appeared hostile to the new procedure. On 26 July 1936, one of his assistants, Diogo Furtado, gave a presentation at the Parisian meeting of the Société Médico-Psychologique on the results of the second cohort of patients leucotomised by Lima. Sobral Cid, who had supplied Moniz with the first set of patients for leucotomy from his own hospital in Lisbon, attended the meeting where he denounced frontal lobe surgery, declaring that the patients who had been returned to his care post-operatively were "diminished" and had suffered a "degradation of personality". He also claimed that the changes Moniz observed in patients were more properly attributed to shock and brain trauma, and he derided the theoretical architecture that Moniz had constructed to support the new procedure as "cerebral mythology." At the same meeting the Parisian psychiatrist, Paul Courbon, stated that he could not endorse a surgical technique that was solely supported by theoretical considerations rather than clinical observations. He also opined that the mutilation of an organ could not improve its function and that such cerebral wounds as were occasioned by leucotomy risked the later development of meningitis, epilepsy and brain abscesses. Nonetheless, Moniz's reported successful surgical treatment of 14 out of 20 patients led to the rapid adoption of the procedure on an experimental basis by individual clinicians in countries such as Brazil, Cuba, Italy, Romania and the United States during the 1930s.
Italian leucotomy.
Throughout the remainder of the 1930s the number of leucotomies performed in most countries where the technique was adopted remained quite low. In Britain, which was later a major centre for leucotomy, only six operations had been undertaken prior to 1942. Generally, medical practitioners who attempted the procedure adopted a cautious approach and few patients were leucotomised prior to the 1940s. Italian neuropsychiatrists, who were typically early and enthusiastic adopters of leucotomy, were exceptional in eschewing such a gradualist course.
Leucotomy was first reported in the Italian medical press in 1936 and Moniz published an article in Italian on the technique in the following year. In 1937, he was invited to Italy to demonstrate the procedure and for a two-week period in June of that year he visited medical centres in Trieste, Ferrara, and one close to Turin – the Racconigi Hospital – where he instructed his Italian neuropsychiatric colleagues on leucotomy and also oversaw several operations. Leucotomy was featured at two Italian psychiatric conferences in 1937 and over the next two years a score of medical articles on Moniz's psychosurgery was published by Italian clinicians based in medical institutions located in Racconigi, Trieste, Naples, Genoa, Milan, Pisa, Catania and Rovigo. The major centre for leucotomy in Italy was the Racconigi Hospital, where the experienced neurosurgeon Ludvig Puusepp provided a guiding hand. Under the medical directorship of Emilio Rizzatti, the medical personnel at this hospital had completed at least 200 leucotomies by 1939. Reports from clinicians based at other Italian institutions detailed significantly smaller numbers of leucotomy operations.
Experimental modifications of Moniz's operation were introduced with little delay by Italian medical practitioners. Most notably, in 1937 Amarro Fiamberti, the medical director of a psychiatric institution in Varese, first devised the transorbital procedure whereby the frontal lobes were accessed through the eye sockets. Fiamberti's method was to puncture the thin layer of orbital bone at the top of the socket and then inject alcohol or formalin into the white matter of the frontal lobes through this aperture. Using this method, while sometimes substituting a leucotome for a hypodermic needle, it is estimated that he leucotomised about 100 patients in the period up to the outbreak of World War II. Fiamberti's innovation of Moniz's method would later prove inspirational for Walter Freeman's development of transorbital lobotomy.
American leucotomy.
The first prefrontal leucotomy on American soil was performed at the George Washington University Hospital on 14 September 1936 by the neuropsychiatrist Walter Freeman and his friend and colleague, the neurosurgeon, James W. Watts. Freeman had first encountered Moniz at the London hosted Second International Congress of Neurology in 1935 where he had presented a poster exhibit of the Portuguese neurologist's work on cerebral angiography. Fortuitously occupying a booth next to Moniz, Freeman, delighted by their chance meeting, formed a highly favourable impression of Moniz, later remarking upon his "sheer genius". According to Freeman, if they had not met in person it is highly unlikely that he would have ventured into the domain of frontal lobe psychosurgery. Freeman's interest in psychiatry was the natural outgrowth of his appointment in 1924 as the medical director of the Research Laboratories of the Government Hospital for the Insane in Washington, known colloquially as St Elizabeth's. Ambitious and a prodigious researcher, Freeman, who favoured an organic model of mental illness causation, spent the next several years exhaustively, yet ultimately fruitlessly, investigating a neuropathological basis for insanity. Chancing upon a preliminary communication by Moniz on leucotomy in the spring of 1936, Freeman initiated a correspondence in May of that year. Writing that he had been considering psychiatric brain surgery previously, he informed Moniz that, "having your authority I expect to go ahead". Moniz, in return, promised to send him a copy of his forthcoming monograph on leucotomy and urged him to purchase a leucotome from a French supplier.
Upon receipt of Moniz's monograph, Freeman reviewed it anonymously for the "Archives of Neurology and Psychiatry". Praising the text as one whose "importance can scarcely be overestimated", he summarised Moniz's rationale for the procedure as based on the fact that while no physical abnormality of cerebral cell bodies was observable in the mentally ill, their cellular interconnections may harbour a "fixation of certain patterns of relationship among various groups of cells" and that this resulted in obsessions, delusions and mental morbidity. While recognising that Moniz's thesis was inadequate, for Freeman it had the advantage of circumventing the search for diseased brain tissue in the mentally ill by instead suggesting that the problem was a functional one of the brain's internal wiring where relief might be obtained by severing problematic mental circuits.
In 1937 Freeman and Watts adapted Lima and Moniz's surgical procedure, and created the "Freeman-Watts technique", also known as the "Freeman-Watts standard prefrontal lobotomy," which they styled the "precision method".
Transorbital lobotomy.
The Freeman-Watts prefrontal lobotomy still required drilling holes in the scalp, so surgery had to be performed in an operating room by trained neurosurgeons. Walter Freeman believed this surgery would be unavailable to those he saw as needing it most: patients in state mental hospitals that had no operating rooms, surgeons, or anesthesia and limited budgets. Freeman wanted to simplify the procedure so that it could be carried out by psychiatrists in psychiatric hospitals.
Inspired by the work of Italian psychiatrist Amarro Fiamberti, Freeman at some point conceived of approaching the frontal lobes through the eye sockets instead of through drilled holes in the skull. In 1945 he took an icepick from his own kitchen and began testing the idea on grapefruit and cadavers. This new "transorbital" lobotomy involved lifting the upper eyelid and placing the point of a thin surgical instrument (often called an orbitoclast or leucotome, although quite different from the wire loop leucotome described above) under the eyelid and against the top of the eyesocket. A mallet was used to drive the orbitoclast through the thin layer of bone and into the brain along the plane of the bridge of the nose, around fifteen degrees toward the interhemispherical fissure. The orbitoclast was malleted five centimeters (2 in) into the frontal lobes, and then pivoted forty degrees at the orbit perforation so the tip cut toward the opposite side of the head (toward the nose). The instrument was returned to the neutral position and sent a further two centimeters ( in) into the brain, before being pivoted around twenty-eight degrees each side, to cut outwards and again inwards. (In a more radical variation at the end of the last cut described, the butt of the orbitoclast was forced upwards so the tool cut vertically down the side of the cortex of the interhemispherical fissure; the "Deep frontal cut".) All cuts were designed to transect the white fibrous matter connecting the cortical tissue of the prefrontal cortex to the thalamus. The leucotome was then withdrawn and the procedure repeated on the other side.
Freeman performed the first transorbital lobotomy on a live patient in 1946. Its simplicity suggested the possibility of carrying it out in mental hospitals lacking the surgical facilities required for the earlier, more complex procedure (Freeman suggesting that, where conventional anesthesia was unavailable, electroconvulsive therapy be used to render the patient unconscious). In 1947, the Freeman and Watts partnership ended, as the latter was disgusted by Freeman's modification of the lobotomy from a surgical operation into a simple "office" procedure. Between 1940 and 1944, 684 lobotomies were performed in the United States. However, because of the fervent promotion of the technique by Freeman and Watts, those numbers increased sharply towards the end of the decade. In 1949, the peak year for lobotomies in the US, 5,074 procedures were undertaken, and by 1951 over 18,608 individuals had been lobotomized in the US.
Prevalence.
In the United States, approximately 40,000 people were lobotomized. In Great Britain, 17,000 lobotomies were performed, and the three Nordic countries of Finland, Norway, and Sweden had a combined figure of approximately 9,300 lobotomies. Scandinavian hospitals lobotomized 2.5 times as many people per capita as hospitals in the US. Sweden lobotomized at least 4,500 people between 1944 and 1966, mainly women. This figure includes young children. In Norway, there were 2,500 known lobotomies. In Denmark, there were 4,500 known lobotomies, mainly young women, as well as mentally retarded children. In Japan, the majority of lobotomies were performed on children with behavior problems. The Soviet Union banned the practice in 1950 on moral grounds, and Japan and Germany soon followed suit. By the late 1970s, the practice of lobotomy had generally ceased.
Criticism.
As early as 1944 an author in the "Journal of Nervous and Mental Disease" remarked: "The history of prefrontal lobotomy has been brief and stormy. Its course has been dotted with both violent opposition and with slavish, unquestioning acceptance." Beginning in 1947 Swedish psychiatrist Snorre Wohlfahrt evaluated early trials, reporting that it is "distinctly hazardous to leucotomize schizophrenics" and lobotomy to be "still too imperfect to enable us, with its aid, to venture on a general offensive against chronic cases of mental disorder" and stating that "Psychosurgery has as yet failed to discover its precise indications and contraindications and the methods must unfortunately still be regarded as rather crude and hazardous in many respects." In 1948 Norbert Wiener, the author of "", said: "refrontal lobotomy ... has recently been having a certain vogue, probably not unconnected with the fact that it makes the custodial care of many patients easier. Let me remark in passing that killing them makes their custodial care still easier."
Concerns about lobotomy steadily grew. Soviet psychiatrist Vasily Gilyarovsky criticized lobotomy and the mechanistic brain localization assumption used to carry out lobotomy: "It is assumed that the transection of white substance of the frontal lobes impairs their connection with the thalamus and eliminates the possibility to receive from it stimuli which lead to irritation and on the whole derange mental functions. This explanation is mechanistic and goes back to the narrow localizationism characteristic of psychiatrists of America, from where leucotomy was imported to us." The USSR officially banned the procedure in 1950 on the initiative of Gilyarovsky. Doctors in the Soviet Union concluded that the procedure was "contrary to the principles of humanity" and "'through lobotomy' an insane person is changed into an idiot." By the 1970s, numerous countries had banned the procedure as had several US states.
In 1977 the US Congress, during the presidency of Jimmy Carter, created the National Committee for the Protection of Human Subjects of Biomedical and Behavioral Research to investigate allegations that psychosurgery—including lobotomy techniques—were used to control minorities and restrain individual rights. The committee concluded that some extremely limited and properly performed psychosurgery could have positive effects.
There have been calls in the early 21st century for the Nobel Foundation to rescind the prize it awarded to Moniz for developing the lobotomy, a decision that has been called an astounding error of judgment at the time and one that psychiatry might still need to learn from, but the Foundation declined to take action and has continued to host an article defending the results of the procedure.
Significant literary and cinematic portrayals.
Lobotomies have been featured in several literary and cinematic presentations that both reflected society's attitude towards the procedure and, at times, changed it. Writers and film-makers have played a pivotal role in forming a negative public sentiment towards the procedure.
Sources.
Print Sources
Online sources

</doc>
<doc id="46191" url="https://en.wikipedia.org/wiki?curid=46191" title="Tillage">
Tillage

Tillage is the agricultural preparation of soil by mechanical agitation of various types, such as digging, stirring, and overturning. Examples of human-powered tilling methods using hand tools include shovelling, picking, mattock work, hoeing, and raking. Examples of draft-animal-powered or mechanized work include ploughing (overturning with moldboards or chiseling with chisel shanks), rototilling, rolling with cultipackers or other rollers, harrowing, and cultivating with cultivator shanks (teeth). Small-scale gardening and farming, for household food production or small business production, tends to use the smaller-scale methods above, whereas medium- to large-scale farming tends to use the larger-scale methods. There is a fluid continuum, however. Any type of gardening or farming, but especially larger-scale commercial types, may also use low-till or no-till methods as well.
Tillage is often classified into two types, primary and secondary. There is no strict boundary between them so much as a loose distinction between tillage that is deeper and more thorough (primary) and tillage that is shallower and sometimes more selective of location (secondary). Primary tillage such as ploughing tends to produce a rough surface finish, whereas secondary tillage tends to produce a smoother surface finish, such as that required to make a good seedbed for many crops. Harrowing and rototilling often combine primary and secondary tillage into one operation.
"Tillage" can also mean the land that is tilled. The word "cultivation" has several senses that overlap substantially with those of "tillage". In a general context, both can refer to agriculture. Within agriculture, both can refer to any of the kinds of soil agitation described above. Additionally, "cultivation" or "cultivating" may refer to an even narrower sense of shallow, selective secondary tillage of row crop fields that kills weeds while sparing the crop plants.
Tillage systems.
Reduced tillage.
Reduced tillage leaves between 15 and 30% residue cover on the soil or 500 to 1000 pounds per acre (560 to 1100 kg/ha) of small grain residue during the critical erosion period. This may involve the use of a chisel plow, field cultivators, or other implements. See the general comments below to see how they can affect the amount of residue.
Intensive tillage.
Intensive tillage leaves less than 15% crop residue cover or less than 500 pounds per acre (560 kg/ha) of small grain residue. This type of tillage is often referred to as conventional tillage but as conservational tillage is now more widely used than intensive tillage (in the United States), it is often not appropriate to refer to this type of tillage as conventional. Intensive tillage often involves multiple operations with implements such as a mold board, disk, and/or chisel plow. Then a finisher with a harrow, rolling basket, and cutter can be used to prepare the seed bed. There are many variations.
Conservation tillage.
Conservation tillage leaves at least 30% of crop residue on the soil surface, or at least 1,000 lb/ac (1,100 kg/ha) of small grain residue on the surface during the critical soil erosion period. This slows water movement, which reduces the amount of soil erosion. Conservation tillage also benefits farmers by reducing fuel consumption and soil compaction. By reducing the number of times the farmer travels over the field, farmers realize significant savings in fuel and labor. In most years since 1997, conservation tillage was used in US cropland more than intensive or reduced tillage.
However, conservation tillage delays warming of the soil due to the reduction of dark earth exposure to the warmth of the spring sun, thus delaying the planting of the next year's spring crop of corn.
Zone tillage.
Zone tillage is a form of modified deep tillage in which only narrow strips are tilled, leaving soil in between the rows untilled. This type of tillage agitates the soil to help reduce soil compaction problems and to improve internal soil drainage.
Purpose.
Zone tillage is designed to only disrupt the soil in a narrow strip directly below the crop row. In comparison to no-till, which relies on the previous year’s plant residue to protect the soil and aides in postponement of the warming of the soil and crop growth in Northern climates, zone tillage creates approximately a 5-inch-wide strip that simultaneously breaks up plow pans, assists in warming the soil and helps to prepare a seedbed. When combined with cover crops, zone tillage helps replace lost organic matter, slows the deterioration of the soil, improves soil drainage, increases soil water and nutrient holding capacity, and allows necessary soil organisms to survive.
Usage.
It has been successfully used on farms in the mid-west and west for over 40 years and is currently used on more than 36% of the U.S. farmland. Some specific states where zone tillage is currently in practice are Pennsylvania, Connecticut, Minnesota, Indiana, Wisconsin, and Illinois.
Unfortunately, there aren't consistent yield results in the Northern Cornbelt states however; there is still interest in deep tillage within the agriculture industry. In areas that are not well-drained, deep tillage may be used as an alternative to installing more expensive tile drainage.
Effects of tillage.
Positive.
Plowing:
Definitions.
"Primary tillage" loosens the soil and mixes in fertilizer and/or plant material, resulting in soil with a rough texture.
"Secondary tillage" produces finer soil and sometimes shapes the rows, preparing the seed bed. It also provides weed control throughout the growing season during the maturation of the crop plants, unless such weed control is instead achieved with low-till or no-till methods involving herbicides.
History of tilling.
Tilling was first performed via human labor, sometimes involving slaves. Hoofed animals could also be used to till soil via trampling. The wooden plow was then invented. It could be pulled by mule, ox, elephant, water buffalo, or similar sturdy animal. Horses are generally unsuitable, though breeds such as the scyne could work. The steel plow allowed farming in the American Midwest, where tough prairie grasses and rocks caused trouble. Soon after 1900, the farm tractor was introduced, which eventually made modern large-scale agriculture possible.
Alternatives to tilling.
Modern agricultural science has greatly reduced the use of tillage. Crops can be grown for several years without any tillage through the use of herbicides to control weeds, crop varieties that tolerate packed soil, and equipment that can plant seeds or fumigate the soil without really digging it up. This practice, called no-till farming, reduces costs and environmental change by reducing soil erosion and diesel fuel usage. Researchers are investigating farming in polyculture that would eliminate the need for both tillage and pesticides, such as no-dig gardening.

</doc>
<doc id="46193" url="https://en.wikipedia.org/wiki?curid=46193" title="Threshing machine">
Threshing machine

The thrashing machine, or, in modern spelling, threshing machine (or simply thresher), was first invented by Scottish mechanical engineer Andrew Meikle for use in agriculture. It was devised (c. 1786) for the separation of grain from stalks and husks. For thousands of years, grain was separated by hand with flails, and was very laborious and time-consuming, taking about one-quarter of agricultural labor by the 18th century. Mechanization of this process took much of the drudgery out of farm labour.
Early social impacts.
The Swing Riots in the UK were partly a result of the threshing machine. Following years of war, high taxes and low wages, farm labourers finally revolted in 1830. These farm labourers had faced unemployment for a number of years due to the widespread introduction of the threshing machine and the policy of enclosing fields. No longer were thousands of men needed to tend the crops, a few would suffice. With fewer jobs, lower wages and no prospects of things improving for these workers the threshing machine was the final straw, the machine was to place them on the brink of starvation. The Swing Rioters smashed threshing machines and threatened farmers who had them.
The riots were dealt with very harshly. Nine of the rioters were hanged and a further 450 were transported to Australia.
Later adoption.
Early threshing machines were hand-fed and horse-powered. They were small by today's standards and were about the size of an upright piano. Later machines were steam-powered, driven by a portable engine or traction engine. Isaiah Jennings, a skilled inventor, created a small thresher that doesn't harm the straw in the process. In 1834, John Avery and Hiram Abial Pitts devised significant improvements to a machine that automatically threshes and separates grain from chaff, freeing farmers from a slow and laborious process. Avery and Pitts were granted United States patent #542 on December 29, 1837.
John Ridley, an Australian inventor, also developed a threshing machine in South Australia in 1843.
The 1881 "Household Cyclopedia" said of Meikle's machine:
Steam-powered machines used belts connected to a traction engine; often both engine and thresher belonged to a contractor who toured the farms of a district. Steam remained a viable commercial option until the early post-WWII years.
Farming process.
Threshing is just one process in getting cereals to the grinding mill and customer.
The wheat needs to be grown, cut, stooked (shocked, bundled), hauled, threshed, de-chaffed, straw baled, and then the grain hauled to a grain elevator. For many years each of these steps was an individual process, requiring teams of workers and many machines. In the steep hill wheat country of Palouse in the Northwest of the United States, steep ground meant moving machinery around was problematic and prone to rolling. To reduce the amount of work on the sidehills, the idea arose of combining the wheat binder and thresher into one machine, known as a combine harvester. About 1910, horse pulled combines appeared and became a success. Later, gas and diesel engines appeared with other refinements and specifications.
Modern developments.
In Europe and Americas.
Modern day combine harvesters (or simply combines) operate on the same principles and use the same components as the original threshing machines built in the 19th century. Combines also perform the reaping operation at the same time. The name "combine" is derived from the fact that the two steps are combined in a single machine. Also, most modern combines are self-powered (usually by a diesel engine) and self-propelled, although tractor powered, pull type combines models were offered by John Deere and Case International into the 1990s.
Today, as in the 19th century, the threshing begins with a cylinder and concave. The cylinder has sharp serrated bars, and rotates at high speed (about 500 RPM), so that the bars beat against the grain. The concave is curved to match the curve of the cylinder, and serves to hold the grain as it is beaten. The beating releases the grain from the straw and chaff.
Whilst the majority of the grain falls through the concave, the straw is carried by a set of "walkers" to the rear of the machine, allowing any grain and chaff still in the straw to fall below. Below the straw walkers, a fan blows a stream of air across the grain, removing dust and fines and blowing them away.
The grain, either coming through the concave or the walkers, meets a set of sieves mounted on an assembly called a shoe, which is shaken mechanically. The top sieve has larger openings, and serves to remove large pieces of chaff from the grain. The lower sieve separates clean grain, which falls through, from incompletely threshed pieces. The incompletely threshed grain is returned to the cylinder by means of a system of conveyors, where the process repeats.
Some threshing machines were equipped with a bagger, which invariably held two bags, one being filled, and the other being replaced with an empty. A worker called a "sewer" removed and replaced the bags, and sewed full bags shut with a needle and thread. Other threshing machines would discharge grain from a conveyor, for bagging by hand. Combines are equipped with a grain tank, which accumulates grain for deposit in a truck or wagon.
A large amount of chaff and straw would accumulate around a threshing machine, and several innovations, such as the air chaffer, were developed to deal with this. Combines generally chop and disperse straw as they move through the field, though the chopping is disabled when the straw is to be baled, and chaff collectors are sometimes used to prevent the dispersal of weed seed throughout a field.
The corn sheller was almost identical in design, with slight modifications to deal with the larger kernel size and presence of cobs. Modern-day combines can be adjusted to work with any grain crop, and many unusual seed crops.
Both the older and modern machines require a good deal of skill to operate. The concave clearance, cylinder speed, fan velocity, sieve sizes, and feeding rate must be adjusted for crop conditions.
Another development in Asia.
From the early 20th century, petrol or diesel-powered threshing machines, designed especially to thresh rice, the most important crop in Asia, have been developed along different lines to the modern combine.
Even after the combine was invented and became popular, a new compact-size thresher called a "harvester", with wheels, still remains in use and at present it is available from a Japanese agricultural manufacturer. The compact-size machine is very convenient to handle in small terrace fields in mountain areas where a large machine, such as combine, is not usable.
People there use this harvester with a modern compact binder.
Preservation.
A number of older threshing machines have survived into preservation. They are often to be seen in operation at live steam festivals and traction engine rallies such as the Great Dorset Steam Fair in England, and the Western Minnesota Steam Threshers Reunion in northwest Minnesota.
Musical references.
Irish songwriter John Duggan immortalised the threshing machine in a song "The Old Thrashing Mill". The song has been recorded by Foster and Allen and Brendan Shine.
On the Alan Lomax collection Songs of Seduction (Rounder Select, 2000), there's a bawdy Irish folk song called "The Thrashing Machine" sung by tinker Annie O'Neil, as recorded in the early 20th Century.
In his film score for "Of Mice and Men" (1939) and consequently in his collection "Music for the Movies" (1942), American composer Aaron Copland titled a section of the score "Threshing Machines," to suit a scene in the Lewis Milestone film where Curley is threatening Slim over giving May a puppy, when many of the itinerant worker men are standing around or working on threshers.
In the song Thrasher from the album Rust Never Sleeps, Neil Young compares the modern threshing machine's technique of separating wheat from wheat stalks to the natural forces of time that separate close friends from one another.

</doc>
<doc id="46195" url="https://en.wikipedia.org/wiki?curid=46195" title="Moss, Norway">
Moss, Norway

Its administrative district covers areas east of the town, such as the island of Dillingøy in the lake Vansjø. Parts of the town are located on the peninsula of Jeløy. Moss city has 30,723 inhabitants (2012).
Name.
The Old Norse form of the name was "Mors". It may be derived from an old root "mer-" which means to "divide" or "split".
The adjacent topography shares similar etymology:
History.
Archeological finds suggest that there were settlements in the area more than 7,000 years ago and continuously through the Iron Age, Viking Age, through to modern times. During the Viking era, the place was known as "Varna" (forne, vorne, front-protection?) and was the site of a cooperative for battleships held by local warlords on behalf of the king.
The first literary reference to the name Mo(u)ſs(ß) is from Bishop Eystein Aslaksson's Red book (NRA AM fol. 328) from 1396, and by then the town had become a commercial center with craftsmen and mills. By the 16th century, the town's port was significant enough to warrant its own customs official. Liquor distilleries became one of the dominant industries, and it was not until 1607, after the Reformation, that the town got its own church.
By 1700, Moss had become a hub for both ship and land traffic between Copenhagen and Christiania, and in 1704 Moss Jernverk (Moss Ironworks) was established just north of the city center. By 1720 it received its charter as a merchant town, with its own official. This may have had background in an important battle in 1716 that was fought in the town square in Moss in which Norwegian troops commanded by Vincent Budde prevailed over invading Swedish forces, sent by Charles XII to capture Akershus Fortress. In 1767 a local resident built a "pleasure pavilion" near the town, which survives as the Hotel Refsnes Gods.
In 1814, Moss became the site for the signing of the Convention of Moss, which effectively put an end to the Dano-Norwegian kingdom. This set the stage for economic development that has persisted to this day.
On the morning of 14 July 2006, a bolide exploded above the nearby town of Rygge - moments later, several stony meteorites fell over Moss. A number of meteorites were recovered by local residents and visiting meteorite hunters, which after analysis and classification, were found to be a rare type of carbonaceous chondrite.
Coat-of-arms.
The coat-of-arms is from modern times. They were granted on 2 April 1954. Moss became a separate city in 1786 and received its first seal in the same year. The seal showed a church under some clouds, the whole thing placed within a circle. Above the circle there were some fasces, the freedom symbol of the late 19th century. A later seal, dating from around 1829, shows the same composition, but now also with six birds flying around the church.
When in the 1930s the city wanted to adopt a coat-of-arms and the birds were chosen as a possible symbol. The original birds probably were doves, symbol of peace. In 1934, the idea of the crow was launched, since the nickname of the inhabitants was 'crows'. The arms were finally granted in 1954 and show a yellow crow on a red background. It was designed by Christian Stenersen.
There is a tale being told in Moss about the Church fire: The city of Moss always had a lot of crows, most likely because of the corn being harvested in the region. The fire disturbed the crows that started to make a lot of noise and the inhabitants rescued the church from total destruction. After this episode the idea of crow as arms was launched.
Norwegian lady statues.
Moss and Virginia Beach, Virginia in the United States are sister cities. On Good Friday, 27 March 1891, the Norwegian bark "Dictator", whose home port was Moss, was lost in the treacherous waters of the Graveyard of the Atlantic. The ship had been en route to England from Pensacola, Florida with a cargo of Georgia Pine lumber. After being caught and disabled in a storm, she was headed for port at Hampton Roads, Virginia to make repairs when she encountered another storm just off Virginia Beach.
Working in the high winds and seas, lifesaving crews from shore were able to save some of the 17 persons aboard. However, the pregnant wife of Captain J.M. Jorgensen, Johanne, and their 4-year-old son Carl were among the 7 persons who drowned.
The ship's wooden female figurehead had washed ashore. It was placed in a vertical position facing the ocean near the boardwalk as a memorial to those who lost their lives in the shipwreck. It was a landmark there for more than 60 years, but gradually became weathered and eroded.
In 1962, Norwegian sculptor Ørnulf Bast was commissioned to create two nine-foot bronze replicas of the original figurehead by the City of Moss. The Norwegian Lady Statues were unveiled on 22 September 1962. One was presented as a gift to Virginia Beach, and an exact duplicate was erected in Moss to unite the two sister cities. Each statue gives the appearance of facing the other across the Atlantic Ocean.
On 13 October 1995, Queen Sonja of Norway visited the Norwegian Lady statue in Virginia Beach, and placed memorial flowers.
Industry.
The town is known for paper mills, as well as metalworks and other factories. Dillingøy is known as a place for alternative non-military civil service. Moss is mentioned since the Renaissance and was the site of the signing of the Convention of Moss in 1814, which solidified the union with Sweden. The headquarters of textile producer Helly Hansen were located in Moss until 2009. The maker of international hotel keycards, Trio Ving, also has their headquarters here.
Transport.
Moss is served by Moss Airport, Rygge, which is located in the neighboring municipality of Rygge. It opened as a civilian airport in 2007 and is served predominantly by low-cost airlines, particularly Ryanair. The railway Østfold Line runs through Moss, stopping at Moss Station, which is the southern terminus of one service of the Oslo Commuter Rail and an intermediate stop for regional trains. Moss connects across the Oslofjord to Horten via the Moss–Horten Ferry. There are also bus-lines to Oslo Airport, Gardermoen, Gothenburg, Copenhagen, Oslo in addition to local bus lines. Moss port is one of the top 3 busiest container ports in Norway (messured in TEUs) .
International relations.
Twin towns — Sister cities.
The following cities are twinned with Moss:
Use of preposition with "Moss".
"In Moss" is translated "i Moss". In the 1800s one said Moss "på Moss".

</doc>
<doc id="46196" url="https://en.wikipedia.org/wiki?curid=46196" title="Marl">
Marl

__NOTOC__
Marl or marlstone is a calcium carbonate or lime-rich mud or mudstone which contains variable amounts of clays and silt. The dominant carbonate mineral in most marls is calcite, but other carbonate minerals such as aragonite, dolomite, and siderite may be present. Marl was originally an old term loosely applied to a variety of materials, most of which occur as loose, earthy deposits consisting chiefly of an intimate mixture of clay and calcium carbonate, formed under freshwater conditions; specifically an earthy substance containing 35–65% clay and 65–35% carbonate. It also describes a habit of coralline red alga. The term is today often used to describe indurated marine deposits and lacustrine (lake) sediments which more accurately should be named 'marlstone'. Marlstone is an indurated rock of about the same composition as marl, more correctly called an earthy or impure argillaceous limestone. It has a blocky subconchoidal fracture, and is less fissile than shale. The term 'marl' is widely used in English-language geology, while the terms "Mergel" and "Seekreide" (German for "lake chalk") are used in European references.
The lower stratigraphic units of the chalk cliffs of Dover consist of a sequence of glauconitic marls followed by rhythmically banded limestone and marl layers. Upper Cretaceous cyclic sequences in Germany and marl–opal-rich Tortonian-Messinian strata in the Sorbas basin related to multiple sea drawdown have been correlated with Milankovitch orbital forcing.
Marl as lacustrine sediment is common in postglacial lake-bed sediments, often found underlying peat bogs. It has been used as a soil conditioner and acid soil neutralizing agent.
Types used in agriculture.
Marl was extensively mined in Central New Jersey as a soil conditioner in the 1800s. In 1863, the most common marl was blue marl. While the specific composition and properties of the marl varied depending on what layer is found in, blue marl was generally composed of 38.70% silicic acid and sand, 30.67% oxide of iron, 13.91% carbonate of lime, 11.22% water, 4.47% potash, 1.21% magnesia, 1.14% phosphoric acid, and .31% sulphuric acid.
Marl was in high demand for farms. An example of the amount of marl mined comes from a report from 1880, from Marlboro, Monmouth County, New Jersey, which reported the following tons of marl sold:
In the Centennial Exhibition report in 1877, marl is described in many different forms and came from sixty-nine marl pits in and around New Jersey. The report identified a number of agricultural marls types, including clay marl, blue marl, red marl, high bank marl, shell layer marl, under shell layer marl, sand marl, green marl, gray marl, and clayey marl.

</doc>
<doc id="46202" url="https://en.wikipedia.org/wiki?curid=46202" title="Pink noise">
Pink noise

Pink noise or noise is a signal or process with a frequency spectrum such that the power spectral density (energy or power per Hz) is inversely proportional to the frequency of the signal. In pink noise, each octave (halving/doubling in frequency) carries an equal amount of noise power. The name arises from the pink appearance of visible light with this power spectrum.
Within the scientific literature the term pink noise is sometimes used a little more loosely to refer to any noise with a power spectral density of the form
where "f" is frequency and 0 < α < 2, with exponent α usually close to 1. These pink-like noises occur widely in nature and are a source of considerable interest in many fields. The distinction between the noises with α near 1 and those with a broad range of α approximately corresponds to a much more basic distinction. The former (narrow sense) generally come from condensed matter systems in quasi-equilibrium, as discussed below. The latter (broader sense) generally correspond to a wide range of non-equilibrium driven dynamical systems.
The term "flicker noise" is sometimes used to refer to pink noise, although this is more properly applied only to its occurrence in electronic devices. Mandelbrot and Van Ness proposed the name "fractional noise" (sometimes since called "fractal noise") to emphasize that the exponent of the spectrum could take non-integer values and be closely related to fractional Brownian motion, but the term is very rarely used.
Description.
There is equal energy in all octaves (or similar log bundles) of frequency. In terms of power at a constant bandwidth, pink noise falls off at 3 dB per octave. At high enough frequencies pink noise is never dominant. (White noise is equal energy per hertz.)
The human auditory system, which processes frequencies in a roughly logarithmic fashion approximated by the Bark scale, does not perceive different frequencies with equal sensitivity; signals around 1–4 kHz sound loudest for a given intensity. However, humans still differentiate between white noise and pink noise with ease.
Graphic equalizers also divide signals into bands logarithmically and report power by octaves; audio engineers put pink noise through a system to test whether it has a flat frequency response in the spectrum of interest. Systems that do not have a flat response can be equalized by creating an inverse filter using a graphic equalizer. Because pink noise has a tendency to occur in natural physical systems it is often useful in audio production. Pink noise can be processed, filtered, and/or effects can be added to produce desired sounds. Pink noise generators are commercially available.
One parameter of noise, the peak versus average energy contents, or crest factor, is important for testing purposes, such as for audio power amplifier and loudspeaker capabilities because the signal power is a direct function of the crest factor. Various crest factors of pink noise can be used in simulations of various levels of dynamic range compression in music signals. On some digital pink noise generators the crest factor can be specified.
Generalization to more than one dimension.
The spectrum of pink noise is only for one-dimensional signals. For two-dimensional signals (e.g., images) the spectrum is reciprocal to "f 2". In general, in an "n"-dimensional system, the spectrum is reciprocal to "f n". For higher-dimensional signals it is still true (by definition) that each octave carries an equal amount of noise power. The frequency spectrum of two-dimensional signals, for instance, is also two-dimensional, and the area covered by succeeding octaves is four times as large.
Occurrence.
In the past quarter century, pink noise has been discovered in the temporal fluctuations of an extraordinarily diverse number of physical and biological systems (Press, 1978; see articles in Handel & Chung, 1993, and references therein). Examples of its occurrence include fluctuations in tide and river heights, quasar light emissions, heart beat, firings of single neurons, and resistivity in solid state devices. The most accessible introduction to the significance of pink noise is one given by Martin Gardner (1978) in his Scientific American column "Mathematical Games". In this particular column, Gardner asked for the sense in which music imitates nature. Sounds in nature are not musical in that they tend to be either too repetitive (bird song, insect noises) or too chaotic (ocean surf, wind in trees, and so forth). The answer to this question was given in a statistical sense by Voss and Clarke (1975, 1978), who showed that pitch and loudness fluctuations in speech and music are pink noises. So music is like tides not in terms of how tides sound, but in how tide heights vary.
Because pink noise occurs in many physical, biological and economic systems, some researchers describe it as being ubiquitous. In physical systems, it is present in some meteorological data series, the electromagnetic radiation output of some astronomical bodies, and in almost all electronic devices (referred to as flicker noise). In biological systems, it is present in, for example, heart beat rhythms, neural activity, and the statistics of DNA sequences, as a generalized pattern.
In financial systems, it is often referred to as a "long-term memory effect". Also, it describes the statistical structure of many natural images (images from the natural environment). Recently, pink noise has also been successfully applied to the modeling of mental states in psychology, and used to explain stylistic variations in music from different cultures and historic periods. Richard F. Voss and J. Clarke claim that almost all musical melodies, when each successive note is plotted on a scale of pitches, will tend towards a pink noise spectrum. Similarly, a generally pink distribution pattern has been observed in film shot length by researcher James E. Cutting of Cornell University, in the study of 150 popular movies released from 1935 to 2005.
Pink noise has also been found to be endemic in human response. Gilden et al. (1995) found extremely pure examples of this noise in the time series formed upon iterated production of temporal and spatial intervals. Later, Gilden (1997) and Gilden (2001) found that time series formed from reaction time measurement and from iterated two-alternative forced choice also produced pink noises.
There are no simple mathematical models to create pink noise. Although self-organised criticality has been able to reproduce pink noise in sandpile models, these do not have a Gaussian distribution or other expected statistical qualities. It is usually generated by filtering white noise or inverse Fourier transform.
There are many theories of the origin of pink noise. Some theories attempt to be universal, while others are applicable to only a certain type of material, such as semiconductors. Universal theories of pink noise remain a matter of current research interest.
A hypothesis (referred to as the Tweedie hypothesis) has been proposed to explain the genesis of pink noise on the basis of a mathematical convergence theorem related to the central limit theorem of statistics. The Tweedie convergence theorem describes the convergence of certain statistical processes towards a family of statistical models known as the Tweedie distributions. These distributions are characterized by a variance to mean power law, that have been variously identified in the ecological literature as Taylor's law and in the physics literature as "fluctuation scaling". When this variance to mean power law is demonstrated by the method of expanding enumerative bins this implies the presence of pink noise, and vice versa. Both of these effects can be shown to be the consequence of mathematical convergence such as how certain kinds of data will converge towards the normal distribution under the central limit theorem. This hypothesis also provides for an alternative paradigm to explain power law manifestations that have been attributed to self-organized criticality.
Electronic devices.
A pioneering researcher in this field was Aldert van der Ziel.
In electronics, white noise will be stronger than pink noise (flicker noise) above some corner frequency. There is no known lower bound to pink noise in electronics. Measurements made down to 10−6 Hz (taking several weeks) have not shown a ceasing of pink-noise behaviour.
A pink noise source is sometimes included on analog synthesizers (although a white noise source is more common), both as a useful audio sound source for further processing, and also as a source of random control voltages for controlling other parts of the synthesizer.
The principal sources of pink noise in electronic devices are almost invariably the slow fluctuations of properties of the condensed-matter materials of the devices. In many cases the specific sources of the fluctuations are known. These include fluctuating configurations of defects in metals, fluctuating occupancies of traps in semiconductors, and fluctuating domain structures in magnetic materials. The explanation for the approximately pink spectral form turns out to be relatively trivial, usually coming from a distribution of kinetic activation energies of the fluctuating processes. Since the frequency range of the typical noise experiment (e.g., 1 Hz — 1 kHz) is low compared with typical microscopic "attempt frequencies" (e.g., 1014 Hz), the exponential factors in the Arrhenius equation for the rates are large. Relatively small spreads in the activation energies appearing in these exponents then result in large spreads of characteristic rates. In the simplest toy case, a flat distribution of activation energies gives exactly a pink spectrum, because formula_2

</doc>
<doc id="46203" url="https://en.wikipedia.org/wiki?curid=46203" title="35 mm film">
35 mm film

35 mm film is the film gauge most commonly used for motion pictures and chemical still photography (see 135 film). The name of the gauge refers to the width of the photographic film, which consists of strips 34.98 ±0.03 mm (1.377 ±0.001 inches) wide. The standard negative pulldown for movies ("single-frame" format) is four perforations per frame along both edges, which results in 16 frames per foot of film. For still photography, the standard frame has eight perforations on each side.
A variety of largely proprietary gauges were devised for the numerous camera and projection systems being developed independently in the late 19th century and early 20th century, ranging from 13 mm to 75 mm (0.51–2.95 in), as well as a variety of film feeding systems. This resulted in cameras, projectors, and other equipment having to be calibrated to each gauge. The 35 mm width, originally specified as 1.375 inches, was introduced in 1892 by William Dickson and Thomas Edison, using film stock supplied by George Eastman. Film 35 mm wide with four perforations per frame became accepted as the international standard gauge in 1909, and remained by far the dominant film gauge for image origination and projection until the advent of digital photography and cinematography, despite challenges from smaller and larger gauges, because its size allowed for a relatively good trade-off between the cost of the film stock and the quality of the images captured.
The gauge has been versatile in application. It has been modified to include sound, redesigned to create a safer film base, formulated to capture color, has accommodated a bevy of widescreen formats, and has incorporated digital sound data into nearly all of its non-frame areas. Eastman Kodak, Fujifilm and Agfa-Gevaert are some companies which offered 35 mm films.
The ubiquity of 35 mm movie projectors in commercial movie theaters made 35 mm the only motion picture format that could be played in almost any cinema in the world, until digital projection largely superseded it early in the 21st century.
Early history.
In 1880 George Eastman began to manufacture gelatin dry photographic plates in Rochester, New York. Along with W. H. Walker, Eastman invented a holder for a roll of picture-carrying gelatin layer coated paper. Hannibal Goodwin's invention of nitrocellulose film base in 1887 was the first transparent, flexible film. Eastman's was the first major company, however, to mass-produce these components, when in 1889 Eastman realized that the dry-gelatino-bromide emulsion could be coated onto this clear base, eliminating the paper.
With the advent of flexible film, Thomas Alva Edison quickly set out on his invention, the Kinetoscope, which was first shown at the Brooklyn Institute of Arts and Sciences on 9 May 1893. The Kinetoscope was a film loop system intended for one-person viewing. Edison, along with assistant W. K. L. Dickson, followed that up with the Kinetophone, which combined the Kinetoscope with Edison's cylinder phonograph. Beginning in March 1892, Eastman and then, from April 1893 into 1896, New York's Blair Camera Co. supplied Edison with film stock. At first Blair would supply only 40 mm (1-9/16 in) film stock that would be trimmed and perforated at the Edison lab to create 1-⅜ inch (34.925 mm) gauge filmstrips, then at some point in 1894 or 1895, Blair began sending stock to Edison that was cut exactly to specification. Edison's aperture defined a single frame of film at 4 perforations high. Edison claimed exclusive patent rights to his design of 35 mm motion picture film, with four sprocket holes per frame, forcing his only major filmmaking competitor, American Mutoscope & Biograph, to use a 68 mm film that used friction feed, not sprocket holes, to move the film through the camera. A court judgment in March 1902 invalidated Edison's claim, allowing any producer or distributor to use the Edison 35 mm film design without license. Filmmakers were already doing so in Britain and Europe, where Edison had failed to file patents.
At the time, film stock was usually supplied unperforated and punched by the filmmaker to their standards with perforation equipment. A variation developed by the Lumière Brothers used a single circular perforation on each side of the frame towards the middle of the horizontal axis. It was Edison's format, however, that became first the dominant standard and then the "official" standard of the newly formed Motion Picture Patents Company, a trust established by Edison, which agreed in 1909 to what would become the standard: 35 mm gauge, with Edison perforations and a 1.3 aspect ratio. Scholar Paul C. Spehr describes the importance of these developments:
The film format was introduced into still photography as early as 1913 (the Tourist Multiple) but first became popular with the launch of the Leica camera, created by Oskar Barnack in 1925.
Amateur interest.
The costly image-forming silver compounds in a film stock's emulsion meant from the start that 35 mm filmmaking was to be an expensive hobby with a high barrier to entry for the public at large. Furthermore, the nitrocellulose film base of all early film stock was highly flammable, creating considerable risk for those not accustomed to the precautions necessary in its handling. The cost of film stock was directly proportional to its surface area, so a smaller film gauge for amateur use was the obvious path to affordability. The downside was that smaller images were less sharp and detailed, and because less light could be put through them in the finished film the size of an acceptably bright projected image was also limited.
Birt Acres was the first to attempt an amateur format, creating Birtac in 1898 by slitting the film into 17.5 mm widths. By the early 1920s, several formats had successfully split the amateur home movies market away from 35 mm: 28 mm (1.1 in) (1912), 9.5 mm (0.37 in) (1922), 16 mm (0.63 in) (1923), and Pathe Rural, a 17.5 mm format designed for safety film (1926). Eastman Kodak's 16 mm format won the amateur market and is still widely in use today, mainly in the Super 16 variation, which remains popular with professional filmmakers. The 16 mm size was specifically chosen to prevent third-party slitting, as it was easy to create 17.5 mm stock from slitting 35 mm stock in two. It also was the first major format to be released with only fireproof cellulose diacetate (and later cellulose triacetate) "safety film" base. This amateur market would be further diversified by the introduction of 8 mm film (0.31 in) in 1932, intended for amateur filmmaking and "home movies". By law, 16 mm and 8 mm gauge stock (and 35 mm films intended for non-theatrical use) had to be manufactured on safety stock. The effect of these gauges was to essentially make the 35 mm gauge almost the exclusive province of professional filmmakers, a divide which mostly remains to this day.
Still cameras.
Just as the format was recognized as a standard in 1909, still film cameras were developed that took advantage of the 35 mm format and allowed a large number of exposures for each length of film loaded into the camera. The frame size was increased to 24×36 mm. Although the first design was patented as early as 1908, the first commercial 35 mm camera was the 1913 Tourist Multiple, for movie and still photography, soon followed by the Simplex providing selection between full and half frame format. Oskar Barnack built his prototype Ur-Leica in 1913 and had it patented, but Ernst Leitz did not decide to produce it before 1924. The first Leica camera to be fully standardised was the Leica Standard of 1932.
How film works.
Inside the photographic emulsion are millions of light-sensitive silver halide crystals. Each crystal is a compound of silver plus a halogen (such as bromine, iodine or chlorine) held together in a cubical arrangement by electrical attraction. When the crystal is struck with light, free-moving silver ions build up a small collection of uncharged atoms. These small bits of silver, too small to even be visible under a microscope, are the beginning of a latent image. Developing chemicals use the latent image specks to build up density, an accumulation of enough metallic silver to create a visible image.
The emulsion is attached to the film base with a transparent adhesive called the subbing layer. Below the base is an undercoat called the anti-halation backing, which usually contains absorber dyes or a thin layer of silver or carbon (called rem-jet on color negative stocks). Without this coating, bright points of light would penetrate the emulsion, reflect off the inner surface of the base, and re-expose the emulsion, creating a halo around these bright areas. The anti-halation backing can also serve to reduce static buildup, which was a significant problem with old black-and-white films. The film, which runs through the camera at per second, could build up enough static electricity to cause a spark bright enough to expose the film; anti-halation backing solved this problem. Color films have multiple layers of silver halide emulsion to separately record the red, green and blue thirds of the spectrum. For every silver halide grain there is a matching color coupler grain (except Kodachrome film, to which color couplers were added during processing). The top layer of emulsion is sensitive to blue; below it is a yellow filter layer to block blue light; and under that is a green-sensitive layer followed by a red-sensitive layer.
Just as in black-and-white, the first step in color development converts exposed silver halide grains into metallic silver – except that an equal amount of color dye will be formed as well. The color couplers in the blue-sensitive layer will form yellow dye during processing, the green layer will form magenta dye and the red layer will form cyan dye. A bleach step will convert the metallic silver back into silver halide, which is then removed along with the unexposed silver halide in the fixer and wash steps, leaving only color dyes.
In the 1980s Eastman Kodak invented the T-Grain, a synthetically manufactured silver halide grain that had a larger, flat surface area and allowed for greater light sensitivity in a smaller, thinner grain. Thus Kodak could solve the problem of higher speed (greater light sensitivity—see film speed) which required larger grain and therefore more "grainy" images. With T-Grain technology, Kodak refined the grain structure of all their "EXR" line of motion picture film stocks (which was eventually incorporated into their "MAX" still stocks). Fuji films followed suit with their own grain innovation, the tabular grain in their SUFG (Super Unified Fine Grain) SuperF negative stocks, which are made up of thin hexagonal tabular grains.
Attributes.
Color.
Originally, film was a strip of cellulose nitrate coated with black-and-white photographic emulsion. Early film pioneers, like D. W. Griffith, color tinted or toned portions of their movies for dramatic impact, and by 1920, 80 to 90 percent of all films were tinted. The first successful natural color process was Britain's Kinemacolor (1908–1914), a two-color additive process that used a rotating disk with red and green filters in front of the camera lens and the projector lens. But any process that photographed and projected the colors sequentially was subject to color "fringing" around moving objects, and a general color flickering.
In 1916, William Van Doren Kelley began developing Prizma, the first commercially viable American color process using 35 mm film. Initially, like Kinemacolor, it photographed the color elements one after the other and projected the results by additive synthesis. Ultimately, Prizma was refined to bipack photography, with two strips of film, one treated to be sensitive to red and the other not, running through the camera face to face. Each negative was printed on one surface of the same duplitized print stock and each resulting series of black-and-white images was chemically toned to transform the silver into a monochrome color, either orange-red or blue-green, resulting in a two-sided, two-colored print that could be shown with any ordinary projector. This system of two-color bipack photography and two-sided prints was the basis for many later color processes, such as Multicolor, Brewster Color and Cinecolor.
Although it had been available previously, color in Hollywood feature films first became truly practical from the studios' commercial perspective with the advent of Technicolor, whose main advantage was quality prints in less time than its competitors. In its earliest incarnations, Technicolor was another two-color system that could reproduce a range of reds, muted bluish greens, pinks, browns, tans and grays, but not real blues or yellows. "Toll of the Sea", released in 1922, was the first film printed in their subtractive color system. Technicolor's camera photographed each pair of color-filtered frames simultaneously on one strip of black-and-white film by means of a beam splitter prism behind the camera lens. Two prints on half-thickness stock were made from the negative, one from only the red-filtered frames, the other from the green-filtered frames. After development, the silver images on the prints were chemically toned to convert them into images of the approximately complementary colors. The two strips were then cemented together back to back, forming a single strip similar to duplitized film.
In 1928, Technicolor started making their prints by the imbibition process, which was mechanical rather than photographic and allowed the color components to be combined on the same side of the film. Using two matrix films bearing hardened gelatin relief images, thicker where the image was darker, aniline color dyes were transferred into the gelatin coating on a third, blank strip of film.
Technicolor re-emerged as a three-color process for cartoons in 1932 and live action in 1934. Using a different arrangement of a beam-splitter cube and color filters behind the lens, the camera simultaneously exposed three individual strips of black-and-white film, each one recording one-third of the spectrum, which allowed virtually the entire spectrum of colors to be reproduced. A printing matrix with a hardened gelatin relief image was made from each negative, and the three matrices transferred color dyes into a blank film to create the print.
Two-color processes, however, were far from extinct. In 1934, William T. Crispinel and Alan M. Gundelfinger revived the Multicolor process under the company name Cinecolor. Cinecolor saw considerable use in animation and low-budget pictures, mainly because it cost much less than three-color Technicolor. If color design was carefully managed, the lack of colors such as true green could pass unnoticed. Although Cinecolor used the same duplitized stock as Prizma and Multicolor, it had the advantage that its printing and processing methods yielded larger quantities of finished film in less time.
In 1950 Kodak announced the first Eastman color 35 mm negative film (along with a complementary positive film) that could record all three primary colors on the same strip of film. An improved version in 1952 was quickly adopted by Hollywood, making the use of three-strip Technicolor cameras and bipack cameras (used in two-color systems such as Cinecolor) obsolete in color cinematography. This "monopack" structure is made up of three separate emulsion layers, one sensitive to red light, one to green and one to blue.
Safety film.
Although Eastman Kodak had first introduced acetate-based film, it was far too brittle and prone to shrinkage, so the dangerously flammable nitrate-based cellulose films were generally used for motion picture camera and print films. In 1949 Kodak began replacing all nitrocellulose (nitrate-based) films with the safer, more robust cellulose triacetate-based "Safety" films. In 1950 the Academy of Motion Picture Arts and Sciences awarded Kodak with a Scientific and Technical Academy Award (Oscar) for the safer triacetate stock. By 1952, all camera and projector films were triacetate-based. Most if not all film prints today are made from synthetic polyester safety base (which started replacing Triacetate film for prints in the early 1990s). The downside of polyester film is that it is extremely strong, and, in case of a fault, will stretch and not break–potentially causing damage to the projector and ruining a fairly large stretch of film: 2–3 ft or ~2 seconds. Also, polyester film will melt if exposed to the projector lamp for too long. Original camera negative is still made on a triacetate base, and some intermediate films (certainly including internegatives or "dupe" negatives, but not necessarily including interpositives or "master" positives) are also made on a triacetate base as such films must be spliced during the "negative assembly" process, and the extant negative assembly process is solvent-based. Polyester films are not compatible with solvent-based assembly processes.
Other types.
Besides black & white and color negative films, there are black & white and color reversal films, which when developed create a positive ("natural") image that is projectable. There are also films sensitive to non-visible wavelengths of light, such as infrared.
Common formats.
Academy format.
In the conventional motion picture format, frames are four perforations tall, with an aspect ratio of 1.375:1, 22 mm by 16 mm (0.866 in × 0.630 in). This is a derivation of the aspect ratio and frame size designated by Thomas Edison (24.89 mm by 18.67 mm or 0.980 in by 0.735 in) at the dawn of motion pictures, which was an aspect ratio of 1.33:1. The first sound features were released in 1926–27, and while Warner Bros. was using synchronized phonograph discs (sound-on-disc), Fox placed the soundtrack in an optical record directly on the film (sound-on-film) on a strip between the sprocket holes and the image frame. "Sound-on-film" was soon adopted by the other Hollywood studios, resulting in an almost square image ratio of 0.860 in by 0.820 in.
By 1929, most movie studios had revamped this format using their own house aperture plate size to try to recreate the older screen ratio of 1.33:1. Furthermore, every theater chain had their own house aperture plate size in which the picture was projected. These sizes often did not match up even between theaters and studios owned by the same company, and therefore, uneven projection practices occurred.
In November 1929, the Society of Motion Pictures Engineers set a standard aperture ratio of 0.800 in by 0.600 in. Known as the "1930 standard," studios which followed the suggested practice of marking their camera viewfinders for this ratio were: Paramount-Famous-Lasky, Metro-Goldwyn Mayer, United Artists, Pathe, Universal, RKO, Tiffany-Stahl, Mack Sennett, Darmour, and Educational. The Fox Studio markings were the same width but allowed .04 in more height.
In 1932, in refining this ratio, the Academy of Motion Picture Arts and Sciences expanded upon this 1930 standard. The camera aperture became 22 mm by 16 mm (0.866 in by 0.630 in), and the projected image would use an aperture plate size of 0.825 by 0.600 in (21 by 15 mm), yielding an aspect ratio of 1.375:1. This became known as the "Academy" ratio, named so after them. Since the 1950s the aspect ratio of some theatrically released motion picture films has been 1.85:1 (1.66:1 in Europe) or 2.35:1 (2.40:1 after 1970). The image area for "TV transmission" is slightly smaller than the full "Academy" ratio at 21 mm by 16 mm (0.816 in by 0.612 in), an aspect ratio of 1.33:1. Hence when the "Academy" ratio is referred to as having an aspect ratio of 1.33:1, it is done so mistakenly.
Widescreen.
The commonly used anamorphic format uses a similar four-perf frame, but an anamorphic lens is used on the camera and projector to produce a wider image, today with an aspect ratio of about 2.39:1 (more commonly referred to as 2.40:1). The ratio was formerly 2.35:1—and is still often mistakenly referred to as such—until an SMPTE revision of projection standards in 1970). The image, as recorded on the negative and print, is horizontally compressed (squeezed) by a factor of 2.
The unexpected success of the Cinerama widescreen process in 1952 led to a boom in film format innovations to compete with the growing audiences of television and the dwindling audiences in movie theaters. These processes could give theatergoers an experience that television could not at that time—color, stereophonic sound and panoramic vision. Before the end of the year, 20th Century Fox had narrowly "won" a race to obtain an anamorphic optical system invented by Henri Chrétien, and soon began promoting the Cinemascope technology as early as the production phase.
Looking for a similar alternative, other major studios hit upon a simpler, less expensive solution by April 1953: using a removable aperture plate in the film projector gate, the top and bottom of the frame could be cropped to create a wider aspect ratio. Paramount Studios began this trend with their aspect ratio of 1.66:1, first used in "Shane", which was originally shot for Academy ratio. It was Universal Studios, however, with their May release of "Thunder Bay" that introduced the now standard 1.85:1 format to American audiences and brought attention to the industry the capability and low cost of equipping theaters for this transition.
Other studios followed suit with aspect ratios of 1.75:1 up to 2:1. For a time, these various ratios were used by different studios in different productions, but by 1956, the aspect ratio of 1.85:1 became the "standard" US format. These "flat" films are photographed with the full Academy frame, but are matted (most often with a mask in the theater projector, not in the camera) to obtain the "wide" aspect ratio. This standard, in some European nations, became 1.66:1 instead of 1.85:1, although some productions with pre-determined American distributors compose for the latter to appeal to US markets.
In September 1953, 20th Century Fox debuted CinemaScope with their production of "The Robe" to great success. CinemaScope became the first marketable usage of an anamorphic widescreen process and became the basis for a host of "formats," usually suffixed with "-scope," that were otherwise identical in specification, although sometimes inferior in optical quality. (Some developments, such as SuperScope and Techniscope, however, were truly entirely different formats.) By the early 1960s, however, Panavision would eventually solve many of the Cinemascope lenses' technical limitations with their own lenses, and by 1967, Cinemascope was replaced by Panavision and other third-party manufacturers.
The 1950s and 1960s saw many other novel processes using 35 mm, such as VistaVision, SuperScope, Technirama, and Techniscope, most of which ultimately became obsolete. VistaVision, however, would be revived decades later by Lucasfilm and other studios for special effects work, while a SuperScope variant became the predecessor to the modern Super 35 format that is popular today.
Super 35.
The concept behind Super 35 originated with the Tushinsky Brothers' SuperScope format, particularly the SuperScope 235 specification from 1956. In 1982, Joe Dunton revived the format for "Dance Craze", and Technicolor soon marketed it under the name "Super Techniscope" before the industry settled on the name Super 35. The central driving idea behind the process is to return to shooting in the original silent "Edison" 1.33:1 full 4-perf negative area (24.89 mm by 18.67 mm or 0.980 in by 0.735 in), and then crop the frame either from the bottom or the center (like 1.85:1) to create a 2.40:1 aspect ratio (matching that of anamorphic lenses) with an area of 24 mm by 10 mm (0.945 in by 0.394 in). Although this cropping may seem extreme, by expanding the negative area out perf-to-perf, Super 35 creates a 2.40:1 aspect ratio with an overall negative area of 240 square millimeters (0.372 sq in), only 9 mm2 (0.014 sq in) less than the 1.85:1 crop of the Academy frame (248.81 mm2 or 0.386 sq in). The cropped frame is then converted at the intermediate stage to a 4-perf anamorphically squeezed print compatible with the anamorphic projection standard. This allows an "anamorphic" frame to be captured with non-anamorphic lenses, which are much more common. Up to 2000, once the film was photographed in Super 35, an optical printer was used to anamorphose (squeeze) the image. This optical step reduced the overall quality of the image and made Super 35 a controversial subject among cinematographers, many who preferred the higher image quality and frame negative area of anamorphic photography (especially with regard to granularity). With the advent of Digital intermediates (DI) at the beginning of the 21st century, however, Super 35 photography has become even more popular, since everything could be done digitally, scanning the original 4-perf 1.33:1 (or 3-perf 1.78:1) picture and cropping it to the 2.39:1 frame already in-computer, without anamorphosing stages, and also without creating an additional optical generation with increased grain. This process of creating the aspect ratio in the computer allows the studios to perform all post-production and editing of the movie in its original aspect (1.33:1 or 1.78:1) and to then release the cropped version, while still having the original when necessary (for Pan & Scan, HDTV transmission, etc.).
3-Perf.
The non-anamorphic widescreen ratios (most commonly 1.85:1) used in modern feature films makes inefficient use of the available image area on 35 mm film using the standard 4-perf pulldown; the height of a 1.85:1 frame occupying only 65% of the distance between the frames. It is clear, therefore, that a change to a 3-perf pulldown would allow for a 25% reduction in film consumption whilst still accommodating the full 1.85:1 frame. Ever since the introduction of these widescreen formats in the 1950s various film directors and cinematographers have argued in favour of the industry making such a change. The Canadian cinematographer Miklos Lente invented and patented a three-perforation pull down system which he called "Trilent 35" in 1975 though he was unable to persuade the industry to adopt it.
The idea was later taken up by the Swedish film-maker Rune Ericson who was a strong advocate for the 3-perf system. Ericson shot his 51st feature "Pirates of the Lake" in 1986 using two Panaflex cameras modified to 3-perf pulldown and suggested that the industry could change over completely over the course of ten-years. However the movie industry did not make the change mainly because it would have required the modification of the thousands of existing 35 mm projectors in movie theaters all over the world. Whilst it would have been possible to shoot in 3-perf and then convert to standard 4-perf for release prints the extra complications this would cause and the additional optical printing stage required made this an unattractive option at the time for most film makers.
However, in television production, where compatibility with an installed base of 35 mm film projectors is unnecessary, the 3-perf format is sometimes used, giving—if used with Super 35—the 16:9 ratio used by HDTV and reducing film usage by 25 percent. Because of 3-perf's incompatibility with standard 4-perf equipment, it can utilize the whole negative area between the perforations (Super 35 mm film) without worrying about compatibility with existing equipment; the Super 35 image area includes what would be the soundtrack area in a standard print. All 3-perf negatives require optical or digital conversion to standard 4-perf if a film print is desired, though 3-perf can easily be transferred to video with little to no difficulty by modern telecine or film scanners. With digital intermediate now a standard process for feature film post-production, 3-perf is becoming increasingly popular for feature film productions which would otherwise be averse to an optical conversion stage.
VistaVision.
The VistaVision motion picture format was created in 1954 by Paramount Pictures to create a finer-grained negative and print for flat widescreen films. Similar to still photography, the format uses a camera running 35 mm film horizontally instead of vertically through the camera, with frames that are eight perforations long, resulting in a wider aspect ratio of 1.5:1 and greater detail, as more of the negative area is used per frame. This format is unprojectable in standard theaters and requires an optical step to reduce the image into the standard 4-perf vertical 35 mm frame.
While the format was dormant by the early 1960s, the camera system was revived for visual effects by John Dykstra at Industrial Light and Magic, starting with "Star Wars", as a way of reducing granularity in the optical printer by having increased original camera negative area at the point of image origination. Its usage has again declined since the dominance of computer-based visual effects, although it still sees limited utilization.
Perforations.
These two perforations have remained by far the most commonly used ones. BH perforations are also known as "N" (negative) and KS as "P" (positive). The Bell & Howell perf remains the standard for camera negative films because of its perforation dimensions in comparison to most printers, thus it can keep a steady image compared to other perforations.
During continuous contact printing, the raw stock and the negative are placed next to one another around the sprocket wheel of the printer. The negative, which is the closer of the two to the sprocket wheel (thus creating a slightly shorter path), must have a marginally shorter pitch between perforations (0.1866 in pitch); the raw stock has a long pitch (0.1870 in). While cellulose nitrate and cellulose diacetate stocks used to shrink during processing slightly enough to have this difference naturally occur, modern safety stocks do not shrink at the same rate, and therefore negative (and some intermediate) stocks are perforated at a pitch of 0.2% shorter than print stock.
Innovations in sound.
Three different digital soundtrack systems for 35 mm cinema release prints were introduced during the 1990s. They are: Dolby Digital, which is stored between the perforations on the sound side; SDDS, stored in two redundant strips along the outside edges (beyond the perforations); and DTS, in which sound data is stored on separate compact discs synchronized by a timecode track on the film just to the right of the analog soundtrack and left of the frame. Because these soundtrack systems appear on different parts of the film, one movie can contain all of them, allowing broad distribution without regard for the sound system installed at individual theatres.
The analogue optical track technology has also changed: in the early years of the 21st century distributors changed to using cyan dye optical soundtracks instead of applicated tracks, which use environmentally unfriendly chemicals to retain a silver (black-and-white) soundtrack. Because traditional incandescent exciter lamps produce copious amounts of infra-red light, and cyan tracks do not absorb infra-red light, this change has required theaters to replace the incandescent exciter lamp with a complementary colored red LED or laser. These LED or laser exciters are backwards-compatible with older tracks. The film "Anything Else" (2003) was the first to be released with only cyan tracks.
To facilitate this changeover, intermediate prints known as "high magenta" prints were distributed. These prints used a silver plus dye soundtrack that were printed into the magenta dye layer. The advantage gained was an optical soundtrack, with low levels of sibilant (cross-modulation) distortion, on both types of sound heads.
3D systems for theatrical 35 mm presentation.
The success of digitally projected 3D movies in the first two decades of 21st century led to a demand from some theater owners to be able to show these movies in 3D without incurring the high capital cost of installing digital projection equipment. To satisfy that demand, a number of systems had been proposed for 3D systems based on 35 mm film by Technicolor, Panavision and others. These systems are improved version of the "over-under" stereo 3D prints first introduced in the 1960s.
To be attractive to exhibitors, these schemes offered 3D films that can be projected by a standard 35 mm cinema projector with minimal modification, and so they are based on the use of "over-under" film prints. In these prints a left-right pair of 2.39:1 non-anamorphic images are substituted for the one 2.39:1 anamorphic image of a 2D "scope" print. The frame dimensions are based on those of the Techniscope 2-perf camera format used in the 1960s and '70s. However, when used for 3D the left and right frames are pulled down together, thus the standard 4-perf pulldown is retained, minimising the need for modifications to the projector or to long-play systems. The linear speed of film through the projector and sound playback both remain exactly the same as in normal 2D operation.
The Technicolor system uses the polarisation of light to separate the left and right eye images and for this they rent to exhibitors a combination splitter-polarizer-lens assembly which can be fitted to a lens turret in the same manner as an anamorphic lens. In contrast, the Panavision system uses a spectral comb filter system, but their combination splitter-filter-lens is physically similar to the Technicolor assembly and can be used in the same way. No other modifications are required to the projector for either system, though for the Technicolor system a silver screen is necessary, as it would be with polarised-light digital 3D. Thus a programme can readily include both 2D and 3D segments with only the lens needing to be changed between them.
In June 2012 Panavision 3D systems for both 35 mm film and digital projection were withdrawn from the market by DVPO theatrical (who marketed these system on behalf of Panavision) citing "challenging global economic and 3D market conditions".
Decline.
In transition period centered around 2005-2015, the rapid conversion of the cinema exhibition industry to digital projection has seen 35 mm film projectors removed from most of the projection rooms as they are replaced by digital projectors. By mid 2010's, most of the theaters across the world have been converted to digital projection. Film though remains in a niche market of enthusiasts and format lovers.
Technical specifications.
Technical specifications for 35 mm film are standardized by SMPTE.
35 mm spherical
Super 35 mm film
35 mm anamorphic

</doc>
<doc id="46204" url="https://en.wikipedia.org/wiki?curid=46204" title="Prince of Wales">
Prince of Wales

Prince of Wales () was a title granted to princes born in Wales from the 12th century onwards; the term replaced the use of the word "king". One of the last Welsh princes, Llywelyn ap Gruffudd, was killed in battle in 1282 by Edward I, king of England, whose son Edward, born in Caernarfon Castle, was invested as Prince of Wales: the first English person to claim the title. 
Since the 13th century, the title is granted to the heir apparent to the English or British monarch, but the failure to be granted the title does not affect the rights to royal succession. The title is granted to the royal heir apparent as a personal honour or dignity, and the title is not heritable, merging with the Crown on accession to the throne. The title Earl of Chester is always given in conjunction with that of Prince of Wales. The Prince of Wales usually has other titles and honours. 
The current Prince of Wales is Prince Charles, the eldest son of Elizabeth II, who is Queen of the United Kingdom and 15 other independent Commonwealth realms as well as Head of the 53-member Commonwealth of Nations. The wife of the Prince of Wales is entitled to the title Princess of Wales. Prince Charles' first wife, Diana, used that title but his second wife, Camilla, uses only the title Duchess of Cornwall because the other title has become so popularly associated with Diana.
Roles and responsibilities.
The Prince of Wales is the heir apparent of the Queen of the United Kingdom. No formal public role or responsibility has been legislated by Parliament or otherwise delegated to him by law or custom, either as heir apparent or as Prince of Wales. 
The current Prince now often assists the Queen in the performance of her duties, for example representing the Queen when welcoming dignitaries to London and attending State dinners during State visits. He has also represented the Queen and the United Kingdom overseas at state and ceremonial occasions such as state funerals.
History.
Welsh usage.
For most of the post-Roman period, Wales was divided into several smaller states. Before the Norman conquest of England, the most powerful Welsh ruler at any given time was generally known as King of the Britons. In the 12th and 13th centuries, this title evolved into "Prince of Wales" (see "Brut y Tywysogion"). In Latin, the new title was ', and in Welsh it was '. The literal translation of ' is "leader". (The verb ' means "to lead".)
Only a handful of native princes had their claim to the overlordship of Wales recognised by the English Crown. The first known to have used such a title was Owain Gwynedd, adopting the title Prince of the Welsh around 1165 after earlier using "rex Waliae" ("King of Wales"). His grandson Llywelyn the Great is not known to have used the title "Prince of Wales" as such, although his use, from around 1230, of the style "Prince of Aberffraw, Lord of Snowdon" was tantamount to a proclamation of authority over most of Wales, and he did use the title "Prince of North Wales" as did his predecessor Dafydd ab Owain Gwynedd.
In 1240, the title was theoretically inherited by his son Dafydd ap Llywelyn, though he is not known to have used it. Instead he styled himself as "Prince of Wales" around 1244, the first Welsh prince to do so. In 1246, his nephew Llywelyn ap Gruffudd succeeded to the throne of Gwynedd, and used the style as early as 1258. In 1267, with the signing of the Treaty of Montgomery, he was recognised by both King Henry III of England and the representative of the Papacy as Prince of Wales. In 1282, Llywelyn was killed during Edward I of England's invasion of Wales and although his brother Dafydd ap Gruffudd succeeded to the Welsh princeship, issuing documents as prince, his principality was not recognised by the English Crown.
Three Welshmen, however, claimed the title of Prince of Wales after 1283.
The first was Madog ap Llywelyn, a member of the house of Gwynedd, who led a nationwide revolt in 1294-5, defeating English forces in battle near Denbigh and seizing Caernarfon Castle. His revolt was suppressed, however, after the Battle of Maes Moydog in March 1295, and the prince was imprisoned in London.
In the 1370s, Owain Lawgoch, an English-born descendant of one of Llywelyn ap Gruffudd's brothers, claimed the title of Prince of Wales, but was assassinated in France in 1378 before he could return to Wales to claim his inheritance.
It is Owain Glyndŵr, however, whom many Welsh people regard as being the last native Prince. On 16 September 1400, he was proclaimed Prince of Wales by his supporters, and held parliaments at Harlech Castle and elsewhere during his revolt, which encompassed all of Wales. It was not until 1409 that his revolt in quest of Welsh independence was suppressed by Henry IV.
As title of heir apparent.
The tradition of investing the heir-apparent of the monarch with the title of "Prince of Wales" is usually considered to have begun in 1301, when King Edward I of England invested his son Edward of Caernarfon with the title at a Parliament held in Lincoln. According to legend, the king had promised the Welsh that he would name "a prince born in Wales, who did not speak a word of English" and then produced his infant son, who had been born at Caernarfon, to their surprise. However, the story may well be apocryphal, as it can only be traced to the 16th century, and, in the time of Edward I, the English aristocracy spoke Norman French, not English (some versions of the legend include lack of knowledge in "both" languages as a requirement, and one reported version has the very specific phrase "born on Welsh soil and speaking no other language").
William Camden wrote in his 1607 work "Britannia" that originally the title "Prince of Wales" was not conferred automatically upon the eldest living son of the King of England because Edward II (who had been the first English Prince of Wales) neglected to invest his eldest son, the future Edward III, with that title. It was Edward III who revived the practice of naming the eldest son Prince of Wales, which was then maintained by his successors:
"But King Edward the Second conferred not upon his sonne Edward the title of Prince of Wales, but onely the name of Earle of Chester and of Flint, so farre as ever I could learne out of the Records, and by that title summoned him to Parliament, being then nine yeres old. King Edward the Third first created his eldest sonne Edward surnamed the Blacke Prince, the Mirour of Chivalrie (being then Duke of Cornwall and Earle of Chester), Prince of Wales by solemne investure, with a cap of estate and Coronet set on his head, a gold ring put upon his finger, and a silver vierge delivered into his hand, with the assent of Parliament." 
Nevertheless, according to conventional wisdom since 1301 the Prince of Wales has usually been the eldest living son (if and only if he is also the heir-apparent) of the King or Queen Regnant of England (subsequently of Great Britain, 1707, and of the United Kingdom, 1801). That he is also the heir-apparent is important. Following the death of Prince Arthur, the Prince of Wales, Henry VII invested his second son, the future Henry VIII, with the title—although only after it was clear that Arthur's wife, Catherine of Aragon, was not pregnant; when Frederick, Prince of Wales died while his father reigned, George II created Frederick's son (the king's grandson and new heir-apparent) George Prince of Wales. The title is not automatic and is not heritable; it merges into the Crown when a prince accedes to the throne, or lapses on his death leaving the sovereign free to re-grant it to the new heir-apparent (such as the late prince's son or brother). Prince Charles was created Prince of Wales on 26 July 1958, some six years after he became heir-apparent, and had to wait another eleven years for his investiture, on 1 July 1969.
The title Prince of Wales is nowadays always conferred along with the Earldom of Chester. The convention began in 1399; all previous Princes of Wales also received the earldom, but separately from the title of Prince. Indeed, before 1272 a hereditary and not necessarily royal Earldom of Chester had already been created several times, eventually merging in the Crown each time. The earldom was recreated, merging in the Crown in 1307 and again in 1327. Its creations since have been associated with the creations of the Prince of Wales.
On 31 October 1460, Richard of York was briefly created Prince of Wales and Earl of Chester, Duke of Cornwall and Lord Protector of England by an Act of Parliament following the Act of Accord, as part of his arrangement to succeed Henry VI as king instead of Henry's own son. However Richard was killed in battle soon afterwards.
Heraldic insignia and investiture.
Insignia.
As heir apparent to the reigning sovereign, the Prince of Wales bears the Royal Arms differenced by a white label of three points. To represent Wales he bears the Coat of Arms of the Principality of Wales, crowned with the heir-apparent's crown, on an inescutcheon-en-surtout. This was first used by the future King Edward VIII in 1910, and followed by the current Prince of Wales, Prince Charles.
He has a badge of three ostrich feathers (which can be seen on the reverse of the previous design for decimal British two pence coins dated up to 2008); it dates back to the Black Prince and is his as the English heir even before he is made Prince of Wales.
In addition to these symbols used most frequently, he has a special standard for use in Wales itself. Moreover, as Duke of Rothesay he has a special coat of arms for use in Scotland (and a corresponding standard); as Duke of Cornwall the like for use in the Duchy of Cornwall. Representations of all three may be found at List of British flags.
"For theories about the origin of the ostrich feather badge and of the motto """" (German for "I serve"), see Prince of Wales's feathers".
Investiture.
Princes of Wales may be invested, but investiture is not necessary to be created Prince of Wales. Peers were also invested, but investitures for peers ceased in 1621, during a time when peerages were being created so frequently that the investiture ceremony became cumbersome. Most investitures for Princes of Wales were held in front of Parliament, but in 1911, the future Edward VIII was invested in Caernarfon Castle in Wales. The present Prince of Wales was also invested there, in 1969. During the reading of the letters patent creating the Prince, the Honours of the Principality of Wales are delivered to the Prince. The coronet of the heir-apparent bears four-crosses pattée alternating with four fleurs-de-lis, surmounted by a single arch (the Sovereign's crowns are of the same design, but use two arches). A gold rod is also used in the insignia; gold rods were formally used in the investitures of dukes, but survive now in the investitures of Princes of Wales only. Also part of the insignia are a ring, a sword and a robe.
Other titles.
Since 1301 the title Earl of Chester has generally been granted to heirs apparent to the English throne, and from the late 14th century it has been given only in conjunction with that of Prince of Wales. Both titles must be created for each individual and are not automatically acquired. The Earldom of Chester was one of the most powerful earldoms in medieval England extending principally over the counties of Cheshire and Flintshire.
A Prince of Wales also holds a number of additional titles. As heir apparent to the English/British throne he is—if the eldest living son of the monarch—Duke of Cornwall. As heir apparent to the Scottish throne he is Duke of Rothesay, Earl of Carrick, Baron of Renfrew, Lord of the Isles, and Prince and Great Steward of Scotland.
Individual Princes have also held additional titles, which were theirs prior to becoming Prince of Wales. Henry of Bolingbroke (later Henry IV) was Duke of Hereford and Duke of Lancaster. Prince Henry (later Henry VIII), Prince Charles (later Charles I) and Prince George (later George V) were each Duke of York. Prior to his father inheriting the English throne in 1603, the future Charles I was created Duke of Albany and Earl of Ross in Scotland. Both Prince Frederick (eldest son of George II) and his son Prince George (later George III) were Duke of Edinburgh.
Heir apparent versus heir presumptive.
The title Prince of Wales is given only to the heir apparent—somebody who cannot be displaced in the succession to the throne by any future birth. The succession had followed male-preference primogeniture, which meant that the heir apparent was the eldest son of the reigning monarch or, if he was deceased, "his" eldest son and so on, or if the monarch's eldest son had died without issue, the monarch's second eldest son, etc. As such, a daughter of the sovereign who was next in line to the throne was never the heir apparent because she would be displaced in the succession by any future legitimate son of the sovereign.
Along with the other Commonwealth realms, the United Kingdom in 2011 committed to the Perth Agreement, which proposed changes to the laws governing succession, including altering the primogeniture to absolute cognatic. The Succession to the Crown Act 2013 was introduced to the British parliament on 12 December 2012, published the next day, and received Royal Assent on 25 April 2013. It was brought into force on 26 March 2015, at the same time as the other realms implemented the Perth Agreement in their own laws. No woman has yet held the title "Princess of Wales" in her own right.
Since the title of "Prince of Wales" is not automatic, there have been times when it was held by no one. There was no heir apparent during the reign of King George VI, who had no sons. Princess Elizabeth was heiress presumptive and was hence not eligible to be titled Princess of Wales. After it became unlikely that George VI would father more children, the option of bestowing the title of "Princess of Wales" was considered, but ultimately rejected, due in large part to a lack of enthusiasm for the idea from Elizabeth herself. There was also no Prince of Wales for the first several years of the reign of Elizabeth II. Prince Charles was not named Prince of Wales until 1958, when he was nine years old.
The title of "Princess of Wales" has always been held by the Prince's wife in her capacity as spouse of the heir apparent and therefore future queen consort. The current Princess of Wales is Camilla, Duchess of Cornwall, who automatically assumed the title upon her legal marriage to Prince Charles. Camilla however has chosen not to be publicly known by the title due to its association with her predecessor, Diana.
List of Princes of Wales.
Prince of Wales as title of English or British heir apparent.
The oldest Prince of Wales (as the English and British heir apparent) at the start of his tenure was George Frederick Ernest Albert, later George V, who was 36 years, 5 months and 6 days old when he assumed the title. HRH The Duke of Cambridge will surpass this record if he is created Prince of Wales any time after 16 November 2018 (two days after his father's 70th birthday).
The longest-serving Prince of Wales was Albert Edward, later Edward VII, who served for 59 years, 1 month and 14 days. Charles Philip Arthur George, the longest-serving heir apparent and current Prince of Wales, will surpass this record if he remains the Prince of Wales until 10 September 2017.

</doc>
<doc id="46205" url="https://en.wikipedia.org/wiki?curid=46205" title="Whitefish (fisheries term)">
Whitefish (fisheries term)

Whitefish or white fish is a fisheries term referring to several species of demersal fish with fins, particularly cod ("Gadus morhua"), Caspian kutum ("Rutilus kutum"), whiting ("Merluccius bilinearis"), and haddock ("Melanogrammus aeglefinus"), but also hake ("Urophycis"), pollock ("Pollachius"), or others. Whitefish (Coregonidae) is also the name of several species of Atlantic freshwater fish, so the use of the two-word term 'white fish' is less misleading.
White fish live on or near the seafloor, and can be contrasted with the oily or pelagic fish which live in the water column away from the seafloor. Unlike oily fish, white fish contain oils only in their liver, rather than in their gut, and can therefore be gutted as soon as they are caught, on board the ship. White fish have dry and white flesh.
White fish can be divided into benthopelagic fish (round fish which live "near" the sea bed, such as cod and coley) and benthic fish (which live "on" the sea bed, such as flatfish like plaice).
White fish is sometimes eaten straight but is often used reconstituted for fishsticks, gefilte fish, lutefisk, surimi (imitation crabmeat), etc. For centuries it was preserved by drying as stockfish and clipfish and traded as a world commodity. It is commonly used as the fish in the classic British dish of fish and chips.
Nutritional information.
1 fillet of whitefish, mixed species (198g) contains the following nutritional information according to the USDA:

</doc>
<doc id="46206" url="https://en.wikipedia.org/wiki?curid=46206" title="Clairvaux Abbey">
Clairvaux Abbey

Clairvaux Abbey (Latin: "Clara Vallis") is a Cistercian monastery in Ville-sous-la-Ferté, 15 km from Bar-sur-Aube, in the Aube department in northeastern France. The original building, founded in 1115 by St. Bernard, is now in ruins. Clairvaux Abbey was a good example of the general layout of a Cistercian monastery. The Abbey has been listed since 1926 as a historical monument by the French Ministry of Culture.
The grounds are now occupied and used by Clairvaux Prison, a high-security prison.
Description.
An additional wall, running from north to south, bisected the monastery into an "inner" and "outer" ward. The inner ward housed the monastic buildings, while the agricultural and other artisan endeavors were carried out in the outer ward.
The precincts were entered by a gateway at the extreme western extremity, giving admission to the lower ward. Here the barns, granaries, stables, shambles, workshops, and workmen's lodgings were located, Convenience was the only consideration for design. A single gatehouse afforded communication through the wall separating the outer from the inner ward.
On passing through the gateway, monks and visitors entered the outer court of the inner ward, to face the western facade of the monastic church. Immediately to the right of entrance was the abbot's residence, in close proximity to the guest-house. On the other side of the court were stables for the accommodation of the horses of the guests and their attendants. The church occupied a central position, with the great cloister to the south, surrounded by the chief monastic buildings. Further to the east, the smaller cloister contained the infirmary, novices' lodgings, and quarters for the aged monks. Beyond the smaller cloister, and separated from the monastic buildings by a wall, lay the vegetable gardens and orchards. Large fish ponds were also located in the area east of the monastic buildings. The ponds were an important feature of monastic life, and much care was given by the monks to their construction and maintenance. They often remain as one of the few visible traces of these vast monasteries.
The church consists of a vast nave of eleven bays, entered by a narthex, with a transept and short apsidal choir. To the east of each limb of the transept are two square chapels, divided according to Cistercian rule by solid walls. Nine radiating chapels, similarly divided, surround the apse. The stalls of the monks occupy the four eastern bays of the nave, forming the ritual choir. There was a second range of stalls in the extreme western bays of the nave for the lay brothers. The cloister was located to the south of the church so that its inhabitants could benefit from ample sunshine.
The chapter house opened out of the east walk of the cloister in parallel with the south transept.

</doc>
<doc id="46208" url="https://en.wikipedia.org/wiki?curid=46208" title="Dudley Moore">
Dudley Moore

Dudley Stuart John Moore, CBE (19 April 193527 March 2002) was an English actor, comedian, musician and composer.
Moore first came to prominence in the UK as one of the four writer-performers in the comedy revue "Beyond the Fringe" from 1960, and with one member of that team, Peter Cook, collaborated on the television series "Not Only... But Also". The double act worked on other projects until the mid-1970s, by which time Moore had settled in Los Angeles to concentrate on his film acting.
His solo career as a comedy film actor was heightened by the success of hit Hollywood films, particularly "Foul Play", "10" and "Arthur". He received an Oscar nomination for the latter role.
Early life.
Moore was born at the original Charing Cross Hospital in central London, the son of Ada Francis (née Hughes), a secretary, and John Moore, a railway electrician. His father was Scottish, from Glasgow. Moore was brought up in Dagenham, Essex. He was notably short at and was born with club feet that required extensive hospital treatment and, coupled with his diminutive stature, made him the butt of jokes from other children. His right foot responded well to corrective treatment and had straightened itself by the time he was six, but his left foot became permanently twisted and consequently his left leg below the knee was withered. This was something he remained very self-conscious of throughout his life.
Moore became a choirboy at the age of six. At age eleven he earned a scholarship to the Guildhall School of Music, where he took up harpsichord, organ, violin, musical theory and composition. He rapidly developed into a highly talented pianist and organist and was playing the pipe organ at local church weddings by the age of 14. He attended Dagenham County High School where he received musical tuition from a dedicated teacher, Peter Cork, who became a friend and confidant to Moore and continued to correspond with him until 1994.
Moore's musical talent won him an organ scholarship to Magdalen College, Oxford. While studying music and composition there, he also performed with Alan Bennett in the Oxford Revue. During his university years, Moore had developed a love of jazz music and soon became an accomplished jazz pianist and composer. He began working with such leading musicians as John Dankworth and Cleo Laine.
In 1960, he left Dankworth's band to work on "Beyond the Fringe".
Career.
"Beyond the Fringe".
John Bassett, a graduate of Wadham College, Oxford recommended Moore, his jazz bandmate and a rising cabaret talent, to producer Robert Ponsonby, who was putting together a comedy revue entitled "Beyond the Fringe". Bassett also chose Jonathan Miller. Moore then recommended Bennett, who in turn suggested Peter Cook.
"Beyond the Fringe" was at the forefront of the 1960s UK satire boom, although the show's original runs in Edinburgh and the provinces in 1960 had had a lukewarm response. When the revue transferred to the Fortune Theatre in London, in a revised production by Donald Albery and William Donaldson, it became a sensation, thanks in some part, to a favourable review by Kenneth Tynan. There were also a number of musical items in the show, using Dudley Moore's music, most famously an arrangement of the Colonel Bogey March which resists Moore's repeated attempts to bring it to an end.
In 1962, the show transferred to the John Golden Theatre in New York, with its original cast. President John F. Kennedy attended a performance on 10 February 1963. The show continued in New York until 1964.
Partnership with Peter Cook.
When Moore returned to the UK he was offered his own series on the BBC, "Not Only... But Also" (1965, 1966, 1970). It was commissioned specifically as a vehicle for Moore, but when he invited Peter Cook on as a guest, their comedy partnership was so notable that it became a permanent fixture of the series. Cook and Moore are most remembered for their sketches as two working class men, Pete and Dud, in macs and cloth caps, commenting on politics and the arts, but they fashioned a series of one-off characters, usually with Moore in the role of interviewer to one of Cook's upper class eccentrics.
The pair developed an unorthodox method for scripting the material, using a tape recorder to tape an ad-libbed routine that they would then have transcribed and edited. This would not leave enough time to fully rehearse the script, so they often had a set of cue cards. Moore was famous for 'corpsing' — the programmes often went on live, and Cook would deliberately make him laugh in order to get an even bigger reaction from the studio audience. The BBC wiped much of the series, though some of the soundtracks (which were issued on record) have survived.
In 1968, Cook and Moore briefly switched to ATV for four one-hour programmes entitled "Goodbye Again", however, they were not as critically well-received as the BBC shows.
On film, Moore and Cook appeared in the 1966 British comedy film, "The Wrong Box", before co-writing and co-starring in "Bedazzled" (1967) with Eleanor Bron and directed by Stanley Donen. The pair closed the decade with appearances in the ensemble caper film, "Monte Carlo or Bust" and Richard Lester's "The Bed-Sitting Room", based on the play by Spike Milligan and John Antrobus.
In 1968 and 1969, Moore embarked on two solo comedy ventures, firstly in the film "30 Is a Dangerous Age, Cynthia" and secondly, on stage, for an Anglicised adaptation of Woody Allen's "Play It Again, Sam" at the Globe Theatre in London's West End.
In the 1970s, the relationship between Moore and Cook became increasingly strained as the latter's alcoholism began affecting his work. However, in 1971, Cook and Moore took sketches from "Not Only...But Also" and "Goodbye Again", together with new material, to create the stage revue, "Behind the Fridge". This show toured Australia in 1972 before transferring to New York City in 1973, re-titled as "Good Evening". Cook frequently appeared on and off stage the worse for drink. Nonetheless, the show proved very popular and it won Tony and Grammy Awards.
When the Broadway run of "Good Evening" ended, Moore stayed on in the U.S. to pursue his film acting ambitions in Hollywood, but the pair reunited to host "Saturday Night Live" on 24 January 1976 during the SNL first season. They performed a number of their classic stage routines, including "One Leg Too Few" and "Frog and Peach" among others, in addition to participating in some skits with the show's ensemble.
It was during the Broadway run of "Good Evening" that Cook persuaded Moore to take the humour of Pete and Dud farther on long-playing records as Derek and Clive. Chris Blackwell circulated bootleg copies to friends in the music business and the popularity of the recording convinced Cook to release it commercially as "Derek and Clive (Live)" (1976). Two further "Derek and Clive" albums, "Derek and Clive Come Again" (1977), and "Derek and Clive Ad Nauseam" (1978), were later released. The latter was also filmed for a documentary, "Derek and Clive Get the Horn". In the film it is clear tensions between the two men were at breaking point, with Moore at one point walking out of the recording room singing, 'Breaking up is so easy to do.' In 2009, it came to light that, at the time, three separate British police forces had wanted them to be prosecuted under obscenity laws for their "Derek and Clive" comedy recordings.
The last significant appearance for the partnership was in 1978's "The Hound of the Baskervilles", where Moore played Dr. Watson to Cook's Sherlock Holmes, as well as three other roles: in drag, as a one-legged man and appearing at the start and end of the film as a flamboyant and mischievous pianist. He also wrote the film's score. Co-star Terry-Thomas described it as, "the most outrageous film I ever appeared in ... there was no magic ... it was bad!". The film was not a success critically or financially.
Moore and Cook eventually reunited for the annual American benefit for the homeless, "Comic Relief" in 1987 and again in 1989 for a British audience at the Amnesty International benefit, "The Secret Policeman's Biggest Ball".
Moore was deeply affected by the death of Cook in 1995, and for weeks would regularly telephone Cook's home in London, just to hear his friend's voice on the telephone answering machine. Moore attended Cook's memorial service in London and, at the time, many people who knew him noted that Moore was behaving strangely and attributed it to grief or drinking. In November 1995, Moore teamed up with friend and humorist Martin Lewis in organising a two-day salute to Cook in Los Angeles that Moore co-hosted with Lewis.
In December 2004, the Channel 4 television station in the United Kingdom broadcast "Not Only But Always", a TV film dramatising the relationship between Moore and Cook, although the principal focus of the production was on Cook. Around the same time the relationship between the two was also the subject of a stage play called "" by Chris Bartlett and Nick Awde. For this production Moore is the main subject. Set in a chat-show studio in the 1980s, it focuses on Moore's comic and personal relationship with Cook and the directions their careers took after the split of the partnership.
Music.
During the 1960s he formed the Dudley Moore Trio, with drummer Chris Karan and bassist Pete McGurk. Following McGurk's suicide in June 1968, Peter Morgan joined the group as his replacement.
Moore's admitted principal musical influences were Oscar Peterson and Erroll Garner. In an interview he recalled the day he finally mastered Garner's unique left-hand strum and was so excited that he walked around for several days with his left hand constantly playing that cadence. His early recordings included "My Blue Heaven", "Lysie Does It", "Poova Nova", "Take Your Time", "Indiana", "Sooz Blooz", "Baubles, Bangles & Beads", "Sad One for George" and "Autumn Leaves". The trio performed regularly on British television, made numerous recordings and had a long-running residency at Peter Cook's London nightclub, the Establishment. Amongst other albums, they recorded, "The Dudley Moore Trio", "Dudley Moore plays The Theme from Beyond the Fringe and All That Jazz", "The World of Dudley Moore", "The Other Side Of Dudley Moore" and "Genuine Dud".
Moore was a close friend of record producer Chris Gunning and played piano (uncredited) on the 1969 single "Broken Hearted Pirates" which Gunning produced for Simon Dupree and the Big Sound. In 1976 he played piano on Larry Norman's album "In Another Land", in particular on the song "The Sun Began to Rain". In 1981 he recorded "Smilin' Through" with Cleo Laine.
He composed the soundtracks for the films "Bedazzled" (1967), "30 Is a Dangerous Age, Cynthia" (1968), "Inadmissible Evidence" (1968), "Staircase" (1969), "The Hound of the Baskervilles" (1978) and "Six Weeks" (1982), among others.
Later career in film, television and music.
In the late 1970s, Moore moved to Hollywood, where he had a supporting role in the hit film "Foul Play" (1978) with Goldie Hawn and Chevy Chase. The following year saw his breakout role in Blake Edwards's "10", which became one of the biggest box-office hits of 1979 and gave him an unprecedented status as a romantic leading man. Moore followed up with the comedy film "Wholly Moses!", which was not a major success.
In 1981, Moore appeared in the title role of the comedy "Arthur", an even bigger hit than "10". Co-starring Liza Minnelli and Sir John Gielgud, it was both commercially and critically successful, Moore receiving an Oscar nomination for Best Actor, whilst Gielgud won the Best Supporting Actor Oscar for his role as Arthur's stern but compassionate manservant. Moore lost to Henry Fonda (for "On Golden Pond"). He did, however, win a Golden Globe award for Best Actor in a Musical/Comedy. In the same year, on British television, Moore was the featured guest subject on "An Audience With...".
His subsequent films, "Six Weeks" (1982), "Lovesick" (1983), "Romantic Comedy" (1983) and "Unfaithfully Yours" (1984) were only moderate successes, but he had another hit in 1984, starring in the Blake Edwards directed "Micki + Maude", co-starring Amy Irving. This won him another Golden Globe for Best Actor in a Musical/Comedy.
Later films, including "Best Defense" (1984), ' (1985), "Like Father Like Son" (1987), ', a sequel to the original, "Crazy People" (1990), "Blame It on the Bellboy" (1992) and an animated adaptation of "King Kong", were inconsistent in terms of both critical and commercial reception. Moore eventually disowned the "Arthur" sequel, but, in later years, Cook would wind him up by claiming he preferred "Arthur 2: On the Rocks" to "Arthur".
In 1986, he once again hosted "Saturday Night Live", albeit without Peter Cook this time.
Moore was the subject of the British "This Is Your Life"—for a second time—in March 1987 when he was surprised by Eamonn Andrews at his Venice Beach restaurant; he had previously been honoured by the programme in December 1972.
In addition to acting, Moore continued to work as a composer and pianist, writing scores for a number of films and giving piano concerts, which were highlighted by his popular parodies of classical favourites. He also appeared as Ko-Ko in Jonathan Miller's production of "The Mikado" in Los Angeles in March 1988.
In the 1990s, Moore starred as a man named David trying to catch chickens in a series of popular Tesco adverts. He stated, in a later interview, that this was the highlight of his career so far, and that he was paid "£20,000 for each advert". In 1991, he released the album "Songs Without Words" and in 1992 "Live From an Aircraft Hangar", recorded at London's Royal Albert Hall.
He collaborated with the conductor Sir Georg Solti in 1991 to create a Channel 4 television series, "Orchestra!", which was designed to introduce audiences to the symphony orchestra. He later worked with the American conductor Michael Tilson Thomas on a similar television series, "Concerto!" (1993), likewise designed to introduce audiences to classical music concertos.
Moore appeared in two series for CBS, "Dudley" (1993) and "Daddy's Girls" (1994), however both were cancelled before the end of their run.
Moore had been interviewed for "The New York Times" in 1987 by the music critic Rena Fruchter, herself an accomplished pianist, and the two became close friends. By 1995, Moore's film career was on the wane and he was having trouble remembering his lines, a problem he had never previously encountered. It was for this reason he was sacked from Barbra Streisand's film "The Mirror Has Two Faces", however, his difficulties were, in fact, due to the onset of the medical condition that eventually led to his death. Opting to concentrate on the piano, he enlisted Fruchter as an artistic partner. They performed as a duo in the US and Australia. However, his disease soon started to make itself apparent there as well, as his fingers would not always do what he wanted them to do. Further symptoms such as slurred speech and loss of balance were misinterpreted by the public and the media as a sign of drunkenness. Moore himself was at a loss to explain this. He moved into Fruchter's family home in New Jersey and stayed there for five years, but this, however, placed a great strain both on her marriage and her friendship with Moore, and she later set him up in the house next door.
Entrepreneur.
Moore co-owned, with producer Tony Bill, a fashionable restaurant in Venice, California (1980s–2000), named 72 Market Street Oyster Bar and Grill. He played the piano whenever he was there.
Personal life.
Moore was married and divorced four times: to actresses Suzy Kendall, Tuesday Weld (by whom he had a son in 1976), Brogan Lane, and Nicole Rothschild (one son, born in 1995).
He maintained good relationships with Kendall, Weld and Lane, but expressly forbade Rothschild to attend his funeral. At the time his illness became apparent he was going through a difficult divorce from Rothschild, despite sharing a house in Los Angeles with her and her previous husband.
Moore dated Susan Anton in the early 1980s, with a lot of talk being made of their height difference: Moore at 5 feet 2½ inches (1.59m) and Anton at 5 feet 11 inches (1.80m).
Health and death.
In April 1997, after spending five days in hospital in New York, Moore was informed that he had calcium deposits in the basal ganglia of his brain and irreversible frontal lobe damage.
In September 1997, Moore underwent quadruple heart bypass surgery in London. He also suffered four strokes.
In June 1998, Nicole Rothschild was reported to have told an American television show that Moore was "waiting to die" due to a serious illness, but these reports were denied by Suzy Kendall.
On 30 September 1999, Moore announced that he was suffering from the terminal degenerative brain disorder progressive supranuclear palsy, some of the early symptoms being so similar to intoxication that he had been reported as being drunk, and that the illness had been diagnosed earlier in the year.
Moore died on the morning of 27 March 2002, as a result of pneumonia, secondary to immobility caused by the palsy, in Plainfield, New Jersey. Rena Fruchter was holding his hand when he died, and she reported his final words were, "I can hear the music all around me". Moore was interred at Hillside Cemetery in Scotch Plains, New Jersey. Fruchter later wrote a memoir of their relationship ("Dudley Moore", Ebury Press, 2004).
Honours and awards.
In November 2001, Moore was appointed a Commander of the Order of The British Empire (CBE). Despite his deteriorating condition, he attended the ceremony in a wheelchair, at Buckingham Palace on 16 November to collect his honour. When asked by the press if he had ever expected to receive an honour, Moore replied "No".

</doc>
<doc id="46209" url="https://en.wikipedia.org/wiki?curid=46209" title="Louis II of Hungary">
Louis II of Hungary

Louis II (, , 1 July 1506 – 29 August 1526) was King of Hungary, Croatia and Bohemia from 1516 to 1526. He was killed during the Battle of Mohács fighting the Ottomans.
Early life.
Louis was the only son of Vladislaus II Jagiellon and his third wife, Anne of Foix-Candale. Louis's father took steps to ensure smooth succession by arranging for the boy to be crowned in his own lifetime; Louis' coronation as king of Hungary took place on 4 June 1508 in Székesfehérvár Basilica, while the coronation as king of Bohemia was held in 1509 in St. Vitus Cathedral.
King of Hungary and Croatia.
In 1515 Louis II was married to Mary of Austria, granddaughter of Emperor Maximilian I, as stipulated by the First Congress of Vienna in 1515. His sister Anne was married to Mary's brother Ferdinand, then a governor on behalf of his brother Charles V, and later Emperor Ferdinand I.
King of Bohemia.
As a king of Bohemia, Louis II became known as the "Ludovicus the Child".
The first thaler coins were minted during his reign in Bohemia, giving later the name to the dollars used in different countries. 
War with Turks.
After his father's death in 1516, the minor Louis II ascended also to the throne of Hungary and Croatia. Louis was adopted by the Holy Roman Emperor Maximilian I in 1515. When Maximilian I died in 1519, Louis was raised by his legal guardian, his cousin George, Margrave of Brandenburg-Ansbach.
Following the accession to the throne of Suleiman I, the sultan sent an ambassador to Louis II to collect the annual tribute that Hungary had been subjected to. Louis refused to pay annual tribute and had the Ottoman ambassador executed and sent the head to the Sultan. Louis believed that the Papal States and other Christian States including Charles V, Holy Roman Emperor would help him. This event hastened the fall of Hungary.
Hungary was in a state of near anarchy under the magnates' rule in 1520. The king's finances were a shambles; he borrowed to meet his household expenses despite the fact that they totaled about one-third of the national income. The country's defenses weakened as border guards went unpaid, fortresses fell into disrepair, and initiatives to increase taxes to reinforce defenses were stifled. In 1521 Sultan Suleiman the Magnificent was well aware of Hungary's weakness.
The Ottoman Empire declared war on the Kingdom of Hungary, Suleiman postponed his plan to besiege Rhodes and made an expedition to Belgrade. Louis failed to coordinate and gather his forces. At the same time, Hungary was unable to get assistance from other European states, which Louis had hoped for. Belgrade and many strategic castles in Serbia were captured by the Ottomans. This was disastrous for Louis' kingdom; without the strategically important cities of Belgrade and Šabac, Hungary, including Buda, was open to further Turkish conquests.
After the siege of Rhodes, in 1526 Suleiman made a second expedition to subdue all of Hungary. Louis made a tactical error when he tried to stop the Ottoman army in an open field battle with a medieval army, insufficient firearms, and obsolete tactics. On 29 August 1526, Louis led his forces against Suleiman the Magnificent of the Ottoman Empire in the disastrous Battle of Mohács. In a pincer movement, the Hungarian army was surrounded by Ottoman cavalry, and in the center, the Hungarian heavy knights and infantry were repulsed and suffered heavy casualties, especially from the well-positioned Ottoman cannons and well-armed and trained Janissary musketeers.
Nearly the entire Hungarian Royal army was destroyed on the battlefield. During the retreat, the twenty-year-old king died in a marsh. As Louis had no legitimate children, Ferdinand was elected as his successor in the Kingdoms of Bohemia and Hungary, but the Hungarian throne was contested by John Zápolya, who ruled the areas of the kingdom conquered by the Turks as an Ottoman client.
Jagiellons in natural line.
Although Louis II's marriage remained childless, he probably had an illegitimate child with his mother's former lady-in-waiting, Angelitha Wass, before his marriage. This son was called John (János in Hungarian). This name appears in sources in Vienna as either János Wass or János Lanthos. The former surname is his mother's maiden name. The latter surname may refer to his occupation. "Lanthos" means "lutenist", or "bard". He received incomes from the Royal Treasury regularly. He had further offspring.

</doc>
<doc id="46210" url="https://en.wikipedia.org/wiki?curid=46210" title="VistaVision">
VistaVision

VistaVision is a higher resolution, widescreen variant of the 35 mm motion picture film format which was created by engineers at Paramount Pictures in 1954.
Paramount did not use anamorphic processes such as CinemaScope but refined the quality of their flat widescreen system by orienting the 35 mm negative horizontally in the camera gate and shooting onto a larger area, which yielded a finer-grained projection print.
As finer-grained film stocks appeared on the market, VistaVision became obsolete. Paramount dropped the format after only seven years, although for another forty years the format was used by some European and Japanese producers for feature films, and by American film studios for high resolution special effects sequences.
In many ways, Vistavision was a testing ground for cinematography ideas that evolved into 70 mm IMAX and OMNIMAX film formats in the 1970s. Both IMAX and OMNIMAX are oriented sideways, like Vistavision.
History.
As a response to an industry recession brought about by the popularity of television, the Hollywood studios turned to large format movies in order to regain audience attendance. The first of these, Cinerama, debuted in September 1952, and consisted of three strips of 35 mm film projected side-by-side onto a giant, curved screen, augmented by seven channels of stereophonic sound.
Five months later, in February of the following year, Twentieth Century-Fox announced that they would soon be introducing a simpler version of Cinerama using anamorphic lenses instead of multiple film strips; a widescreen process that soon became known to the public as CinemaScope.
As a response, Paramount Pictures devised their own system the following month to augment their 3-D process known as Paravision. This process utilized a screen size that yielded an aspect ratio of 5 units wide by 3 units high, or 1.66:1. By using a different sized aperture plate and wider lens, a normal Academy ratio film could be soft matted to this or any other aspect ratio. Shortly thereafter, it was announced that all of their productions would be shot in this ratio.
This "flat" widescreen process was adopted by other studios and by the end of 1953, more than half of the theatres in America had installed wide screens. However, there were drawbacks: because a smaller portion of the image was being used and magnification was increased, excessive grain and soft images plagued early widescreen presentations. Some studios sought to compensate for this by shooting their color pictures with a full aperture gate (rather than the Academy aperture), and then reducing the image in Technicolor's optical printer. This process is a predecessor of today's Super 35 format which also uses a 1.85:1 ratio, but uses 1/3rd more frame area than a standard 1.85:1 matted into a 4:3.
Paramount took this concept a step further, using old Stein cameras from the 1930s which used a two-frame color format that was itself adopted from a 1902 three-frame color film process developed by Edward Raymond Turner. For the aborted early 1930s color process, instead of an image four perforations high, the camera exposed eight perforations (essentially two frames) consisting of one 4-perf image through a red filter and one 4-perf image through a green filter.
In shooting VistaVision, the film was run horizontally rather than vertically, and instead of exposing two simultaneous 4-perf frames, the entire eight perforations were used for one image.
Because of its peculiar horizontal orientation on the negative, VistaVision was sometimes called "Lazy 8" by film professionals. This gave a wider aspect ratio of 1.5:1 versus the conventional 1.37:1 Academy ratio, and a much larger image area. In order to satisfy all theaters with all screen sizes, VistaVision films were shot in such a way that they could be shown in one of three recommended aspect ratios: 1.66:1, 1.85:1 and 2.00:1.
The negative was "scribed" with a new form of cue mark, made at the start of each 2000-foot (610 m) reel. Similar in shape to an "F", the cue mark contained staffs that directed the projectionist to the top of the frame for the three recommended aspect ratios. The projectionist racked his framing so that the staff touched the top of his screen (at the appropriate ratio) and the framing was set for the rest of the reel. On many home video releases these cue marks have been digitally erased.
While most competing widescreen film systems used magnetic audio and true stereophonic sound, early VistaVision carried only Perspecta Stereo, encoded in the optical track.
Loren L. Ryder, chief engineer at Paramount, expressed four general reasons he thought Paramount's VistaVision would be the forerunner of widescreen projection in most theaters:
After months of trade screenings, Paramount introduced VistaVision to the public at Radio City Music Hall on October 14, 1954, with their first film shot in the process, "White Christmas".
"White Christmas", "Strategic Air Command", "To Catch a Thief", "Richard III", and "The Battle of the River Plate" (a.k.a. "Pursuit of the Graf Spee") had very limited (two or three) prints struck in the 8-perf VistaVision format in which they were shot. Although the clarity of these 8-perf prints was striking, they were used only for premiere or preview engagements between 1954 and 1956 and required special projection equipment. This exhibition process was impractical because for the footage to travel through a projector at the normal 24 frames per second, the film had to roll at 3 feet per second, double the speed of 35 mm film and causing many technical and mechanical problems. Aside from these prints all other VistaVision films were shown in the conventional 4-perf format, as planned.
Alfred Hitchcock used VistaVision for many of his films in the 1950s. However, by the late 1950s with the introduction of finer-grained color stocks and the disadvantage of shooting twice as much negative stock, VistaVision became obsolete. Less expensive anamorphic systems such as Ultra Panavision and the more expensive 70 mm format became standard during the later 1950s and 1960s.
Since the last American VistaVision picture, "One-Eyed Jacks" in 1961, the format has not been used as a primary imaging system for American feature films. However, VistaVision's high resolution made it attractive for some special effects work within some later feature films. Many VistaVision cameras were sold off internationally beginning in the early 1960s, which led to a significant number of VistaVision-format productions (which did not use the trade name) in countries such as Italy and Japan from the 1960s to 1980s. The format was used infrequently for lesser-known Japanese films until at least 2000.
Special effects usage.
In 1975 a small group of artists and technicians (including Richard Edlund who was to receive two Academy Awards for his work), revived the long dormant format to create the special effects shots for George Lucas's space epic "Star Wars". A retooled VistaVision camera dubbed the Dykstraflex was used by the group (later called Industrial Light & Magic) in complex process shots. For more than two decades after this VistaVision was often used as an originating and intermediate format for shooting special effects since a larger negative area compensates against the increased grain created when shots are optically composited. By the early 21st century computer-generated imagery, advanced film scanning, digital intermediate methods and film stocks with higher resolutions optimized for special effects work had together rendered VistaVision mostly obsolete even for special effects work. Nevertheless, in 2008 ILM was still using the format in some production steps, such as for "Indiana Jones and the Kingdom of the Crystal Skull" and a VistaVision camera was used in the semi-trailer flip scene in "The Dark Knight" when there were not enough IMAX cameras to cover all the angles needed for the shot. More recently, certain key sequences of the film "Inception" were shot in VistaVision, and in the film "Scott Pilgrim vs. the World", shots that needed to be optically enlarged were shot in VistaVision.
Legacy.
The camera numbered VistaVision #1, used on Cecil B. DeMille's "The Ten Commandments", films by Alfred Hitchcock, and others, was offered at auction on September 30, 2015 by Profiles in History with an estimated value of US$30,000 to $50,000, with a winning bid of US$65,000.
Also offered at the same auction was VistaVision High Speed #1 (VVHS1), which was used to film the parting of the Red Sea in "The Ten Commandments" (1956) and special effects on Star Wars (winning bid US$60,000.)

</doc>
<doc id="46211" url="https://en.wikipedia.org/wiki?curid=46211" title="Tuna">
Tuna

A tuna is a saltwater finfish that belongs to the tribe Thunnini, a sub-grouping of the mackerel family (Scombridae) – which together with the tunas, also includes the bonitos, mackerels, and Spanish mackerels. Thunnini comprises fifteen species across five genera, the sizes of which vary greatly, ranging from the bullet tuna (max. length: , weight: ) up to the Atlantic bluefin tuna (max. length: , weight: ). The bluefin averages , and is believed to live for up to 50 years.
Tuna and mackerel sharks are the only species of fish that can maintain a body temperature higher than that of the surrounding water. An active and agile predator, the tuna has a sleek, streamlined body, and is among the fastest-swimming pelagic fish – the yellowfin tuna, for example, is capable of speeds of up to . Found in warm seas, it is extensively fished commercially, and is popular as a game fish. As a result of over-fishing, stocks of some tuna species such as the southern bluefin tuna have been reduced dangerously close to the point of extinction.
Etymology.
The term "tuna" ultimately derives from "", the Middle Latin form of the  – which is in turn derived from , "rush, dart along".
However, the immediate source for the word tuna in English is American Spanish < Spanish "atún" < Andalusian Arabic "at-tūn", assimilated from "al-tūn" التون Arabic التن : 'tuna fish' < Greco-Latin "thunnus" mentioned above.
Taxonomy.
The Thunnini tribe is a monophyletic clade comprising fifteen species in five genera:
The cladogram is a tool for visualizing and comparing the evolutionary relationships between taxons, and is read left-to-right as if on a timeline. The following cladogram illustrates the relationship between the tunas and other tribes of the family Scombridae. For example, the cladogram illustrates that the skipjack tunas are more closely related to the true tunas than are the slender tunas (the most primitive of the tunas), and that the next nearest relatives of the tunas are the bonitos of the Sardini tribe.
True tuna species.
The "true" tunas are those that belong to the genus "Thunnus". Until recently, it was thought that there were seven "Thunnus" species, and that Atlantic bluefin tuna and Pacific bluefin tuna were subspecies of a single species. In 1999, Collette established that based on both molecular and morphological considerations, they are in fact distinct species.
The genus "Thunnus" is further classified into two subgenera: "Thunnus (Thunnus)" (the bluefin group), and "Thunnus (Neothunnus)" (the yellowfin group).
Other tuna species.
The Thunnini tribe also includes seven additional species of tuna across four genera. They are:
Biology.
Description.
The tuna is a sleek and streamlined fish, adapted for speed. It has two closely spaced dorsal fins on its back; The first is "depressible" – it can be laid down, flush, in a groove that runs along its back. Seven to 10 yellow finlets run from the dorsal fins to the tail, which is lunate – curved like a crescent moon – and tapered to pointy tips. The caudal peduncle, to which the tail is attached, is quite thin, with three stabilizing horizontal keels on each side. The tuna's dorsal side is generally a metallic dark blue, while the ventral side, or underside, is silvery or whitish, for camouflage.
Physiology.
"Thunnus" are widely but sparsely distributed throughout the oceans of the world, generally in tropical and temperate waters at latitudes ranging between about 45° north and south of the equator. All tunas are able to maintain the temperature of certain parts of their body above the temperature of ambient seawater. For example, bluefin can maintain a core body temperature of , in water as cold as . However, unlike "typical" endothermic creatures such as mammals and birds, tuna do not maintain temperature within a relatively narrow range.
Tunas achieve endothermy by conserving the heat generated through normal metabolism. In all tunas, the heart operates at ambient temperature, as it receives cooled blood, and coronary circulation is directly from the gills. The "rete mirabile" ("wonderful net"), the intertwining of veins and arteries in the body's periphery, allows nearly all of the metabolic heat from venous blood to be "re-claimed" and transferred to the arterial blood via a counter-current exchange system, thus mitigating the effects of surface cooling. This allows the tuna to elevate the temperatures of the highly-aerobic tissues of the skeletal muscles, eyes and brain, which supports faster swimming speeds and reduced energy expenditure, and which enables them to survive in cooler waters over a wider range of ocean environments than those of other fish.
Also unlike most fish, which have white flesh, the muscle tissue of tuna ranges from pink to dark red. The red myotomal muscles derive their color from myoglobin, an oxygen-binding molecule, which tuna express in quantities far higher than most other fish. The oxygen-rich blood further enables energy delivery to their muscles.
For powerful swimming animals like dolphins and tuna, cavitation may be detrimental, because it limits their maximum swimming speed. Even if they have the power to swim faster, dolphins may have to restrict their speed, because collapsing cavitation bubbles on their tail are too painful. Cavitation also slows tuna, but for a different reason. Unlike dolphins, these fish do not feel the bubbles, because they have bony fins without nerve endings. Nevertheless, they cannot swim faster because the cavitation bubbles create a vapor film around their fins that limits their speed. Lesions have been found on tuna that are consistent with cavitation damage.
Fishing industry.
Commercial fishing.
Tuna is an important commercial fish. The International Seafood Sustainability Foundation (ISSF) compiled a detailed scientific report on the state of global tuna stocks in 2009, which includes regular updates. According to the ISSF, the most important species for commercial and recreational tuna fisheries are yellowfin ("Thunnus albacares"), bigeye ("T. obesus"), bluefin ("T. thynnus", "T. orientalis", and "T. macoyii"), albacore ("T. alalunga"), and skipjack ("Katsuwonus pelamis").
The report further states:
The Australian government alleged in 2006 that Japan had illegally overfished southern bluefin by taking 12,000 to 20,000 tonnes per year instead of the agreed upon 6,000 tonnes; the value of such overfishing would be as much as US$2 billion. Such overfishing has severely damaged bluefin stocks. According to the WWF, "Japan's huge appetite for tuna will take the most sought-after stocks to the brink of commercial extinction unless fisheries agree on more rigid quotas". Japan's Fisheries Research Agency counters that Australian and New Zealand tuna fishing companies under-report their total catches of southern bluefin tuna and ignore internationally mandated total allowable catch totals.
In recent years, opening day fish auctions at Tokyo's Tsukiji fish market have seen record-setting prices for bluefin tuna, reflecting market demand. In each of 2010, 2011, 2012 and 2013, new record prices have been set for a single fish – the current record is 155.4 million japanese yen (US $1.76 million) for a bluefin, or a unit price of JP¥ 703,167/kg (US$3,603/lb). The opening auction price for 2014 plummeted to less than 5% of the previous year's price, which had drawn complaints for climbing "way out of line". A summary of record-setting auctions are shown in the following table (highlighted values indicate new world records):
In November 2011, a different record was set when a fisherman in Massachusetts caught an 881-pound tuna. It was captured inadvertently using a dragnet. Due to the laws and restrictions on tuna fishing in the United States, federal authorities impounded the fish because it was not caught with a rod and reel. Because of the tuna's deteriorated condition as a result of the trawl net, the fish sold for just under $5,000.
Fishing methods.
Besides for edible purposes, many species of tuna are caught frequently as a game fish, often for recreation or for contests in which money is awarded depending on how heavy the fish weighs in at. Larger specimens are notorious for putting up a fight while hooked, and have been known to injure people who try to catch them, as well as damage their equipment.
Association with whaling.
In 2005 Nauru, defending its vote from Australian criticism at that year's meeting of the International Whaling Commission, argued that some whale species have the potential to devastate Nauru's tuna stocks, and that Nauru's food security and economy relies heavily on fishing. Despite this, Nauru does not permit whaling in its own waters and does not allow other fishing vessels to take or intentionally interact with marine mammals in its Exclusive Economic Zone. In 2010 and 2011 Nauru supported Australian proposals for a western Pacific-wide ban on tuna purse-seining in the vicinity of marine mammals — a measure which was agreed by the Western and Central Pacific Fisheries Commission at its eighth meeting in March 2012.
Association with dolphins.
Dolphins swim beside several tuna species. These include yellowfin tuna in the eastern Pacific Ocean, but not albacore. Tuna schools are believed to associate themselves with dolphins for protection against sharks, which are tuna predators.
Commercial fishing vessels used to exploit this association by searching for dolphin pods. Vessels would encircle the pod with nets to catch the tuna beneath, however the nets were prone to entangling dolphins, injuring or killing them. Public outcry and new government regulations, which are now monitored by NOAA have led to more "dolphin friendly" methods, now generally involving lines rather than nets. However, there are neither universal independent inspection programs nor verification of "dolphin safeness", so these protections are not absolute. According to Consumers Union, the resulting lack of accountability means claims of tuna that is "dolphin safe" should be given little credence.
Fishery practices have changed to be dolphin friendly, which has caused greater bycatch including sharks, turtles and other oceanic fish. Fishermen no longer follow dolphins, but concentrate their fisheries around floating objects such as fish aggregation devices, also known as FADs, which attract large populations of other organisms. Measures taken thus far to satisfy the public demand to protect dolphins can be potentially damaging to other species as well.
Aquaculture.
Increasing quantities of high-grade tuna are reared in net pens and fed bait fish. In Australia, former fishermen raise southern bluefin tuna, "Thunnus maccoyii", and another bluefin species. Farming its close relative, the Atlantic bluefin tuna, "Thunnus thynnus", is beginning in the Mediterranean, North America and Japan. Hawaii approved permits for the first U.S. offshore farming of bigeye tuna in water deep in 2009.
Japan is the biggest tuna consuming nation and is also the leader in tuna farming research. Japan first successfully farm-hatched and raised bluefin tuna in 1979. In 2002, it succeeded in completing the reproduction cycle and in 2007, completed a third generation. The farm breed is known as Kindai tuna. Kindai is the contraction of Kinki University in Japanese (Kinki daigaku). In 2009, Clean Seas, an Australian company which has been receiving assistance from Kinki University managed to breed Southern Bluefin Tuna in captivity and was awarded the second place in World's Best Invention of 2009 by "Time" magazine.
As food.
Tuna are widely regarded as a delicacy in most areas where they are shipped, being prepared in a variety of ways for the sake of achieving specific flavors or textures. When served as a steak, the meat of most species is known for its thickness and somewhat tough texture. Some species (such as the bluefin) are also known for the sheer amount of blood they expel while being gutted.
Canned.
Canned tuna was first produced in Australia in 1903, quickly becoming popular. Tuna is canned in edible oils, in brine, in water, and in various sauces. Tuna may be processed to be "chunked" or "flaked". In the United States, 52% of canned tuna is used for sandwiches; 22% for salads; and 15% for casseroles and dried and pre-packaged meal kits such as General Mills's Tuna Helper line.
In the United States, only Albacore can legally be sold in canned form as "white meat tuna"; in other countries, yellowfin is also acceptable. While in the early 1980s canned tuna in Australia was most likely Southern bluefin, it was usually yellowfin, skipjack, or tongol (labelled "northern bluefin").
As tunas are often caught far from where they are processed, poor interim conservation can lead to spoilage. Tuna is typically gutted by hand, and later pre-cooked for prescribed times of 45 minutes to three hours. The fish are then cleaned and filleted, canned, and sealed, with the dark lateral blood meat often separately canned for pet food. The sealed can is then heated under pressure (called retort cooking) for 2 to 4 hours. This process kills any bacteria, but retains the histamine that can produce rancid flavors. The international standard sets the maximum histamine level at 200 milligrams per kilogram. An Australian study of 53 varieties of unflavored canned tuna found none to exceed the safe histamine level, although some had "off" flavors.
Australian standards once required cans of tuna to contain at least 51% tuna, but these regulations were dropped in 2003. The remaining weight is usually oil or water. In the US, the FDA regulates canned tuna (see part "c").
Nutrition and health.
Tuna can be a good source of omega-3 fatty acids. It can contain per serving. However, the level of omega-3 oils found in canned tuna is highly variable, since some common manufacturing methods destroy much of the omega-3 oils in the fish. Tuna is also a good source of protein.
Mercury levels.
Mercury content in tuna can vary widely. For instance, testing by Rutgers University reportedly found that a can of StarKist had 10 times more mercury than another can of similarly identified tuna. This has prompted a Rutgers University scientist whose staff conducted the mercury analysis to say, "That's one of the reasons pregnant women have to be really careful ... If you happen to get a couple or three cans in the high range at a critical period when you are pregnant, it would not be good." Among those calling for improved warnings about mercury in tuna is the American Medical Association, which adopted a policy that physicians should help make their patients more aware of the potential risks.
A study published in 2008 found that mercury distribution in the meat of farmed tuna is inversely related to the lipid content, suggesting that higher lipid concentration within edible tissues of tuna raised in captivity might, other factors remaining equal, have a diluting effect on mercury content. These findings suggest that choosing to consume a type of tuna that has a relatively higher natural fat content might help reduce the amount of mercury intake, compared to consuming tuna with a low fat content.
Due to their high position in the food chain and the subsequent accumulation of heavy metals from their diet, mercury levels can be high in larger species such as bluefin and albacore.
In 2009 a California appeals court upheld a ruling that canned tuna does not need warning labels as the methylmercury is naturally occurring.
In March 2004, the United States FDA issued guidelines recommending that pregnant women, nursing mothers, and children limit their intake of tuna and other predatory fish. The Environmental Protection Agency provides guidelines on how much canned tuna is safe to eat. Roughly speaking, the guidelines recommend one can of light tuna per week for individuals weighing less than , and two cans per week for those who weigh more.
In 2007 it was reported that some canned light tuna such as yellowfin tuna is significantly higher in mercury than skipjack, and caused Consumers Union and other activist groups to advise pregnant women to refrain from consuming canned tuna.
The Eastern little tuna ("Euthynnus affinis") has been available for decades as a low-mercury, less expensive canned tuna. However, of the five major species of canned tuna imported by the United States it is the least commercially attractive, primarily due to its dark color and more pronounced 'fishy' flavor. Its use has traditionally been restricted to institutional (non-retail) commerce.
A January 2008 investigation conducted by the "New York Times" found potentially dangerous levels of mercury in certain varieties of sushi tuna, reporting levels "so high that the Food and Drug Administration could take legal action to remove the fish from the market."
A book by Jane Hightower, "", published in 2008, discusses human exposure to mercury through eating large predatory fish such as large tuna.
Management and conservation.
The main tuna fishery management bodies are the Western Central Pacific Ocean Fisheries Commission, the Inter-American Tropical Tuna Commission, the Indian Ocean Tuna Commission, the International Commission for the Conservation of Atlantic Tunas, and the Commission for the Conservation of Southern Bluefin Tuna. The five gathered for the first time in Kobe, Japan in January 2007. Environmental organizations made submissions on risks to fisheries and species. The meeting concluded with an action plan drafted by some 60 countries or areas. Concrete steps include issuing certificates of origin to prevent illegal fishing and greater transparency in the setting of regional fishing quotas. The delegates were scheduled to meet at another joint meeting in January or February 2009 in Europe.
In 2010, Greenpeace International added the albacore, bigeye tuna, Pacific bluefin tuna, Atlantic bluefin tuna, southern bluefin tuna, and yellowfin tuna to its seafood red list, which are fish "commonly sold in supermarkets around the world, and which have a very high risk of being sourced from unsustainable fisheries."
Bluefin tuna have been widely accepted as being severely overfished, with some stocks at risk of collapse. According to the International Seafood Sustainability Foundation (a global, nonprofit partnership between the tuna industry, scientists, and the World Wide Fund for Nature), Indian Ocean yellowfin tuna, Pacific Ocean (eastern and western) bigeye tuna, and North Atlantic albacore tuna are all overfished. In April 2009, no stock of skipjack tuna (which makes up roughly 60% of all tuna fished worldwide) was considered to be overfished.
However, the BBC documentary "South Pacific", which first aired in May 2009, stated that, should fishing in the Pacific continue at its current rate, populations of all tuna species could collapse within five years. It highlighted huge Japanese and European tuna fishing vessels, sent to the South Pacific international waters after overfishing their own fish stocks to the point of collapse.
A 2010 tuna fishery assessment report, released in January 2012 by the Secretariat of the Pacific Community, supported this finding, recommending that all tuna fishing should be reduced or limited to current levels and that limits on skipjack fishing be considered.

</doc>
<doc id="46213" url="https://en.wikipedia.org/wiki?curid=46213" title="Glenn Hughes">
Glenn Hughes

Glenn Hughes (born 21 August 1951) is an English rock bassist and vocalist, best known for playing bass and performing vocals for rock pioneers Trapeze, the Mk. III and IV line-ups of Deep Purple , as well as working with Black Sabbath guitarist Tony Iommi as a solo artist. In addition to being an active session musician, Hughes also maintains a notable solo career. He fronted the supergroup Black Country Communion from 2009 to 2013 and from late 2013 to early 2015, California Breed. On 8 April 2016, Hughes was inducted into the Rock and Roll Hall of Fame as a member of Deep Purple. Hughes is acclaimed for his wide vocal range and has been nicknamed "The Voice Of Rock" by fans.
History.
Hughes was born in Cannock. He fronted Finders Keepers in the 1960s as bassist/vocalist, as well as the British funk rock band Trapeze. Hughes was recruited to replace Roger Glover as bassist in Deep Purple in 1973, though he considered himself more a vocalist than a bassist. He was reportedly uninterested in the Deep Purple job until some of the other members proposed that Paul Rodgers of Free be brought in as co-lead vocalist. Though the recruitment of Rodgers fell through, Hughes had now become interested in the "two-lead-singer thing", and David Coverdale was later hired as Deep Purple's lead vocalist. The two would ultimately share lead vocal duties in the band until their break-up in 1976. Battling severe cocaine addiction, Hughes embarked on a solo career following his departure from the group, releasing his first solo album in 1977 called "Play Me Out".
Hughes and Thrall.
In 1982, he joined with ex-Pat Travers guitarist Pat Thrall to form Hughes/Thrall, and they released one self-titled album which went virtually unnoticed at the time. Part of the reason for the album's obscurity was the inability to support it with a proper tour due to both parties suffering from drug addiction. As Hughes stated in a 2007 interview, "The Hughes-Thrall album was a brilliant, brilliant album, but we only did 17 shows because we were too loaded."
Gary Moore, Black Sabbath and ongoing health problems.
In the mid-1980s, Hughes recorded several different albums with bands and artists including Phenomena ("Phenomena", "Phenomena II: Dream Runner"), Gary Moore ("Run For Cover"), and Black Sabbath ("Seventh Star"; originally a solo album by Sabbath guitarist Tony Iommi that was released as a Sabbath album due to record label pressure).
Hughes' health problems due to overeating, drugs and alcohol began to seriously affect his musical projects, and this contributed to very short stints with Gary Moore and Tony Iommi, as Hughes was unable to tour with them properly due to his bad health. In 1985 Black Sabbath re-united with original vocalist Ozzy Osbourne for the one-off Live Aid performance. While waiting for a break in Osbournes' career, Iommi decided to record a solo album and Hughes was brought in to provide the vocals. Due to the aforementioned contractual obligations with the record company the album was released as "Black Sabbath featuring Tony Iommi" in 1986, to generally positive critical reviews, with Hughes in particular putting in a fine performance. While touring to promote the new album Glenn was replaced by vocalist Ray Gillen after six shows due to a fight with Black Sabbath's production manager, as the injuries contributed to a degradation in his voice and he was also in no physical shape to complete the tour.
Health recovery and career rejuvenation.
By the end of the decade, Hughes' realised his ongoing drug problem was derailing him, and by 1991 a clean, sober and fully rejuvenated Hughes returned with the vocal for the hit "America: What Time Is Love?" with KLF. He also recorded all the vocals for former Europe guitarist John Norum's solo album "Face the Truth". He then re-embarked on a solo career that he has primarily focused on to date. In 1999, Hughes did a short tribute tour to Tommy Bolin in Texas, with Tommy's brother Johnnie (of Black Oak Arkansas) on drums.
In 2005 Hughes released "Soul Mover" supporting it with a European tour. He also collaborated with Black Sabbath guitarist Tony Iommi on the 2005 album "Fused". Hughes then released "Music for the Divine" in 2006, which featured Red Hot Chili Peppers members Chad Smith and John Frusciante. Hughes toured in support of the album throughout Europe in autumn 2006.
Released on Edel Records on 17 November 2007 is "Live in Australia", an acoustic CD and companion DVD of a performance at Sydney's famous "Basement" club. The show features songs from most recent Hughes albums, Purple classics and rare gems and covers.
The album, "First Underground Nuclear Kitchen" was released on 9 May 2008 in Europe and on 12 May in the rest of the world.
In 2009, Hughes formed Black Country Communion with Jason Bonham (drums), Joe Bonamassa (guitar) and Derek Sherinian (keyboards). The band has released three albums as of October 2012 and disbanded in March 2013 following the departure of guitarist Bonamassa.
In July 2010 Hughes appeared as a guest vocalist (together with singer Jorn Lande) fronting Heaven & Hell at the High Voltage Rock Festival in London as a tribute to the late Ronnie James Dio.
Autobiography.
Hughes' autobiography was published in May 2011 by British specialist limited edition publishers Foruli. The book, titled 'Deep Purple And Beyond: Scenes From The Life Of A Rock Star', was co-written with author Joel McIver and featured contributions by Tony Iommi, David Coverdale, Ozzy Osbourne and Tom Morello, as well as a foreword by Lars Ulrich of Metallica. An extended paperback edition, retitled 'Glenn Hughes: The Autobiography', was published in late 2011 by Jawbone Press.
In April 2013 Hughes appeared in Readings, Carlton for an Australian launch of "".
Other projects.
On 13 September 2012 Glenn Hughes and Derek Sherinian met Bako Sahakyan, the president of the breakaway Nagorno-Karabakh Republic and organised a concert in Stepanakert.
In 2013, Hughes makes a special guest appearance on the debut, self-titled album from Device. Hughes is featured on the song "Through It All" accompanying David Draiman on Vocals.
Kings of Chaos and California Breed.
Hughes has been touring as a member of Kings of Chaos, as lead vocals, backing vocals and acoustic guitars since early 2013.
In late 2013 he formed a new band called California Breed with drummer Jason Bonham and guitarist Andrew Watt. In 2015, California Breed announced that they had broken up. The group released one self-titled album in 2014. [http://classicrock.teamrock.com/news/2015-01-16/california-breed-are-no-more]
Glenn Hughes solo tour.
Currently in 2015 Glenn Hughes is conducting a solo world tour, featuring guitarist Doug Aldrich and drummer Pontus Engborg.
In October 2015, Glenn revealed in an interview with Metal Shock Finland's Alison Booth, that he is going to play a very special show in Jakarta, Indonesia on December 4th to pay tribute to the late Deep Purple bodyguard, Patsy Collins, killed 40 years ago.
Personal Life.
Hughes was married to Karen Ulibarri, also the former girlfriend of his Deep Purple bandmate, Tommy Bolin. 

</doc>
<doc id="46214" url="https://en.wikipedia.org/wiki?curid=46214" title="Bilateral cingulotomy">
Bilateral cingulotomy

Bilateral cingulotomy is a form of psychosurgery, introduced in 1948 as an alternative to lobotomy. Today it is mainly used in the treatment of depression
and obsessive-compulsive disorder. In the early years of the twenty-first century it was used in Russia to treat addiction. It is also, rarely, used in the treatment of chronic pain. The objective of this surgical procedure is the severing of the supracallosal fibres of the cingulum bundle, which pass through the anterior cingulate gyrus.
History.
Cingulotomy was introduced in the 1940s as an alternative to standard prefrontal leucotomy/lobotomy in the hope of alleviating symptoms of mental illness whilst reducing the undesirable effects of the standard operation (personality changes, etc.). It was suggested by American physiologist John Farquhar Fulton who, at a meeting of the Society of British Neurosurgeons in 1947, said "were it feasible, cingulectomy in man would seem an appropriate place for limited leucotomy". This was derived from the hypothesis of James Papez who thought that the cingulum was a major component of an anatomic circuit believed to play a significant role in emotion. The first reports of the use of cingulotomy on psychiatric patients came from J le Beau in Paris, Hugh Cairns in Oxford, and Kenneth Livingston in Oregon.
Target.
Bilateral Cingulotomy targets the anterior cingulate cortex, which is a part of the limbic system. This system is responsible for the integration of feelings and emotion in the human cortex. It consists of the cingulate gyrus, parahippocampal gyrus, amygdala and the hippocampal formation.
Studies in patients that were a subject to bilateral cingulotomy, that involved fMRI analyses, showed that the anterior cingulate cortex has a key role in cognitive control and is highly likely to be involved in the control of attentional response, whereas the dorsal part of that region of the brain was not identified to be involved in such a process, although this is still under dispute. The function of the dorsal part of the cingulate cortex was connected to the sorting out and processing of conflicting information signals. In addition, neuroimaging studies also indicated that the anterior cingulate cortex participates in the modulation of cortical regions that are of higher order as well as sensory processing areas.
These findings have also been confirmed by stereotactic microelectrode analysis of single cortical neurons in a study, which involved nine patients undergoing bilateral cingulotomy. The study investigated the effect of performing attention demanding tasks on the activity of 36 neurons located in the anterior cingulate cortex. Upon analyzing of the results of the study it was concluded that the anterior cingulate cortex is indeed involved in the modification of cognitive tasks that require attention based on the fact that there was a change in the basal firing rate of neurons in that region during simulation of such tasks.
Neuroimaging also uncovered different sub-regions in the anterior cingulate cortex itself based on their function. These studies showed that the caudal part of the anterior cingulate cortex plays a more important function in cognitive activities that involve attention, salience, interference and response competition. These results combined with electrophysiological investigation of the function of neurons in the anterior cingulate cortex have provided insights that can be used in the improvement of cingulotomy performed on patients treated for OCD. The basis behind this idea is the fact that a variation of certain tasks, Emotional Stroop tasks (ES), which have been particularly identified as exerting effects in OCD patients activate neurons in the more rostral part of the anterior cingulate cortex. Thus, theoretically if bilateral cingulotomy is performed in such patient in the rostral anterior cingulate cortex, better results should be obtained.
Moreover, OCD has been associated with a malformation of the basal ganglia . The function of this part of the human brain has been mapped to be composed of fiber tracks associated with numerous parallel cortico-striato-thalamocortical circuits (CSTC), which are involved in sensorimotor, motor, oculomotor as well as the cognitive processes that are manifested by the limbic system. This pathway involves GABAergic inhibitory projections that serve as one of the means of communication between the different structures involved. It has been hypothesized that some forms of OCD are a result of disinhibition of a one or several of the circuits that operate in the CSTC. This is also indicated by a finding that showed a significant decrease in intracortical inhibition in OCD patients. Thus, lesions in the anterior cingulate cortex might contribute to the lessening of the disinhibition effect.
This theory has been confirmed by another study which assessed the cortical inhibitory and excitatory mechanisms in OCD. The study measured the excitability of motor cortex, as well as intracortical inhibition in OCD patients and a control of healthy individuals. The results showed a significant decrease in intracortical inhibition, which resulted in a slowdown of interstimulus intervals by 3 ms.
In addition to its proximity to and association with the limbic system and the amygdala in particular, which plays a key role in emotional experience, the anterior cingulate cortex shares afferent and efferent pathways with a number of thalamic nuclei as well as the posterior cingulate and part of some parietal, frontal and supplementary motor cortex. All these underline the high likelihood that the anterior cingulate cortex must be linked to OCD.
Functional MRI analyses of the anterior cingulate cortex have also led to the introduction of bilateral cingulotomy for the treatment of chronic pain. Such application was introduced since the anterior cingulate cortex has been found to be related to the processing nociceptive information input. In particular the role of the anterior cingulate cortex is in the interpretation of how a stimulus affects a person rather than its actual physical intensity.
Procedure.
A book published in 1992 described how the operation was carried out at that time. In most cases the procedure started with the medical team taking a number of CT scan X-ray images of the brain of the patient. This step ensured that the exact target, the cingulate cortex was mapped out, so that the surgeon could identify it. Then burr holes are created in the patient’s skull using a drill. Lesions at the targeted tissue were made with the help of fine electrodes inserted at the right angle into the subject’s brain based on plotting charts and making sure important arteries and blood vessels were intact. The electrode was placed in a probe, or a holder, with only its tip projecting. Upon the correct insertion of the holder into the brain tissue, air was injected and more scan images were taken. Then, after the medical team had made sure they were on the right track, the tip of the electrode was advanced to the plane of the cingulate where it was heated to 75-90 °C. Once the first lesion was created it served as a center around which several other lesions were created. In order to confirm whether lesions are made at the right place, scan images were taken postoperatively and analyzed.
Recent technological advances, however, have made bilateral cingulotomy a more precise operation. For example, nowadays a neurosurgical team that performs the procedure can use an MRI to identify the location of the anterior and posterior commissures. This approach allows neurosurgeons to obtain a number of coronal images, which are then used to calculate the stereotactic coordinates of the place in the anterior cingulate cortex, where lesions need to be created. Moreover, the MRI enables to differentiate more precisely the cell composition, and thus easily identify the gray matter in that region. This can then be further confirmed with the help of microelectrode recordings.
Side effects.
Patients usually recover from this operation over a period of 4 days. However, there are cases of subjects released from hospital after as few as 48 hours after the operation. The mild shorter postoperative complications that are most commonly related to bilateral cingulotomy are typical of head interventions and include but are not limited to nausea, vomiting, and headaches. However, in some cases patients exhibit seizures that sometimes appear up to two months after the surgical intervention. It has been questioned whether this is relevant and can be attributed to cingulotomy because such seizures were observed in patients that already had a history of this condition.
Case studies.
A recent study conducted at the Massachusetts General Hospital analyzed the outcome of bilateral cingulotomy in 44 patients for the treatment of OCD in the period between 1965 and 1986. Patients were followed up over a long term and evaluated based on several criteria: 1) how many of them were responders after a period of 6 months, 2) how many cingulotomies a patient had undergone before the examination of the effectiveness of the procedure, 3) whether the patient showed any significant change after the most recent procedure, and 4) what the side effects related to the procedure were.
The follow-up of the patients produced contradictory results, which indicated that bilateral cingulotomy is not the optimal treatment for OCD. Of the 44 patients, only 32% both fit the "responder" criteria and showed significant improvement compared to the other subjects. Another 14% exhibited some signs of improvement. Multiple cingulotomies correlated with a higher likelihood of continuing to respond to follow-up inquiries (6% more often fit the full "responder" criteria, 11% more often fit the partial "responder" criteria. However, the side effects associated with the procedure were numerous. Among the complaints that patients had after the surgery were apathy and deficits in memory, although these were rarely reported. In addition, some subjects complained of some form of urinary disturbance, ranging from urinary retention to incontinence. Hydrocephalus (2%) and seizures (2%) were also observed.
Another clinical study investigated the effect of bilateral cingulotomy for the treatment of refractory chronic pain. In this case, 23 patients who were subject to 28 cingulotomies in total were followed up. The analyses aimed at determining how much the pain of each individual was affected after the procedure with the help of a questionnaire. In addition, the examiners tried to evaluate the impacts on social and family relations of the participants in the study. Based on the data obtained, cingulotomy for treatment of chronic pain showed promising results. 72% reported improvement in the level of pain experienced, and 50% indicated that they no longer required painkillers after cingulotomy. More than half of the patients also claimed that the surgical procedure was beneficial and contributed to the improvement of their social interactions.

</doc>
<doc id="46215" url="https://en.wikipedia.org/wiki?curid=46215" title="70 mm film">
70 mm film

70 mm film (or 65 mm film) is a wide high-resolution film gauge for still and motion picture photography, with higher resolution than the standard 35 mm motion picture film format. As used in cameras, the film is wide. For projection, the original 65 mm film is printed on film. The additional 5 mm are for 4 magnetic strips holding six tracks of sound. Although later 70 mm prints use digital sound encoding, the vast majority of existing and surviving 70 mm prints predate this technology. Each frame is five perforations tall, with an aspect ratio of 2.20:1. The vast majority of cinemas are unable to handle 70 mm film, and so original 70 mm films are shown using either 35 mm prints in the regular CinemaScope/Panavision aspect ratio of 2.35:1, or, in later years, by means of digital projectors at these venues.
History.
Films formatted with a width of 70 mm have existed since the early days of the motion picture industry. The first 70 mm format film was most likely footage of the Henley Regatta, which was projected in 1896 and 1897, but may have been filmed as early as 1894. It required a specially built projector built by Herman Casler in Canastota, New York and had a ratio similar to full frame, with an aperture of by . There were also several film formats of various sizes from 50 to 68 mm which were developed from 1884 onwards, including Cinéorama (not to be confused with the entirely distinct "Cinerama" format), started in 1900 by Raoul Grimoin-Sanson. In 1914 the Italian Filoteo Alberini invented a panoramic film system utilising a 70 mm wide film called Panoramica.
Fox Grandeur.
In 1928, Fox Film Corporation started working on a wide film format using 70 mm film which they named Grandeur. This was one of a number of wide-film processes developed by some of the major film studios at about that time. However, due to strong resistance from movie theater owners, who were in the process of equipping their theaters for sound, none of these systems became commercially successful. Fox dropped Grandeur in 1930.
Todd-AO.
Producer Mike Todd had been one of the founders of Cinerama, a wide-screen movie process that was launched in 1952. Cinerama employed three 35 mm film projectors running in synchronism to project a wide (2.6:1) image onto a deeply curved screen. Although the results were impressive, the system was expensive, cumbersome and had some serious shortcomings due to the need to match up three separate projected images. Todd left the company to develop a system of his own which, he hoped, would be as impressive as Cinerama, yet be simpler and cheaper and avoid the problems associated with three-strip projection; in his own words, he wanted "Cinerama out of one hole".
In collaboration with the American Optical company, Todd developed a system which was to be called "Todd-AO". This uses a single 70 mm wide film and was introduced with the film "Oklahoma!" in October 1955. The 70 mm film is perforated at the same pitch (0.187 inch, 4.75mm) as standard 35 mm film. With a five-perforation pull-down, the Todd-AO system provides a frame dimension of 1.912 inch (48.56mm) by 0.816 inch (20.73mm) giving an aspect ratio of 2.2:1.
The original version of Todd-AO used a frame rate of 30 per second, 25% faster than the 24 frames per second that was (and is) the standard; this was changed after the second film – "Around the World in 80 Days" - because of the need to produce (24 frame/sec) 35 mm reduction prints from the Todd-AO 65mm negative. The Todd-AO format was originally intended to use a deeply curved Cinerama-type screen but this failed to survive beyond the first few films. However, in the 1960s and 70s, such films as "The Sound of Music" (which had been filmed in Todd-AO) and "Patton" (which had been filmed in a copycat process known as Dimension 150) were shown in some Cinerama cinemas, which allowed for deeply curved screens.
Todd-AO adopted a similar multi-channel magnetic sound system to the one developed for Cinemascope two years earlier, recorded on "stripes" of magnetic oxide deposited on the film. However Todd-AO has six channels instead of the four of Cinemascope and due to the wider stripes and faster film speed provides superior audio quality. Five of these six channels are fed to five speakers spaced behind the screen, and the sixth is fed to surround speakers around the walls of the auditorium.
Panavision and the 65/70mm format.
Panavision developed their own 65/70mm system that was technically compatible and virtually identical to Todd-AO. Monikered as Super Panavision 70, it used spherical lenses and the same 2.20:1 aspect ratio at 24 frames per second. Panavision also had another 65mm system, (Ultra Panavision 70), which sprang from the MGM Camera 65 system they helped develop for MGM that was used to film "Raintree County" and "Ben-Hur". Both Ultra Panavision 70 and MGM Camera 65 employed an anamorphic lens with a 1.25x squeeze on a 65mm negative (as opposed to 35mm CinemaScope which used a 2x compression, or 8-perf, horizontally filmed 35mm Technirama which used a 1.5x compression). When projected on a 70mm print, a 1.25x anamorphic projection lens was used to decompress the image to an aspect ratio of 2.76:1, one of the widest ever used in commercial cinema.
Decline.
Due to the high cost of 70 mm film and the expensive projection system and screen required to use the stock, distribution for films using the stock was limited, although this did not always hurt profits. Most 70 mm films were also re-released on 35mm film for a wider distribution after the initial debut of the film. "South Pacific" (1958), "Lawrence of Arabia" (1962), "My Fair Lady" (1964), and "The Sound of Music" (1965) are well-known films widely shown in 70 mm format with a general release in 35 mm format.
Blow-ups.
During the 1970s, use of 65 mm stock for original photography declined markedly. However 70 mm "blow-ups" of films made in 35 mm were sometimes made for prestige showings. These included such films as "Camelot" (1967), "Oliver!" (1968), "Cromwell" (1970), and "Fiddler on the Roof" (1971). These enlargements did not have the sharpness and smoothness of 35 mm origination, but these larger prints allowed for a brighter image on very big screens and were more stable when projected. In addition 70 mm prints also had better sound quality than was possible from 35 mm. However these "blow-ups" rarely used the full six channels of the Todd-AO system and instead used the four-track mixes made for 35 mm prints, the additional half-left and half-right speakers of the Todd-AO layout being fed with a simple mix of the signals intended for the adjacent speakers (known as a "spread") or simply left blank. However, if a 70mm film was shown in a Cinerama theatre, the Cinerama sound system was used. From 1976 onwards many 70 mm prints used Dolby noise reduction on the magnetic tracks but Dolby disapproved of the "spread" and instead re-allocated the 6 available tracks to provide for left, center and right screen channels, left and right surround channels plus a "low-frequency enhancement" channel to give more body to low-frequency bass. This layout came to be known as "5.1" (the "point one" is the low-frequency enhancement channel) and was subsequently adopted for digital sound systems used with 35 mm.
In the 1980s the use of these "blow-ups" increased with large numbers of 70 mm prints being made of some blockbusters of the period such as the 125 70 mm prints made of "The Empire Strikes Back" (1980). However the early 1990s saw the advent of digital sound systems (Dolby Digital, DTS and SDDS) for 35 mm prints which meant that 35 mm could finally match 70 mm for sound quality but at a far lower cost. Coupled with the rise of the multiplex cinema, which meant that audiences were increasingly seeing films on relatively small screens rather than the giant screens of the old "Picture Palaces", this meant that the expensive 70 mm format went out of favour again. The DTS digital sound-on-disc system was adapted for use with 70 mm film, thus saving the significant costs of magnetic striping, but this has not been enough to stop the decline, and 70 mm prints were rarely made.
Current use.
In the late 20th century, the usage of 65 mm negative film drastically reduced, in part due to the high cost of 65 mm raw stock and processing. Some of the few films since 1990 shot entirely on 65 mm stock are Kenneth Branagh's "Hamlet" (1996), Ron Fricke's "Baraka" (1992), and its sequel "Samsara" (2011), Paul Thomas Anderson's "The Master" (2012) and Quentin Tarantino's "The Hateful Eight" (2015). Other films used 65 mm cameras sparingly, for selected scenes or special effects. Films with limited 65 mm footage include
Terrence Malick's "The New World" (2005) and Christopher Nolan's latest four movies, "The Dark Knight" (featured 28 minutes of IMAX footage), "Inception", "The Dark Knight Rises" (over an hour in IMAX) and "Interstellar".
Since the 2010s most of the movie theaters across the world have converted to digital projection systems, largely eliminating 70mm film projectors. 70mm has retained a niche market of amateurs and enthusiasts.
Digital 70 mm cameras.
There are three types of digital cinema cameras with a 65 mm sensor, the Phantom 65, the Arri Alexa 65 and the forthcoming IMAX 2D Digital Camera. Otti International's Phil Kroll developed the world's first 65/70 mm telecine transfer system. This camera has been used in Hollywood to digitally master 70 and 65 mm films.
Home media.
For home theater, VHS and DVD did not offer enough resolution to carry the full image quality captured by 70 mm film, and VHS and DVD video transfers were usually prepared from 35 mm reduction elements. The high-definition Blu-ray format, in contrast, can potentially reveal the quality advantage of 70 mm productions. Although telecine machines for 70 mm scanning are uncommon, high-resolution transfers from high-quality full-gauge elements can reveal impressive technical quality.
Uses of 70 mm.
Ultra Panavision.
An anamorphic squeeze combined with 65 mm film allowed for extremely wide aspect ratios to be used while still preserving quality. This was used in the 1957 film "Raintree County (film)" and to incredible success with the 1959 film "Ben-Hur" and The Hateful Eight, which was filmed with the MGM Camera 65 process at an aspect ratio of 2.76:1. It required the use of a 1.25x anamorphic lens to horizontally compress the image, and a corresponding lens on the projector to uncompress it.
Special effects.
Limited use of 65 mm film was revived in the late 1970s for some of the visual effects sequences in films like "Close Encounters of the Third Kind", mainly because the larger negative did a better job than 35 mm negative of minimizing visible film grain during optical compositing. Since the 1990s, a handful of films (such as "Spider-Man 2") have used it for this purpose, but the usage of digital intermediate for compositing has largely negated these issues. Digital intermediate offers other benefits such as lower cost and a greater range of available lenses and accessories to ensure a consistent look to the footage.
IMAX.
A horizontal variant of 70 mm, with an even bigger picture area, is used for the high-performance IMAX format which uses a frame that is 15 perforations wide on 70 mm film. The Dynavision and Astrovision systems each use slightly less film per frame and vertical pulldown to save print costs while being able to project onto an IMAX screen. Both were rare, with Astrovision largely used in Japanese planetariums. In the 2014 movie "Interstellar", a significant amount was shot in the IMAX format. Other scenes were shot in either 35 mm or in the standard 'vertical' 5-perf 65 mm format. IMAX introduced a digital projection system in the late 2000s and most IMAX venues have migrated to digital setup.
70 mm 3D early use.
The first commercial introduction of 70 mm single projector 3D was the 1967 release of "Con la muerte a la espalda", a Spanish/French/Italian co-production which used a process called Hi-Fi Stereo 70, itself based on a simplified, earlier developed soviet process called Stereo-70. This process captured two anamorphic images, one for each eye, side by side on 65 mm film. A special lens on a 70 mm projector added polarization and merged the two images on the screen. The 1971 re-release of Warner Bros.' "House of Wax" used the side-by-side StereoVision format and was distributed in both anamorphically squeezed 35 mm and deluxe non-anamorphic 70 mm form. The system was developed by Allan Silliphant and Chris Condon of StereoVision International Inc., which handled all technical and marketing aspects on a five-year special-royalty basis with Warner Bros. The big screen 3D image was both bright and clear, with all the former sync and brightness problems of traditional dual 35 mm 3D eliminated. Still, it took many years more before IMAX began to test the water for big-screen 3D, and sold the concept to Hollywood executives.
IMAX 3D.
Hollywood has released films shot on 35 mm as IMAX blow-up versions. Many 3D films were shown in the 70 mm IMAX format. "The Polar Express" in IMAX 3D 70 mm earned 14 times as much, per screen, as the simultaneous 2D 35 mm release of that film in the fall of 2004.
In 2011 IMAX introduced a 3D Digital camera based on two Phantom 65 cores. The camera has been used for documentaries as well as Hollywood films, the first being the 2014 release of "".
Technical specifications.
Ultra Panavision 70 (MGM Camera 65).
"Same as Standard 65mm except"
Showscan.
"Same as Standard 65 mm except"
IMAX Dome / OMNIMAX.
"Same as IMAX except"
Omnivision Cinema 180.
"same as standard 65/70 except:"
Omnivision started in Sarasota, Florida. Theatres were designed to compete with Omnimax but with much lower startup and operating costs. Most theatres were built in fabric domed structures designed by Siemens Corporation. The last known OmniVision theatres to exist in USA are The Alaska Experience Theatre in Anchorage, Alaska, built in 1981 (closed in 2007, reopened in 2008), and the Hawaii Experience Theatre in Lahaina, Hawaii (closed in 2004). Rainbow's End (Theme Park) in NZ had the only remaining permanent Cinema 180 attraction until May 2015 when it was demolished.
One of the few producers of 70 mm films for Cinema 180 was the German company Cinevision (today AKPservices GmbH, Paderborn).

</doc>
<doc id="46216" url="https://en.wikipedia.org/wiki?curid=46216" title="Israeli–Palestinian conflict">
Israeli–Palestinian conflict

The Israeli–Palestinian conflict ( "al-Niza'a al'Filastini al 'Israili"; "Ha'Sikhsukh Ha'Yisraeli-Falestini") is the ongoing struggle between Israelis and Palestinians that began in the mid-20th century. The conflict is wide-ranging, and the term is sometimes also used in reference to the earlier sectarian conflict in Mandatory Palestine, between the Jewish "yishuv" and the Arab population under British rule. It has been referred to as the world's "most intractable conflict".
Despite a long-term peace process and the general reconciliation of Israel with Egypt and Jordan, Israelis and Palestinians have failed to reach a final peace agreement. The remaining key issues are: mutual recognition, borders, security, water rights, control of Jerusalem, Israeli settlements, Palestinian freedom of movement, and Palestinian right of return. The violence of the conflict, in a region rich in sites of historic, cultural and religious interest worldwide, has been the object of numerous international conferences dealing with historic rights, security issues and human rights, and has been a factor hampering tourism in and general access to areas that are hotly contested.
Many attempts have been made to broker a two-state solution, involving the creation of an independent Palestinian state alongside the State of Israel (after Israel's establishment in 1948). In 2007, the majority of both Israelis and Palestinians, according to a number of polls, preferred the two-state solution over any other solution as a means of resolving the conflict. Moreover, a majority of Jews see the Palestinians' demand for an independent state as just, and thinks Israel can agree to the establishment of such a state. The majority of Palestinians and Israelis in the West Bank and Gaza Strip have expressed a preference for a two-state solution. Mutual distrust and significant disagreements are deep over basic issues, as is the reciprocal scepticism about the other side's commitment to upholding obligations in an eventual agreement.
Within Israeli and Palestinian society, the conflict generates a wide variety of views and opinions. This highlights the deep divisions which exist not only between Israelis and Palestinians, but also within each society. A hallmark of the conflict has been the level of violence witnessed for virtually its entire duration. Fighting has been conducted by regular armies, paramilitary groups, terror cells, and individuals. Casualties have not been restricted to the military, with a large number of fatalities in civilian population on both sides. There are prominent international actors involved in the conflict.
The two parties engaged in direct negotiation are the Israeli government, currently led by Benjamin Netanyahu, and the Palestine Liberation Organization (PLO), currently headed by Mahmoud Abbas. The official negotiations are mediated by an international contingent known as the Quartet on the Middle East (the "Quartet") represented by a special envoy, that consists of the United States, Russia, the European Union, and the United Nations. The Arab League is another important actor, which has proposed an alternative peace plan. Egypt, a founding member of the Arab League, has historically been a key participant.
Since 2006, the Palestinian side has been fractured by conflict between the two major factions: Fatah, the traditionally dominant party, and its later electoral challenger, Hamas. After Hamas's electoral victory in 2006, the Quartet conditioned future foreign assistance to the Palestinian National Authority (PA) on the future government's commitment to non-violence, recognition of the State of Israel, and acceptance of previous agreements. Hamas rejected these demands, which resulted in the Quartet's suspension of its foreign assistance program, and the imposition of economic sanctions by the Israelis. A year later, following Hamas's seizure of power in the Gaza Strip in June 2007, the territory officially recognized as the PA was split between Fatah in the West Bank, and Hamas in the Gaza Strip. The division of governance between the parties had effectively resulted in the collapse of bipartisan governance of the PA. However, in 2014, a Palestinian Unity Government, composed of both Fatah and Hamas, was formed. The latest round of peace negotiations began in July 2013 and was suspended in 2014.

</doc>
<doc id="46223" url="https://en.wikipedia.org/wiki?curid=46223" title="Sowing">
Sowing

Sowing is the process of planting seeds. An area or object that has had seeds planted will be described as being sowed.
Plants which are usually sown.
Among the major field crops, oats, wheat, and rye are sown, grasses and legumes are seeded, and maize and soybeans are planted. In planting, wider rows (generally 75 cm (30 in) or more) are used, and the intent is to have precise, even spacing between individual seeds in the row; various mechanisms have been devised to count out individual seeds at exact intervals.
Sowing depth.
In sowing, little if any soil is placed over the seeds. More precisely, seeds can be generally sown into the soil by maintaining a planting depth of about 2-3 times the size of the seed.
Sowing types and patterns.
For hand sowing, several sowing types exist; these include:
Several patterns for sowing may be used together with these types; these include:
Types of sowing.
Hand sowing.
Hand sowing or (planting) is the process of casting handfuls of seed over prepared ground, or broadcasting (from which the technological term is derived). Usually, a drag or harrow is employed to incorporate the seed into the soil. Though labor-intensive for any but small areas, this method is still used in some situations. Practice is required to sow evenly and at the desired rate. A hand seeder can be used for sowing, though it is less of a help than it is for the smaller seeds of grasses and legumes.
Hand sowing may be combined with pre-sowing in seed trays. This allows the plants to come to strength indoors during cold periods (e.g. spring in temperate countries).
In agriculture, most seed is now sown using a seed drill, which offers greater precision; seed is sown evenly and at the desired rate. The drill also places the seed at a measured distance below the soil, so that less seed is required. The standard design uses a fluted feed metering system, which is volumetric in nature; individual seeds are not counted. Rows are typically about 10–30 cm apart, depending on the crop species and growing conditions. Several row opener types are used depending on soil type and local tradition. Grain drills are most often drawn by tractors, but can also be pulled by horses. Pickup trucks are sometimes used, since little draft is required.
A seed rate of about 100 kg of seed per hectare (2 bushels per acre) is typical, though rates vary considerably depending on crop species, soil conditions, and farmer's preference. Excessive rates can cause the crop to lodge, while too thin a rate will result in poor utilisation of the land, competition with weeds and a reduction in the yield.
Open field.
Open-field planting refers to the form of sowing used historically in the agricultural context whereby fields are prepared generically and left open, as the name suggests, before being sown directly with seed. The seed is frequently left uncovered at the surface of the soil before germinating and therefore exposed to the prevailing climate and conditions like storms etc. This is in contrast to the seedbed method used more commonly in domestic gardening or more specific (modern) agricultural scenarios where the seed is applied beneath the soil surface and monitored and manually tended frequently to ensure more successful growth rates and better yields.
Pre-treatment of seed and soil before sowing.
Before sowing, certain seeds first require a treatment prior to the sowing process.
This treatment may be seed scarification, stratification, seed soaking or seed cleaning with cold (or medium hot) water.
Seed soaking is generally done by placing seeds in medium hot water for at least 24 to up to 48 hours
Seed cleaning is done especially with fruit, as the flesh of the fruit around the seed can quickly become prone to attack from insects or plagues. To clean the seed, usually seed rubbings with cloth/paper is performed, sometimes assisted with a seed washing. Seed washing is generally done by submerging cleansed seeds 20 minutes in 50 degree Celsius water. This (rather hot than moderately hot) water kills any organisms that may have survived on the skin of a seed. Especially with easily infected tropical fruit such as lychees and rambutans, seed washing with high temperature water is vital.
In addition to the mentioned seed pretreatments, seed germination is also assisted when disease-free soil is used. Especially when trying to germinate difficult seed (e.g. certain tropical fruit), prior treatment of the soil (along with the usage of the most suitable soil; e.g. potting soil, prepared soil or other substrates) is vital. The two most used soil treatments are pasteurisation and sterilisation. Depending on the necessity, pasteurisation is to be preferred as this does not kill all organisms. Sterilisation can be done when trying to grow truly difficult crops. To pasteurise the soil, the soil is heated for 15 minutes in an oven of 120 °C.

</doc>
<doc id="46224" url="https://en.wikipedia.org/wiki?curid=46224" title="16 mm film">
16 mm film

16 mm film is a historically popular and economical gauge of film. 16 mm refers to the width of the film, with other common film gauges including 8 and 35 mm. It is generally used for non-theatrical (e.g., industrial, educational) film making or for low budget motion pictures. It also existed as a popular amateur or home movie making format for several decades, alongside 8 mm film, and later Super 8 film. In 1923, Eastman Kodak released the first 16 mm "outfit" consisting of a camera, projector, tripod, screen and splicer for $335. RCA-Victor introduced a 16 mm sound movie projector in 1932 
and developed an optical sound-on-film 16 mm camera, released in 1935.
History.
Eastman Kodak introduced 16 mm film in 1923 as a less expensive amateur alternative to 35 mm film. During the 1920s, the format was often referred to as sub-standard by the professional industry.
Kodak hired Willard Beech Cook from his 28 mm Pathescope of America company to create the new 16 mm "Kodascope Library". In addition to making home movies, people could buy or rent films from the library, a key selling aspect of the format.
Intended for amateur use, 16 mm film was one of the first formats to use acetate safety film as a film base. Kodak never used nitrate film for the format because of the high flammability of the nitrate base. 35 mm nitrate was discontinued in 1952.
Production evolution.
The silent 16 mm format was initially aimed at the home enthusiast, but by the 1930s it had begun to make inroads into the educational market. The addition of optical sound tracks and, most notably, Kodachrome in 1935, gave an enormous boost to the 16 mm market. Used extensively in WW2, there was a huge expansion of 16 mm professional filmmaking in the post-war years. Films for government, business, medical and industrial clients created a large network of 16 mm professional filmmakers and related service industries in the 1950s and 1960s. The advent of television production also enhanced the use of 16 mm film, initially for its advantage of cost and portability over 35 mm. At first used as a news-gathering format, the 16 mm format was also used to create television programming shot outside the confines of the more rigid television studio production sets. The home movie market gradually switched to the even less expensive 8 mm film and Super 8 mm format.
16 mm has been extensively used for television production with light cameras in many countries before portable video cameras appeared. Replacing analog video devices, digital video has made significant inroads in television production use. Nevertheless, 16 mm is still in use in its Super 16 ratio (see below) for low cost productions.
Format standards.
Standard 16 mm.
The picture taking area of standard 16 mm is 10.26 mm by 7.49 mm, an aspect ratio of 1.37:1, the standard pre-widescreen Academy ratio for 35 mm. The "nominal" picture projection area (per SMPTE RP 20-2003) is 0.380 in by 0.284 in, and the maximum picture projection area (per SMPTE 233-2003) is 0.384 in by 0.286 in, each implying an aspect ratio of 1.34:1. Double-perf 16 mm film, the original format, has a perforation at both sides of every frame line. Single-perf is perforated at one side only, making room for an optical or magnetic soundtrack along the other side.
Super 16 mm.
The variant called Super 16 mm, Super 16, or 16 mm Type W, developed by Swedish cinematographer Rune Ericson in 1969, uses single-sprocket film, and takes advantage of the extra room for an expanded picture area of 7.41 mm by 12.52 mm with a wider aspect ratio of 1.67. Super 16 cameras are usually 16 mm cameras that have had the film gate and ground glass in the viewfinder modified for the wider frame. Since Super 16 takes up the space originally reserved for the soundtrack, films shot in this format can be enlarged by optical printing to 35 mm for projection. However, with the recent development of digital intermediate workflows, it is now possible to digitally enlarge to 35 mm with virtually no quality loss (given a high quality digital scan), or alternatively to use high-quality video equipment for the original image capture.
In 2009, German lens manufacturer Vantage introduced a series of anamorphic lenses under its HAWK brand. These provided a 1.3x squeeze factor (as opposed to the standard 2x) specifically for the Super 16 format. These lenses let camera operators use the entire Super 16 frame for 2.35:1 widescreen photography.
Ultra 16 mm.
The DIY-crafted Ultra-16 is a variation of Super 16. Cinematographer Frank G. DeMarco is credited with inventing Ultra 16 in 1996 while shooting tests for Darren Aronofsky's "Pi". Ultra 16 is created by widening the left and right sides of the gate of a standard 16 mm camera by 0.7 mm to expose part of the vertical area between the perforations. Perforation placement on standard 16 mm film (to the left of the division between frames) accommodates use of this normally unexposed area. The Ultra-16 format, with frame dimensions of 11.66 mm by 6.15 mm, provides a frame size between standard 16 mm and Super 16—while avoiding the expense of converting a 16 mm camera to Super 16, the lens requirements of Super 16 cameras, and image vignetting caused by traditional 16 mm cameras. Thus, standard 16 mm optics achieve a wider image. The image readily converts to NTSC/PAL (1.33 ratio), HDTV (1.78 ratio) and to 35 mm film (1.85 ratio), using either both the full vertical frame or the full width (intersprocket) frame, depending upon application.
Modern usage.
The two major suppliers of 16 mm film today are Kodak and Agfa (Fuji closed its film manufacturing facility on 31 December 2012). 16 mm film is used in television, such as for the "Hallmark Hall of Fame" anthology (it has since been produced in 16:9 high definition) and "Friday Night Lights" and "The O.C." as well as the "The Walking Dead" in the US. In the UK, the format is exceedingly popular for dramas and commercials. The British Broadcasting Corporation (BBC) played a large part in the development of the format. They worked extensively with Kodak during the 1950s and 1960s to bring 16 mm to a professional level, since the BBC needed cheaper, more portable production solutions while maintaining a higher quality than was offered at the time, when the format was mostly for home display of theatrical shorts, newsreels, and cartoons, documentary capture and display for various purposes (including education), and limited "high end" amateur use. Today the format also is frequently used for student films, while usage in documentary has almost disappeared. With the advent of HDTV, Super 16 film is still used for some productions destined for HD. Some low-budget theatrical features are shot on 16 mm and super 16 mm such as Kevin Smith's 16 mm 1994 independent hit "Clerks"
Thanks to advances in film stock and digital technology—specifically digital intermediate (DI)—the format has dramatically improved in picture quality since the 1970s, and is now a revitalized option. "Vera Drake", for example, was shot on Super 16 mm film, digitally scanned at a high resolution, edited and color graded, and then printed out onto 35 mm film via a laser film recorder. Because of the digital process, the final 35 mm print quality is good enough to fool some professionals into thinking it was shot on 35 mm.
In Britain most exterior television footage was shot on 16 mm from the 1960s until the 1990s, when the development of more portable television cameras and videotape machines led to video replacing 16 mm in many instances. Many drama shows and documentaries were made entirely on 16 mm, notably "Brideshead Revisited", "The Jewel in the Crown", "The Ascent of Man" and "Life on Earth". More recently, the advent of widescreen television has led to the use of "Super" 16. For example, the 2008/09 BBC fantasy drama series "Merlin" was shot in Super 16.
The BBC considers Super16 a standard definition film format, but other broadcasting and production companies may have different outlook. In particular, "Scrubs" has been shot on Super16 from the start and is aired either as 4:3 SD (first 7 seasons) or as 16:9 HD (seasons 8 and 9). John Inwood, the cinematographer of the series, believed that footage from his Aaton XTR Prod camera was not only sufficient to air in high definition, it "looked terrific." However, the BBC has recently announced that it would no longer accept 16 mm as an origination format for High Definition video transfer.
The Academy Award winning "Leaving Las Vegas" (1995) was shot on 16 mm.
The first 2 seasons of "Buffy the Vampire Slayer" were shot on 16 mm and was switched to 35 mm for its later seasons.
The first season of the popular series "Sex and the City" was shot on 16 mm. Later seasons were shot on 35 mm. All three seasons of "Veronica Mars" were shot on 16 mm and aired in HD. "This Is Spinal Tap", and Christopher Guest's subsequent mockumentary films, are shot in Super 16 mm.
The first 3 seasons of "Stargate SG-1" (bar the season 3 finale and the effects shots) were shot in 16 mm, before switching to 35 mm for later seasons.
The 2009 Academy Award winner for Best Picture, "The Hurt Locker", was shot using Aaton Super 16 mm cameras and Fujifilm 16 mm film stocks. The cost savings over 35 mm allowed the production to utilize multiple cameras for many shots, exposing over 1,000,000 feet of film.
British Napoleonic era drama Sharpe (TV series) was shot on Super 16 mm right through to the film Sharpe's Challenge (2006). For the last film in the series, Sharpe's Peril (2008), the producers switched to 35 mm.
"Moonrise Kingdom" was shot using super 16 mm.
Digital 16 mm.
A number of digital cameras approximate the look of the 16 mm format by using 16 mm-sized sensors and taking 16 mm lenses. These cameras include the Ikonoskop A-Cam DII (2008) and the Digital Bolex (2012). The Blackmagic Pocket Cinema Camera (2013) has a Super 16-sized sensor.
Cameras.
Professional cameras.
Today, the professional industry tends to use 16 mm cameras from Aaton and Arri, most notably the Aaton Xtera, Aaton XTRprod, Arriflex 16SR3, and Arriflex 416. Aaton also released the A-Minima, which is about the size of a video camcorder and is used for specialized filming requiring smaller, more versatile cameras. Photo Sonics have special extremely high speed cameras for 16 mm that film at up to 1,000 frames per second. Panavision has produced the Panaflex 16, nicknamed "Elaine".
Amateur cameras.
For amateur, hobbyist, and student use, it is more economical to use older models from Arri, Aaton, Auricon, Beaulieu, Bell and Howell, Bolex, Canon, Cinema Products, Eclair, Keystone, Krasnogorsk, Mitchell, and others.
Film reproduction methods.
Most original movie production companies that use film shoot on 35 mm. The 35 mm size must be converted or reduced to 16 mm for 16 mm systems. There are multiple ways of obtaining a 16 mm print from 35 mm. The preferred method is to strike a 16 mm negative from the original 35 mm negative and then make a print from the new 16 mm negative. A 16 mm negative struck from the original 35 mm negative is called an "original". A new 16 mm print made from a print with no negative is called a "reversal".
16 mm prints can be made from many combinations of size and format, each with a distinct, descriptive name:
Film traders often refer to 16 mm prints by the print's production method, i.e., an "original", "reversal", "dupe down", "double dupe", or "double dupe down".
Color fading of old film and color recovery.
Over time, the cyan, magenta and yellow dyes that form the image in color 16 mm film inevitably fade. The rate of deterioration depends on storage conditions and the film type. In the case of Kodachrome amateur and documentary films and Technicolor IB (imbibition process) color prints, the dyes are so stable and the deterioration so slow that even prints now over 70 years old typically show no obvious problems.
Unfortunately, dyes in the far more common Eastmancolor print film and similar products from other manufacturers are notoriously unstable. Prior to the introduction of a longer-lasting "low fade" type in 1979, Eastmancolor prints routinely suffered from easily seen color shift and fading within ten years. The dyes degrade at different rates, with magenta being the longest-lasting, eventually resulting in a pale reddish image with little if any other color discernible.
In the process of digitizing old color films, even badly faded source material can sometimes be restored to full color through digital techniques that amplify the faded dye colors.

</doc>
<doc id="46225" url="https://en.wikipedia.org/wiki?curid=46225" title="8 mm film">
8 mm film

8 mm film is a motion picture film format in which the filmstrip is eight millimeters wide. It exists in two main versions — the original standard 8 mm film, also known as regular 8 mm or Double 8 mm, and Super 8. Although both standard 8 mm and Super 8 are 8 mm wide, Super 8 has a larger image area because of its smaller and more widely spaced perforations.
There are also two other varieties of Super 8—Single 8 mm and Straight-8 that require different cameras, but produce a final film with the same dimensions.
Standard 8.
The standard 8 mm (also known as regular 8) film format was developed by the Eastman Kodak company during the Great Depression and released to the market in 1932 to create a home movie format that was less expensive than 16 mm. The film spools actually contain a 16 mm film with twice as many perforations along each edge as normal 16 mm film; on its first pass through the camera, the film is exposed only along half of its width. When the first pass is complete, the operator opens the camera and flips and swaps the spools (the design of the spool hole ensures that the operator does this properly) and the same film is subsequently exposed along its other edge, the edge left unexposed on the first pass. After the film is developed, the processor splits it down the middle, resulting in two lengths of 8 mm film, each with a single row of perforations along one edge. Each frame is half the width and half the height of a 16 mm frame, so there are four times the number of frames in a given film area, which is what makes it cost less. Because of the two passes of the film, the format was sometimes called "Double 8". The frame size of regular 8 mm is 4.8 mm × 3.5 mm and 1 meter of film contains 264 pictures. Normally Double 8 is filmed at 16 frames per second.
Common length film spools allowed filming of about 3 minutes to 4.5 minutes at 12, 15, 16 and 18 frames per second.
Kodak ceased sales of standard 8 mm film under its own brand in the early 1990s, but continued to manufacture the film, which was sold via independent film stores. Black-and-white 8 mm film is still manufactured in the Czech Republic, and several companies buy bulk quantities of 16 mm film to make regular 8 mm by re-perforating the stock, cutting it into 25 foot (7.6 m) lengths, and collecting it into special standard 8 mm spools, which they then sell. Re-perforation requires special equipment. Some specialists also produce Super 8 mm film from existing 16 mm, or even 35 mm film stock.
Super 8.
In 1965, Super-8 film was released and was quickly adopted by the amateur film-maker. It featured a better quality image, and was easier to use mainly due to a cartridge-loading system that did not require re-loading—and re-"threading" halfway through. Super 8 was often erroneously criticized, since the film gates in some cheaper Super 8 cameras were plastic, as was the pressure plate built into the cartridge; the standard 8 cameras had a permanent metal film gate that was regarded as more reliable in keeping the film flat and the image in focus. In reality, this was not the case. The plastic pressure plate could be moulded to far tighter tolerances than their metal counterparts could be machined. 
To easily differentiate Super 8 film from Standard 8, projector spools for the former had larger spindle holes. Therefore, it was not possible to mount a Super 8 spool on a Standard 8 projector, and vice versa.
Single 8.
Another version of Super-8 film, Single-8, was produced by Fuji in Japan. Introduced in 1965 as an alternative to the Kodak Super 8 format, it had the same final film dimensions, but with a different cassette. Unlike the co-axial design of Super 8, the Single 8 cartridge featured one spool above the other.
Straight 8.
A number of camera companies offered single width 8 mm film in magazines and spools, but the format faded when Kodak introduced Kodachrome, as this was only available in the double 8 mm format. It continued for some time outside the United States, with Agfa and Svema offering reversal film.
UltraPan 8.
Introduced in 2011 by Nicholas Kovats, and implemented by Jean-Louis Seguin, this format uses Standard 8 film in a modified Bolex (H16 or H8) camera. The area of film exposed per frame is 10.52mm × 3.75mm, having an aspect ratio of 2.8:1. There are effectively two UP8 frames for every one 16 mm frame. The design means there is no waste of film emulsion for the targeted aspect ratio. Earlier versions of this general idea date from the 1950s and exactly the same design occurs in implementations of the 1960s and 1970s. The current implementation of the idea gains impetus from the relative ease with which digital delivery systems can handle what would otherwise have required, in the past, either a dedicated mechanical projector or the transfer to another film format for which projectors were already available.

</doc>
<doc id="46226" url="https://en.wikipedia.org/wiki?curid=46226" title="Anthony Zinni">
Anthony Zinni

Anthony Charles Zinni (born September 17, 1943) is a retired United States Marine Corps general and a former Commander in Chief of the United States Central Command (CENTCOM). In 2002, he was selected to be a special envoy for the United States to Israel and the Palestinian Authority.
While serving as special envoy, Zinni was also an instructor in the Department of International Studies at the Virginia Military Institute. Currently, he is an instructor at the Sanford School of Public Policy at Duke University, a public speaker, and an author of best-selling books on his military career and foreign affairs, including "Battle for Peace". , he was involved in the corporate world, joining M.I.C. Industries as its president for International Operations in 2005.
Zinni also serves or has served on the advisory boards of a number of companies, including the security testing firm, Mu Dynamics, based in Sunnyvale, California. He joined Duke University's Terry Sanford Institute of Public Policy in Spring 2008 as the Sanford Distinguished Lecturer in Residence and taught a new course in the Hart Leadership Program.
, he serves as Chairman of the Board of Governors of the Middle East Institute. He has been named Honorary Chairman of that institution.
General Zinni also serves as an Honorary Board Member of the Non Profit Wine Country Marines - a 501 (C) 3 dedicated to helping wounded service members, and aiding the welfare of currently serving service members, as well as addressing veterans employment and transition and healthcare.
He has been credited for foresight in predicting the dangers of terrorism coming out of Afghanistan before the September 11 attacks of 2001 and supporting the Iraq War troop surge of 2007. In October 2009 he came out firmly in support of General Stanley A. McChrystal's request for up to 40,000 additional troops in Afghanistan.
Early life and education.
Zinni was born in Conshohocken, Pennsylvania, the son of Lilla (Disabatino), a seamstress and homemaker, and Antonio Zinni, a chauffeur. His parents were of Italian descent.
In 1965, Zinni graduated from Villanova University with a degree in economics, and was commissioned a second lieutenant in the United States Marine Corps. After completion of the Basic School, he was assigned to the 2nd Marine Division, where he served as a platoon commander, company executive officer, and company commander in the 1st Battalion, 6th Marines. He also served as a company commander in the 1st Infantry Training Regiment during this tour.
Career.
U.S. Marine Corps.
In 1967, Zinni was assigned as an infantry battalion advisor to the Vietnamese Marine Corps. Following the Vietnam War, he was ordered to the Basic School where he served as a tactics instructor, platoon commander, and company executive officer. In 1970, he returned to Vietnam as a company commander in 1st Battalion, 5th Marines where he was wounded, evacuated, and subsequently assigned to the 3rd Force Service Support Group on Okinawa. There he served as a company commander and guard officer. In 1971, Zinni returned to the 2nd Marine Division, where he served as a company commander in the 1st Battalion, 8th Marines, Aide de Camp to the Commanding General, and Officer in Charge of the Infantry Training Center. In 1974, he was assigned to Headquarters Marine Corps, where he was assigned as the Retention and Release Officer and Plans Officer in the Officer Assignment Branch of the Manpower Department.
Zinni again served in the 2nd Marine Division in 1978, as the Operations Officer of the 3rd Battalion, 2nd Marines, Executive Officer of the 1st Battalion, 8th Marines, Executive Officer of the 8th Marine Regiment and Commanding Officer of the 2nd Battalion, 8th Marines. In 1981, he was assigned as an operations and tactics instructor at the Marine Corps Command and Staff College at Quantico, Virginia. He was next assigned to the Operations Division at Headquarters, U.S. Marine Corps where he served as the Head of the Special Operations and Terrorism Counteraction Section and as the Head, Marine Air-Ground Task Force Concepts and Capabilities Branch. In 1984, he earned his master's degree from Central Michigan University. In 1986, he was selected as a fellow on the Chief of Naval Operations Strategic Studies Group. From 1987 to 1989, Zinni served on Okinawa as the regimental commander of the 9th Marine Regiment and the Commanding Officer of the 35th Marine Expeditionary Unit, which was twice deployed to the Philippines to conduct emergency security operations and disaster relief operations. Upon his return to the U.S., he was assigned as the Chief of Staff of the Marine Air-Ground Training and Education Center at Marine Corps Base Quantico.
His initial general officer assignment was as the Deputy Director of Operations at the U.S. European Command. In 1991, he served as the Chief of Staff and Deputy Commanding General of Combined Task Force Operation Provide Comfort during the Kurdish relief effort in Turkey and Iraq. He also served as the Military Coordinator for Operation Provide Hope, the relief effort for the former Soviet Union. In 1992-93, he served as the Director for Operations for the Unified Task Force in Somalia for Operation Restore Hope. Also in 1993, he served as the Assistant to the U.S. Special Envoy to Somalia during Operation Continued Hope. Zinni was assigned as the Deputy Commanding General, U.S. Marine Corps Combat Development Command, Quantico, Virginia, from 1992 to 1994.
From 1994 to 1996, he served as the Commanding General, 1st Marine Expeditionary Force. During early 1995, Zinni served as Commander of the Combined Task Force for Operation United Shield, protecting the withdrawal of U.N. forces from Somalia.
From September 1996 until August 1997, Zinni served as the Deputy Commander in Chief, United States Central Command. His final tour was from August 1997 to September 2000 as the Commander in Chief, United States Central Command, MacDill Air Force Base, Florida. He organized Operation Desert Fox, a series of airstrikes against Iraq during December 1998, with the stated purpose of degrading Iraq's weapons of mass destruction program. Following this, he retired in autumn 2000.
Zinni has attended the John F. Kennedy Special Warfare Center and School, Amphibious Warfare School, Marine Corps Command and Staff College, and the National War College. He holds a bachelor's degree in economics and two Master of Arts degrees, one in international relations and another in management and supervision.
His son, Anthony Zinni, serves in the Marine Corps, and was promoted to the rank of major effective September 1, 2010.
Testimony before Congress.
On March 15, 2000, Zinni testified before Congress:
Iraq remains the most significant near-term threat to U.S. interests in the Persian Gulf region. This is primarily due to its large conventional military force, pursuit of WMD, oppressive treatment of Iraqi citizens, refusal to comply with United Nations Security Council resolutions (UNSCR), persistent threats to enforcement of the no-fly zones (NFZ), and continued efforts to violate UN Security Council sanctions through oil smuggling.
While Iraq's WMD capabilities were degraded under UN supervision and set back by Coalition strikes, some capabilities remain and others could quickly be regenerated. Despite claims that WMD efforts have ceased, Iraq probably is continuing clandestine nuclear research, retains stocks of chemical and biological munitions, and is concealing extended-range SCUD missiles, possibly equipped with CBW payloads. Even if Baghdad reversed its course and surrendered all WMD capabilities, it retains the scientific, technical, and industrial infrastructure to replace agents and munitions within weeks or months. A special concern is the absence of a UN inspection and monitoring presence, which until December 1998 had been paramount to preventing large-scale resumption of prohibited weapons programs. A new disarmament regime must be reintroduced into Iraq as soon as possible and allowed to carry out the mandates dictated by the post-Gulf War UN resolutions.
Zinni also warned about terrorism:
Extremists like Osama bin Laden and his World Islamic Front network benefit from the global nature of communications that permits recruitment, fund raising, and direct connections to sub-elements worldwide . . . Terrorists are seeking more lethal weaponry to include: chemical, biological, radiological, and even nuclear components with which to perpetrate more sensational attacks . . . Three Iran, & Sudan of the seven recognized state-sponsors of terrorism are within this potentially volatile area, and the Taliban regime in Afghanistan has been sanctioned by the UN Security Council for its harboring of Osama bin Laden. Nearly one half of the 28 recognized terrorist organizations have operational sites within the region. Afghanistan has emerged as a catalyst for regional instability offering sanctuary, support, and training facilities to a growing number of extremist elements.
Personal life.
Zinni holds positions on several boards of directors of major U.S. corporations. In addition, he has held academic positions that include the Stanley Chair in Ethics at the Virginia Military Institute, the Nimitz Chair at the University of California, Berkeley, the Hofheimer Chair at the Joint Forces Staff College, and the Harriman Professorship of Government and membership on the board of the Reves Center for International Studies at the College of William and Mary. He has worked as Chairman of the Board of the Middle East Institute, with the University of California's Institute on Global Conflict and Cooperation and the Henry Dunant Centre for humanitarian dialogue in Geneva. He is also a Distinguished Advisor at the Center for Strategic and International Studies and a member of the Council on Foreign Relations. He was the Executive Vice President for Dyncorp International from July 18, 2007 to the end of 2008. He served on the Board of Directors of Dyncorp International prior to that position.
He serves or has served on the Board of Trustees of the National Constitution Center in Philadelphia, which is a museum dedicated to the U.S. Constitution.
In April 2004, Zinni gave a lecture entitled "From the Battlefield to the Negotiating Table: Preventing Deadly Conflict" at the University of San Diego's Joan B. Kroc Institute for Peace & Justice Distinguished Lecture Series.
In 2004, Zinni was named in a "New York Times" investigative report by Diana B. Henriques as being among the "retired or former military people" recruited to the corporate boards and sales forces of investment firms engaged in deceptive marketing of financial instruments aimed at military veterans in order to lend them credibility. The investment firm that had recruited Zinni, First Command Financial Planning, Inc., responded in written comments to a subsequent United States House of Representatives investigation that "It would be unfortunate if anyone inferred that these honorable individuals would take any action or support any organization that did not act in the best interests of service members." The U.S. Securities and Exchange Commission (SEC) and National Association of Securities Dealers (NASD) subsequently concluded that First Command "willfully violated the Securities Act of 1933 Section 17(a)(2) dealing with inter-state fraud." In particular, the SEC concluded that First Command had sold mutual fund investments to veterans termed "systematic plans" which had very high sales charges termed "front-end sales loads", "by, in part, making misleading statements and omissions concerning, among other things: (a) comparisons between the systematic plan and other mutual fund investments; (b) the availability of the Thrift Savings Plan ("TSP"), which offers military investors many of the features of a systematic plan at lower costs; and (c) the efficacy of the front-end sales load in ensuring that investors remain committed to the systematic plan." In December 2004, First Command entered into a $12 million settlement with the SEC and NASD without admitting guilt.
In 2006, Zinni argued that more troops were needed in Iraq in the context of preventing the then-budding civil war.
In 2007, he worked on a report entitled "National Security and the Threat of Climate Change" with 11 other retired military commanders. The report stated that global warming would act as a threat multiplier to global conflict.
General Zinni is also a "Distinguished Military Fellow" for the Center for Defense Information, a part of the World Security Institute.
In 2009, Zinni reported that he had been offered and accepted the post of United States Ambassador to Iraq for the Barack Obama administration, but that the appointment had been subsequently withdrawn without explanation. The administration's final choice for the ambassadorship was Christopher R. Hill.
On June 26, 2009, General Anthony (Tony) Zinni (USMC ret.), then a member of the BAE Systems, Inc. Board, has been appointed Chairman of the BAE Systems, Inc. Board and, pending appointment of a permanent successor to Walt Havenstein, Acting President and CEO of BAE Systems, Inc. Tony will also join the BAE Systems Executive Committee in his capacity as Acting President and CEO of BAE Systems, Inc.
General Zinni also serves or has served on the board of Kaseman which has teamed up with Blackwater to pursue security work for the State Department.
Since 2011, Anthony Zinni is a member of the board of the Peace Research Endowment.
Political involvement.
An effort to get him to run for the U.S. Senate has stalled indefinitely, Zinni having said he will never run for office. He says his decision to endorse President George W. Bush in 2000 was a mistake, and in 2003, indicated that he plans to avoid politics in the future. However, on March 3, 2006, Zinni joined fellow former United States Marines General Joseph P. Hoar, Lt. General Greg Newbold, Lt. General Frank Petersen, and Congressman Jack Murtha in endorsing fellow former U.S. Marine and Secretary of the Navy Jim Webb for U.S. Senate in Virginia. Zinni had been floated as a possible Vice Presidential running mate of Barack Obama, the 2008 Presidential nominee of the Democratic Party.
Awards and decorations.
Zinni's decorations include the following:
In addition to his U.S. military decorations, Zinni holds decorations from France, Italy, Bahrain, Egypt, Yemen, Vietnam, and Kuwait.
His civilian awards include the Papal Gold Cross of Honor, the Union League's Abraham Lincoln Award, the Italic Studies Institute's Global Peace Award, the Distinguished Sea Service Award from the Naval Order of the United States, the Eisenhower Distinguished Service Award from the Veterans of Foreign Wars, The Chapman Award from the Marine Corps University Foundation, the Penn Club Award, the St. Thomas of Villanova Alumni Medal, the George P. Shultz Award for Public Service from the U.S. Department of State, and UNICO National's Grand Patriot Award.

</doc>
<doc id="46227" url="https://en.wikipedia.org/wiki?curid=46227" title="Focus on the Family">
Focus on the Family

Focus on the Family (FOTF or FotF) is an American Christian conservative organization founded in 1977 by psychologist James Dobson, based in Colorado Springs, Colorado. It is active in promoting an interdenominational effort toward its socially conservative views on public policy. Focus on the Family is one of a number of evangelical parachurch organizations that rose to prominence in the 1980s.
Focus on the Family's stated mission is "nurturing and defending the God-ordained institution of the family and promoting biblical truths worldwide." It promotes abstinence-only sexual education; creationism; adoption by married, opposite-sex parents; school prayer; and traditional gender roles. It opposes abortion; divorce; gambling; LGBT rights, particularly LGBT adoption and same-sex marriage; pornography; pre-marital sex; and substance abuse. Psychologists, psychiatrists, and social scientists have criticized Focus on the Family for trying to misrepresent their research to bolster FOTF's fundamentalist political agenda and ideology.
The core promotional activities of the organization include a daily radio broadcast by its president Jim Daly and his colleagues, providing free resources according to Focus on the Family views, and publishing magazines, videos, and audio recordings. The organization also produces programs for targeted audiences, such as "Adventures in Odyssey" for children, dramas, and "Family Minute".
History and organization.
In March 2005, Hodel retired and Jim Daly, formerly the Vice President in charge of Focus on the Family's International Division, assumed the role of president and chief executive officer.
In November 2008, the organization announced that it was eliminating 202 jobs, representing 18 percent of its workforce. The organization also cut its budget from $160 million in fiscal 2008 to $138 million for fiscal 2009.
In February 2009, Dobson resigned his chairmanship, He left Focus on the Family in early 2010, and subsequently founded Family Talk as a non-profit organization and launched a new broadcast that began airing nationally on May 3, 2010. He is no longer affiliated with Focus on the Family.
Ministries.
Marriage and family.
Focus on the Family sees its primary ministry as the strengthening of what it considers traditional marriages and families, based on what it sees as a fundamentalist view of Biblical teachings. The group strongly opposes same-sex marriage. Their website offers online tracts on topics regarding marriage and parenting.
Love Won Out.
Focus on the Family formed Love Won Out, an ex-gay ministry, in 1998 and sold it to Exodus International in 2009. (Exodus ceased activities in June 2013, issuing a statement which repudiated its aims and apologized for the harm their pursuit has caused to LGBT people.)
Wait No More.
Focus on the Family's Wait No More ministry works with adoption agencies, church leaders and ministry partners to recruit families to adopt children from foster care. The program co-sponsors several adoption conferences throughout the country each year. Since November 2008, more than 2,700 families have started the adoption process through Wait No More. In Colorado, the number of children waiting for adoption dropped from about 800 to 350, due in-part to the efforts of Wait No More. Focus on the Family's efforts to encourage adoption among Christian families is part of a larger effort by Evangelicals to, in their perception, live out what they see as the "biblical mandate" to help children. Focus on the Family supports laws to prevent couples from adopting who are cohabiting together outside of marriage as well as homosexual couples.
Option Ultrasound Program.
Focus on the Family's Option Ultrasound Program (OUP) provides grants to qualifying crisis pregnancy centers to cover 80 percent of the cost of an ultrasound machine or sonography training. As of October 31, 2014, the program has provided 655 grants to centers in all 50 states and Bucharest, Romania. Focus on the Family began OUP in 2004 with the goal of convincing women not to have abortions. FOTF officials said that ultrasound services help a woman better understand her pregnancy and baby's development, creating an important "bonding opportunity" between "mother and unborn child".
The Option Ultrasound Program reported in 2014 that it has helped prevent more than 270,000 abortions since 2004. A study released in February 2012 shows that ultrasounds do not have a direct impact on an abortion decision. In 2011, FOTF President Jim Daly announced that while FOTF will continue to fight for the overturn of Roe v. Wade, in the meantime he would like to work with pro-choice groups like Planned Parenthood who state they want to make abortion "safe, legal and rare" towards the shared goal of making abortion less common. Rep. Michele Bachmann (R-Minn.) introduced a sonogram bill in 2011 and – citing Focus on the Family – told Congress that "78 percent of women who see and hear the fetal heartbeat choose life." She was later corrected by Focus on the Family, which released a statement saying they did not release such data.
Radio Theatre.
Focus on The Family Radio Theatre is a series of audio dramas adapting classic literature, mystery mini-series and biographical productions, extending its reach to the mainstream as well as the Christian audience. The endeavor began through the efforts of "Adventures in Odyssey" producers Dave Arnold and Paul McCusker, along with casting director Philip Glassborow based in England.
"Radio Theatre" began in 1996 with a 90-minute radio drama based on Charles Dickens' "A Christmas Carol", which was produced and aired as a broadcast special. It was aired on over 1300 outlets, 325 of which were general market stations. It has since become a perennial on some of those stations.
The drama continued with historical biographies of Squanto ("The Legend of Squanto"), Jesus ("The Luke Reports") and Dietrich Bonhoeffer ("Bonhoeffer: The Cost of Freedom").
Over several years beginning in 1998 Focus on the Family Radio Theater released an audio dramatization of C. S. Lewis' epic novel series "The Chronicles of Narnia", with David Suchet providing the voice of Aslan, and over 100 English actors rounding out the cast. Lewis' stepson, Douglas Gresham, serves as host—sharing his personal stories at the beginning of each audio drama. A full box set of all 7 plays in one volume was released in the mid-2000s.
Radio Theatre also released an original miniseries, the "Father Gilbert Mysteries", which tells of the spiritual mysteries encountered by Louis Gilbert, a cop-turned-Anglican-priest, who lives in Stonebridge, Sussex, and ministers to the people of the town from St. Mark's Church. Nine episodes have been produced in four volumes available on cassette and CD.
FOTF also produces a children's radio drama entitled "Adventures in Odyssey". It began in 1987 as Family Portraits, starring John Avery Whittaker (aka "Whit"). It was renamed "Odyssey USA" in November 1987 and took on its present name, "Adventures in Odyssey", in April 1988.
FOTF also produced a radio miniseries based on their videos, "The Last Chance Detectives".
In 2009, FOTF's Radio Theatre produced an audio drama of C.S. Lewis' "The Screwtape Letters", starring Andy Serkis (who played Gollum from the "Lord of the Rings" movies) as the voice of Screwtape. The audio drama was also accompanied by the release of www.screwtape.com, the only site authorized by the C.S. Lewis estate to represent "The Screwtape Letters".
Boundless.org.
Boundless.org is Focus on the Family's website for young adults featuring articles, a blog, a podcast, and a conference. The website covers topics such as singleness, dating, relationships, popular culture, career and sex. Boundless.org recommends online dating as a means for Christian singles to find potential spouses.
Day of Dialogue.
The Day of Dialogue is a student-led event which takes place April 16. Founders describe the goal of the event, created in opposition to the anti-bullying and anti-homophobic Day of Silence, as "encouraging honest and respectful conversation among students about God's design for sexuality." It was previously known as the Day of Truth and was founded by the Alliance Defense Fund in 2005.
National Day of Prayer.
The National Day of Prayer Task Force is an American evangelical conservative Christian non-profit organization which organizes, coordinates, and presides over Evangelical Christian religious observances each year on the National Day of Prayer. The website of the NDP Task Force states that "its business affairs are separate" from those of Focus on the Family, but also that "between 1990 and 1993, Focus on the Family did provide grants in support of the NDP Task Force" and that "Focus on the Family is compensated for services rendered." Shirley Dobson, wife of James Dobson, has been chairwoman of the NDP Task Force since 1991.
Other ministries.
Focus on the Family has a number of additional ministries. Many are aimed at specific demographics including teenage boys and girls, children, college students, families, young adults, parents, while others are aimed at specific concerns, such as sexual problems, entertainment, and politics. Many have their own regular publications.
Political positions and activities.
Focus on the Family's 501(c)(3) status prevents them from advocating any individual political candidate. Focus on the Family's magazine "Citizen" is exclusively devoted to cultural and public policy issues. FOTF also has an affiliated group, CitizenLink, though the two groups are legally separate. As a 501(c)(4) social welfare group, CitizenLink has fewer political lobbying restrictions. FOTF's revenue in 2012 was USD $90.5 million, and that of CitizenLink was USD $8 million.
Focus on the Family supports teaching of what it considers to be traditional "family values". It supports student-led and initiated prayer and supports the practice of corporal punishment. It strongly opposes LGBT rights, abortion, pornography, gambling, and pre-marital and extramarital sexual activity. Focus on the Family also embraces and reflects the wider political agenda of its audience, for instance promoting a religiously-centered conception of American identity and the support of Israel.
Focus on the Family maintains a strong pro-life stand against abortion, and provides grant funding and medical training to assist crisis pregnancy centers (CPCs; also known as pregnancy resource centers) in obtaining ultrasound machines. According to the organization, this funding, which has allowed CPCs to provide pregnant women with live sonogram images of the developing fetus, has led directly to the birth of over 1500 babies who would have otherwise been aborted. The organization has been staunchly opposed to public funding for elective abortions.
Focus on the Family broadcasts an eponymous national talk radio program. The program has a range of themes, such as fundamentalist Christian-oriented assistance for victims of rape or child abuse; parenting difficulties; child adoption; husband/wife roles; family history and traditions; struggles with gambling, pornography, alcohol, and drugs.
Focus on the Family has been a prominent supporter of the pseudoscience of intelligent design, publishing pro-intelligent design articles in its "Citizen" magazine and selling intelligent design videos on its website. Focus on the Family co-published the intelligent design videotape "Unlocking the Mystery of Life" with the Discovery Institute, hub of the intelligent design movement. Focus on the Family employee Mark Hartwig is also a fellow of the Discovery Institute's Center for Science and Culture, a connection which has helped to publicize intelligent design extensively. Focus on the Family's Family.org is a significant online resource for intelligent design articles.
2008 Presidential campaign.
In the 2008 U.S. presidential election, Focus on the Family shifted from support of Mike Huckabee to not supporting any candidate, to finally accepting the Republican ticket once Sarah Palin was added to the ticket. Prior to the election, a television and letter campaign was launched predicting terrorist attacks in four U.S. cities and equating the U.S. with Nazi Germany. This publicity was condemned by the Anti Defamation League. Within a month before the general election, Focus on the Family began distributing a 16-page letter titled "Letter from 2012 in Obama's America", which describes an imagined American future in which "many of our freedoms have been taken away by a liberal Supreme Court of the United States and a majority of Democrats in both the House of Representatives and the Senate." According to "USA Today", the letter "is part of an escalation in rhetoric from Christian right activists" trying to paint Democratic Party presidential nominee Senator Barack Obama in a negative light.
Focus on the Family Action supported Senator Saxby Chambliss (R-Ga.) in his successful December 2, 2008, runoff election win. The organization, according to the "Colorado Independent", donated $35,310 in radio ads to the Chambliss runoff campaign effort. As the "Independent" reports, the Focus-sponsored ads were aired in about a dozen Georgia markets. The commercials were produced in the weeks after Focus laid off 202 employees – some 20 percent of its workforce – because of the national economic crisis.
Opposition to same-sex marriage.
Focus on the Family works to preserve its interpretation of biblical ideals of marriage and parenthood, taking a strong stance against LGBT rights, including same-sex marriage. Dobson expressed great concern for the institution of marriage in a 2003 letter to the Christian community. In reference to the same-sex marriage movement, Dobson says that the institution of marriage "…is about to descend into a state of turmoil unlike any other in human history." Focus on the Family believes that marriage should be defined as only being between a man and a woman. Dobson supported the failed Federal Marriage Amendment, which would have defined marriage as a union between one man and one woman, preventing courts and state legislatures from challenging this definition.
In the same letter, Dobson says that traditional marriage is the cornerstone of society, and he states that the goal of the gay and lesbian movement is not to redefine marriage but to destroy the institution itself. "Most gays and lesbians do not want to marry each other…the intention here is to destroy marriage altogether." Dobson argues that, without the institution of marriage, everyone would enjoy the benefits of marriage without limiting the number of partners or their gender. Focus on the Family views allowing same-sex marriage as "…a stepping-stone on the road to eliminating all societal restrictions on marriage and sexuality."
Focus on the Family asserts that the Bible lays out the correct plan for marriage and family. Dobson says that "God created Eve to complement Adam physically, spiritually, and emotionally". Dobson also uses the biblical figure Paul to affirm his views on marriage. He states that Paul maintained that men and women mutually complete each other, and to exchange a "natural relationship for an unnatural one is sinful".
In reference to same-sex marriage and same-sex couples with children, Dobson states, "Same-sex relationships undermine the future generation's understanding of the fundamental principles of marriage, parenthood, and gender." He also stated that the alleged destruction of what it considers to be the traditional family by permitting same-sex marriage will lead to "unstable homes for children".
Focus on the Family became more active in the same-sex marriage opposition movement after the Supreme Court of Canada declared that restricting marriage to opposite-sex couples is a violation of the Canadian Charter of Rights and Freedoms in 2003.
Dobson spoke at the 2004 rally against gay marriage called Mayday for Marriage. It was here for the first time that he endorsed a presidential candidate, George W. Bush. Here he denounced the Supreme Court rulings in favor of gay rights, and he urged rally participants to get out and vote so that the battle against gay rights could be won in the Senate.
In an interview with Christianity Today magazine, Dobson also explained that he was not in favor of civil unions. He stated that civil unions are just same-sex marriage under a different name. The main priority of the opposing same-sex marriage movement is to define marriage on the federal level as between a man and a woman and combat the passage of civil unions later.
Civil rights advocacy groups identify Focus on the Family as a major opponent of gay rights. The Southern Poverty Law Center, a civil rights and hate group monitoring organization, described Focus on the Family as one of a "dozen major groups help drive the religious right's anti-gay crusade".
Focus on the Family is a member of ProtectMarriage.com, a coalition formed to sponsor California Proposition 8, a ballot initiative to restrict marriage to opposite-sex couples, which passed in 2008, but was subsequently struck down as being unconstitutional by a federal court in Perry v. Schwarzenegger, with the ruling currently stayed by the 9th Circuit Court.
Misrepresentation of research.
Social scientists have criticized Focus on the Family for misrepresenting their research in order to bolster its own perspective. Researcher Judith Stacey whose work Focus on the Family used to claim that gays and lesbians do not make good parents, said that the claim was "a direct misrepresentation of the research." She elaborated, "Whenever you hear Focus on the Family, legislators or lawyers say, 'Studies prove that children do better in families with a mother and a father,' they are referring to studies which compare two-parent heterosexual households to single-parent households. The studies they are talking about do not cite research on families headed by gay and lesbian couples." FOTF claimed that Stacey's allegation was without merit and that their position is that the best interests of children are served when there is a father and a mother. "We haven't said anything about sexual orientation" said Glenn Stanton.
James Dobson cited the research of Kyle Pruett and Carol Gilligan in a "Time Magazine" guest article in the service of a claim that two women cannot raise a child; upon finding out that her work had been used in this way, Gilligan wrote a letter to Dobson asking him to apologize and to cease and desist from citing her work, describing herself as "mortified to learn that you had distorted my work...Not only did you take my research out of context, you did so without my knowledge to support discriminatory goals that I do not agree with...there is nothing in my research that would lead you to draw the stated conclusions you did in the "Time" article." Pruett wrote a similar letter, in which he said that Dobson "cherry-picked a phrase to shore up highly (in my view) discriminatory purposes. This practice is condemned in real science, common though it may be in pseudo-science circles. There is nothing in my longitudinal research or any of my writings to support such conclusions", and asked that FOTF not cite him again without permission.
After Elizabeth Saewyc's research on teen suicide was used by Focus on the Family to promote the pseudoscience of conversion therapy she said that "the research has been hijacked for somebody's political purposes or ideological purposes and that's worrisome", and that research in fact linked the suicide rate among LGBT teens to harassment, discrimination, and closeting. Other scientists who have criticized Focus on the Family for misrepresenting their findings include Robert Spitzer, Gary Remafedi and Angela Phillips.
Football advertisements.
In 2010, Focus on the Family bought ad time during Super Bowl XLIV to air a commercial featuring Heisman Trophy winning Florida Gators quarterback Tim Tebow and his mother, Pam. Prior to becoming pregnant with Tim, Pam had contracted amoebic dysentery and fell into a coma. She discovered she was pregnant while recovering. Because of the medications used to treat her, the fetus experienced a severe placental abruption. Doctors expected a stillbirth and recommended an abortion. The Tebows decided against it, citing their strong faith. In the ad, Pam described Tim as a "miracle baby" who "almost didn't make it into this world", and further elaborated that "with all our family's been through, we have to be tough" (after which Pam was promptly tackled by Tim). The ad itself made no reference to abortion or Christianity, and directed viewers to the organization's website.
Women's rights groups asked CBS not to air the then-unseen ad, arguing that it was divisive. Planned Parenthood released a video response of its own featuring fellow NFL player Sean James. The claim that Tebow's family chose not to perform an abortion was also widely criticized; critics felt that the claim was implausible because it would be unlikely for doctors to recommend the procedure because abortion is illegal in the Philippines. CBS's decision to run the ad was also criticized for deviating from its past policy to reject advocacy-type ads during the Super Bowl, including ads by left-leaning groups such as PETA, MoveOn.org and the United Church of Christ (which wanted to run an ad that was pro-same-sex marriage). However, CBS stated that "we have for some time moderated our approach to advocacy submissions after it became apparent that our stance did not reflect public sentiment or industry norms on the issue."
Focus on the Family produced another commercial which ran during the second quarter of the January 14, 2012 Denver Broncos-New England Patriots AFC Divisional Playoff broadcast on CBS, featuring children reciting the Bible verse John 3:16. The game, given the months of preceding hype and media exposure for Tim Tebow (who now played for the Broncos), was seen by more than 30 million viewers, making it the most-watched AFC Divisional Playoff in more than a decade. The ad did not generate nearly the amount of controversy that surrounded the Super Bowl commercial. It did get some national media attention, though, with Jim Daly telling "USA Today" its purpose was to "help everyone understand some numbers are more important than the ones on the scoreboard."
Recognitions and awards.
In 2008, Dobson's "Focus on the Family" program was nominated for induction into the National Radio Hall of Fame. Nominations were made by the 157 members of the Hall of Fame and voting on inductees was handed over to the public using online voting. The nomination drew the ire of gay rights activists, who launched efforts to have the program removed from the nominee list and to vote for other nominees to prevent "Focus on the Family" from winning. However, on July 18, 2008, it was announced that the program had won and would be inducted into the Radio Hall of Fame in a ceremony on November 8, 2008. Truth Wins Out, a gay rights group, protested the ceremony with over 300 protesters.
International associates and regional offices.
New Zealand.
"Focus on the Family New Zealand" is an organisation promoting a conservative Christian ideology. It has a similar agenda to the Focus on the Family organisation in the United States. Focus on the Family supported a Citizens Initiated Referendum on the repeal of section 59 of the Crimes Act 1961.
Other countries.
Controversy.
The Singapore branch of FotF came under criticism in October 2014 over allegations of sexism and promoting gender stereotypes during their workshops on managing relationships for junior college students. The workshop received a complaint from both a Hwa Chong Junior College student, as well as negative feedback from the college management as being 'ineffective' and will stop by the end of the year.
Headquarters.
The Focus on the Family headquarters is a four building, complex located off of Interstate 25 in northern Colorado Springs, Colorado, with its own ZIP code (80995). The buildings consist of the Administration building, International building, Welcome Center and Operations building (currently unused), and totals 526,070 square feet.
Focus on the Family moved to its current headquarters from Pomona, California, in 1993, with 1200 employees. In 2002, the number of employees peaked at 1,400. By September 2011, after a number of years of layoffs, they had 650 employees remaining. Christopher Ott of "Salon" said in 1998 that the FOTF campus has "handsome new brick buildings, professional landscaping and even its own traffic signs" and that "The buildings and grounds are well-maintained and comfortable. If there is any ostentatious or corrupt influence here, it is nowhere in sight."
While visiting the Focus on the Family complex, a couple had asked the staff if handling the sightseers in the main building was a distraction. The staff told the couple that it was a distraction; afterwards the couple donated $4 million to have a welcome center built. A visiting family donated of wood trim from the family's Pennsylvania lumber business so FOTF could build its administration building. As of 1998, James Dobson, in his welcome center film, compares his decision to build the headquarters in Colorado Springs to the founding of the temple in Jerusalem.

</doc>
<doc id="46228" url="https://en.wikipedia.org/wiki?curid=46228" title="Henry Fonda">
Henry Fonda

Henry Jaynes Fonda (May 16, 1905 – August 12, 1982) was a celebrated American film and stage actor with a career spanning more than five decades.
Fonda made his mark early as a Broadway actor. He also appeared in 1938 in plays performed in White Plains, New York, with Joan Tompkins. He made his Hollywood debut in 1935, and his career gained momentum after his Academy Award-nominated performance as Tom Joad in "The Grapes of Wrath", a 1940 adaptation of John Steinbeck's novel about an Oklahoma family who moved west during the Dust Bowl. Throughout six decades in Hollywood, Fonda cultivated a strong, appealing screen image in such classics as "The Ox-Bow Incident", "Mister Roberts" and "12 Angry Men". Later, Fonda moved both toward darker epics such as Sergio Leone's "Once Upon a Time in the West" and lighter roles in family comedies such as "Yours, Mine and Ours" with Lucille Ball, winning the Academy Award for Best Actor at the 54th Academy Awards for the movie "On Golden Pond", his final film role.
Fonda was the patriarch of a family of famous actors, including daughter Jane Fonda, son Peter Fonda, granddaughter Bridget Fonda, and grandson Troy Garity. His family and close friends called him "Hank". In 1999, he was named the sixth-Greatest Male Star of All Time by the American Film Institute.
Family history and early life.
Fonda's ancestors from Genoa, Italy, migrated to the Netherlands in the 15th century. In 1642, a branch of the Fonda family immigrated to the Dutch colony of New Netherland on the East Coast of North America. They were among the first Dutch population to settle in what is now upstate New York, establishing the town of Fonda, New York. By 1888, many of their descendants had relocated to Nebraska.
Henry Fonda was born in Grand Island, Nebraska, to advertising-printing jobber William Brace Fonda, and his wife, Elma Herberta (née Jaynes), in the second year of their marriage.
Fonda was brought up as a Christian Scientist, though he was baptized an Episcopalian at St. Stephen's Episcopal Church in Grand Island. He said, "My whole damn family was nice." They were a close family and highly supportive, especially in health matters, as they avoided doctors due to their religion. Despite having a religious background, he later became an agnostic. Fonda was a bashful, short boy who tended to avoid girls, except his sisters, and was a good skater, swimmer, and runner. He worked part-time in his father's print plant and imagined a possible career as a journalist. Later, he worked after school for the phone company. He also enjoyed drawing. Fonda was active in the Boy Scouts of America; Teichmann reports that he reached the rank of Eagle Scout. When he was about 14, his father took him to observe the lynching, of a young black man accused of rape. This enraged the young Fonda and he kept a keen awareness of prejudice for the rest of his life. By his senior year in high school, Fonda had grown to more than six feet tall, but remained shy. He attended the University of Minnesota, where he majored in journalism, but he did not graduate. He took a job with the Retail Credit Company.
Career.
Early stage work.
At age 20, Fonda started his acting career at the Omaha Community Playhouse, when his mother's friend Dodie Brando (mother of Marlon Brando) recommended that he try out for a juvenile part in "You and I", in which he was cast as Ricky. He was fascinated by the stage, learning everything from set construction to stage production, and embarrassed by his acting ability. When he received the lead in "Merton of the Movies", he realized the beauty of acting as a profession, as it allowed him to deflect attention from his own tongue-tied personality and create stage characters relying on someone else's scripted words. Fonda decided to quit his job and go East in 1928 to seek his fortune.
He arrived on Cape Cod and played a minor role at the Cape Playhouse in Dennis, Massachusetts. A friend took him to Falmouth, MA where he joined and quickly became a valued member of the University Players, an intercollegiate summer stock company. There he worked with Margaret Sullavan, his future wife. James Stewart joined the Players a few months after Fonda left, though they were soon to become lifelong friends. Fonda left the Players at the end of their 1931-1932 season after appearing in his first professional role in "The Jest", by Sem Benelli. Joshua Logan, a young sophomore at Princeton who had been double-cast in the show, gave Fonda the part of Tornaquinci, "an elderly Italian man with a long white beard and even longer hair." Also in the cast of "The Jest" with Fonda and Logan were Bretaigne Windust, Kent Smith, and Eleanor Phelps.
The tall (6 ft, 1.5 in) and slim (160 lb) Fonda headed for New York City, to be with his now wife, Margaret Sullavan. The marriage was brief, but when James Stewart came to New York his luck changed. Getting contact information from Joshua Logan, Jimmy, as he was called, found Hank Fonda and these small town boys found they had a lot in common, as long as they didn't discuss politics. The two men became roommates and honed their skills on Broadway. Fonda appeared in theatrical productions from 1926 to 1934. They fared no better than many Americans in and out of work during the Great Depression, sometimes lacking enough money to take the subway.
Entering Hollywood.
Fonda got his first break in films when he was hired in 1935 as Janet Gaynor's leading man in 20th Century Fox's screen adaptation of "The Farmer Takes a Wife"; he reprised his role from the Broadway production of the same name, which had gained him critical recognition. Suddenly, Fonda was making $3,000 a week and dining with Hollywood stars such as Carole Lombard. Stewart soon followed him to Hollywood, and they roomed together again, in lodgings next door to Greta Garbo. In 1935, Fonda starred in the RKO film "I Dream Too Much" with the opera star Lily Pons. "The New York Times" announced him as "Henry Fonda, the most likable of the new crop of romantic juveniles."
Fonda's film career blossomed as he costarred with Sylvia Sidney and Fred MacMurray in "The Trail of the Lonesome Pine" (1936), the first Technicolor movie filmed outdoors.
He starred with ex-wife Margaret Sullavan in "The Moon's Our Home", and a short rekindling of their relationship led to a brief but temporary consideration of remarriage. Fonda got the nod for the lead role in "You Only Live Once" (1937), also costarring Sidney, and directed by Fritz Lang. He was a critical success opposite Bette Davis, who had picked him, in the film "Jezebel" (1938). This was followed by the title role in "Young Mr. Lincoln" (1939), his first collaboration with director John Ford, and that year he played Frank James in "Jesse James" (1939). Another 1939 film was "Drums Along the Mohawk", also directed by Ford.
Fonda's successes led Ford to recruit him to play Tom Joad in the film version of John Steinbeck's novel "The Grapes of Wrath" (1940). A reluctant Darryl Zanuck, who preferred Tyrone Power, insisted on Fonda's signing a seven-year contract with his studio, Twentieth Century-Fox. Fonda agreed, and was ultimately nominated for an Academy Award for his work in the 1940 film, which many consider to be his finest role. Fonda starred in "The Return of Frank James" (1940) with Gene Tierney. He then played opposite Barbara Stanwyck in Preston Sturges' "The Lady Eve" (1941), and again teamed with Tierney in the successful screwball comedy "Rings on Her Fingers" (1942). She was one of Fonda's favorite co-stars, and they appeared in three films together. He was acclaimed for his role in "The Ox-Bow Incident" (1943).
Fonda enlisted in the United States Navy to fight in World War II, saying, "I don't want to be in a fake war in a studio." Previously, Stewart and he had helped raise funds for the defense of Britain. Fonda served for three years, initially as a Quartermaster 3rd Class on the destroyer . He was later commissioned as a Lieutenant Junior Grade in Air Combat Intelligence in the Central Pacific and was awarded the Navy Presidential Unit Citation and the Bronze Star.
Postwar career.
After the war, Fonda took a break from movies and attended Hollywood parties and enjoyed civilian life. Stewart and Fonda would listen to records and invite Johnny Mercer, Hoagy Carmichael, Dinah Shore, and Nat King Cole over for music, with the latter giving the family piano lessons. Fonda played Wyatt Earp in "My Darling Clementine" (1946), which was directed by John Ford. Fonda did seven postwar films until his contract with Fox expired, the last being Otto Preminger's "Daisy Kenyon" (1947), opposite Joan Crawford. He starred in "The Fugitive" (1947), which was the first film of Ford's new production company, Argosy Pictures. In 1948, he appeared in a subsequent Argosy/Ford production, "Fort Apache", as a rigid Army colonel, along with John Wayne and Shirley Temple in her first adult role.
Refusing another long-term studio contract, Fonda returned to Broadway, wearing his own officer's cap to originate the title role in "Mister Roberts", a comedy about the Navy, where Fonda, a junior officer, wages a private war against the captain. He won a 1948 Tony Award for the part. Fonda followed that by reprising his performance in the national tour and with successful stage runs in "Point of No Return" and "The Caine Mutiny Court-Martial". After an eight-year absence from films, he starred in the 1955 film version of "Mister Roberts" with James Cagney, William Powell, and Jack Lemmon, continuing a pattern of bringing his acclaimed stage roles to life on the big screen. On the set of "Mister Roberts", Fonda came to blows with John Ford, who punched him during filming, and Fonda vowed never to work for the director again. While he kept that vow for years, Fonda spoke glowingly of Ford in Peter Bogdanovich's documentary "Directed by John Ford" and in a documentary on Ford's career alongside Ford and James Stewart. Fonda refused to participate until he learned that Ford had insisted on casting Fonda as the lead in the film version of "Mr. Roberts", reviving Fonda's film career after concentrating on the stage for years.
After "Mr. Roberts", Fonda next acted in Paramount Pictures's production of the Leo Tolstoy epic "War and Peace" (1956), in which he played Pierre Bezukhov opposite Audrey Hepburn; it took two years to shoot. Fonda worked with Alfred Hitchcock in 1956, playing a man falsely accused of robbery in "The Wrong Man"; the unusual semidocumentary work of Hitchcock's was based on an actual incident and partly filmed on location.
In 1957, Fonda made his first foray into production with "12 Angry Men", based on a teleplay and a script by Reginald Rose and directed by Sidney Lumet. The low-budget movie was completed in 17 days of filming, mostly in one claustrophobic jury room. It had a strong cast, including Jack Klugman, Lee J. Cobb, Martin Balsam, and E. G. Marshall. The intense film about 12 jurors deciding the fate of a young Puerto Rican man accused of murder was well received by critics worldwide. Fonda shared the Academy Award and Golden Globe nominations with co-producer Reginald Rose and won the 1958 BAFTA Award for Best Actor for his performance as "Juror #8", who with logic and persistence eventually sways all the jurors to an acquittal. Early on, the film drew poorly, but after winning critical acclaim and awards, it proved a success. In spite of the good outcome, Fonda vowed that he would never produce a movie again, fearing that failing as a producer might derail his acting career. After acting in the western movies "The Tin Star" (1957) and "Warlock" (1959), Fonda returned to the production seat for the NBC western television series "The Deputy" (1959–1961), in which he starred as Marshal Simon Fry. His co-stars were Allen Case and Read Morgan.
During the 1960s, Fonda performed in a number of war and western epics, including 1962's "The Longest Day" and the Cinerama production "How the West Was Won", 1965's "In Harm's Way", and "Battle of the Bulge". In the Cold War suspense film "Fail-Safe" (1964), Fonda played the President of the United States who tries to avert a nuclear holocaust through tense negotiations with the Soviets after American bombers are mistakenly ordered to attack the USSR. He also returned to more light-hearted cinema in "Spencer's Mountain" (1963), which was the inspiration for the TV series, "The Waltons".
Fonda appeared against type as the villain 'Frank' in 1968's "Once Upon a Time in the West". After initially turning down the role, he was convinced to accept it by actor Eli Wallach and director Sergio Leone, who flew from Italy to the United States to persuade him to take the part. Fonda had planned on wearing a pair of brown-colored contact lenses, but Leone preferred the paradox of contrasting close-up shots of Fonda's innocent-looking blue eyes with the vicious personality of the character Fonda played.
Fonda's relationship with Jimmy Stewart survived their disagreements over politics — Fonda was a liberal Democrat, and Stewart a conservative Republican. After a heated argument, they avoided talking politics with each other. The two men teamed up for 1968's "Firecreek", where Fonda again played the heavy. In 1970, Fonda and Stewart co-starred in the western "The Cheyenne Social Club", a minor film in which they humorously argued politics. They had first appeared together on film in "On Our Merry Way" (1948), a comedy which also starred William Demarest and Fred MacMurray and featured a grown-up Carl "Alfalfa" Switzer, who had acted as a child in "Our Gang".
Late career.
Despite approaching his seventies, Fonda continued to work in theater, television and film through the 1970s. In 1970, Fonda appeared in three films; the most successful was "The Cheyenne Social Club". The other two films were "Too Late the Hero", in which Fonda played a secondary role, and "There Was a Crooked Man", about Paris Pitman Jr. (played by Kirk Douglas) trying to escape from an Arizona prison.
Fonda returned to both foreign and television productions, which provided career sustenance through a decade in which many aging screen actors suffered waning careers. He starred in the ABC television series "The Smith Family" between 1971 and 1972. A TV-movie adaptation of John Steinbeck's novel, 1973's "The Red Pony", earned Fonda an Emmy nomination. After the unsuccessful Hollywood melodrama, "Ash Wednesday", he filmed three Italian productions released in 1973 and 1974. The most successful of these, "My Name is Nobody", presented Fonda in a rare comedic performance as an old gunslinger whose plans to retire are dampened by a "fan" of sorts.
Fonda continued stage acting throughout his last years, including several demanding roles in Broadway plays. He returned to Broadway in 1974 for the biographical drama, "Clarence Darrow", for which he was nominated for a Tony Award. Fonda's health had been deteriorating for years, but his first outward symptoms occurred after a performance of the play in April 1974, when he collapsed from exhaustion. After the appearance of a cardiac arrhythmia brought on by prostate cancer, he had a pacemaker installed following cancer surgery. Fonda returned to the play in 1975. After the run of a 1978 play, "First Monday of October", he took the advice of his doctors and quit plays, though he continued to star in films and television.
Fonda appeared in a revival of "The Time of Your Life" that opened in March 17, 1972, at the Huntington Hartford Theater in Los Angeles, where Fonda, Richard Dreyfuss, Gloria Grahame, Ron Thompson, Strother Martin, Jane Alexander, Lewis J. Stadlen, Richard X. Slattery, and Pepper Martin were among the cast with Edwin Sherin directing.
In 1976, Fonda appeared in several notable television productions, the first being "Collision Course", the story of the volatile relationship between President Harry Truman (E. G. Marshall) and General MacArthur (Fonda), produced by ABC. After an appearance in the acclaimed Showtime broadcast of "Almos' a Man", based on a story by Richard Wright, he starred in the epic NBC miniseries "Captains and Kings", based on Taylor Caldwell's novel. Three years later, he appeared in ABC's "", but the miniseries was overshadowed by its predecessor, "Roots". Also in 1976, Fonda starred in the World War II blockbuster "Midway".
Fonda finished the 1970s in a number of disaster films. The first of these was the 1977 Italian killer octopus thriller "Tentacoli" ("Tentacles") and
"Rollercoaster", in which Fonda appeared with Richard Widmark and a young Helen Hunt. He performed again with Widmark, Olivia de Havilland, Fred MacMurray, and José Ferrer in the killer bee action film "The Swarm". He also acted in the global disaster film "Meteor" (his second role as a sitting President of the United States after "Fail-Safe"), with Sean Connery, Natalie Wood, and Karl Malden, and the Canadian production "City on Fire", which also featured Shelley Winters and Ava Gardner. Fonda had a small role with his son, Peter, in "Wanda Nevada" (1979), with Brooke Shields.
As Fonda's health declined and he took longer breaks between filming, critics began to take notice of his extensive body of work. In 1979, he was inducted into the American Theater Hall of Fame for his achievements on Broadway. Lifetime Achievement awards from the Golden Globes and Academy Awards followed in 1980 and 1981, respectively.
Fonda continued to act into the early 1980s, though all but one of the productions in which he was featured before his death were for television. The television works included the critically acclaimed live performance of Preston Jones' "The Oldest Living Graduate" and the Emmy-nominated "Gideon's Trumpet" (co-starring Fay Wray in her last performance) about Clarence Gideon's fight to have the right to publicly funded legal counsel for the indigent.
"On Golden Pond" in 1981, the film adaptation of Ernest Thompson's play, marked one final professional and personal triumph for Fonda. Directed by Mark Rydell, the project provided unprecedented collaborations between Fonda and Katharine Hepburn, along with Fonda and his daughter, Jane. The elder Fonda played an emotionally brittle and distant father who becomes more accessible at the end of his life. Jane Fonda has said that elements of the story mimicked their real-life relationship, and helped them resolve certain issues. She bought the film rights in the hope that her father would play the role, and later described it as "a gift to my father that was so unbelievably successful."
Premiered in December 1981, the film was well received by critics, and after a limited release on December 4, "On Golden Pond" developed enough of an audience to be widely released on January 22. With 10 Academy Award nominations, the film earned nearly $120 million at the box office, becoming an unexpected blockbuster. In addition to wins for Hepburn (Best Actress), and Thompson (Screenplay), "On Golden Pond" brought Fonda his only Oscar - for Best Actor (he was the oldest recipient of the award; it also earned him a Golden Globe Best Actor award). Fonda was by that point too ill to attend the ceremony, and his daughter Jane accepted on his behalf. She said when accepting the award that her dad would probably quip, "Well, ain't I lucky." After Fonda's death, some film critics called this performance "his last and greatest role".
Fonda's final performance was in the 1981 television drama "Summer Solstice" with Myrna Loy. It was filmed after "On Golden Pond" had wrapped and Fonda was in rapidly declining health.
Personal life.
Marriages and children.
Fonda was married five times and had three children, one of them adopted. His marriage to Margaret Sullavan in 1931 soon ended in separation, which was finalized in a 1933 divorce.
In 1936, he married Frances Ford Seymour Brokaw, widow of a wealthy industrialist, George Tuttle Brokaw. The Brokaws had a daughter, Frances de Villers, nicknamed "Pan", who had been born soon after the Brokaws marriage in 1931.
Fonda met his future wife Frances at Denham Studios in England on the set of "Wings of the Morning", the first British picture to be filmed in Technicolor. They had two children, Jane (born December 21, 1937) and Peter (born February 23, 1940), both of whom became successful actors. They have each had Oscar nominations.
In August 1949, Fonda announced to Frances that he wanted a divorce so he could remarry; their 13 years of marriage had not been happy ones for him. Devastated by Fonda's confession, and plagued by emotional problems for many years, Frances went into the Austen Riggs Psychiatric Hospital in January 1950 for treatment. She committed suicide there on April 14. Before her death, she had written six notes to various individuals, but left no final message for her husband. Fonda quickly arranged a private funeral with only himself and his mother-in-law, Sophie Seymour, in attendance. Years later, Dr. Margaret Gibson, the psychiatrist who had treated Frances at Austen Riggs, described Henry Fonda: "He was a cold, self-absorbed person, a complete narcissist." 
Later in 1950, Fonda married Susan Blanchard, with whom he had been having an affair since sometime in 1948. She was 21 years old, the daughter of Australian-born interior designer Dorothy Hammerstein, and the step-daughter of Oscar Hammerstein II. Together, they adopted a daughter, Amy Fishman (born 1953). They divorced three years later. Blanchard was in awe of Fonda, and she described her role in the marriage as "a geisha", doing everything she could to please him, dealing with and solving problems he would not acknowledge.
In 1957, Fonda married the Italian baroness Afdera Franchetti They divorced in 1961. Soon after, in 1965, Fonda married Shirlee Mae Adams, and remained with her until his death in 1982.
Fonda's relationship with his children has been described as "emotionally distant". Fonda loathed displays of feeling in himself or others, and this was a consistent part of his character. Whenever he felt that his emotional wall was being breached, he had outbursts of anger, exhibiting a furious temper that terrified his family. In Peter Fonda's 1998 autobiography "Don't Tell Dad" (1998), he described how he was never sure how his father felt about him. He never volunteered to his father that he loved him until he was elderly, and Peter finally heard, "I love you, son." His daughter Jane rejected her father's friendships with Republican actors such as John Wayne and James Stewart. Their relationship became extremely strained as Jane Fonda became a left-wing activist.
Jane Fonda reported feeling detached from her father, especially during her early acting days. In 1958, she met Lee Strasberg while visiting her father at Malibu. The Fonda and Strasberg families were neighbors, and she had developed a friendship with Strasberg's daughter, Susan. Jane Fonda began studying acting with Strasberg, learning the techniques of "The Method" of which Strasberg was a renowned proponent. This proved to be a pivotal point in her career. As Jane Fonda developed her skill as an actress, she became frustrated with her father's talent that, to her, appeared a demonstration of effortless ability.
Politics.
Fonda was an ardent supporter of the Democratic Party and "an admirer" of U.S. President Franklin D. Roosevelt. In 1960, Fonda appeared in a campaign commercial for presidential candidate John F. Kennedy. The ad focused on Kennedy's naval service during World War II, specifically the famous PT-109 incident.
On acting.
In the late 1950s, when Jane Fonda asked her father how he prepared before going on stage, she was baffled by his answer, "I don't know, I stand there, I think about my wife, Afdera, I don't know."
The writer Al Aronowitz, while working on a profile of Jane Fonda for "The Saturday Evening Post" in the 1960s, asked Henry Fonda about method acting: "I can't articulate about the Method", he told me, "because I never studied it. I don't mean to suggest that I have any feelings one way or the other about it...I don't know what the Method is and I don't care what the Method is. Everybody's got a method. Everybody can't articulate about their method, and I can't, if I have a method—and Jane sometimes says that I use the Method, that is, the capital letter Method, without being aware of it. Maybe I do; it doesn't matter."
Aronowitz reported Jane saying, "My father can't articulate the way he works. He just can't do it. He's not even conscious of what he does, and it made him nervous for me to try to articulate what I was trying to do. And I sensed that immediately, so we did very little talking about it...he said, 'Shut up, I don't want to hear about it.' He didn't want me to tell him about it, you know. He wanted to make fun of it."
Death and legacy.
Fonda died at his Los Angeles home on August 12, 1982, from heart disease. Fonda's wife, Shirlee, his daughter Jane, and his son Peter were at his side when he died. He suffered from prostate cancer, but this did not directly cause his death and was noted only as a concurrent ailment on his death certificate.
Fonda requested that no funeral be held, and he was promptly cremated. President Ronald Reagan, a former actor himself, hailed Fonda as "a true professional dedicated to excellence in his craft. He graced the screen with a sincerity and accuracy which made him a legend."
Fonda is widely recognized as one of the Hollywood greats of the classic era. On the centenary of his birth, May 16, 2005, Turner Classic Movies (TCM) honored him with a marathon of his films. Also in May 2005, the United States Post Office released a 37-cent postage stamp with an artist's drawing of Fonda as part of their "Hollywood legends" series. The Fonda Theatre in Hollywood, originally known as the Carter DeHaven Music Box, was named for the actor in 1985 by the Nederlander Organization.
Filmography.
From the beginning of his career in 1935 through his last projects in 1981, Fonda appeared in 106 films, television programs, and shorts. Through the course of his career, he appeared in many critically acclaimed films, including such classics as "12 Angry Men" and "The Ox-Bow Incident". He was nominated for an Academy Award for Best Actor for his role in 1940's "The Grapes of Wrath" and won for his part in 1981's "On Golden Pond". Fonda made his mark in Westerns (which included his most villainous role as Frank in "Once Upon a Time in the West") and war films, and made frequent appearances in both television and foreign productions late in his career.

</doc>
<doc id="46229" url="https://en.wikipedia.org/wiki?curid=46229" title="Beyond the Fringe">
Beyond the Fringe

Beyond the Fringe was a British comedy stage revue written and performed by Peter Cook, Dudley Moore, Alan Bennett, and Jonathan Miller. It played in London's West End and then in America, both on tour and on New York's Broadway in the early 1960s. Hugely successful, it is widely regarded as seminal to the rise of satirical comedy in 1960s Britain.
The show.
The show was conceived in 1960 by an Oxford graduate, Robert Ponsonby, artistic director for the Edinburgh International Festival, with the idea of bringing together the best of revues by the Cambridge Footlights and The Oxford Revue, which had both transferred to Edinburgh for short runs in previous years. John Bassett, a graduate of Wadham College, Oxford, who was Ponsonby's assistant, recommended Dudley Moore, his jazz bandmate and a rising cabaret talent. Moore in turn recommended Alan Bennett, who had had a hit at Edinburgh a few years before. Bassett also chose Jonathan Miller, who had been a Footlights star in 1957. Miller recommended Cook.
Bennett and Miller were already pursuing careers in academia and medicine respectively, but Cook had an agent, having written a West End revue for Kenneth Williams. Cook's agent negotiated a higher weekly fee for him, but by the time the agent's fee was deducted Cook actually earned less than the others from the initial run.
The majority of the sketches were by Cook and were largely based on material written for other revues. Among the entirely new material were "The End of the World", "TVPM" and "The Great Train Robbery". Cook and Moore revived some of the sketches on their later television and stage shows, most famously the two-hander "One Leg Too Few".
The show's runs in Edinburgh and the provinces had a lukewarm response, but when the revue transferred to the Fortune Theatre in London, in a revised production by Donald Albery and William Donaldson, it became a sensation, thanks in some part to a favourable review by Kenneth Tynan.
In 1962 the show transferred to the John Golden Theatre in New York, with its original cast. President John F. Kennedy attended a performance on 10 February 1963. The show continued in New York, with most of the original cast, until 1964, when Paxton Whitehead replaced Miller, while the London version continued with a different cast until 1966.
Controversy.
The revue was widely considered to be ahead of its time, both in its unapologetic willingness to debunk figures of authority, and by virtue of its inherently surrealistic comedic vein. Humiliation of authority was something only previously delved into in "The Goon Show" and, arguably, "Hancock's Half Hour", with such parliamentarians as Sir Winston Churchill and Harold Macmillan coming under special scrutiny — although the BBC were predisposed to frown upon it. Macmillan — according to Cook — was not particularly fond of the slurred caricature and charade of senile forgetfulness (marked by a failure to pronounce 'Conservative Party' coherently) handed down on him in Cook's impersonation. Since "Beyond the Fringe" was not owned by the BBC, however, the quartet enjoyed relative carte blanche. The only protocol they were obliged to adhere to stipulated that their sketches be sent to the Lord Chamberlain for approval prior to performance (a requirement abolished in 1968). 
Most specifically, its lampooning of the British war effort in a sketch titled "The Aftermyth of the War" was scorned by some war veterans for its supposed insensitivity. (One British visitor to the Broadway performance was said to have stood up and shouted 'rotters!' at a sketch he found distasteful, before apparently sitting down again and enjoying the remainder of the show, while another, at the first performance in Edinburgh allegedly stood up and declared that the 'young bounders don't know the first thing about it!' and promptly left the auditorium.) In response to these negative audience reactions, the "Beyond the Fringe" team insisted that they were not ridiculing the efforts of those involved in the war, but were challenging the subsequent media portrayal of them.
Influence.
Many see "Beyond the Fringe" as the forerunner to British television programmes "That Was the Week That Was", "At Last the 1948 Show" and "Monty Python's Flying Circus".
As with the established comedy revue, it was a series of satirical sketches and musical pieces using a minimal set, looking at events of the day. It effectively represented the views and disappointments of the first generation of British people to grow up after World War II, and gave voice to a sense of the loss of national purpose with the end of the British Empire. Although all of the cast contributed material, the most often quoted pieces were those by Cook, many of which had appeared before in his Cambridge Footlights revues. The show broke new ground with Peter Cook's impression of then Prime Minister Harold Macmillan; on one occasion, this was performed with Macmillan in the audience, and Cook added an "ad lib" ridiculing Macmillan for turning up to watch. In 2006, Jonathan Miller recounted that the breach of decorum this represented was a source of embarrassment to both audience and performers.
The show is credited with giving many other performers the courage to be satirical and more improvisational in their manner, and broke the conventions of not lampooning the Royal Family or the government of the day. However, the show wasn't all that satirical, merely making fun of things — such as war films — though even this was a step forward in comedy. Shakespearean drama was another target of their comedy. There were also a number of musical items in the show, using Dudley Moore's music, most famously an arrangement of the Colonel Bogey March which resists Moore's repeated attempts to bring it to an end.
The show prefigured the Satire Boom of the 1960s. Without it, there might not have been either "That Was the Week That Was" or "Private Eye", the satirical magazine which originated at the same time, that partially survived due to financial support from Peter Cook, and that served as the model for the later American "Spy Magazine". Cook and Moore formed a comedy team and appeared in the popular television show "Not Only... But Also", and the 1967 film "Bedazzled". Cook also launched his club, The Establishment, around this time. Many of the members of Monty Python recall being inspired by "Beyond the Fringe".
The retrospective show "Before the Fringe", broadcast during the early years of BBC 2, took its title from this production. It consisted of performances of material that was popular in theatrical revue before the advent of "Beyond the Fringe".
International Success.
The show's success was not limited to the UK. In 1962, it also opened in South Africa. Next it arrived in the US. First the Broadway Company opened on October 27, 1962, then it was performed by the National Company in 1963. Subsequently, opening on October 8, 1964, the National Touring Company took it on a nationwide tour for six months as "Beyond the Fringe '65" under the auspices of Alexander H. Cohen, with the cast consisting of Robert Cessna, Donald Cullen, Joel Fabiani, and James Valentine. Slight changes were made to adapt the show for American audiences, for instance the opening number (discussing America) was retitled "Home Thoughts from Abroad".
The show was revived in slightly altered form in Los Angeles in 2000 and 2001 by Joseph Dunn's ReEstablishment Theater to critical acclaim.
Legacy.
All four original members of Beyond the Fringe feature prominently in the play "", by Chris Bartlett and Nick Awde, although performed by other actors. Appropriately, the comedy drama had a sellout run at the 2005 Edinburgh Festival Fringe before transferring to London's West End at The Venue, in 2006, in a version starring Kevin Bishop as Moore, Tom Goodman-Hill as Cook, Fergus Craig as Alan Bennett and Colin Hoult as Jonathan Miller. It subsequently embarked on a nationwide tour.

</doc>
<doc id="46230" url="https://en.wikipedia.org/wiki?curid=46230" title="James Dobson">
James Dobson

James Clayton "Jim" Dobson, Jr. (born April 21, 1936) is an American evangelical Christian author, psychologist, and founder in 1977 of Focus on the Family (FOTF), which he led until 2003. In the 1980s he was ranked as one of the most influential spokesmen for conservative social positions in American public life. Although never an ordained minister, he was called "the nation's most influential evangelical leader" by "Time" while "Slate" portrayed him as a successor to evangelical leaders Billy Graham, Jerry Falwell, and Pat Robertson.
He is no longer affiliated with Focus on the Family. Dobson founded Family Talk as a non-profit organization in 2010 and launched a new radio broadcast, "Family Talk with Dr. James Dobson", that began on May 3, 2010 on over 300 stations nationwide. As part of his former role in the organization, he produced "Focus on the Family", a daily radio program which according to the organization was broadcast in more than a dozen languages and on over 7,000 stations worldwide, and reportedly heard daily by more than 220 million people in 164 countries. "Focus on the Family" was also carried by about sixty U.S. television stations daily. He founded the Family Research Council in 1981.
Background.
James Dobson was born to Myrtle Georgia (née Dillingham) and James C. Dobson, Sr., on April 21, 1936, in Shreveport, Louisiana. From his earliest childhood, religion played a central part in his life. He once told a reporter that he learned to pray before he learned to talk, and says he gave his life to Jesus at the age of three, in response to an altar call by his father. He is the son, grandson, and great-grandson of Church of the Nazarene ministers, although he does not speak for the denomination in any capacity.
His father, James Dobson Sr. (1911–1977), never went to college. He was a traveling evangelist, chiefly in the southwest. The parents took their young son along to watch his father preach. Like most Nazarenes, they forbade dancing and going to movies. Young "Jimmie Lee" (as he was called) concentrated on his studies.
Dobson studied academic psychology, which most evangelical Christians in the 1950s and 1960s did not look upon favorably. He came to believe that he was being called to become a Christian counselor or perhaps a Christian psychologist. He attended Pasadena College (now Point Loma Nazarene University) as an undergraduate and served as captain of the school's tennis team. In 1967 Dobson received his doctorate in psychology from the University of Southern California; he served in the faculty of the university's Keck School of Medicine for 14 years. 
For a time, Dobson worked as an assistant to Paul Popenoe at the Institute of Family Relations, a marriage-counseling center, in Los Angeles.
Dobson arguably first became well-known with the publication of "Dare to Discipline" (1970), which encouraged parents to use corporal punishment in disciplining their children. Dobson's social and political opinions are widely read among many evangelical church congregations in the United States. 
Dobson publishes monthly bulletins, also called "Focus on the Family", which are dispensed as inserts in some Sunday church-service bulletins.
Dobson interviewed serial killer Ted Bundy on-camera the day before Bundy's execution on January 24, 1989. The interview became controversial because Bundy was given an opportunity to attempt to explain his actions (the rape and murder of 30 young women). Bundy claimed in the interview (in a reversal of his previous stance) that violent pornography played a significant role in molding and crystallizing his fantasies. In May 1989, during an interview with John Tanner, a Republican Florida prosecutor, Dobson called for Bundy to be forgiven. The Bundy tapes gave Focus on the Family revenues of over $1 million, $600,000 of which it donated to anti-pornography groups and to anti-abortion groups.
Dobson stepped down as President and CEO of Focus on the Family in 2003, and resigned from the position of chairman of the board in February 2009. Dobson explained his departure as a result of "significant philosophical differences" with successor Jim Daly.
In 2010 Dobson founded Family Talk,
a non-profit organization that produces his radio program, “Dr. James Dobson’s Family Talk”.
Dobson frequently appears as a guest on the Fox News Channel.
Personal life.
Dobson married his wife, Shirley Deere, on August 26, 1960; they have two children, Danae and Ryan. Ryan Dobson (born in California in 1970), who graduated from Biola University in La Mirada, California, is a public speaker, specializing on issues relating to youth and the pro-life movement. He was adopted by the Dobsons and is an ardent supporter of adoption, especially adoption of troubled children.
Degrees, positions, and awards.
Dobson attended Point Loma Nazarene University, where he was team captain of the tennis team, most valuable player in 1956 and 1958, and later returned to coach in 1968-1969. Dobson earned a PhD in child development from the University of Southern California in 1967. He was an Associate Clinical Professor of Pediatrics at the University of Southern California School of Medicine for 14 years. He spent 17 years on the staff of the Children's Hospital of Los Angeles in the Division of Child Development and Medical Genetics. Dobson is a licensed psychologist in the State of California.
At the invitation of Presidents and Attorneys General, Dobson has also served on government advisory panels and testified at several government hearings. He has been given the "Layman of the Year" award by the National Association of Evangelicals in 1982, "The Children's Friend" honor by Childhelp USA (an advocate agency against child abuse) in 1987, and the Humanitarian Award by the California Psychological Association in 1988. In 2005, Dobson received an honorary doctorate (his 16th) from Indiana Wesleyan University and was inducted into IWU's "Society of World Changers", while speaking at the university's Academic Convocation.
In 2008, Dobson's "Focus on the Family" program was nominated for induction into the National Radio Hall of Fame. Nominations were made by the 157 members of the Hall of Fame and voting on inductees was handed over to the public using online voting. The nomination drew the ire of gay rights activists, who launched efforts to have the program removed from the nominee list and to vote for other nominees to prevent it from being approved. However, the program garnered enough votes and was subsequently inducted into the Radio Hall of Fame.
Social views.
Views on marriage.
James Dobson is a strong proponent of traditional marriage, which he defines as "one where husband and wife are lawfully married, are committed to each other for life," and have a homemaker mother and breadwinner father. According to his view, women are not deemed inferior to men because both are created in God's image, but each gender has biblically-mandated roles. He recommends that married women with children under the age of 18 focus on mothering, rather than work outside the home.
In the 2004 book "Marriage Under Fire", Dobson suggests that heterosexual marriage rates in Denmark, Norway, and Sweden have been falling, and that this is due to the recognition of same-sex relationships by those countries during the 1990s. He remarks that the "institution of marriage in those countries is rapidly dying" as a result, with most young people cohabiting or choosing to remain single (living alone) and illegitimacy rates rising in some Norwegian counties up to 80%.
Dobson writes that "every civilization in the world" has been built upon marriage. He also believes that homosexuality is neither a choice nor genetic, but is caused by external factors during early childhood. He anecdotally cites as evidence the life of actress Anne Heche, who was previously in a relationship with Ellen DeGeneres. Criticizing "the realities of judicial tyranny," Dobson has written that "here is no issue today that is more significant to our culture than the defense of the family. Not even the war on terror eclipses it."
Critics have stated that Dobson's views on homosexuality do not represent the mainstream views of the mental health community, with Dan Gilgoff noting the positions of the American Psychiatric Association and the American Psychological Association on homosexuality.
Views on schooling.
Focus on the Family supports private school vouchers and tax credits for religious schools. According to Focus on the Family website, Dobson believes that parents are ultimately responsible for their children's education, and encourages parents to visit their children's schools to ask questions and to join the PTA so that they may voice their opinions. Dobson opposes sex education curricula that are not abstinence-only. According to People for the American Way, Focus on the Family material has been used to challenge a book or curriculum taught in public schools. Critics, such as People for the American Way, allege that Focus on the Family encourages Christian teachers to establish prayer groups in public schools. Dobson supports student-led prayer in public schools, and believes that allowing student-led Christian prayer in schools does not violate the First Amendment to the United States Constitution.
Views on discipline within the family.
In his book "Dare to Discipline", Dobson advocated the spanking of children up to eight years old when they misbehave, but warns that "corporal punishment should not be a frequent occurrence" and that "discipline must not be harsh and destructive to the child's spirit." He warns against "harsh spanking" because "It is not necessary to beat the child into submission; a little bit of pain goes a long way for a young child. However, the spanking should be of sufficient magnitude to cause the child to cry genuinely."
Dobson has called disciplining children to be a necessary but unpleasant part of raising children that should only be carried out by qualified parents:
In his book "The Strong-Willed Child", Dobson suggests that if authority is portrayed correctly to a child, the child will understand how to interact with other authority figures:
In Dobson's opinion, parents must uphold their authority and do so consistently: "When you are defiantly challenged, win decisively." In "The Strong-Willed Child", Dobson draws an analogy between the defiance of a family pet and that of a small child, and concludes that "just as surely as a dog will occasionally challenge the authority of his leaders, so will a little child — only more so." (emphasis in original)
When asked "How long do you think a child should be allowed to cry after being punished? Is there a limit?" Dobson responded:
Sociologists John Bartkowski and Christopher Ellison have stated that Dobson's views "diverge sharply from those recommended by contemporary mainstream experts" and are not based on any sort of empirical testing, but rather are nothing more than expressions of his religious doctrines of "biblical literalism and 'authority-mindedness.'"
Views on tolerance and diversity.
In the winter of 2004-2005, the We Are Family Foundation sent American elementary schools approximately 60,000 copies of a free DVD using popular cartoon characters (most notably SpongeBob SquarePants) to "promote tolerance and diversity." Dobson contended that "tolerance" and "diversity" are "buzzwords" that the We Are Family Foundation misused as part of a "hidden agenda" to promote homosexuality. Kate Zernik noted Dobson asserting: "tolerance and its first cousin, diversity, 'are almost always buzzwords for homosexual advocacy.'" He stated on the Focus on the Family website that "childhood symbols are apparently being hijacked to promote an agenda that involves teaching homosexual propaganda to children." He offered as evidence the association of many leading LGBT rights organizations, including GLAAD, GLSEN, HRC, and PFLAG, with the We Are Family Foundation as shown by links which he claims once existed on their website.
The We Are Family Foundation countered that Dobson had mistaken their organization with "an unrelated Web site belonging to another group called 'We Are Family,' which supports gay youth." Dobson countered,
In September 2005, Tolerance.org published a follow-up message advertising the DVD's continued availability, including We Are Family Foundation president Nancy Hunt's speculation that many of the DVDs may be "still sitting in boxes, unused, because of Dobson's vitriolic attack."
Views on homosexuality.
Dobson believes that God defines marriage as between one man and one woman only and describes this as the central stabilizing institution of society. Dobson believes that any sexual activity outside of such a union — including homosexuality — cannot be approved by God. In Dobson's view, homosexuality is a choice that is made through influences in a child's environment rather than an inborn trait. He states that homosexual behavior, specifically "unwanted same-sex attraction", has been and can be "overcome" through understanding developmental models for homosexuality and choosing to heal the complex developmental issues which led to same-sex attraction.
Focus on the Family ministry sponsors the monthly conference Love Won Out, where participants hear "powerful stories of ex-gay men and women." Parents, Families and Friends of Lesbians and Gays (P-FLAG) has protested against the conference in Orlando, questioning both its methodology and supposed success. In regards to the conference, Dobson has stated that "Gay activists come with preconceived notions about who we are and what we believe and about the hate that boils from within, which is simply not true. Regardless of what the media might say, Focus on the Family has no interest in promoting hatred toward homosexuals or anyone else. We also don't wish to deprive them of their basic constitutional rights... The Constitution applies to all of us." Dobson strongly opposes the movement to legitimize same-sex relationships. In his book "Bringing Up Boys," Dobson states, "he disorder is not typically 'chosen.' Homosexuals deeply resent being told that they selected this same-sex inclination in pursuit of sexual excitement or some other motive. It is unfair, and I don't blame them for being irritated by that assumption. Who among us would knowingly choose a path that would result in alienation from family, rejection by friends, disdain from the heterosexual world, exposure to sexually transmitted diseases such as AIDS and tuberculosis, and even a shorter lifespan?"
Sociologist Judith Stacey criticized Dobson for claiming that sociological studies show that gay couples do not make good parents. She stated that Dobson's claim "is a direct misrepresentation of my research." In response to Dobson's claim that "there have been more than ten thousand studies that have showed that children do best when they are raised with a mother and a father who are committed to each other," Stacey replied that "ll of those studies that Dobson is referring to are studies that did not include gay or lesbian parents as part of the research base."
Dobson objected to a bill expanding the prohibition of sexual orientation-based discrimination in the areas of "public accommodation, housing practices, family planning services and twenty other areas." He said that, were such a bill passed, public businesses could no longer separate locker rooms and bathrooms by gender, which he claimed would lead to a situation where, "every woman and little girl will have to fear that a predator, bisexual, cross-dresser or even a homosexual or heterosexual male might walk in and relieve himself in their presence."
Political and social influence.
Although Dobson initially remained somewhat distant from Washington politics, in 1981 he founded the Family Research Council as a political arm through which "social conservative causes" could achieve greater political influence. In 1996, he cast a vote for U.S. Taxpayers' Party Presidential candidate Howard Phillips.
In late 2004, Dobson led a campaign to block the appointment of Arlen Specter to head of the Senate Judiciary Committee because of Specter's pro-choice stance on abortion. Responding to a question by Fox News personality Alan Colmes on whether he wanted the Republican Party to be known as a "big-tent party," he replied, "I don't want to be in the big tent... I think the party ought to stand for something." In 2006, Family Research Council spent more than a half million dollars to promote a constitutional amendment to ban same-sex marriage in its home state of Colorado.
A May 2005 article by Hedges in "Harper's Magazine" described Dobson as "perhaps the most powerful figure in the Dominionist movement" and "a crucial player in getting out the Christian vote for George W. Bush." Discernment Ministries, a site that describes dominionism as a heresy, characterized Dobson as belonging to the "Patriotic American" brand of dominionism, calling him "One of its most powerful leaders."
In November 2004, Dobson was described by the online magazine "Slate" as "America's most influential evangelical leader." The article explained "Forget Jerry Falwell and Pat Robertson, who in their dotage have marginalized themselves with gaffes... Dobson is now America's most influential evangelical leader, with a following reportedly greater than that of either Falwell or Robertson at his peak... Dobson may have delivered Bush his victories in Ohio and Florida." Further, "He's already leveraging his new power. When a thank-you call came from the White House, Dobson issued the staffer a blunt warning that Bush 'needs to be more aggressive' about pressing the religious right's pro-life, anti-gay rights agenda, or it would 'pay a price in four years.'... Dobson has sometimes complained that the Republican Party may take the votes of social conservatives for granted, and has suggested that evangelicals may withhold support from the GOP if the party does not more strongly support conservative family issues: "Does the Republican Party want our votes, no string attached — to court us every two years, and then to say, 'Don't call me, I'll call you' — and not to care about the moral law of the universe? ... Is that what they want? Is that the way the system works? Is this the way it's going to be? If it is, I'm gone, and if I go, I will do everything I can to take as many people with me as possible."
However, in 2006, Dobson said that, while "there is disillusionment out there with Republicans" and "that worries me greatly," he nonetheless suggested voters turn out and vote Republican in 2006. "My first inclination was to sit this one out," but according to "The New York Times", Dobson then added that "he had changed his mind when he looked at who would become the leaders of Congressional committees if the Democrats took over."
Dobson garnered national media attention once again in February 2008 after releasing a statement in the wake of Senator John McCain's expected success in the so-called "Super Tuesday" Republican primary elections. In his statement, Dobson said: "I cannot, and will not, vote for Senator John McCain, as a matter of conscience," and indicated that he would refrain from voting altogether if McCain were to become the Republican candidate, echoing other conservative commentators' concerns about the Senator's conservatism. He endorsed Mike Huckabee for president. After McCain selected a pro-life candidate, Sarah Palin, as his running mate, Dobson said that he was more enthusiastic in his support for the Republican ticket. When Palin's 17-year-old daughter's pregnancy was revealed, Dobson issued a press release commending Palin's stance, saying,
On June 24, 2008, Dobson publicly criticized statements made by U.S. Presidential candidate Barack Obama in Obama's 2006 "Call to Renewal" address. Dobson stated that Obama was "distorting the traditional understanding of the Bible to fit his own world view." On October 23, 2008, Dobson published a "Letter from 2012 in Obama's America" that proposed that an Obama presidency could lead to: mandated homosexual teachings across all schools; the banning of firearms in entire states; the end of the Boy Scouts, home schooling, Christian school groups, Christian adoption agencies, and talk radio; pornography on prime-time and daytime television; mandatory bonuses for gay soldiers; terrorist attacks across America; the nuclear bombing of Tel Aviv; the conquering of most of Eastern Europe by Russia; the end of health care for Americans over 80; out-of-control gasoline prices; and complete economic disaster in the United States, among other catastrophes. In the days after the 2008 presidential election, Dobson stated on his radio program that he was mourning the Obama election, claiming that Obama supported infanticide, would be responsible for the deaths of millions of unborn children, and was "going to appoint the most liberal justices to the Supreme Court, perhaps, that we've ever had."
Dobson is an intelligent design supporter and has spoken at conferences supporting the subject, and frequently criticizes evolution. In 2007, Dobson was one of 25 evangelicals who called for the ouster of Rev. Richard Cizik from his position at the National Association of Evangelicals because Cizik had taken a stance urging evangelicals to take global warming seriously.
On June 13, 2007, the National Right to Life Committee ousted Colorado Right to Life after the latter ran a full-page ad criticizing Dobson.
On May 30, 2010, Dobson delivered the pre-race invocation at the NASCAR Coca-Cola 600 automobile race, raising criticism about his association with a sport associated with sponsors and activities which would not meet his definition of family-friendly.
At a National Day of Prayer event in the U.S. Capitol, Dobson called Barack Obama "the abortion president." He said, "President Obama, before he was elected, made it very clear that he wanted to be the abortion president. He didn’t make any bones about it. This is something that he really was going to promote and support, and he has done that, and in a sense he is the abortion president." Among others, Rep. Janice Hahn complained because Dobson used the National Day of Prayer for partisan purposes. She said, "Dobson just blew a hole into this idea of being a nonpartisan National Day of Prayer. It was very disturbing to me...and really a shame. James Dobson hijacked the National Day of Prayer — this nonpartisan, nonpolitical National Day of Prayer — to promote his own distorted political agenda."
Ecumenical relations.
Dobson and Charles Colson were two participants in a 2000 conference at the Vatican on the global economy's impact on families. During the conference, the two Protestants met with Pope John Paul II. Dobson later told Catholic News Service that though he has theological differences with Roman Catholicism, "when it comes to the family, there is far more agreement than disagreement, and with regard to moral issues from abortion to premarital sex, safe-sex ideology and homosexuality, I find more in common with Catholics than with some of my evangelical brothers and sisters."
In November 2009, Dobson signed an ecumenical statement known as the "" calling on evangelicals, Catholics and Eastern Orthodox Christians not to comply with rules and laws permitting abortion, same-sex marriage and other matters that go against their religious consciences.
Publications.
Dobson has authored or co-authored 36 books, including:

</doc>
<doc id="46231" url="https://en.wikipedia.org/wiki?curid=46231" title="Super 8 film">
Super 8 film

Super 8 mm film is a motion picture film format released in 1965 by Eastman Kodak as an improvement of the older "Double" or "Regular" 8 mm home movie format.
The film is nominally 8 mm wide, exactly the same as the older standard 8 mm film, and also has perforations on only one side. However, the dimensions of the perforations are smaller than those on older 8 mm film, which allowed the exposed area to be made larger. The Super 8 standard also specifically allocates the border opposite the perforations for an oxide stripe upon which sound can be magnetically recorded.
Unlike Super 35, the film stock used for Super 8 is not compatible with standard 8mm film cameras.
There are several different varieties of the film system used for shooting, but the final film in each case has the same dimensions. By far the most popular system was the Kodak system.
Kodak Super 8 system.
Launched in 1965, Super 8 film comes in plastic light-proof cartridges containing coaxial supply and take-up spools loaded with of film, with 72 frames per foot, for a total of approximately 3,600 frames per film cartridge. This was enough film for 2.5 minutes at the professional motion picture standard of 24 frames per second, and for 3 minutes and 20 seconds of continuous filming at 18 frames per second (upgraded from Standard 8 mm's 16 frame/s) for amateur use. A cartridge later became available which could be used in specifically designed cameras, but that Kodak cartridge is no longer produced. Super 8 film was typically a reversal stock. Kodak makes two types of reversal film in this format today; one color (Ektachrome 100D/7285) and one black and white (Tri-X/7266). The Ektachrome 64T stock has recently been discontinued along with the 100d stock. In addition to reversals, Kodak also offers two negative stocks (Vision3 200T/7213 and Vision3 500T/7219). In the 1990s Pro-8 mm pioneered custom loading of several Super 8 stocks, and their current inventory mirrors closely what is currently available to the professional cinematographer from Kodak and Fuji. Today Super 8 color negative film is available directly from Kodak for professional use and is typically transferred to video through the telecine process for use in television advertisement, music videos and other film projects.
The Super 8 plastic cartridge is probably the fastest loading film system ever developed, as it can be loaded into the Super 8 camera in less than two seconds without the need to directly thread or even touch the film. In addition, coded notches cut into the Super 8 film cartridge exterior allowed the camera to recognize the film speed automatically. Not all cameras can read all the notches correctly, however, and not all cartridges are notched correctly (such as Kodak Vision2 200T). Canon keeps an exhaustive list of their Super 8 cameras with detailed specifications on what film speeds can be used with their cameras. Usually, testing one cartridge of film can help handle any uncertainty a filmmaker may have about how well their Super 8 camera reads different film stocks. Color stocks were generally available only in tungsten (3400K), and almost all Super 8 cameras come with a switchable daylight filter built in, allowing for both indoor and outdoor shooting.
The original Super 8 film release was a silent system only, but in 1973 a sound on film version was released. The sound film had a magnetic soundtrack and came in larger cartridges than the original because the cartridge had to accommodate the sound recording head in the film path. Sound film also requires a longer film path (for smoothing the film movement before it reached the recording head), and a second aperture for the recording head. Sound cameras were compatible with silent cartridges, but not vice versa. Sound film was typically filmed at a speed of 18 or 24 frames per second. Kodak discontinued the production of Super 8 sound film in 1997, citing environmental regulations as the reason (the adhesive used to bond the magnetic track to the film was environmentally hazardous).
Kodak still manufactures several color and black-and-white Super 8 reversal film stocks, but in 2005 announced the discontinuation of the most popular stock Kodachrome due to the decline of facilities equipped for the K-14 process. Kodachrome was "replaced" by a new ISO 64 Ektachrome, which used the simpler E-6 process. The last roll of Kodachrome was processed on January 18, 2011 (although announced last date of processing was December 30, 2010) in Parsons, Kansas, by the sole remaining lab capable of processing the format.
Super 8 film stocks other than Kodachrome—from color and black and white reversal, to color negative—can be processed same day in several labs around the world.
In April 2010, Kodak announced the discontinuation of Plus-X and E64T. In the same press release, they also announced that they would be replacing E64T with a Super-8 version of Ektachrome 100D, a popular reversal stock available to 16mm and 35mm users. Previous to Kodak's announcement, the stock had been supplied by third-party vendors such as Yale Film & Video, Pro8mm and Spectra Film and Video in the United States, and Witter Kinotechnik in Germany.
Kodak does not offer processing for its black and white Super-8 films, preferring instead to refer its users to third-party processors.
Kodak has also introduced several Super 8 negative stocks cut from their Vision film series, ISO 200 and ISO 500 which can be used in very low light. Kodak reformulated the emulsions for the B&W reversal stocks Plus-X (ISO 100) and Tri-X (ISO 200), in order to give them more sharpness. Many updates of film stocks are in response to the improvement of digital video technology. The growing popularity and availability of non-linear editing systems has allowed film-makers to shoot Super 8 film but edit on video, thereby avoiding much of the scratches and dust that can accrue when editing the actual film. Super 8 Films may be transferred through telecine to video and then imported into computer-based editing systems. Along with the computer editing option a number of enthusiasts still choose to edit super 8 film with a viewer and rewinds and then project their edit master on a film projector and movie screen.
Some feature films have been shot on Super 8 mm and most that have current distribution today were edited on video. Some titles include "Colony," "Brand Upon the Brain!," "Things," "Since I Don't Have You," "Beasties," "Nudist Colony of the Dead," "Nekromantik," "Bleak Future," and "Curse of the Queerwolf."
Negative stocks, however, must be transferred to video in order to be viewed and edited properly. Unlike 16mm and larger formats, the capability to make a film-based work print does not exist for Super-8 in the U.S., because it would not be cost-effective. The German company Andec Filmtechnik offers a printing service for Super-8 negative in Germany, but it is unknown how much longer it will continue to do so, given the relatively low usage of it. In addition, projecting the processed film would produce a negative image, and would also damage the film itself.
Kodak Super 8 mm cartridges cannot be reloaded; however, a reloadable cartridge was manufactured in the Soviet Union.
Kodak discontinued reversal print stocks several years ago. Andec Film in Berlin now makes prints from Super 8 negative film, although optical blow-ups to 16mm or 35mm are available at other labs.
In December 2012, Kodak discontinued the last 'official' colour reversal stock for Super 8, Ektachrome 100D, and released Vision 50D negative to the Super 8 format. This can be printed to a Super 8 positive by Andec or transferred to digital. The cost of printing negative is higher than developing reversal for shooters who wish to project their movie. The only 'official' reversal film still being manufactured by Kodak is Tri-X which is a monochrome emulsion.
Wittner Kinotechnik still takes reversal still films and slits them for use in the Super 8 format, plus a special lubricant to allow it to be used without jamming due to the stock's thickness. This does not harm the processing chemistry as it is inert. Wittner formerly offered Fujifilm Velvia 50 and has, at times, also made Velvia 100 and Astia available in this format.
Wittner presently offers Agfa Aviphot colour reversal film in the Super 8 format. It is a daylight balanced 200 ISO film which processes to E6. This is slit from fresh aerial photography film, a specialist product still made by Agfa in Belgium. Aviphot has the same emulsion as the discontinued RSX-II slide film.
Fujifilm Single-8 system.
Fujifilm of Japan developed an alternative format called Single-8, which was released in 1965 as a different option to the Kodak Super 8 format.
Single-8 cartridges without a press plate are of a different design from a Super 8 cartridge, resembling a cassette-style design (supply and take-up reels side by side) as opposed to Super 8's coaxial cartridge design (one reel on top of the other). Therefore, Single-8 film cartridges can only be used in Single-8 cameras. However, the film loaded in a Single-8 cartridge has exactly the same dimensions as Super 8 (though it is made of a thinner & stronger polyester base, rather than the acetate base of Super 8 film), and can be viewed in any Super 8 projector after processing. Fuji, however, recommended that only tape splices be used when combining Single-8 footage with Super-8, as cement would cause damage to the Single-8 footage. Also, when jammed, Single-8 footage had a tendency to stretch in the projector, unlike the acetate-based Super-8 film, which simply broke.
Although never as popular as Super 8, the format existed in parallel. On 2 June 2009 Fuji announced the end of Single-8 motion picture film. Tungsten balanced 200 ASA Fuji RT200N ceased to be manufactured by May 2010. Daylight balanced 25 ASA Fujichrome R25N remained available until March 2012. Fuji's in-house processing service was available until September 2013.
Polaroid Polavision.
An instant 8mm film released in 1977 by Polaroid, Polavision uses the same perforations as Super 8mm film. It can be projected through a Super 8mm projector if the film is transferred from the original cartridge to a 8mm reel. However, because of the additive process, the picture will be much darker.
Double Super 8.
Double Super 8 film (commonly abbreviated as DS8) is a 16 mm wide film but has Super 8 size sprockets. It is used in the same way as standard 8 mm film in that the film is run through the camera twice, exposing one side on each pass. During processing, the film is split down the middle and the two pieces spliced together to produce a single strip for projection in a Super 8 projector. Because it has sprockets on both sides of the film, the pin-registration is superior to Super 8 film and so picture stability is better.
Super Duper 8 (AKA Max 8).
Super-Duper-8, or S-D-8 was created out of the need for widescreen compatibility without having to use expensive optical adaptors or excessive cropping. Since magnetic sound-striped film is no longer available, the creators of Sleep Always experimented with widening the camera gate to expose into the sound track region to achieve this. The result is a 20% wider image than previously possible which also gives better clarity to the image. Pro8mm sells Max-8 widescreen cameras, which are remade Super 8 cameras. These cameras have an aspect ratio of about 1.58:1 (which among common aspect ratios is closest to , and second-closest to 1.66:1 classic European widescreen aka 5:3 aka VistaVision), so less cropping is needed to convert the image to widescreen (especially ) than the traditional 1.33 ratio.
Equipment.
The last major manufacturer to produce Super 8 cameras was the French company Beaulieu. Beaulieu cameras have been the basis for several newer cameras offered by the U.S.-based Pro8mm company. Older Super 8 cameras are available from specialized retailers and auction sites such as eBay.
Kodak, ADOX, and Foma (in DS8 only) are the only companies currently making Super 8 film stock of their own. Kodak offers some of their latest Vision 3 color negative stock. One or more other Super 8 specialists (such as Pro8mm, Spectra (both in Los Angeles), Wittner Cinetec (in Hamburg, Germany) and Kahlfilm (in Brühl, Germany) slit raw 35 mm film stock from Fuji, Kodak and ORWO, perforate it, and repackage it in Kodak Super 8 cartridges. Due to Kodak's discontinuation of Kodachrome 40 in 2006 (the one stock that for four decades used to be almost synonymous to Super 8 as a medium itself), the Super 8 market opened for new stocks and competing film manufacturers. There are now more varieties of Super 8 film available than ever before, but ironically very few retailers still stock Super 8 film, as there is virtually no demand from "ordinary" consumers.
One country where it remained widely available well past the 1990s is the UK, where the chain Jessops carried one film: Kodak Ektachrome 64T. Until 2002 it was also available in Boots, a British high-street chain-pharmacy. In 2007 it was reported that Jessops were scaling back their film stocks and would no longer stock Super 8 film.
There were rumours of Super 8 cameras and films being manufactured and sold in North Korea, partly to be found in specialty photography stores in a few Southeast Asian countries, by a company named Kim Chek, and indeed this has been confirmed by North Korean embassies, but the only way to buy such products is to visit those countries. 
In 2015, Logmar of Denmark made a one-off batch of 50 "digicanical" pro-level Super 8 cameras to celebrate the 50th anniversary of Super 8.
In 2016, Eastman Kodak showed a concept for a new Super 8 camera, its first such camera in over 30 years.
Sound.
The super 8mm system was one of the few film formats where the requirements of sound were designed in from the start. The sound track was added on the edge of the film opposite to the perforations (see the illustration at head of the article). Standard 8mm had the stripe between the perforations and the edge of the film which made good contact with a magnetic head problematic. A balance stripe was sometimes added on the opposite edge to facilitate spooling of the film. Projectors that could record and play sound appeared before sound cameras. The sound was recorded 18 frames in advance of the picture (as opposed to 56 frames for standard 8mm). This short distance of just 3 inches facilitated the relatively compact size of the later sound cartridges. Some projectors used the balance stripe to provide a second channel and hence stereo sound.
Super 8mm was also specified with an optical sound track. This occupied the same location as the magnetic track. Picture to sound separation in this format was 22 frames. Projectors and cameras obviously could not record sound in this system, but optical sound package movies became briefly popular, particularly in Europe (mainly because they were cheaper to produce - though the projectors cost more). Although the optical sound should have been inferior in quality to magnetic sound (running at 3.6 inches per second for 24 frames per second), in practice it was often much better, largely because packaged movie magnetic sound was often poorly recorded.
Packaged movies.
Although the 8 mm format was originally intended for creating amateur films, condensed versions of popular cinema releases were available up until the mid-1980s, for projection at home. These were generally edited to fit onto a or reel. Many Charlie Chaplin films, and other silent movies were available. The Walt Disney Studio released excerpts from many of their animated feature films, as well as some shorts, in both Standard and Super 8, some even with magnetic sound. New releases of material were not stopped by major studios until the mid-1980s in the U.S. Releases of trailers, shorts, and a few feature films still continues in the U.K.
In-flight movies.
Starting in 1971 In-flight movies (previously 16 mm) were shown in Super 8 format until video distribution became the norm. The films were printed with an optical sound track (amateur films use magnetic sound), and spooled into proprietary cassettes that often held a whole 2-hour movie.
Popularity.
Amateur usage of Super 8 has been largely replaced by video, but the format is often used by professionals in music videos, TV commercials, and special sequences for television and feature film projects, as well as by many visual artists. For a professional cinematographer, Super 8 is another tool to use alongside larger formats. Some seek to imitate the look of old home movies, or create a stylishly grainy look. Many independent filmmakers such as Derek Jarman, Dave Markey, Sean Pecknold, Jem Cohen, Damon Packard, Sam Raimi, Jesse Richards, Harmony Korine, Teod Richter, Jörg Buttgereit, Nathan Schiff and Guy Maddin have made extensive use of 8 mm film. Oliver Stone, for example, has used it several times in his more recent films, such as "The Doors", "Natural Born Killers", "Nixon", "U Turn", and "JFK" where his director of photography Robert Richardson employed it to evoke a period or to give a different look to scenes. The PBS series "Globe Trekker" uses approximately five minutes of Super 8 footage per episode. In the UK, broadcasters such as the BBC still occasionally make use of Super 8 in both drama and documentary contexts, usually for creative effect. A recent example of particular note was the 2005 BBC2 documentary series, Define Normal, which was shot largely on Super 8, with only interviews and special timelapse photography utilising more conventional digital formats. Most recently, John Mellencamp's 2011 documentary film, "It's About You," was shot entirely in Super-8.
Thanks to over a dozen film stocks and certain features common in Super 8 cameras but unavailable in video camcorders–notably the ability to expose single frames and shoot at several non video standard frame rates, including time-exposure and slow motion–Super 8 provides an ideal inexpensive medium for traditional stop-motion and cel animation and other types of filming speed effects not common to video cameras.
Another visual effect uncommon in video cameras that certain high-end Super 8 cameras can do in-camera is the lap dissolve. Upon activation of the lap dissolve feature, the shot being filmed fades to black, the camera back-winds the film to the beginning of the fade and, at the beginning of the next shot, fades in.
Film festivals.
To give further support to filmmakers dedicated to shooting on Super 8 mm film, many film festivals and screenings—such as the Flicker Film Festival, and Super Gr8 Film Festival—exist to give filmmakers a place to screen their Super 8 mm films. Many of these screenings shun video and are only open to films shot on film. Some require film to be turned in undeveloped and thus not permitting any editing, providing an additional challenge to the filmmaker. These include such the Bentley Film Festival and straight 8, which runs screenings at the Cannes Film Festival and many other festivals and events worldwide, where a sound track is required to be supplied with a completed but unprocessed cartridge. In the 2005 Cannes Film Festival, a Super 8 short film ("The Man Who Met Himself") by British filmmaker Ben Crowe, shot on the now discontinued Kodachrome 40 format, was the first Super 8 film to be nominated for the Short Film Palme D'Or in the Official Selection.
In the UK, the Cambridge International Super 8 Film Festival, with the support of the film industry, runs a competition programme of more than 60 films every year. The festival also features work of Super 8 filmmakers, industry talks, and a workshop.
The United States Super 8 Film + Digital Video Festival receives close to 100 Super 8 entries every year.
A number of experimental filmmakers, such as Mason Shefa and Stan Brakhage, continue to work extensively in the format and festivals such as the Images Festival (Toronto), the Media City Film Festival (Windsor, ONT), and TIE (based in Colorado) regularly project Super 8 films as part of their programming.
In June 2010, the Super8 Shots film festival was launched in Galway, Ireland, the first Super 8 festival to occur in Ireland, and included classes on basics and uses of film through to processing your own film.
Chicago 8: A Small Gauge Film Festival started in 2011, and will feature yearly programming of small gauge film from around the world.
Educational use.
Several post-secondary institutions in the United States continue to utilize Super 8 in Film and Cinema programs. For example, both City College of San Francisco's Cinema Department and the University of North Texas' Radio, Television and Film Department require the use of Super 8.
This experience gives students the basics of film production and editing. Importantly, it also emphasizes the need for detailed pre-production planning, especially for in-camera edits. Further, the use of Super 8 leads students into the Regular 16 and Super 16 films shot in higher level courses.
Until 1999, the University of Southern California's famous School of Cinematic Arts required students to shoot some of their projects using Super 8, but digital video is now favored instead.
In popular culture.
The backdrop of the 2011 film "Super 8" involves a group of teenagers in the fictional Ohio town Lillian filming their own Super 8 movie depicting their experience with a landlocked alien in the summer of 1979.
The 2012 film "Sinister" contains Super 8 shots used to depict the various gruesome murders.

</doc>
<doc id="46233" url="https://en.wikipedia.org/wiki?curid=46233" title="Siegfried Sassoon">
Siegfried Sassoon

Siegfried Loraine Sassoon, (8 September 1886 – 1 September 1967) was an English poet, writer, and soldier. Decorated for bravery on the Western Front, he became one of the leading poets of the First World War. His poetry both described the horrors of the trenches, and satirised the patriotic pretensions of those who, in Sassoon's view, were responsible for a jingoism-fuelled war. Sassoon became a focal point for dissent within the armed forces when he made a lone protest against the continuation of the war in his "Soldier's Declaration" of 1917, culminating in his admission to a military psychiatric hospital; this resulted in his forming a friendship with Wilfred Owen, who was greatly influenced by him. Sassoon later won acclaim for his prose work, notably his three-volume fictionalised autobiography, collectively known as the "Sherston trilogy".
Early life and education.
Siegfried Sassoon was born and grew up in the neo-gothic mansion named "Weirleigh" (after its builder, Harrison Weir), in Matfield, Kent, to a Jewish father and an Anglo-Catholic mother. His father, Alfred Ezra Sassoon (1861–1895), son of Sassoon David Sassoon, was a member of the wealthy Baghdadi Jewish Sassoon merchant family. For marrying outside the faith, Alfred was disinherited. Siegfried's mother, Theresa, belonged to the Thornycroft family, sculptors responsible for many of the best-known statues in London—her brother was Sir Hamo Thornycroft. There was no German ancestry in Siegfried's family; his mother named him Siegfried because of her love of Wagner's operas. His middle name, Loraine, was the surname of a clergyman with whom she was friendly.
Siegfried was the first of three sons, the others being Michael and Hamo. When he was four years old his parents separated. During his father's weekly visits to the boys, Theresa locked herself in the drawing-room. In 1895 Alfred Sassoon died of tuberculosis.
Sassoon was educated at the New Beacon School, Sevenoaks, Kent; at Marlborough College, Marlborough, Wiltshire (where he was a member of Cotton House), and at Clare College, Cambridge, where from 1905 to 1907 he read history. He went down from Cambridge without a degree and spent the next few years hunting, playing cricket and writing verse: some he published privately. Since his father had been disinherited from the Sassoon fortune for marrying a non-Jew, Siegfried had only a small private fortune that allowed him to live modestly without having to earn a living (however, he would later be left a generous legacy by an aunt, Rachel Beer, allowing him to buy the great estate of Heytesbury House in Wiltshire.) His first published success, "The Daffodil Murderer" (1913), was a parody of John Masefield's "The Everlasting Mercy". Robert Graves, in "Good-Bye to All That" describes it as a "parody of Masefield which, midway through, had forgotten to be a parody and turned into rather good Masefield."
Sassoon expressed his opinions on the political situation before the onset of the First World War thus—"France was a lady, Russia was a bear, and performing in the county cricket team was much more important than either of them". Sassoon wanted to play for Kent County Cricket Club; Kent Captain Frank Marchant was a neighbour of Sassoon. Siegfried often turned out for Bluehouses at the Nevill Ground, where he sometimes played alongside Arthur Conan Doyle. He also played cricket for his house at Marlborough College, once taking 7 wickets for 18 runs. Although an enthusiast, Sassoon was not good enough to play for Kent, but he played cricket for Matfield, and later for the Downside Abbey team, continuing into his seventies.
War service.
The Western Front: Military Cross.
Motivated by patriotism, Sassoon joined the British Army just as the threat of a new European war was recognized, and was in service with the Sussex Yeomanry on 4 August 1914, the day the United Kingdom of Great Britain and Ireland declared war on Germany. He broke his arm badly in a riding accident and was put out of action before even leaving England, spending the spring of 1915 convalescing. (Rupert Brooke, whom Siegfried had briefly met, died in April on the way to Gallipoli.) He was commissioned into the 3rd Battalion (Special Reserve), Royal Welch Fusiliers, as a second lieutenant on 29 May 1915. On 1 November his younger brother Hamo was killed in the Gallipoli Campaign, and in the same month Siegfried was sent to the 1st Battalion in France. There he met Robert Graves, and they became close friends. United by their poetic vocation, they often read and discussed each other's work. Though this did not have much perceptible influence on Graves's poetry, his views on what may be called 'gritty realism' profoundly affected Sassoon's concept of what constituted poetry. He soon became horrified by the realities of war, and the tone of his writing changed completely: where his early poems exhibit a Romantic, dilettantish sweetness, his war poetry moves to an increasingly discordant music, intended to convey the ugly truths of the trenches to an audience hitherto lulled by patriotic propaganda. Details such as rotting corpses, mangled limbs, filth, cowardice and suicide are all trademarks of his work at this time, and this philosophy of 'no truth unfitting' had a significant effect on the movement towards Modernist poetry.
Sassoon's periods of duty on the Western Front were marked by exceptionally brave actions, including the single-handed capture of a German trench in the Hindenburg Line. Armed with grenades, he scattered sixty German soldiers:He went over with bombs in daylight, under covering fire from a couple of rifles, and scared away the occupants. A pointless feat, since instead of signalling for reinforcements, he sat down in the German trench and began reading a book of poems which he had brought with him. When he went back he did not even report. Colonel Stockwell, then in command, raged at him. The attack on Mametz Wood had been delayed for two hours because British patrols were still reported to be out. "British patrols" were Siegfried and his book of poems. "I'd have got you a D.S.O., if you'd only shown more sense," stormed Stockwell.Sassoon's bravery was inspiring to the extent that soldiers of his company said that they felt confident only when they were accompanied by him. He often went out on night-raids and bombing patrols and demonstrated ruthless efficiency as a company commander. Deepening depression at the horror and misery the soldiers were forced to endure produced in Sassoon a paradoxically manic courage, and he was nicknamed "Mad Jack" by his men for his near-suicidal exploits. On 27 July 1916 he was awarded the Military Cross; the citation read:
Robert Graves described Sassoon as engaging in suicidal feats of bravery. Sassoon was also later (unsuccessfully) recommended for the Victoria Cross.
War opposition: Craiglockhart.
Despite his decorations and reputation, in 1917 Sassoon decided to make a stand against the conduct of the war. One of the reasons for his violent anti-war feeling was the death of his friend David Cuthbert Thomas, who appears as "Dick Tiltwood" in the Sherston trilogy. Sassoon would spend years trying to overcome his grief.
At the end of a spell of convalescent leave, Sassoon declined to return to duty; instead, encouraged by pacifist friends such as Bertrand Russell and Lady Ottoline Morrell, he sent a letter to his commanding officer entitled "". Forwarded to the press and read out in the House of Commons by a sympathetic member of parliament, the letter was seen by some as treasonous ("I am making this statement as an act of wilful defiance of military authority") or at best as condemning the war government's motives ("I believe that the war upon which I entered as a war of defence and liberation has now become a war of aggression and conquest"). Rather than court-martial Sassoon, the Under-Secretary of State for War, Ian Macpherson, decided that he was unfit for service and had him sent to Craiglockhart War Hospital near Edinburgh, where he was officially treated for neurasthenia ("shell shock").
Before declining to return to active service Sassoon had thrown the ribbon of his Military Cross into the river Mersey. According to his description of this incident in "Memoirs of an Infantry Officer" he did not, as one would infer from the context of his action, do this as a symbolic rejection of militaristic values, but simply out of the need to perform some destructive act in catharsis of the black mood which was afflicting him; his account states that one of his pre-war sporting trophies, had he had one to hand, would have served his purpose equally well.
The novel "Regeneration", by Pat Barker, is a fictionalised account of this period in Sassoon's life, and was made into a film starring James Wilby as Sassoon and Jonathan Pryce as W. H. R. Rivers, the psychiatrist responsible for Sassoon's treatment. Rivers became a kind of surrogate father to the troubled young man, and his sudden death in 1922 was a major blow to Sassoon.
At Craiglockhart, Sassoon met Wilfred Owen, a fellow poet who would eventually exceed him in fame. It was thanks to Sassoon that Owen persevered in his ambition to write better poetry. A manuscript copy of Owen's "Anthem for Doomed Youth" containing Sassoon's handwritten amendments survives as testimony to the extent of his influence and is currently on display at London's Imperial War Museum. Sassoon became to Owen "Keats and Christ and Elijah"; surviving documents demonstrate clearly the depth of Owen's love and admiration for him. Both men returned to active service in France, but Owen was killed in 1918. Sassoon, despite all this, was promoted to lieutenant, and having spent some time out of danger in Palestine, eventually returned to the Front. On 13 July 1918, Sassoon was almost immediately wounded again—by friendly fire when he was shot in the head by a fellow British soldier who had mistaken him for a German near Arras, France. As a result, he spent the remainder of the war in Britain. By this time he had been promoted acting captain. He relinquished his commission on health grounds on 12 March 1919, but was allowed to retain the rank of captain. After the war, Sassoon was instrumental in bringing Owen's work to the attention of a wider audience. Their friendship is the subject of Stephen MacDonald's play, "Not About Heroes".
Post-war.
Editor and novelist.
Having lived for a period at Oxford, where he spent more time visiting literary friends than studying, he dabbled briefly in the politics of the Labour movement, and in 1919 took up a post as literary editor of the socialist "Daily Herald". He lived at 54 Tufton Street, Westminster from 1919 to 1925; the house is no longer standing, but the location of his former home is marked by a memorial plaque.
During his period at the "Herald", Sassoon was responsible for employing several eminent names as reviewers, including E. M. Forster and Charlotte Mew, and commissioned original material from "names" like Arnold Bennett and Osbert Sitwell. His artistic interests extended to music. While at Oxford he was introduced to the young William Walton, to whom he became a friend and patron. Walton later dedicated his "Portsmouth Point" overture to Sassoon in recognition of his financial assistance and moral support.
Sassoon later embarked on a lecture tour of the USA, as well as travelling in Europe and throughout Britain. He acquired a car, a gift from the publisher Frankie Schuster, and became renowned among his friends for his lack of driving skill, but this did not prevent him making full use of the mobility it gave him.
Sassoon was a great admirer of the Welsh poet Henry Vaughan. On a visit to Wales in 1923, he paid a pilgrimage to Vaughan's grave at Llansantffraed, Powys, and there wrote one of his best-known peacetime poems, "At the Grave of Henry Vaughan". The deaths of three of his closest friends - Edmund Gosse, Thomas Hardy and Frankie Schuster (the publisher) - within a short space of time, came as another serious setback to his personal happiness.
At the same time, Sassoon was preparing to take a new direction. While in America, he had experimented with a novel. In 1928, he branched out into prose, with "Memoirs of a Fox-Hunting Man", the anonymously-published first volume of a fictionalised autobiography, which was almost immediately accepted as a classic, bringing its author new fame as a humorous writer. The book won the 1928 James Tait Black Award for fiction. Sassoon followed it with "Memoirs of an Infantry Officer" (1930) and "Sherston's Progress" (1936). In later years, he revisited his youth and early manhood with three volumes of genuine autobiography, which were also widely acclaimed. These were "The Old Century", "The Weald of Youth" and "Siegfried's Journey".
Personal life.
Affairs.
Sassoon, having matured greatly as a result of his military service, continued to seek emotional fulfilment, initially in a succession of love affairs with men, including:
Only the last of these made a permanent impression, though Shaw remained his close friend throughout his life.
Marriage.
In September 1931, Sassoon rented and began to live at Fitz House, Teffont Magna, Wiltshire. In December 1933, he married Hester Gatty, who was many years his junior. The marriage led to the birth of a child, something which he had purportedly craved for a long time:
George became a scientist, linguist and author, and was adored by Siegfried, who wrote several poems addressed to him. However, the marriage broke down after the Second World War, Sassoon apparently unable to find a compromise between the solitude he enjoyed and the companionship he craved.
Separated from his wife in 1945, Sassoon lived in seclusion at Heytesbury in Wiltshire, although he maintained contact with a circle which included E M Forster and J R Ackerley. One of his closest friends was the young cricketer Dennis Silk. He formed a close friendship with Vivien Hancock, headmistress of Greenways School at Ashton Gifford, which his son George attended. The relationship provoked Hester to make some strong accusations against Vivien Hancock, who responded with the threat of legal action.
Religion.
Towards the end of his life, he converted to Roman Catholicism. He had hoped that Ronald Knox, a Roman Catholic priest and writer whom he admired, would instruct him in the faith, but Knox was too ill to do so. The priest Sebastian Moore was chosen to instruct him instead, and Sassoon was admitted to the faith at Downside Abbey, close to his home. He also paid regular visits to the nuns at Stanbrook Abbey, and the abbey press printed commemorative editions of some of his poems. During this time he also became interested in the supernatural, and joined the Ghost Club.
Death and awards.
Sassoon was appointed Commander of the Order of the British Empire in the 1951 New Year Honours. Siegfried Sassoon died one week before his 81st birthday, of stomach cancer, and is buried at St Andrew's Church, Mells, Somerset, close to Ronald Knox.
Legacy.
On 11 November 1985, Sassoon was among sixteen Great War poets commemorated on a slate stone unveiled in Westminster Abbey's Poet's Corner. The inscription on the stone was written by friend and fellow War poet Wilfred Owen. It reads: "My subject is War, and the pity of War. The Poetry is in the pity."
The year 2003 saw the publication of "Memorial Tablet", an authorised audio CD of readings by Sassoon recorded during the late 1950s. These included extracts from "Memoirs of an Infantry Officer" and "The Weald of Youth", as well as several war poems including Attack, The Dug-Out, At Carnoy and Died of Wounds, and postwar works. The CD also included comment on Sassoon by three of his Great War contemporaries: Edmund Blunden, Edgell Rickword and Henry Williamson.
Siegfried Sassoon's only child, George Sassoon, died of cancer in 2006. George had three children, two of whom were killed in a car crash in 1996. His daughter by his first marriage, Kendall Sassoon, is Patron-in-Chief of the Siegfried Sassoon Fellowship, and a Lady Associate Royal Welch Fusilier.
Sassoon's long-lost Military Cross turned up in a relative's attic in May 2007. Subsequently, the medal was put up for sale by his family. It was bought by the Royal Welch Fusiliers for display at their museum in Caernarfon.
Sassoon's other service medals went unclaimed until 1985 when his son George obtained them from the Army Medal Office, then based at Droitwich. The "late claim" medals consisting of the 1914-15 Star, Victory Medal and British War Medal along with Sassoon's CBE and Warrant of Appointment were auctioned by Sotheby's in 2008.
In June 2009, the University of Cambridge announced plans to purchase a valuable archive of Sassoon's papers from his family, to be added to the university library's existing Sassoon collection. On 4 November 2009 it was reported that this purchase would be supported by £550,000 from the National Heritage Memorial Fund, meaning that the University still needed to raise a further £110,000 on top of the money already received in order to meet the full £1.25 million asking price. The funds were successfully raised, and in December 2009 it was announced that the University had received the papers. Included in the collection are war diaries kept by Sassoon while he served on the Western Front and in Palestine, a draft of "" (1917), notebooks from his schooldays, and post-war journals. Other items in the collection include love letters to his wife Hester, and photographs and letters from other writers. Sassoon was an undergraduate at the university, as well as being made an honorary fellow of Clare College, and the collection will be housed at the Cambridge University Library. As well as private individuals, funding came from the Monument Trust, the JP Getty Jr Trust, and Sir Siegmund Warburg's Voluntary Settlement.
In 2010, "Dream Voices: Siegfried Sassoon, Memory and War", a major exhibition of Sassoon's life and archive, was held at Cambridge University.
Several of Sassoon's poems have been set to music, some during his lifetime, notably by Cyril Rootham, who co-operated with the author.
The discovery in 2013 of an early draft of one of Sassoon's best-known anti-war poems had biographers saying they would rewrite portions of their work about the poet. In the poem, 'Atrocities,' which concerned the killing of German prisoners by their British counterparts, the early draft shows that some lines were cut and others watered down. The poet's publisher was nervous about publishing the poem, and held it for publication in an expurgated version at a later date. Said Sassoon biographer Jean Moorcroft Wilson on learning of the discovery of the early draft: "This is very exciting material. I want to rewrite my biography and I probably shall be able to get some of it in. It's a treasure trove."

</doc>
<doc id="46238" url="https://en.wikipedia.org/wiki?curid=46238" title="Refrigeration">
Refrigeration

Refrigeration is a process of moving heat from one location to another in controlled conditions. The work of heat transport is traditionally driven by mechanical work, but can also be driven by heat, magnetism, electricity, laser, or other means. Refrigeration has many applications, including, but not limited to: household refrigerators, industrial freezers, cryogenics, and air conditioning. Heat pumps may use the heat output of the refrigeration process, and also may be designed to be reversible, but are otherwise similar to refrigeration units.
Refrigeration has had a large impact on industry, lifestyle, agriculture and settlement patterns. The idea of preserving food dates back to the ancient Roman and Chinese empires. However, refrigeration technology has rapidly evolved in the last century, from ice harvesting to temperature-controlled rail cars. The introduction of refrigerated rail cars contributed to the westward expansion of the United States, allowing settlement in areas that were not on main transport channels such as rivers, harbors, or valley trails. Settlements were also developing in infertile parts of the country, filled with new natural resources. These new settlement patterns sparked the building of large cities which are able to thrive in areas that were otherwise thought to be inhospitable, such as Houston, Texas and Las Vegas, Nevada. In most developed countries, cities are heavily dependent upon refrigeration in supermarkets, in order to obtain their food for daily consumption. The increase in food sources has led to a larger concentration of agricultural sales coming from a smaller percentage of existing farms. Farms today have a much larger output per person in comparison to the late 1800s. This has resulted in new food sources available to entire populations, which has had a large impact on the nutrition of society.
History.
Earliest forms of cooling.
The seasonal harvesting of snow and ice is an ancient practice estimated to have begun earlier than 1000 B.C. A Chinese collection of lyrics from this time period known as the Shih king, describes religious ceremonies for filling and emptying ice cellars. However, little is known about the construction of these ice cellars or what the ice was used for. The next ancient society to harvest ice may have been the Jews according to the book of Proverbs, which reads, “As the cold of snow in the time of harvest, so is a faithful messenger to them who sent him.” Historians have interpreted this to mean that the Jews used ice to cool beverages rather than to preserve food. Other ancient cultures such as the Greeks and the Romans dug large snow pits insulated with grass, chaff, or branches of trees as cold storage. Like the Jews, the Greeks and Romans did not use ice and snow to preserve food, but primarily as a means to cool beverages. The Egyptians also developed methods to cool beverages, but in lieu of using ice to cool water, the Egyptians cooled water by putting boiling water in shallow earthen jars and placing them on the roofs of their houses at night. Slaves would moisten the outside of the jars and the resulting evaporation would cool the water. The ancient people of India used this same concept to produce ice. The Persians stored ice in a pit called a Yakhchal and may have been the first group of people to use cold storage to preserve food. In the Australian outback before a reliable electricity supply was available where the weather could be hot and dry, many farmers used a "Coolgardie safe". This consisted of a room with hessian "curtains" hanging from the ceiling soaked in water. The water would evaporate and thereby cool the hessian curtains and thereby the air circulating in the room. This would allow many perishables such as fruit butter and cured meats to be kept that would normally spoil in the heat.
Ice harvesting.
Before 1830, few Americans used ice to refrigerate foods due to a lack of ice-storehouses and iceboxes. As these two things became more widely available, individuals used axes and saws to harvest ice for their storehouses. This method proved to be difficult, dangerous, and certainly did not resemble anything that could be duplicated on a commercial scale.
Despite the difficulties of harvesting ice, Frederic Tudor thought that he could capitalize on this new commodity by harvesting ice in New England and shipping it to the Caribbean islands as well as the southern states. In the beginning, Tudor lost thousands of dollars, but eventually turned a profit as he constructed icehouses in Charleston, Virginia and in the Cuban port town of Havana. These icehouses as well as better insulated ships helped reduce ice wastage from 66% to 8%. This efficiency gain influenced Tudor to expand his ice market to other towns with icehouses such as New Orleans and Savannah. This ice market further expanded as harvesting ice became faster and cheaper after one of Tudor’s suppliers, Nathaniel Wyeth, invented a horse-drawn ice cutter in 1825. This invention as well as Tudor’s success inspired others to get involved in the ice trade and the ice industry grew.
Ice became a mass-market commodity by the early 1830s with the price of ice dropping from six cents per pound to a half of a cent per pound. In New York City, ice consumption increased from 12,000 tons in 1843 to 100,000 tons in 1856. Boston’s consumption leapt from 6,000 tons to 85,000 tons during that same period. Ice harvesting created a “cooling culture” as majority of people used ice and iceboxes to store their dairy products, fish, meat, and even fruits and vegetables. These early cold storage practices paved the way for many Americans to accept the refrigeration technology that would soon take over the country.
Refrigeration research.
The history of artificial refrigeration began when Scottish professor William Cullen designed a small refrigerating machine in 1755. Cullen used a pump to create a partial vacuum over a container of diethyl ether, which then boiled, absorbing heat from the surrounding air. The experiment even created a small amount of ice, but had no practical application at that time.
In 1758, Benjamin Franklin and John Hadley, professor of chemistry, collaborated on a project investigating the principle of evaporation as a means to rapidly cool an object at Cambridge University, England. They confirmed that the evaporation of highly volatile liquids, such as alcohol and ether, could be used to drive down the temperature of an object past the freezing point of water. They conducted their experiment with the bulb of a mercury thermometer as their object and with a bellows used to "quicken" the evaporation; they lowered the temperature of the thermometer bulb down to , while the ambient temperature was . They noted that soon after they passed the freezing point of water (32 °F), a thin film of ice formed on the surface of the thermometer's bulb and that the ice mass was about a quarter inch thick when they stopped the experiment upon reaching . Franklin wrote, "From this experiment, one may see the possibility of freezing a man to death on a warm summer's day". In 1805, American inventor Oliver Evans described a closed vapor-compression refrigeration cycle for the production of ice by ether under vacuum.
In 1820, the English scientist Michael Faraday liquefied ammonia and other gases by using high pressures and low temperatures, and in 1834, an American expatriate to Great Britain, Jacob Perkins, built the first working vapor-compression refrigeration system in the world. It was a closed-cycle that could operate continuously, as he described in his patent:
His prototype system worked although it did not succeed commercially.
In 1842, a similar attempt was made by American physician, John Gorrie, who built a working prototype, but it was a commercial failure. Like many of the medical experts during this time, Gorrie thought too much exposure to tropical heat led to mental and physical degeneration, as well as the spread of diseases such as malaria. He conceived the idea of using his refrigeration system to cool the air for comfort in homes and hospitals to prevent disease. American engineer Alexander Twining took out a British patent in 1850 for a vapour compression system that used ether.
The first practical vapor compression refrigeration system was built by James Harrison, a British journalist who had emigrated to Australia. His 1856 patent was for a vapour compression system using ether, alcohol or ammonia. He built a mechanical ice-making machine in 1851 on the banks of the Barwon River at Rocky Point in Geelong, Victoria, and his first commercial ice-making machine followed in 1854. Harrison also introduced commercial vapour-compression refrigeration to breweries and meat packing houses, and by 1861, a dozen of his systems were in operation. He later entered the debate of how to compete against the American advantage of unrefrigerated beef sales to the United Kingdom. In 1873 he prepared the sailing ship "Norfolk" for an experimental beef shipment to the United Kingdom, which used a cold room system instead of a refrigeration system. The venture was a failure as the ice was consumed faster than expected.
The first gas absorption refrigeration system using gaseous ammonia dissolved in water (referred to as "aqua ammonia") was developed by Ferdinand Carré of France in 1859 and patented in 1860. Carl von Linde, an engineer specializing in steam locomotives and professor of engineering at the Technological University of Munich in Germany, began researching refrigeration in the 1860s and 1870s in response to demand from brewers for a technology that would allow year-round, large-scale production of lager; he patented an improved method of liquefying gases in 1876. His new process made possible using gases such as ammonia, sulfur dioxide (SO2) and methyl chloride (CH3Cl) as refrigerants and they were widely used for that purpose until the late 1920s.
Thaddeus Lowe, an American balloonist, held several patents on ice-making machines. His "Compression Ice Machine" would revolutionize the cold-storage industry. In 1869, other investors and he purchased an old steamship onto which they loaded one of Lowe's refrigeration units and began shipping fresh fruit from New York to the Gulf Coast area, and fresh meat from Galveston, Texas back to New York, but because of Lowe's lack of knowledge about shipping, the business was a costly failure.
Commercial use.
In 1842, John Gorrie created a system capable of refrigerating water to produce ice. Although it was a commercial failure, it inspired scientists and inventors around of the world. France’s Ferdinand Carre was one of the inspired and he created an ice producing system that was simpler and smaller than that of Gorrie. During the Civil War, cities such as New Orleans could no longer get ice from New England via the coastal ice trade. Carre’s refrigeration system became the solution to New Orleans ice problems and by 1865 the city had three of Carre’s machines. In 1867, in San Antonio, Texas, a French immigrant named Andrew Muhl built an ice-making machine to help service the expanding beef industry before moving it to Waco in 1871. In 1873, the patent for this machine was contracted by the Columbus Iron Works, a company acquired by the W. C. Bradley Co., which went on to produce the first commercial ice-makers in the US.
By the 1870s, breweries had become the largest users of harvested ice. Though the ice-harvesting industry had grown immensely by the turn of the 20th century, pollution and sewage had begun to creep into natural ice, making it a problem in the metropolitan suburbs. Eventually, breweries began to complain of tainted ice. Public concern for the purity of water, from which ice was formed, began to increase in the early 1900s with the rise of germ theory. Numerous media outlets published articles connecting diseases such as typhoid fever with natural ice consumption. This caused ice harvesting to become illegal in certain areas of the country. All of these scenarios increased the demands for modern refrigeration and manufactured ice. Ice producing machines like that of Carre’s and Muhl’s were looked to as means of producing ice to meet the needs of grocers, farmers, and food shippers.
Refrigerated railroad cars were introduced in the US in the 1840s for short-run transport of dairy products, but these used harvested ice to maintain a cool temperature.
The new refrigerating technology first met with widespread industrial use as a means to freeze meat supplies for transport by sea from the British Dominions and other countries to the British Isles. The first to achieve this breakthrough was an entrepreneur who had emigrated to New Zealand. William Soltau Davidson thought that Britain's rising population and meat demand could mitigate the slump in world wool markets that was heavily affecting New Zealand. After extensive research, he commissioned the "Dunedin" to be refitted with a compression refrigeration unit for meat shipment in 1881. On February 15, 1882, the "Dunedin" sailed for London with what was to be the first commercially successful refrigerated shipping voyage, and the foundation of the refrigerated meat industry.
"The Times" commented "Today we have to record such a triumph over physical difficulties, as would have been incredible, even unimaginable, a very few days ago...". The "Marlborough"—sister ship to the "Dunedin" – was immediately converted and joined the trade the following year, along with the rival New Zealand Shipping Company vessel "Mataurua", while the German Steamer "Marsala" began carrying frozen New Zealand lamb in December 1882. Within five years, 172 shipments of frozen meat were sent from New Zealand to the United Kingdom, of which only 9 had significant amounts of meat condemned. Refrigerated shipping also led to a broader meat and dairy boom in Australasia and South America. J & E Hall of Dartford, England outfitted the 'SS Selembria' with a vapor compression system to bring 30,000 carcasses of mutton from the Falkland Islands in 1886. In the years ahead, the industry rapidly expanded to Australia, Argentina and the United States.
By the 1890s, refrigeration played a vital role in the distribution of food. The meat-packing industry relied heavily on natural ice in the 1880s and continued to rely on manufactured ice as those technologies became available. By 1900, the meat-packing houses of Chicago had adopted ammonia-cycle commercial refrigeration. By 1914, almost every location used artificial refrigeration. The big meat-packers, Armour, Swift, and Wilson, had purchased the most expensive units which they installed on train cars and in branch houses and storage facilities in the more remote distribution areas. 
By the middle of the 20th century, refrigeration units were designed for installation on trucks or lorries. Refrigerated vehicles are used to transport perishable goods, such as frozen foods, fruit and vegetables, and temperature-sensitive chemicals. Most modern refrigerators keep the temperature between –40 and –20 °C, and have a maximum payload of around 24,000 kg gross weight (in Europe). 
Although commercial refrigeration quickly progressed, it had limitations that prevented it from moving into the household. First, most refrigerators were far too large. Some of the commercial units being used in 1910 weighed between five and two hundred tons. Second, commercial refrigerators were expensive to produce, purchase, and maintain. Lastly, these refrigerators were unsafe. It was not uncommon for commercial refrigerators to catch fire, explode, or leak toxic gases. Refrigeration did not become a household technology until these three challenges were overcome.
Home and consumer use.
During the early 1800s, consumers preserved their food by storing food and ice purchased from ice harvesters in iceboxes. In 1803, Thomas Moore patented a metal-lined butter-storage tub which became the prototype for most iceboxes. These iceboxes were used until nearly 1910 and the technology did not progress. In fact, consumers that used the icebox in 1910 faced the same challenge of a moldy and stinky icebox that consumers had in the early 1800s. 
General Electric (GE) was one of the first companies to overcome these challenges. In 1911, GE released a household refrigeration unit that was powered by gas. The use of gas eliminated the need for motor and decreased the size of the refrigerator. However, electric companies that were customers of GE did not benefit from a gas-powered unit. Thus, GE invested in developing an electric model. In 1927, GE released the Monitor Top, the first refrigerator to run off electricity.
In 1930, Frigidaire, one of GE’s main competitors, synthesized Freon. With the invention of synthetic refrigerants based mostly on a chlorofluorocarbon (CFC) chemical, safer refrigerators were possible for home and consumer use. Freon led to the development of smaller, lighter, and cheaper refrigerators. The average price of a refrigerator dropped from $275 to $154 with the synthesis of Freon. This lower price allowed ownership of refrigerators in American households to exceed 50%. Freon is a trademark of the DuPont Corporation and refers to these CFCs, and later hydro chlorofluorocarbon (HCFC) and hydro fluorocarbon (HFC), refrigerants developed in the late 1920s. These refrigerants were considered at the time to be less harmful than the commonly-used refrigerants of the time, including methyl formate, ammonia, methyl chloride, and sulfur dioxide. The intent was to provide refrigeration equipment for home use without danger. These CFC refrigerants answered that need. In the 1970s, though, the compounds were found to be reacting with atmospheric ozone, an important protection against solar ultraviolet radiation, and their use as a refrigerant worldwide was curtailed in the Montreal Protocol of 1987.
Impact on settlement patterns.
In the last century refrigeration allowed new settlement patterns to emerge. This new technology has allowed for new areas to be settled that are not on a natural channel of transport such as a river, valley trail or harbor that may have otherwise not been settled. Refrigeration has given opportunities to early settlers to expand westward and into rural areas that were unpopulated. These new settlers with rich and untapped soil saw opportunity to profit by sending raw goods to the eastern cities and states. In the 20th century, refrigeration has made “Galactic Cities” such as Dallas, Phoenix and Los Angeles possible.
Refrigerated rail cars.
The refrigerated rail car, along with the dense railroad network, became an exceedingly important link between the marketplace and the farm allowing for a national opportunity rather than a just a regional one. Before the invention of the refrigerated rail car it was impossible to ship perishable food products long distances. The beef packing industry made the first demand push for refrigeration cars. The railroad companies were slow to adopt this new invention because of their heavy investments in cattle cars, stockyards, and feedlots. Refrigeration cars were also complex and costly compared to other rail cars, which also slowed the adoption of the refrigerated rail car. After the slow adoption of the refrigerated car, the beef packing industry dominated the refrigerated rail car business with their ability to control ice plants and the setting of icing fees. The United States Department of Agriculture estimated that in 1916 over sixty-nine percent of the cattle killed in the country was done in plants involved in interstate trade. The same companies that were also involved in the meat trade later implemented refrigerated transport to include vegetables and fruit. The meat packing companies had much of the expensive machinery, such as refrigerated cars, and cold storage facilities that allowed for them to effectively distribute all types of perishable goods. During World War I, a national refrigerator car pool was established by the United States Administration to deal with problem of idle cars and was later continued after the war. The idle car problem was the problem of refrigeration cars sitting pointlessly in between seasonal harvests. This meant that very expensive cars sat in rail yards for a good portion of the year while making no revenue for the car’s owner. The car pool was a system where cars were distributed to areas as crops matured ensuring maximum use of the cars. Refrigerated rail cars moved eastward from vineyards, orchards, fields, and gardens in western states to satisfy Americas consuming market in the east. The refrigerated car made it possible to transport perishable crops hundreds and even thousands of miles. The most noticeable effect the car gave was a regional specialization of vegetables and fruits. The refrigeration rail car was widely used for the transportation of perishable goods up until the 1950s. By the 1960s the nation's interstate highway system was adequately complete allowing for trucks to carry the majority of the perishable food loads and to push out the old system of the refrigerated rail carts.
Expansion west and into rural areas.
The widespread use of refrigeration allowed for a vast amount of new agricultural opportunities to open up in the United States. New markets emerged throughout the United States in areas that were previously uninhabited and far-removed from heavily populated areas. New agricultural opportunity presented itself in areas that were considered rural such as states in the south and in the west. Shipments on a large scale from the south and California were both made around the same time although natural ice was used from the Sierras in California rather than manufactured ice in the south. Refrigeration allowed for many areas to specialize in the growing of specific fruits. California specialized in several fruits, grapes, peaches, pears, plums, and apples while Georgia became famous for specifically its peaches. In California, the acceptance of the refrigerated rail carts lead to an increase of car loads from 4,500 carloads in 1895 to between 8,000 and 10,000 carloads in 1905. The Gulf States, Arkansas, Missouri and Tennessee entered into strawberry production on a large-scale while Mississippi became the center of the tomato industry. New Mexico, Colorado, Arizona, and Nevada grew cantaloupes. Without refrigeration this would have not been possible. By 1917, well-established fruit and vegetable areas that were close to eastern markets felt the pressure of competition from these distant specialized centers. Refrigeration was not limited to meat, fruit and vegetables but it also encompassed dairy product and dairy farms. In the early twentieth century large cities got their dairy supply from farms as far as 400 miles. Dairy products were not as easily transported great distances like fruits and vegetables due to greater perishability. Refrigeration made production possible in the west far from eastern markets, so much in fact that dairy farmers could pay transportation cost and still undersell their eastern competitors. Refrigeration and the refrigerated rail gave opportunity to areas with rich soil far from natural channel of transport such as a river, valley trail or harbors.
Rise of the galactic city.
"Edge city" was a term coined by Joel Garreau, whereas the term "galactic city" was coined by Lewis Mumford. These terms refer to a concentration of business, shopping, and entertainment outside a traditional downtown or central business district in what had previously been a residential or rural area. There were several factors contributing to the growth of these cities such as Los Angeles, Las Vegas, Houston, and Phoenix. The factors that contributed to these large cities include reliable automobiles, highway systems, refrigeration, and agricultural production increases. Large cities such as the ones mentioned above have not been uncommon in history but what separates these cities from the rest are that these cities are not along some natural channel of transport, or at some crossroad of two or more channels such as a trail, harbor, mountain, river, or valley. These large cities have been developed in areas that only a few hundred years ago would have been uninhabitable. Without a cost efficient way of cooling air and transporting water and food great distances these large cities would have never developed. The rapid growth of these cities was influenced by refrigeration and an agricultural productivity increase, allowing more distant farms to effectively feed the population.
Impact on agriculture and food production.
Agriculture’s role in developed countries has drastically changed in the last century due to many factors, including refrigeration. Statistics from the 2007 census gives information on the large concentration of agricultural sales coming from a small portion of the existing farms in the United States today. This is a partial result of the market created for the frozen meat trade by the first successful shipment of frozen sheep carcasses coming from New Zealand in the 1880s. As the market continued to grow, regulations on food processing and quality began to be enforced. Eventually, electricity was introduced into rural homes in the United States, which allowed refrigeration technology to continue to expand on the farm, increasing output per person. Today, refrigeration’s use on the farm reduces humidity levels, avoids spoiling due to bacterial growth, and assists in preservation.
Demographics.
The introduction of refrigeration and evolution of additional technologies drastically changed agriculture in the United States. During the beginning of the 20th century, farming was a common occupation and lifestyle for United States citizens, as most farmers actually lived on their farm. In 1935, there were 6.8 million farms in the United States and a population of 127 million. Yet, while the United States population has continued to climb, citizens pursuing agriculture continue to decline. Based on the 2007 US Census, less than one percent of a population of 310 million people claim farming as an occupation today. However, the increasing population has led to an increasing demand for agricultural products, which is met through a greater variety of crops, fertilizers, pesticides, and improved technology. Improved technology has decreased the risk and time involved if agricultural management and allows larger farms to increase their output per person to meet society’s demand.
Meat packing and trade.
Prior to 1882, the South Island of New Zealand had been experimenting with sowing grass and crossbreeding sheep, which immediately gave their farmers economic potential in the exportation of meat. In 1882, the first successful shipment of sheep carcasses was sent from Port Chalmers in Dunedin, New Zealand to London. By the 1890s, the frozen meat trade became increasingly more profitable in New Zealand, especially in Canterbury, where 50% of exported sheep carcasses came from in 1900. It wasn’t long before Canterbury meat was known for the highest quality, creating a demand for New Zealand meat around the world. In order to meet this new demand, the farmers improved their feed so sheep to be ready for the slaughter in only seven months. This new method of shipping lead to an economic boom in New Zealand by the mid 1890s.
In the United States, the Meat Inspection Act of 1891 was put in place in the United States because local butchers felt the refrigerated railcar system was unwholesome. When meat packing began to take off, consumers became nervous about the quality of the meat for consumption. The Jungle, a book written by Upton Sinclair, brought negative attention to the meat packing industry, by drawing to light unsanitary working conditions and processing of diseased animals. The book caught the attention of President Theodore Roosevelt, and the 1906 Meat Inspection Act was put into place as an amendment to the Meat Inspection Act of 1891. This new act focused on the quality of the meat and environment it is processed in.
Electricity in rural areas.
In the early 1930s, 90 percent of the urban population had electric power, in comparison to only 10 percent of rural homes. At the time, power companies did not feel that extending power to rural areas would produce enough profit to make it worth their while. However, in the midst of the Great Depression, President Franklin D. Roosevelt realized that rural areas would continue to lag behind urban areas in both poverty and production if they were not electrically wired. On May 11, 1935, the president signed an executive order called the Rural Electrification Administration, also known as REA. The agency provided loans to fund electric infrastructure in the rural areas. In just a few years, 300,000 people in rural areas of the United States had received power in their homes.
While electricity dramatically improved working conditions on farms, it also had a large impact on the safety of food production. Refrigeration systems were introduced to the farming process, which helped in food preservation and kept food supplies safe. Refrigeration also allowed for production of perishable commodities, which could then be shipped throughout the United States. As a result, the United States farmers quickly became the most productive in the world.
Farm use.
In order to reduce humidity levels and spoiling due to bacterial growth, refrigeration is used for meat, produce, and dairy processing in farming today. Refrigeration systems are used the heaviest in the warmer months for farming produce, which must be cooled as soon as possible in order to meet quality standards and increase the shelf life. Meanwhile, dairy farms refrigerate milk year round to avoid spoiling.
Effects on lifestyle and diet.
A trip to the market before refrigeration became widespread would have been different from a trip today. In the late 19th Century and into the very early 20th Century, other than staple foods (sugar, rice, and beans), your diet was affected heavily by the seasons and what could be grown relatively close to your region. Today, thanks to refrigeration, we are no longer restricted by these limitations. Refrigeration played a large part in the feasibility and then popularity of the modern supermarket. If you are willing to pay slightly more for a fruit or vegetable that is out of season in your region, your local supermarket will most likely have what you are looking for. Refrigerators have led to a huge increase in meat and dairy as a portion of overall supermarket sales. As well as changing the goods purchased at the market, the ability to store these foods for extended periods of time has led to an increase in leisure time. Prior to the advent of the household refrigerator, people would have to shop on a daily basis for the supplies needed for their meals.
Impact on nutrition.
The introduction of refrigeration allowed for the hygienic handling and storage of perishables, and as such, promoted output growth, consumption, and nutrition. The change in our method of food preservation moved us away from salts to a more manageable sodium level. The ability to move and store perishables such as meat and dairy led to a 1.7% increase in dairy consumption and overall protein intake by 1.25% annually in the US after the 1890s. People were not only consuming these perishables because it became easier for they themselves to store them, but because the innovations in refrigerated transportation and storage led to less spoilage and waste, and drove the prices of these products down. Refrigeration accounts for at least 5.1% of the increase in adult stature (in the US) through improved nutrition including the indirect effects associated with improvements in the quality of nutrients and the reduction in illness, the overall impact was considerably larger. Recent studies have also shown a negative relationship between the number of refrigerators in a household and the rate of gastric cancer mortality.
Current applications of refrigeration.
Probably the most widely used current applications of refrigeration are for air conditioning of private homes and public buildings, and refrigerating foodstuffs in homes, restaurants and large storage warehouses. The use of refrigerators in kitchens for storing fruits and vegetables has allowed adding fresh salads to the modern diet year round, and storing fish and meats safely for long periods.
Optimum temperature range for perishable food storage is .
In commerce and manufacturing, there are many uses for refrigeration. Refrigeration is used to liquify gases - oxygen, nitrogen, propane and methane, for example. In compressed air purification, it is used to condense water vapor from compressed air to reduce its moisture content. In oil refineries, chemical plants, and petrochemical plants, refrigeration is used to maintain certain processes at their needed low temperatures (for example, in alkylation of butenes and butane to produce a high octane gasoline component). Metal workers use refrigeration to temper steel and cutlery. In transporting temperature-sensitive foodstuffs and other materials by trucks, trains, airplanes and seagoing vessels, refrigeration is a necessity.
Dairy products are constantly in need of refrigeration, and it was only discovered in the past few decades that eggs needed to be refrigerated during shipment rather than waiting to be refrigerated after arrival at the grocery store. Meats, poultry and fish all must be kept in climate-controlled environments before being sold. Refrigeration also helps keep fruits and vegetables edible longer.
One of the most influential uses of refrigeration was in the development of the sushi/sashimi industry in Japan. Before the discovery of refrigeration, many sushi connoisseurs were at risk of contracting diseases. The dangers of unrefrigerated sashimi were not brought to light for decades due to the lack of research and healthcare distribution across rural Japan. Around mid-century, the Zojirushi corporation, based in Kyoto, made breakthroughs in refrigerator designs, making refrigerators cheaper and more accessible for restaurant proprietors and the general public.
Methods of refrigeration.
Methods of refrigeration can be classified as "non-cyclic", "cyclic", "thermoelectric" and "magnetic".
Non-cyclic refrigeration.
This refrigeration method cools a contained area by melting ice, or by sublimating dry ice. Perhaps the simplest example of this is a portable cooler, where items are put in it, then ice is poured over the top. Regular ice can maintain temperatures near, but not below the freezing point, unless salt is used to cool the ice down further (as in a traditional ice-cream maker). Dry ice can reliably bring the temperature well below freezing.
Cyclic refrigeration.
This consists of a refrigeration cycle, where heat is removed from a low-temperature space or source and rejected to a high-temperature sink with the help of external work, and its inverse, the thermodynamic power cycle. In the power cycle, heat is supplied from a high-temperature source to the engine, part of the heat being used to produce work and the rest being rejected to a low-temperature sink. This satisfies the second law of thermodynamics.
A "refrigeration cycle" describes the changes that take place in the refrigerant as it alternately absorbs and rejects heat as it circulates through a refrigerator. It is also applied to heating, ventilation, and air conditioning HVACR work, when describing the "process" of refrigerant flow through an HVACR unit, whether it is a packaged or split system.
Heat naturally flows from hot to cold. Work is applied to cool a living space or storage volume by pumping heat from a lower temperature heat source into a higher temperature heat sink. Insulation is used to reduce the work and energy needed to achieve and maintain a lower temperature in the cooled space. The operating principle of the refrigeration cycle was described mathematically by Sadi Carnot in 1824 as a heat engine.
The most common types of refrigeration systems use the reverse-Rankine vapor-compression refrigeration cycle, although absorption heat pumps are used in a minority of applications.
Cyclic refrigeration can be classified as:
Vapor cycle refrigeration can further be classified as:
Vapor-compression cycle.
The vapor-compression cycle is used in most household refrigerators as well as in many large commercial and industrial refrigeration systems. Figure 1 provides a schematic diagram of the components of a typical vapor-compression refrigeration system. The thermodynamics of the cycle can be analyzed on a diagram as shown in Figure 2. In this cycle, a circulating refrigerant such as Freon enters the compressor as a vapor. From point 1 to point 2, the vapor is compressed at constant entropy and exits the compressor as a vapor at a higher temperature, but still below the vapor pressure at that temperature. From point 2 to point 3 and on to point 4, the vapor travels through the condenser which cools the vapor until it starts condensing, and then condenses the vapor into a liquid by removing additional heat at constant pressure and temperature. Between points 4 and 5, the liquid refrigerant goes through the expansion valve (also called a throttle valve) where its pressure abruptly decreases, causing flash evaporation and auto-refrigeration of, typically, less than half of the liquid. That results in a mixture of liquid and vapor at a lower temperature and pressure as shown at point 5. The cold liquid-vapor mixture then travels through the evaporator coil or tubes and is completely vaporized by cooling the warm air (from the space being refrigerated) being blown by a fan across the evaporator coil or tubes. The resulting refrigerant vapor returns to the compressor inlet at point 1 to complete the thermodynamic cycle.
The above discussion is based on the ideal vapor-compression refrigeration cycle, and does not take into account real-world effects like frictional pressure drop in the system, slight thermodynamic irreversibility during the compression of the refrigerant vapor, or non-ideal gas behavior, if any.
More information about the design and performance of vapor-compression refrigeration systems is available in the classic "Perry's Chemical Engineers' Handbook".
Vapor absorption cycle.
In the early years of the twentieth century, the vapor absorption cycle using water-ammonia systems was popular and widely used. After the development of the vapor compression cycle, the vapor absorption cycle lost much of its importance because of its low coefficient of performance (about one fifth of that of the vapor compression cycle). Today, the vapor absorption cycle is used mainly where fuel for heating is available but electricity is not, such as in recreational vehicles that carry LP gas. It is also used in industrial environments where plentiful waste heat overcomes its inefficiency.
The absorption cycle is similar to the compression cycle, except for the method of raising the pressure of the refrigerant vapor. In the absorption system, the compressor is replaced by an absorber which dissolves the refrigerant in a suitable liquid, a liquid pump which raises the pressure and a generator which, on heat addition, drives off the refrigerant vapor from the high-pressure liquid. Some work is needed by the liquid pump but, for a given quantity of refrigerant, it is much smaller than needed by the compressor in the vapor compression cycle. In an absorption refrigerator, a suitable combination of refrigerant and absorbent is used. The most common combinations are ammonia (refrigerant) with water (absorbent), and water (refrigerant) with lithium bromide (absorbent).
Gas cycle.
When the working fluid is a gas that is compressed and expanded but doesn't change phase, the refrigeration cycle is called a "gas cycle". Air is most often this working fluid. As there is no condensation and evaporation intended in a gas cycle, components corresponding to the condenser and evaporator in a vapor compression cycle are the hot and cold gas-to-gas heat exchangers in gas cycles.
The gas cycle is less efficient than the vapor compression cycle because the gas cycle works on the reverse Brayton cycle instead of the reverse Rankine cycle. As such the working fluid does not receive and reject heat at constant temperature. In the gas cycle, the refrigeration effect is equal to the product of the specific heat of the gas and the rise in temperature of the gas in the low temperature side. Therefore, for the same cooling load, a gas refrigeration cycle needs a large mass flow rate and is bulky.
Because of their lower efficiency and larger bulk, "air cycle" coolers are not often used nowadays in terrestrial cooling devices. However, the air cycle machine is very common on gas turbine-powered jet aircraft as cooling and ventilation units, because compressed air is readily available from the engines' compressor sections. Such units also serve the purpose of pressurizing the aircraft.
Thermoelectric refrigeration.
Thermoelectric cooling uses the Peltier effect to create a heat flux between the junction of two different types of materials. This effect is commonly used in camping and portable coolers and for cooling electronic components and small instruments.
Magnetic refrigeration.
Magnetic refrigeration, or adiabatic demagnetization, is a cooling technology based on the magnetocaloric effect, an intrinsic property of magnetic solids. The refrigerant is often a paramagnetic salt, such as cerium magnesium nitrate. The active magnetic dipoles in this case are those of the electron shells of the paramagnetic atoms.
A strong magnetic field is applied to the refrigerant, forcing its various magnetic dipoles to align and putting these degrees of freedom of the refrigerant into a state of lowered entropy. A heat sink then absorbs the heat released by the refrigerant due to its loss of entropy. Thermal contact with the heat sink is then broken so that the system is insulated, and the magnetic field is switched off. This increases the heat capacity of the refrigerant, thus decreasing its temperature below the temperature of the heat sink.
Because few materials exhibit the needed properties at room temperature, applications have so far been limited to cryogenics and research.
Other methods.
Other methods of refrigeration include the air cycle machine used in aircraft; the vortex tube used for spot cooling, when compressed air is available; and thermoacoustic refrigeration using sound waves in a pressurized gas to drive heat transfer and heat exchange; steam jet cooling popular in the early 1930s for air conditioning large buildings; thermoelastic cooling using a smart metal alloy stretching and relaxing. Many Stirling cycle heat engines can be run backwards to act as a refrigerator, and therefore these engines have a niche use in cryogenics. In addition there are other types of cryocoolers such as Gifford-McMahon coolers, Joule-Thomson coolers, pulse-tube refrigerators and, for temperatures between 2 mK and 500 mK, dilution refrigerators.
Fridge Gate.
The Fridge Gate method is a theoretical application of using a single logic gate to drive a refrigerator in the most energy efficient way possible without violating the laws of thermodynamics. It operates on the fact that there are two energy states in which a particle can exist: the ground state and the excited state. The excited state carries a little more energy than the ground state, small enough so that the transition occurs with high probability. There are three components or particle types associated with the fridge gate. The first is on the interior of the fridge, the second on the outside and the third is connected to a power supply which heats up every so often that it can reach the E state and replenish the source. In the cooling step on the inside of the fridge, the g state particle absorbs energy from ambient particles, cooling them, and itself jumping to the e state. In the second step, on the outside of the fridge where the particles are also at an e state, the particle falls to the g state, releasing energy and heating the outside particles. In the third and final step, the power supply moves a particle at the e state, and when it falls to the g state it induces an energy-neutral swap where the interior e particle is replaced by a new g particle, restarting the cycle.
Capacity ratings.
The measured capacity of refrigeration is always dimensioned in units of power. Domestic and commercial refrigerators may be rated in kJ/s, or Btu/h of cooling. For commercial and industrial refrigeration systems, most of the world uses the kilowatt (kW) as the basic unit of refrigeration. Typically, commercial and industrial refrigeration systems in North America are rated in tons of refrigeration (TR). Historically, one TR was defined as the energy removal rate that will freeze one short ton of water at 0 °C (32 °F) in one day. This was very important because many early refrigeration systems were in ice houses. The simple unit allowed owners of these early refrigeration systems to measure a day's output of ice against energy consumption, and to compare their plant to one down the street. While ice houses make up a much smaller part of the refrigeration industry than they once did, the unit TR has remained in North America. The unit's value as historically defined was approximately 11,958 Btu/hr (3.505 kW), and has now been conventionally redefined as exactly 12,000 Btu/hr (3.517 kW).
A refrigeration system's coefficient of performance (CoP) is very important in determining a system's overall efficiency. It is defined as refrigeration capacity in kW divided by the energy input in kW. While CoP is a very simple measure of performance, it is typically not used for industrial refrigeration in North America. Owners and manufacturers of these systems typically use performance factor (PF). A system's PF is defined as a system's energy input in horsepower divided by its refrigeration capacity in TR. Both CoP and PF can be applied to either the entire system or to system components. For example, an individual compressor can be rated by comparing the energy needed to run the compressor versus the expected refrigeration capacity based on inlet volume flow rate. It is important to note that both CoP and PF for a refrigeration system are only defined at specific operating conditions, including temperatures and thermal loads. Moving away from the specified operating conditions can dramatically change a system's performance.

</doc>
<doc id="46242" url="https://en.wikipedia.org/wiki?curid=46242" title="Charles Lane Poor">
Charles Lane Poor

Charles Lane Poor (January 18, 1866 – September 27, 1951) was an American astronomy professor, noted for his opposition to Einstein's theory of relativity.
Biography.
He was born on January 18, 1866 in Hackensack, New Jersey to Edward Erie Poor.
He graduated from the City College of New York and received a Ph.D. in 1892 from Johns Hopkins University. Poor became an astronomer and professor of celestial mechanics at Columbia University from 1903 to 1944, when he was named Professor Emeritus. He published several works disputing the evidence for Einstein's theory of relativity during the 1920s. Poor published a series of papers that reflect objections the theory of relativity.
For 25 years, Poor was chairman of the admissions committee of the New York Yacht Club. In addition, he was a fellow of the Royal Astronomical Society and an associate fellow of the American Academy of Arts and Sciences. He served several terms as mayor of Dering Harbor on Long Island, New York, and invented a "line of position computers" for yachting navigation. At Columbia University, Poor was a teacher of the astronomer Samuel A. Mitchell, who went on to become director of the Leander McCormick Observatory at the University of Virginia.
He died on September 27, 1951.
Legacy.
One of Poor's sons, Edmund Ward Poor, was one of ten co-founders of Grumman Aircraft on Long Island. Another son was Alfred Easton Poor, an architect.

</doc>
<doc id="46245" url="https://en.wikipedia.org/wiki?curid=46245" title="Kenneth Wolstenholme">
Kenneth Wolstenholme

Kenneth Wolstenholme DFC & Bar (17 July 1920 – 25 March 2002) was the football commentator for BBC television in the 1950s and 1960s, most notable for his commentary during the 1966 FIFA World Cup which included the famous phrase "some people are on the pitch...they think it's all over...it is now!", as Geoff Hurst scored England's fourth goal.
Early life.
Wolstenholme was born in Worsley, Lancashire. His family were Primitive Methodists and his brother attended Elmfield College. He attended Farnworth Grammar School, where Alan Ball, Jr. (on whom Wolstenholme commentated in the 1966 World Cup Final) was also a pupil some years later.
World War II.
Wolstenholme started his career as a journalist with a newspaper in Manchester, as a member of the RAFVR he was soon called up.
In 1941, Wolstenholme qualified as a bomber pilot and was posted to 107 Squadron, flying Mark IV Bristol Blenheims out of RAF Great Massingham, Norfolk. At the start of 1943 he transferred to de Havilland Mosquito with 105 Squadron, part of Air Vice-Marshal Don Bennett's No. 8 Group RAF Pathfinder Group. Wolstenholme completed more than 100 highly hazardous sorties over Occupied Europe and in May 1944 was awarded the DFC.
The following year he won a Bar to his DFC for his continual bravery in raids on Germany in a period of exceptionally heavy night fighter activity. He finished the war as an acting Squadron Leader, having spent its last stages working in the RAF's public relations department.
Career.
After the war, he became a freelance journalist, working for BBC radio before moving to television in 1948. He lived in Worcester Park, Surrey. He covered the 1959 All-Ireland Senior Hurling Championship Final between Kilkenny and Waterford for BBC Television, an experience which moved him to describe hurling as his second favourite sport in the world after his first love, soccer.
1966 World Cup: "They think it's all over".
While most sports commentators gain some recognition if their career is long enough, Wolstenholme is best remembered for his commentary of the 1966 Football World Cup Final at Wembley Stadium, specifically the impromptu words he used with impeccable timing as the match came to a conclusion during injury time, as a small pitch invasion took place just as Geoff Hurst scored to put England 4–2 ahead:
These have become some of the most famous words in British sport, and a well known phrase in modern English. Wolstenholme always said that it was just a natural verbal piecing together of the situation before him and it took years before he realised just how well it fitted. Although unrehearsed, and spoken in the particular circumstances of the game, the words echoed to an extent those of German commentator Herbert Zimmermann - "It's over! Over! Over! Germany are the World Champions" - when West Germany won the World Cup in 1954.
Football commentator.
Wolstenholme commentated on English domestic football's most famous games of the 1950s and 1960s, including the first ever game featured on "Match of the Day" in 1964. He covered every FA Cup final between 1949 and 1971, the year of Arsenal's "double".
For the BBC he commentated on the 1960 European Cup Final between Real Madrid and Eintracht Frankfurt at Hampden Park, widely regarded as one of the greatest football matches ever played. Further highlights include his presence in the Estádio Nacional in Lisbon as Celtic overcame Internazionale in the 1967 European Cup Final, at Wembley as Manchester United defeated Benfica to capture the 1968 European Cup and also the BBC's main man at the 1970 World Cup, commentating on the final between Brazil and Italy. 
He left the corporation in 1971 after David Coleman was installed as the BBC's top commentator, his final BBC commentary being on the 1971 European Cup final between Ajax and Panathinaikos at Wembley Stadium.
Wolstenholme later commentated for Tyne Tees Television in the mid to late 1970s. After this, he went into semi-retirement, but re-appeared on TV to provide reports and occasional features for Channel 4 when they earned rights in the early 1990s to show Serie A games from Italy. He also took on an acting role, appearing in the BBC Radio 4 comedy series "Lenin of the Rovers" as football commentator Frank Lee Brian.
In 1998, Wolstenholme made a special appearance in EA Sports' videogame "World Cup 98", as the sole commentator on the game's classic World Cup matches, recreations of historic World Cup finals that included sepia-toned renditions of the 1930 and 1938 editions.
Legacy.
His most famous phrase was used as the title for the sports quiz programme "They Think It's All Over", on which he once appeared as a guest.
Bill Oddie wrote a song about Wolstenholme for the BBC radio comedy show "I'm Sorry, I'll Read That Again" which includes the lines: "I'm going Wolsten-home/And you can't get Wolsten "(worse than)" him!" In another sketch on "ISIRTA" a lady contestant in a television quiz show was awarded Wolstenholme as a prize.
Bolton Wanderers.
Kenneth was a boyhood supporter of Bolton Wanderers and was present as a guest for the final game at Burnden Park in April 1997. As an encore at the club's former home he re-created those words which had made him famous some 31 years earlier only using words which incorporated a Bolton theme.
He also narrated the club's "End of an Era" video which was released as part of Bolton's move from Burnden Park to the Reebok Stadium.
His famous words "They think it's all over, it is now" are engraved on a flagstone in the Churchgate area of Bolton town centre.

</doc>
<doc id="46247" url="https://en.wikipedia.org/wiki?curid=46247" title="Peter III of Russia">
Peter III of Russia

Peter III (21 February 1728 – ) () was emperor of Russia for six months in 1762. He was born in Kiel as Karl Peter Ulrich, the only child of Charles Frederick, Duke of Holstein-Gottorp and Anna Petrovna, the elder surviving daughter of Peter the Great. The German Peter could hardly speak Russian and pursued a strongly pro-Prussian policy, which made him an unpopular leader. He was deposed and possibly assassinated as a result of a conspiracy led by his German wife Princess Sophie Friederike Auguste von Anhalt-Zerbst-Dornburg, who succeeded him to the throne as Catherine II. His death could also have been the result of a drunken brawl with his bodyguard when he was being held captive after Catherine's coup.
Early life and character.
Peter was born in Kiel, in the duchy of Holstein-Gottorp. His parents were Charles Frederick, Duke of Holstein-Gottorp (a nephew of Charles XII of Sweden), and Anna Petrovna (a daughter of Emperor Peter I and Empress Catherine I of Russia). His mother died three months after his birth. In 1739, Peter's father died, and he became Duke of Holstein-Gottorp as Charles Peter Ulrich ().
When his aunt, Anna's younger sister Elizabeth, became Empress of Russia, she brought Peter from Germany to Russia and proclaimed him her heir presumptive in the autumn of 1742. Previously in 1742, the 14-year-old Peter was proclaimed King of Finland during the Russo-Swedish War (1741–1743), when Russian troops held Finland. This proclamation was based on his succession rights to territories held by his childless granduncle, the late Charles XII of Sweden who also had been Grand Duke of Finland. About the same time, in October 1742, he was chosen by the Swedish parliament to become heir presumptive to the Swedish throne. However, the Swedish parliament was unaware of the fact that he had also been proclaimed heir presumptive to the throne of Russia, and when their envoy arrived in Saint Petersburg in November, it was too late. It has been reported that the underage Peter's succession rights to Sweden were renounced on his behalf.
Empress Elizabeth arranged for Peter to marry his second cousin, Sophia Augusta Frederica (later Catherine the Great), daughter of Christian August, Prince of Anhalt-Zerbst and Johanna Elisabeth of Holstein-Gottorp. The young princess formally converted to Russian Orthodoxy and took the name Ekaterina Alexeievna (i.e., Catherine). They married on 21 August 1745. The marriage was not a happy one, but produced one son: the future Emperor Paul; and one daughter: Anna Petrovna (20 December 1757 – 19 March 1759). Catherine later claimed that Paul was not fathered by Peter: that, in fact, they had never consummated the marriage. During the sixteen years of their residence in Oranienbaum, Catherine took numerous lovers, while her husband did the same in the beginning.
The classical view of Peter's character is mainly drawn out of the memoirs of his wife and successor. She described him as an "idiot", "drunkard from Holstein", "good-for-nothing" etc. This portrait of Peter can be found in most history books, including 1911 Encyclopaedia Britannica:
There have been many attempts to revise the traditional characterisation of Peter and his policies. The Russian historian A.S. Mylnikov gives us a very different view of Peter III:
The German historian Elena Palmer goes even further, portraying Peter III as a cultured, open-minded emperor who tried to introduce various courageous, even democratic reforms in the 18th century's Russia. Currently, a newly established union is working on a project to build a memorial for Peter III in Kiel (North Germany), his city of birth.
The reign.
Foreign policy.
After Peter succeeded to the Russian throne (), he withdrew Russian forces from the Seven Years' War and concluded a peace treaty () with Prussia (the "Miracle of the House of Brandenburg"). He gave up Russian conquests in Prussia and offered 12,000 troops to make an alliance with Frederick II of Prussia (). Russia thus switched from an enemy of Prussia to an ally — Russian troops withdrew from Berlin and marched against the Austrians. This dramatically shifted the balance of power in Europe, suddenly handing the delighted Frederick the initiative. Frederick recaptured southern Silesia (October 1762) and subsequently forced Austria to the negotiating table.
As Duke of Holstein-Gottorp, Peter planned war against Denmark in order to restore parts of Schleswig to his Duchy. He focused on making alliances with Sweden and with England to ensure that they would not interfere on Denmark's behalf, while Russian forces gathered at Kolberg in Russian-occupied Pomerania. Alarmed at the Russian troops concentrating near their borders, unable to find any allies to resist Russian aggression, and short of money to fund a war, the government of Denmark threatened in late June to invade the free city of Hamburg in northern Germany to force a loan from it. Peter considered this a "casus belli", and prepared for open warfare against Denmark.
In June 1762, 40,000 Russian troops assembled in Pomerania under General Pyotr Rumyantsev, preparing to face 27,000 Danish troops under the French general Count St. Germain in case the Russian-Denmark freedom conference (scheduled for 1 July 1762 in Berlin under the patronage of Frederick II) failed to resolve the issue. But shortly before this Peter lost his throne () and the conference did not occur. The issue of Schleswig remained unresolved. Peter was accused of planning an unpatriotic war.
While historically Peter's planned war against Denmark was seen as being a political failure, recent scholarship has portrayed it as part of a pragmatic plan to secure his Holstein-Gottorp duchy and to expand the common Holstein-Russian power northward and westwards —he saw gaining territory and influence in Denmark and Northern Germany as more useful to Russia than taking East Prussia. Equally, he saw that friendship with Prussia and with Britain, following the latter's triumph in the Seven Years War, could offer more to aid his plans than alliance with either Austria or France.
Domestic reforms.
During his 186-day period of government, Peter III passed 220 new laws which he had developed and elaborated during his life as a crown prince. Elena Palmer claims that his reforms were of a democratic nature. He proclaimed religious freedom — in those times a revolutionary step, that not even the advanced Western Europe had taken. He fought corruption within government, established public litigation and abolished the secret police — a repressive organ started under Peter I and intended to expose it as betrayer of the state for its mercilessness and torture methods. Catherine recreated this institution and it remained present in Russia thereon. He established obligatory education for aristocrats — all aristocrats had to provide their children with education and report it to the senate. Furthermore, in some cities technical schools were established for middle and lower class children. Peter began the reorganization and modernization of the Russian army.
One of his most popular reforms was the manifesto of February 1762 that exempted the nobility from obligatory state and military service (established by Peter the Great) and gave them freedom to travel abroad. On the day Peter submitted this manifesto, the parliament proposed building a pure gold statue of him, but Peter refused, saying that there must be much better uses for gold in the country.
Peter III's economic policy reflected the rising influence of Western capitalism and the merchant class or “Third Estate” that accompanied it. He established the first state bank in Russia, rejected the nobility's monopoly on trade and encouraged mercantilism by increasing grain exports and forbidding the import of sugar and other materials that could be found in Russia.
Peter's short reign also addressed serfdom law and the status of serfs within Russia. For the first time, the killing of a peasant by a landowner became an act punishable by law. State peasants were given higher social status than estate peasants, and all peasants under the servitude of the church were transformed into the economy peasants similar to the state peasants. Peter also took further interest in church affairs, implementing his grandfather's plan to secularize church and monastic lands.
Overthrow.
The reign of Peter III is cast by Palmer as progressive for its focus on transforming economically developed feudal Russia to a more advanced European state. Palmer claims that his reform efforts were welcomed by society as a whole. It is Palmer's further contention that a plot against him by members of the government and influential nobles is unjustified: that the aristocratic names in the list of conspirators belonged to Guards officers, those who had lost influence and impoverished families who had no access to high government positions and were forced into service, some resentment within the Guard could not have led to a change of government. A revolt of the Guards regiments against the emperor, to whom they had sworn allegiance, could only lead to an alternative emperor. Palmer claims that the conspiracy against Peter III was carried out by Catherine and Guards officer Alexei Grigoryevich Orlov and was in fact nothing more than a murder for personal reasons. With the aid of the two Guards troops that Peter had planned to discipline more harshly, the emperor was arrested and forced to abdicate on 28 June. Shortly thereafter, he was transported to Ropsha, where he was supposedly assassinated, although no one is sure how Peter died.
Aftermath.
In December 1796, after succeeding Catherine, Peter's son the Emperor Paul, who disliked his mother, arranged for his remains to be exhumed and then reburied with full honors in the Peter and Paul Cathedral, where other tsars were buried.
After his death, four fake Peters (five if Šćepan Mali of Montenegro is included) came forth, supported by revolts among the people who believed in a rumor that Peter had not died, but had secretly been imprisoned by Catherine. The most famous was the Cossack Yemelyan Pugachev. Under this guise, he led what came to be known as Pugachev's Rebellion in 1774, ultimately crushed by Catherine's forces. In addition, Kondratii Selivanov, who led a castrating sect known as the Skoptsy, claimed to be both Jesus and Peter III.
The legend of Peter is still talked about, especially in the town where he lived most of his life, former Oranienbaum, later Lomonosov, situated on the southern coast of the Gulf of Finland, 40 km west of St. Petersburg. Peter's palace is the only one of the famous palaces in the St. Petersburg area that was not captured by the Germans during Second World War. During the war, the building was a school and people say the ghost of Peter protected the children of Oranienbaum from getting hurt by bombs. Furthermore, it was near this town that the siege of Leningrad ended in January 1944. People say that Peter, after his death, stopped Hitler's army near Leningrad, as the living Peter stopped the Russian army near Berlin.
Cultural references.
Peter has been depicted on screen a number of times, almost always in films concerning his wife Catherine. He was portrayed by Douglas Fairbanks, Jr. in the 1934 film "The Rise of Catherine the Great" and by Sam Jaffe in "The Scarlet Empress" the same year. In 1991 Reece Dinsdale portrayed him in the television series "Young Catherine". "La Tempesta" (1958) depicts Yemelyan Pugachev's effort to force his recognition as Peter III and offers a critical view of Catherine the Great, with Van Heflin in the role of Pugachev and Viveca Lindfors as Catherine. He was also depicted as a cowardly, drunken, wife beater in the Japanese anime Le Chevalier D'Eon.

</doc>
