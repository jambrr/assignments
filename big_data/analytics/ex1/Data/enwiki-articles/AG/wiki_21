<doc id="40648" url="https://en.wikipedia.org/wiki?curid=40648" title="Buffy Summers">
Buffy Summers

Buffy Anne Summers is a fictional character from the "Buffy the Vampire Slayer" franchise. She first appeared in the 1992 film "Buffy the Vampire Slayer" before going on to appear in the television series and subsequent comic book of the same name. The character has also appeared in the spin-off series "Angel", as well as numerous non-canon expanded universe material, such as novels, comics, and video games. Buffy was portrayed by Kristy Swanson in the film, and later by Sarah Michelle Gellar in the television series. Giselle Loren has lent her voice to the character in both the "Buffy" video games and an unproduced animated series, while Kelly Albanese lent her voice to the character in the "Buffy the Vampire Slayer Season Eight" motion comics.
Buffy is the protagonist of the story, and the series depicts her life and adventures as she grows up. In the film, she is a high school cheerleader who learns that she is the Slayer (a Chosen One gifted with the strength and skills to fight vampires, demons, and the forces of darkness). The television series shows Buffy carrying out her destiny in a small town built atop a portal to hell (Hellmouth), surrounded by a group of friends and family who support her in her mission. In the comic book continuation, she is a young woman who has accepted her duties and is now responsible for training others like her.
Buffy was created to subvert the stereotypical female horror film victim--Whedon wanted to create a strong female cultural icon. In 2004, Buffy was ranked at number 13 in Bravo's list of The 100 Greatest TV Characters. In June 2010, "Entertainment Weekly" ranked her third in its list of the 100 Greatest Characters of the Last 20 Years. AOL named her the sixth Most Memorable Female TV Character. She was ranked at No. 5 in AfterEllen.com's Top 50 Favorite Female TV Characters.
Appearances.
Film.
The character of Buffy first appears in the 1992 film "Buffy the Vampire Slayer", played by Kristy Swanson. The film, written by Joss Whedon, depicts Buffy as a shallow high school cheerleader who is informed by a man named Merrick (Donald Sutherland) that she has been chosen by fate to battle the undead. Buffy reluctantly undergoes training in her abilities by Merrick, and as her responsibility as the Slayer causes her to become alienated from her valley girl peers, she finds friendship and romance with fellow outcast Pike (Luke Perry). Merrick eventually comes to respect Buffy's rebellious nature, and she defeats vampire king Lothos (Rutger Hauer) by relying on her own contemporary style as opposed to traditional Slayer conventions. Although this film is not in continuity with the later television series, in 1999, author Christopher Golden adapted Joss Whedon's original script into a comic book entitled "The Origin", which Whedon later confirmed to be "pretty much" canonical.
On May 25, 2009, "The Hollywood Reporter" revealed Roy Lee and Doug Davison of Vertigo Entertainment would be working with Fran Rubel Kuzui and Kazi Kuzui on a relaunch of the "Buffy" series for the big screen. The series would not be a sequel or prequel to the existing movie or television franchise and Joss Whedon will have no involvement in the project. None of the cast or original characters from the television series will be featured. Television series executive producer Marti Noxon later reflected that this story might have been produced by the studio in order to frighten Joss into taking reins of the project. Studio interest in the project has continued, however. A script was rejected in 2011.
Television.
Buffy returned in Joss Whedon's television series "Buffy the Vampire Slayer", this time played by Sarah Michelle Gellar for all of the show's 144 episodes. In season one (1997), Buffy begins to accept the responsibilities and dangers of her calling as the Slayer after moving to the small California town of Sunnydale. She becomes best friends with Xander Harris (Nicholas Brendon) and Willow Rosenberg (Alyson Hannigan), and meets her new Watcher, Rupert Giles (Anthony Stewart Head). Together, they form the Scooby Gang, and work together to battle various supernatural occurrences which plague Sunnydale High. In the season finale, Buffy battles the vampiric villain known as the Master (Mark Metcalf), and is drowned in the process. She is resuscitated by Xander and rises to defeat the vampire lord. In the show's second season (1997–1998), Buffy continues to come to terms with her destiny, finds forbidden love with benevolent vampire Angel (David Boreanaz), and clashes with new villains Spike (James Marsters) and Drusilla (Juliet Landau). In the episode "Surprise", Buffy loses her virginity to Angel, an event which triggers the loss of his soul and unleashes his sadistic alter-ego, Angelus. Angelus proceeds to subject the characters to mental and physical torture for the remainder of the season. In the final episode of season two, Buffy is forced to reveal her identity as the Slayer to her mother (Kristine Sutherland), and send the newly good Angel to hell in order to save the world. She then leaves Sunnydale for Los Angeles in the hopes of escaping her life as the Slayer. Season three (1998–1999) sees Buffy reconnect to her calling, her friends, and her family after her departure, as well as make difficult life decisions regarding her relationship with the resurrected Angel. She must also deal with the introduction of rebellious new Slayer Faith (Eliza Dushku), who becomes increasingly destructive and disloyal over the course of the season. In the season finale, Buffy stabs Faith in an attempt to save Angel's life, and leads her classmates into a climactic battle against the demonic Mayor of Sunnydale (Harry Groener). Angel then leaves Sunnydale in hopes that Buffy can have a more normal life without him.
In the fourth season (1999–2000), Buffy balances her Slayer duties with her new life as a college student at UC Sunnydale. She experiences some difficulty adjusting to college life, and becomes increasingly disconnected from her friends, who all seem to be moving in different directions. Buffy eventually finds a new love interest in the form of Riley Finn (Marc Blucas), a soldier in the demon-hunting government task force known as The Initiative. She briefly joins forces with Riley's team, until they discover one of the Initiative's experiments, Adam (George Hertzberg), is creating an army of demon-human hybrids. Buffy unites with her friends to defeat Adam in a spell which invokes the power of the First Slayer. During "Buffy" season four, Buffy also appears in the first season of spin-off series "Angel" (1999–2000), guest starring in the episodes "I Will Remember You" and "Sanctuary". In season five (2000–2001), Buffy battles the hell-goddess Glory (Clare Kramer), and fully embraces her destiny for the first time. A younger sister named Dawn (Michelle Trachtenberg) mysteriously appears in Buffy's household, her existence having been seamlessly integrated with memories of the other characters. Buffy suffers emotional turmoil throughout this season, including the realization Dawn is not actually her sister, the deterioration of her relationship with Riley, the discovery that Spike has become obsessed with her, and her mother's death from a brain aneurysm. While on a quest to learn more about her nature as the Slayer, Buffy is told "death is her gift, a message she has difficulty understanding until the episode "The Gift", in which she sacrifices herself to save Dawn and the world by diving into Glory's interdimensional portal and closing it.
Season six (2001–2002) depicts Buffy's struggle with depression after her friends, believing she was trapped in a Hell dimension, performed a spell to bring her back from the dead; however, she was actually in Heaven, and feels great loss after being ripped out. Forced to take a mundane and degrading job slinging burgers at the Doublemeat Palace after realizing her family were in financial ruin, she sinks into a deep depression amid feelings of self-loathing and spends much of the season on a downward spiral alienating her friends and family and embarking on a violent sexual relationship with the vampire Spike which leaves neither satisfied and spawns dire consequences for the both of them. As the season draws to a close, Buffy is forced to battle her best friend when Willow becomes psychotic with dark magics after the human, Warren (Adam Busch) shoots and kills Willow's girlfriend Tara (Amber Benson) and wounds Buffy in the process. Willow then tries to destroy the world to end all suffering, although Xander gets through to her in the end. Buffy then promises to change her self-destructive behavior in order to be there for her sister. In the final season of the show (2002–2003), things start to come around for Buffy when Principal Robin Wood (D. B. Woodside) hires her as a school counselor for the newly rebuilt Sunnydale High School and she has repaired her relationships with Dawn and her friends. However, she is also confronted with the threat of the First Evil and becomes a reluctant leader to the Potential Slayers, who are initially respectful of her, but become increasingly more alienated by her tactics and decisions throughout the season. She unexpectedly becomes emotionally close with Spike, who has sought out his soul in an effort to prove himself to her. In the show's final episode "Chosen", Buffy shares her power with her fellow Slayers before leading them into an epic battle against an army of Turok-Han vampires. She also confesses her love to a disbelieving Spike before he sacrifices himself to save the world; as he dies, Buffy escapes Sunnydale's destruction with the surviving characters. Following the end of "Buffy the Vampire Slayer", the character maintains a presence in the fifth season of "Angel" (2003–2004), but does not appear onscreen. In the episode "The Girl in Question", Angel and a resurrected Spike travel to Rome to find her, where they learn she is apparently now dating the Immortal. Sarah Michelle Gellar was approached to appear as Buffy in "Angel"'s one hundredth episode, but declined, so the character of Cordelia Chase (Charisma Carpenter) was used instead. She was asked to appear in the second to last episode of the series, "Power Play", but had to decline due to outside conflicts.
Between 2001 and 2004, Joss Whedon and Jeph Loeb developed a 4-minute pilot episode for "Buffy the Animated Series", which was set during the show's first season. Had the series been picked up by a network, the series would have focused upon Buffy (voiced by Giselle Loren) in more high-school adventures. Following a 2008 leak of the pilot to YouTube, Loeb expressed some hope the series may be resurrected in some form.
Literature.
As the main character of the franchise, Buffy appears in almost all "Buffy the Vampire Slayer" literature. This includes a Dark Horse ongoing comic book and a series of novels. Buffy's debut into literature came in the comic "Dark Horse Presents 1998 Annual" on August 26, 1998, while her first prose appearance was in "Halloween Rain" by Christopher Golden and Nancy Holder on October 5, 1998. Most of these stories occur between episodes and seasons of the television series, however, some are set outside the timeline of the show to explore in depth other areas of Buffy's history. Christopher Golden adapted the film into a comic entitled "The Origin" (1999) which more closely resembles Joss Whedon's original script. In 2003, Scott Lobdell and Fabian Nicieza wrote a Year One-style run on the "Buffy" comic book series which filled the gap between the film and the first season of the show. These stories explain how Buffy's relationship with Pike ended, as well as fleshing out events alluded to in the television series, such as the time she spent in a mental institution and her parents' divorce. The novel "Queen of the Slayers" (2005) by Nancy Holder offers a potential follow-up to the television series; set after season seven, it depicts Buffy living in Italy with the morally ambiguous Immortal.
Buffy also makes appearances in literature outside of her own titular series. In the "Tales of the Slayers" comic one-shot "Broken Bottle of Djinn" (2002) by Doug Petrie and Jane Espenson, Buffy battles a spirit in Sunnydale High, while the "Tales of the Vampires" comic book story "Antique" (2004) by Drew Goddard sees her breaking into Dracula's castle to rescue Xander from the infamous vampire. Volume II of the similar series of novels "Tales of the Slayer" (2003) features two stories about Buffy; the character battles a mummified spirit in Todd A. McIntosh's "All That You Do Comes Back Unto Thee," while Jane Espenson's "Again Sunnydale" sees a season six-era Buffy sent back in time to high school, when her mother is still alive but Dawn does not exist.
In 2007, Buffy's story was continued when Joss Whedon resurrected "Buffy the Vampire Slayer" as a comic book. These comics differ from previous "Buffy" literature in that they are the official continuation of the television series and are considered canon. In "Season Eight" (2007–11), it establishes Buffy is not living with the Immortal in Rome which is simply a cover story to ensure her safety as she is now the leader of a global organization which recruits and trains Slayers to deal with demonic threats worldwide. However, a mysterious group led by the masked villain 'Twilight' believe the Slayers themselves are the danger, should they begin to consider themselves superior to mankind. In the story "Wolves at the Gate", Buffy shares a sexual encounter with fellow Slayer Satsu; however Satsu leaves soon after because she realizes Buffy cannot return her feelings. Earlier in the series, the audience glimpsed Buffy's most personal sexual fantasy in her dreamspace: a threesome with Angel and Spike. The series' final two arcs are in some sense are about Buffy's relationships with Angel and Spike. In "Twilight", Buffy mysteriously obtains super powers and discovers that Twilight is in fact Angel. Despite his actions, Buffy succumbs to her passions and engages in an act of passionate airborne sex with Angel, only having them return on waking up in a paradise dimension which will replace the existing universe. In spite of the lure of eternal happiness together in this new dimension, the two return to Earth to assist their friends against extra-dimensional demons, when Spike arrives. In "Last Gleaming", Spike's information leads them to source both of magic and of Twilight's power, a mystical "seed" buried beneath Sunnydale. Giles plans to destroy it, but Twilight possesses Angel and snaps his neck. Distraught, Buffy smashes the seed herself. Twilight is stopped but magic is also removed from the universe. Though Slayers and vampires retain their powers, witches for example are left entirely powerless. A pariah in the community of Slayers and former witches, Buffy moves to San Francisco where she lives with her sister and Xander, and resumes her former duties as Slayer: patrolling at night for vampires.
In "Buffy the Vampire Slayer Season Nine" (2011–13), Buffy adjusts to the new status quo of a world without magic. As a consequence of demons no longer being able to enter their dimension and fully possess human bodies, newly sired vampires are feral and mindless creatures, which Xander dubs "zompires". Buffy faces a new antagonist in Severin, who works with the rogue slayer Simone Doffler and has the power to drain others of their mystical energy. In the story arc "Apart (of Me)", Buffy mistakenly believes that she is pregnant and makes the difficult decision to have an abortion. Spike also leaves San Francisco after it becomes difficult for him to be around Buffy anymore. Willow also leaves town on a mission to restore her abilities, leaving Buffy to patrol for vampires with new friends, including SFPD officer Robert Dowling and teenager Billy Lane. As the season goes on, Buffy teams with a council of powerful supernatural beings to take on Severin. However, Severin is able to steal the powers of Illyria, a member of that group, including her time travel abilities. When Buffy learns Dawn is dying as a result of the end of magic and Willow returns with restored powers, they venture to the Deeper Well, a tomb for ancient demons in England, in the hopes of finding magic to save Dawn, who begins fading out of existence. Desperate to save Dawn, Xander informs Simone and Severin of Buffy's plan because Severin wants the same magic to avert the events of "Season Eight" using time travel. Within the Well, Buffy fights and kills Simone who had intentionally became a vampire; Severin and Illyria, in an act of redemption, sacrifice their lives to restore magic to the universe. After this, Willow is able to bring Dawn back from nonexistence. Buffy and Willow begin to realize however that the rules of magic and vampire abilities have been entirely rewritten by their actions. In "Season Ten" (2014–present), Buffy and her friends grapple with the new rules of magic, the resulting new threats and the obligation to make new alliances.
Concept and creation.
The character of Buffy was conceived by Joss Whedon as a way of subverting the cliché of "the little blonde girl who goes into a dark alley and gets killed in every horror film". Whedon stated "Rhonda the Immortal Waitress" was the first incarnation of Buffy in his head, "the idea of a seemingly insignificant female who in fact turns out to be extraordinary." When asked how he came up with the name of "Buffy," Whedon states "It was the name that I could think of that I could take the least seriously. There is no way you could hear the name Buffy and think, "This is an important person. To juxtapose that with Vampire Slayer, just felt like that kind of thing—a B movie. But a B movie that had something more going on. That was my dream." Whedon claims the title was criticized for being too silly, and the television network begged him to change it. He refused, insisting "You don't understand. It has to be this. This is what it is." Jason Middleton feels that Buffy avoids the "final girl" character trope seen in horror films, where the androgynous and celibate heroine gets to outlive her friends and exact revenge on their killer; in Middleton's words, "she... gets to have sex with boys and "still" kill the monster".
Whedon always intended for the character to become an icon, claiming "I wanted her to be a hero that existed in people's minds the way Wonder Woman or Spider-Man does, you know? I wanted her to be a doll or an action figure. I wanted Barbie with Kung Fu grip! I wanted her to enter the mass consciousness and the imaginations of growing kids because I think she's a cool character, and that was always the plan. I wanted Buffy to be a cultural phenomenon, period." In developing Buffy, Whedon was greatly inspired by Kitty Pryde, a character from the pages of the superhero comic "X-Men". He admits, "If there's a bigger influence on Buffy than Kitty, I don't know what it was... She was an adolescent girl finding out she has great power and dealing with it." In a 2009 interview, Whedon revealed he only recently realised how much he saw of himself in Buffy. After years of relating more to Xander, he says, "Buffy was always the person that I was in that story because I'm not in every way." Whedon openly wonders why his identification figure is a woman, but describes it as "a real autobiographical kind of therapy for me" to be writing a strong female character like Buffy.
According to Whedon, Buffy "had been brewing in for many years" before finally appearing in the "Buffy the Vampire Slayer" film played by Kristy Swanson. However, he was not satisfied with the character's treatment in the film, feeling "that's not quite her. It's a start, but it's not quite the girl." Although Whedon's vision of female empowerment was not as apparent as he would have liked in the 1992 film, he was given a second chance when Gail Berman approached him with the idea of re-creating it as a television series. Adapting the concept of the movie into a television series, Whedon decided to reinvent the character of Buffy slightly. The shallow cheerleader of the original film had grown more mature and open-minded, identifying with social outcasts such as Willow and Xander, and instead, the character of Cordelia was created to embody what Buffy once was. Early in the television series, make-up supervisor Todd McIntosh was instructed to make Buffy "a soft and sort of earthy character." He gave Gellar a soft, muted green make-up and kept her look very natural. However, it was later decided this was inappropriate for the character, and Buffy needed to look more like a valley girl. McIntosh switched her make-up around, giving her frosted eyeshadow and lip colors, bright turquoise and aqua marines, bubblegum colored nails, and bleach-blonde hair, causing the character to "blossom."

</doc>
<doc id="40650" url="https://en.wikipedia.org/wiki?curid=40650" title="Scale (anatomy)">
Scale (anatomy)

In most biological nomenclature, a scale (Greek λεπίς "lepis", Latin "squama") is a small rigid plate that grows out of an animal's skin to provide protection. In lepidopteran (butterfly and moth) species, scales are plates on the surface of the insect wing, and provide coloration. Scales are quite common and have evolved multiple times through convergent evolution, with varying structure and function.
Scales are generally classified as part of an organism's integumentary system. There are various types of scales according to shape and to class of animal.
Fish scales.
Fish scales are dermally derived, specifically in the mesoderm. This fact distinguishes them from reptile scales paleontologically.
Genetically, the same genes involved in tooth and hair development in mammals are also involved in scale development.
Cosmoid scales.
True cosmoid scales can only be found on the extinct Crossopterygians. The inner layer of the scale is made of lamellar bone. On top of this lies a layer of spongy or vascular bone and then a layer of dentine-like material called cosmine. The upper surface is keratin. The coelacanth has modified cosmoid scales that lack cosmine and are thinner than true cosmoid scales.
Ganoid scales.
Ganoid scales can be found on gars (family Lepisosteidae), bichirs, and reedfishes (family Polypteridae). Ganoid scales are similar to cosmoid scales, but a layer of ganoin lies over the cosmine layer and under the enamel. Ganoin scales are diamond-shaped, shiny, and hard. 
Within the ganoin are guanine compounds, iridescent derivatives of guanine found in a DNA molecule. The iridescent property of these chemicals provide the ganoin its shine.
Placoid scales.
Placoid scales are found on cartilaginous fish including sharks. These scales, also called denticles, are similar in structure to teeth, and have one median spine and two lateral spines.
Leptoid scales.
Leptoid scales are found on higher-order bony fish. As they grow they add concentric layers. They are arranged so as to overlap in a head-to-tail direction, like roof tiles, allowing a smoother flow of water over the body and therefore reducing drag. They come in two forms:
Reptilian scales.
Reptile scale types include: cycloid, granular (which appear bumpy), and keeled (which have a center ridge). Scales usually vary in size, the stouter, larger scales cover parts that are often exposed to physical stress (usually the feet, tail and head), while scales are small around the joints for flexibility. Most snakes have extra broad scales on the belly, each scale covering the belly from side to side.
The scales of all reptiles have an epidermal component (what one sees on the surface), but many reptiles, such as crocodylians and turtles, have osteoderms underlying the epidermal scale. Such scales are more properly termed scutes. Snakes, tuataras and many lizards lack osteoderms. All reptilian scales have a dermal papilla underlying the epidermal part, and it is there that the osteoderms, if present, would be formed.
Avian scales.
Birds' scales are found mainly on the toes and metatarsus, but may be found further up on the ankle in some birds. The scales and scutes of birds were thought to be homologous to those of reptiles, but are now agreed to have evolved independently, being degenerate feathers.
Mammalian scales.
An example of a scaled mammal is the pangolin. Its scales are made of keratin and are used for protection, similar to an armadillo's armor. They have been convergently evolved, being unrelated to the animal's distant reptile ancestors, except that they use a similar gene.
On the other hand, the musky rat-kangaroo, a marsupial with a number of especially primitive traits, has scales on its feet and tail[http://rainforest-australia.com/rkangaroo.htm]</ref> that appear to be linked to ancestral reptile scales. It also has five toes (unlike any other kangaroo or rat-kangaroo), and engages in a more primitive-seeming hopping behavior than its kangaroo cousins.
Anomalures also have scales on their tail undersides.
Arthropod scales.
Butterflies and moths - the order Lepidoptera (Greek "scale-winged") - have membranous wings covered in delicate, powdery scales, which are modified setae. Each scale consists of a series of tiny stacked platelets of organic material, and butterflies tend to have the scales broad and flattened, while moths tend to have the scales narrower and more hair-like. Scales are usually pigmented, but some types of scales are metallic, or iridescent, without pigments; because the thickness of the platelets is on the same order as the wavelength of visible light the plates lead to structural coloration and iridescence through the physical phenomenon described as thin-film optics. The most common color produced in this fashion is blue, such as in the "Morpho" butterflies. Other colors can be seen on the Sunset moth.

</doc>
<doc id="40651" url="https://en.wikipedia.org/wiki?curid=40651" title="Scale (music)">
Scale (music)

In music theory, a scale is any set of musical notes ordered by fundamental frequency or pitch. A scale ordered by increasing pitch is an ascending scale, and a scale ordered by decreasing pitch is a descending scale. Some scales contain different pitches when ascending than when descending. For example, the Melodic minor scale.
Often, especially in the context of the common practice period, most or all of the melody and harmony of a musical work is built using the notes of a single scale, which can be conveniently represented on a staff with a standard key signature.
Due to the principle of octave equivalence, scales are generally considered to span a single octave, with higher or lower octaves simply repeating the pattern. A musical scale represents a division of the octave space into a certain number of scale steps, a scale step being the recognizable distance (or interval) between two successive notes of the scale. However, there is no need for scale steps to be equal within any scale and, particularly as demonstrated by microtonal music, there is no limit to how many notes can be injected within any given musical interval. 
A measure of the width of each scale step provides a method to classify scales. For instance, in a chromatic scale each scale step represents a semitone interval, while a major scale is defined by the interval pattern T–T–S–T–T–T–S, where T stands for whole tone (an interval spanning two semitones), and S stands for semitone. Based on their interval patterns, scales are put into categories including diatonic, chromatic, major, minor, and others.
A specific scale is defined by its characteristic interval pattern and by a special note, known as its first degree (or tonic). The tonic of a scale is the note selected as the beginning of the octave, and therefore as the beginning of the adopted interval pattern. Typically, the name of the scale specifies both its tonic and its interval pattern. For example, C major indicates a major scale with a C tonic.
Background.
Scales, steps, and intervals.
Scales are typically listed from low to high. Most scales are "octave-repeating", meaning their pattern of notes is the same in every octave (the Bohlen–Pierce scale is one exception). An octave-repeating scale can be represented as a circular arrangement of pitch classes, ordered by increasing (or decreasing) pitch class. For instance, the increasing C major scale is C–D–E–F–G–A–B–with the bracket indicating that the last note is an octave higher than the first note, and the decreasing C major scale is C–B–A–G–F–E–D–[C, with the bracket indicating an octave lower than the first note in the scale.
The distance between two successive notes in a scale is called a scale step.
The notes of a scale are numbered by their steps from the root of the scale. For example, in a C major scale the first note is C, the second D, the third E and so on. Two notes can also be numbered in relation to each other: C and E create an interval of a third (in this case a major third); D and F also create a third (in this case a minor third).
Scales and pitch.
A single scale can be manifested at many different pitch levels. For example, a C major scale can be started at C4 (middle C; see scientific pitch notation) and ascending an octave to C5; or it could be started at C6, ascending an octave to C7. As long as all the notes can be played, the octave they take on can be altered.
Types of scale.
Scales may be described according to the intervals they contain:
or by the number of different pitch classes they contain:
"The number of the notes that make up a scale as well as the quality of the intervals between successive notes of the scale help to give the music of a culture area its peculiar sound quality." "The pitch distances or intervals among the notes of a scale tell us more about the sound of the music than does the mere number of tones."
Harmonic content.
The notes of a scale form intervals with each of the other notes of the chord in combination. A 5-note scale has 10 of these harmonic intervals, a 6-note scale has 15, a 7-note scale has 21, an 8-note scale has 28. Though the scale is not a chord, and might never be heard more than one note at a time, still the absence, presence, and placement of certain key intervals plays a large part in the sound of the scale, the natural movement of melody within the scale, and the selection of chords taken naturally from the scale.
A musical scale that contains tritones is called tritonic (though the expression is also used for any scale with just three notes per octave, whether or not it includes a tritone), and one without tritones is "atritonic". A scale or chord that contains semitones is called hemitonic, and without semitones is anhemitonic. The significance of these categories lies in their bases of semitones and tritones being the severest of dissonances, which is often desirable to avoid. Most scales used across the planet are anhemitonic.
Scales in composition.
Scales can be abstracted from performance or composition. They are also often used precompositionally to guide or limit a composition. Explicit instruction in scales has been part of compositional training for many centuries. One or more scales may be used in a composition, such as in Claude Debussy's "L'Isle Joyeuse". To the right, the first scale is a whole tone scale, while the second and third scales are diatonic scales. All three are used in the opening pages of Debussy's piece.
Western music.
Scales in traditional Western music generally consist of seven notes and repeat at the octave. Notes in the commonly used scales (see just below) are separated by whole and half step intervals of "tones" and "semitones." The harmonic minor scale includes a three-semitone step; the anhemitonic pentatonic includes two of those and no semitones.
Western music in the Medieval and Renaissance periods (1100–1600) tends to use the white-note diatonic scale C–D–E–F–G–A–B. Accidentals are rare, and somewhat unsystematically used, often to avoid the tritone.
Music of the common practice periods (1600–1900) uses three types of scale:
These scales are used in all of their transpositions. The music of this period introduces "modulation," which involves systematic changes from one scale to another. Modulation occurs in relatively conventionalized ways. For example, major-mode pieces typically begin in a "tonic" diatonic scale and modulate to the "dominant" scale a fifth above.
In the 19th century (to a certain extent), but more in the 20th century, additional types of scales were explored:
A large variety of other scales exists, some of the more common being:
Scales such as the pentatonic scale may be considered "gapped" relative to the diatonic scale. An "auxiliary scale" is a scale other than the primary or original scale. See: modulation (music) and Auxiliary diminished scale.
Naming the notes of a scale.
In many musical circumstances, a specific note of the scale is chosen as the tonic—the central and most stable note of the scale, also known as the root note. Relative to a choice of tonic, the notes of a scale are often labeled with numbers recording how many scale steps above the tonic they are. For example, the notes of the C major scale (C, D, E, F, G, A, B) can be labeled {1, 2, 3, 4, 5, 6, 7}, reflecting the choice of C as tonic. The expression scale degree refers to these numerical labels. Such labeling requires the choice of a "first" note; hence scale-degree labels are not intrinsic to the scale itself, but rather to its modes. For example, if we choose A as tonic, then we can label the notes of the C major scale using A = 1, B = 2, C = 3, and so on. When we do so, we create a new scale called the A minor scale. See the musical note article for how the notes are customarily named in different countries.
The scale degrees of a heptatonic (7-note) scale can also be named using the terms tonic, supertonic, mediant, subdominant, dominant, submediant, subtonic. If the subtonic is a semitone away from the tonic, then it is usually called the leading-tone (or leading-note); otherwise the leading-tone refers to the raised subtonic. Also commonly used is the (movable do) solfège naming convention in which each scale degree is denoted by a syllable. In the major scale, the solfege syllables are: Do, Re, Mi, Fa, So (or Sol), La, Ti (or Si), Do (or Ut).
In naming the notes of a scale, it is customary that each scale degree be assigned its own letter name: for example, the A major scale is written A–B–C–D–E–F–G rather than A–B–D–D–E–E–G. However, it is impossible to do this in scales that contain more than seven notes.
Scales may also be identified by using a binary system of twelve zeros or ones to represent each of the twelve notes of a chromatic scale. It is assumed that the scale is tuned using 12-tone equal temperament (so that, for instance, C is the same as D), and that the tonic is in the leftmost position. For example the binary number 101011010101, equivalent to the decimal number 2773, would represent any major scale (such as C–D–E–F–G–A–B). This system includes scales from 100000000000 (2048) to 111111111111 (4095), providing a total of 2048 possible species, but only 352 unique scales containing from 1 to 12 notes.
Scales may also be shown as semitones (or fret positions) from the tonic. For instance, 0 2 4 5 7 9 11 denotes any major scale such as C–D–E–F–G–A–B, in which the first degree is, obviously, 0 semitones from the tonic (and therefore coincides with it), the second is 2 semitones from the tonic, the third is 4 semitones from the tonic, and so on. Again, this implies that the notes are drawn from a chromatic scale tuned with 12-tone equal temperament.
Scalar transposition.
Composers often transform musical patterns by moving every note in the pattern by a constant number of scale steps: thus, in the C major scale, the pattern C–D–E might be shifted up, or transposed, a single scale step to become D–E–F. This process is called "scalar transposition" and can often be found in musical sequences. Since the steps of a scale can have various sizes, this process introduces subtle melodic and harmonic variation into the music. This variation is what gives scalar music much of its complexity.
Jazz and blues.
Through the introduction of blue notes, jazz and blues employ scale intervals smaller than a semitone. The blue note is an interval that is technically neither major nor minor but "in the middle", giving it a characteristic flavour. For instance, in the key of E, the blue note would be either a note between G and G or a note moving between both. In blues a pentatonic scale is often used. In jazz many different modes and scales are used, often within the same piece of music. Chromatic scales are common, especially in modern jazz.
Non-Western scales.
In Western music, scale notes are often separated by equally tempered tones or semitones, creating 12 notes per octave. Many other musical traditions use scales that include other intervals or a different number of pitches. These scales originate within the derivation of the harmonic series. Musical intervals are complementary values of the harmonic overtones series. Many musical scales in the world are based on this system, except most of the musical scales from Indonesia and the Indochina Peninsulae, which are based on inharmonic resonance of the dominant metalophone and xylophone instruments. A common scale in Eastern music is the pentatonic scale, consisting of five notes. The Middle Eastern Hejaz scale has some intervals of three semitones. Gamelan music uses a small variety of scales including Pélog and Sléndro, none including equally tempered nor harmonic intervals. Indian classical music uses a moveable seven-note scale. Indian Rāgas often use intervals smaller than a semitone. Arabic music maqamat may use quarter tone intervals. In both rāgas and maqamat, the distance between a note and an inflection (e.g., śruti) of that same note may be less than a semitone.
Microtonal scales.
The term "microtonal music" usually refers to music with roots in traditional Western music that uses non-standard scales or scale intervals. In the late 19th century, Mexican composer Julián Carrillo created microtonal scales that he called "Sonido 13". The composer Harry Partch made custom musical instruments to play compositions based on 43-note scale system, and the American jazz vibraphonist Emil Richards experimented with such scales in his Microtonal Blues Band in the 1970s. Easley Blackwood wrote compositions in all equal-tempered scales from 13 to 24 notes. Erv Wilson introduced concepts such as Combination Product Sets (Hexany), Moments of Symmetry and golden horagrams, used by many modern composers. Microtonal scales are also used in traditional Indian Raga music, which uses a variety of modes not only as modes or scales, but also as defining elements of the song, or raga.

</doc>
<doc id="40655" url="https://en.wikipedia.org/wiki?curid=40655" title="Leigh Brackett">
Leigh Brackett

Leigh Douglass Brackett (December 7, 1915 – March 18, 1978) was an American writer, particularly of science fiction, and has been referred to as the "Queen of Space Opera". She was also a screenwriter, known for her work on such films as "The Big Sleep" (1945), "Rio Bravo" (1959), "The Long Goodbye" (1973) and "The Empire Strikes Back" (1980).
Life.
Leigh Brackett was born December 7, 1915 in Los Angeles, California and grew up there. On December 31, 1946, at age 31, she married Edmond Hamilton in San Gabriel, California, and moved with him to Kinsman, Ohio. She died of cancer in 1978 in Lancaster, California.
Career.
Fiction writer.
Brackett was first published in her mid-twenties. Her first published science fiction story was "Martian Quest", which appeared in the February 1940 issue of "Astounding Science Fiction". Her earliest years as a writer (1940–42) were her most productive in numbers of stories written. Occasional stories have social themes, such as "The Citadel of Lost Ships" (1943), which considers the effects on the native cultures of alien worlds of Earth's expanding trade empire.
Brackett's first novel, "No Good from a Corpse", published in 1944, was a hard-boiled mystery novel in the tradition of Raymond Chandler. This led to her first major screenwriting assignment. At the same time, though, Brackett's science fiction stories were becoming more ambitious. "Shadow Over Mars" (1944) was her first novel-length science fiction story, and though still somewhat rough-edged, marked the beginning of a new style, strongly influenced by the characterization of the 1940s detective story and film noir.
In 1946, the same year that Brackett married science fiction author Edmond Hamilton, "Planet Stories" published the novella "Lorelei of the Red Mist". Brackett only finished the first half before turning it over to Ray Bradbury, so that she could leave to work on "The Big Sleep". The story's main character is a thief called Hugh Starke.
Brackett returned from her break from science-fiction writing, caused by her cinematic endeavors, in 1948. From then on to 1951, she produced a series of science fiction adventure stories that were longer than her previous work. To this period belong such classic representations of her planetary settings as "The Moon that Vanished" and the novel-length "Sea-Kings of Mars" (1949), later published as "The Sword of Rhiannon", a vivid description of Mars before its oceans evaporated.
With "Queen of the Martian Catacombs" (1949), Brackett created a character that she later returned to, Eric John Stark. Stark, an orphan from Earth, is raised by the semi-sentient aboriginals of Mercury, who are later killed by Earthmen. He is saved from the same fate by a Terran official, who adopts Stark and becomes his mentor. When threatened, however, Stark frequently reverts to the primitive N'Chaka, the "man without a tribe" that he was on Mercury. From 1949 to 1951, Stark (whose name echoes that of the hero in "Lorelei") appeared in three tales, all published in "Planet Stories"; the aforementioned "Queen", "Enchantress of Venus", and finally "Black Amazon of Mars". With this last story, Brackett's period of writing high adventure ended.
Brackett's stories thereafter adopted a more elegiac tone. They no longer celebrated the conflicts of frontier worlds, but lamented the passing away of civilizations. The stories now concentrated more upon mood than on plot. The reflective, retrospective nature of these stories is indicated in the titles: "The Last Days of Shandakor"; "Shannach — the Last"; "Last Call from Sector 9G".
This last story was published in the very last issue (Summer 1955) of "Planet Stories", always Brackett's most reliable market for science fiction. With the disappearance of "Planet Stories" and, later in 1955, of "Startling Stories" and "Thrilling Wonder Stories", the market for Brackett's brand of story dried up, and the first phase of her career as a science fiction author ended. A few other stories trickled out over the next decade, and old stories were revised and published as novels. A new production of this period was one of Brackett's most critically acclaimed science fiction novels, "The Long Tomorrow" (1955). This novel describes an agrarian, deeply technophobic society that develops after a nuclear war.
But most of Brackett's writing after 1955 was for the more lucrative film and television markets. In 1963 and 1964, she briefly returned to her old Martian milieu with a pair of stories; "The Road to Sinharat" can be regarded as an affectionate farewell to the world of "Queen of the Martian Catacombs", while the other – with the intentionally ridiculous title of "Purple Priestess of the Mad Moon" – borders on parody. She and her husband shared Guest of Honor duties at the 22nd World Science Fiction Convention in Oakland, California.
After another hiatus of nearly a decade, Brackett returned to science fiction in the seventies with the publication of "The Ginger Star" (1974), "The Hounds of Skaith" (1974), and "The Reavers of Skaith" (1976), collected as "The Book of Skaith" in 1976. This trilogy brought Eric John Stark back for adventures upon the extrasolar planet of Skaith (rather than his old haunts of Mars and Venus).
"Brackett's Solar System".
Often referred to as the Queen of Space Opera, Brackett also wrote planetary romance. Almost all of her planetary romances take place within a common invented universe, the Leigh Brackett Solar System, which contains richly detailed fictional versions of the consensus Mars and Venus of science fiction in the 1930s–1950s. Mars thus appears as a marginally habitable desert world, populated by ancient, decadent, and mostly humanoid races; Venus as a primitive, wet jungle planet, occupied by vigorous, primitive tribes and reptilian monsters. Brackett's Skaith combines elements of Brackett's other worlds with fantasy elements.
Though the influence of Edgar Rice Burroughs is apparent in Brackett's Mars stories, the differences between their versions of Mars are great. Brackett's Mars is set firmly in a world of interplanetary commerce and competition, and one of the most prominent themes of Brackett's stories is the clash of planetary civilizations; the stories both illustrate and criticize the effects of colonialism on civilizations which are either older or younger than those of the colonizers, and thus they have relevance to this day. Burroughs' heroes set out to remake entire worlds according to their own codes; Brackett's heroes (often antiheroes) are at the mercy of trends and movements far bigger than they are.
Screenwriter.
Shortly after Brackett broke into science fiction writing, she also wrote her first screenplays. Hollywood director Howard Hawks was so impressed by her novel "No Good from a Corpse" that he had his secretary call in "this guy Brackett" to help William Faulkner write the script for "The Big Sleep" (1946). The film, written by Brackett, William Faulkner, and Jules Furthman, and starring Humphrey Bogart, is considered one of the best movies ever made in the genre. However, after her marriage, Brackett took a long break from screenwriting.
When she returned to screenwriting in the mid-1950s, she wrote for both TV and movies. Howard Hawks hired her to write or co-write several John Wayne pictures, including "Rio Bravo" (1959), "Hatari!" (1962), "El Dorado" (1966) and "Rio Lobo" (1970). Because of her background with "The Big Sleep", Robert Altman hired her to adapt Raymond Chandler's novel "The Long Goodbye" for the screen.
"The Empire Strikes Back".
Brackett worked on the screenplay for the first "Star Wars" sequel "The Empire Strikes Back". The film won the Hugo Award in 1981. This script was a departure for Brackett, since until then, all of her science fiction had been in the form of novels and short stories.
The exact role which Brackett played in writing the script for "Empire" is the subject of some dispute. What is agreed on by all is that George Lucas asked Brackett to write the screenplay based on his story outline. It is also known that Brackett wrote a finished first draft which was delivered to Lucas shortly before Brackett's death from cancer on March 18, 1978. Two drafts of a new screenplay were written by Lucas and, following the delivery of the screenplay for "Raiders of the Lost Ark", turned over to Lawrence Kasdan for a new approach. Both Brackett and Kasdan (though not Lucas) were given credit for the final script.
Some fans were reported to believe that they could detect traces of Brackett's influence in both the dialogue and the treatment of the space opera genre in "Empire". However, Laurent Bouzereau, in his book "Star Wars: The Annotated Screenplays", states that Lucas disliked the direction of Brackett's screenplay and discarded it. He then produced two screenplays before turning the results over to Kasdan. 
Brackett's screenplay has never been officially or legally published. According to Stephen Haffner, it can be read at one of two locations: the Jack Williamson Special Collections library at Eastern New Mexico University in Portales, New Mexico (but may not be copied or checked out); and the archives at Lucasfilm, Ltd. in California.

</doc>
<doc id="40656" url="https://en.wikipedia.org/wiki?curid=40656" title="13th century BC">
13th century BC

The 13th century BC was the period from 1300 to 1201 BC.
Significant persons.
Although many human societies were literate in this period, some individual persons mentioned in this article ought to be considered legendary rather than historical.
Sovereign States.
See: List of sovereign states in the 13th century BC.

</doc>
<doc id="40657" url="https://en.wikipedia.org/wiki?curid=40657" title="14th century BC">
14th century BC

The 14th century BC is a century which lasted from the year 1400 BC until 1301 BC.
Sovereign States.
See: List of sovereign states in the 14th century BC.

</doc>
<doc id="40658" url="https://en.wikipedia.org/wiki?curid=40658" title="15th century BC">
15th century BC

The 15th century BC is a century which lasted from 1500 BC to 1401 BC.
Sovereign States.
See: List of sovereign states in the 15th century BC.

</doc>
<doc id="40659" url="https://en.wikipedia.org/wiki?curid=40659" title="Kingsbury Commitment">
Kingsbury Commitment

The Kingsbury Commitment of 1913 was an out-of-court settlement of the government's antitrust challenge of AT&T's growing vertical monopoly over the phone industry. In return for the government's agreement not to pursue its case against AT&T as a monopolist, AT&T agreed to divest the controlling interest it had acquired in the Western Union telegraph company, and to allow non-competing independent telephone companies to interconnect with the AT&T long distance network.
The government had been increasingly worried that AT&T and the other Bell Companies were monopolizing the industry. Under Theodore N. Vail from 1907, AT&T had bought Bell-associated companies and organized them into new hierarchies. AT&T had also acquired many of the independents, and bought control of Western Union, giving it a monopolistic position in both telephone and telegraph communication. A key strategy was to refuse to connect its long distance network — technologically, by far the finest and most extensive in the land — with local independent carriers. Without the prospect of long distance services, the market position of many independents became untenable. Vail stated that there should be "one policy, one system and universal service, no collection of separate companies could give the public the service that [the Bell... system could give." 
AT&T's strategies prompted complaints and attracted the attention of the Justice Department. Faced with a government investigation for antitrust violations, AT&T entered into negotiations.
In the Kingsbury Commitment, actually a letter from AT&T Vice President Nathan Kingsbury of December 19, AT&T agreed with the Attorney General to divest itself of Western Union, to provide long distance services to independent exchanges under certain conditions and to refrain from acquisitions if the Interstate Commerce Commission objected. 
The Commitment did not settle all the differences between independents and Bell companies, but it did avert the federal takeover many had expected. AT&T was allowed to buy market-share, as long as it sold an equal number of subscribers to independents. Crucially, while the Kingsbury Commitment obliged it to connect its long distance service to independent local carriers, AT&T did not agree to interconnect its local services with other local providers. Nor did AT&T agree to any interconnection with independent long distance carriers.
Consequently, AT&T was able to consolidate its control over both the most profitable urban markets and long distance traffic. The Willis Graham Act allowed AT&T to begin acquiring more local telephone systems, with the genial oversight of the Interstate Commerce Commission. By 1924, the ICC approved AT&T’s acquisition of 223 of the 234 independent telephone companies. Between 1921 and 1934, the ICC approved 271 of the 274 purchase requests of AT&T. With the creation of the Federal Communications Commission in 1934, the government regulated the rates charged by AT&T.
The entire network was nationalized during World War I from June 1918 to July 1919. Following re-privatization, AT&T resumed its near-monopoly position. In 1956, AT&T and the Justice Department agreed on a consent decree to end an antitrust suit brought against AT&T in 1949. Under the decree, AT&T restricted its activities to those related to running the national telephone system, and special projects for the federal government. 
In 1968, FCC regulators intervened when the Bell System tried to prevent a mobile communications system, the Carterfone, from connecting to telephone lines. That decision established the principle that customers could connect any lawful device to the telephone network, even to offer a competing service. In the mid 1970s, emerging long distance competitors like MCI and Sprint faced the same tactic of denying interconnection, which regulators quashed, followed by a series of efforts by the Bell System phone companies to escalate the costs of interconnection as an indirect means of excluding competition. These battles resulted a large amount of antitrust litigation and ultimately led to the 1982 breakup of the Bell System. 
In 1982, AT&T and the Justice Department agreed on tentative terms for settlement of anti-trust suit filed against AT&T in 1974, under which AT&T divested itself of its local telephone operations, which became known as the "Baby Bells." In return, the Justice Department agreed to lift the restrictions on AT&T activities contained in the 1956 Consent Decree.

</doc>
<doc id="40664" url="https://en.wikipedia.org/wiki?curid=40664" title="USS Merrimack">
USS Merrimack

USS "Merrimack", or variant spelling USS "Merrimac", may be any one of several ships commissioned in the United States Navy and named after the Merrimack River.

</doc>
<doc id="40667" url="https://en.wikipedia.org/wiki?curid=40667" title="Accidental (music)">
Accidental (music)

In music, an accidental is a note of a pitch (or pitch class) that is not a member of the scale or mode indicated by the most recently applied key signature. In musical notation, the sharp (), flat (), and natural () symbols, among others, mark such notes—and those symbols are also called accidentals. In the measure (bar) where it appears, an accidental sign raises or lowers the immediately following note (and any repetition of it in the bar) from its normal pitch, overriding sharps or flats (or their absence) in the key signature. A note is usually raised or lowered by a semitone, although microtonal music may use "fractional" accidental signs. One occasionally sees double sharps or flats, which raise or lower the indicated note by a whole tone. Accidentals apply within the measure and octave in which they appear, unless canceled by another accidental sign, or tied into a following measure. If a note has an accidental and the note is repeated in a different octave within the same measure, the accidental does not apply to the same note of the different octave.
The modern accidental signs derive from the two forms of the lower-case letter "b" used in Gregorian chant manuscripts to signify the two pitches of B, the only note that could be altered. The "round" "b" became the flat sign, while the "square" "b" diverged into the sharp and natural signs.
Sometimes the black keys on a musical keyboard are called accidentals or "sharps", and the white keys are called "naturals".
Standard use of accidentals.
In most cases, a sharp raises the pitch of a note one semitone while a flat lowers it a semitone. A natural is used to cancel the effect of a flat or sharp. This system of accidentals operates in conjunction with the key signature, whose effect continues throughout an entire piece, unless canceled by another key signature. An accidental can also be used to cancel a previous accidental or reinstate the flats or sharps of the key signature.
Accidentals apply to subsequent notes on the same staff position for the remainder of the measure in which they occur, unless explicitly changed by another accidental, as shown at right. Notes on other staff positions. Once a barline is passed, the effect of the accidental ends, except when a note affected by an accidental is tied to the same note across a barline. Subsequent notes at the same staff position in the second or later bars are not affected by the accidental carried through with the tied note. 
Though this convention is still in use particularly in tonal music, it may be cumbersome in music that features frequent accidentals, as is often the case in non-tonal music. As a result, an alternative system of note-for-note accidentals has been adopted, with the aim of reducing the number of accidentals required to notate a bar. The system is as follows:
Because seven of the twelve notes of the chromatic equal-tempered scale are naturals (the "white notes", A; B; C; D; E; F; and G on a piano keyboard) this system can significantly reduce the number of naturals required in a notated passage.
Occasionally an accidental may change the note by more than a semitone: for example, if a G is followed in the same measure by a G, the flat sign on the latter note means it is two semitones lower than if no accidental were present. Thus, the effect of the accidental must be understood in relation to the "natural" meaning of the note's staff position. For the sake of clarity, some composers put a natural in front of the accidental. Thus, if in this example the composer wanted the note a semitone lower than G-natural, he might put first a sign to cancel the previous G, then the . However, in most contexts, an F could be used instead.
Double accidentals raise or lower the pitch of a note by two semitones,""Double Sharp" ()—raises the pitch two half steps. "Double Flat" ()—lowers the pitch two half steps."</ref> an innovation developed as early as 1615. This applies to the written note, ignoring key signature. An F with a double sharp applied raises it a whole step so it is enharmonically equivalent to a G. Usage varies on how to notate the situation in which a note with a double sharp is followed in the same measure by a note with a single sharp: some publications simply use the single accidental for the latter note, whereas others use a combination of a natural and a sharp, with the natural being understood to apply to only the second sharp.
The double accidental with respect to a specific key signature raises or lowers the notes containing a sharp or flat by a semitone. For example, when in the key of C minor or E major, F, C, G, and D contain a sharp. Adding a double accidental (double sharp) to F in this case only raisees F by one further semitone, creating G natural. Conversely, adding a double sharp to any other note not sharped or flatted in the key signature raises the note by two semitones with respect to the chromatic scale. For example, in the aforementioned key signature, any note that is not F, C, G, and D is raised by two semitones instead of one, so an A double sharp raises the note A natural to the enharmonic equivalent of B natural.
Courtesy accidentals.
In modern scores, a barline cancels an accidental (except for a tied note)—but publishers often use a "courtesy accidental" (also called a "cautionary accidental" or "reminder accidental") to remind the musician of the correct pitch if the same note occurs in the following measure. This practice varies, though a few situations require a courtesy accidental, such as
Other uses are inconsistent. Courtesy accidentals are sometimes enclosed in parentheses to emphasize their role as reminders.
Publishers of free jazz music and some atonal music sometimes eschew all courtesy accidentals.
Microtonal notation.
Composers of microtonal music have developed a number of notations for indicating the various pitches outside of standard notation. One such system for notating quarter tones, used by the Czech Alois Hába and other composers, is shown on the right.
In the 19th and early 20th centuries, when Turkish musicians switched from their traditional notation systems—which were not staff-based—to the European staff-based system, they refined the European accidental system so they could notate Turkish scales that use intervals smaller than a tempered semitone. There are several such systems, which vary as to how they divide the octave they presuppose or the graphical shape of the accidentals. The most widely used system (created by Rauf Yekta Bey) uses a system of 4 sharps (roughly +25 cents, +75 cents, +125 cents and +175 cents) and 4 flats (roughly −25 cents, −75 cents, −125 cents and −175 cents), none of which correspond to the tempered sharp and flat. They presuppose a Pythagorean division of the octave taking the Pythagorean comma (about an 8th of the tempered tone, actually closer to 24 cents, defined as the difference between 7 octaves and 12 just-intonation fifths) as the basic interval. The Turkish systems have also been adopted by some Arab musicians.
Ben Johnston created a system of notation for pieces in just intonation where the unmarked C, F, and G major chords are just major chords (4:5:6) and accidentals are used to create just tuning in other keys. Between 2000 and 2003, Wolfgang von Schweinitz and Marc Sabat developed the Extended Helmholtz-Ellis JI Pitch Notation, a modern adaptation and extension of the notation principles first used by Hermann von Helmholtz, Arthur von Oettingen and Alexander John Ellis which is rapidly becoming adopted by musicians working in extended just intonation.
History of notation of accidentals.
The three principal symbols indicating whether a note should be raised or lowered in pitch are derived from variations of the small letter "b": the sharp () and natural () signs from the square "b quadratum", and the flat sign () from the round "b rotundum" "b".
In the early Middle Ages, a widespread musical tradition was based on the hexachord system defined by Guido of Arezzo. The basic system, called "musica recta", had three overlapping hexachords. Change from one hexachord to another was possible, called a "mutation". A major problem with the system was that mutation from one hexachord to another could introduce intervals like the tritone which were considered undesirable, and then dissonance could arise. To avoid the dissonance, a practice called musica ficta arose from the late 12th century onward. This introduced modifications of the hexachord, so that "false" or "feigned" notes could be sung, partly to avoid dissonance. At first only B could be flattened, moving from the "hexachordum durum" (the "hard hexachord") G–A–B–C–D–E where B is natural, to the "hexachordum molle" (the "soft hexachord") F–G–A–B–C–D where it is flat. The note B is not present in the third hexachord "hexachordum naturale" (the "natural hexachord") C–D–E–F–G–A.
The different kinds of B were eventually written differently, so as to distinguish them in music theory treatises and in notation. The flat sign derives from a round "b" that signified the soft hexachord, "hexachordum molle", particularly the presence of B. The name of the flat sign in French is "bémol" from medieval French "bé mol" which in modern French is "bé mou" "soft b". The natural sign and the sharp sign derive from variations of a square "b" that signified the hard hexachord, "hexachordum durum", where the note in question is B. The name of the natural sign in French is "bécarre" from medieval French "bé quarre" which in modern French is "bé carré" "square b". In German music notation the letter "B" or "b" always designates B while the letter "H" or "h" – a deformation of a square "b" – designates B.
As polyphony became more complex, notes other than B required alteration to avoid undesirable harmonic or melodic intervals (especially the augmented fourth, or tritone, that music theory writers referred to as "diabolus in musica", i.e., "the devil in music"). The first sharp in use was F, then came the second flat E, then C, G, etc.; by the 16th century B, E, A, D, G and F, C, G, D and A were all in use to a greater or lesser extent.
However, those accidentals were often not notated in vocal part-books (but the correct pitches were always notated in tablature). The notational practice of not marking implied accidentals—leaving them to the performer to supply instead—was called musica ficta (i.e., "feigned music").
Strictly speaking the medieval signs and indicated that the melody is progressing inside a (fictive) "hexachord" of which the signed note is the "mi" or the "fa" respectively. That means they refer to a group of notes "around" the marked note, rather than indicating that the note itself is necessarily an accidental. For example, when a semitone relationship is indicated between F and G, either by placing a mi-sign () on F or a fa-sign () on G, only the context can determine whether this means, in modern terms, F-G or F-G, or even F–G. The use of either the mi-sign on F or the fa-sign on G means only that "some kind of F goes to some kind of G, proceeding by a semitone".
The convention of an accidental remaining in force through a measure developed only gradually over the 18th century. Before then, accidentals only applied to immediately repeated notes or short groups when the composer felt it was obvious that the accidental should continue. The older practice continued in use well into the 18th century by many composers, notably Johann Sebastian Bach. The newer convention did not achieve general currency until early in the 19th century.

</doc>
<doc id="40671" url="https://en.wikipedia.org/wiki?curid=40671" title="List of sports history organisations">
List of sports history organisations

This is a list of sports history organizations

</doc>
<doc id="40673" url="https://en.wikipedia.org/wiki?curid=40673" title="American goldfinch">
American goldfinch

The American goldfinch ("Spinus tristis"), also known as the eastern goldfinch, is a small North American bird in the finch family. It is migratory, ranging from mid-Alberta to North Carolina during the breeding season, and from just south of the Canadian border to Mexico during the winter.
The only finch in its subfamily to undergo a complete molt, the American goldfinch displays sexual dimorphism in its coloration; the male is a vibrant yellow in the summer and an olive color during the winter, while the female is a dull yellow-brown shade which brightens only slightly during the summer. The male displays brightly colored plumage during the breeding season to attract a mate.
The American goldfinch is a granivore and adapted for the consumption of seedheads, with a conical beak to remove the seeds and agile feet to grip the stems of seedheads while feeding. It is a social bird, and will gather in large flocks while feeding and migrating. It may behave territorially during nest construction, but this aggression is short-lived. Its breeding season is tied to the peak of food supply, beginning in late July, which is relatively late in the year for a finch. This species is generally monogamous, and produces one brood each year.
Human activity has generally benefited the American goldfinch. It is often found in residential areas, attracted to bird feeders which increase its survival rate in these areas. Deforestation also creates open meadow areas which are its preferred habitat.
Taxonomy.
The American goldfinch was one of the many species originally described by Linnaeus in the landmark 1758 10th edition of his work "Systema Naturae". It was initially included in the genus "Spinus", a group containing New World goldfinches and siskins, but in 1976, "Spinus" was merged into the genus "Carduelis" as a subgenus. Recent studies resurrect the genus "Spinus". Its closest relatives are the lesser goldfinch "(S. psaltria)", Lawrence's goldfinch "(S. lawrencei)", and the siskins. Although it shares a name with the European goldfinch, the two are in separate subgenera and are not directly related. "Carduelis" is derived from "carduus", the Latin word for thistle; the species name "tristis" is Latin for 'sorrowful'. There are four recognized subspecies of the American goldfinch:
This seems to be the most ancient extant species of the Meso American Spinus/Carduelis evolutive radiation, whose parental species is Carduelis/Spinus lawrencei.
Description.
The American goldfinch is a small finch, long, with a wingspan of . It weighs between . Among standard measurements, the wing chord is , the tail is , the culmen is and the tarsus is . The beak is small, conical, and pink for most of the year, but turns bright orange with the spring molt in both sexes.
The shape and size of the beak aid in the extraction of seeds from the seed heads of thistles, sunflowers, and other plants.
The American goldfinch undergoes a molt in the spring and autumn. It is the only cardueline finch to undergo a molt twice a year. During the winter molt it sheds all its feathers; in the spring, it sheds all but the wing and tail feathers, which are dark brown in the female and black in the male. The markings on these feathers remain through each molt, with bars on the wings and white under and at the edges of the short, notched tail. The sexual dimorphism displayed in plumage coloration is especially pronounced after the spring molt, when the bright color of the male's summer plumage is needed to attract a mate.
Once the spring molt is complete, the body of the male is a brilliant lemon yellow, a color produced by carotenoid pigments from plant materials in its diet, with a striking jet black cap and white rump that is visible during flight. The female is mostly brown, lighter on the underside with a yellow bib. After the autumn molt, the bright summer feathers are replaced by duller plumage, becoming buff below and olive-brown above, with a pale yellow face and bib. The autumn plumage is almost identical in both sexes, but the male has yellow shoulder patches. In some winter ranges, the goldfinches lose all traces of yellow, becoming a predominantly medium tan-gray color with an olive tinge evident only on close viewing.
The immature American goldfinch has a dull brown back, and the underside is pale yellow. The shoulders and tail are dull black with buff-colored, rather than white, markings on wings and rump. This coloration is the same in both genders.
The song of the American goldfinch is a series of musical warbles and twitters, often with a long note. A "tsee-tsi-tsi-tsit" call is often given in flight; it may also be described as "per-chic-o-ree". While the female incubates the eggs, she calls to her returning mate with a soft continuous "teeteeteeteete" sound. The young begin to use a call of "chick-kee" or "chick-wee" shortly before fledging, which they use until they have left the nest entirely. There are two defense calls made by adults during nesting; a "sweeet" call made to rally other goldfinches to the nest and distract predators, and a "bearbee" used to signal to the nestlings to quiet them and get them to crouch down in the nest to become less conspicuous.
Distribution and habitat.
The American goldfinch prefers open country where weeds thrive, such as fields, meadows, flood plains, as well as roadsides, orchards, and gardens. It may also be found in open deciduous and riparian woodlands and areas of secondary growth. This habitat preference continues during the spring and autumn migrations.
The summer breeding range stretches across North America from coast to coast. It is bounded on the north by Saskatchewan and stretches south across North America to North Carolina on the east coast, and northern California on the west coast. The American goldfinch is a short-distance migrant, moving south in response to colder weather and lessened food supply. The migration is completed in compact flocks, which travel in an erratic, wavelike flight pattern.
Its winter range includes southern Canada and stretches south through the United States to parts of Mexico. In winter, in the northern part of its range, the finch may move nearer to feeders if they are available. In southern ranges, during winter, they remain in areas similar to the fields and flood plains where they live during the summer months.
Attempts were made to introduce the American goldfinch into Bermuda in the 19th century, and Tahiti in 1938, but the species failed to become established.
Behavior.
The American goldfinch flies in a distinctive undulating pattern, creating a wave-shaped path. This normally consists of a series of wing beats to lift the bird, then folding in the wings and gliding in an arc before repeating the pattern. Birds often vocalize during the flapping phase of the pattern and then go silent during the coasting phase. The call made during flight is "per-twee-twee-twee", or "ti-di-di-di", punctuated by the silent periods.
The American goldfinch is gregarious during the non-breeding season, when it is often found in large flocks, usually with other finches. During the breeding season, it lives in loose colonies. While the nest is being constructed, the male will act aggressively toward other males who intrude into his territory, driving them away, and the female reacts in the same way toward other females. This aggressiveness subsides once the eggs have been laid.
The American goldfinch does not act aggressively toward predators within its territory; its only reaction is alarm calling. Predators include snakes, weasels, squirrels, and blue jays, which may destroy eggs or kill young, and hawks and cats, which pose a threat to both young and adults. As of 2007, the oldest known American goldfinch was 10 years and 5 months old.
Diet.
The American goldfinch is a diurnal feeder. According to the Cornell Lab of Ornithology, the species is one of the strictest vegetarians in the bird world. It is mainly granivorous, but will occasionally eat insects, which are also fed to its young to provide protein. Its diet consists of the seeds from a wide variety of annual plants, often those of weeds grasses and trees, such as thistle, teasel, dandelion, ragweed, mullein, cosmos, goatsbeard, sunflower, and alder. However, it also consumes tree buds, maple sap, and berries. It will eat at bird feeders provided by humans, particularly in the winter months, preferring Niger seed (commonly and erroneously called thistle seed).
Unlike some finch species, the American goldfinch uses its feet extensively in feeding. It frequently hangs from seedheads while feeding in order to reach the seeds more easily. In the spring, the American goldfinch feeds on the catkins hanging from birches and alders by pulling one up with its beak and using its toes to hold the catkin still against the branch. This dexterity enables it to take advantage of food sources relatively inaccessible to potential competitors, increasing its chances of survival.
Reproduction.
The American goldfinch begins its breeding season later in the year than any other finch and later than any other native North American bird, besides occasionally the sedge wren. This may be related to the abundance of seeds in the late summer months, as seeds represent the majority of their diet.
The courtship rituals of the American goldfinch include aerial maneuvers and singing by males, who begin courtship in late July. The flight displays begin as the male pursues the female, who flies in zigzagging evasive patterns. The male is able to signal his quality and fitness, both in the short term (current body condition) and long term (genes), through ornamentation (bill color and plumage). If a female accepts the male as a mate, the pair will fly in wide circles, as the male warbles throughout the flight.
Once a male has found a mate, he selects a territory, marking the boundaries by warbling as he flies from perch to perch. After circling the perimeter, he performs two flight displays, first repeating a low, flat flight, then flying in an exaggerated version of normal flight, tucking his wings close to his body, plummeting earthwards and catching himself as he spreads his wings to glide upward in a series of loops. Two or three pairs may group their territories together in a loose colony, perhaps to aid in defense against predators.
American goldfinches lay four to six bluish-white eggs, which are oval in shape and about , roughly the size of a peanut. It is thought that they are laid during the night. The eggs are incubated by the female alone, though the male brings her food as she nests, and most mating pairs raise only one brood each year.
The chicks hatch 12–14 days after incubation begins. Like all passerines, the chicks are altricial; they are born naked, with reddish bodies, pale grey down, and closed eyes. The mother bird feeds her young regurgitated seeds and insects as they grow. The hatchlings develop quickly, opening their eyes after three days, and completing the growth of olive-brown juvenile plumage after 11–15 days, at which time they begin to practice short flights close to the nest. For up to three weeks after fledging, they are still fed by the male, who locates them by listening for their fledging call. The chicks stop giving this call when they become entirely independent.
American goldfinches are occasionally victims of brood parasites, particularly brown-headed cowbirds. One study found that 9% of nests had brown-headed cowbird eggs in them. American goldfinches make very poor hosts for brood parasites, with studies showing low hatching rates of brown-headed cowbird eggs and no fledging success. This is despite the fact that the American goldfinch has no known behavioral adaptations against brood parasites. It is thought that the inability of brown-headed cowbird chicks to survive is due to a failure to get enough nutrition; the seed-rich diet of American goldfinch chicks varies from the usual insect-rich diet of other hosts.
Relationship with humans.
The American goldfinch is found in residential areas throughout its range. Backyard birders attract it using feeders containing Nyjer thistle seed, or by planting grasses and perennial plants, such as zinnias, cosmos, bee balm, or globe thistle, which produce seedheads favored by finches. Although some controversy surrounds bird feeding (see bird feeder for details), an increase in backyard feeding by humans has generally been beneficial to this species.
The American goldfinch is not threatened by human activity, and is widespread throughout its range. The clearing of forests by humans, though harmful to many species, has benefited the American goldfinch. Clearing of woodlands causes declines in numbers of neotropical migrants, while favoring short-distance migrants and permanent residents. This benefits the American goldfinch both as a short-distance migrant, and because the created open areas are the preferred environment of the bird, where weeds thrive which produce the primary food source of the American goldfinch.
State bird.
The American goldfinch is the state bird of Iowa and New Jersey, where it is called the "eastern goldfinch", and Washington, where it is called the "willow goldfinch". It was chosen by schoolchildren in Washington in 1951.

</doc>
<doc id="40674" url="https://en.wikipedia.org/wiki?curid=40674" title="Meissen">
Meissen

Meissen (in German orthography: "Meißen") is a town of approximately 30,000 about northwest of Dresden on both banks of the Elbe river in the Free State of Saxony, in eastern Germany. Meissen is the home of Meissen porcelain, the Albrechtsburg castle, the Gothic Meissen Cathedral and the Meissen Frauenkirche. The "Grosse Kreisstadt" is the capital of the Meissen district.
History.
Meissen is sometimes known as the "cradle of Saxony". The city grew out of the early Slavic settlement of "Mis(s)ni", named for the small river Mis(s)na today Meis(s)abach (see Miesbach/Musbach/Mosbach), inhabited by the Slavic Glomacze tribe and was founded as a German town by King Henry the Fowler of Germany in 929. In 968, the Diocese of Meissen was founded, and Meissen became the episcopal see of a bishop. The Catholic bishopric was suppressed in 1581 after the diocese accepted the Protestant Reformation (1559), but re-created in 1921 with its seat first at Bautzen and now at the Katholische Hofkirche in Dresden.
The Margraviate of Meissen was founded in 968 as well, with the city as the capital of the Margraves of Meissen. A market town by 1000, Meissen passed to the Duchy of Poland in 1002 under Boleslaw I the Brave, afterwards into hands of Henry II a few months later and the House of Wettin in 1089. In 1015 Meissen was besieged by the Poles led by future King Mieszko II. The city was at the forefront of the Ostsiedlung, or intensive German settlement of the rural Slavic lands east of the Elbe, and its reception of city rights dates to 1332.
The construction of Meissen Cathedral was begun in 1260 on the same hill as the Albrechtsburg castle. The resulting lack of space led to the cathedral being one of the smallest cathedrals in Europe. The church is also known as being one of the purest examples of Gothic architecture.
In 1423 Meissen became capital of the Electorate of Saxony. In 1464 the capital was moved to Dresden.
In 1759 the Austrians defeated the Prussians at the Battle of Meissen.
During World War II, a subcamp of Flossenbürg concentration camp was located in Meissen.
Porcelain.
Meissen is famous for the manufacture of porcelain, based on extensive local deposits of china clay (kaolin) and potter's clay (potter's earth). Meissen porcelain was the first high quality porcelain to be produced outside of the Orient.
The first European porcelain was manufactured in Meissen in 1710, when by decree of King Augustus II the Strong the Royal-Polish and Electoral-Saxon Porcelain Factory ("Königlich-Polnische und Kurfürstlich-Sächsische Porzellan-Manufaktur") was opened in the Albrechtsburg. In 1861, it was moved to the "Triebisch" river valley of Meissen, where the porcelain factory can still be found today. Along with porcelain, other ceramics are also manufactured.
Main sights.
The Albrechtsburg, the former residence of the House of Wettin, is regarded as being the first castle to be used as a royal residence in the German-speaking world. Built between 1472 and 1525, it is a fine example of late Gothic style. It was redecorated in the 19th century with a range of murals depicting Saxon history. Today the castle is a museum. Nearby is the 13th-century Gothic Meissen Cathedral ("Meißner Dom"), whose chapel is one of the most famous burial places of the Wettin family. The hill on which the castle and the cathedral are built offers a view over the roofs of the old town.
Meissen's historical district is located mostly around the market at the foot of the castle hill. It contains many buildings of Renaissance architecture. Also imposing is the view from the 57 metre high tower of the "Frauenkirche" (Church of Our Lady), situated in the old market-place. This church, not to be confused with the Dresden Frauenkirche, was first mentioned in a 1205 deed issued by Bishop Dietrich II and after a blaze about 1450 rebuilt in the Late Gothic style of a hall church. Its tower hosts the world's first porcelain carillon, manufactured in 1929 on the occasion of the town's 1000-years-jubilee. Another popular tourist sight is the world-famous Meissen porcelain factory.
From spring to autumn, several festivals take place in Meissen, such as the pottery market or the "Weinfest", which celebrates the wine harvest. Meissen wine is produced at the vineyards in the river valley ("Elbtal") around the town, part of the Saxonian wine region, one of the northernmost in Europe.
Twin towns – Sister cities.
Meissen is twinned with:

</doc>
<doc id="40676" url="https://en.wikipedia.org/wiki?curid=40676" title="Luc Besson">
Luc Besson

Luc Besson (; born 18 March 1959) is a French film director, screenwriter, and producer. He is known for directing and producing thrillers and action films that are visually rich. Critics cite Besson as a pivotal figure in the Cinéma du look movement, a specific, highly visual style produced from the 1980s into the early 1990s. "Subway" (1985), "The Big Blue" (1988) and "Nikita" (1990) are all considered to be of this stylistic school. Besson had been nominated for Best Director and Best Picture César Awards for his films ' and '. He won Best Director and Best French Director for his sci-fi action film "The Fifth Element" (1997). His sci-fi thriller film "Lucy" (2014) is France's biggest export success.
In 1980, he founded his own production company, called "Les Films du Loup," and later "Les Films du Dauphin." This was superseded in 2000 by his co-founding EuropaCorp film company with his longtime collaborator, Pierre-Ange Le Pogam. Besson has been involved with filmmaking for more than 50 films, spanning 26 years, as writer, director, and/or producer. He is also being declared the John Hughes of action films because his screen writing skills are more famous than his directing.
Early life.
Besson was born in Paris, to parents who both worked as Club Med scuba-diving instructors. Influenced by this milieu, as a child Besson planned to become a marine biologist. He spent much of his youth traveling with his parents to tourist resorts in Italy, Yugoslavia, and Greece. The family returned to France when Besson was 10. His parents promptly divorced and each remarried.
"Here there is two families, and I am the only bad souvenir of something that doesn't work," he said in the "International Herald Tribune". "And if I disappear, then everything is perfect. The rage to exist comes from here. I have to do something! Otherwise I am going to die."
At the age of 17, Besson had a diving accident that left him unable to dive.
Career.
Out of boredom, Besson started writing stories, including the background to what he later developed as "The Fifth Element" (1997), one of his most popular movies. The film is inspired by the French comic books which Besson read as a teenager. He reportedly worked on the first drafts of "Le Grand Bleu" while still in his teens. Besson directed and co-wrote the screenplay of this science fiction thriller with the screenwriter, Robert Mark Kamen.
At 18, Besson returned to his birthplace of Paris. There he took odd jobs in film to get a feel for the industry. He worked as an assistant to directors including Claude Faraldo and Patrick Grandperret. Besson directed three short films, a commissioned documentary, and several commercials.
After this, he moved to the United States for three years, but returned to Paris, where he formed his own production company. He first named it "Les Films du Loup," but changed it to "Les Films du Dauphin". In the early 1980s, Besson met Éric Serra and asked him to compose the score for his first short film, "L'Avant dernier". He later used Serra as a composer for other films of his.
Since the late 20th century, Besson has written and produced numerous action movies, including the "Taxi" (1998–2007) and "The Transporter" (2002–2008) series, and the Jet Li films "Kiss of the Dragon" and "Unleashed/Danny the Dog." His English-language films "Taken", "Taken 2" and "Taken 3", all starring Liam Neeson, have been major successes, with "Taken 2" becoming the largest-grossing export French film. Besson produced the promotional movie for the Paris bid for the 2012 Summer Olympics.
Besson won Best Director and Best French Director for his film "The Fifth Element" (1997). he was nominated for Best Director and Best Picture César Awards for his films ' (1994) and ' (1999).
Cinéma du look.
Critics cite Besson as a pivotal figure in the Cinéma du look movement, a specific, highly visual style produced from the 1980s into the early 1990s. "Subway" (1985), "The Big Blue" (1988) and "Nikita" (1990) are all considered to be of this stylistic school. The term was coined by critic Raphaël Bassan in a 1989 essay in "La Revue du Cinema n° 449." A partisan of the experimental cinema and friend of the New Wave (""nouvelle vague"") directors, Bassan grouped Besson with Jean-Jacques Beineix and Leos Carax as three directors who shared the style of ""le look."" These directors were later described critically as favoring style over substance, and spectacle over narrative.
Besson, along with most of the filmmakers so categorized, was uncomfortable with the label, particularly in light of the achievements of their forebears: France's New Wave. "Jean-Luc Godard and François Truffaut were rebelling against existing cultural values and used cinema as a means of expression simply because it was the most avant-garde medium at the time," said Besson in a 1985 interview in "The New York Times". "Today, the revolution is occurring entirely within the industry and is led by people who want to change the look of movies by making them better, more convincing and pleasurable to watch.
"Because it's becoming increasingly difficult to break into this field, we have developed a psychological armor and are ready to do anything in order to work", he added in this same interview. "I think our ardor alone is going to shake the pillars of the moviemaking establishment."
Besson directed a biopic of Aung San Suu Kyi called "The Lady" (original title "Dans la Lumiere"), which was released in the fall of 2011. He also worked on "Lockout", which was released in April 2012.
Work.
Many of Besson's films have achieved popular, if not critical, success. One such release was "Le Grand Bleu". 
"When the film had its premiere on opening night at the 1988 Cannes Film Festival, it was mercilessly drubbed, but no matter; it was a smash," observed the "International Herald Tribune" in a 2007 profile of Besson. "Embraced by young people who kept returning to see it again, the movie sold 10 million tickets and quickly became what the French call a 'film générationnel,' a defining moment in the culture."
Besson created the Arthur series, which comprises "Arthur and the Minimoys", "Arthur and the Forbidden City", "Arthur and the Vengeance of Maltazard" and "Arthur and the War of the Two Worlds". He directed "Arthur and the Invisibles", an adaptation of the first two books of the collection. A film with live action and animation, it was released in the UK and the US and starred Freddie Highmore, Madonna, Snoop Dogg, Mia Farrow, Robert De Niro and David Bowie.
Critical evaluation.
Besson has been described as "the most Hollywood of French filmmakers." Tobias Scott wrote that his "slick, commercial" action movies were "so interchangeable—drugs, sleaze, chuckling supervillainy, and Hong Kong-style effects—that each new project probably starts with white-out on the title page."
American film critic Armond White has praised Besson, whom he ranks as one of the best film producers, for refining and revolutionizing action film. He wrote that Besson dramatizes the struggle of his characters "as a conscientious resistance to human degradation".
Personal life.
Besson has been married four times; first, in 1986, to actress Anne Parillaud who would star in Besson's "Nikita" (1990). Besson and Parillaud had a daughter, Juliette, born in 1987. The couple divorced in 1991.
Besson's second wife was actress Maïwenn Le Besco, who was 15 when they began dating in 1991. They were married in late 1992 when Le Besco was pregnant with their daughter Shanna, who was born on 3 January 1993. Le Besco later claimed that their relationship inspired Besson's film "" (1994). Their marriage ended in 1997, when Besson became involved with actress Milla Jovovich during the filming of "The Fifth Element" (1997). He married the 22-year-old on 14 December 1997, at the age of , but they divorced in 1999.
On 28 August 2004, at the age of , Besson married film producer Virginie Silla. The couple has three children: Talia, Satine, and Mao Besson.
Legacy and honours.
Among Besson's awards are the Brussels International Festival of Fantasy Film Critics Prize, Fantasporto Audience Jury Award-Special Mention, Best Director, and Best Film, for "Le Dernier Combat" in 1983; the Italian National Syndicate of Film Journalists Silver Ribbon-Best Director-Foreign Film, for "La Femme Nikita", 1990; the Alexander Korda Award for Best British Film, "Nil by Mouth", 1997; and the Best Director Cesar Award, for "The Fifth Element", 1997.
Film company.
In 2000, Besson superseded his production company by co-founding EuropaCorp with Pierre-Ange Le Pogam, with whom he had frequently worked since 1985. Le Pogam had then been Distribution Director with Gaumont. EuropaCorp has had strong growth based on several English-language films, with international distribution. It has production facilities in Paris, Normandy, and Hollywood, and is establishing distribution partnerships in Japan and China.

</doc>
<doc id="40677" url="https://en.wikipedia.org/wiki?curid=40677" title="Cornish">
Cornish

Cornish is the adjective and demonym associated with Cornwall, the most southwesterly part of the United Kingdom. It may refer to:

</doc>
<doc id="40679" url="https://en.wikipedia.org/wiki?curid=40679" title="Abort">
Abort

Abort can mean:

</doc>
<doc id="40680" url="https://en.wikipedia.org/wiki?curid=40680" title="Absolute gain">
Absolute gain

Absolute gain may refer to:

</doc>
<doc id="40682" url="https://en.wikipedia.org/wiki?curid=40682" title="Access">
Access

Access may refer to: getting in

</doc>
<doc id="40684" url="https://en.wikipedia.org/wiki?curid=40684" title="Access control">
Access control

In the fields of access control is the selective restriction of access to a place or other 
resource. The act of "accessing" may mean consuming, entering, or using. Permission to access a resource is called "authorization".
Locks and login credentials are two analogous mechanisms of access control.
Physical security.
Geographical access control may be enforced by personnel (e.g., border guard, bouncer, ticket checker), or with a device such as a turnstile. There may be fences to avoid circumventing this access control. An alternative of access control in the strict sense (physically controlling access itself) is a system of checking authorized presence, see e.g. Ticket controller (transportation). A variant is exit control, e.g. of a shop (checkout) or a country.
The term access control refers to the practice of restricting entrance to a property, a building, or a room to authorized persons. Physical access control can be achieved by a human (a guard, bouncer, or receptionist), through mechanical means such as locks and keys, or through technological means such as access control systems like the mantrap. Within these environments, physical key management may also be employed as a means of further managing and monitoring access to mechanically keyed areas or access to certain small assets.
Physical access control is a matter of who, where, and when. An access control system determines who is allowed to enter or exit, where they are allowed to exit or enter, and when they are allowed to enter or exit. Historically, this was partially accomplished through keys and locks. When a door is locked, only someone with a key can enter through the door, depending on how the lock is configured. Mechanical locks and keys do not allow restriction of the key holder to specific times or dates. Mechanical locks and keys do not provide records of the key used on any specific door, and the keys can be easily copied or transferred to an unauthorized person. When a mechanical key is lost or the key holder is no longer authorized to use the protected area, the locks must be re-keyed.
Electronic access control uses computers to solve the limitations of mechanical locks and keys. A wide range of credentials can be used to replace mechanical keys. The electronic access control system grants access based on the credential presented. When access is granted, the door is unlocked for a predetermined time and the transaction is recorded. When access is refused, the door remains locked and the attempted access is recorded. The system will also monitor the door and alarm if the door is forced open or held open too long after being unlocked.
Access control system operation.
When a credential is presented to a reader, the reader sends the credential’s information, usually a number, to a control panel, a highly reliable processor. The control panel compares the credential's number to an access control list, grants or denies the presented request, and sends a transaction log to a database. When access is denied based on the access control list, the door remains locked. If there is a match between the credential and the access control list, the control panel operates a relay that in turn unlocks the door. The control panel also ignores a door open signal to prevent an alarm. Often the reader provides feedback, such as a flashing red LED for an access denied and a flashing green LED for an access granted.
The above description illustrates a single factor transaction. Credentials can be passed around, thus subverting the access control list. For example, Alice has access rights to the server room, but Bob does not. Alice either gives Bob her credential, or Bob takes it; he now has access to the server room. To prevent this, two-factor authentication can be used. In a two factor transaction, the presented credential and a second factor are needed for access to be granted; another factor can be a PIN, a second credential, operator intervention, or a biometric input.
There are three types (factors) of authenticating information:
Passwords are a common means of verifying a user's identity before access is given to information systems. In addition, a fourth factor of authentication is now recognized: someone you know, whereby another person who knows you can provide a human element of authentication in situations where systems have been set up to allow for such scenarios. For example, a user may have their password, but have forgotten their smart card. In such a scenario, if the user is known to designated cohorts, the cohorts may provide their smart card and password, in combination with the extant factor of the user in question, and thus provide two factors for the user with the missing credential, giving three factors overall to allow access.
Credential.
A credential is a physical/tangible object, a piece of knowledge, or a facet of a person's physical being, that enables an individual access to a given physical facility or computer-based information system. Typically, credentials can be something a person knows (such as a number or PIN), something they have (such as an access badge), something they are (such as a biometric feature) or some combination of these items. This is known as multi-factor authentication. The typical credential is an access card or key-fob, and newer software can also turn users' smartphones into access devices.
There are many card technologies including magnetic stripe, bar code, Wiegand, 125 kHz proximity, 26-bit card-swipe, contact smart cards, and contactless smart cards. Also available are key-fobs, which are more compact than ID cards, and attach to a key ring. Biometric technologies include fingerprint, facial recognition, iris recognition, retinal scan, voice, and hand geometry. The built-in biometric technologies found on newer smartphones can also be used as credentials in conjunction with access software running on mobile devices. In addition to older more traditional card access technologies, newer technologies such as Near field communication (NFC) and Bluetooth low energy also have potential to communicate user credentials to readers for system or building access.
Access control system components.
An access control point, which can be a door, turnstile, parking gate, elevator, or other physical barrier, where granting access can be electronically controlled. Typically, the access point is a door. An electronic access control door can contain several elements. At its most basic, there is a stand-alone electric lock. The lock is unlocked by an operator with a switch. To automate this, operator intervention is replaced by a reader. The reader could be a keypad where a code is entered, it could be a card reader, or it could be a biometric reader. Readers do not usually make an access decision, but send a card number to an access control panel that verifies the number against an access list. To monitor the door position a magnetic door switch can be used. In concept, the door switch is not unlike those on refrigerators or car doors. Generally only entry is controlled, and exit is uncontrolled. In cases where exit is also controlled, a second reader is used on the opposite side of the door. In cases where exit is not controlled, free exit, a device called a request-to-exit (REX) is used. Request-to-exit devices can be a push-button or a motion detector. When the button is pushed, or the motion detector detects motion at the door, the door alarm is temporarily ignored while the door is opened. Exiting a door without having to electrically unlock the door is called mechanical free egress. This is an important safety feature. In cases where the lock must be electrically unlocked on exit, the request-to-exit device also unlocks the door.
Access control topology.
Access control decisions are made by comparing the credential to an access control list. This look-up can be done by a host or server, by an access control panel, or by a reader. The development of access control systems has seen a steady push of the look-up out from a central host to the edge of the system, or the reader. The predominant topology circa 2009 is hub and spoke with a control panel as the hub, and the readers as the spokes. The look-up and control functions are by the control panel. The spokes communicate through a serial connection; usually RS-485. Some manufactures are pushing the decision making to the edge by placing a controller at the door. The controllers are IP enabled, and connect to a host and database using standard networks.
Types of readers.
Access control readers may be classified by the functions they are able to perform:
Some readers may have additional features such as an LCD and function buttons for data collection purposes (i.e. clock-in/clock-out events for attendance reports), camera/speaker/microphone for intercom, and smart card read/write support.
Access control readers may also be classified by their type of identification technology.
Access control system topologies.
1. Serial controllers. Controllers are connected to a host PC via a serial RS-485 communication line (or via 20mA current loop in some older systems). External RS-232/485 converters or internal RS-485 cards have to be installed, as standard PCs do not have RS-485 communication ports.
Advantages:
Disadvantages: 
2. Serial main and sub-controllers. All door hardware is connected to sub-controllers (a.k.a. door controllers or door interfaces). Sub-controllers usually do not make access decisions, and instead forward all requests to the main controllers. Main controllers usually support from 16 to 32 sub-controllers.
Advantages:
Disadvantages: 
3. Serial main controllers & intelligent readers. All door hardware is connected directly to intelligent or semi-intelligent readers. Readers usually do not make access decisions, and forward all requests to the main controller. Only if the connection to the main controller is unavailable, will the readers use their internal database to make access decisions and record events. Semi-intelligent reader that have no database and cannot function without the main controller should be used only in areas that do not require high security. Main controllers usually support from 16 to 64 readers. All advantages and disadvantages are the same as the ones listed in the second paragraph. 
4. Serial controllers with terminal servers. In spite of the rapid development and increasing use of computer networks, access control manufacturers remained conservative, and did not rush to introduce network-enabled products. When pressed for solutions with network connectivity, many chose the option requiring less efforts: addition of a terminal server, a device that converts serial data for transmission via LAN or WAN.
Advantages:
Disadvantages:
All the RS-485-related advantages and disadvantages also apply.
5. Network-enabled main controllers. The topology is nearly the same as described in the second and third paragraphs. The same advantages and disadvantages apply, but the on-board network interface offers a couple of valuable improvements. Transmission of configuration and user data to the main controllers is faster, and may be done in parallel. This makes the system more responsive, and does not interrupt normal operations. No special hardware is required in order to achieve redundant host PC setup: in the case that the primary host PC fails, the secondary host PC may start polling network controllers. The disadvantages introduced by terminal servers (listed in the fourth paragraph) are also eliminated. 
6. IP controllers. Controllers are connected to a host PC via Ethernet LAN or WAN.
Advantages:
Disadvantages: 
7. IP readers. Readers are connected to a host PC via Ethernet LAN or WAN.
Advantages:
Disadvantages:
The advantages and disadvantages of IP controllers apply to the IP readers as well.
Security risks.
The most common security risk of intrusion through an access control system is by simply following a legitimate user through a door, and this is referred to as "tailgating". Often the legitimate user will hold the door for the intruder. This risk can be minimized through security awareness training of the user population, or more active means such as turnstiles. In very high security applications this risk is minimized by using a sally port, sometimes called a security vestibule or mantrap, where operator intervention is required presumably to assure valid identification.
The second most common risk is from levering a door open. This is relatively difficult on properly secured doors with strikes or high holding force maglocks. The lever could be as small as a screwdriver or big as a crow bar. Fully implemented access control systems include forced door monitoring alarms. These vary in effectiveness, usually failing from high false positive alarms, poor database configuration, or lack of active intrusion monitoring.
The third most common security risk is natural disasters. In order to mitigate risk from natural disasters, the structure of the building, down to the quality of the network and computer equipment vital. From an organizational perspective, the leadership will need to adopt and implement an All Hazards Plan, or Incident Response Plan. The highlights of any incident plan determined by the National Incident Management System must include Pre-incident planning, during incident actions, disaster recovery, and after action review.
Similar to levering is crashing through cheap partition walls. In shared tenant spaces the divisional wall is a vulnerability. A vulnerability along the same lines is the breaking of sidelights.
Spoofing locking hardware is fairly simple and more elegant than levering. A strong magnet can operate the solenoid controlling bolts in electric locking hardware. Motor locks, more prevalent in Europe than in the US, are also susceptible to this attack using a doughnut shaped magnet. It is also possible to manipulate the power to the lock either by removing or adding current, although most Access Control systems incorporate battery back-up systems and the locks are almost always located on the secure side of the door. 
Access cards themselves have proven vulnerable to sophisticated attacks. Enterprising hackers have built portable readers that capture the card number from a user’s proximity card. The hacker simply walks by the user, reads the card, and then presents the number to a reader securing the door. This is possible because card numbers are sent in the clear, no encryption being used. To counter this, dual authentication methods, such as a card plus a PIN, should always be used.
Many access control credentials unique serial numbers are programmed in sequential order during manufacturing. Known as a sequential attack, if an intruder has a credential once used in the system they can simply increment or decrement the serial number until they find a credential that is currently authorized in the system. Ordering credentials with random unique serial numbers is recommended to counter this threat.
Finally, most electric locking hardware still have mechanical keys as a fail-over. Mechanical key locks are vulnerable to bumping.
The need-to-know principle.
The need to know principle can be enforced with user access controls and authorization procedures and its objective is to ensure that only authorized individuals gain access to information or systems necessary to undertake their duties.
Computer security.
In computer security, general access control includes authorization, authentication, access approval, and audit. A more narrow definition of access control would cover only access approval, whereby the system makes a decision to grant or reject an access request from an already authenticated subject, based on what the subject is authorized to access. Authentication and access control are often combined into a single operation, so that access is approved based on successful authentication, or based on an anonymous access token. Authentication methods and tokens include passwords, biometric scans, physical keys, electronic keys and devices, hidden paths, social barriers, and monitoring by humans and automated systems.
In any access-control model, the entities that can perform actions on the system are called "subjects", and the entities representing resources to which access may need to be controlled are called "objects" (see also Access Control Matrix). Subjects and objects should both be considered as software entities, rather than as human users: any human users can only have an effect on the system via the software entities that they control.
Although some systems equate subjects with "user IDs", so that all processes started by a user by default have the same authority, this level of control is not fine-grained enough to satisfy the principle of least privilege, and arguably is responsible for the prevalence of malware in such systems (see computer insecurity).
In some models, for example the object-capability model, any software entity can potentially act as both subject and object.
, access-control models tend to fall into one of two classes: those based on capabilities and those based on access control lists (ACLs).
Both capability-based and ACL-based models have mechanisms to allow access rights to be granted to all members of a "group" of subjects (often the group is itself modeled as a subject).
Access control systems provide the essential services of "authorization", "identification and authentication" ("I&A"), "access approval", and "accountability" where:
Access control models.
Access to accounts can be enforced through many types of controls.
Telecommunication.
In telecommunication, the term "access control" is defined in U.S. Federal Standard 1037C with the following meanings: 
This definition depends on several other technical terms from Federal Standard 1037C.
Public policy.
In public policy, access control to restrict access to systems ("authorization") or to track or monitor behavior within systems ("accountability") is an implementation feature of using trusted systems for security or social control.

</doc>
<doc id="40687" url="https://en.wikipedia.org/wiki?curid=40687" title="Access time">
Access time

Access time is the time delay or latency between a request to an electronic system, and the access being completed or the requested data returned

</doc>
<doc id="40688" url="https://en.wikipedia.org/wiki?curid=40688" title="Baud">
Baud

In telecommunication and electronics, baud (, unit symbol Bd) is the unit for symbol rate or modulation rate in "symbols per second" or "pulses per second". It is the number of distinct symbol changes (signaling events) made to the transmission medium per second in a digitally modulated signal or a line code.
Digital data modem manufacturers commonly define the baud as the modulation rate of data transmission and express it as bits per second.
Baud is related to gross bit rate expressed as bits per second.
Definitions.
The symbol duration time, also known as unit interval, can be directly measured as the time between transitions by looking into an eye diagram of an oscilloscope. The symbol duration time "T"s can be calculated as:
where "f"s is the symbol rate.
There is also a chance of miscommunication which leads to ambiguity.
In digital systems (i.e., using discrete/discontinuous values) with binary code, 1 Bd = 1 bit/s. By contrast, non-digital (or analog) systems use a continuous range of values to represent information and in these systems the exact informational size of 1 Bd varies.
The baud unit is named after Émile Baudot, the inventor of the Baudot code for telegraphy, and is represented in accordance with the rules for SI units. That is, the first letter of its symbol is uppercase (Bd), but when the unit is spelled out, it should be written in lowercase (baud) except when it begins a sentence.
The baud is scaled using standard decimal prefixes, so that for example
Relationship to gross bit rate.
The symbol rate is related to gross bit rate expressed in bit/s.
The term baud has sometimes incorrectly been used to mean bit rate, since these rates are the same in old modems as well as in the simplest digital communication links using only one bit per symbol, such that binary "0" is represented by one symbol, and binary "1" by another symbol. In more advanced modems and data transmission techniques, a symbol may have more than two states, so it may represent more than one bit. A bit (binary digit) always represents one of two states.
If "N" bits are conveyed per symbol, and the gross bit rate is "R", inclusive of channel coding overhead, the symbol rate "f"s can be calculated as
In that case "M"=2"N" different symbols are used. In a modem, these may be sinewave tones with unique combinations of amplitude, phase and/or frequency. For example, in a 64QAM modem, "M"=64, and so the bit rate is "N"=6 ( 6=log2(64) ) times the baud. In a line code, these may be "M" different voltage levels.
The ratio might not even be an integer; in 4B3T coding, the bit rate is 4/3 baud. (A typical basic rate interface with a 160 kbit/s raw data rate operates at 120 kBd.) On the other hand, Manchester coding has a bit rate equal to 1/2 the baud.
By taking information per pulse "N" in bit/pulse to be the base-2-logarithm of the number of distinct messages "M" that could be sent, Hartley constructed a measure of the gross bitrate "R" as

</doc>
<doc id="40690" url="https://en.wikipedia.org/wiki?curid=40690" title="Acoustic coupler">
Acoustic coupler

In telecommunications, an acoustic coupler is an interface device for coupling electrical signals by acoustical means—usually into and out of a telephone instrument.
The link is achieved through converting electric signals from the phone line to sound and reconvert sound to electric signals needed for the end terminal, such as a teletypewriter, and back, rather than through direct electrical connection.
History and applications.
Prior to its breakup in 1984, Bell System's legal monopoly over telephony in the United States allowed the company to impose strict rules on how consumers could access their network. Customers were prohibited from connecting equipment not made or sold by Bell to the network. The same set-up was operative in nearly all countries, where the telephone companies were nationally owned. In many households, telephones were hard-wired to wall terminals before connectors like RJ11 and BS 6312 became standardized.
The situation was similar in other countries. In Australia, until 1975 the PMG, a Government monopoly, owned all telephone wiring and equipment in user premises and prohibited attachment of third party devices, and while most handsets were connected by 600 series connectors, these were peculiar to Australia so imported equipment could not be directly connected in any case, despite the general electrical compatibility.
It was not until a landmark court ruling regarding the Hush-A-Phone in 1956 that the use of a phone attachment (by a third party vendor) was allowed for the first time; though AT&T's right to regulate any device connected to the telephone system was upheld by the courts, they were instructed to cease interference towards Hush-A-Phone users. A second court decision in 1968 regarding the Carterfone further allowed "any device not harmful to the system" to be connected directly to the AT&T network. This decision enabled the proliferation of later innovations like answering machines, fax machines, and modems.
When inventors began developing devices to send non-voice signals over the telephone line, the need for a workaround for the Bell restrictions was apparent. As early as 1937, telefax machines used by newspapers were using some kind of couplers, possibly acoustic but more likely magnetic for single-directional communication. Multiplexed bidirectional telephone coupling was not needed by these early fax machines.
Robert Weitbrecht created a workaround for the Bell restrictions in 1963. He developed a coupling device that converted sound from the ear piece of the telephone handset to electrical signals, and converts the electrical pulses coming from the teletypewriter to sound that goes into the mouth piece of the telephone handset. His acoustic coupler is known as the Weitbrecht Modem.
The Weitbrecht Modem inspired other engineers to develop other modems to work with 8-bit ASCII terminals at a faster rate. Such modems or couplers were developed around 1966 by John van Geen at the Stanford Research Institute (now SRI International), that mimicked handset operations. An early commercial model was built by Livermore Data Systems in 1968. One would dial the computer system (which would have telephone company datasets) on one's phone, and when the connection was established, place the handset into the acoustic modem. 
Since the handsets were all supplied by the telephone company, most had the same shape, simplifying the physical interface. A microphone and a speaker inside the modem box would pick up and transmit the signaling tones, and circuitry would convert those audio shift-key encoded frequency binary signals for an RS232 output socket. With luck one could get 300 baud (~bits/second) transmission rates, but 150 baud was more typical. 
That speed was sufficient for typewriter-based terminals, as the IBM 2741, running at 134.5 baud, or a teleprinter, running at 110 baud.
The practical upper limit for acoustic-coupled modems was 1200-baud, first made available in 1973 by Vadic and 1977 by AT&T. It became widespread in 1985 with advent of the Hayes Smartmodem 1200A. Such devices facilitated the creation of dial-up bulletin board systems, a forerunner of modern internet chat rooms, message boards, and e-mail.
Design.
Usually, a standard telephone handset was placed into a cradle that had been engineered to fit closely (by the use of rubber seals) around the microphone and earpiece of the handset. A modem would modulate a loudspeaker in the cup attached to the handset's microphone, and sound from the loudspeaker in the telephone handset's earpiece would be picked up by a microphone in the cup attached to the earpiece. In this way signals could be passed in both directions.
Acoustic couplers were sensitive to external noise and depended on the widespread standardization of the dimensions of telephone handsets. Direct electrical connections to telephone networks, once they were made legal, rapidly became the preferred method of attaching modems, and the use of acoustic couplers dwindled. Acoustic couplers are still used by people travelling in areas of the world where electrical connection to the telephone network is illegal or impractical. Many models of TDDs (Telecommunications Device for the Deaf) still have a built-in acoustic coupler, which allow more universal use with pay phones and for 911 calls by deaf people.
An acoustic coupler is prominently shown early in the 1983 film "WarGames", when character David Lightman (depicted by actor Matthew Broderick) places a telephone handset into the cradle of a film prop acoustic modem to accentuate the act of using telephone lines for interconnection to the developing computer networks of the period—in this case, a military command computer.

</doc>
<doc id="40692" url="https://en.wikipedia.org/wiki?curid=40692" title="Active laser medium">
Active laser medium

The active laser medium (also called gain medium or lasing medium) is the source of optical gain within a laser. The gain results from the stimulated emission of electronic or molecular transitions to a lower energy state from a higher energy state
previously populated by a pump source.
Examples of active laser media include:
In order to fire a laser, the active gain medium must be in a nonthermal energy distribution known as a population inversion. The preparation of this state requires an external energy source and is known as laser pumping. Pumping may be achieved with electrical currents (e.g. semiconductors, or gases via high-voltage discharges) or with light, generated by discharge lamps or by other lasers (semiconductor lasers). More exotic gain media can be pumped by chemical reactions, nuclear fission, or with high-energy electron beams.
Example of a model of gain medium.
A universal model valid for all laser types does not exist. 
The simplest model includes two systems of sub-levels: upper and lower. Within each sub-level system, the fast transitions ensure that thermal equilibrium is reached quickly, leading to the Maxwell–Boltzmann statistics of excitations among sub-levels in each system "(fig.1)". The upper level is assumed to be metastable.
Also, gain and refractive index are assumed independent of a particular way of excitation. 
For good performance of the gain medium, the separation between sub-levels should be larger than working temperature; then, at pump frequency formula_1, the absorption dominates.
In the case of amplification of optical signals, the lasing frequency is called "signal frequency." However, the same term is used even in the laser oscillators, when amplified radiation is used to transfer energy rather than information. The model below seems to work well for most optically-pumped solid-state lasers.
Cross-sections.
The simple medium can be characterized with effective cross-sections of absorption and emission at frequencies formula_1 and formula_3.
The relative concentrations can be defined as formula_8 and formula_9.
The rate of transitions of an active center from ground state to the excited state can be expressed with formula_10 and
The rate of transitions back to the ground state can be expressed with formula_11,
where formula_12 and formula_13 are effective cross-sections of absorption at the frequencies of the signal and the pump. 
formula_14 and formula_15 are the same for stimulated emission;
formula_16 is rate of the spontaneous decay of the upper level.
Then, the kinetic equation for relative populations can be written as follows: 
formula_17
The dynamic saturation intensities can be defined:
formula_18,
formula_19.
The absorption at strong signal:
formula_20.
The gain at strong pump:
formula_21,
where formula_22
is determinant of cross-section. 
Gain never exceeds value formula_23, and absorption never exceeds value formula_24.
At given intensities formula_25, formula_26 of pump and signal, the gain and absorption
can be expressed as follows:
formula_27,
formula_28,
where 
formula_29, 
formula_30, 
formula_31, 
formula_32 .
Identities.
The following identities take place:
formula_33, formula_34
The state of gain medium can be characterized with a single parameter, such as population of the upper level, gain or absorption.
Efficiency of the gain medium.
The efficiency of a gain medium can be defined as
formula_35.
Within the same model, the efficiency can be expressed as follows:
formula_36.
For the efficient operation both intensities, pump and signal should exceed their saturation intensities;
formula_37, and formula_38.
The estimates above are valid for a medium uniformly filled with pump and signal light. The spatial hole burning may slightly reduce the efficiency because some regions are pumped well, but the pump is not efficiently withdrawn by the signal in the nodes of
the interference of counter-propagating waves.

</doc>
<doc id="40693" url="https://en.wikipedia.org/wiki?curid=40693" title="Adaptive communications">
Adaptive communications

Adaptive communications can mean any communications system, or portion thereof, that automatically uses feedback information obtained from the system itself or from the signals carried by the system to modify dynamically one or more of the system operational parameters to improve system performance or to resist degradation. 
The modification of a system parameter may be discrete, as in hard-switched diversity reception, or may be continuous, as in a 
predetection combining algorithm.

</doc>
<doc id="40694" url="https://en.wikipedia.org/wiki?curid=40694" title="Adaptive predictive coding">
Adaptive predictive coding

Adaptive predictive coding (APC) is a narrowband analog-to-digital conversion that uses a one-level or multilevel sampling system in which the value of the signal at each sampling instant is predicted according to a linear function of the past values of the quantized signals.
APC is related to linear predictive coding (LPC) in that both use adaptive predictors. However, APC uses fewer prediction coefficients, thus requiring a higher sampling rate than LPC.

</doc>
<doc id="40695" url="https://en.wikipedia.org/wiki?curid=40695" title="Adder–subtractor">
Adder–subtractor

In digital circuits, an adder–subtractor is a circuit that is capable of adding or subtracting numbers (in particular, binary).
Below is a circuit that does adding "or" subtracting" depending on a control signal.
It is also possible to construct a circuit that performs both addition and subtraction at the same time.
Construction.
Having an "n"-bit adder for formula_1 and formula_2, then formula_3.
Then, assume the numbers are in two's complement.
Then to perform formula_4, two's complement theory says to invert each bit with a NOT gate then add one.
This yields formula_5, which is easy to do with a slightly modified adder.
By preceding each formula_1 input bit on the adder with a 2-to-1 multiplexer where:
that has control input formula_11 that is also connected to the initial carry, then the modified adder performs
This works because when formula_14 the formula_1 input to the adder is really formula_16 and the carry in is formula_17. Adding formula_2 to formula_16 and formula_17 yields the desired subtraction of formula_21.
A way you can mark number formula_1 as positive or negative without using a multiplexer on each bit is to use a XOR (Exclusive OR) gate to precede each bit instead. 
This produces the same truth table for the bit arriving at the adder as the multiplexer solution does since the XOR gate output will be what the input bit is when formula_12 and the inverted input bit when formula_13.
Role in the arithmetic logic unit.
Adders are a part of the core of an arithmetic logic unit (ALU).
The control unit decides which operations an ALU should perform (based on the op code being executed) and sets the ALU operation.
The formula_11 input to the adder–subtractor above would be one such control line from the control unit.
The adder–subtractor above could easily be extended to include more functions.
For example, a 2-to-1 multiplexer could be introduced on each formula_27 that would switch between zero and formula_27; this could be used (in conjunction with formula_14) to yield the two's complement of formula_1 since formula_31.
A further step would be to change the 2-to-1 mux on formula_1 to a 4-to-1 with the third input being zero, then replicating this on formula_27 thus yielding the following output functions:
By adding more logic in front of the adder, a single adder can be converted into much more than just an adder—an ALU.

</doc>
<doc id="40696" url="https://en.wikipedia.org/wiki?curid=40696" title="Address">
Address

Address or The Address may refer to:

</doc>
<doc id="40699" url="https://en.wikipedia.org/wiki?curid=40699" title="Adjacent-channel interference">
Adjacent-channel interference

Adjacent-channel interference (ACI) is interference caused by extraneous power from a signal in an adjacent channel. ACI may be caused by inadequate filtering (such as incomplete filtering of unwanted modulation products in FM systems), improper tuning or poor frequency control (in the reference channel, the interfering channel or both).
ACI is distinguished from crosstalk.
Origin.
The adjacent-channel interference which receiver A experiences from a transmitter B is the sum of the power that B emits into A's channel—known as the "unwanted emission", and represented by the ACLR (Adjacent Channel Leakage Ratio)—and the power that A picks up from B's channel, which is represented by the ACS (Adjacent Channel Selectivity). B emitting power into A's channel is called adjacent-channel leakage (unwanted emissions). It occurs for two reasons. First, because RF filters require a roll-off, and do not eliminate a signal completely. Second, due to intermodulation in B's amplifiers, which cause the transmitted spectrum to spread beyond what was intended. Therefore, B emits some power in the adjacent channel which is picked up by A. A receives some emissions from B's channel due to the roll off of A's selectivity filters. Selectivity filters are designed to "select" a channel. Similarly, B's signal suffers intermodulation distortion passing through A's RF input amplifiers, leaking more power into adjacent frequencies.
Avoidance procedure.
Broadcast regulators frequently manage the broadcast spectrum in order to minimize adjacent-channel interference. For example, in North America, FM radio stations in a single region cannot be licensed on adjacent frequencies — that is, if a station is licensed on 99.5 MHz in a city, the first-adjacent frequencies of 99.3 MHz and 99.7 MHz cannot be used anywhere within a certain distance of that station's transmitter, and the second-adjacent frequencies of 99.1 MHz and 99.9 MHz are restricted to specialized usages such as low-power stations. Similar restrictions formerly applied to third-adjacent frequencies as well (i.e. 98.9 MHz and 100.1 MHz in the example above), but these are no longer observed. 

</doc>
<doc id="40700" url="https://en.wikipedia.org/wiki?curid=40700" title="Advanced Data Communication Control Procedures">
Advanced Data Communication Control Procedures

In telecommunication, Advanced Data Communication Control Procedures (or Protocol) (ADCCP) is a bit-oriented data link layer protocol used to provide point-to-point and point-to-multipoint transmission of data frames that contain error control information. It places data on a network and ensures proper delivery to a destination. ADCCP is based on the IBM's SDLC protocol. The HDLC by ISO and LAPB by ITU/CCITT are based on ADCCP.
ADCCP is an ANSI standard, X3.66, derived from IBM's Synchronous Data Link Control (SDLC) protocol, and is functionally equivalent to the ISO High-Level Data Link Control (HDLC) standard.
ADCCP has 3 main modes – NRM (Normal Response mode akin to SDLC), ABM (Asynchronous Balanced mode - akin to HDLC) and ARM (Asynchronous Response mode)

</doc>
<doc id="40701" url="https://en.wikipedia.org/wiki?curid=40701" title="Aerial insert">
Aerial insert

In telecommunications an aerial insert is a segment of cabling that rises from ground to a point above ground, followed by an overhead run, e.g. on poles, followed by a drop back into the ground. An aerial insert is used in places where it is not possible or practical to place a cable underground. Aerial inserts might be encountered in crossing deep ditches, canals, rivers, or subway lines.

</doc>
<doc id="40702" url="https://en.wikipedia.org/wiki?curid=40702" title="Aeronautical Emergency Communications System Plan">
Aeronautical Emergency Communications System Plan

In telecommunication, the Aeronautical Emergency Communications System Plan (AECS) provides for the operation of aeronautical communications stations, on a voluntary, organized basis, to provide the President and the Federal Government, as well as heads of state and local governments, or their designated representatives, and the aeronautical industry with an expeditious means of communications during an emergency.

</doc>
<doc id="40703" url="https://en.wikipedia.org/wiki?curid=40703" title="AIOD leads">
AIOD leads

In land-line telephony, AIOD leads are Terminal equipment leads used solely to transmit automatic identified outward dialing (AIOD) data from a PBX to the public switched telephone network or to switched service networks ("e.g.," EPSCS), so that a vendor can provide a detailed monthly bill identifying long distance calling usage by individual PBX stations, tie trunks, or the attendant console. It resembles common channel signalling in that the AIOD leads provide data for all trunks, but is used only for billing, thus resembling automatic number identification.

</doc>
<doc id="40704" url="https://en.wikipedia.org/wiki?curid=40704" title="Airborne radio relay">
Airborne radio relay

Airborne radio relay is a technique employing aircraft fitted with radio relay stations for the purpose of increasing the range, flexibility, or physical security of communications systems.
Use in Vietnam.
One of the first uses of airborne radio relay was by the United States Army's 1st Cavalry Division in the Battle of Ia Drang during the Vietnam War, which employed the technique to improve communications with commanders at headquarters. The action of war had shifted to the borders of Laos and Cambodia, where the hilly terrain made the monetary and human cost of seizing and holding high ground, and airlifting and installing radio relay equipment prohibitive. In 1968, the Department of the Army provided four specially-equipped relay aircraft to the Division, which proved invaluable throughout the country, in particular, during the 1st Cavalry Division's relief of Khe Sanh in 1968.
The use of airborne radio relay was a great success, although two problems arose during the Vietnam War. The first was the limitations of the aircraft used as relays. The 1st Cavalry Division had originally used C-7 Caribous as the relay aircraft, but when these planes were turned over to the Air Force, the equipment was installed in single-engine Otter aircraft, which were too underpowered to carry the heavy equipment required for relay. Eventually, the 1st Signal Brigade was provided with six specially-equipped U-21 aircraft for use in relay operations. The second problem was that of radio frequency interference: the limited frequency spectrum in use for combat radios meant that relay aircraft often interfered with the communication of ground units when their frequencies were overridden by the airborne units. The Army eventually assigned certain frequencies for airborne relay only, although this further limited the frequencies available to ground units.

</doc>
<doc id="40706" url="https://en.wikipedia.org/wiki?curid=40706" title="Alarm sensor">
Alarm sensor

In telecommunication, the term alarm sensor has the following meanings: 
1. In communications systems, a device that can sense an abnormal condition within the system and provide a signal indicating the presence or nature of the abnormality to either a local or remote alarm indicator, and (b) may detect events ranging from a simple contact opening or closure to a time-phased automatic shutdown and restart cycle. 
2. In a physical security system, an approved device used to indicate a change in the physical environment of a facility or a part thereof. 
3. In electronic security systems, a physical device or change/presence of any electronic signal/logic which causes trigger to electronic circuit to perform application specific operation. In electronic alarm systems the use of this trigger event done by such devices is to turn on the alarm or siren producing sound and/or perform a security calling through telephone lines.
"Note:" Alarm sensors may also be redundant or chained, such as when one alarm sensor is used to protect the housing, cabling, or power protected by another alarm sensor.
Source: from Federal Standard 1037C and from MIL-STD-188 and from TRISHAM Software Systems

</doc>
<doc id="40707" url="https://en.wikipedia.org/wiki?curid=40707" title="A-law algorithm">
A-law algorithm

An A-law algorithm is a standard companding algorithm, used in European 8-bit PCM digital communications systems to optimize, "i.e.," modify, the dynamic range of an analog signal for digitizing. It is one of two versions of the G.711 standard from ITU-T, the other version being the similar µ-law, used in North America and Japan.
For a given input "x", the equation for A-law encoding is as follows,
where "A" is the compression parameter. In Europe, formula_2.
A-law expansion is given by the inverse function,
The reason for this encoding is that the wide dynamic range of speech does not lend itself well to efficient linear digital encoding. A-law encoding effectively reduces the dynamic range of the signal, thereby increasing the coding efficiency and resulting in a signal-to-distortion ratio that is superior to that obtained by linear encoding for a given number of bits.
Comparison to μ-law.
The μ-law algorithm provides a greater larger dynamic range than the A-law at the cost of worse proportional distortion for small signals. By convention, A-law is used for an international connection if at least one country uses it.

</doc>
<doc id="40708" url="https://en.wikipedia.org/wiki?curid=40708" title="Allan variance">
Allan variance

The Allan variance (AVAR), also known as two-sample variance, is a measure of frequency stability in clocks, oscillators and amplifiers. It is named after David W. Allan. It is expressed mathematically as
The Allan deviation (ADEV) is the square root of Allan variance. It is also known as "sigma-tau", and is expressed mathematically as
The "M-sample variance" is a measure of frequency stability using M samples, time T between measures and observation time formula_3. "M"-sample variance is expressed as
The "Allan variance" is intended to estimate stability due to noise processes and not that of systematic errors or imperfections such as frequency drift or temperature effects. The Allan variance and Allan deviation describe frequency stability, i.e. the stability in frequency. See also the section entitled "Interpretation of value" below.
There are also different adaptations or alterations of "Allan variance", notably the modified Allan variance MAVAR or MVAR, the total variance, and the Hadamard variance. There also exist time stability variants such as time deviation TDEV or time variance TVAR. Allan variance and its variants have proven useful outside the scope of timekeeping and are a set of improved statistical tools to use whenever the noise processes are not unconditionally stable, thus a derivative exists.
The general "M"-sample variance remains important since it allows dead time in measurements and bias functions allows conversion into Allan variance values. Nevertheless, for most applications the special case of 2-sample, or "Allan variance" with formula_5 is of greatest interest.
Background.
When investigating the stability of crystal oscillators and atomic clocks it was found that they did not have a phase noise consisting only of white noise, but also of white frequency noise and flicker frequency noise. These noise forms become a challenge for traditional statistical tools such as standard deviation as the estimator will not converge. The noise is thus said to be divergent. Early efforts in analysing the stability included both theoretical analysis and practical measurements.
An important side-consequence of having these types of noise was that, since the various methods of measurements did not agree with each other, the key aspect of repeatability of a measurement could not be achieved. This limits the possibility to compare sources and make meaningful specifications to require from suppliers. Essentially all forms of scientific and commercial uses were then limited to dedicated measurements which hopefully would capture the need for that application.
To address these problems, David Allan introduced the M-sample variance and (indirectly) the two-sample variance. While the two-sample variance did not completely allow all types of noise to be distinguished, it provided a means to meaningfully separate many noise-forms for time-series of phase or frequency measurements between two or more oscillators. Allan provided a method to convert between any M-sample variance to any N-sample variance via the common 2-sample variance, thus making all M-sample variances comparable. The conversion mechanism also proved that M-sample variance does not converge for large M, thus making them less useful. IEEE later identified the 2-sample variance as the preferred measure.
An early concern was related to time and frequency measurement instruments which had a dead time between measurements. Such a series of measurements did not form a continuous observation of the signal and thus introduced a systematic bias into the measurement. Great care was spent in estimating these biases. The introduction of zero dead time counters removed the need, but the bias analysis tools have proved useful.
Another early aspect of concern was related to how the bandwidth of the measurement instrument would influence the measurement, such that it needed to be noted. It was later found that by algorithmically changing the observation formula_3, only low formula_3 values would be affected while higher values would be unaffected. The change of formula_3 is done by letting it be an integer multiple formula_9 of the measurement timebase formula_10.
The physics of crystal oscillators was analyzed by D. B. Leeson and the result is now referred to as Leeson's equation. The feedback in the oscillator will make the white noise and flicker noise of the feedback amplifier and crystal become the power-law noises of formula_12 white frequency noise and formula_13 flicker frequency noise respectively. These noise forms have the effect that the standard variance estimator does not converge when processing time error samples. This mechanics of the feedback oscillators was unknown when the work on oscillator stability started but was presented by Leeson at the same time as the statistical tools was made available by David W. Allan. For a more thorough presentation on the Leeson effect see modern phase noise literature.
Interpretation of value.
Allan variance is defined as one half of the time average of the squares of the differences between successive readings of the frequency deviation sampled over the sampling period. The Allan variance depends on the time period used between samples: therefore it is a function of the sample period, commonly denoted as τ, likewise the distribution being measured, and is displayed as a graph rather than a single number. A low Allan variance is a characteristic of a clock with good stability over the measured period.
Allan deviation is widely used for plots (conveniently in log-log format) and presentation of numbers. It is preferred as it gives the relative amplitude stability, allowing ease of comparison with other sources of errors.
An Allan deviation of 1.3×10−9 at observation time 1 s (i.e. τ = 1 s) should be interpreted as there being an instability in frequency between two observations a second apart with a relative root mean square (RMS) value of 1.3×10−9. For a 10-MHz clock, this would be equivalent to 13 mHz RMS movement. If the phase stability of an oscillator is needed then the time deviation variants should be consulted and used.
One may convert the Allan variance and other time-domain variances into frequency-domain measures of time (phase) and frequency stability. The following link shows these relationships and how to perform these conversions:
http://www.allanstime.com/Publications/DWA/Conversion_from_Allan_variance_to_Spectral_Densities.pdf
Definitions.
M-sample variance.
The formula_14-sample variance is defined (here in a modernized notation form) as
where formula_16 is the phase angle (in radians) measured at time formula_17, or with average fractional frequency time series
where formula_14 is the number of frequency samples used in variance, formula_20 is the time between each frequency sample and formula_3 is the time-length of each frequency estimate.
An important aspect is that formula_14-sample variance model can include dead-time by letting the time formula_20 be different from that of formula_3.
Allan variance.
The Allan variance is defined as
which is conveniently expressed as
where formula_3 is the observation period, formula_28 is the "n"th fractional frequency average over the observation time formula_3.
The samples are taken with no dead-time between them, which is achieved by letting
Allan deviation.
Just as with standard deviation and variance, the Allan deviation is defined as the square root of the Allan variance.
Supporting definitions.
Oscillator model.
The oscillator being analysed is assumed to follow the basic model of
The oscillator is assumed to have a nominal frequency of formula_33, given in cycles per second (SI unit: hertz). The nominal angular frequency formula_34 (in radians per second) is given by
The total phase can be separated into a perfectly cyclic component formula_36, along with a fluctuating component formula_37:
Time error.
The time error function "x"("t") is the difference between expected nominal time and actual normal time
For measured values a time error series TE("t") is defined from the reference time function "T"REF("t") as
Frequency function.
The frequency function formula_41 is the frequency over time defined as
Fractional frequency.
The fractional frequency "y"("t") is the normalized difference between the frequency formula_41 and the nominal frequency formula_33:
Average fractional frequency.
The average fractional frequency is defined as
where the average is taken over observation time "τ", the "y"("t") is the fractional frequency error at time "t" and "τ" is the observation time.
Since "y"("t") is the derivative of "x"("t"), we can without loss of generality rewrite it as
Estimators.
The definition is based on the statistical expected value, integrating over infinite time. Real world situation does not allow for such time-series, in which case a statistical estimator needs to be used in its place. A number of different estimators will be presented and discussed.
Conventions.
The relation between the number of fractional frequency samples and time error series is fixed in the relationship
where "T" is the time between measurements. For Allan variance, the time being used has "T" set to the observation time "τ".
The time error sample series let "N" denote the number of samples ("x"0 ..."x""N-1") in the series. The traditional convention uses index 1 through "N".
which gives
For the Allan variance assumption of "T" being "τ" it becomes
The average fractional frequency sample series let "M" denote the number of samples (formula_54) in the series. The traditional convention uses index 1 through "M".
As a shorthand average fractional frequency is often written without the average bar over it. This is however formally incorrect as the fractional frequency and average fractional frequency is two different functions. A measurement instrument able to produce frequency estimates with no dead-time will actually deliver a frequency average time series which only needs to be converted into average fractional frequency and may then be used directly.
Fixed τ estimators.
A first simple estimator would be to directly translate the definition into
or for the time series
These formulas however only provide the calculation for the "τ" = "τ"0 case. To calculate for a different value of "τ", a new time-series needs to be provided.
Non-overlapped variable τ estimators.
If taking the time-series and skipping past "n" − 1 samples a new (shorter) time-series would occur with "τ"0 as the time between the adjacent samples, for which the Allan variance could be calculated with the simple estimators. These could be modified to introduce the new variable "n" such that no new time-series would have to be generated, but rather the original time series could be reused for various values of "n". The estimators become
with formula_58,
and for the time series
with formula_60.
These estimators have a significant drawback in that they will drop a significant amount of sample data as only 1/"n" of the available samples is being used.
Overlapped variable τ estimators.
A technique presented by J.J. Snyder provided an improved tool, as measurements was overlapped in "n" overlapped series out of the original series. The overlapping Allan variance estimator was introduced in. This can be shown to be equivalent to averaging the time or normalized frequency samples in blocks of "n" samples prior to processing. The resulting predictors becomes
or for the time series
The overlapping estimators have far superior performance over the non-overlapping estimators as "n" rises and the time-series is of moderate length. The overlapped estimators have been accepted as the preferred Allan variance estimators in IEEE, ITU-T and ETSI standards for comparable measurements such as needed for telecommunication qualification.
Modified Allan variance.
In order to address the inability to separate white phase modulation from flicker phase modulation using traditional Allan variance estimators an algorithmic filtering to reduce the bandwidth by "n". This filtering provides a modification to the definition and estimators and is now identifies as a separate class of variance called modified Allan variance. The modified Allan variance measure is a frequency stability measure, just as the Allan variance.
Time stability estimators.
The Allan variance and Allan deviation provides the frequency stability variance and deviation. The time stability can be calculated from the Allan variance
and similarly for Allan deviation to time deviation
Other estimators.
Further developments have produced improved estimation methods for the same stability measure, the variance/deviation of frequency, but these are known by separate names such as the Hadamard variance, modified Hadamard variance, the total variance, modified total variance and the Theo variance. These distinguish themselves in better use of statistics for improved confidence bounds or ability to handle linear frequency drift.
Confidence intervals and equivalent degrees of freedom.
Statistical estimators will calculate an estimated value on the sample series used. The estimates may deviate from the true value and the range of values which for some probability will contain the true value is referred to as the confidence interval. The confidence interval depends on the number of observations in the sample series, the dominant noise type, and the estimator being used. The width is also dependent on the statistical certainty for which the confidence interval values forms a bounded range, thus the statistical certainty that the true value is within that range of values. For variable-τ estimators, the "τ"0 multiple "n" is also a variable.
Confidence interval.
The confidence interval can be established using chi-squared distribution by using the distribution of the sample variance:
where "s""2" is the sample variance of our estimate, "σ"2 is the true variance value, "d.f." is the degrees of freedom for the estimator and "χ"2 is the degrees of freedom for a certain probability. For a 90% probability, covering the range from the 5% to the 95% range on the probability curve, the upper and lower limits can be found using the inequality:
which after rearrangement for the true variance becomes:
Effective degrees of freedom.
The degrees of freedom represents the number of free variables capable of contributing to the estimate. Depending on the estimator and noise type, the effective degrees of freedom varies. Estimator formulas depending on "N" and "n" has been empirically found to be:
Power-law noise.
The Allan variance will treat various power-law noise types differently, conveniently allowing them to be identified and their strength estimated. As a convention, the measurement system width (high corner frequency) is denoted "f""H".
As found in and in modern forms.
The Allan variance is unable to distinguish between WPM and FPM, but is able to resolve the other power-law noise types. In order to distinguish WPM and FPM, the modified Allan variance needs to be employed.
The above formulas assume that
and thus that the bandwidth of the observation time is much lower than the instruments bandwidth. When this condition is not met, all noise forms depend on the instrument's bandwidth.
α-μ mapping.
The detailed mapping of a phase modulation of the form
where
or frequency modulation of the form
into the Allan variance of the form
can be significantly simplified by providing a mapping between α and μ. A mapping between α and "K"α is also presented for convenience:
The mapping is taken from.
General Conversion from Phase Noise.
A signal with spectral phase noise formula_73 with units rad2/Hz can be converted to Allan Variance by:
formula_74
Linear response.
While Allan variance is intended to be used to distinguish noise forms, it will depend on some but not all linear responses to time. They are given in the table:
Thus, linear drift will contribute to output result. When measuring a real system, the linear drift or other drift mechanism may need to be estimated and removed from the time-series prior to calculating the Allan variance.
Time and frequency filter properties.
In analysing the properties of Allan variance and friends, it has proven useful to consider the filter properties on the normalize frequency. Starting with the definition for Allan variance for
where
Replacing the time series of formula_77 with the Fourier transformed variant formula_78 the Allan variance can be expressed in the frequency domain as
Thus the transfer function for Allan variance is
Bias functions.
The "M"-sample variance, and the defined special case Allan variance, will experience systematic bias depending on different number of samples "M" and different relationship between "T" and "τ". In order address these biases the bias-functions "B"1 and "B"2 has been defined and allows for conversion between different "M" and "T" values.
These bias functions are not sufficient for handling the bias resulting from concatenating "M" samples to the "Mτ"0 observation time over the "MT"0 with has the dead-time distributed among the "M" measurement blocks rather than at the end of the measurement. This rendered the need for the "B"3 bias.
The bias functions are evaluated for a particular µ value, so the α-µ mapping needs to be done for the dominant noise form as found using noise identification. Alternatively as proposed in and elaborated in the µ value of the dominant noise form may be inferred from the measurements using the bias functions.
B1 bias function.
The "B"1 bias function relates the "M"-sample variance with the 2-sample variance (Allan variance), keeping the time between measurements "T" and time for each measurements "τ" constant, and is defined as
where
The bias function becomes after analysis
B2 bias function.
The "B"2 bias function relates the 2-sample variance for sample time "T" with the 2-sample variance (Allan variance), keeping the number of samples "N" = 2 and the observation time "τ" constant, and is defined
where
The bias function becomes after analysis
"B"3 bias function.
The "B"3 bias function relates the 2-sample variance for sample time "MT"0 and observation time "Mτ"0 with the 2-sample variance (Allan variance) and is defined as
where
The "B"3 bias function is useful to adjust non-overlapping and overlapping variable "τ" estimator values based on dead-time measurements of observation time "τ"0 and time between observations "T"0 to normal dead-time estimates.
The bias function becomes after analysis (for the "N" = 2 case)
where
τ bias function.
While formally not formulated, it has been indirectly inferred as a consequence of the α-µ mapping. When comparing two Allan variance measure for different τ assuming same dominant noise in the form of same µ coefficient, a bias can be defined as
The bias function becomes after analysis
Conversion between values.
In order to convert from one set of measurements to another the "B"1, "B"2 and τ bias functions can be assembled. First the "B"1 function converts the ("N"1, "T"1, "τ"1) value into (2, "T"1, "τ"1), from which the "B"2 function converts into a (2, "τ"1, "τ"1) value, thus the Allan variance at "τ"1. The Allan variance measure can be converted using the τ bias function from "τ"1 to "τ"2, from which then the (2, "T"2, "τ"2) using "B"2 and then finally using "B"1 into the ("N"2, "T"2, "τ"2) variance. The complete conversion becomes
where
Similarly, for concatenated measurements using M sections, the logical extension becomes
Measurement issues.
When making measurements to calculate Allan variance or Allan deviation a number of issues may cause the measurements to degenerate. Covered here is the effects specific to Allan variance, where results would be biased.
Measurement bandwidth limits.
A measurement system is expected to have a bandwidth at or below that of the Nyquist rate as described within the Shannon–Hartley theorem. As can be seen in the power-law noise formulas, the white and flicker noise modulations both depends on the upper corner frequency formula_98 (these systems is assumed to be low-pass filtered only). Considering the frequency filter property it can be clearly seen that low-frequency noise has greater impact on the result. For relatively flat phase modulation noise types (e.g. WPM and FPM), the filtering has relevance, whereas for noise types with greater slope the upper frequency limit becomes of less importance, assuming that the measurement system bandwidth is wide relative the formula_3 as given by
When this assumption is not met, the effective bandwidth formula_98 needs to be notated alongside the measurement. The interested should consult NBS TN394.
If however one adjust the bandwidth of the estimator by using integer multiples of the sample time formula_102 then the system bandwidth impact can be reduced to insignificant levels. For telecommunication needs, such methods have been required in order to ensure comparability of measurements and allow some freedom for vendors to do different implementations. The ITU-T Rec. G.813 for the TDEV measurement.
It can be recommended that the first formula_10 multiples be ignored such that the majority of the detected noise is well within the passband of the measurement systems bandwidth.
Further developments on the Allan variance was performed to let the hardware bandwidth be reduced by software means. This development of a software bandwidth allowed for addressing the remaining noise and the method is now referred to modified Allan variance. This bandwidth reduction technique should not be confused with the enhanced variant of modified Allan variance which also changes a smoothing filter bandwidth.
Dead time in measurements.
Many measurement instruments of time and frequency have the stages of arming time, time-base time, processing time and may then re-trigger the arming. The arming time is from the time the arming is triggered to when the start event occurs on the start channel. The time-base then ensures that minimum amount of time goes prior to accepting an event on the stop channel as the stop event. The number of events and time elapsed between the start event and stop event is recorded and presented during the processing time. When the processing occurs (also known as the dwell time) the instrument is usually unable to do another measurement. After the processing has occurred, an instrument in continuous mode triggers the arm circuit again. The time between the stop event and the following start event becomes dead time during which the signal is not being observed. Such dead time introduces systematic measurement biases, which needs to be compensated for in order to get proper results. For such measurement systems will the time "T" denote the time between the adjacent start events (and thus measurements) while formula_3 denote the time-base length, i.e. the nominal length between the start and stop event of any measurement.
Dead time effects on measurements have such an impact on the produced result that much study of the field have been done in order to quantify its properties properly. The introduction of zero dead-time counters removed the need for this analysis. A zero dead-time counter has the property that the stop-event of one measurement is also being used as the start-event of the following event. Such counters creates a series of event and time timestamp pairs, one for each channel spaced by the time-base. Such measurements have also proved useful in order forms of time-series analysis.
Measurements being performed with dead time can be corrected using the bias function "B"1, "B"2 and "B"3. Thus, dead time as such is not prohibiting the access to the Allan variance, but it makes it more problematic. The dead time must be known such that the time between samples "T" can be established.
Measurement length and effective use of samples.
Studying the effect on the confidence intervals that the length "N" of the sample series have, and the effect of the variable τ parameter "n" the confidence intervals may become very large since the effective degree of freedom may become small for some combination of "N" and "n" for the dominant noise-form (for that τ).
The effect may be that the estimated value may be much smaller or much greater than the real value, which may lead to false conclusions of the result.
It is recommended that the confidence interval is plotted along with the data, such that the reader of the plot is able to be aware of the statistical uncertainty of the values.
It is recommended that the length of the sample sequence, i.e. the number of samples "N" is kept high to ensure that confidence interval is small over the τ-range of interest.
It is recommended that the τ-range as swept by the "τ"0 multiplier "n" is limited in the upper end relative "N" such that the read of the plot is not being confused by highly unstable estimator values.
It is recommended that estimators providing better degrees of freedom values be used in replacement of the Allan variance estimators or as complementing them where they outperform the Allan variance estimators. Among those the Total variance and Theo variance estimators should be considered.
Dominant noise type.
A large number of conversion constants, bias corrections and confidence intervals depends on the dominant noise type. For proper interpretation shall the dominant noise type for the particular τ of interest be identified through noise identification. Failing to identify the dominant noise type will produce biased values. Some of these biases may be of several order of magnitude, so it may be of large significance.
Linear drift.
Systematic effects on the signal is only partly cancelled. Phase and frequency offset is cancelled, but linear drift or other high degree forms of polynomial phase curves will not be cancelled and thus form a measurement limitation. Curve fitting and removal of systematic offset could be employed. Often removal of linear drift can be sufficient. Use of linear drift estimators such as the Hadamard variance could also be employed. A linear drift removal could be employed using a moment based estimator.
Measurement instrument estimator bias.
Traditional instruments provided only the measurement of single events or event pairs. The introduction of the improved statistical tool of overlapping measurements by J.J. Snyder allowed for much improved resolution in frequency readouts, breaking the traditional digits/time-base balance. While such methods is useful for their intended purpose, using such smoothed measurements for Allan variance calculations would give a false impression of high resolution, but for longer τ the effect is gradually removed and the lower τ region of the measurement has biased values. This bias is providing lower values than it should, so it is an overoptimistic (assuming that low numbers is what one wishes) bias reducing the usability of the measurement rather than improving it. Such smart algorithms can usually be disabled or otherwise circumvented by using time-stamp mode which is much preferred if available.
Practical measurements.
While several approaches to measurement of Allan variance can be devised, a simple example may illustrate how measurements can be performed.
Measurement.
All measurements of Allan variance will in effect be the comparison of two different clocks. Lets consider a reference clock and a device under test (DUT), and both having a common nominal frequency of 10 MHz. A time-interval counter is being used to measure the time between the rising edge of the reference (channel A) and the rising edge of the device under test.
In order to provide evenly spaced measurements will the reference clock be divided down to form the measurement rate, triggering the time-interval counter (ARM input). This rate can be 1 Hz (using the 1 PPS output of a reference clock) but other rates like 10 Hz and 100 Hz can also be used. The speed of which the time-interval counter can complete the measurement, output the result and prepare itself for the next arm will limit the trigger frequency.
A computer is then useful to record the series of time-differences being observed.
Post-processing.
The recorded time-series require post-processing to unwrap the wrapped phase, such that a continuous phase error is being provided. If necessary should also logging and measurement mistakes be fixed. Drift estimation and drift removal should be performed, the drift mechanism needs to be identified and understood for the sources. Drift limitations in measurements can be severe, so letting the oscillators become stabilized by long enough time being powered on is necessary.
The Allan variance can then be calculated using the estimators given, and for practical purposes the overlapping estimator should be used due to its superior use of data over the non-overlapping estimator. Other estimators such as Total or Theo variance estimators could also be used if bias corrections is applied such that they provide Allan variance compatible results.
To form the classical plots, the Allan deviation (square root of Allan variance) is plotted in log-log format against the observation interval tau.
Equipment and software.
The time-interval counter is typically an off the shelf counter commercially available. Limiting factors involve single-shot resolution, trigger jitter, speed of measurements and stability of reference clock. The computer collection and post-processing can be done using existing commercial or public domain software. Highly advanced solutions exists which will provide measurement and computation in one box.
Research history.
The field of frequency stability has been studied for a long time, however it was found during the 1960s that there was a lack of coherent definitions. The NASA-IEEE Symposium on Short-Term Stability in 1964 was followed with the IEEE Proceedings publishing a special issue on Frequency Stability in its February 1966 issue.
The NASA-IEEE Symposium on Short-Term Stability in November 1964 brought together many fields and uses of short and long term stability with papers from many different contributors. The articles and panel discussions is interesting in that they concur on the existence of the frequency flicker noise and the wish to achieve a common definition for short and long term stability (even if the conference name only reflect the short-term stability intention).
The IEEE proceedings on Frequency Stability 1966 included a number of important papers including those of David Allan, James A. Barnes, L. S. Cutler and C. L. Searle and D. B. Leeson. These papers helped shape the field.
The classical "M"-sample variance of frequency was analysed by David Allan in along with an initial bias function. This paper tackles the issues of dead-time between measurements and analyses the case of M frequency samples (called "N" in the paper) and variance estimators. It provides the now standard "α" to "µ" mapping. It clearly builds on James Barnes work as detailed in his article in the same issue. The initial bias functions introduced assumes no dead-time, but the formulas presented includes dead-time calculations. The bias function assumes the use of the 2-sample variance as a base-case, since any other variants of "M" may be chosen and values may be transferred via the 2-sample variance to any other variance for of arbitrary "M". Thus, the 2-sample variance was only implicitly used and not clearly stated as the preference even if the tools where provided. It however laid the foundation for using the 2-sample variance as the base case of comparison among other variants of the "M"-sample variance. The 2-sample variance case is a special case of the "M"-sample variance which produces an average of the frequency derivative.
The work on bias functions was significantly extended by James Barnes in in which the modern B1 and B2 bias functions was introduced. Curiously enough, it refers to the "M"-sample variance as "Allan variance" while referencing to Allan's paper "Statistics of Atomic Frequency Standards". With these modern bias functions, full conversion among "M"-sample variance measures of variating "M", "T" and τ values could used, by conversion through the 2-sample variance.
James Barnes and David Allan further extended the bias functions with the B3 function in to handle the concatenated samples estimator bias. This was necessary to handle the new use of concatenated sample observations with dead time in between.
The IEEE Technical Committee on Frequency and Time within the IEEE Group on Instrumentation & Measurements provided a summary of the field in 1970 published as NBS Technical Notice 394. This paper could be considered first in a line of more educational and practical papers aiding the fellow engineers in grasping the field. In this paper the 2-sample variance with "T" = "τ" is being the recommended measurement and it is referred to as Allan variance (now without the quotes). The choice of such parametrisation allows good handling of some noise forms and to get comparable measurements, it is essentially the least common denominator with the aid of the bias functions B1 and B2.
An improved method for using sample statistics for frequency counters in frequency estimation or variance estimation was proposed by J.J. Snyder. The trick to get more effective degrees of freedom out of the available dataset was to use overlapping observation periods. This provides a square-root "n" improvement. It was included into the overlapping Allan variance estimator introduced in. The variable τ software processing was also included in. This development improved the classical Allan variance estimators likewise providing a direct inspiration going into the work on modified Allan variance.
The confidence interval and degrees of freedom analysis, along with the established estimators was presented in.
Educational and practical resources.
The field of time and frequency and its use of Allan variance, Allan deviation and friends is a field involving many aspects, for which both understanding of concepts and practical measurements and post-processing requires care and understanding. Thus, there is a realm of educational material stretching some 40 years available. Since these reflect the developments in the research of their time, they focus on teaching different aspect over time, in which case a survey of available resources may be a suitable way of finding the right resource.
The first meaningful summary is the NBS Technical Note 394 "Characterization of Frequency Stability". This is the product of the Technical Committee on Frequency and Time of the IEEE Group on Instrumentation & Measurement. It gives the first overview of the field, stating the problems, defining the basic supporting definitions and getting into Allan variance, the bias functions "B"1 and "B"2, the conversion of time-domain measures. This is useful as it is among the first references to tabulate the Allan variance for the five basic noise types.
A classical reference is the NBS Monograph 140 from 1974, which in chapter 8 has "Statistics of Time and Frequency Data Analysis". This is the extended variant of NBS Technical Note 394 and adds essentially in measurement techniques and practical processing of values.
An important addition will be the "Properties of signal sources and measurement methods". It covers the effective use of data, confidence intervals, effective degree of freedom likewise introducing the overlapping Allan variance estimator. It is a highly recommended reading for those topics.
The IEEE standard 1139 "Standard definitions of Physical Quantities for Fundamental Frequency and Time Metrology" is beyond that of a standard a comprehensive reference and educational resource.
A modern book aimed towards telecommunication is Stefano Bregni "Synchronisation of Digital Telecommunication Networks". This summarises not only the field but also much of his research in the field up to that point. It aims to include both classical measures likewise telecommunication specific measures such as MTIE. It is a handy companion when looking at telecommunication standard related measurements.
The NIST Special Publication 1065 "Handbook of Frequency Stability Analysis" of W.J. Riley is a recommended reading for anyone wanting to pursue the field. It is rich of references and also covers a wide range of measures, biases and related functions that a modern analyst should have available. Further it describes the overall processing needed for a modern tool.
Uses.
Allan variance is used as a measure of frequency stability in a variety of precision oscillators, such as crystal oscillators, atomic clocks and frequency-stabilized lasers over a period of a second or more. Short term stability (under a second) is typically expressed as phase noise. The Allan variance is also used to characterize the bias stability of gyroscopes, including fiber optic gyroscopes and MEMS gyroscopes.
50th Anniversary.
In 2016, IEEE-UFFC is going to be publishing a "Special Issue to celebrate the 50th anniversary of the Allan Variance (1966-2016)". A guest editor for that issue will be David's former colleague at NIST, Judah Levine, who is the most recent recipient of the I. I. Rabi Award.

</doc>
<doc id="40711" url="https://en.wikipedia.org/wiki?curid=40711" title="Alternate party">
Alternate party

Alternate party diversion is an optional feature of telephone services, whereby a call may be routed to a different number based on time-out and precedence schemes set up by the customer.
Technical definition.
Alternate party: In multilevel precedence and preemption, the call receiver, i.e. the destination user, to which a precedence call will be diverted. Diversion will occur when the response timer expires, when the call receiver is busy on a call of equal or higher precedence, or when the call receiver is busy with access resources that are non-preemptable. Alternate party diversion is an optional terminating feature that is subscribed to by the call receiver. Thus, the alternate party is specified by the call receiver at the time of subscription. 
Source: From Federal Standard 1037C and from MIL-STD-188

</doc>
<doc id="40712" url="https://en.wikipedia.org/wiki?curid=40712" title="Ambient noise level">
Ambient noise level

In atmospheric sounding and noise pollution, ambient noise level (sometimes called background noise level, reference sound level, or room noise level) is the background sound pressure level at a given location, normally specified as a reference level to study a new intrusive sound source.
Ambient sound levels are often measured in order to map sound conditions over a spatial regime to understand their variation with locale. In this case the product of the investigation is a sound level contour map. Alternatively ambient noise levels may be measured to provide a reference point for analyzing an intrusive sound to a given environment. For example, sometimes aircraft noise is studied by measuring ambient sound without presence of any overflights, and then studying the noise addition by measurement or computer simulation of overflight events. Or roadway noise is measured as ambient sound, prior to introducing a hypothetical noise barrier intended to reduce that ambient noise level. 
Ambient noise level is measured with a sound level meter. It is usually measured in dB relative to a reference pressure of 0.00002 Pa, "i.e.," 20 μPa (micropascals) in SI units. A pascal is a newton per square meter. The centimeter-gram-second system of units, the reference sound pressure for measuring ambient noise level is 0.0002 dyn/cm2. Most frequently ambient noise levels are measured using a frequency weighting filter, the most common being the A-weighting scale, such that resulting measurements are denoted dB(A), or decibels on the A-weighting scale.

</doc>
<doc id="40713" url="https://en.wikipedia.org/wiki?curid=40713" title="Amplitude distortion">
Amplitude distortion

Amplitude distortion is distortion occurring in a system, subsystem, or device when the output amplitude is not a linear function of the input amplitude under specified conditions. 
Generally, output is a linear function of input only for a fixed portion of the transfer characteristics. In this region, Ic=βIb where Ic is collector current and Ib is base current, following linear relation y=mx.
When output is not in this portion, two forms of amplitude distortion might arise
Due to the additional outputs, this form of distortion is definitely unwanted in audio, radio and telecommunication amplifiers, and it occurs for more than two waves as well.
In a narrowband system such as a radio communication system, unwanted outputs such as X-Y and 2X+Y will be remote from the wanted band and so be ignored by the system. In contrast, 2X-Y and 2Y-X will be close to the wanted signals. These so-called third order distortion products (third order as m+n = 3) tend to dominante the non-linear distortion of narrowband systems.
Amplitude distortion is measured with the system operating under steady-state conditions with a sinusoidal input signal. When other frequencies are present, the term ""amplitude"" refers to that of the fundamental only.

</doc>
<doc id="40716" url="https://en.wikipedia.org/wiki?curid=40716" title="Angular misalignment loss">
Angular misalignment loss

In waveguide design and construction, angular misalignment loss is power loss caused by the deviation from optimum angular alignment of the axes of source-to-waveguide, waveguide-to-waveguide, or waveguide-to-detector. The waveguide may be dielectric (an optical fiber) or metallic. Angular misalignment loss does not include lateral offset loss and longitudinal offset loss.
Source: from Federal Standard 1037C

</doc>
<doc id="40717" url="https://en.wikipedia.org/wiki?curid=40717" title="Antenna blind cone">
Antenna blind cone

In telecommunications, antenna blind cone (sometimes called a cone of silence or antenna blind spot) is the volume of space, usually approximately conical with its vertex at the antenna, that cannot be scanned by an antenna because of limitations of the antenna radiation pattern and mount.
"Note:" An example of an antenna blind cone is that of an Air Route Surveillance Radar (ARSR). The horizontal radiation pattern of an ARSR antenna is very narrow. The vertical radiation pattern is fan-shaped, reaching approximately 70° of elevation above the horizontal plane. As the fan antenna is rotated about a vertical axis, it can illuminate targets only if they are 70° or less from the horizontal plane. Above that elevation, they are in the antenna blind cone. 

</doc>
<doc id="40719" url="https://en.wikipedia.org/wiki?curid=40719" title="Antenna height above average terrain">
Antenna height above average terrain

In United States telecommunication terminology, antenna height above average terrain is the antenna height above the average terrain elevations from from the antenna for the eight directions spaced evenly for each 45° of azimuth starting with true north.
In general, a different antenna height above average terrain will be determined in each direction from the antenna. The average of these eight heights is the antenna height above average terrain. In some cases, such as seashore, fewer than eight directions may be used.
Source: from Federal Standard 1037C and from MIL-STD-188

</doc>
<doc id="40720" url="https://en.wikipedia.org/wiki?curid=40720" title="Antenna noise temperature">
Antenna noise temperature

In telecommunication, antenna noise temperature is the temperature of a hypothetical resistor at the input of an ideal noise-free receiver that would generate the same output noise power per unit bandwidth as that at the antenna output at a specified frequency. In other words, antenna noise temperature is a parameter that describes how much noise an antenna produces in a given environment. This temperature is not the physical temperature of the antenna. Moreover, an antenna does not have an intrinsic "antenna temperature" associated with it; rather the temperature depends on its gain pattern and the thermal environment that it is placed in.
Antenna noise temperature has contributions from several sources:
Galactic noise is high below 1000 MHz. At around 150 MHz, it is approximately 1000K. At 2500 MHz, it has leveled off to around 10K.
Earth has an accepted standard temperature of 290K.
The level of the sun's contribution depends on the solar flux. It is given by

</doc>
<doc id="40721" url="https://en.wikipedia.org/wiki?curid=40721" title="Aperture-to-medium coupling loss">
Aperture-to-medium coupling loss

In telecommunication, aperture-to-medium coupling loss is the difference between the theoretical antenna gain of a very large antenna, such as the antennas in beyond-the-horizon microwave links, and the gain that can be realized in practice. 
"Note 1:" Aperture-to-medium coupling loss is related to the ratio of the scatter angle to the antenna beamwidth. 
"Note 2:" The "very large antennas" are referred to in wavelengths; thus, this loss can apply to line-of-sight systems also. 

</doc>
<doc id="40723" url="https://en.wikipedia.org/wiki?curid=40723" title="Area broadcast shift">
Area broadcast shift

In telecommunication, an area broadcast shift or radio watch shift is the changing from listening to radio transmissions intended for one broadcast area to listening to transmissions intended for another broadcast area.
An area broadcast shift may occur when a ship or aircraft crosses the boundary between listening areas.
Shift times, on the date a ship or aircraft is expected to pass into another area, must be strictly observed or the ship or aircraft will miss messages intended for it.
Source: from Federal Standard 1037C

</doc>
<doc id="40724" url="https://en.wikipedia.org/wiki?curid=40724" title="Arithmetic overflow">
Arithmetic overflow

The term arithmetic overflow or simply overflow has the following meanings. 
Most computers distinguish between two kinds of overflow conditions. A carry occurs when the result of an addition or subtraction, considering the operands and result as unsigned numbers, does not fit in the result. Therefore, it is useful to check the carry flag after adding or subtracting numbers that are interpreted as unsigned values. An "overflow" proper occurs when the result does not have the sign that one would predict from the signs of the operands (e.g. a negative result when adding two positive numbers). Therefore, it is useful to check the overflow flag after adding or subtracting numbers that are represented in two's complement form (i.e. they are considered signed numbers).
There are several methods of handling overflow:
Division by zero is "not" a form of arithmetic overflow. Mathematically, division by zero within reals is explicitly undefined.
Overflow bugs.
Unanticipated arithmetic overflow is a fairly common cause of program errors. Such overflow bugs may be hard to discover and diagnose because they may manifest themselves only for very large input data sets, which are less likely to be used in validation tests.
For example, an unhandled arithmetic overflow in the engine steering software was the primary cause of the crash of the 1996 maiden flight of the Ariane 5 rocket. The software had been considered bug-free since it had been used in many previous flights, but those used smaller rockets which generated lower acceleration than Ariane 5.

</doc>
<doc id="40725" url="https://en.wikipedia.org/wiki?curid=40725" title="Arithmetic shift">
Arithmetic shift

In computer programming, an arithmetic shift is a shift operator, sometimes known as a signed shift (though it is not restricted to signed operands). The two basic types are the arithmetic left shift and the arithmetic right shift. For binary numbers it is a bitwise operation that shifts all of the bits of its operand; every bit in the operand is simply moved a given number of bit positions, and the vacant bit-positions are filled in. Instead of being filled with all 0s, as in logical shift, when shifting to the right, the leftmost bit (usually the sign bit in signed integer representations) is replicated to fill in all the vacant positions (this is a kind of sign extension).
Some authors prefer the terms "sticky right-shift" and "zero-fill right-shift" for arithmetic and logical shifts respectively.
Arithmetic shifts can be useful as efficient ways of performing multiplication or division of signed integers by powers of two. Shifting left by "n" bits on a signed or unsigned binary number has the effect of multiplying it by 2"n". Shifting right by "n" bits on a two's complement "signed" binary number has the effect of dividing it by 2"n", but it always rounds down (towards negative infinity). This is different from the way rounding is usually done in signed integer division (which rounds towards 0). This discrepancy has led to bugs in more than one compiler.
For example, in the x86 instruction set, the SAR instruction (arithmetic right shift) divides a signed number by a power of two, rounding towards negative infinity. However, the IDIV instruction (signed divide) divides a signed number, rounding towards zero. So a SAR instruction cannot be substituted for an IDIV by power of two instruction nor vice versa.
Formal definition.
The formal definition of an arithmetic shift, from Federal Standard 1037C is that it is:
An important word in the FS 1073C definition is "usually".
Equivalence of arithmetic left shift and multiplication.
Arithmetic "left" shifts are equivalent to multiplication by a (positive, integral) power of the radix (e.g. a multiplication by a power of 2 for binary numbers). Arithmetic left shifts are, with two exceptions, identical in effect to logical left shifts. The first exception is the minor trap that arithmetic shifts may trigger arithmetic overflow whereas logical shifts do not. Obviously that exception only hits in real world use cases if a trigger signal for such an overflow is needed by the design it is used for. The second exception is the MSB is preserved. Processors typically do not offer logical and arithmetic left shift operations with a significant difference, if any.
Non-equivalence of arithmetic right shift and division.
However, arithmetic "right" shifts are major traps for the unwary, specifically in the treatment of rounding of negative integers. For example, in the usual two's complement representation of negative integers, −1 is represented as all 1's; for an 8-bit signed integer this is 1111 1111. An arithmetic right-shift by 1 (or 2, 3, …, 7) yields 1111 1111 again, which is still −1. This corresponds to rounding down (towards negative infinity), but is not the usual convention for division.
It is frequently stated that arithmetic right shifts are equivalent to division by a (positive, integral) power of the radix (e.g. a division by a power of 2 for binary numbers), and hence that division by a power of the radix can be optimized by implementing it as an arithmetic right shift. (A shifter is much simpler than a divider. On most processors, shift instructions will execute more quickly than division instructions.) Guy L. Steele quotes a large number of 1960s and 1970s programming handbooks, manuals, and other specifications from companies and institutions such as DEC, IBM, Data General, and ANSI that make such statements. However, as Steele points out, they are all wrong.
Logical right shifts are equivalent to division by a power of the radix (usually 2) only for positive or unsigned numbers. Arithmetic right shifts are equivalent to logical right shifts for positive signed numbers. Arithmetic right shifts for negative numbers in N−1's complement (usually two's complement) is roughly equivalent to division by a power of the radix (usually 2), where for odd numbers rounding downwards is applied (not towards 0 as usually expected).
Arithmetic right shifts for negative numbers would be equivalent to division using rounding towards 0 in one's complement representation of signed numbers as was used by some historic computers, but this is no longer in general use.
Handling the issue in programming languages.
The (1999) ISO standard for the C programming language defines the C language's right shift operator in terms of divisions by powers of 2. Because of the aforementioned non-equivalence, the standard explicitly excludes from that definition the right shifts of signed numbers that have negative values. It doesn't specify the behaviour of the right shift operator in such circumstances, but instead requires each individual C compiler to define the behaviour of shifting negative values right.
Applications.
In applications where consistent rounding down is desired, arithmetic right shifts for signed values are useful. An example is in downscaling raster coordinates by a power of two, which maintains even spacing. For example, right shift by 1 sends 0, 1, 2, 3, 4, 5, … to 0, 0, 1, 1, 2, 2, …, and −1, −2, −3, −4, … to −1, −1, −2, −2, …, maintaining even spacing as −2, −2, −1, −1, 0, 0, 1, 1, 2, 2, … By contrast, integer division with rounding towards zero sends −1, 0, and 1 all to 0 (3 points instead of 2), yielding −2, −1, −1, 0, 0, 0, 1, 1, 2, 2, … instead, which is irregular at 0.

</doc>
<doc id="40726" url="https://en.wikipedia.org/wiki?curid=40726" title="Automatic repeat request">
Automatic repeat request

Automatic Repeat reQuest (ARQ), also known as Automatic Repeat Query, is an error-control method for data transmission that uses acknowledgements (messages sent by the receiver indicating that it has correctly received a data frame or packet) and timeouts (specified periods of time allowed to elapse before an acknowledgment is to be received) to achieve reliable data transmission over an unreliable service. If the sender does not receive an acknowledgment before the timeout, it usually re-transmits the frame/packet until the sender receives an acknowledgment or exceeds a predefined number of re-transmissions .
The types of ARQ protocols include
All three protocols usually use some form of sliding window protocol
to tell the transmitter to determine which (if any) packets need to be retransmitted.
These protocols reside in the Data Link or Transport Layers of the OSI model.
A number of patents exist for the use of ARQ in live video contribution environments. In these high throughput environments negative acknowledgements are used to drive down overheads.
Examples.
The Transmission Control Protocol uses a variant of Go-Back-N ARQ to ensure reliable transmission of data over the Internet Protocol, which does not provide guaranteed delivery of packets; with Selective Acknowledgement (SACK), it uses Selective Repeat ARQ.
The ITU-T G.hn standard, which provides a way to create a high-speed (up to 1 Gbit/s) local area network using existing residential wiring (power lines, telephone lines, and coaxial cables), uses Selective Repeat ARQ to ensure reliable transmission over noisy media.
ARQ systems were widely used on shortwave radio to ensure reliable delivery of data such as for telegrams. These systems came in forms called ARQ-E and ARQ-M, which also included the ability to multiplex two or four channels.

</doc>
<doc id="40727" url="https://en.wikipedia.org/wiki?curid=40727" title="Articulation score">
Articulation score

In telecommunication, an articulation score (AS) is a subjective measure of the intelligibility of a voice system in terms of the percentage of words correctly understood over a channel perturbed by interference.
Articulation scores have been experimentally obtained as functions of varying word content, bandwidth, audio signal-to-noise ratio and the experience of the talkers and listeners involved.

</doc>
<doc id="40728" url="https://en.wikipedia.org/wiki?curid=40728" title="Artificial transmission line">
Artificial transmission line

In telecommunication, an artificial transmission line (art line) is a four-terminal electrical network that has the characteristic impedance, transmission time delay, phase shift, or other parameter(s) of a real transmission line. It can be used to simulate a real transmission line in one or more of these respects.
Source: from Federal Standard 1037C

</doc>
<doc id="40731" url="https://en.wikipedia.org/wiki?curid=40731" title="Asynchronous operation">
Asynchronous operation

In telecommunications, asynchronous operation or asynchronous working is where a sequence of operations is executed such that the operations are executed out of time coincidence with any event. It can also be an operation that occurs without a regular or predictable time relationship to a specified event; e.g., the calling of an error diagnostic routine that may receive control at any time during the execution of a computer program.
Sources.
from Federal Standard 1037C and from MIL-STD-188

</doc>
<doc id="40732" url="https://en.wikipedia.org/wiki?curid=40732" title="Atmospheric duct">
Atmospheric duct

In telecommunication, an atmospheric duct is a horizontal layer in the lower atmosphere in which the vertical refractive index gradients are such that radio signals (and light rays) are guided or ducted, tend to follow the curvature of the Earth, and experience less attenuation in the ducts than they would if the ducts were not present. The duct acts as an atmospheric dielectric waveguide and limits the spread of the wavefront to only the horizontal dimension.
Atmospheric ducting is a mode of propagation of electromagnetic radiation, usually in the lower layers of Earth’s atmosphere, where the waves are bent by atmospheric refraction. In over-the-horizon radar, ducting causes part of the radiated and target-reflection energy of a radar system to be guided over distances far greater than the normal radar range. It also causes long distance propagation of radio signals in bands that would normally be limited to line of sight.
Normally radio "ground waves" propagate along the surface as creeping waves. That is, they are only diffracted around the curvature of the earth. This is one reason that early long distance radio communication used long wavelengths. The best known exception is that HF (3–30 MHz.) waves are reflected by the ionosphere.
The reduced refractive index due to lower densities at the higher altitudes in the Earth's atmosphere bends the signals back toward the Earth. Signals in a higher refractive index layer, "i.e.," duct, tend to remain in that layer because of the reflection and refraction encountered at the boundary with a lower refractive index material. In some weather conditions, such as inversion layers, density changes so rapidly that waves are guided around the curvature of the earth at constant altitude. 
Phenomena of atmospheric optics related to atmospheric ducting include the green flash, Fata Morgana, superior mirage, mock mirage of astronomical objects and the Novaya Zemlya effect.

</doc>
<doc id="40733" url="https://en.wikipedia.org/wiki?curid=40733" title="Attack time">
Attack time

In telecommunication, attack time is the time between the instant that a signal at the input of a device or circuit exceeds the activation threshold of the device or circuit and the instant that the device or circuit reacts in a specified manner, or to a specified degree, to the input. Attack time occurs in devices such as clippers, peak limiters, compressors, and voxes.

</doc>
<doc id="40734" url="https://en.wikipedia.org/wiki?curid=40734" title="ARJ">
ARJ

ARJ (Archived by Robert Jung) is a software tool designed by Robert K. Jung for creating high-efficiency compressed file archives. ARJ is currently on version 2.86 for DOS and 3.20 for Windows and supports 16-bit, 32-bit and 64-bit Intel architectures.
ARJ was one of two mainstream archivers for DOS and Windows during the early and mid-1990s, with PKZIP being its competition. Parts of ARJ were covered by . Generally ARJ was less popular than PKZIP, but it did enjoy a niche market during the BBS era and in the warez scene. This was largely due to ARJ's creation and handling of multi-volume archives (archives which are split into smaller files which are then suitable for dial-up transfers and floppy distribution) being more robust than PKZIP's.
File format support in other software.
ARJ compressed files with the Filename extension .arj can be unpacked with various tools other than the ARJ software. There exists a free software re-implement of the tool. The software 7-Zip and Zipeg can also unpack .arj files. For Mac there exists a standalone utility UnArjMac.

</doc>
<doc id="40735" url="https://en.wikipedia.org/wiki?curid=40735" title="Attenuation">
Attenuation

In physics, attenuation (in some contexts also called extinction) is the gradual loss in intensity of any kind of flux through a medium. For instance, dark glasses attenuate sunlight, lead attenuates X-rays, and water attenuates both light and sound.
In electrical engineering and telecommunications, attenuation affects the propagation of waves and signals in electrical circuits, in optical fibers, and in air (radio waves). Electrical attenuators and optical attenuators are common manufactured components in this field.
Background.
In many cases, attenuation is an exponential function of the path length through the medium. In chemical spectroscopy, this is known as the Beer–Lambert law. In engineering, attenuation is usually measured in units of decibels per unit length of medium (dB/cm, dB/km, etc.) and is represented by the attenuation coefficient of the medium in question. Attenuation also occurs in earthquakes; when the seismic waves move farther away from the epicenter, they grow smaller as they are attenuated by the ground.
Ultrasound.
One area of research in which attenuation figures strongly is in ultrasound physics. Attenuation in ultrasound is the reduction in amplitude of the ultrasound beam as a function of distance through the imaging medium. Accounting for attenuation effects in ultrasound is important because a reduced signal amplitude can affect the quality of the image produced. By knowing the attenuation that an ultrasound beam experiences traveling through a medium, one can adjust the input signal amplitude to compensate for any loss of energy at the desired imaging depth.
Wave equations which take acoustic attenuation into account can be written on a fractional derivative form, see the article on acoustic attenuation or e.g. the survey paper.
Attenuation coefficient.
Attenuation coefficients are used to quantify different media according to how strongly the transmitted ultrasound amplitude decreases as a function of frequency. The attenuation coefficient (formula_1) can be used to determine total attenuation in dB in the medium using the following formula:
As this equation shows, besides the medium length and attenuation coefficient, attenuation is also linearly dependent on the frequency of the incident ultrasound beam. Attenuation coefficients vary widely for different media. In biomedical ultrasound imaging however, biological materials and water are the most commonly used media. The attenuation coefficients of common biological materials at a frequency of 1 MHz are listed below:
There are two general ways of acoustic energy losses: absorption and scattering, for instance light scattering.
Ultrasound propagation through homogeneous media is associated only with absorption and can be characterized with absorption coefficient only. Propagation through heterogeneous media requires taking into account scattering. Fractional derivative wave equations can be applied for modeling of lossy acoustical wave propagation, see also acoustic attenuation and Ref.
Light attenuation in water.
Shortwave radiation emitted from the sun have wavelengths in the visible spectrum of light that range from 360 nm (violet) to 750 nm (red). When the sun’s radiation reaches the sea-surface, the shortwave radiation is attenuated by the water, and the intensity of light decreases exponentially with water depth. The intensity of light at depth can be calculated using the Beer-Lambert Law.
In clear open waters, visible light is absorbed at the longest wavelengths first. Thus, red, orange, and yellow wavelengths are absorbed at higher water depths, and blue and violet wavelengths reach the deepest in the water column. Because the blue and violet wavelengths are absorbed last compared to the other wavelengths, open ocean waters appear deep-blue to the eye.
In near-shore (coastal) waters, sea water contains more phytoplankton than the very clear central ocean waters. Chlorophyll-a pigments in the phytoplankton absorb light, and the plants themselves scatter light, making coastal waters less clear than open waters. Chlorophyll-a absorbs light most strongly in the shortest wavelengths (blue and violet) of the visible spectrum. In near-shore waters where there are high concentrations of phytoplankton, the green wavelength reaches the deepest in the water column and the color of water to an observer appears green-blue or green.
Earthquake.
The energy with which an earthquake affects a location depends on the running distance. The attenuation in the signal of ground motion intensity plays an important role in the assessment of possible strong groundshaking. A seismic wave loses energy as it propagates through the earth (attenuation). This phenomenon is tied in to the dispersion of the seismic energy with the distance. There are two types of dissipated energy:
Electromagnetic.
Attenuation decreases the intensity of electromagnetic radiation due to absorption or scattering of photons. Attenuation does not include the decrease in intensity due to inverse-square law geometric spreading. Therefore, calculation of the total change in intensity involves both the inverse-square law and an estimation of attenuation over the path.
The primary causes of attenuation in matter are the photoelectric effect, compton scattering, and, for photon energies of above 1.022 MeV, pair production.
Radiography.
See Attenuation coefficient.
Optics.
Attenuation in fiber optics, also known as transmission loss, is the reduction in intensity of the light beam (or signal) with respect to distance travelled through a transmission medium. Attenuation coefficients in fiber optics usually use units of dB/km through the medium due to the relatively high quality of transparency of modern optical transmission media. The medium is typically a fiber of silica glass that confines the incident light beam to the inside. Attenuation is an important factor limiting the transmission of a digital signal across large distances. Thus, much research has gone into both limiting the attenuation and maximizing the amplification of the optical signal.
Empirical research has shown that attenuation in optical fiber is caused primarily by both scattering and absorption.
Light scattering.
The propagation of light through the core of an optical fiber is based on total internal reflection of the lightwave. Rough and irregular surfaces, even at the molecular level of the glass, can cause light rays to be reflected in many random directions. This type of reflection is referred to as "diffuse reflection", and it is typically characterized by wide variety of reflection angles. Most objects that can be seen with the naked eye are visible due to diffuse reflection. Another term commonly used for this type of reflection is "light scattering". Light scattering from the surfaces of objects is our primary mechanism of physical observation.
Light scattering from many common surfaces can be modelled by lambertian reflectance.
Light scattering depends on the wavelength of the light being scattered. Thus, limits to spatial scales of visibility arise, depending on the frequency of the incident lightwave and the physical dimension (or spatial scale) of the scattering center, which is typically in the form of some specific microstructural feature. For example, since visible light has a wavelength scale on the order of one micrometer (one millionth of a meter), scattering centers will have dimensions on a similar spatial scale.
Thus, attenuation results from the incoherent scattering of light at internal surfaces and interfaces. In (poly)crystalline materials such as metals and ceramics, in addition to pores, most of the internal surfaces or interfaces are in the form of grain boundaries that separate tiny regions of crystalline order. It has recently been shown that, when the size of the scattering center (or grain boundary) is reduced below the size of the wavelength of the light being scattered, the scattering no longer occurs to any significant extent. This phenomenon has given rise to the production of transparent ceramic materials.
Likewise, the scattering of light in optical quality glass fiber is caused by molecular-level irregularities (compositional fluctuations) in the glass structure. Indeed, one emerging school of thought is that a glass is simply the limiting case of a polycrystalline solid. Within this framework, "domains" exhibiting various degrees of short-range order become the building-blocks of both metals and alloys, as well as glasses and ceramics. Distributed both between and within these domains are microstructural defects that will provide the most ideal locations for the occurrence of light scattering. This same phenomenon is seen as one of the limiting factors in the transparency of IR missile domes.
UV-Vis-IR absorption.
In addition to light scattering, attenuation or signal loss can also occur due to selective absorption of specific wavelengths, in a manner similar to that responsible for the appearance of color. Primary material considerations include both electrons and molecules as follows:
The selective absorption of infrared (IR) light by a particular material occurs because the selected frequency of the light wave matches the frequency (or an integral multiple of the frequency) at which the particles of that material vibrate. Since different atoms and molecules have different natural frequencies of vibration, they will selectively absorb different frequencies (or portions of the spectrum) of infrared (IR) light.
Applications.
In optical fibers, attenuation is the rate at which the signal light decreases in intensity. For this reason, glass fiber (which has a low attenuation) is used for long-distance fiber optic cables; plastic fiber has a higher attenuation and, hence, shorter range. There also exist optical attenuators that decrease the signal in a fiber optic cable intentionally.
Attenuation of light is also important in physical oceanography. This same effect is an important consideration in weather radar, as raindrops absorb a part of the emitted beam that is more or less significant, depending on the wavelength used.
Due to the damaging effects of high-energy photons, it is necessary to know how much energy is deposited in tissue during diagnostic treatments involving such radiation. In addition, gamma radiation is used in cancer treatments where it is important to know how much energy will be deposited in healthy and in tumorous tissue.
Radio.
Attenuation is an important consideration in the modern world of wireless telecommunication. Attenuation limits the range of radio signals and is affected by the materials a signal must travel through (e.g., air, wood, concrete, rain). See the article on path loss for more information on signal loss in wireless communication.

</doc>
<doc id="40737" url="https://en.wikipedia.org/wiki?curid=40737" title="Attenuator">
Attenuator

An attenuator could mean:

</doc>
<doc id="40738" url="https://en.wikipedia.org/wiki?curid=40738" title="Attribute">
Attribute

Attribute may refer to:

</doc>
<doc id="40741" url="https://en.wikipedia.org/wiki?curid=40741" title="Audit (telecommunication)">
Audit (telecommunication)

In telecommunications, an audit is one of:
The simplest audits consist of comparing current telecommunications billing and usage to the underlying rate structure whether that is dictated by contract, tariff, or price list. Complex audits utilize software applications, direct bargaining with service providers and activity reports that include detail down to an individual employee's usage.
Auditing methods and consultants.
In business, companies with significant telecommunications costs or a telecommunications focus normally either conduct audits internally or hire a consultant. No matter the method, typical audits encompass one or more of the following:
A common misconception is that a Telecom Audit only relates to the area of telecom cost, when if fact it encompasses just about every communications service that a business expends its budget on.
Audits may focus on mobile phones and devices, Internet service or land line telephony, or they may encompass all three.
Independent telecom auditing firms are not affiliated with telecom companies that sell mobile phones and devices, Internet service, long distance calling or land line telephony. They are independent and work on contingency.

</doc>
<doc id="40742" url="https://en.wikipedia.org/wiki?curid=40742" title="Audit trail">
Audit trail

An audit trail (also called audit log) is a security-relevant chronological record, set of records, and/or destination and source of records that provide documentary evidence of the sequence of activities that have affected at any time a specific operation, procedure, or event. Audit records typically result from activities such as financial transactions, scientific research and health care data transactions, or communications by individual people, systems, accounts, or other entities.
The process that creates an audit trail is typically required to always run in a privileged mode, so it can access and supervise all actions from all users; a normal user should not be allowed to stop/change it. Furthermore, for the same reason, trail file or database table with a trail should not be accessible to normal users. Another way of handling this issue is through the use of a role-based security model in the software. The software can operate with the closed-looped controls, or as a 'closed system', as required by many companies when using audit trail functionality.
Industry uses of the audit trail.
In telecommunication, the term means a record of both completed and attempted accesses and service, or data forming a logical path linking a sequence of events, used to trace the transactions that have affected the contents of a record.
In information or communications security, information audit means a chronological record of system activities to enable the reconstruction and examination of the sequence of events and/or changes in an event.
In nursing research, it refers to the act of maintaining a running log or journal of decisions relating to a research project, thus making clear the steps taken and changes made to the original protocol.
In accounting, it refers to documentation of detailed transactions supporting summary ledger entries. This documentation may be on paper or on electronic records.
In online proofing, it pertains to the version history of a piece of artwork, design, photograph, video, or web design proof in a project.
In clinical research, server based systems call Clinical Trial Management Systems (CTMS) require audit trails. Anything regulatory or QA-QC related also requires audit trails.

</doc>
<doc id="40743" url="https://en.wikipedia.org/wiki?curid=40743" title="Aurora (disambiguation)">
Aurora (disambiguation)

An aurora is a natural light display in the sky seen predominantly in the high latitudes.
Aurora may also refer to:

</doc>
<doc id="40745" url="https://en.wikipedia.org/wiki?curid=40745" title="Authenticator">
Authenticator

An authenticator is a way to prove to a computer system that you really are who you are (called authentication). It is either:
Authenticator tokens are common when one program needs to authenticate itself to a larger server or cloud repeatedly. For instance, you (the human) might sign on to a secure website with your name and password, after which you can surf around inside the secure server, visiting different web pages. Every time you move to a new page, however, the server must believe that you are the same person who originally signed in (otherwise it will refuse). Your browser keeps an authenticator token, which it sends upon every page request (often as a browser cookie), that does this. 
More complex situations might involve a program that runs automatically (say, at 4:00am every morning) that similarly requires authentication to get at the data it needs, but there's no human around to log in for them. An authenticator token must be prepared in advance that this program uses. Ultimately, some human must authenticate to create such a token. 

</doc>
<doc id="40746" url="https://en.wikipedia.org/wiki?curid=40746" title="Automated information system">
Automated information system

An automated information system (AIS) is an assembly of computer hardware, software, firmware, or any combination of these, configured to accomplish specific information-handling operations, such as communication, computation, dissemination, processing, and storage of information. Included are computers, word processing systems, networks, or other electronic information handling systems, and associated equipment. Management information systems are a common example of automated information systems.

</doc>
<doc id="40747" url="https://en.wikipedia.org/wiki?curid=40747" title="Automated information systems security">
Automated information systems security

In telecommunication, automated information systems security comprises measures and controls that ensure confidentiality, integrity, and availability of the information processed and stored by automated information systems. The unauthorized disclosure, modification, or destruction may be accidental or intentional.
Automated information systems security includes consideration of all computer hardware and software functions, characteristics and features; operational procedures; accountability procedures; and access controls at the central computer facility, remote computer, and terminal facilities; management constraints; physical structures and devices, such as computers, transmission lines, and power sources; and personnel and communications controls needed to provide an acceptable level of risk for the automated information system and for the data and information contained in the system. Automated information systems security also includes the totality of security safeguards needed to provide an acceptable protection level for an automated information system and for the data handled by an automated information system.
In INFOSEC, automated information systems security is a synonym for computer security.

</doc>
<doc id="40748" url="https://en.wikipedia.org/wiki?curid=40748" title="Automatic callback">
Automatic callback

In telecommunication, an automatic callback is a computer telephony calling feature that permits a user, when encountering a busy condition or other condition where the called individual is unavailable, to instruct the system to retain the called number and to establish the call when there is an available line or when the called number is no longer busy. Automatic callback may be implemented in the terminal, in the telephone exchange, or shared between them. Automatic callback is not the same as camp-on.
Source: from Federal Standard 1037C
Using callback on popular business telephone systems.
Comdial Digitech, DSU, Impact 
Place an intercom call and press CAMP. Your phone will disconnect from the attempted call. When the phone you rang is available, your phone will ring with five ring bursts. Press intercom to ring the other phone. 
To use with calls made in the voice-announce mode, press intercom before the camp button.
To cancel, press intercom and dial "#6".
Comdial ExecuTech System 2000
Make an intercom call. At the busy signal, dial "*6". Hang up. When the desired extension becomes idle, the calling telephone receives five tone bursts. To answer callback rings, lift the handset. The called telephone will ring.
To cancel auto call back before it rings, press "ITCM", dial "#6" and hang up. 
Comdial Digital Impression
When you reach a station that is busy or does not answer, press CAMP. When the phone you wish to reach becomes idle, your phone will ring with five short tones. Press ITCM to cause the other phone to ring. 
To cancel the callback, press ITCM and dial "#6".
If the extension you call in voice announce mode is not answered, press ITCM before pressing CAMP.
Database Systems Corp. PACER Phone System
Custom callback is integrated into the CRM application that signals the phone system to redial a number on a particular date and time. Call is automatically assigned to the original agent or assigned to a hunt group associated with a particular campaign.
Executone Encore CX
Press CALLBK when you hear the busy tone. Answer the callback by lifting the handset or pressing MON. 
Inter-Tel Eclipse IDS Integrated Operator Terminal
Press the Call Back key at the busy signal. Press the RLS key. When your line is free and the extension you called is idle, your extension will ring. When the calls rings back to you, press the RLS key.
Inter-Tel Eclipse2
Associate Display and Basic Digital Phone
Press "6" at the busy signal and hang up. Your phone will ring when the extension if available. Press "6" again to cancel before you get your callback.
'"Isoetec Digital Systems
Display/Data Phone"'
Press "Cb." Soft key at the busy signal. Replace the handset or press "HF". Wait for the double tone. When the extension is no longer busy, it will automatically call you back.
Isoetec IDS M Series Telephones
When you hear the busy signal, press the "CALL BACK" key. Hang up. When you are signaled, lift the handset or press the "HF" key. Press the blinking "CALL BACK" key. 
PCS Digital Telephone 
Press "cbck" at the busy signal. When a station is available, pick up the handset.
To cancel, press "del".
Vodavi StarPlus Phone System
Press the pre-programmed CALL BACK button. Hang up. When the busy station becomes available, you will be signaled. 
Source: from MainResource.com

</doc>
<doc id="40749" url="https://en.wikipedia.org/wiki?curid=40749" title="Automatic call distributor">
Automatic call distributor

In telephony, an automatic call distributor (ACD) or automated call distribution system, is a device or system that distributes incoming calls to a specific group of terminals or agents based on the customer's selection, customer's telephone number, selected incoming line to the system, or time of day the call was processed. It is often part of a computer telephony integration (CTI) system.
Routing incoming calls is the task of the ACD system. ACD systems are often found in offices that handle large volumes of incoming phone calls from callers who have no need to talk to a specific person but who require assistance from any of multiple persons (e.g., customer service representatives) at the earliest opportunity.
The system consists of hardware for the terminals and switches, phone lines, and software for the routing strategy. The routing strategy is a rule-based set of instructions that tells the ACD how calls are handled inside the system. Typically this is an algorithm that determines the best available employee or employees to respond to a given incoming call. To help make this match, additional data are solicited and reviewed to find out why the customer is calling. Sometimes the caller's caller ID or ANI is used; more often a simple IVR is used to ascertain the reason for the call.
Originally, the ACD function was internal to the Private Branch Exchange of the company. However, the closed nature of these systems limited their flexibility. A system was then designed to enable common computing devices, such as server PCs, to make routing decisions. For this, generally the PBX would issue information about incoming calls to this external system and receive a direction of the call in response.
An additional function for these external routing applications is to enable CTI. This allows improved efficiency for call center agents by matching incoming phone calls with relevant data on their PC via screen pop.
A common protocol to achieve this is CSTA; however, almost every PBX vendor has its own flavor of CSTA, and CSTA is quite hard to program because of its complex nature. Various vendors have developed intermediate software that hides these complexities and expedites the work of programmers.
Also, these protocols enable call centers consisting of PBXs from multiple vendors to be treated as one virtual contact center. All real-time and historical statistical information can then be shared amongst call center sites.
One of the first large and separate ACDs was a modified 5XB switch used by New York Telephone in the early 1970s to distribute calls among hundreds of 4-1-1 information operators.
Distribution methods.
There are multiple choices for distributing incoming calls from a queue.

</doc>
<doc id="40751" url="https://en.wikipedia.org/wiki?curid=40751" title="Automatic data processing">
Automatic data processing

Automatic data processing (ADP) may refer to:

</doc>
<doc id="40753" url="https://en.wikipedia.org/wiki?curid=40753" title="Automatic link establishment">
Automatic link establishment

Automatic Link Establishment, commonly known as ALE, is the worldwide de facto standard for digitally initiating and sustaining HF radio communications. ALE is a feature in an HF communications radio transceiver system, that enables the radio station to make contact, or initiate a circuit, between itself and another HF radio station or network of stations. The purpose is to provide a reliable rapid method of calling and connecting during constantly changing HF ionospheric propagation, reception interference, and shared spectrum use of busy or congested HF channels.
Mechanism.
A standalone ALE radio combines an HF SSB radio transceiver with an internal microprocessor and MFSK modem. It is programmed with a unique ALE Address, similar to a phone number (or on newer generations, a username). When not actively in contact with another station, the HF SSB transceiver constantly scans through a list of HF frequencies called channels, listening for any ALE signals transmitted by other radio stations. It decodes calls and soundings sent by other stations and uses the Bit error rate to store a quality score for that frequency and sender-address.
To reach a specific station, the caller enters the ALE Address. On many ALE radios this is similar to dialing a phone number. The ALE controller selects the best available idle channel for that destination address. After confirming the channel is indeed idle, it then sends a brief selective calling signal identifying the intended recipient. When the distant scanning station detects ALE activity, it stops scanning and stays on that channel until it can confirm whether or not the call is for it. The two stations' ALE controllers automatically handshake to confirm that a link of sufficient quality has been established, then notify the operators that the link is up. If the callee fails to respond or the handshaking fails, the originating ALE node usually selects another frequency either at random or by making a guess of varying sophistication.
Upon successful linking, the receiving station generally emits an audible alarm and shows a visual alert to the operator, thus indicating the incoming call. It also indicates the callsign or other identifying information of the linked station, similar to Caller ID. The operator then un-mutes the radio and answers the call then can talk in a regular conversation or negotiates a data link using voice or the ALE built-in short text message format. Alternatively, digital data can be exchanged via a built-in or external modem (such as a STANAG 5066 or MIL-STD-188-110B serial tone modem) depending on needs and availability. The ALE built-in text messaging facility can be used to transfer short text messages as an "orderwire" to allow operators to coordinate external equipment such as phone patches or non-embedded digital links, or for short tactical messages. 
Operator skill.
Due to the vagaries of ionospheric communications, HF radio as used by large governmental organizations in the mid-20th century was traditionally the domain of highly skilled and trained radio operators. One of the new characteristics that embedded microprocessors and computers brought to HF radio via ALE, was alleviation of the need for the radio operator to constantly monitor and change the radio frequency manually to compensate for ionospheric conditions or interference. For the average user of ALE, after learning how to work the basic functions of the HF transceiver, it became similar to operating a cellular mobile phone. For more advanced functions and programming of ALE controllers and networks, it became similar to the use of menu-enabled consumer equipment or the optional features typically encountered in software. In a professional or military organization, this does not eliminate the need for skilled and trained communicators to coordinate the per-unit authorized frequency lists and node addresses - it merely allows the deployment of relatively unskilled technicians as "field communicators" and end-users of the existing coordinated architecture.
Common applications.
An ALE radio system enables connection for voice conversation, alerting, data exchange, texting, instant messaging, email, file transfer, image, geo-position tracking, or telemetry. With a radio operator initiating a call, the process normally takes a few minutes for the ALE to pick an HF frequency that is optimum for both sides of the communication link. It signals the operators audibly and visually on both ends, so they can begin communicating with each other immediately. In this respect, the longstanding need in HF radio for repetitive calling on pre-determined time schedules or tedious monitoring static is eliminated. It is useful as a tool for finding optimum channels to communicate between stations in real-time. In modern HF communications, ALE has largely replaced HF prediction charts, propagation beacons, chirp sounders, propagation prediction software, and traditional radio operator educated guesswork. ALE is most commonly used for hooking up operators for voice contacts on SSB (single sideband modulation), HF internet connectivity for email, SMS phone texting or text messaging, real-time chat via HF text, Geo Position Reporting, and file transfer. High Frequency Internet Protocol or HFIP may be used with ALE for internet access via HF.
Techniques.
The essence of ALE techniques is the use of automatic channel selection, scanning receivers, selective calling, handshaking, and robust burst modems. An ALE node decodes all received ALE signals heard on the channel(s) it monitors. It utilizes the fact that all ALE messages utilize Forward error correction (FEC) redundancy. By noting how much error-correction occurred in each received and decoded message, an ALE node can detect the "quality" of the path between the sending station and itself. This information is coupled with the ALE address of the sending node and the channel the message was received on, and stored in the node's Link Quality Analysis (LQA) memory. When a call is initiated, the LQA lookup table is searched for matches involving the target ALE address and the best historic channel is used to call the target station. This reduces the likelihood that the call has to be repeated on alternate frequencies. Once the target station has heard the call and responded, a bell or other signalling device will notify both operators that a link has been established. At this point, the operators may coordinate further communication via orderwire text messages, voice, or other means. If further digital communication is desired, it may take place via external data modems or via optional modems built into the ALE terminal.
This unusual usage of FEC redundancy is the primary innovation that differentiates ALE from previous selective calling systems which either decoded a call or failed to decode due to noise or interference. A binary outcome of "Good enough" or not gave no way of automatically choosing between two channels, both of which are currently good enough for minimum communications. The redundancy-based scoring inherent in ALE thus allows for selecting the "best" available channel and (in more advanced ALE nodes) using all decoded traffic over some time window to sort channels into a list of decreasing probability-to-contact, significantly reducing co-channel interference to other users as well as dramatically decreasing the time needed to successfully link with the target node.
Techniques used in the ALE standard include automatic signaling, automatic station identification (sounding), polling, message store-and-forward, linking protection and anti-spoofing to prevent hostile denial of service by ending the channel scanning process. Optional ALE functions include polling and the exchange of orderwire commands and messages. The orderwire message, known as AMD (Automatic Message Display), is the most commonly used text transfer method of ALE, and the only universal method that all ALE controllers have in common for displaying text. It is common for vendors to offer extensions to AMD for various non-standard features, although dependency on these extensions undermines interoperability. As in all interoperability scenarios, care should be taken to determine if this is acceptable before using such extensions.
History and precedents.
ALE evolved from older HF radio selective calling technology. It combined existing channel-scanning selective calling concepts with microprocessors (enabling FEC decoding and quality scoring decisions), burst transmissions (minimizing co-channel interference), and transponding (allowing unattended operation and incoming-call signalling). Early ALE systems were developed in the late 1970s and early 1980s by several radio manufacturers. The first ALE-family controller units were external rack mounted controllers connected to control military radios, and were rarely interoperable across vendors.
Various methods and proprietary digital signaling protocols were used by different manufacturers in first generation ALE, leading to incompatibility. Later, a cooperative effort among manufacturers and the US government resulted in a second generation of ALE that included the features of first generation systems, while improving performance. The second generation 2G ALE system standard in 1986, MIL-STD-188-141A, was adopted in FED-STD-1045 for US federal entities. In the 1980s, military and other entities of the US government began installing early ALE units, using ALE controller products built primarily by US companies. The primary application during the first 10 years of ALE use was government and military radio systems, and the limited customer base combined with the necessity to adhere to MILSPEC standards kept prices extremely high. Over time, demand for ALE capabilities spread and by the late 1990s, most new government HF radios purchased were designed to meet at least the minimum ALE interoperability standard, making them eligible for use with standard ALE node gear. Radios implementing at least minimum ALE node functionality as an option internal to the radio became more common and significantly more affordable. As the standards were adopted by other governments worldwide, more manufacturers produced competitively priced HF radios to meet this demand. The need to interoperate with government organizations prompted many non-government organizations (NGOs) to at least partially adopt ALE standards for communication. As non-military experience spread and prices came down, other civilian entities started using 2G ALE. By the year 2000, there were enough civilian and government organizations worldwide using ALE that it became a de facto HF interoperability standard for situations where a priori channel and address coordination is possible.
In the late 1990s, a third generation 3G ALE with significantly improved capability and performance was included in MIL-STD-188-141B, retaining backward compatibility with 2G ALE, and was adopted in NATO STANAG 4538. Civilian and non-government adoption rates are much lower than 2G ALE due to the extreme cost as compared to surplus or entry-level 2G gear as well as the significantly increased system and planning complexity necessary to realize the benefits inherent in the 3G specification. For many militaries, whose needs for maximized intra-organizational capability and capacity always strain existing systems, the additional cost and complexity of 3G is far more compelling.
Reliability.
ALE enables rapid unscheduled communication and message passing without requiring complex message centers, multiple radios and antennas, or highly trained operators. With the removal of these potential sources of failure, the tactical communication process becomes much more robust and reliable. The effects extend beyond mere Force multiplication of existing communications methods; units such as helicopters, when outfitted with ALE radios, can now reliably communicate in situations where the crew are too busy to operate a traditional non-line of sight radio. This ability to enable tactical communication in conditions where dedicated trained operators and hardware are inappropriate is often considered to be the true improvement offered by ALE.
ALE is a critical path toward increased interoperability between organizations. By enabling a station to participate nearly simultaneously in many different HF networks, ALE allows for convenient cross-organization message passing and monitoring without requiring dedicated separate equipment and operators for each partner organization. This dramatically reduces staffing and equipment considerations, while enabling small mobile or portable stations to participate in multiple networks and subnetworks. The result is increased resilience, decreased fragility, increased ability to communicate information effectively, and the ability to rapidly add to or replace communication points as the situation demands.
When combined with Near Vertical Incidence Skywave (NVIS) techniques and sufficient channels spread across the spectrum, an ALE node can provide greater than 95% success linking on the first call, nearly on par with SATCOM systems. This is significantly more reliable than cellphone infrastructure during disasters or wars yet is mostly immune to such considerations itself.
Standards and protocols.
Global standards for ALE are based on the original US MIL-STD 188-141A and FED-1045, known as 2nd Generation (2G) ALE. 2G ALE uses non-synchronised scanning of channels, and it takes several seconds to half a minute to repeatedly scan through an entire list of channels looking for calls. Thus it requires sufficient duration of transmission time for calls to connect or link with another station that is unsynchronised with its calling signal. The vast majority of ALE systems in use in the world at the present time are 2G ALE.
3G technical characteristics.
Newer standards of ALE called 3rd Generation or 3G ALE, use accurate time synchronization (via a defined time-synch protocol as well as the option of GPS-locked clocks) to achieve faster and more dependable linking. Through synchronization, the calling time to achieve a link may be reduced to less than 10 seconds. The 3G ALE modem signal also provides better robustness and can work in channel conditions that are less favorable than 2G ALE. Dwell groups, limited callsigns, and shorter burst transmissions enable more rapid intervals of scanning. All stations in the same group scan and receive each channel at precisely the same time window. Although 3G ALE is more reliable and has significantly enhanced channel-time efficiency, the existence of a large installed base of 2G ALE radio systems and the wide availability of moderately priced (often military surplus) equipment, has made 2G the baseline standard for global interoperability.
Basis for HF interoperability communications.
Interoperability is a critical issue for the disparate entities which use radiocommunications to fulfill the needs of organizations. Largely due to the ubiquity of 2G ALE, it became the primary method for providing interoperability on HF between governmental and non-governmental disaster relief and emergency communications entities, and amateur radio volunteers. With digital techniques increasingly employed in communications equipment, a universal digital calling standard was needed, and ALE filled the gap. Nearly every major HF radio manufacturer in the world builds ALE radios to the 2G standard to meet the high demand that new installations of HF radio systems conform to this standard protocol. Disparate entities that historically used incompatible radio methods were then able to call and converse with each other using the common 2G ALE platform. Some manufacturers and organizations have utilized the AMD feature of ALE to expand the performance and connectivity. In some cases, this has been successful, and in other cases, the use of proprietary preamble or embedded commands has led to interoperability problems.
Tactical communication and resource management.
ALE serves as a convenient method of beyond line of sight communication. Originally developed to support military requirements, ALE is useful to many organizations who find themselves managing widely located units. United States Immigration and Customs Enforcement and United States Coast Guard are two members of the Customs Over the Horizon Enforcement Network(COTHEN), a MIL-STD 188-141A ALE network. All U.S. armed forces operate multiple similar networks. Similarly, shortwave utility listeners have documented frequency and callsign lists for many nations' military and guard units, as well as networks operated by oil exploration and production companies and public utilities in many countries.
Emergency / disaster relief or extraordinary situation response communications.
ALE radio communication systems for both HF regional area networks and HF interoperability communications are in service among emergency and disaster relief agencies as well as military and guard forces. Extraordinary response agencies and organizations use ALE to respond to situations in the world where conventional communications may have been temporarily overloaded or damaged. In many cases, it is in place as alternative back-channel for organizations that may have to respond to situations or scenarios involving the loss of conventional communications. Earthquakes, storms, volcanic eruptions, and power or communication infrastructure failures are typical situations in which organizations may deem ALE necessary to operations. ALE networks are common among organizations engaged in extraordinary situation response such as: natural and man-made disasters, transportation, power, or telecommunication network failures, war, peacekeeping, or stability operations. Organizations known to use ALE for Emergency management, disaster relief, ordinary communication or extraordinary situation response include: Red Cross, FEMA, Disaster Medical Assistance Teams, NATO, Federal Bureau of Investigation, United Nations, AT&T, Civil Air Patrol, SHARES, State of California Emergency Management Agency (CalEMA), other US States' Offices of Emergency Services or Emergency Management Agencies, and Amateur Radio Emergency Service(ARES).
International HF telecommunications for disaster relief.
The International Telecommunications Union (ITU), in response to the need for interoperation in international disaster response spurred largely by humanitarian relief, included ALE in its Telecommunications for Disaster Relief recommendations. The increasing need for instant connectivity for logistical and tactical disaster relief response communications, such as the 2004 Indian Ocean earthquake tsunami led to ITU actions of encouragement to countries around the world toward loosening restrictions on such communications and equipment border transit during catastrophic disasters. The IARU Global Amateur Radio Emergency Communications Conferences (GAREC) and IARU Global Simulated Emergency Tests have included ALE.
Use in amateur radio.
Amateur radio operators began sporadic ALE operation on a limited basis in the early to mid-1990s, with commercial ALE radios and ALE controllers. In 2000, the first widely available software ALE controller for the Personal Computer, "PCALE", became available, and hams started to set up stations based on it. In 2001, the first organized and coordinated global ALE nets for International Amateur Radio began. In August 2005, ham radio operators supporting communications for emergency Red Cross shelters used ALE for Disaster Relief operations during the Hurricane Katrina disaster. After the event, hams developed more permanent ALE emergency/disaster relief networks, including internet connectivity, with a focus on interoperation between organizations. The amateur radio HFLink Automatic Link Establishment system uses an open net protocol to enable all amateur radio operators and amateur radio nets worldwide to participate in ALE and share the same ALE channels legally and interoperably. Amateur radio operators may use it to call each other for voice or data communications.
Amateur radio interoperability adaptations.
Amateur radio operators commonly provide local, regional, national, and international emergency / disaster relief communications. The need for interoperability on HF led to the adoption of Automatic Link Establishment ALE open networks by hams. Amateur radio adapted 2G ALE techniques, by utilizing the common denominators of the 2G ALE protocol, with a limited subset of features found in the majority of all ALE radios and controllers. Each amateur radio ALE station uses the operator's callsign as the address, also known as the ALE Address, in the ALE radio controller. The lowest common denominator technique enables any manufacturer's ALE radios or software to be utilized for HF interoperability communications and networking. Known as Ham-Friendly ALE, the amateur radio ALE standard is used to establish radio-communications, through a combination of active ALE on internationally recognized automatic data frequencies, and passive ALE scanning on voice channels. In this technique, active ALE frequencies include pseudo-random periodic polite station identification, while passive ALE frequencies are silently scanned for selective calling. ALE systems include Listen Before Transmit as a standard function, and in most cases this feature provides better busy channel detection of voice and data signals than the human ear. Ham-Friendly ALE technique is also known as 2.5G ALE, because it maintains 2G ALE compatibility while employing some of the adaptive channel management features of 3G ALE, but without the accurate GPS time synchronization of 3G ALE.
Disaster relief HF network.
Hot standby nets are in constant operation 24/7/365 for International Emergency and Disaster Relief communications. The Ham Radio Global ALE High Frequency Network, which began service in June 2007, is the world's largest intentionally open ALE network for internet connectivity. It is a free open network staffed by volunteers, and utilized by amateur radio operators supporting disaster relief organizations.
International coordination.
International amateur radio ALE High Frequency channels are frequency coordinated with all Regions of the International Amateur Radio Union(IARU entity of ITU), for international, regional, national, and local use in the Amateur Radio Service. All Amateur Radio ALE channels use "USB" Upper Sideband standard. Different rules, regulations, and bandplans of the region and local country of operation apply to use of various channels. Some channels may not be available in every country. Primary or global channels are in common with most countries and regions.
International channels.
"This listing is current as of March 2014. See HFLINK for more information about Amateur Radio ALE Automatic Link Establishment."

</doc>
<doc id="40754" url="https://en.wikipedia.org/wiki?curid=40754" title="Automatic message exchange">
Automatic message exchange

Automatic message exchange (AME): In an adaptive high-frequency (HF) radio network, an automated process allowing the transfer of a message from message injection to addressee reception, without human intervention. Through the use of machine-addressable transport guidance information, "i.e.," the message header, the message is automatically routed through an on-line direct connection through single or multiple transmission media.
Source: from Federal Standard 1037C

</doc>
<doc id="40755" url="https://en.wikipedia.org/wiki?curid=40755" title="Automatic redial">
Automatic redial

In telecommunication, an automatic redial is a service feature that allows the user to dial, by depressing a single key or a few keys, the most recent telephone number dialed at that instrument.
"Note:" Automatic redial is often associated with the telephone instrument, but may be provided by a PBX, or by the central office. "Synonym" last number redial. "Contrast with" automatic calling unit.
Often one must subscribe to a caller ID for use of this function on a landline.

</doc>
<doc id="40756" url="https://en.wikipedia.org/wiki?curid=40756" title="Automatic sounding">
Automatic sounding

In telecommunication, automatic sounding is the testing of selected channels for quality by providing a very brief identifying transmission that may be used by other stations to evaluate connectivity, and availability, and to identify known working channels for immediate or later use for communications or calling. They are often used to maintain connectivity in digital communications high frequency radio networks.
Automatic soundings are primarily intended to increase the efficiency of the automatic link establishment (ALE) function, thereby increasing system throughput. 
In ALE, the sounding information consists of a heavily error-corrected short message identifying the sender. Recipients decode it and use the bit error rate to calculate and store a (channel, node, quality) tuple. As ionospheric conditions and mobile-node locations change, these quality tuples will shift. The stored data can be used to maximize the chance that the best channel to link with a given partner will be chosen first.

</doc>
<doc id="40757" url="https://en.wikipedia.org/wiki?curid=40757" title="Automatic switching system">
Automatic switching system

In data communications, an automatic switching system is a switching system in which all the operations required to execute the three phases of Information transfer transactions are automatically executed in response to signals from a user end-instrument. 
In an automatic switching system, the information-transfer transaction is performed without human intervention, except for initiation of the access phase and the disengagement phase by a user. 
In telephony, it refers to a telephone exchange in which all the operations required to set up, supervise, and release connections required for telephone calls are automatically performed in response to signals from a calling device. This distinction lost importance as manual switching declined during the 20th century.

</doc>
<doc id="40758" url="https://en.wikipedia.org/wiki?curid=40758" title="Auxiliary power">
Auxiliary power

Auxiliary power is electric power that is provided by an alternate source and that serves as backup for the primary power source at the station main bus or prescribed sub-bus. 
An offline unit provides electrical isolation between the primary power source and the critical technical load whereas an online unit does not. 
A Class A power source is a primary power source, i.e., a source that assures an essentially continuous supply of power. 
Types of auxiliary power services include Class B, a standby power plant to cover extended outages of the order of days; Class C, a 10-to-60-second quick-start unit to cover short-term outages of the order of hours; and Class D, an uninterruptible non-break unit using stored energy to provide continuous power within specified voltage and frequency tolerances.

</doc>
<doc id="40760" url="https://en.wikipedia.org/wiki?curid=40760" title="Availability">
Availability

In reliability theory and reliability engineering, the term availability has the following meanings:
For example, a unit that is capable of being used 100 hours per week (168 hours) would have an availability of 100/168. However, typical availability values are specified in decimal (such as 0.9998). In high availability applications, a metric known as nines, corresponding to the number of nines following the decimal point, is used. With this convention, "five nines" equals 0.99999 (or 99.999%) availability.
Introduction.
Availability of a system is typically measured as a factor of its reliability – as reliability increases, so does availability. 
Availability of a system may also be increased by the strategy of focusing on increasing testability, diagnostics and maintainability and not on reliability. Improving maintainability during the early design phase is generally easier than reliability (and Testability & diagnostics). Maintainability estimates (item Repair replacement rates) are also generally more accurate. However, because the uncertainties in the reliability estimates (and also in diagnostic times) are in most cases very large, it is likely to dominate the availability (and the prediction uncertainty) problem, even while maintainability levels are very high. Furthermore, when reliability is not under control, then many and different sorts of issues may arise, for example:
The problem of unreliability may also become out of control due to the "domino effect" of maintenance induced failures after repairs and more and more increasing efforts of problem solving, re-engineering en service efforts. Only focusing on maintainability is therefore not enough! 
Reliability needs to be evaluated and improved related to both availability and the cost of ownership (due to cost of spare parts, maintenance man-hours, transport costs, storage cost, part obsolete risks etc.). Often a trade-off is needed between the two. There might be a maximum ratio between availability and cost of ownership. Testability of a system should also be addressed in the availability plan as this is the link between reliability and maintainability. The maintenance strategy can influence the reliability of a system (e.g. by preventive and/or predictive maintenance), although it can never bring it above the inherent reliability. So, Maintainability and Maintenance strategies influences the availability of a system. In theory this can be almost unlimited if one would be able to always repair any fault in an infinitely short time. This is in practice impossible. Repair-ability is always limited due to testability, manpower and logistic considerations. Reliability is not limited (Reliable items can be made that outlast the life of a machine with almost 100% certainty). For high levels of system availability (e.g. the availability of engine trust in an aircraft), the use of redundancy may be the only option. Refer to reliability engineering.
An availability plan should clearly provide a strategy for availability control. Whether only Availability or also Cost of Ownership is more important depends on the use of the system. For example, a system that is a critical link in a production system – e.g. a big oil platform – is normally allowed to have a very high cost of ownership if this translates to even a minor increase in availability, as the unavailability of the platform results in a massive loss of revenue which can easily exceed the high cost of ownership. A proper reliability plan should always address RAMT analysis in its total context. RAMT stands in this case for Reliability, Availability, Maintainability/Maintenance and Testability in context to the customer needs.
Representation.
The most simple representation for availability is as a ratio of the expected value of the uptime of a system to the aggregate of the expected values of up and down time, or
If we define the status function formula_2 as
therefore, the availability "A"("t") at time "t" > 0 is represented by
Average availability must be defined on an interval of the real line. If we consider an arbitrary constant formula_5, then average availability is represented as
Limiting (or steady-state) availability is represented by
Limiting average availability is also defined on an interval formula_8 as,
Availability is the probability that an item will be in an operable and commitable state at the start of a mission when the mission is called for at a random time, and is generally defined as uptime divided by total time (uptime plus downtime).
Methods and techniques to model availability.
Fault tree analysis and related software are developed to calculate (analytic or by simulation) availability of a system or a functional failure condition within a system including many factors like:
Furthermore, these methods are capable to identify the most critical items and failure modes or events that impact availability.
Definitions within systems engineering.
Availability, inherent (Ai) 
The probability that an item will operate satisfactorily at a given point in time when used under stated conditions in an ideal support environment. It excludes logistics time, waiting or administrative downtime, and preventive maintenance downtime. It includes corrective maintenance downtime. Inherent availability is generally derived from analysis of an engineering design and is calculated as the mean time to failure (MTTF) divided by the mean time to failure plus the mean time to repair (MTTR). It is based on quantities under control of the designer.
Availability, achieved (Aa) 
The probability that an item will operate satisfactorily at a given 
point in time when used under stated conditions in an ideal support environment (i.e., that personnel, tools, spares, etc. are instantaneously available). It excludes logistics time and waiting or administrative downtime. It includes active preventive and corrective maintenance downtime.
Availability, operational (Ao) 
The probability that an item will operate satisfactorily at a given point in time when used in an actual or realistic operating and support environment. It includes logistics time, ready time, and waiting or administrative downtime, and both preventive and corrective maintenance downtime. This value is equal to the mean time between failure (MTBF) divided by the mean time between failure plus the mean downtime (MDT). This measure extends the definition of availability to elements controlled by the logisticians and mission planners such as quantity and proximity of spares, tools and manpower to the hardware item.
Refer to Systems engineering for more details
Basic example.
If we are using equipment which has a mean time to failure (MTTF) of 81.5 years and mean time to repair (MTTR) of 1 hour:
MTTF in hours = 81.5 × 365 × 24 = 713940 (This is a reliability parameter and often has a high level of uncertainty!)
Outage due to equipment in hours per year = 1/rate = 1/MTTF = 0.01235 hours per year.
Literature.
Availability is well established in the literature of stochastic modeling and optimal maintenance. Barlow and Proschan define availability of a repairable system as "the probability that the system is operating at a specified time t." Blanchard [1998 gives a qualitative definition of availability as "a measure of the degree of a system which is in the operable and committable state at the start of mission when the mission is called for at an unknown random point in time." This definition comes from the MIL-STD-721. Lie, Hwang, and Tillman developed a complete survey along with a systematic classification of availability.
Availability measures are classified by either the time interval of interest or the mechanisms for the system downtime. If the time interval of interest is the primary concern, we consider instantaneous, limiting, average, and limiting average availability. The aforementioned definitions are developed in Barlow and Proschan Lie, Hwang, and Tillman [1977, and Nachlas The second primary classification for availability is contingent on the various mechanisms for downtime such as the inherent availability, achieved availability, and operational availability. (Blanchard [1998, Lie, Hwang, and Tillman Mi [1998 gives some comparison results of availability considering inherent availability.
Availability considered in maintenance modeling can be found in Barlow and Proschan for replacement models, Fawzi and Hawkes [1991 for an R-out-of-N system with spares and repairs, Fawzi and Hawkes for a series system with replacement and repair, Iyer [1992 for imperfect repair models, Murdock for age replacement preventive maintenance models, Nachlas [1998, 1989 for preventive maintenance models, and Wang and Pham for imperfect maintenance models.
Applications.
Availability is used extensively in power plant engineering. For example, the North American Electric Reliability Corporation implemented the Generating Availability Data System in 1982.

</doc>
<doc id="40762" url="https://en.wikipedia.org/wiki?curid=40762" title="Backbone">
Backbone

Backbone is the Vertebral column of a vertebrate organism. 

</doc>
<doc id="40765" url="https://en.wikipedia.org/wiki?curid=40765" title="Back-to-back connection">
Back-to-back connection

A back-to-back connection is the direct connection of the output of one device to the input of a similar or related device.
Telecommunications.
In telecommunications, a back-to-back connection can be formed by connecting a transmitter directly to a receiver without a transmission line in between. This is used for equipment measurements and testing purposes. The back-to-back connection eliminates the effects of the transmission channel or medium.
In some cases, the output of a receiving device is instead connected to the input of a transmitting device.
Power transmission.
A back-to-back connection for electric power transmission is a high-voltage direct-current (HVDC) system with both ends in the same switchyard. This is used to couple asynchronously operated power grids or for connecting power grids of different frequencies where no DC transmission line is necessary.
Electronics.
In electronics, a back-to-back connection is the connection of two identical or similar components in series with the opposite polarity. This is used to convert polarised components to non-polar use. Common examples include:

</doc>
<doc id="40766" url="https://en.wikipedia.org/wiki?curid=40766" title="Backward channel">
Backward channel

In a data transmission circuit a backward channel is the channel that passes data in a direction opposite to that of its associated forward channel. The backward channel is usually used for transmission of request, supervisory, acknowledgement, or error-control signals. The direction of flow of these signals is opposite to that in which user information is being transferred. The backward-channel bandwidth is usually less than that of the primary channel, that is, the forward (user information) channel. For example, ADSL's upstream channel, considered a backward channel for some types of analysis, typically has a bandwidth less than one-fourth of the downstream channel.
In data transmission, it is a secondary channel in which the direction of transmission is constrained to be opposite to that of the primary, "i.e.", the forward (user-information) channel. The direction of transmission in the backward channel is restricted by the control interchange circuit that controls the direction of transmission in the primary channel.

</doc>
<doc id="40767" url="https://en.wikipedia.org/wiki?curid=40767" title="Balanced line">
Balanced line

In telecommunications and professional audio, a balanced line or balanced signal pair is a transmission line consisting of two conductors of the same type, each of which have equal impedances along their lengths and equal impedances to ground and to other circuits. The chief advantage of the balanced line format is good rejection of external noise when fed to a differential amplifier. Common forms of balanced line are twin-lead, used for radio frequency signals and twisted pair, used for lower frequencies. They are to be contrasted to unbalanced lines, such as coaxial cable, which is designed to have its return conductor connected to ground, or circuits whose return conductor actually is ground. Balanced and unbalanced circuits can be interconnected using a transformer called a balun.
Circuits driving balanced lines must themselves be balanced to maintain the benefits of balance. This may be achieved by transformer coupling or by merely balancing the impedance in each conductor.
Lines carrying symmetric signals (those with equal but opposite voltages to ground on each leg) are often incorrectly referred to as "balanced", but this is actually differential signaling. Balanced lines and differential signaling are often used together, but they are not the same thing. Differential signalling does not make a line balanced, nor does noise rejection in balanced cables require differential signalling.
Explanation.
Transmission of a signal over a balanced line reduces the influence of noise or interference due to external stray electric fields. Any external signal sources tend to induce only a common mode signal on the line, and the balanced impedances to ground minimizes differential pickup due to stray electric fields. The conductors are sometimes twisted together to ensure that each conductor is equally exposed to any external magnetic fields that could induce unwanted noise.
Some balanced lines also have electrostatic shielding to reduce the amount of noise introduced. The cable is often wrapped in foil, copper wire, or a copper braid. This shield provides immunity to RF interference but does not provide immunity to magnetic fields.
Some balanced lines use 4-conductor star quad cable to provide immunity to magnetic fields. The geometry of the cable ensures that magnetic fields will causes equal interference of both legs of the balanced circuit. This balanced interference is a common-mode signal that can easily be removed by a transformer or balanced differential receiver.
A balanced line allows a differential receiver to reduce the noise on a connection by rejecting common-mode interference. The lines have the same impedance to ground, so the interfering fields or currents induce the same voltage in both wires. Since the receiver responds only to the difference between the wires, it is not influenced by the induced noise voltage. If balanced line is used in an unbalanced circuit, with different impedances from each conductor to ground, currents induced in the separate conductors will cause different voltage drops to ground, thus creating a voltage differential, making the line more susceptible to noise. Examples of twisted pairs include Category 5 cable.
Compared to unbalanced circuits, balanced lines reduce the amount of noise per distance, allowing a longer cable run to be practical. This is because electromagnetic interference will affect both signals the same way. Similarities between the two signals are automatically removed at the end of the transmission path when one signal is subtracted from the other.
Telephone systems.
The first application for balanced lines was for telephone lines. Interference that was of little consequence on a telegraph system (which is in essence digital) could be very disturbing for a telephone user. The initial format was to take two single-wire unbalanced telegraph lines and use them as a pair. This proved insufficient, however, with the growth of electric power transmission which tended to use the same routes. A telephone line running alongside a power line for many miles will inevitably have more interference induced in one leg than the other since one of them will be nearer to the power line. This issue was addressed by swapping the positions of the two legs every few hundred yards with a cross-over, thus ensuring that both legs had equal interference induced and allowing common-mode rejection to do its work. As the telephone system grew, it became preferable to use cable rather than open wires to save space, and also to avoid poor performance during bad weather. The cable construction used for balanced telephone cables was twisted pair; however, this did not become widespread until repeater amplifiers became available. For an unamplified telephone line, a twisted pair cable could only manage a maximum distance of 30 km. Open wires, on the other hand, with their lower capacitance, had been used for enormous distances—the longest was the 1500 km from New York to Chicago built in 1893. Loading coils were used to improve the distance achievable with cable but the problem was not finally overcome until amplifiers started to be installed in 1912. Twisted pair balanced lines are still widely used for local loops, the lines that connect each subscriber's premises to their respective exchange.
Telephone trunk lines, and especially frequency division multiplexing carrier systems, are usually 4-wire circuits rather than 2-wire circuits (or at least they were before fibre-optic became widespread) and require a different kind of cable. This format requires the conductors to be arranged in two pairs, one pair for the sending (go) signal and the other for the return signal. The greatest source of interference on this kind of transmission is usually the crosstalk between the go and return circuits themselves. The most common cable format is star quad, where the diagonally opposite conductors form the pairs. This geometry gives maximum common mode rejection between the two pairs. An alternative format is DM quad which consists of two twisted pairs with the twisting at different pitches.
Audio systems.
An example of balanced lines is the connection of microphones to a mixer in professional systems. Classically, both dynamic and condenser microphones used transformers to provide a differential-mode signal. While transformers are still used in the large majority of modern dynamic microphones, more recent condenser microphones are more likely to use electronic drive circuitry. Each leg, irrespective of any signal, should have an identical impedance to ground. Pair cable (or a pair-derivative such as star quad) is used to maintain the balanced impedances and close twisting of the cores ensures that any interference is common to both conductors. Providing that the receiving end (usually a mixing console) does not disturb the line balance, and is able to ignore common-mode (noise) signals, and can extract differential ones, then the system will have excellent immunity to induced interference.
Typical professional audio sources, such as microphones, have three-pin XLR connectors. One is the shield or chassis ground, while the other two are signal connections. These signal wires carry two copies of the same signal, but with opposite polarity. (They are often termed "hot" and "cold," and the AES14-1992(r2004) Standard EIA Standard RS-297-A suggest that the pin that carries the positive signal that results from a positive air pressure on a transducer will be deemed 'hot'. Pin 2 has been designated as the 'hot' pin, and that designation serves useful for keeping a consistent polarity in the rest of the system.) Since these conductors travel the same path from source to destination, the assumption is that any interference is induced upon both conductors equally. The appliance receiving the signals compares the difference between the two signals (often with disregard to electrical ground) allowing the appliance to ignore any induced electrical noise. Any induced noise would be present in equal amounts and in identical polarity on each of the balanced signal conductors, so the two signals’ difference from each other would be unchanged. The successful rejection of induced noise from the desired signal depends in part on the balanced signal conductors receiving the same amount and type of interference. This typically leads to twisted, braided, or co-jacketed cables for use in balanced signal transmission.
Balanced and differential.
Most explanations of balanced lines assume symmetric (antiphase) signals but this is an unfortunate confusion - signal symmetry and balanced lines are quite independent of each other. Essential in a balanced line is matched impedances in the driver, line and receiver. These conditions ensure that external noise affects each leg of the differential line equally and thus appears as a common mode signal that is removed by the receiver. There are balanced drive circuits that have excellent common-mode impedance matching between "legs" but do "not" provide symmetric signals. Symmetric differential signals exist to prevent interference "to" other circuits - the electromagnetic fields are canceled out by the equal and opposite currents. But they are not necessary for interference rejection "from" other circuits.
Baluns.
To convert a signal from balanced to unbalanced requires a balun. For example, baluns can be used to send line level audio or E-carrier level 1 signals over coaxial cable (which is unbalanced) through of Category 5 cable by using a pair of baluns at each end of the CAT5 run. The balun takes the unbalanced signal, and creates an inverted copy of that signal. It then sends these 2 signals across the CAT5 cable as a balanced, differential signal. Upon reception at the other end, the balun takes the difference of the two signals, thus removing any noise picked up along the way and recreating the unbalanced signal.
A once common application of a radio frequency balun was found at the antenna terminals of a television receiver. Typically a 300-ohm balanced twin lead antenna input could only be connected to a coaxial cable from a cable TV system through a balun.
Characteristic impedance.
The characteristic impedance formula_1 of a transmission line is an important parameter at higher frequencies of operation. For a parallel 2-wire transmission line,
where formula_3 is half the distance between the wire centres, formula_4 is the wire radius and formula_5, formula_6 are respectively the permeability and permittivity of the surrounding medium. A commonly used approximation that is valid when the wire separation is much larger than the wire radius and in the absence of magnetic materials is
where formula_8 is the relative permittivity of the surrounding medium.
Electric power lines.
In electric power transmission, the three conductors used for three-phase power transmission are referred to as a balanced line since the instantaneous sum of the three line voltages is nominally zero. However, "balance" in this field is referring to the symmetry of the source and load: it has nothing to do with the impedance balance of the line itself, the sense of the meaning in telecommunications.
For the transmission of single-phase electric power as used for railway electrification systems, two conductors are used to carry in-phase and out-of-phase voltages such that the line is balanced.
Bipolar HVDC lines at which each pole is operated with the same voltage toward ground are also balanced lines.

</doc>
<doc id="40768" url="https://en.wikipedia.org/wiki?curid=40768" title="Balance return loss">
Balance return loss

In telecommunications, balance return loss is one of two things:

</doc>
<doc id="40769" url="https://en.wikipedia.org/wiki?curid=40769" title="Balancing network">
Balancing network

In a hybrid set, hybrid coil, or resistance hybrid, balancing network is a circuit used to match, "i.e.", to balance, the impedance of a uniform transmission line, (e.g., a twisted metallic pair, coaxial cable, etc.) over a selected range of frequencies. A balancing network is required to ensure isolation between the two ports of the four-wire side of the hybrid.
A balancing network can also be a device used between a balanced device or line and an unbalanced device or line for the purpose of transforming from balanced to unbalanced or from unbalanced to balanced.
Source: from Federal Standard 1037C and from MIL-STD-188

</doc>
<doc id="40771" url="https://en.wikipedia.org/wiki?curid=40771" title="Bandwidth compression">
Bandwidth compression

In telecommunication, the term bandwidth compression has the following meanings: 
Bandwidth compression implies a reduction in normal bandwidth of an information-carrying signal without reducing the information content of the signal. This can be accomplished with lossless data compression techniques. For more information read the Increasing speeds section in the Modem article. Bandwidth Compression is a core feature of WAN Optimization appliances to improve bandwidth efficiency.
Bandwidth Compression Software.
Wanos Networks

</doc>
<doc id="40772" url="https://en.wikipedia.org/wiki?curid=40772" title="Barrage jamming">
Barrage jamming

Barrage jamming is radio jamming accomplished by transmitting a band of frequencies that is large with respect to the bandwidth of a single emitter. Barrage jamming may be accomplished by presetting multiple jammers on adjacent frequencies, by using a single wideband transmitter, or by using a transmitter capable of frequency sweep fast enough to appear radiating simultaneously over wide band (e.g. a carcinotron). Barrage jamming makes it possible to jam emitters on different frequencies simultaneously and reduces the need for operator assistance or complex control equipment. These advantages are gained at the expense of reduced jamming power at any given frequency. Barrage jamming has to be used against frequency-agile radars, which change frequencies too quickly to follow them in a conventional way. The use of barrage jamming may also affect the communications capability of the jamming source in a negative fashion.
Photomultiplier tubes were popularized during World War II since they could be used to as high bandwidth (up to several hundred MHz) noise sources. Although wide bandwidth sources typically suffer from low spectral power per unit frequency, the photomultiplier tube offers high gain (about formula_1) amplification of photon shot noise, making it advantageous over other lower gain noise sources.

</doc>
<doc id="40773" url="https://en.wikipedia.org/wiki?curid=40773" title="Baseband">
Baseband

Baseband is a signal that has a very narrow frequency range, i.e. a spectral magnitude that is nonzero only for frequencies in the vicinity of the origin (termed "f" = 0) and negligible elsewhere. In telecommunications and signal processing, baseband signals are transmitted without modulation, that is, without any shift in the range of frequencies of the signal, and are low frequency - contained within the band of frequencies from close to 0 hertz up to a higher cut-off frequency or maximum bandwidth. Baseband can be synonymous with lowpass or non-modulated, and is differentiated from passband, bandpass, carrier-modulated, intermediate frequency, or radio frequency (RF).
Various uses.
Baseband bandwidth.
A "baseband bandwidth" is equal to the highest frequency of a signal or system, or an upper bound on such frequencies, for example the upper cut-off frequency of a Lowpass filter. By contrast, passband bandwidth is the difference between a highest frequency and a nonzero lowest frequency.
Baseband channel.
A "baseband channel" or "lowpass channel" (or "system", or "network") is a communication channel that can transfer frequencies that are very near zero. Examples are serial cables and local area networks (LANs), as opposed to passband channels such as radio frequency channels and passband filtered wires of the analog telephone network. Frequency division multiplexing (FDM) allows an analog telephone wire to carry a baseband telephone call, concurrently as one or several carrier-modulated telephone calls.
Digital baseband transmission.
Digital baseband transmission, also known as line coding, aims at transferring a digital bit stream over baseband channel, typically an unfiltered wire, contrary to passband transmission, also known as "carrier-modulated" transmission. Passband transmission makes communication possible over a bandpass filtered channel, such as the telephone network local-loop or a band-limited wireless channel.
Baseband transmission in Ethernet.
The word "BASE" in Ethernet physical layer standards, for example 10BASE5, 100BASE-TX and 1000BASE-SX, implies baseband digital transmission (i.e. that a line code and an unfiltered wire are used).
Baseband processor.
A baseband processor also known as BP or BBP is used to process the down-converted digital signal to retrieve essential data for the wireless digital system. The baseband
processing block in receiver is usually responsible for providing observable data: code pseudo-ranges and carrier phase measurements, as well as navigation data. 
Baseband signal.
A "baseband signal" or "lowpass signal" is a signal that can include frequencies that are very near zero, by comparison with its highest frequency (for example, a sound waveform can be considered as a baseband signal, whereas a radio signal or any other modulated signal is not).
Equivalent baseband signal.
An "equivalent baseband signal" or "equivalent lowpass signal" is—in analog and digital modulation methods with constant carrier frequency (for example ASK, PSK and QAM, but not FSK)—a complex valued representation of the modulated physical signal (the so-called passband signal or RF signal). The equivalent baseband signal is formula_1 where formula_2 is the inphase signal, formula_3 the quadrature phase signal, and formula_4 the imaginary unit. In a digital modulation method, the formula_2 and formula_3 signals of each modulation symbol are evident from the constellation diagram. The frequency spectrum of this signal includes negative as well as positive frequencies. The physical passband signal corresponds to
Modulation.
A signal at baseband is often used to modulate a higher frequency carrier signal in order that it may be transmitted via radio. Modulation results in shifting the signal up to much higher frequencies (radio frequencies, or RF) than it originally spanned. A key consequence of the usual double-sideband amplitude modulation (AM) is that the range of frequencies the signal spans (its spectral bandwidth) is doubled. Thus, the RF bandwidth of a signal (measured from the lowest frequency as opposed to 0 Hz) is twice its baseband bandwidth. Steps may be taken to reduce this effect, such as single-sideband modulation. Some transmission schemes such as frequency modulation use even more bandwidth.
The figure shows what happens with AM modulation:

</doc>
<doc id="40775" url="https://en.wikipedia.org/wiki?curid=40775" title="Basic exchange telecommunications radio service">
Basic exchange telecommunications radio service

In telecommunication, a basic exchange telecommunications radio service (BETRS) is a commercial service that can extend telephone service to rural areas by replacing the local loop with radio communications. In the BETRS, non-government ultra high frequency (UHF) and very high frequency (VHF) common carrier and the private radio service frequencies are shared.

</doc>
<doc id="40776" url="https://en.wikipedia.org/wiki?curid=40776" title="Basic service">
Basic service

In telecommunications, basic service is: 

</doc>
<doc id="40777" url="https://en.wikipedia.org/wiki?curid=40777" title="Basic service element">
Basic service element

In telecommunication, a basic service element (BSE) is: 
BSEs constitute optional capabilities to which the customer may subscribe or decline to subscribe.

</doc>
<doc id="40778" url="https://en.wikipedia.org/wiki?curid=40778" title="Basic serving arrangement">
Basic serving arrangement

In telecommunication, the term basic serving arrangement (BSA) has the following meanings: 

</doc>
<doc id="40779" url="https://en.wikipedia.org/wiki?curid=40779" title="BCH code">
BCH code

In coding theory, the BCH codes form a class of cyclic error-correcting codes that are constructed using finite fields. BCH codes were invented in 1959 by French mathematician Alexis Hocquenghem, and independently in 1960 by Raj Bose and D. K. Ray-Chaudhuri. The acronym "BCH" comprises the initials of these inventors' names.
One of the key features of BCH codes is that during code design, there is a precise control over the number of symbol errors correctable by the code. In particular, it is possible to design binary BCH codes that can correct multiple bit errors. Another advantage of BCH codes is the ease with which they can be decoded, namely, via an algebraic method known as syndrome decoding. This simplifies the design of the decoder for these codes, using small low-power electronic hardware.
BCH codes are used in applications such as satellite communications, compact disc players, DVDs, disk drives, solid-state drives and two-dimensional bar codes.
Definition and illustration.
Primitive narrow-sense BCH codes.
Given a prime power and positive integers and with , a primitive narrow-sense BCH code over the finite field with code length and distance at least is constructed by the following method.
Let be a primitive element of .
For any positive integer , let be the minimal polynomial of over .
The generator polynomial of the BCH code is defined as the least common multiple .
It can be seen that is a polynomial with coefficients in and divides .
Therefore, the polynomial code defined by is a cyclic code.
Example.
Let and (therefore ). We will consider different values of . There is a primitive root in satisfying
its minimal polynomial over is
The minimal polynomials of the first seven powers of are
The BCH code with formula_6 has generator polynomial
formula_7
It has minimal Hamming distance at least 3 and corrects up to one error. Since the generator polynomial is of degree 4, this code has 11 data bits and 4 checksum bits.
The BCH code with formula_8 has generator polynomial
formula_9
It has minimal Hamming distance at least 5 and corrects up to two errors. Since the generator polynomial is of degree 8, this code has 7 data bits and 8 checksum bits.
The BCH code with formula_10 and higher has generator polynomial
formula_11
This code has minimal Hamming distance 15 and corrects 7 errors. It has 1 data bit and 14 checksum bits. In fact, this code has only two codewords: 000000000000000 and 111111111111111.
General BCH codes.
General BCH codes differ from primitive narrow-sense BCH codes in two respects.
First, the requirement that formula_12 be a primitive element of formula_13 can be relaxed. By relaxing this requirement, the code length changes from formula_14 to formula_15 the order of the element formula_16
Second, the consecutive roots of the generator polynomial may run from formula_17 instead of formula_18
Definition. Fix a finite field formula_19 where formula_20 is a prime power. Choose positive integers formula_21 such that formula_22 formula_23 and formula_24 is the multiplicative order of formula_20 modulo formula_26
As before, let formula_12 be a primitive formula_28th root of unity in formula_29 and let formula_30 be the minimal polynomial over formula_31 of formula_32 for all formula_33
The generator polynomial of the BCH code is defined as the least common multiple formula_34
Note: if formula_35 as in the simplified definition, then formula_36 is automatically 1, and the order of formula_20 modulo formula_28 is automatically formula_39
Therefore, the simplified definition is indeed a special case of the general one.
Special cases.
The generator polynomial formula_42 of a BCH code has coefficients from formula_43
In general, a cyclic code over formula_44 with formula_42 as the generator polynomial is called a BCH code over formula_46
The BCH code over formula_13 with formula_42 as the generator polynomial is called a Reed–Solomon code. In other words, a Reed–Solomon code is a BCH code where the decoder alphabet is the same as the channel alphabet.
Properties.
The generator polynomial of a BCH code has degree at most formula_49. Moreover, if formula_50 and formula_40, the generator polynomial has degree at most formula_52.
Each minimal polynomial formula_30 has degree at most formula_24.
Therefore, the least common multiple of formula_55 of them has degree at most formula_49.
Moreover, if formula_57 then formula_58 for all formula_59.
Therefore, formula_42 is the least common multiple of at most formula_61 minimal polynomials formula_30 for odd indices formula_63 each of degree at most formula_24.
A BCH code has minimal Hamming distance at least formula_65.
Suppose that formula_66 is a code word with fewer than formula_65 non-zero terms. Then
Recall that formula_17 are roots of formula_70 hence of formula_66.
This implies that formula_72 satisfy the following equations, for each formula_73:
In matrix form, we have
The determinant of this matrix equals
The matrix formula_77 is seen to be a Vandermonde matrix, and its determinant is
which is non-zero. It therefore follows that formula_79 hence formula_80
A BCH code is cyclic. 
A polynomial code of length formula_28 is cyclic if and only if its generator polynomial divides formula_82
Since formula_42 is the minimal polynomial with roots formula_84 it suffices to check that each of formula_17 is a root of formula_82
This follows immediately from the fact that formula_12 is, by definition, an formula_28th root of unity.
Decoding.
There are many algorithms for decoding BCH codes. The most common ones follow this general outline:
During some of these steps, the decoding algorithm may determine that the received vector has too many errors and cannot be corrected. For example, if an appropriate value of "t" is not found, then the correction would fail. In a truncated (not primitive) code, an error location may be out of range. If the received vector has more errors than the code can correct, the decoder may unknowingly produce an apparently valid message that is not the one that was sent.
Calculate the syndromes.
The received vector formula_89 is the sum of the correct codeword formula_90 and an unknown error vector formula_91
The syndrome values are formed by considering formula_89 as a polynomial and evaluating it at formula_93
Thus the syndromes are
for formula_95 to formula_96
Since formula_97 are the zeros of formula_70 of which
formula_99 is a multiple, formula_100
Examining the syndrome values thus isolates the error vector so one can begin to solve for it.
If there is no error, formula_101 for all formula_102
If the syndromes are all zero, then the decoding is done.
Calculate the error location polynomial.
If there are nonzero syndromes, then there are errors. The decoder needs to figure out how many errors and the location of those errors.
If there is a single error, write this as formula_103
where formula_59 is the location of the error and formula_105 is its magnitude. Then the first two syndromes are
so together they allow us to calculate formula_105 and provide some information about formula_59 (completely determining it in the case of Reed–Solomon codes).
If there are two or more errors,
It is not immediately obvious how to begin solving the resulting syndromes for the unknowns formula_111 and formula_112
First step is finding locator polynomial
Two popular algorithms for this task are:
Peterson–Gorenstein–Zierler algorithm.
Peterson's algorithm is the step 2 of the generalized BCH decoding procedure. Peterson's algorithm is used to calculate the error locator polynomial coefficients formula_115 of a polynomial
Now the procedure of the Peterson–Gorenstein–Zierler algorithm. Expect we have at least 2"t" syndromes "s""c"...,"s""c"+2"t"−1.
Let "v" = "t".
Factor error locator polynomial.
Now that you have the formula_131 polynomial, its roots can be found in the form formula_132 by brute force for example using the Chien search algorithm. The exponential
powers of the primitive element formula_12 will yield the positions where errors occur in the received word; hence the name 'error locator' polynomial.
The zeros of Λ("x") are "α"−"i"1, ..., "α"−"i""v".
Calculate error values.
Once the error locations are known, the next step is to determine the error values at those locations. The error values are then used to correct the received values at those locations to recover the original codeword.
For the case of binary BCH, (with all characters readable) this is trivial; just flip the bits for the received word at these positions, and we have the corrected code word. In the more general case, the error weights formula_134 can be determined by solving the linear system
Forney algorithm.
However, there is a more efficient method known as the Forney algorithm.
Let
And the error evaluator polynomial
Finally:
where
Than if syndromes could be explained by an error word, which could be nonzero only on positions formula_141, then error values are
For narrow-sense BCH codes, "c" = 1, so the expression simplifies to:
Explanation of Forney algorithm computation.
It is based on Lagrange interpolation and techniques of generating functions.
Consider formula_144 and for the sake of simplicity suppose formula_145 for formula_146 and formula_147 for formula_148 Then
We want to compute unknowns formula_151 and we could simplify the context by removing the formula_152 terms. This leads to the error evaluator polynomial
Thanks to formula_154 we have
Thanks to formula_121 (the Lagrange interpolation trick) the sum degenerates to only one summand for formula_157
To get formula_111 we just should get rid of the product. We could compute the product directly from already computed roots formula_160 of formula_161 but we could use simpler form.
As formal derivative
we get again only one summand in
So finally
This formula is advantageous when one computes the formal derivative of formula_121 form
yielding:
where
Decoding based on extended Euclidean algorithm.
An alternate process of finding both the polynomial Λ and the error locator polynomial is based on Yasuo Sugiyama's adaptation of the Extended Euclidean algorithm. Correction of unreadable characters could be incorporated to the algorithm easily as well.
Let formula_169 be positions of unreadable characters. One creates polynomial localising these positions formula_170
Set values on unreadable positions to 0 and compute the syndromes.
As we have already defined for the Forney formula let formula_171
Let us run extended Euclidean algorithm for locating least common divisor of polynomials formula_172 and formula_173
The goal is not to find the least common divisor, but a polynomial formula_174 of degree at most formula_175 and polynomials formula_176 such that formula_177
Low degree of formula_174 guarantees, that formula_179 would satisfy extended (by formula_180) defining conditions for formula_181
Defining formula_182 and using formula_183 on the place of formula_131 in the Fourney formula will give us error values.
The main advantage of the algorithm is that it meanwhile computes formula_185 required in the Forney formula.
Explanation of the decoding process.
The goal is to find a codeword which differs from the received word minimally as possible on readable positions. When expressing the received word as a sum of nearest codeword and error word, we are trying to find error word with minimal number of non-zeros on readable positions. Syndrom formula_186 restricts error word by condition
We could write these conditions separately or we could create polynomial
and compare coefficients near powers formula_189 to formula_190
Suppose there is unreadable letter on position formula_192 we could replace set of syndromes formula_193 by set of syndromes formula_194 defined by equation formula_195 Suppose for an error word all restrictions by original set formula_193 of syndromes hold,
than
New set of syndromes restricts error vector
the same way the original set of syndromes restricted the error vector formula_199 Note, that except the coordinate formula_192 where we have formula_201 an formula_202 is zero, if formula_203 For the goal of locating error positions we could change the set of syndromes in the similar way to reflect all unreadable characters. This shortens the set of syndromes by formula_204
In polynomial formulation, the replacement of syndromes set formula_193 by syndromes set formula_194 leads to
Therefore
After replacement of formula_209 by formula_172, one would require equation for coefficients near powers formula_211
One could consider looking for error positions from the point of view of eliminating influence of given positions similarly as for unreadable characters. If we found formula_212 positions such that eliminating their influence leads to obtaining set of syndromes consisting of all zeros, than there exists error vector with errors only on these coordinates.
If formula_131 denotes the polynomial eliminating the influence of these coordinates, we obtain
In Euclidean algorithm, we try to correct at most formula_215 errors (on readable positions), because with bigger error count there could be more codewords in the same distance from the received word. Therefore, for formula_131 we are looking for, the equation must hold for coefficients near powers starting from
In Forney formula, formula_131 could be multiplied by a scalar giving the same result.
It could happen that the Euclidean algorithm finds formula_131 of degree higher than formula_215 having number of different roots equal to its degree, where the Fourney formula would be able to correct errors in all its roots, anyways correcting such many errors could be risky (especially with no other restrictions on received word). Usually after getting formula_131 of higher degree, we decide not to correct the errors. Correction could fail in the case formula_131 has roots with higher multiplicity or the number of roots is smaller than its degree. Fail could be detected as well by Forney formula returning error outside the transmitted alphabet.
Correct the errors.
Using the error values and error location, correct the errors and form a corrected code vector by subtracting error values at error locations.
Decoding examples.
Decoding of binary code without unreadable characters.
Consider a BCH code in GF(24) with formula_223 and formula_224. (This is used in QR codes.) Let the message to be transmitted be 1 0 1 1, or in polynomial notation, formula_225
The "checksum" symbols are calculated by dividing formula_226 by formula_42 and taking the remainder, resulting in formula_228 or [ 1 0 0 0 0 1 0 1 0 0 ]. These are appended to the message, so the transmitted codeword is [ 1 1 0 1 1 1 0 0 0 0 1 0 1 0 0 ].
Now, imagine that there are two bit-errors in the transmission, so the received codeword is [ 1 0 1 1 1 0 0 0 1 0 1 0 0 ]. In polynomial notation:
In order to correct the errors, first calculate the syndromes. Taking formula_230 we have formula_231 formula_232 formula_233 formula_234 formula_235 and formula_236
Next, apply the Peterson procedure by row-reducing the following augmented matrix.
Due to the zero row, is singular, which is no surprise since only two errors were introduced into the codeword.
However, the upper-left corner of the matrix is identical to , which gives rise to the solution formula_238 formula_239
The resulting error locator polynomial is formula_240 which has zeros at formula_241 and formula_242
The exponents of formula_12 correspond to the error locations.
There is no need to calculate the error values in this example, as the only possible value is 1.
Decoding with unreadable characters.
Suppose the same scenario, but the received word has two unreadable characters [ 1 0 ? 1 1 ? 0 0 1 0 1 0 0 ]. We replace the unreadable characters by zeros while creating the polynom reflecting their positions formula_244 We compute the syndromes formula_245 and formula_246 (Using log notation which is independent on GF(24) isomorphisms. For computation checking we can use the same representation for addition as was used in previous example. Hexadecimal description of the powers of formula_12 are consecutively 1,2,4,8,3,6,C,B,5,A,7,E,F,D,9 with the addition based on bitwise xor.)
Let us make syndrome polynomial
compute
Run the extended Euclidean algorithm:
We have reached polynomial of degree at most 3, and as
we get
Therefore
Let formula_254 Don't worry that formula_255 Find by brute force a root of formula_181 The roots are formula_257 and formula_258 (after finding for example formula_259 we can divide formula_121 by corresponding monom formula_261 and the root of resulting monom could be found easily).
Let
Let us look for error values using formula
where formula_160 are roots of formula_266 formula_267 We get
Fact, that formula_269 should not be surprising.
Corrected code is therefore [ 1 0 1 1 0 0 1 0 1 0 0].
Decoding with unreadable characters with a small number of errors.
Let us show the algorithm behaviour for the case with small number of errors. Let the received word is [ 1 0 ? 1 1 ? 0 0 0 1 0 1 0 0 ].
Again, replace the unreadable characters by zeros while creating the polynom reflecting their positions formula_244
Compute the syndromes formula_271 and formula_272
Create syndrome polynomial
Let us run the extended Euclidean algorithm:
formula_275
We have reached polynomial of degree at most 3, and as
we get
Therefore,
Let formula_279 Don't worry that formula_255 The root of formula_131 is formula_282
Let 
Let us look for error values using formula formula_285 where formula_160 are roots of polynomial formula_266
formula_288
We get
The fact that formula_290 should not be surprising.
Corrected code is therefore [ 1 0 1 1 0 0 0 1 0 1 0 0].

</doc>
<doc id="40780" url="https://en.wikipedia.org/wiki?curid=40780" title="Beam diameter">
Beam diameter

The beam diameter or beam width of an electromagnetic beam is the diameter along any specified line that is perpendicular to the beam axis and intersects it. Since beams typically do not have sharp edges, the diameter can be defined in many different ways. Five definitions of the beam width are in common use: D4σ, 10/90 or 20/80 knife-edge, 1/e2, FWHM, and D86. The beam width can be measured in units of length at a particular plane perpendicular to the beam axis, but it can also refer to the angular width, which is the angle subtended by the beam at the source. The angular width is also called the beam divergence.
Beam diameter is usually used to characterize electromagnetic beams in the optical regime, and occasionally in the microwave regime, that is, cases in which the aperture from which the beam emerges is very large with respect to the wavelength.
Beam diameter usually refers to a beam of circular cross section, but not necessarily so. A beam may, for example, have an elliptical cross section, in which case the orientation of the beam diameter must be specified, for example with respect to the major or minor axis of the elliptical cross section. The term "beam width" may be preferred in applications where the beam does not have circular symmetry.
Width definitions.
Rayleigh beamwidth.
The angle between the maximum peak of radiated power and the first null (no power radiated in this direction) is called the Rayleigh beamwidth.
Full width at half maximum.
The simplest way to define the width of a beam is to choose two diametrically opposite points at which the irradiance is a specified fraction of the beam's peak irradiance, and take the distance between them as a measure of the beam's width. An obvious choice for this fraction is ½ (−3 dB), in which case the diameter obtained is the full width of the beam at half its maximum intensity (FWHM). This is also called the "half-power beam width" (HPBW).
1/e2 width.
The 1/e2 width is equal to the distance between the two points on the marginal distribution that are 1/e2 = 0.135 times the maximum value. In many cases, it makes more sense to take the distance between points where the intensity falls to 1/e2 = 0.135 times the maximum value. If there are more than two points that are 1/e2 times the maximum value, then the two points closest to the maximum are chosen. The 1/e2 width is important in the mathematics of Gaussian beams.
The American National Standard Z136.1-2007 for Safe Use of Lasers (p. 6) defines the beam diameter as the distance between diametrically opposed points in that cross-section of a beam where the power per unit area is 1/e (0.368) times that of the peak power per unit area. This is the beam diameter definition that is used for computing the maximum permissible exposure to a laser beam. In addition, the Federal Aviation Administration also uses the 1/e definition for laser safety calculations in FAA Order 7400.2F, "Procedures for Handling Airspace Matters," February 16, 2006, p. 29-1-2.
Measurements of the 1/e2 width only depend on three points on the marginal distribution, unlike D4σ and knife-edge widths that depend on the integral of the marginal distribution. 1/e2 width measurements are noisier than D4σ width measurements. For multimodal marginal distributions (a beam profile with multiple peaks), the 1/e2 width usually does not yield a meaningful value and can grossly underestimate the inherent width of the beam. For multimodal distributions, the D4σ width is a better choice. For an ideal single-mode Gaussian beam, the D4σ, D86 and 1/e2 width measurements would give the same value.
For a Gaussian beam, the relationship between the 1/e2 width and the full width at half maximum is formula_1, where formula_2 is the full width of the beam at 1/e2.
D4σ or second moment width.
The D4σ width of a beam in the horizontal or vertical direction is 4 times σ, where σ is the standard deviation of the horizontal or vertical marginal distribution, respectively. Mathematically, the D4σ beam width in the x-dimension for the beam profile formula_3 is expressed as
where
is the centroid of the beam profile in the x-direction.
When a beam is measured with a laser beam profiler, the wings of the beam profile influence the D4σ value more than the center of the profile since the wings are weighted by the square of its distance, "x"2, from the center of the beam. If the beam does not fill more than a third of the beam profiler’s sensor area, then there will be a significant number of pixels at the edges of the sensor that register a small baseline value (the background value). If the baseline value is large or if it is not subtracted out of the image, then the computed D4σ value will be larger than the actual value because the baseline value near the edges of the sensor are weighted in the D4σ integral by "x"2. Therefore, baseline subtraction is necessary for accurate D4σ measurements. The baseline is easily measured by recording the average value for each pixel when the sensor is not illuminated. The D4σ width, unlike the FWHM and 1/e2 widths, is meaningful for multimodal marginal distributions — that is, beam profiles with multiple peaks — but requires careful subtraction of the baseline for accurate results. The D4σ is the ISO international standard definition for beam width.
Knife-edge width.
Before the advent of the CCD beam profiler, the beam width was estimated using the knife-edge technique: slice a laser beam with a razor and measure the power of the clipped beam as a function of the razor position. The measured curve is the integral of the marginal distribution, and starts at the total beam power and decreases monotonically to zero power. The width of the beam is defined as the distance between the points of the measured curve that are 10% and 90% (or 20% and 80%) of the maximum value. If the baseline value is small or subtracted out, the knife-edge beam width always corresponds to 60%, in the case of 20/80, or 80%, in the case of 10/90, of the total beam power no matter what the beam profile. On the other hand, the D4σ, 1/e2, and FWHM widths encompass fractions of power that are beam-shape dependent. Therefore, the 10/90 or 20/80 knife-edge width is a useful metric when the user wishes to be sure that the width encompasses a fixed fraction of total beam power. Most CCD beam profiler's software can compute the knife-edge width numerically.
Fusing Knife-edge Technology with Imaging.
The main drawback of the Knife-edge technique is that the measured value is displayed only on the scanning direction, minimizing the amount of relevant beam information. To overcome this drawback, an innovative technology offered commercially allows multiple directions beam scanning to create an image like beam representation.
By mechanically moving the knife edge across the beam, the amount of energy inpinging the detector area is determined by the obstruction. The profile is then measured from the knife-edge velocity and its relation to the detector’s energy reading. Unlike other systems, a unique scanning technique uses several different oriented knife-edges to sweep across the beam. By using tomographic reconstruction, mathematical processes reconstruct the laser beam size in different orientations to an image similar to the one produced by CCD cameras. The main advantage of this scanning method is that it's free from pixel size limitations (like in CCD cameras) and allows beam reconstructions with wavelengths which are impossible with existing CCD technology. Reconstruction is possible for beams in deep UV to far IR. Pioneering this technology is attributed to Duma Optronics.
D86 width.
The D86 width is defined as the diameter of the circle that is centered at the centroid of the beam profile and contains 86% of the beam power. The solution for D86 is found by computing the area of increasingly larger circles around the centroid until the area contains 0.86 of the total power. Unlike the previous beam width definitions, the D86 width is not derived from marginal distributions. The percentage of 86, rather than 50, 80, or 90, is chosen because a circular Gaussian beam profile integrated down to 1/e2 of its peak value contains 86% of its total power. The D86 width is often used in applications that are concerned with knowing exactly how much power is in a given area. For example, applications of high-energy laser weapons and lidars require precise knowledge of how much transmitted power actually illuminates the target.
ISO11146 beam width for elliptic beams.
The definition given before holds for stigmatic (circular symmetric) beams only. For astigmatic beams however, a more rigorous definition of the beam width has to be used, 
and
This definition also incorporates information about x-y-correlation formula_8, but for circular symmetric beams, both definitions are the same.
Some new symbols appeared within the formulas, which are the first- and second-order moments
the beam power 
and 
Using this general definition, also the beam's azimutal-angle formula_16 can be expressed. It is the angle between the beam's directions of minimum and maximum elongation, known as principal axis, and the laboratory system, being the formula_17- and formula_18-axis of the detector and given by 
Measurement.
International standard ISO 11146-1:2005 specifies methods for measuring beam widths (diameters), divergence angles and beam propagation ratios of laser beams (if the beam is stigmatic) and for general astigmatic beams ISO 11146-2 is applicable. The D4σ beam width is the ISO standard definition and the measurement of the M² beam quality parameter requires the measurement of the D4σ widths.
The other definitions provide complementary information to the D4σ. The D4σ and knife-edge widths are sensitive to the baseline value, whereas the 1/e2 and FWHM widths are not. The fraction of total beam power encompassed by the beam width depends on which definition is used.
The width of laser beams can be measured by capturing an image on a camera, or by using a laser beam profiler.

</doc>
<doc id="40781" url="https://en.wikipedia.org/wiki?curid=40781" title="Beam divergence">
Beam divergence

The beam divergence of an electromagnetic beam is an angular measure of the increase in beam diameter or radius with distance from the optical aperture or antenna aperture from which the electromagnetic beam emerges. The term is relevant only in the "far field", away from any focus of the beam. Practically speaking, however, the far field can commence physically close to the radiating aperture, depending on aperture diameter and the operating wavelength.
Beam divergence is often used to characterize electromagnetic beams in the optical regime, for cases in which the aperture from which the beam emerges is very large with respect to the wavelength. However, it is also used in the Radio Frequency (RF) regime for cases in which the antenna is operating in the so-called optical region and is likewise very large relative to a wavelength.
Beam divergence usually refers to a beam of circular cross section, but not necessarily so. A beam may, for example, have an elliptical cross section, in which case the orientation of the beam divergence must be specified, for example with respect to the major or minor axis of the elliptical cross section.
The divergence of a beam can be calculated if one knows the beam diameter at two separate points far from any focus ("Di", "Df"), and the distance ("l") between these points. The beam divergence, formula_1, is given by 
If a collimated beam is focused with a lens, the diameter formula_3 of the beam in the rear focal plane of the lens is related to the divergence of the initial beam by 
where "f" is the focal length of the lens.
Like all electromagnetic beams, lasers are subject to divergence, which is measured in milliradians (mrad) or degrees. For many applications, a lower-divergence beam is preferable. Neglecting divergence due to poor beam quality, the divergence of a laser beam is proportional to its wavelength and inversely proportional to the diameter of the beam at its narrowest point. For example, an ultraviolet laser that emits at a wavelength of 308 nm will have a lower divergence than an infrared laser at 808 nm, if both have the same minimum beam diameter. The divergence of good-quality laser beams is modeled using the mathematics of Gaussian beams.
Gaussian laser beams are said to be diffraction limited when their radial beam divergence formula_5 is close to the minimum possible value, which is given by 
where formula_7 is the laser wavelength and formula_8 is the radius of the beam at its narrowest point, which is called the "beam waist". This type of beam divergence is observed from optimized laser cavities. Information on the diffraction-limited divergence of a coherent beam is inherently given by the N-slit interferometric equation.

</doc>
<doc id="40782" url="https://en.wikipedia.org/wiki?curid=40782" title="Beam steering">
Beam steering

Beam steering (also spelled beamsteering or beam-steering) is about changing the direction of the main lobe of a radiation pattern.
In radio systems, beam steering may be accomplished by switching the antenna elements or by changing the relative phases of the RF signals driving the elements.
In acoustics, beam steering is used to direct the audio from loudspeakers to a specific location in the listening area. This is done by changing the magnitude and phase of two or more loudspeakers installed in a column where the combined sound is added and cancelled at the required position. Commercially, this type of loudspeaker arrangement is known as a line array. This technique has been around for many years but since the emergence of modern DSP (Digital Signal Processing) technology there are now many commercially available products on the market. Beam Steering and Directivity Control using DSP was pioneered in the early 1990s by Duran Audio who launched a technology called DDC (Digital Directivity Control).
In optical systems, beam steering may be accomplished by changing the refractive index of the medium through which the beam is transmitted or by the use of mirrors, prisms, lenses, or rotating diffraction gratings. Examples of optical beam steering approaches include mechanical mirror-based gimbals or beam-director units, galvanometer mechanisms that rotate mirrors, Risley prisms, phased-array optics, and microelectromechanical systems (MEMS) using micro-mirrors.
Source: from Federal Standard 1037C

</doc>
<doc id="40783" url="https://en.wikipedia.org/wiki?curid=40783" title="Beamwidth">
Beamwidth

In telecommunication, the term beamwidth has the following meanings:
1. In a radio antenna pattern, the half power beam width is the angle between the half-power (-3 dB) points of the main lobe, when referenced to the peak effective radiated power of the main lobe. See beam diameter.
Beamwidth is usually but not always expressed in degrees and for the horizontal plane.
2. For the optical regime, "see" beam divergence.

</doc>
<doc id="40785" url="https://en.wikipedia.org/wiki?curid=40785" title="Bel">
Bel

Bel can mean:

</doc>
<doc id="40786" url="https://en.wikipedia.org/wiki?curid=40786" title="Bias">
Bias

Bias is an inclination or outlook to present or hold a partial perspective, often accompanied by a refusal to consider the possible merits of alternative points of view. Biases are learned implicitly within cultural contexts. People may develop biases toward or against an individual, an ethnic group, a nation, a religion, a social class, a political party, theoretical paradigms and ideologies within academic domains, or a species. Biased means one-sided, lacking a neutral viewpoint, or not having an open mind. Bias can come in many forms and is related to prejudice and intuition.
In science and engineering, a bias is a systematic error.
Cognitive biases.
A cognitive bias is a repeating or basic misstep in thinking, assessing, recollecting, or other cognitive processes. That is, a pattern of deviation from standards in judgment, whereby inferences may be created unreasonably. People create their own "subjective social reality" from their own perceptions, their view of the world may dictate their behaviour. Thus, cognitive biases may sometimes lead to perceptual distortion, inaccurate judgment, illogical interpretation, or what is broadly called irrationality. However some cognitive biases are taken to be adaptive, and thus may lead to success in the appropriate situation. Furthermore, cognitive biases may allow speedier choices when speed is more valuable than precision. Other cognitive biases are a "by-product" of human processing limitations, coming about because of an absence of appropriate mental mechanisms, or just from human limitations in information processing.
Anchoring.
Anchoring is a psychological heuristic that describes the propensity to rely on the first piece of information encountered when making decisions. According to this heuristic, individuals begin with an implicitly suggested reference point (the "anchor") and make adjustments to it to reach their estimate. For example, the initial price offered for a used car sets the standard for the rest of the negotiations, so that prices lower than the initial price seem more reasonable even if they are still higher than what the car is worth.
Apophenia.
Apophenia, also known as patternicity, or agenticity, is the human tendency to perceive meaningful patterns within random data. Apophenia is well documented as a rationalization for gambling. Gamblers may imagine that they see patterns in the numbers which appear in lotteries, card games, or roulette wheels. One variation of this is known as the "gambler's fallacy".
Pareidolia is the visual or auditory form of apophenia. It has been suggested that pareidolia combined with hierophany may have helped ancient societies organize chaos and make the world intelligible.
Attribution bias.
An attribution bias can happen when individuals assess or attempt to discover explanations behind their own and others' behaviors. People make attributions about the causes of their own and others’ behaviors; but these attributions don't necessarily precisely reflect reality. Rather than operating as objective perceivers, individuals are inclined to perceptual slips that prompt biased understandings of their social world. When judging others we tend to assume their actions are the result of internal factors such as personality, whereas we tend to assume our own actions arise because of the necessity of external circumstances. There are a wide range of sorts of attribution biases, such as the ultimate attribution error, fundamental attribution error, actor-observer bias, and self-serving bias.
Confirmation bias.
Confirmation bias is the tendency to search for, interpret, favor, and recall information in a way that confirms one's beliefs or hypotheses while giving disproportionately less attention to information that contradicts it. The effect is stronger for emotionally charged issues and for deeply entrenched beliefs. People also tend to interpret ambiguous evidence as supporting their existing position. Biased search, interpretation and memory have been invoked to explain attitude polarization (when a disagreement becomes more extreme even though the different parties are exposed to the same evidence), belief perseverance (when beliefs persist after the evidence for them is shown to be false), the irrational primacy effect (a greater reliance on information encountered early in a series) and illusory correlation (when people falsely perceive an association between two events or situations). Confirmation biases contribute to overconfidence in personal beliefs and can maintain or strengthen beliefs in the face of contrary evidence. Poor decisions due to these biases have been found in political and organizational contexts.
Framing.
Framing involves the social construction of social phenomena by mass media sources, political or social movements, political leaders, and so on. It is an influence over how people organize, perceive, and communicate about reality. It can be positive or negative - depending on the audience and what kind of information is being presented. For political purposes, framing often presents facts in such a way that implicates a problem that is in need of a solution. Members of political parties attempt to frame issues in a way that makes a solution favoring their own political leaning appear as the most appropriate course of action for the situation at hand. As understood in social theory, framing is a schema of interpretation, a collection of anecdotes and stereotypes, that individuals rely on to understand and respond to events. People use filters to make sense of the world, the choices they then make are influenced by their creation of a frame.
Cultural bias is the related phenomenon of interpreting and judging phenomena by standards inherent to one's own culture. Numerous such biases exist, concerning cultural norms for color, location of body parts, mate selection, concepts of justice, linguistic and logical validity, acceptability of evidence, and taboos. Ordinary people may tend to imagine other people as basically the same, not significantly more or less valuable, probably attached emotionally to different groups and different land.
Halo effect.
Halo effect is when an observer's overall impression of a person, organization, brand, or product influences their feelings about that entity's character or properties. The halo effect is a specific type of confirmation bias, wherein positive sentiments in one area cause questionable or impartial characteristics to be seen positively. The halo effect works in negative directions, sometimes known as the horns effect. So that, if the observer likes one aspect of something, they will have a positive predisposition toward everything about it. If the observer dislikes one aspect of something, they will have a negative predisposition toward everything about it. A person’s appearance has been found to produce a halo effect. The halo effect is also present in the field of brand marketing, affecting perception of companies and non-governmental organizations (NGOs).
Self-serving bias.
Self-serving bias is the tendency for cognitive or perceptual processes to be distorted by the individual's need to maintain and enhance self-esteem. It is the propensity to credit accomplishment to our own capacities and endeavors, yet attribute failure to outside factors, to dismiss the legitimacy of negative criticism, concentrate on positive qualities and accomplishments yet disregard flaws and failures. Studies have demonstrated that this bias can affect behavior in the workplace, in interpersonal relationships, playing sports, and in consumer decisions.
Conflicts of interest.
A conflict of interest (COI) is when a person or association has intersecting interests (financial, personal, etcetera) which could potentially corrupt. The potential conflict is autonomous of actual improper actions, it can be found and intentionally defused before corruption, or the appearance of corruption, happens. "A conflict of interest is a set of circumstances that creates a risk that professional judgement or actions regarding a primary interest will be unduly influenced by a secondary interest." It exists if the circumstances are sensibly accepted to present a hazard that choices made may be unduly impacted by auxiliary interests.
Bribery.
Bribery is the giving of money, goods or other forms of recompense to in order to influence the recipient's behavior. Bribes can include money (including tips), goods, rights in action, property, privilege, emolument, gifts, perks, skimming, return favors, discounts, sweetheart deals, kickbacks, funding, donations, campaign contributions, sponsorships, stock options, secret commissions, or promotions. Expectations of when a monetary transaction is appropriate can differ from place to place. Political campaign contributions in the form of cash are considered criminal acts of bribery in some countries, while in the United States they are legal provided they adhere to election law. Tipping, is considered bribery in some societies, but not others.
Favoritism.
Favoritism, sometimes known as in-group favoritism, or in-group bias, refers to a pattern of favoring members of one's in-group over out-group members. This can be expressed in evaluation of others, in allocation of resources, and in many other ways. This has been researched by psychologists, especially social psychologists, and linked to group conflict and prejudice. Cronyism is favoritism of long-standing friends, especially by appointing them to positions of authority, regardless of their qualifications. Nepotism is favoritism granted to relatives.
Funding bias.
Funding bias refers to the tendency of a scientific study to support the interests of the study's financial sponsor. This phenomenon is recognized sufficiently that researchers undertake studies to examine bias in past published studies. It can be caused by any or all of: a conscious or subconscious sense of obligation of researchers towards their employers, misconduct or malpractice, publication bias, or reporting bias.
Insider trading.
Insider trading is the trading of a public company's stock or other securities (such as bonds or stock options) by individuals with access to non-public information about the company. In various countries, trading based on insider information is illegal because it is seen as unfair to other investors who do not have access to the information as the investor with insider information could potentially make far larger profits that a typical investor could not make. Some claim that illegal insider trading raises the cost of capital for securities issuers, thus decreasing overall economic growth, others have argued that insider trading should be allowed and could, in fact, benefit markets.
Lobbying.
Lobbying is the attempt to impact choices made by administrators, frequently lawmakers or individuals from administrative agencies. Lobbyists may be among a legislator's constituencies, or not; they may engage in lobbying as a business, or not. Lobbying is often spoken of with contempt, the implication is that people with inordinate socioeconomic power are corrupting the law in order to serve their own interests. When people who have a duty to act on behalf of others, such as elected officials with a duty to serve their constituents' interests or more broadly the common good, stand to benefit by shaping the law to serve the interests of some private parties, there is a conflict of interest. This can lead to all sides in a debate looking to sway the issue by means of lobbyists.
Match fixing.
In organized sports, match fixing occurs when a match is played to a completely or partially pre-determined result, violating the rules of the game and often the law. There is a variety of reasons for this, but the most common is in exchange for a payoff from gamblers. Players might also intentionally perform poorly to get an advantage in the future (such as a better draft pick, or an easier opponent in a playoff), or to rig a handicap system. Match-fixing generally refers to fixing the final result of the game. Another form of match-fixing, known as spot-fixing, involves fixing small events within a match which can be gambled upon, but which are unlikely to prove decisive in determining the final result of the game.
Regulatory issues.
Self-regulation is the process whereby an organization monitors its own adherence to legal, ethical, or safety standards, rather than have an outside, independent agency such as a third party entity monitor and enforce those standards. Self-regulation of any group can create a conflict of interest. If any organization, such as a corporation or government bureaucracy, is asked to eliminate unethical behavior within their own group, it may be in their interest in the short run to eliminate the appearance of unethical behavior, rather than the behavior itself.
Regulatory capture is a form of political corruption that can occur when a regulatory agency, created to act in the public interest, instead advances the commercial or political concerns of special interest groups that dominate the industry or sector it is charged with regulating. Regulatory capture occurs because groups or individuals with a high-stakes interest in the outcome of policy or regulatory decisions can be expected to focus their resources and energies in attempting to gain the policy outcomes they prefer, while members of the public, each with only a tiny individual stake in the outcome, will ignore it altogether. Regulatory capture is a risk to which a regulatory agency is exposed by its very nature.
Shilling.
Shilling is deliberately giving spectators the feeling that one is an energetic autonomous client of a vendor for whom one is working. The effectiveness of shilling relies on crowd psychology to encourage other onlookers or audience members to purchase the goods or services (or accept the ideas being marketed). Shilling is illegal in some places, but legal in others. An example of shilling is paid reviews that give the impression of being autonomous opinions.
Statistical biases.
A statistical bias is a method of calculating a statistic which produces a consistent error.
Contextual biases.
Academic bias.
Academic bias is the bias or perceived bias of scholars allowing their beliefs to shape their research and the scientific community. Claims of bias are often linked to claims by conservatives of pervasive bias against political conservatives and religious Christians. Some have argued that these claims are based upon anecdotal evidence which would not reliably indicate systematic bias, and have suggested that this divide is due to self-selection of conservatives choosing not to pursue academic careers. 
There is some evidence that perception of classroom bias may be rooted in issues of sexuality, race, class and sex as much or more than in religion.
Educational bias.
Bias in education refers to real or perceived bias in the educational system. The content of school textbooks is often the issue of debate, as their target audience is young people, and the term "whitewashing" is used to refer to selective removal of critical or damaging evidence or comment. Religious bias in textbooks is observed in countries where religion plays a dominant role. There can be many forms of educational bias. Some overlooked aspects, occurring especially with the pedagogical circles of public and private schools—sources that are unrelated to fiduciary or mercantile impoverishment which may be unduly magnified—include teacher bias as well as a general bias against women who are going into STEM research.
Experimenter bias.
In science research, experimenter bias occurs when experimenter expectancies regarding study results bias the research outcome. Examples of experimenter bias include conscious or unconscious influences on subject behavior including creation of demand characteristics that influence subjects, and altered or selective recording of experimental results themselves.
Full text on net bias.
Full text on net (or FUTON) bias is a tendency of scholars to cite academic journals with open access—that is, journals that make their full text available on the internet without charge—in their own writing as compared with toll access publications. Scholars can more easily discover and access articles that have their full text on the internet, which increases authors' likelihood of reading, quoting, and citing these articles, this may increase the impact factor of open access journals relative to journals without open access.
The related bias, no abstract available bias (NAA bias) is scholars' tendency to cite journal articles that have an abstract available online more readily than articles that do not.
Inductive bias.
Inductive bias occurs within the field of machine learning. In machine learning one seeks to develop algorithms that are able to "learn" to anticipate a particular output. To accomplish this, the learning algorithm is given training cases that show the expected connection. Then the learner is tested with new examples. Without further assumptions, this problem cannot be solved exactly as unknown situations may not be predictable. The inductive bias of the learning algorithm is the set of assumptions that the learner uses to predict outputs given inputs that it has not encountered. It may bias the learner towards the correct solution, the incorrect, or be correct some of the time. A classical example of an inductive bias is Occam's Razor, which assumes that the simplest consistent hypothesis is the best.
Media bias.
Media bias is the bias or perceived bias of journalists and news producers within the mass media in the selection of events, the stories that are reported, and how they are covered. The term generally implies a pervasive or widespread bias violating the standards of journalism, rather than the perspective of an individual journalist or article. There are varied forms of media bias, including agenda-setting, gatekeeping, sensationalism, and others. The level of media bias in different nations is debated. There are also watchdog groups that report on media bias.
Practical limitations to media neutrality include the inability of journalists to report all available stories and facts, the requirement that selected facts be linked into a coherent narrative, government influence including overt and covert censorship, the influence of the owners of the news source, concentration of media ownership, the selection of staff, the preferences of an intended audience, and pressure from advertisers.
Bias has been a feature of the mass media since its birth with the invention of the printing press. The expense of early printing equipment restricted media production to a limited number of people. Historians have found that publishers often served the interests of powerful social groups.
Publication bias.
Publication bias is a type of bias with regard to what academic research is likely to be published because of a tendency of researchers, and journal editors, to prefer some outcomes rather than others e.g. results showing a significant finding, leads to a problematic bias in the published literature. This can propagate further as literature reviews of claims about support for a hypothesis will themselves be biased if the original literature is contaminated by publication bias. Studies with significant results often do not appear to be superior to studies with a null result with respect to quality of design. However, statistically significant results have been shown to be three times more likely to be published compared to papers with null results.
Reporting bias & social desirability bias.
In epidemiology and empirical research, reporting bias is defined as "selective revealing or suppression of information" of undesirable behavior by subjects or researchers.
It refers to a tendency to under-report unexpected or undesirable experimental results, while being more trusting of expected or desirable results. This can propagate, as each instance reinforces the status quo, and later experimenters justify their own reporting bias by observing that previous experimenters reported different results.
Social desirability bias is a bias within social science research where survey respondents can tend to answer questions in a manner that will be viewed positively by others. It can take the form of over-reporting laudable behavior, or under-reporting undesirable behavior. This bias interferes with the interpretation of average tendencies as well as individual differences. The inclination represents a major issue with self-report questionnaires; of special concern are self-reports of abilities, personalities, sexual behavior, and drug use.
Prejudices.
Bias and prejudice are usually considered to be closely related. Prejudice is prejudgment, or forming an opinion before becoming aware of the relevant facts of a case. The word is often used to refer to preconceived, usually unfavorable, judgments toward people or a person because of gender, political opinion, social class, age, disability, religion, sexuality, race/ethnicity, language, nationality, or other personal characteristics. Prejudice can also refer to unfounded beliefs and may include "any unreasonable attitude that is unusually resistant to rational influence".
Classism.
Classism is discrimination on the basis of social class. It includes attitudes that benefit the upper class at the expense of the lower class, or vice versa.
Lookism.
Lookism is stereotypes, prejudice, and discrimination on the basis of physical attractiveness, or more generally to people whose appearance matches cultural preferences. Many people make automatic judgments of others based on their physical appearance that influence how they respond to those people.
Racism.
Racism consists of ideologies based on a desire to dominate or a belief in the inferiority of another race. It may also hold that members of different races should be treated differently.
Sexism.
Sexism is discrimination based on a person's sex or gender. Sexism can affect any gender, but it is particularly documented as affecting women and girls. It has been linked to stereotypes and gender roles, and may include the belief that one sex or gender is intrinsically superior to another.

</doc>
<doc id="40788" url="https://en.wikipedia.org/wiki?curid=40788" title="Bias distortion">
Bias distortion

In telecommunication, the term bias distortion has the following meanings: 
Bias distortion is expressed in percent of the system-specified unit interval.

</doc>
<doc id="40789" url="https://en.wikipedia.org/wiki?curid=40789" title="Bilateral synchronization">
Bilateral synchronization

In telecommunication, bilateral synchronization (or bilateral control) is a synchronization control system between exchanges A and B in which the clock at telephone exchange A controls the data received at exchange B and the clock at exchange B controls the data received at exchange A. 
Bilateral synchronization is usually implemented by deriving the timing from the incoming bitstream.
Source: from Federal Standard 1037C in support of MIL-STD-188

</doc>
<doc id="40792" url="https://en.wikipedia.org/wiki?curid=40792" title="Bipolar signal">
Bipolar signal

In telecommunication, a bipolar signal is a signal that may assume either of two polarities, neither of which is zero. 
A bipolar signal may have a two-state non-return-to-zero (NRZ) or a three-state return-to-zero (RZ) binary coding scheme. 
A bipolar signal is usually symmetrical with respect to zero amplitude, "i.e.," the absolute values of the positive and negative signal states are nominally equal.

</doc>
<doc id="40793" url="https://en.wikipedia.org/wiki?curid=40793" title="Bit-count integrity">
Bit-count integrity

In telecommunication, the term bit-count integrity (BCI) has the following meanings: 
"Note:" Bit-count integrity is not the same as bit integrity, which requires that the delivered bits correspond exactly with the original bits.
Source: from Federal Standard 1037C and from MIL-STD-188

</doc>
<doc id="40794" url="https://en.wikipedia.org/wiki?curid=40794" title="Bit error rate">
Bit error rate

In digital transmission, the number of bit errors is the number of received bits of a data stream over a communication channel that have been altered due to noise, interference, distortion or bit synchronization errors.
The bit error rate (BER) is the number of bit errors per unit time. The bit error ratio (also BER) is the number of bit errors divided by the total number of transferred bits during a studied time interval. BER is a unitless performance measure, often expressed as a percentage.
The bit error probability "pe" is the expectation value of the bit error ratio. The bit error ratio can be considered as an approximate estimate of the bit error probability. This estimate is accurate for a long time interval and a high number of bit errors.
Example.
As an example, assume this transmitted bit sequence:
0 1 1 0 0 0 1 0 1 1
and the following received bit sequence:
0 0 1 0 1 0 1 0 0 1,
The number of bit errors (the underlined bits) is, in this case, 3. The BER is 3 incorrect bits divided by 10 transferred bits, resulting in a BER of 0.3 or 30%.
Packet error ratio.
The packet error ratio (PER) is the number of incorrectly received data packets divided by the total number of received packets. A packet is declared incorrect if at least one bit is erroneous. The expectation value of the PER is denoted packet error probability "pp", which for a data packet length of "N" bits can be expressed as
assuming that the bit errors are independent of each other. For small bit error probabilities, this is approximately
Similar measurements can be carried out for the transmission of frames, blocks, or symbols.
Factors affecting the BER.
In a communication system, the receiver side BER may be affected by transmission channel noise, interference, distortion, bit synchronization problems, attenuation, wireless multipath fading, etc.
The BER may be improved by choosing a strong signal strength (unless this causes cross-talk and more bit errors), by choosing a slow and robust modulation scheme or line coding scheme, and by applying channel coding schemes such as redundant forward error correction codes.
The "transmission BER" is the number of detected bits that are incorrect before error correction, divided by the total number of transferred bits (including redundant error codes). The "information BER", approximately equal to the decoding error probability, is the number of decoded bits that remain incorrect after the error correction, divided by the total number of decoded bits (the useful information). Normally the transmission BER is larger than the information BER. The information BER is affected by the strength of the forward error correction code.
Analysis of the BER.
The BER may be evaluated using stochastic (Monte Carlo) computer simulations. If a simple transmission channel model and data source model is assumed, the BER may also be calculated analytically. An example of such a data source model is the Bernoulli source.
Examples of simple channel models used in information theory are:
A worst-case scenario is a completely random channel, where noise totally dominates over the useful signal. This results in a transmission BER of 50% (provided that a Bernoulli binary data source and a binary symmetrical channel are assumed, see below).
In a noisy channel, the BER is often expressed as a function of the normalized carrier-to-noise ratio measure denoted Eb/N0, (energy per bit to noise power spectral density ratio), or Es/N0 (energy per modulation symbol to noise spectral density).
For example, in the case of QPSK modulation and AWGN channel, the BER as function of the Eb/N0 is given by:
formula_3.
People usually plot the BER curves to describe the performance of a digital communication system. In optical communication, BER(dB) vs. Received Power(dBm) is usually used; while in wireless communication, BER(dB) vs. SNR(dB) is used.
Measuring the bit error ratio helps people choose the appropriate forward error correction codes. Since most such codes correct only bit-flips, but not bit-insertions or bit-deletions, the Hamming distance metric is the appropriate way to measure the number of bit errors. Many FEC coders also continuously measure the current BER.
A more general way of measuring the number of bit errors is the Levenshtein distance.
The Levenshtein distance measurement is more appropriate for measuring raw channel performance before frame synchronization, and when using error correction codes designed to correct bit-insertions and bit-deletions, such as Marker Codes and Watermark Codes.
Mathematical draft.
The BER is the likelihood of a bit misinterpretation due to electrical noise formula_4. Considering a bipolar NRZ transmission, we have
formula_5 for a "1" and formula_6 for a "0". Each of formula_7 and formula_8 has a period of formula_9.
Knowing that the noise has a bilateral spectral density formula_10,
formula_7 is formula_12
and formula_8 is formula_14.
Returning to BER, we have the likelihood of a bit misinterpretation formula_15.
formula_16 and formula_17
where formula_18 is the threshold of decision, set to 0 when formula_19.
We can use the average energy of the signal formula_20 to find the final expression :
formula_21
Bit error rate test.
BERT or bit error rate test is a testing method for digital communication circuits that uses predetermined stress patterns consisting of a sequence of logical ones and zeros generated by a test pattern generator.
A BERT typically consists of a test pattern generator and a receiver that can be set to the same pattern. They can be used in pairs, with one at either end of a transmission link, or singularly at one end with a loopback at the remote end. BERTs are typically stand-alone specialised instruments, but can be personal computer–based. In use, the number of errors, if any, are counted and presented as a ratio such as 1 in 1,000,000, or 1 in 1e06.
Bit error rate tester.
A bit error rate tester (BERT), also known as a "bit error ratio tester" or "bit error rate test solution" (BERTs) is electronic test equipment used to test the quality of signal transmission of single components or complete systems.
The main building blocks of a BERT are:

</doc>
<doc id="40795" url="https://en.wikipedia.org/wiki?curid=40795" title="Bit inversion">
Bit inversion

In telecommunications, bit inversion means the changing of the state of a bit to the opposite state, i.e. the changing of a 0 bit to 1 or of a 1 bit to 0. It also refers to the changing of a "state representing a given bit" to the opposite state.
"Source: Federal Standard 1037C and MIL-STD-188"

</doc>
<doc id="40796" url="https://en.wikipedia.org/wiki?curid=40796" title="Bit pairing">
Bit pairing

In telecommunication, bit pairing is the practice of establishing, within a code set, a number of subsets that have an identical bit representation except for the state of a specified bit. 
"Note:" An example of bit pairing occurs in the International Alphabet No. 5 and the American Standard Code for Information Interchange (ASCII), where the upper case letters are related to their respective lower case letters by the state of bit six.

</doc>
<doc id="40798" url="https://en.wikipedia.org/wiki?curid=40798" title="Bit-sequence independence">
Bit-sequence independence

In telecommunication, bit-sequence independence is a characteristic of some digital data transmission systems that impose no restrictions on, or modification of, the transmitted bit sequence.
Bit-sequence-independent protocols are in contrast to protocols that reserve certain bit sequences for special meanings, such as the flag sequence, 01111110, for HDLC, SDLC, and ADCCP protocols.
Bit-sequence-independence allows only line codes that have the same number of transitions per bit, otherwise, the line code is dependent on the bit sequence and, therefore, bit-sequence dependent.

</doc>
<doc id="40799" url="https://en.wikipedia.org/wiki?curid=40799" title="Bit slip">
Bit slip

In digital transmission, bit slip is the loss or gain of a bit or bits, caused by clock drift – variations in the respective clock rates of the transmitting and receiving devices.
One cause of bit slippage is overflow of a receive buffer that occurs when the transmitter's clock rate exceeds that of the receiver. This causes one or more bits to be dropped for lack of storage capacity.
One way to maintain timing between transmitting and receiving devices is to employ an asynchronous protocol such as start-stop. Alternatively, bit slip can be prevented by using a self-clocking signal (such as a signal modulated using OQPSK) or using a line coding such as Manchester encoding.
Another cause is "losing count", as on a hard drive: if a hard drive encounters a long string of 0s, without any 1s (or a string of 1s without 0s), it may lose track of the frame between fields, and suffer bit slip.
When a pulse of N consecutive zero bits are sent, clock drift may cause the hardware to apparently detect N-1 zero bits or N+1 zero bits—both kinds of errors are called bit slip.
Thus one prevents long strings without change via such devices as run length limited codes.
Many communication systems use linear feedback shift register scrambling to prevent long strings of 0s (or other symbol),
including VSAT, 1000BASE-T, RFC 2615, etc.
While a scrambler makes the "losing count" type of bit slip error occur far less often,
when bit slip errors do occur (perhaps for other reasons), 
scramblers have the property of expanding small errors that add or lose a single bit into a much longer burst of errors.
The optimized cipher feedback mode (OCFB), the statistical self-synchronization mode, and the "one-bit CFB mode" also expand small bit-slip errors into a longer burst of errors, but eventually recover and produce the correct decrypted plaintext.
A bit-slip error when using any other block cipher mode of operation generally results in complete corruption of the rest of the message.

</doc>
<doc id="40801" url="https://en.wikipedia.org/wiki?curid=40801" title="Bit-stream transmission">
Bit-stream transmission

In telecommunication, the term bit-stream transmission has the following meanings:
1. In bit-oriented systems, the transmission of bit strings.
2. In character-oriented systems, the transmission of bit streams that represent characters.
In bit-stream transmission, the bits usually occur at fixed time intervals, start and stop signals are not used, and the bit patterns follow each other in sequence without interruption.

</doc>
<doc id="40802" url="https://en.wikipedia.org/wiki?curid=40802" title="Bit stuffing">
Bit stuffing

In data transmission and telecommunication, bit stuffing (also known—uncommonly—as positive justification) is the insertion of non information bits into data. Stuffed bits should not be confused with overhead bits.
Bit stuffing is used for various purposes, such as for bringing bit streams that do not necessarily have the same or rationally related bit rates up to a common rate, or to fill buffers or frames. The location of the stuffing bits is communicated to the receiving end of the data link, where these extra bits are removed to return the bit streams to their original bit rates or form. Bit stuffing may be used to synchronize several channels before multiplexing or to rate-match two single channels to each other.
Applications include Plesiochronous Digital Hierarchy and Synchronous Digital Hierarchy.
Another use of bit stuffing is for run length limited coding: to limit the number of consecutive bits of the same value in the data to be transmitted. A bit of the opposite value is inserted after the maximum allowed number of consecutive bits. Since this is a general rule the receiver doesn't need extra information about the location of the stuffing bits in order to do the de-stuffing.
This is done to create additional signal transitions to ensure reliable reception or to escape special reserved code words such as frame sync sequences when the data happens to contain them.
Applications include Controller Area Network, HDLC, and Universal Serial Bus.
Bit stuffing does not ensure that the payload is intact ("i.e." not corrupted by transmission errors); it is merely a way of attempting to ensure that the transmission starts and ends at the correct places. Error detection and correction techniques are used to check the frame for corruption after its delivery and, if necessary, the frame will be re-sent.
Zero-bit insertion.
Zero-bit insertion is a particular type of bit stuffing used in some data transmission protocols to aid clock recovery from the data stream. It was popularized by IBM's SDLC (later renamed HDLC).
The name relates to the insertion of only 0 bits. No 1 bits are inserted to limit sequences of 0 bits.
SDLC and Low- and full-speed USB data are sent NRZI encoded: a 0 bit causes a signal transition, whereas a 1 bit causes no change. After a long sequence of 1 bits there could be no transitions in the transmitted data, and it would be possible for the transmitter and receiver clocks to lose synchronisation. By inserting a 0 after five (SDLC) or six (USB) sequential 1s the transmitter guarantees a maximum time between transitions. The receiver can synchronise its clock against the transitions to ensure proper data recovery.
In SDLC the transmitted bit sequence "01111110" containing six adjacent 1 bits is the Flag byte. Bit stuffing ensures that this pattern can never occur in normal data, so it can be used as a marker for the beginning and end of frame without any possibility of being confused with normal data.
The main disadvantage of this form of bit-stuffing is that the code rate is unpredictable; it depends on the data being transmitted.
"Source: from Federal Standard 1037C in support of MIL-STD-188"

</doc>
<doc id="40803" url="https://en.wikipedia.org/wiki?curid=40803" title="Bit-synchronous operation">
Bit-synchronous operation

Bit-synchronous operation is a type of digital communication in which the data circuit-terminating equipment (DCE), data terminal equipment (DTE), and transmitting circuits are all operated in bit synchronism with a clock signal.
In bit-synchronous operation, clock timing is usually delivered at twice the modulation rate, and one bit is transmitted or received during each clock cycle.
Bit-synchronous operation is sometimes erroneously referred to as digital synchronization.

</doc>
<doc id="40804" url="https://en.wikipedia.org/wiki?curid=40804" title="Black facsimile transmission">
Black facsimile transmission

In telecommunication, the term black facsimile transmission has the following meanings: 

</doc>
<doc id="40806" url="https://en.wikipedia.org/wiki?curid=40806" title="Black recording">
Black recording

In telecommunication, the term black recording has the following meanings:
Source: Federal Standard 1037C and MIL-STD-188.

</doc>
<doc id="40807" url="https://en.wikipedia.org/wiki?curid=40807" title="Blind transmission">
Blind transmission

A blind transmission, in telecommunications, is a transmission made without obtaining a receipt, or acknowledgment of reception, from the intended receiving station. Blind transmissions may occur or be necessary when security constraints, such as radio silence, are imposed, when technical difficulties with a sender's receiver or receiver's transmitter occur, or when lack of time precludes the delay caused by waiting for receipts.

</doc>
<doc id="40808" url="https://en.wikipedia.org/wiki?curid=40808" title="Block">
Block

Block may refer to:

</doc>
<doc id="40809" url="https://en.wikipedia.org/wiki?curid=40809" title="Block check character">
Block check character

In telecommunications, a block check character (BCC) is a character added to a transmission block to facilitate error detection.
In longitudinal redundancy checking and cyclic redundancy checking, block check characters are computed for, and added to, each message block transmitted. This block check character is compared with a second block check character computed by the receiver to determine whether the transmission is error free.

</doc>
<doc id="40810" url="https://en.wikipedia.org/wiki?curid=40810" title="Blocking">
Blocking

Blocking may refer to:

</doc>
<doc id="40814" url="https://en.wikipedia.org/wiki?curid=40814" title="Branch">
Branch

A branch ( or , ) or tree branch (sometimes referred to in botany as a ramus) is a woody structural member connected to but not part of the central trunk of a tree (or sometimes a shrub). Large branches are known as boughs and small branches are known as twigs.
While branches can be nearly horizontal, vertical, or diagonal, the majority of trees have upwardly diagonal branches.
The term "twig" often refers to a terminus, while "bough" refers only to branches coming directly from the trunk.
Words.
Because of the enormous quantity of branches in the world, there are a variety of names in English alone for them. In general however, unspecific words for a branch (such as rise and rame) have been replaced by the word branch itself.
Specific terms.
A bough can also be called a limb or arm, and though these are arguably metaphors, both are widely accepted synonyms for bough.
A crotch or fork is an area where a trunk splits into two or more boughs.
A twig is frequently referred to as a sprig as well, especially when it has been plucked. Other words for twig include branchlet, spray, and surcle, as well as the technical terms surculus and ramulus.
Branches found under larger branches can be called underbranches.
Some branches from specific trees have their own names, such as osiers and withes or withies, which come from willows. Often trees have certain words which, in English, are naturally collocated, such as holly and mistletoe, which usually employ the phrase "sprig of" (as in, a "sprig of mistletoe"). Similarly, the branch of a cherry tree is generally referred to as a "cherry branch", while other such formations (i.e., "acacia branch" or "orange branch") carry no such alliance. A good example of this versatility is oak, which could be referred to as variously an "oak branch", an "oaken branch", a "branch of oak", or the "branch of an oak ".
Once a branch has been cut or in any other way removed from its source, it is most commonly referred to as a stick, and a stick employed for some purpose (such as walking, spanking, or beating) is often called a rod. Thin, flexible sticks are called switches, wands, shrags, or vimina (singular vimen).
History and etymology.
In Old English, there are numerous words for branch, including ', ', ', and '. 
There are also numerous descriptive words, such as ' (that is, something that has bled, or "bloomed", out), ' (literally "little bough"), ' (literally "on growth"), and ' (literally "offspringing"). Numerous other words for twigs and boughs abound, including "", which still surves as the "-toe" in "mistletoe".
Latin words for branch are "ramus" or "cladus". The second term is an affix found in other modern words such as cladodonts (prehistoric sherks with branched teeth) or cladogram.

</doc>
<doc id="40815" url="https://en.wikipedia.org/wiki?curid=40815" title="Brewster's angle">
Brewster's angle

Brewster's angle (also known as the polarization angle) is an angle of incidence at which light with a particular polarization is perfectly transmitted through a transparent dielectric surface, with no reflection. When "unpolarized" light is incident at this angle, the light that is reflected from the surface is therefore perfectly polarized. This special angle of incidence is named after the Scottish physicist Sir David Brewster (1781–1868).
Explanation.
When light encounters a boundary between two media with different refractive indices, some of it is usually reflected as shown in the figure above. The fraction that is reflected is described by the Fresnel equations, and is dependent upon the incoming light's polarization and angle of incidence. 
The Fresnel equations predict that light with the "p" polarization (electric field polarized in the same plane as the incident ray and the surface normal) will not be reflected if the angle of incidence is 
where "n"1 is the refractive index of the initial medium through which the light propagates (the "incident medium"), and "n"2 is the index of the other medium. This equation is known as Brewster's law, and the angle defined by it is Brewster's angle.
The physical mechanism for this can be qualitatively understood from the manner in which electric dipoles in the media respond to "p"-polarized light. One can imagine that light incident on the surface is absorbed, and then re-radiated by oscillating electric dipoles at the interface between the two media. The polarization of freely propagating light is always perpendicular to the direction in which the light is travelling. The dipoles that produce the transmitted (refracted) light oscillate in the polarization direction of that light. These same oscillating dipoles also generate the reflected light. However, dipoles do not radiate any energy in the direction of the dipole moment. If the refracted light is "p"-polarized and propagates exactly perpendicular to the direction in which the light is predicted to be specularly reflected, the dipoles point along the specular reflection direction and therefore no light can be reflected. (See diagram, above)
With simple geometry this condition can be expressed as
where "θ"1 is the angle of reflection (or incidence) and "θ"2 is the angle of refraction.
Using Snell's law,
one can calculate the incident angle at which no light is reflected:
Solving for "θ"B gives
For a glass medium () in air (), Brewster's angle for visible light is approximately 56°, while for an air-water interface (), it is approximately 53°. Since the refractive index for a given medium changes depending on the wavelength of light, Brewster's angle will also vary with wavelength.
The phenomenon of light being polarized by reflection from a surface at a particular angle was first observed by Étienne-Louis Malus in 1808. He attempted to relate the polarizing angle to the refractive index of the material, but was frustrated by the inconsistent quality of glasses available at that time. In 1815, Brewster experimented with higher-quality materials and showed that this angle was a function of the refractive index, defining Brewster's law. 
Brewster's angle is often referred to as the "polarizing angle", because light that reflects from a surface at this angle is entirely polarized perpendicular to the incident plane (""s"-polarized") A glass plate or a stack of plates placed at Brewster's angle in a light beam can, thus, be used as a polarizer. The concept of a polarizing angle can be extended to the concept of a Brewster wavenumber to cover planar interfaces between two linear bianisotropic materials. In the case of reflection at Brewster's angle, the reflected and refracted rays are mutually perpendicular.
Applications.
Polarized sunglasses use the principle of Brewster's angle to reduce glare from the sun reflecting off horizontal surfaces such as water or road. In a large range of angles around Brewster's angle, the reflection of "p"-polarized light is lower than "s"-polarized light. Thus, if the sun is low in the sky, reflected light is mostly "s"-polarized. Polarizing sunglasses use a polarizing material such as Polaroid sheets to block horizontally-polarized light, preferentially blocking reflections from horizontal surfaces. The effect is strongest with smooth surfaces such as water, but reflections from roads and the ground are also reduced.
Photographers use the same principle to remove reflections from water so that they can photograph objects beneath the surface. In this case, the polarizing filter camera attachment can be rotated to be at the correct angle (see figure).
When recording a hologram, light is typically incident at Brewster's angle. Because the incident light is p-polarized, it is not back reflected from the transparent back-plane of the holographic film. This avoids unwanted interference effects in the hologram. 
Brewster angle prisms are used in laser physics. The polarized laser light enters the prism at Brewster's angle without any reflective losses. 
Brewster windows.
Gas lasers typically use a window tilted at Brewster's angle to allow the beam to leave the laser tube. Since the window reflects some "s"-polarized light but no "p"-polarized light, the round trip loss for the "s" polarization is higher than that of the "p" polarization. This causes the laser's output to be "p" polarized due to competition between the two modes.

</doc>
<doc id="40816" url="https://en.wikipedia.org/wiki?curid=40816" title="Bridge-to-bridge station">
Bridge-to-bridge station

In telecommunication, a bridge-to-bridge station is a ship station operating in the port operations service in which messages are restricted to navigational communications and which is capable of operation from the ship's navigational bridge or, in the case of a dredge, from its main control station, operating on a frequency or frequencies in the 156-162 MHz band. 
Source: From Federal Standard 1037C and from the NTIA Manual of Regulations and Procedures for Federal Radio Frequency Management

</doc>
<doc id="40817" url="https://en.wikipedia.org/wiki?curid=40817" title="Bridging loss">
Bridging loss

Bridging loss is the loss, at a given frequency, that results when an impedance is connected across a transmission line. It is expressed as the ratio, in decibels, of the signal power delivered to a given point in a system downstream from the bridging point prior to bridging, to the signal power delivered to the given point after bridging.
Source: from Federal Standard 1037C and from MIL-STD-188

</doc>
<doc id="40818" url="https://en.wikipedia.org/wiki?curid=40818" title="Brightness">
Brightness

Brightness is an attribute of visual perception in which a source appears to be radiating or reflecting light. In other words, brightness is the perception elicited by the luminance of a visual target. This is a subjective attribute/property of an object being observed and one of the color appearance parameters of color appearance models. Brightness refers to an absolute term and should not be confused with Lightness.
Terminology.
The adjective "bright" derives from an Old English "beorht" with the same meaning via metathesis giving Middle English "briht". The word is from a Common Germanic *', ultimately from a PIE root with a closely related meaning, *' "white, bright".
"Brightness" was formerly used as a synonym for the photometric term "luminance" and (incorrectly) for the radiometric term "radiance".
As defined by the US "Federal Glossary of Telecommunication Terms" (FS-1037C), "brightness" should now be used only for non-quantitative references to physiological sensations and perceptions of light.
A given target luminance can elicit different perceptions of brightness in different contexts; see, for example, White's illusion.
In the RGB color space, brightness can be thought of as the arithmetic mean "μ" of the red, green, and blue color coordinates (although some of the three components make the light seem brighter than others, which, again, may be compensated by some display systems automatically):
Brightness is also a color coordinate in the HSB or HSV color space (hue, saturation, and brightness or value).
With regard to stars, brightness is quantified as apparent magnitude and absolute magnitude.
Brightness is, at least in some respects, the antonym of darkness.
Brightness of sounds.
The term "brightness" is also used in discussions of sound timbres, in a rough analogy with visual brightness. Timbre researchers consider brightness to be one of the perceptually strongest distinctions between sounds, and formalize it acoustically as an indication of the amount of high-frequency content in a sound, using a measure such as the spectral centroid.

</doc>
<doc id="40821" url="https://en.wikipedia.org/wiki?curid=40821" title="Buffer">
Buffer

Buffer may refer to: 

</doc>
<doc id="40822" url="https://en.wikipedia.org/wiki?curid=40822" title="Burst switching">
Burst switching

In a packet switched network, burst switching is a capability in which each network switch extracts routing instructions from an incoming packet header to establish and maintain the appropriate switch connection for the duration of the packet, following which the connection is automatically released. 
In concept, burst switching is similar to connectionless mode transmission, but differs in that burst switching implies an intent to establish the switch connection in near real time so that only minimum buffering is required at the node switch.
A variant of burst switching used in optical networks is optical burst switching.

</doc>
<doc id="40823" url="https://en.wikipedia.org/wiki?curid=40823" title="Burst transmission">
Burst transmission

In telecommunication, the term burst transmission or data burst has the following meanings:
Burst transmission, however, enables communications between data terminal equipment (DTEs) and a data network operating at dissimilar data signaling rates.

</doc>
<doc id="40825" url="https://en.wikipedia.org/wiki?curid=40825" title="Busy signal (disambiguation)">
Busy signal (disambiguation)

A busy signal is information communicated to a user or apparatus attempting a connection, indicating the requested connection cannot be completed.

</doc>
<doc id="40826" url="https://en.wikipedia.org/wiki?curid=40826" title="Busy verification">
Busy verification

In a public switched telephone network, busy verification is a network-provided service feature that permits an attendant to verify the busy or idle state of station lines and to break into the conversation.
A 440 Hz tone is applied to the line for 2 seconds, followed by a 0.5 second burst every 10 seconds, to alert both parties that the attendant is connected to the circuit.

</doc>
<doc id="40827" url="https://en.wikipedia.org/wiki?curid=40827" title="Bypass">
Bypass

Bypass may refer to:

</doc>
<doc id="40828" url="https://en.wikipedia.org/wiki?curid=40828" title="Cable television relay service station">
Cable television relay service station

In telecommunication, a cable television relay service station (CARS) is a fixed or mobile station used for the transmission of television and related audio signals, signals of standard and FM broadcast stations, signals of instructional television fixed stations, and cablecasting from the point of reception to a terminal point from which the signals are distributed to the public.
Source: from Federal Standard 1037C and from the Code of Federal Regulations, Telecommunications Parts 0-199

</doc>
<doc id="40829" url="https://en.wikipedia.org/wiki?curid=40829" title="Call">
Call

Call may refer to:

</doc>
<doc id="40830" url="https://en.wikipedia.org/wiki?curid=40830" title="Call collision">
Call collision

In telecommunications, a call collision (commonly known as glare) is one of two things:
If you have ever tried to make a call out on a PBX, and been accidentally connected to an incoming call, you have experienced glare. This can sometimes happen at home too, if you pick up your phone to make a call out at the exact second that a call is about to start ringing in.
Multi-line hunting attempts to avoid glare by selecting circuits in opposite preference order so the highest numbered line, which is last choice for incoming calls, is first choice for outgoing calls, like so:
With PRI circuits, the channel selection sequence is specified when the circuit is provisioned. Common practice is to have the PBX use descending channel selection, and the carrier to use ascending. Glare is not common on PRI circuits because the signalling is so fast, however it is not impossible (especially if there are subtle differences in the timers at either end, and the circuit is being used at near-capacity). The users will not experience a connection to an unexpected call (as would be the case with analog circuits), because glare causes protocol errors that generally prevent any sort of successful connection. Instead, one or both of the call attempts might fail, and ideally an error would appear in the logs (this depends on the logging capabilities of the systems at either end of the circuit). Glare is quite rare on PRI circuits, and can be difficult to troubleshoot.
For old, analog PBX trunks, glare would be reduced by using ground start signalling, which offered better answer and disconnect supervision. IE: Nortel BSP discouraged using loop start trunks for this and other reasons. Long Distance exchanges in the 1950s and 60s incorporated Glare Detectors to alleviate the problem.

</doc>
<doc id="40832" url="https://en.wikipedia.org/wiki?curid=40832" title="Call duration">
Call duration

In telecommunications, the term call duration has the following meanings: 
Source: from Federal Standard 1037C

</doc>
<doc id="40833" url="https://en.wikipedia.org/wiki?curid=40833" title="Called-party camp-on">
Called-party camp-on

In telecommunication, a called-party camp-on is a communication system service feature that enables the system to complete an access attempt in spite of issuance of a user blocking signal. This is most often found in a switchboard system at a company. Instead of going to voicemail or simply sitting on hold until the line is free, this feature places you in a queue whereby the moment the line clears, the call will be put through.
Systems that provide this feature monitor the busy user until the user blocking signal ends, and then proceed to complete the requested access. This feature permits holding an incoming telephone call until the called party is free.

</doc>
<doc id="40834" url="https://en.wikipedia.org/wiki?curid=40834" title="Calling-party camp-on">
Calling-party camp-on

In telecommunication, a calling-party camp-on is a service feature that enables the telephone exchange to complete an access attempt in spite of temporary unavailability of system transmission or switching facilities required to establish the requested access. 
Systems that provide calling party camp-on monitor the system facilities until the necessary facilities become available, and then proceed to complete the requested access. Such systems may or may not issue a system blocking signal to inform the calling party of the access delay.

</doc>
<doc id="40836" url="https://en.wikipedia.org/wiki?curid=40836" title="Call processing">
Call processing

In telecommunication, the term call processing has the following meanings: 
"Volume Call Processing" is the handling of calls when there are far more incoming calls than can be answered by an individual or group of attendants.

</doc>
<doc id="40837" url="https://en.wikipedia.org/wiki?curid=40837" title="Call-second">
Call-second

In telecommunication, a call-second is a unit used to measure communications traffic density. 
"Note 1:" A call-second is equivalent to 1 call with a duration of 1 second. 
"Note 2:" One user making two 75-second calls is equivalent to two users each making one 75-second call. Each case produces 150 call-seconds of traffic. 
"Note 3:" The acronym CCS (Centum Call Seconds) is often used to describe 100 call-seconds.
"Note 4:" 3600 call-seconds = 36 CCS = 1 call-hour. 
"Note 5:" 3600 call-seconds per hour = 36 CCS per hour = 1 call-hour per hour = 1 erlang = 1 traffic unit.
In a communication network, a trunk (link) can carry numerous concurrent calls by means of multiplexing. Hence a particular number of CCS can be carried in infinitely many ways as calls are established and cleared over time. For example 3600 could be one call for an hour, or 2 (possibly concurrent) calls for half an hour each. CCS gives a measure of the average number of concurrent calls (i.e. Erlangs) over a time period of one hour.
Hence:

</doc>
<doc id="40838" url="https://en.wikipedia.org/wiki?curid=40838" title="Call set-up time">
Call set-up time

In telecommunication, the term call set-up time has the following meanings: 
"Note:" Call set-up time is the summation of: (a) call request time—the time from initiation of a calling signal to the delivery to the caller of a proceed-to-select signal; (b) selection time—the time from the delivery of the proceed-to-select signal until all the selection signals have been transmitted; and (c) post selection time—the time from the end of the transmission of the selection signals until the delivery of the call-connected signal to the originating terminal.

</doc>
<doc id="40839" url="https://en.wikipedia.org/wiki?curid=40839" title="Call-sign allocation plan">
Call-sign allocation plan

In telecommunication, call-sign allocation plan is the table of allocation of international call sign series contained in the current edition of the "International Telecommunication Union (ITU) "Radio Regulations"."
"Note:" In the table of allocation, the first two characters of each call sign (whether two letters or one number and one letter, in that order) identify the nationality of the station. In certain instances where the complete alphabetical block is allocated to a single nation, the first letter is sufficient for national identity. Individual assignments are made by appropriate national assignment authorities from the national allocation.

</doc>
<doc id="40840" url="https://en.wikipedia.org/wiki?curid=40840" title="Call tracing">
Call tracing

In telecommunication, call tracing is a procedure that permits an entitled user to be informed about the routing of data for an established connection, identifying the entire route from the origin to the destination.
There are two types of call tracing. Permanent call tracing permits tracing of all calls. On-demand call tracing permits tracing, upon request, of a specific call, provided that the called party dials a designated code immediately after the call to be traced is disconnected immediately. 

</doc>
<doc id="40841" url="https://en.wikipedia.org/wiki?curid=40841" title="Camp-on busy signal">
Camp-on busy signal

In telecommunication, the term camp-on busy signal has the following meanings: 

</doc>
<doc id="40842" url="https://en.wikipedia.org/wiki?curid=40842" title="Cancel character">
Cancel character

In telecommunication, the term cancel character has the following meanings: 

</doc>
<doc id="40843" url="https://en.wikipedia.org/wiki?curid=40843" title="Capacitive coupling">
Capacitive coupling

Capacitive coupling is the transfer of energy within an electrical network or between distant networks by means of displacement current between circuit(s) nodes, induced by the electric field. This coupling can have an intentional or accidental effect.
In its simplest implementation, capacitive coupling is achieved by placing a capacitor between two nodes. In its general form the coupling is described by a capacitance matrix "Cij". Where "Cii" are self-capacitance coefficients and "Cij" "i≠j" are mutual capacitance coefficients.
Use in analog circuits.
In analog circuits, a coupling capacitor is used to connect two circuits such that only the AC signal from the first circuit can pass through to the next while DC is blocked. This technique helps to isolate the DC bias settings of the two coupled circuits. Capacitive coupling is also known as "AC coupling" and the capacitor used for the purpose is also known as a "DC-blocking capacitor".
A coupling capacitor's ability to prevent a DC load from interfering with an AC source is particularly useful in Class A amplifier circuits by preventing a 0 volt input being passed to a transistor with additional resistor biasing; creating continuous amplification.
Capacitive coupling has the disadvantage of degrading the low frequency performance of a system containing capacitively coupled units. Each coupling capacitor along with the input electrical impedance of the next stage forms a high-pass filter and the sequence of filters results in a cumulative filter with a −3dB frequency that may be higher than those of each individual filter. So for adequate low frequency response, the capacitors used must have high capacitance ratings. They should be high enough that the reactance of each is at most a tenth of the input impedance of each stage, at the lowest frequency of interest. See Impedance bridging.
Coupling capacitors can also introduce nonlinear distortion at low frequencies. This is not an issue at high frequencies because the voltage across the capacitor stays very close to zero. However, if a signal is allowed to pass through the coupling that is low relative to the RC cutoff frequency, voltages can develop across the capacitor, which for some capacitor types results in changes of capacitance, leading to distortion. This is avoided by choosing capacitor types that have low "voltage coefficient", and by using large values that put the cutoff frequency far lower than the frequencies of the signal.
These disadvantages of capacitively coupling DC-biased transistor amplifier circuits are largely minimized in directly coupled designs.
Use in digital circuits.
AC coupling is also widely used in digital circuits to transmit digital signals with a zero DC component, known as DC-balanced signals. DC-balanced waveforms are useful in communications systems, since they can be used over AC-coupled electrical connections to avoid voltage imbalance problems and charge accumulation between connected systems or components.
For this reason, most modern line codes are designed to produce DC-balanced waveforms. The most common classes of DC-balanced line codes are constant-weight codes and paired-disparity codes.
Gimmick loop.
A gimmick loop is a simple type of capacitive coupler: two closely spaced strands of wire. It provides capacitive coupling of a few picofarads between two nodes. Sometimes the wires are twisted together for physical stability.
Parasitic capacitive coupling.
Capacitive coupling is often unintended, such as the capacitance between two wires or PCB traces that are next to each other. Often one signal can capacitively couple with another and cause what appears to be noise. To reduce coupling, wires or traces are often separated as much as possible, or ground lines or ground planes are run in between signals that might affect each other, so that the lines are capacitively coupled to ground rather than each other. Breadboards are particularly prone to these issues due to the long pieces of metal that line every row creating a several-picofarad capacitor between lines. To prototype high-frequency (10s of MHz) or high-gain analog circuits, often the circuits are built over a ground plane so that the signals couple to ground more than to each other. If a high-gain amplifier's output capacitively couples to its input it often becomes an electronic oscillator.
One rule of thumb says that drivers should be able to drive 25 pF of capacitance which allows for PCB traces up to 0.30 meters.

</doc>
<doc id="40844" url="https://en.wikipedia.org/wiki?curid=40844" title="Capture effect">
Capture effect

In telecommunications, the capture effect, or FM capture effect, is a phenomenon associated with FM reception in which only the stronger of two signals at, or near, the same frequency or channel will be demodulated. 
The capture effect is defined as the complete suppression of the weaker signal at the receiver limiter (if it has one) where the weaker signal is not amplified, but attenuated. When both signals are nearly equal in strength, or are fading independently, the receiver may switch from one to the other and exhibit picket fencing.
The capture effect can occur at the signal limiter, or in the demodulation stage, for circuits that do not require a signal limiter. Some types of radio receiver circuits have a stronger capture effect than others. The measurement of how well a receiver can reject a second signal on the same frequency is called the capture ratio for a specific receiver. It is measured as the lowest ratio of the power of two signals that will result in the suppression of the smaller signal. 
Amplitude modulation, or AM radio, transmission is not subject to this effect. This is one reason that the aviation industry, and others, have chosen to use AM for communications rather than FM, allowing multiple signals to be broadcast on the same channel. Phenomena similar to the capture effect are described in AM when offset carriers of different strengths are present in the passband of a receiver. For example, the aviation glideslope vertical guidance clearance beam is sometimes described as a "capture effect" system, even though it operates using AM signals.
Amplitude modulation immunity to capture effect.
In FM demodulation the receiver tracks the modulated frequency shift of the desired carrier while discriminating against any other signal since it can only follow the deviation of one signal at a time. In AM, the receiver tracks the signal strength of the AM signal as the basis for demodulation. This allows any other signal to be tracked as just another change in amplitude. So it is possible for an AM receiver to demodulate several carriers at the same time, resulting in an audio mix.
If the signals are close but not exactly on the same frequency the mix will not only include the audio from both carriers but depending on the carrier separation an audible tone (a beat signal) may be heard at a frequency equal to the difference in the carrier frequencies involved. For instance, if one carrier is at 1000.000 KHz, and the other is at 1000.150 KHz, then a 150 Hz "beat frequency" tone will result.
This mix can also occur when a second AM carrier is received on a channel that is adjacent to the desired channel if the receiver's ultimate bandwidth is wide enough to include the carriers of both signals. In the US AM broadcast bands this occurs at 10 KHz, which is the US channel spacing for the AM broadcast band. Elsewhere it can occur at 9 KHz, a commonly used channel spacing in many locales.
Modern SDR-based receivers can completely eliminate this by utilizing "brick-wall" filters narrower than the channel spacing that reduce signals outside the passband to inconsequential levels. Where such an overlap within the passband occurs, a high pitched whistle at precisely 9 or 10 KHz can be heard. This is particularly common at night when other carriers from adjacent channels are traveling long distances due to atmospheric bounce.
Because AM assumes short term changes in the amplitude to be information, any electrical impulse will be picked up and demodulated along with the desired carrier. Hence lightning causes crashing noises when picked up by an AM radio near a storm. In contrast, FM suppresses short term changes in amplitude and is therefore much less prone to noise during storms and during reception of electrical noise impulses. 
For digital modulation schemes it has been shown that for properly implemented on-off keying/amplitude-shift keying systems, co-channel rejection can be better than for frequency-shift keying systems.

</doc>
<doc id="40845" url="https://en.wikipedia.org/wiki?curid=40845" title="Carrier">
Carrier

Carrier may refer to:

</doc>
<doc id="40846" url="https://en.wikipedia.org/wiki?curid=40846" title="Carrier sense multiple access with collision avoidance">
Carrier sense multiple access with collision avoidance

It is particularly important for wireless networks, where the collision detection of the alternative CSMA/CD is unreliable due to the hidden node problem.
CSMA/CA is a protocol that operates in the Data Link Layer (Layer 2) of the OSI model.
Details.
Collision avoidance is used to improve the performance of the CSMA method by attempting to divide the channel somewhat equally among all transmitting nodes within the collision domain.
Although CSMA/CA has been used in a variety of wired communication systems, it is particularly beneficial in a wireless LAN due to a common problem of multiple stations being able to see the Access Point, but not each other. This is due to differences in transmit power, and receive sensitivity, as well as distance, and location with respect to the AP. This will cause a station to not be able to 'hear' another station's broadcast. This is the so-called 'hidden node', or 'hidden station' problem. Devices utilizing 802.11 based standards can enjoy the benefits of collision avoidance (RTS / CTS handshake, also Point coordination function), although they do not do so by default. By default they use a Carrier sensing mechanism called 'exponential backoff', or (Distributed coordination function) that relies upon a station attempting to 'listen' for another station's broadcast before sending. CA, or PCF relies upon the AP (or the 'receiver' for Ad hoc networks) granting a station the exclusive right to transmit for a given period of time after requesting it (Request to Send / Clear to Send).
IEEE 802.11 RTS/CTS Exchange.
CSMA/CA can optionally be supplemented by the exchange of a Request to Send (RTS) packet sent by the sender S, and a Clear to Send (CTS) packet sent by the intended receiver R. Thus alerting all nodes within range of the sender, receiver or both, to not transmit for the duration of the main transmission. This is known as the IEEE 802.11 RTS/CTS exchange. Implementation of RTS/CTS helps to partially solve the hidden node problem that is often found in wireless networking.
Performance.
CSMA/CA performance is based largely upon the modulation technique used to transmit the data between nodes. Studies show that under ideal propagation conditions (simulations), Direct Sequence Spread Spectrum (DSSS) provides the highest throughput for all nodes on a network when used in conjunction with CSMA/CA and the IEEE 802.11 RTS/CTS exchange under light network load conditions. Frequency Hopping Spread Spectrum (FHSS) follows distantly behind DSSS with regard to throughput with a greater throughput once network load becomes substantially heavy. However, the throughput is generally the same under real world conditions due to radio propagation factors.

</doc>
<doc id="40847" url="https://en.wikipedia.org/wiki?curid=40847" title="Carrier sense multiple access with collision detection">
Carrier sense multiple access with collision detection

Carrier sense multiple access with collision detection (CSMA/CD) is a media access control method used most notably in local area networking using early Ethernet technology. It uses a carrier sensing scheme in which a transmitting data station detects other signals while transmitting a frame, and stops transmitting that frame, transmits a jam signal, and then waits for a random time interval before trying to resend the frame.
CSMA/CD is a modification of pure carrier sense multiple access (CSMA). CSMA/CD is used to improve CSMA performance by terminating transmission as soon as a collision is detected, thus shortening the time required before a retry can be attempted.
Procedure.
The following procedure is used to initiate a transmission. The procedure is complete when the frame is transmitted successfully or a collision is detected during transmission.
The following procedure is used to resolve a detected collision. The procedure is complete when retransmission is initiated or the retransmission is aborted due to numerous collisions.
This can be likened to what happens at a dinner party, where all the guests talk to each other through a common medium (the air). Before speaking, each guest politely waits for the current speaker to finish. If two guests start speaking at the same time, both stop and wait for short, random periods of time (in Ethernet, this time is measured in microseconds). The hope is that by each choosing a random period of time, both guests will not choose the same time to try to speak again, thus avoiding another collision.
Methods for collision detection are media dependent, but on an electrical bus such as 10BASE5 or 10BASE2, collisions can be detected by comparing transmitted data with received data or by recognizing a higher than normal signal amplitude on the bus.
Jam signal.
The jam signal or jamming signal is a signal that carries a 32-bit binary pattern sent by a data station to inform the other stations of the collision and that they must not transmit.
The maximum jam-time is calculated as follows: The maximum allowed diameter of an Ethernet installation is limited to 232 bits. This makes a round-trip-time of 464 bits. As the slot time in Ethernet is 512 bits, the difference between slot time and round-trip-time is 48 bits (6 bytes), which is the maximum "jam-time".
This in turn means: A station noting a collision has occurred is sending a 4 to 6 byte long pattern composed of 16 1-0 bit combinations. Note: The size of this jam signal is clearly beyond the minimum allowed frame-size of 64 bytes.
The purpose of this is to ensure that any other node which may currently be receiving a frame will receive the jam signal in place of the correct 32-bit MAC CRC, this causes the other receivers to discard the frame due to a CRC error.
Late collision.
A late collision is a type of collision that happens further into the packet than is allowed for by the protocol standard in question. In 10 megabit shared medium Ethernet, if a collision error occurs after the first 512 bits of data are transmitted by the transmitting station, a late collision is said to have occurred. Importantly, late collisions are not re-sent by the NIC unlike collisions occurring before the first 64 octets; it is left for the upper layers of the protocol stack to determine that there was loss of data.
As a correctly set up CSMA/CD network link should not have late collisions, the usual possible causes are full-duplex/half-duplex mismatch, exceeded Ethernet cable length limits, or defective hardware such as incorrect cabling, non-compliant number of hubs in the network, or a bad NIC.
Applications.
CSMA/CD was used in now-obsolete shared media Ethernet variants (10BASE5, 10BASE2) and in the early versions of twisted-pair Ethernet which used repeater hubs. Modern Ethernet networks, built with switches and full-duplex connections, no longer need to utilize CSMA/CD because each Ethernet segment, or collision domain, is now isolated. CSMA/CD is still supported for backwards compatibility and for half-duplex connections. IEEE Std 802.3, which defines all Ethernet variants, for historical reasons still bears the title "Carrier sense multiple access with collision detection (CSMA/CD) access method and physical layer specifications".

</doc>
<doc id="40848" url="https://en.wikipedia.org/wiki?curid=40848" title="Carrier shift">
Carrier shift

In telecommunication, the term carrier shift has the following meanings: 
"Note 1:" The carrier shift results in a change in carrier power. 
"Note 2:" The carrier shift may be a shift to a higher or to a lower frequency. 

</doc>
<doc id="40849" url="https://en.wikipedia.org/wiki?curid=40849" title="Carrier system">
Carrier system

A carrier system is a telecommunications system that transmits information, such as the voice signals of a telephone call and the video signals of television, by modulation of one or multiple carrier signals above the principle voice frequency or data rate.
Carrier systems typically transmit multiple channels of communication simultaneously over the shared medium using various forms of multiplexing. Prominent multiplexing methods of the carrier signal are time-division multiplexing (TDM) and frequency-division multiplexing. A cable television system is an example of frequency-division multiplexing. Many television programs are carried simultaneously on the same coaxial cable by sending each at a different frequency. Multiple layers of multiplexing may ultimately be performed upon a given input signal. For example, in the public switched telephone network, many telephone calls are sent over shared trunk lines by time-division multiplexing. For long distance calls several of these channels may be sent over a communications satellite link by frequency-division multiplexing. At a given receiving node, specific channels may be demultiplexed individually.
History.
The purpose of carrier systems is to save money by carrying more traffic on less infrastructure. 19th century telephone systems, operating at baseband, could only carry one telephone call on each wire, hence routes with heavy traffic needed many wires.
In the 1920s, frequency-division multiplexing could carry several circuits on the same balanced wires, and by the 1930s L-carrier and similar systems carried hundreds of calls at a time on coaxial cables.
Capacity of these systems increased in the middle of the century, while in the 1950s researchers began to take seriously the possibility of saving money on the terminal equipment by using time-division multiplexing. This work led to T-carrier and similar digital systems for local use.
Due to the shorter repeater spacings required by digital systems, long-distance still used FDM until the late 1970s when optical fiber was improved to the point that digital connections became the cheapest ones for all distances, short and long. By the end of the century analog connections between and within telephone exchanges became rare. 

</doc>
<doc id="40850" url="https://en.wikipedia.org/wiki?curid=40850" title="Carrier-to-receiver noise density">
Carrier-to-receiver noise density

In satellite communications, carrier-to-receiver noise density ("C"/"kT") is the ratio of the power by received carrier to the density of the noise power of the receiver. It determines whether a receiver can lock on to the carrier and if the information encoded in the signal can be retrieved, given the amount of noise present in the received signal. The carrier-to-receiver noise density ratio is usually expressed in dBHz.
The carrier-to-receiver noise density is given by
where "C" is the received carrier power in watts, "k" is Boltzmann constant in joules per kelvin, and "T" is the receiver system noise temperature in kelvins. The receiver noise power density, "kT", is the receiver noise power per hertz.

</doc>
<doc id="40851" url="https://en.wikipedia.org/wiki?curid=40851" title="Carson bandwidth rule">
Carson bandwidth rule

In telecommunication, Carson's bandwidth rule defines the approximate bandwidth requirements of communications system components for a carrier signal that is frequency modulated by a continuous or broad spectrum of frequencies rather than a single frequency. Carson's rule does not apply well when the modulating signal contains discontinuities, such as a square wave. Carson's rule originates from John Renshaw Carson's 1922 paper.
Carson's bandwidth rule is expressed by the relation formula_1 where CBR is the bandwidth requirement, formula_2 is the peak frequency deviation, and formula_3 is the highest frequency in the modulating signal.
For example, a 2-Meter Ham Radio signal using FM mode, with 5 kHz peak deviation, and a maximum audio frequency of 3 kHz, would require an approximate bandwidth 2(5+3) = 16 kHz.
For example, standard broadcast FM has a peak deviation of 75 kHz above and below the carrier. With stereo FM, the highest modulating frequency (which combines L+R and L-R) is 53 kHz.
So most of the energy of standard stereo FM falls in an approximate bandwidth of 2(75+53) = 256 kHz.
(Geographically close FM broadcast transmitters are almost always assigned nominal center frequencies at least 500 kHz apart).
Carson's bandwidth rule is often applied to transmitters, antennas, optical sources, receivers, photodetectors, and other communications system components.
Any frequency modulated signal will have an "infinite" number of sidebands and hence an infinite bandwidth but in practice all significant sideband energy (98% or more) is concentrated within the bandwidth defined by Carson's rule. It is a useful approximation, but setting the arbitrary definition of occupied bandwidth at 98% of the power still means that the power outside the band is only about formula_4 = 17 dB less than the carrier inside, therefore Carson's Rule is of little use in spectrum planning.

</doc>
<doc id="40853" url="https://en.wikipedia.org/wiki?curid=40853" title="Cassegrain antenna">
Cassegrain antenna

In telecommunications and radar, a Cassegrain antenna is a parabolic antenna in which the feed antenna is mounted at or behind the surface of the concave main parabolic reflector dish and is aimed at a smaller convex secondary reflector suspended in front of the primary reflector. The beam of radio waves from the feed illuminates the secondary reflector, which reflects it back to the main reflector dish, which reflects it forward again to form the desired beam. The Cassegrain design is widely used in parabolic antennas, particularly in large antennas such as those in satellite ground stations, radio telescopes, and communication satellites.
Geometry.
The primary reflector is a paraboloid, while the shape of the convex secondary reflector is a hyperboloid. The geometrical condition for radiating a collimated, plane wave beam is that the feed antenna is located at the far focus of the hyperboloid, while the focus of the primary reflector coincides with the near focus of the hyperboloid. Usually the secondary reflector and the feed antenna are located on the central axis of the dish. However in "offset Cassegrain" configurations, the primary dish reflector is asymmetric, and its focus, and the secondary reflector, are located to one side of the dish, so that the secondary reflector does not partially obstruct the beam.
Advantages.
This design is an alternative to the most common parabolic antenna design, called "front feed", in which the feed antenna itself is mounted suspended in front of the dish at the focus, pointed back toward the dish. The Cassegrain design has several advantages over front feed that can justify its increased complexity: 
A disadvantage of the Cassegrain is that the feed horn(s) must have a narrower beamwidth (higher gain) to focus its radiation on the smaller secondary reflector, instead of the wider primary reflector as in front-fed dishes. The angular width the secondary reflector subtends at the feed horn is typically 10° - 15°, as opposed to 120° - 180° the main reflector subtends in a front-fed dish. Therefore the feed horn must be longer for a given wavelength.
Beam waveguide antenna.
A beam waveguide antenna is a type of complicated Cassegrain antenna with a long radio wave path to allow the feed electronics to be located at ground level. It is used in very large steerable radio telescopes and satellite ground antennas, where the feed electronics is too complicated and bulky, or requires too much maintenance and alterations, to locate on the dish; for example those using cryogenically-cooled amplifiers. The beam of incoming radio waves from the secondary reflector is reflected by additional mirrors in a long twisting path through the axes of the altazimuth mount, so the antenna can be steered without interrupting the beam, and then down through the antenna tower to a feed building at ground level.
History.
The Cassegrain antenna design was adapted from the Cassegrain telescope, a type of reflecting telescope developed around 1672 and attributed to French priest Laurent Cassegrain. The first Cassegrain antenna was invented and built in Japan in 1963 by NTT, KDDI and Mitsubishi Electric. The 20 meter I-1 antenna operated at 6.4, 4.2, and 1.7 GHz, and was used in October 1963 in the first trans-Pacific satellite television relay experiments.

</doc>
<doc id="40854" url="https://en.wikipedia.org/wiki?curid=40854" title="Cell relay">
Cell relay

In computer networking, cell relay refers to a method of statistically multiplexing small fixed-length packets, called "cells", to transport data between computers or kinds of network equipment. It is an unreliable, connection-oriented packet switched data communications protocol.
Transmission Rates.
Cell relay transmission rates usually are between 56 kbit/s and several gigabits per second. ATM, a particularly popular form of cell relay, is most commonly used for home DSL connections, which often runs between 128 kbit/s and 1.544 Mbit/s (DS1), and for high-speed backbone connections (OC-3 and faster).
Cell relay protocols have neither flow control nor error correction capability, are information-content independent, and correspond only to layers one and two of the OSI Reference Model. 
Cell relay can be used for delay- and jitter-sensitive traffic such as voice and video.
How Cell Relay Works.
Cell relay systems break variable-length user packets into groups of fixed-length cells, that add addressing and verification information. Frame length is fixed in networking hardware, based on time delay and user packet-length considerations. One user data message may be segmented over many cells. 
Cell relay systems may also carry bitstream-based data such as PDH traffic, by breaking it into streams of cells, with a lightweight synchronization and clock recovery shim. Thus cell relay systems may potentially carry any combination of stream-based and packet-based data. This is a form of statistical time division multiplexing.
Cell relay is an implementation of fast packet-switching technology that is used in connection-oriented broadband integrated services digital networks (B-ISDN, and its better-known supporting technology ATM) and connectionless IEEE 802.6 switched multi-megabit data service (SMDS). 
At any time there is information to be transmitted; the switch basically sends the data units. Connections don’t have to be negotiated like circuit switching. Channels don’t have to be allocated because channels do not exist in ATM, and on condition that there is an adequate amount of bandwidth to maintain it, there can be indefinite transmissions over the same facility.
Cell relay utilizes data cells of a persistent size. Frames are comparable to data packets; however they contrast from cells in that they may fluctuate in size based on circumstances. This type of technology is not secure for the reason that its procedures do not support error handling or data recovery. Per se, all delicate and significant transmissions may perhaps be transported faster via fixed-sized cells, which are simpler to transmit compared to variable-sized frames or packets.
Reliability.
Cell relay is extremely reliable for transporting vital data. Switching devices give the precise method to cells as each endpoint address embedded in a cell. An example of cell relay is ATM, a prevalent form utilized to transfer a cell with a fixed size of 53 bytes.
References.
Minoli, Daniel, and Michael Vitella. ATM and Cell Relay Service for Corporate Environments. New York: McGraw-Hill, 1994. Print.
Minoli, Daniel, and George Dobrowski. Principles of Signaling for Cell Relay and Frame Relay. Boston: Artech House, 1995. Print.
Minoli, Daniel, and George Dobrowski. Principles of Signaling for Cell Relay and Frame Relay. Boston: Artech House, 1995. Print.
Any Transport over MPLS - Cisco Systems." Cisco Systems, Inc. Web. 29 Nov. 2011. <http://www.cisco.com/en/US/docs/ios/12_0s/feature/guide/fsatom26.html>.
Davidson, Robert P. Broadband Networking ABCs for Managers: ATM, BISDN, Cell/frame Relay to SONET. New York: John Wiley & Sons, 1994. Print.
Conti, Marco, Enrico Gregori, and Luciano Lenzini. Metropolitan Area Networks. London: Springer, 1997. Print.

</doc>
<doc id="40858" url="https://en.wikipedia.org/wiki?curid=40858" title="Caesium standard">
Caesium standard

A caesium standard or caesium atomic clock is a primary frequency standard in which electronic transitions between the two hyperfine ground states of caesium-133 atoms are used to control the output frequency. The first caesium clock was built by Louis Essen in 1955 at the National Physical Laboratory in the UK.
Caesium clocks are the most accurate commercially produced time and frequency standards, and serve as the primary standard for the definition of the second in SI (the metric system). By definition, radiation produced by the transition between the two hyperfine ground states of caesium (in the absence of external influences such as the Earth's magnetic field) has a frequency of exactly 9,192,631,770 Hz. That value was chosen so that the caesium second equalled, to the limit of human measuring ability in 1960 when it was adopted, the existing standard ephemeris second based on the Earth's orbit around the Sun. Because no other measurement involving time had been as precise, the effect of the change was less than the experimental uncertainty of all existing measurements.

</doc>
<doc id="40860" url="https://en.wikipedia.org/wiki?curid=40860" title="Channel">
Channel

Channel, channels, and similar terms may refer to:

</doc>
<doc id="40861" url="https://en.wikipedia.org/wiki?curid=40861" title="Channel noise level">
Channel noise level

In telecommunications, the term channel noise level has the following meanings:

</doc>
<doc id="40862" url="https://en.wikipedia.org/wiki?curid=40862" title="Channel reliability">
Channel reliability

In telecommunication, channel reliability (ChR) is the percentage of time a channel was available for use in a specified period of scheduled availability. 
Channel reliability is given by 
where "T" o is the channel total outage time, "T" s is the channel total scheduled time, and "T" a is the channel total available time.

</doc>
<doc id="40863" url="https://en.wikipedia.org/wiki?curid=40863" title="Channel service unit">
Channel service unit

In telecommunications, a channel service unit (CSU) is a line Bridging device for use with T-carrier that:
Common varieties.
CSUs can be categorized by the class of service they support (DS1, DS3, DDS, etc.) and by the capabilities within that class. For example, basic DS1 (T1) CSUs support loopback of each interface and will produce Alarm indication signal to the provider's NIU (Network Interface Device) in the event of loss of signal from the customer-premises equipment (CPE). More advanced units will include internal monitors of the performance of the carrier in both directions and may have test pattern generation and monitor capabilities.
Common practice.
CSUs are required by PSTN providers at digital interfaces that terminate in a DSU on the customer side. They are not used when the service terminates in a modem, such as the DSL family of service. The maintenance capabilities of the CSU provide important guidance as to whether the provider needs to dispatch a repairman to the customer location.

</doc>
<doc id="40864" url="https://en.wikipedia.org/wiki?curid=40864" title="Character-count integrity">
Character-count integrity

Character-count integrity is a telecommunications term for the ability of a certain link to preserve the number of characters in a message (per unit time, in the case of a user-to-user connection). Character-count integrity is not the same as character integrity, which requires that the characters delivered be, in fact, exactly the same as they were originated.

</doc>
<doc id="40865" url="https://en.wikipedia.org/wiki?curid=40865" title="Character interval">
Character interval

Character interval: In a communications system, the total number of unit intervals required to transmit any given character, including synchronizing, information, error checking, or control characters, but not including signals that are not associated with individual characters. 
An example of a time interval that is excluded when determining character interval is any time added between the end of a stop signal and the beginning of the next start signal to accommodate changing transmission conditions, such as a change in data signaling rate or buffering requirements. This added time is defined as a part of the intercharacter interval.

</doc>
<doc id="40866" url="https://en.wikipedia.org/wiki?curid=40866" title="Characteristic impedance">
Characteristic impedance

The characteristic impedance or surge impedance (usually written Z0) of a uniform transmission line is the ratio of the amplitudes of voltage and current of a single wave propagating along the line; that is, a wave travelling in one direction in the absence of reflections in the other direction. Characteristic impedance is determined by the geometry and materials of the transmission line and, for a uniform line, is not dependent on its length. The SI unit of characteristic impedance is the ohm.
The characteristic impedance of a lossless transmission line is purely real, with no reactive component. Energy supplied by a source at one end of such a line is transmitted through the line without being dissipated in the line itself. A transmission line of finite length (lossless or lossy) that is terminated at one end with an impedance equal to the characteristic impedance appears to the source like an infinitely long transmission line and produces no reflections.
Transmission line model.
The characteristic impedance of a transmission line is the ratio of the voltage and current of a wave travelling along the line. When the wave reaches the end of the line, in general, there will be a reflected wave which travels back along the line in the opposite direction. When this wave reaches the source, it adds to the transmitted wave and the ratio of the voltage and current at the input to the line will no longer be the characteristic impedance. This new ratio is called the input impedance. The input impedance of an infinite line is equal to the characteristic impedance since the transmitted wave is never reflected back from the end. It can be shown that an equivalent definition is: the characteristic impedance of a line is that impedance which when terminating an arbitrary length of line at its output will produce an input impedance equal to the characteristic impedance. This is so because there is no reflection on a line terminated in its own characteristic impedance.
Applying the transmission line model based on the telegrapher's equations, the general expression for the characteristic impedance of a transmission line is:
where
Although an infinite line is assumed, since all quantities are per unit length, the characteristic impedance is independent of the length of the transmission line.
The voltage and current phasors on the line are related by the characteristic impedance as:
where the superscripts formula_9 and formula_10 represent forward- and backward-traveling waves, respectively. A surge of energy on a finite transmission line will see an impedance of "Z"0 prior to any reflections arriving, hence "surge impedance" is an alternative name for characteristic impedance.
Lossless line.
The analysis of lossless lines provides an accurate approximation for real transmission lines that simplifies the mathematics considered in modeling transmission lines. A lossless line is defined as a transmission line that has no line resistance. This would imply that the line has infinite conductivity and acts like a perfect conductor. For a lossless line, "R" and "G" are both zero, so the equation for characteristic impedance derived above reduces to:
The above expression is wholly real, since the imaginary term "j" has canceled out, implying that "Z0" is purely resistive. For a lossless line terminated in "Z0", there is no loss of current across the line, and so the voltage remains the same along the line. The lossless line model is a useful approximation for many practical cases, such as low-loss transmission lines and transmission lines with high frequency. For both of these cases, R and G are much smaller than ωL and ωC, respectively, and can thus be ignored.
Surge impedance loading.
In electric power transmission, the characteristic impedance of a transmission line is expressed in terms of the surge impedance loading (SIL), or natural loading, being the power loading at which reactive power is neither produced nor absorbed:
in which formula_13 is the line-to-line voltage in volts.
Loaded below its SIL, a line supplies reactive power to the system, tending to raise system voltages. Above it, the line absorbs reactive power, tending to depress the voltage. The Ferranti effect describes the voltage gain towards the remote end of a very lightly loaded (or open ended) transmission line. Underground cables normally have a very low characteristic impedance, resulting in an SIL that is typically in excess of the thermal limit of the cable. Hence a cable is almost always a source of reactive power.

</doc>
<doc id="40867" url="https://en.wikipedia.org/wiki?curid=40867" title="Chip">
Chip

Chip or chips may refer to:

</doc>
<doc id="40868" url="https://en.wikipedia.org/wiki?curid=40868" title="Chirping">
Chirping

Chirping may refer to: 

</doc>
<doc id="40870" url="https://en.wikipedia.org/wiki?curid=40870" title="Circuit">
Circuit

Circuit may refer to:

</doc>
<doc id="40871" url="https://en.wikipedia.org/wiki?curid=40871" title="Circuit noise level">
Circuit noise level

Circuit noise level: At any point in a transmission system, the ratio of the circuit noise at that point to an arbitrary level chosen as a reference. 
"Note:" The circuit noise level is usually expressed in dBrn0, signifying the reading of a circuit noise meter, or in dBa0, signifying circuit noise meter reading adjusted to represent an interfering effect under specified conditions.

</doc>
<doc id="40872" url="https://en.wikipedia.org/wiki?curid=40872" title="Circuit reliability">
Circuit reliability

Circuit reliability (also time availability) ("CiR") is the percentage of time an electronic circuit was available for use in a specified period of scheduled availability. Circuit reliability is given by where "T" o is the circuit total outage time, "Ts" is the circuit total scheduled time, and "T" a is the circuit total available time. formula_1 
In addition, circuit reliability is the expected lifespan of operation of a functioning system under nominal conditions.

</doc>
<doc id="40873" url="https://en.wikipedia.org/wiki?curid=40873" title="Circuit restoration">
Circuit restoration

In telecommunication, circuit restoration is the process by which a communications circuit is established between two users after disruption or loss of the original circuit. The loss may be widespread due to a natural disaster like an ice storm or hurricane, or local by being cut underground in construction or damaged in a thunderstorm or car accident.
Circuit restoration is usually performed in accordance with planned procedures and priorities. Restoration may be effected automatically, such as by switching to a hot standby, or manually, such as by manual patching.

</doc>
<doc id="40874" url="https://en.wikipedia.org/wiki?curid=40874" title="Circuit switching">
Circuit switching

Circuit switching is a methodology of implementing a telecommunications network in which two network nodes establish a dedicated communications channel (circuit) through the network before the nodes may communicate. The circuit guarantees the full bandwidth of the channel and remains connected for the duration of the communication session. The circuit functions as if the nodes were physically connected as with an electrical circuit.
The defining example of a circuit-switched network is the early analog telephone network. When a call is made from one telephone to another, switches within the telephone exchanges create a continuous wire circuit between the two telephones, for as long as the call lasts.
Circuit switching contrasts with packet switching which divides the data to be transmitted into packets transmitted through the network independently. In packet switching, instead of being dedicated to one communication session at a time, network links are shared by packets from multiple competing communication sessions, resulting in the loss of the quality of service guarantees that are provided by circuit switching.
In circuit switching, the bit delay is constant during a connection, as opposed to packet switching, where packet queues may cause varying and potentially indefinitely long packet transfer delays. No circuit can be degraded by competing users because it is protected from use by other callers until the circuit is released and a new connection is set up. Even if no actual communication is taking place, the channel remains reserved and protected from competing users.
Virtual circuit switching is a packet switching technology that emulates circuit switching, in the sense that the connection is established before any packets are transferred, and packets are delivered in order.
While circuit switching is commonly used for connecting voice circuits, the concept of a dedicated path persisting between two communicating parties or nodes can be extended to signal content other than voice. Its advantage is that it provides for continuous transfer without the overhead associated with packets making maximal use of available bandwidth for that communication. Its disadvantage is that it can be relatively inefficient because unused capacity guaranteed to a connection cannot be used by other connections on the same network.
The call.
For call setup and control (and other administrative purposes), it is possible to use a separate dedicated signalling channel from the end node to the network. ISDN is one such service that uses a separate signalling channel while plain old telephone service (POTS) does not.
The method of establishing the connection and monitoring its progress and termination through the network may also utilize a separate control channel as in the case of links between telephone exchanges which use CCS7 packet-switched signalling protocol to communicate the call setup and control information and use TDM to transport the actual circuit data.
Early telephone exchanges are a suitable example of circuit switching. The subscriber would ask the operator to connect to another subscriber, whether on the same exchange or via an inter-exchange link and another operator. In any case, the end result was a physical electrical connection between the two subscribers' telephones for the duration of the call. The copper wire used for the connection could not be used to carry other calls at the same time, even if the subscribers were in fact not talking and the line was silent.
Compared with datagram packet switching.
Circuit switching contrasts with packet switching which divides the data to be transmitted into small units, called packets, transmitted through the network independently. Packet switching shares available network bandwidth between multiple communication sessions.
Multiplexing multiple telecommunications connections over the same physical conductor has been possible for a long time, but nonetheless each channel on the multiplexed link was either dedicated to one call at a time, or it was idle between calls.
In circuit switching, and virtual circuit switching, a route and bandwidth is reserved from source to destination. Circuit switching can be relatively inefficient because capacity is guaranteed on connections which are set up but are not in continuous use, but rather momentarily. However, the connection is immediately available while established.
Packet switching is the process of segmenting a message/data to be transmitted into several smaller packets. Each packet is labeled with its destination and a sequence number for ordering related packets, precluding the need for a dedicated path to help the packet find its way to its destination. Each packet is dispatched independently and each may be routed via a different path. At the destination, the original message is reordered, based on the packet number, to reproduce the original message. Datagram packet switching networks do not require a circuit to be established and allow many pairs of nodes to communicate concurrently over the same channel.

</doc>
<doc id="40875" url="https://en.wikipedia.org/wiki?curid=40875" title="Circular polarization">
Circular polarization

In electrodynamics, circular polarization of an electromagnetic wave is a polarization in which the electric field of the passing wave does not change strength but only changes direction in a rotary manner.
In electrodynamics the strength and direction of an electric field is defined by what is called an electric field vector. In the case of a circularly polarized wave, as seen in the accompanying animation, the tip of the electric field vector, at a given point in space, describes a circle as time progresses. If the wave is frozen in time, the electric field vector of the wave describes a helix along the direction of propagation.
Circular polarization is a limiting case of the more general condition of elliptical polarization. The other special case is the easier-to-understand linear polarization.
The phenomenon of polarization arises as a consequence of the fact that light behaves as a two-dimensional transverse wave.
General description.
On the right is an illustration of the electric field vectors of a circularly polarized electromagnetic wave. The electric field vectors have a constant magnitude but their direction changes in a rotary manner. Given that this is a plane wave, each vector represents the magnitude and direction of the electric field for an entire plane that is perpendicular to the axis. Specifically, given that this is a circularly polarized plane wave, these vectors indicate that the electric field, from plane to plane, has a constant strength while its direction steadily rotates. Refer to these two images in the plane wave article to better appreciate this. This light is considered to be right-hand, clockwise circularly polarized if viewed by the receiver. Since this is an electromagnetic wave each electric field vector has a corresponding, but not illustrated, magnetic field vector that is at a right angle to the electric field vector and proportional in magnitude to it. As a result, the magnetic field vectors would trace out a second helix if displayed.
Circular polarization is often encountered in the field of optics and in this section, the electromagnetic wave will be simply referred to as light.
The nature of circular polarization and its relationship to other polarizations is often understood by thinking of the electric field as being divided into two components which are at right angles to each other. Refer to the second illustration on the right. The vertical component and its corresponding plane are illustrated in blue while the horizontal component and its corresponding plane are illustrated in green. Notice that the rightward (relative to the direction of travel) horizontal component leads the vertical component by one quarter of a wavelength. It is this quadrature phase relationship which creates the helix and causes the points of maximum magnitude of the vertical component to correspond with the points of zero magnitude of the horizontal component, and vice versa. The result of this alignment is that there are select vectors, corresponding to the helix, which exactly match the maxima of the vertical and horizontal components. (To minimize visual clutter these are the only helix vectors displayed.)
To appreciate how this quadrature phase shift corresponds to an electric field that rotates while maintaining a constant magnitude, imagine a dot traveling clockwise in a circle. Consider how the vertical and horizontal displacements of the dot, relative to the center of the circle, vary sinusoidally in time and are out of phase by one quarter of a cycle. The displacements are said to be out of phase by one quarter of a cycle because the horizontal maximum displacement (toward the left) is reached one quarter of a cycle before the vertical maximum displacement is reached. Now referring again to the illustration, imagine the center of the circle just described, traveling along the axis from the front to the back. The circling dot will trace out a helix with the displacement toward our viewing left, leading the vertical displacement. Just as the horizontal and vertical displacements of the rotating dot are out of phase by one quarter of a cycle in time, the magnitude of the horizontal and vertical components of the electric field are out of phase by one quarter of a wavelength.
The next pair of illustrations is that of left-handed, counter-clockwise circularly polarized light when viewed by the receiver. Because it is left-handed, the rightward (relative to the direction of travel) horizontal component is now "lagging" the vertical component by one quarter of a wavelength rather than leading it.
Reversal of Handedness by Phase Shift.
To convert a given handedness of polarized light to the other handedness one can use a half-wave plate. A half-wave plate shifts a given component of light one half of a wavelength relative to the component to which it is orthogonal.
Reversal of Handedness by Reflection.
The handedness of polarized light is also reversed when it is reflected off of a surface at normal incidence. Upon such reflection, the rotation of the plane of polarization of the reflected light is identical to that of the incident field. However with propagation now in the "opposite" direction, the same rotation direction that would be described as "right handed" for the incident beam, is "left-handed" for propagation in the reverse direction, and vice versa. Aside from the reversal of handedness, the ellipticity of polarization is also preserved (except in cases of reflection by a birefringent surface).
Note that this principle only holds strictly for light reflected at normal incidence. For instance, right circularly polarized light reflected from a dielectric surface at grazing incidence (an angle beyond the Brewster angle) will still emerge as right handed, but elliptically, polarized. Light reflected by a metal at non-normal incidence will generally have its ellipticity changed as well. Such situations may be solved by decomposing the incident circular (or other) polarization into components of linear polarization parallel and perpendicular to the plane of incidence, commonly denoted "p" and "s" respectively. The reflected components in the "p" and "s" linear polarizations are found by applying the Fresnel coefficients of reflection, which are generally different for those two linear polarizations. Only in the special case of normal incidence, where there is no distinction between "p" and "s", are the Fresnel coefficients for the two components identical, leading to the above property.
Conversion to and from Linear Polarization.
Circularly polarized light can be converted into linearly polarized light by passing it through a quarter-wave plate. Passing linearly polarized light through a quarter-wave plate with its axes at 45° to its polarization axis will convert it to circular polarization. In fact, this is the most common way of producing circular polarization in practice. Note that passing linearly polarized light through a quarter-wave plate at an angle "other" than 45° will generally produce elliptical polarization.
Left/right handedness conventions.
Circular polarization may be referred to as right-handed or left-handed, and clockwise or anti-clockwise, depending on the direction in which the electric field vector rotates. Unfortunately, two opposing historical conventions exist.
From the point of view of the source.
Using this convention, polarization is defined from the point of view of the source. When using this convention, left or right handedness is determined by pointing one's left or right thumb away from the source, in the same direction that the wave is propagating, and matching the curling of one's fingers to the direction of the temporal rotation of the field at a given point in space. When determining if the wave is clockwise or anti-clockwise circularly polarized, one again takes the point of view of the source, and while looking away from the source and in the same direction of the wave’s propagation, one observes the direction of the field’s temporal rotation.
Using this convention, the electric field vector of a right handed circularly polarized wave is as follows:
formula_1
As a specific example, refer to the circularly polarized wave in the first animation. Using this convention that wave is defined as right-handed because when one points one's right thumb in the same direction of the wave’s propagation, the fingers of that hand curl in the same direction of the field’s temporal rotation. It is considered clockwise circularly polarized because from the point of view of the source, looking in the same direction of the wave’s propagation, the field rotates in the clockwise direction. The second animation is that of left-handed or anti-clockwise light using this same convention.
This convention is in conformity with the Institute of Electrical and Electronics Engineers (IEEE) standard and as a result it is generally used in the engineering community.
Quantum physicists also use this convention of handedness because it is consistent with their convention of handedness for a particle’s spin.
Radio astronomers also use this convention in accordance with an International Astronomical Union (IAU) resolution made in 1973.
From the point of view of the receiver.
In this alternative convention, polarization is defined from the point of view of the receiver. Using this convention, left or right handedness is determined by pointing one’s left or right thumb toward the source, against the direction of propagation, and then matching the curling of one's fingers to the temporal rotation of the field.
When using this convention, in contrast to the other convention, the defined handedness of the wave matches the handedness of the screw type nature of the field in space. Specifically, if one freezes a right-handed wave in time, when one curls the fingers of one’s right hand around the helix, the thumb will point in the direction which the helix progresses given that sense of rotation. Note that it is the nature of all screws and helices that it does not matter in which direction you point your thumb when determining its handedness.
When determining if the wave is clockwise or anti-clockwise circularly polarized, one again takes the point of view of the receiver and, while looking toward the source, against the direction of propagation, one observes the direction of the field’s temporal rotation.
Just as in the other convention, right-handedness corresponds to a clockwise rotation and left-handedness corresponds to an anti-clockwise rotation.
Many optics textbooks use this second convention.
Uses of the two conventions.
As stated earlier, there is significant confusion with regards to these two conventions. As a general rule the engineering, quantum physics, and radio astronomy communities use the first convention where the wave is observed from the point of view of the source. In many physics textbooks dealing with optics the second convention is used where the light is observed from the point of view of the receiver.
To avoid confusion, it is good practice to specify “as defined from the point of view of the source” or "as defined from the point of view of the receiver" when discussing polarization matters.
The archive of the US Federal Standard 1037C proposes two contradictory conventions of handedness.
FM radio.
The term "circular polarization" is often used erroneously to describe mixed polarity signals used mostly in FM radio (87.5 to 108.0 MHz in the USA), where a vertical and a horizontal component are propagated simultaneously by a single or a combined array. This has the effect of producing greater penetration into buildings and difficult reception areas than a signal with just one plane of polarization. This would be an instance where the polarization would more appropriately be called random polarization because the polarization at a receiver, although constant, will vary depending on the direction from the transmitter and other factors in the transmitting antenna design. See Stokes parameters.
The term "FM radio" above refers to broadcast radio, not 2-way radio (more properly called Land Mobile Radio), which uses vertical polarization almost exclusively.
Circular dichroism.
Circular dichroism (CD) is the differential absorption of left- and right-handed circularly polarized light. Circular dichroism is the basis of a form of spectroscopy that can be used to determine the optical isomerism and secondary structure of molecules.
In general, this phenomenon will be exhibited in absorption bands of any optically active molecule. As a consequence, circular dichroism is exhibited by most biological molecules, because of the dextrorotary (e.g. some sugars) and levorotary (e.g. some amino acids) molecules they contain. Noteworthy as well is that a secondary structure will also impart a distinct CD to its respective molecules. Therefore, the alpha helix, beta sheet and random coil regions of proteins and the double helix of nucleic acids have CD spectral signatures representative of their structures.
Also, under the right conditions, even non-chiral molecules will exhibit magnetic circular dichroism, that is, circular dichroism induced by a magnetic field.
Circularly polarized luminescence.
"Circularly polarized luminescence" (CPL) can occur when either a luminophore or an ensemble of luminophores is chiral. The extent to which emissions are polarized is quantified in the same way it is for circular dichroism, in terms of the "dissymmetry factor", also sometimes referred to as the "anisotropy factor". This value is given by
where formula_3 corresponds to the quantum yield of left-handed circularly polarized light, and formula_4 to that of right-handed light. The maximum absolute value of "g"em, corresponding to purely left- or right-handed circular polarization, is therefore 2. Meanwhile the smallest absolute value that "g"em can achieve, corresponding to linearly polarized or unpolarized light, is zero.
Mathematical description.
The classical sinusoidal plane wave solution of the electromagnetic wave equation for the electric and magnetic fields is
where k is the wavenumber,
is the angular frequency of the wave, formula_7 is an orthogonal formula_8 matrix whose columns span the transverse x-y plane and formula_9 is the speed of light.
Here
is the amplitude of the field and
is the normalized Jones vector in the x-y plane.
If formula_12 is rotated by formula_13 radians with respect to formula_14 and the x amplitude equals the y amplitude the wave is circularly polarized. The Jones vector is
where the plus sign indicates left circular polarization and the minus sign indicates right circular polarization. In the case of circular polarization, the electric field vector of constant magnitude rotates in the x-y plane.
If basis vectors are defined such that
and
then the polarization state can be written in the "R-L basis" as
where
and
Antennas.
A number of different types of antenna elements can be utilized to produce circularly polarized (or nearly so) radiation; following Balanis, one can use "dipole elements":
"two crossed dipoles provide the two orthogonal field components... If the two dipoles are identical, the field intensity of each along zenith ... would be of the same intensity. Also, if the two dipoles were fed with a 90° degree time-phase difference (phase quadrature), the polarization along zenith would be circular... One way to obtain the 90° time-phase difference between the two orthogonal field components, radiated respectively by the two dipoles, is by feeding one of the two dipoles with a transmission line which is 1/4 wavelength longer or shorter than that of the other", p.80;
or "helical elements":
"To achieve circular polarization axial or end-fire mode ... the circumference "C" of the helix must be ... with "C"/wavelength = 1 near optimum, and the spacing about "S" = wavelength/4." p.571;
or "patch elements":
"circular and elliptical polarizations can be obtained using various feed arrangements or slight modifications made to the elements... Circular polarization can be obtained if two orthogonal modes are excited with a 90° time-phase difference between them. This can be accomplished by adjusting the physical dimensions of the patch ... For a square patch element, the easiest way to excite ideally circular polarization is to feed the element at two adjacent edges ... The quadrature phase difference is obtained by feeding the element with a 90° power divider", p.859.
Quantum mechanics.
In the quantum mechanical view, light is composed of photons. Polarization is a manifestation of the intrinsic angular momentum (the spin) of the photon. More specifically, in quantum mechanics the direction of spin of a photon is tied to the handedness of the circularly polarized light and the spin of a beam of photons is similar to the spin of a beam of particles, such as electrons.
In nature.
Only a few mechanisms in nature are known to systematically produce circularly polarized light. In 1911, Albert Abraham Michelson discovered that light reflected from the golden scarab beetle "Chrysina resplendens" is preferentially left-polarized. Since then, circular polarization has been measured in several other scarab beetles like "Chrysina gloriosa", as well as some crustaceans such as the mantis shrimp. In these cases, the underlying mechanism is the molecular-level helicity of the chitinous cuticle.
The bioluminescence of the larvae of fireflies is also circularly polarized, as reported in 1980 for the species "Photuris lucicrescens" and "Photuris versicolor". For fireflies, it is more difficult to find a microscopic explanation for the polarization, because the left and right lanterns of the larvae were found to emit polarized light of opposite senses. The authors suggest that the light begins with a linear polarization due to inhomogeneties inside aligned photocytes, and it picks up circular polarization while passing through linearly birefringent tissue.
Water-air interfaces provide another source of circular polarization. Sunlight that gets scattered back up towards the surface is linearly polarized. If this light is then totally internally reflected back down, its vertical component undergoes a phase shift. To an underwater observer looking up, the faint light outside Snell's window therefore is (partially) circularly polarized.
Weaker sources of circular polarization in nature include multiple scattering by linear polarizers, as in the circular polarization of starlight, and selective absorption by circularly dichroic media.
Two species of Mantis Shrimp have been reported to be able to detect circular polarized light.
Starlight.
The circular polarization of starlight has been observed to be a function of the linear polarization of starlight.
Starlight becomes partially linearly polarized by scattering from elongated interstellar dust grains whose long axes tend to be oriented perpendicular to the galactic magnetic field. According to the Davis-Greenstein mechanism, the grains spin rapidly with their rotation axis along the magnetic field. Light polarized along the direction of the magnetic field perpendicular to the line of sight is transmitted, while light polarized in the plane defined by the rotating grain is blocked. Thus the polarization direction can be used to map out the galactic magnetic field. The degree of polarization is on the order of 1.5% for stars at 1000 parsecs distance.
Normally, a much smaller fraction of circular polarization is found in starlight. Serkowski, Mathewson and Ford measured the polarization of 180 stars in UBVR filters. They found a maximum fractional circular polarization of formula_21, in the R filter.
The explanation is that the interstellar medium is optically thin. Starlight traveling through a kiloparsec column undergoes about a magnitude of extinction, so that the optical depth ~ 1. An optical depth of 1 corresponds to a mean free path, which is the distance, on average that a photon travels before scattering from a dust grain. So on average, a starlight photon is scattered from a single interstellar grain; multiple scattering (which produces circular polarization) is much less likely. Observationally, the linear polarization fraction p ~ 0.015 from a single scattering; circular polarization from multiple scattering goes as formula_22, so we expect a circularly polarized fraction of formula_23.
Light from early-type stars has very little intrinsic polarization. Kemp et al. measured the optical polarization of the Sun at sensitivity of formula_24; they found upper limits of formula_25 for both formula_26 (fraction of linear polarization) and formula_27 (fraction of circular polarization).
The interstellar medium can produce circularly polarized (CP) light from unpolarized light by sequential scattering from elongated interstellar grains aligned in different directions. One possibility is twisted grain alignment along the line of sight due to variation in the galactic magnetic field; another is the line of sight passes through multiple clouds. For these mechanisms the maximum expected CP fraction is formula_28, where formula_26 is the fraction of linearly polarized (LP) light. Kemp & Wolstencroft found CP in six early-type stars (no intrinsic polarization), which they were able to attribute to the first mechanism mentioned above. In all cases, formula_30 in blue light.
Martin showed that the interstellar medium can convert LP light to CP by scattering from partially aligned interstellar grains having a complex index of refraction. This effect was observed for light from the Crab Nebula by Martin, Illing and Angel.
An optically thick circumstellar environment can potentially produce much larger CP than the interstellar medium. Martin suggested that LP light can become CP near a star by multiple scattering in an optically thick asymmetric circumstellar dust cloud. This mechanism was invoked by Bastien, Robert and Nadeau, to explain the CP measured in 6 T-Tauri stars at a wavelength of 768 nm. They found a maximum CP of formula_31. Serkowski measured CP of formula_32 for the red supergiant NML Cygni and formula_33 in the long period variable M star VY Canis Majoris in the H band, ascribing the CP to multiple scattering in circumstellar envelopes. Chrysostomou et al. found CP with q of up to 0.17 in the Orion OMC-1 star-forming region, and explained it by reflection of starlight from aligned oblate grains in the dusty nebula.
Circular polarization of zodiacal light and Milky Way diffuse galactic light was measured at wavelength of 550 nm by Wolstencroft and Kemp. They found values of formula_34, which is higher than for ordinary stars, presumably because of multiple scattering from dust grains.

</doc>
<doc id="40876" url="https://en.wikipedia.org/wiki?curid=40876" title="Circulator">
Circulator

A circulator is a passive non-reciprocal three- or four-port device, in which a microwave or radio frequency signal entering any port is transmitted to the next port in rotation (only). A "port" in this context is a point where an external waveguide or transmission line (such as a microstrip line or a coaxial cable), connects to the device. For a three-port circulator, a signal applied to port 1 only comes out of port 2; a signal applied to port 2 only comes out of port 3; a signal applied to port 3 only comes out of port 1, so to within a phase-factor, the scattering matrix for an ideal three-port circulator is
Types.
Depending on the materials involved, circulators fall into two main categories: ferrite circulators and nonferrite circulators.
Ferrite.
Ferrite circulators are radio frequency circulators which are composed of magnetised ferrite materials. They fall into two main classes: 4-port waveguide circulators based on Faraday rotation of waves propagating in a magnetised material, and 3-port "Y-junction" circulators based on cancellation of waves propagating over two different paths near a magnetised material. Waveguide circulators may be of either type, while more compact devices based on stripline are of the 3-port type. Two or more Y-junctions can be combined in a single component to give four or more ports, but these differ in behaviour from a true 4-port circulator. A permanent magnet produces the magnetic flux through the waveguide. Ferrimagnetic garnet crystal is used in optical circulators.
Though ferrite circulators can provide good 'forward' signal circulation while suppressing greatly the 'reverse' circulation, their major shortcomings, especially at low frequencies, are the bulky sizes and the narrow bandwidths.
Nonferrite.
Early work on nonferrite circulators includes active circulators using transistors that are non-reciprocal in nature. In contrast to ferrite circulators which are passive devices, active circulators require power. Major issues associated with transistor-based active circulators are the power limitation and the signal-to-noise degradation, which are critical when it is used as a duplexer for sustaining the strong transmit power and clean reception of the signal from the antenna.
varactors offer one solution. One study employed a structure similar to a time-varying transmission line with the effective nonreciprocity triggered by a one-direction propagating carrier pump. This is like an AC-powered active circulator. The research claimed to be able to achieve positive gain and low noise for receiving path and broadband nonreciprocity. Another study used resonance with nonreciprocity triggered by angular-momentum biasing, which more closely mimics the way that signals passively circulate in a ferrite circulator. 
In April, 2016 a research team presented the first integrated circuit circulator. It offers the potential for full duplex communication (transmitting and receiving at the same time with a single shared antenna) over a single frequency. The device uses capacitors and a clock and is much smaller than conventional devices.
Applications.
Isolator.
When one port of a three-port circulator is terminated in a matched load, it can be used as an "isolator", since a signal can travel in only one direction between the remaining ports. An isolator is used to shield equipment on its input side from the effects of conditions on its output side; for example, to prevent a microwave source being detuned by a mismatched load.
Duplexer.
In radar, circulators are used as a type of duplexer, to route signals from the transmitter to the antenna and from the antenna to the receiver, without allowing signals to pass directly from transmitter to receiver. The alternative type of duplexer is a "transmit-receive switch" ("TR switch") that alternates between connecting the antenna to the transmitter and to the receiver. The use of chirped pulses and a high dynamic range may lead to temporal overlap of the sent and received pulses, however, requiring a circulator for this function.
In the future-generation cellular communication, people talk about full-duplex radios, where signals can be simultaneously transmitted and received at the same frequency. Given the currently limited, crowded spectrum resource, full-duplexing can directly benefit the wireless communication by twice of the data throughput speed. Currently, the wireless communication is still performed with "half-duplex", where either the signals are transmitted or received at different time frames, if at the same frequency (typically in radar), or the signals are simultaneously transmitted and received at different frequencies (realized by a set of filters called a diplexer).
Reflection amplifier.
A "reflection amplifier" is a type of microwave amplifier circuit utilizing negative resistance diodes such as tunnel diodes and Gunn diodes. Negative resistance diodes can amplify signals, and often perform better at microwave frequencies than two-port devices. However, since the diode is a one-port (two terminal) device, a nonreciprocal component is needed to separate the outgoing amplified signal from the incoming input signal. By using a 3-port circulator with the signal input connected to one port, the biased diode connected to a second, and the output load connected to the third, the output and input can be uncoupled.

</doc>
<doc id="40877" url="https://en.wikipedia.org/wiki?curid=40877" title="Cladding">
Cladding

Cladding of one with another. It may refer to the following:

</doc>
<doc id="40878" url="https://en.wikipedia.org/wiki?curid=40878" title="Cladding mode">
Cladding mode

In fiber optics, a cladding mode is a mode that is confined to the cladding of an optical fiber by virtue of the fact that the cladding has a higher refractive index than the surrounding medium, which is either air or the primary polymer overcoat. These modes are generally undesired.
Modern fibers have a primary polymer overcoat with a refractive index that is slightly higher than that of the cladding, so that light propagating in the cladding is rapidly attenuated and disappears after only a few centimeters of propagation. An exception to this is double-clad fiber, which is designed to support a mode in its inner cladding, as well as one in its core.

</doc>
<doc id="40879" url="https://en.wikipedia.org/wiki?curid=40879" title="Clearing">
Clearing

Clearing may refer to:

</doc>
<doc id="40881" url="https://en.wikipedia.org/wiki?curid=40881" title="Thomas L. Cleave">
Thomas L. Cleave

Thomas Latimer (Peter) Cleave (1906–1983) was a surgeon captain who researched the negative health effects of consuming refined carbohydrate (notably sugar and white flour) which would not have been available during early human evolution. Known as `Peter' to his friends and colleagues, Cleave was born in Exeter in 1906, and educated at Clifton College. Between 1922-27, he attended medical schools at the Bristol Royal Infirmary, and St Mary's Hospital, London,where he was an academic prodigy winning prize after prize and qualifying at the early age of 21, having passed his primary FRCS examination at the age of 18 and ultimately achieving MRCS and LRCP.
At Bristol, one of his teachers was Rendle Short, who had proposed that appendicitis is caused by a lack of cellulose in the diet (it is worth noting, perhaps, from a biographical perspective, that Cleave's sister had died at the age of eight years from a perforated appendix). Charles Darwin's writings provided the intellectual framework to Cleave's lifelong engagement with the relationship between diet and health, built upon the premise that the human body is ill-adapted to the diet of modern (western) man.
Cleave’s interest focussed on preventative medicine where he observed the harmful effects of the overconsumption of refined carbohydrates such as sugar and refined flour which he called ‘The Saccharine Disease’. He noticed that the saccharine manifestations did not occur in wild creatures or among primitive people living on traditional unrefined food.’
Military career.
Between 1938-1940, he served as Medical Specialist at RN Hospital, Hong Kong. It was during his war service, in 1941, whilst on the battleship King George V, that he acquired his naval nickname `the bran man' when he had sacks of bran brought on board to combat the common occurrence of constipation amongst sailors. Cleave’s intention was ‘to give them bowel movements as efficient as the guns they fired.’ The ship assisted in the sinking of the Bismarck. Cleave was on the bridge as the Bismarck was going down and a fellow officer exclaimed to him ‘Well done Doc, you deserve a medal, our bowels were working like clockwork!’
Following war service, he worked at Royal Naval Hospitals in Chatham (1945–1948), Malta (1949–1951) and Plymouth (1952–1953). He retired from the Royal Navy in 1962 as Surgeon Captain, having finished his naval career as Director of Medical Research at the RN Medical School
Post-military career.
Cleaves unique contribution to medical thought was his realisation that three mechanisms were at work when refined carbohydrates are eaten; fibre depletion, over-consumption and protein stripping, with over-consumption being the most serious.
In 1969 Dr. Cleave brought public attention to the low amount of dietary fiber in modern diets that had become rich in processed ingredients. His work was bolstered by the supporting work of Dr. Denis Burkitt.
Awards and Honors.
In 1979,Cleave was awarded both the Harben gold medal of the Royal Institute of Public Health and Hygiene and the Gilbert Blane medal for naval medicine by the Royal College of Physicians and Surgeons. Dr. Cleave was a 2009 inductee into the Orthomolecular Medicine Hall of Fame.

</doc>
<doc id="40882" url="https://en.wikipedia.org/wiki?curid=40882" title="Clipping">
Clipping

Clipping may refer to:

</doc>
<doc id="40883" url="https://en.wikipedia.org/wiki?curid=40883" title="Closed captioning">
Closed captioning

Closed captioning symbol.svg|thumb|"CC in a TV" symbol was created at WGBH]]
Closed captioning (CC) and subtitling are both processes of displaying text on a television, video screen, or other visual display to provide additional or interpretive information. Both are typically used as a transcription of the audio portion of a program as it occurs (either verbatim or in edited form), sometimes including descriptions of non-speech elements. Other uses have been to provide a textual alternative language translation of a presentation's primary audio language that is usually burned-in (or "open") to the video and unselectable. HTML5 defines subtitles as a "transcription or translation of the dialogue ... when sound is available but not understood" by the viewer (for example, dialogue in a foreign language) and captions as a "transcription or translation of the dialogue, sound effects, relevant musical cues, and other relevant audio information ... when sound is unavailable or not clearly audible" (for example, when audio is muted or the viewer is deaf or hard of hearing).
Terminology.
The term "closed" (versus "open") indicates that the captions are not visible until activated by the viewer, usually via the remote control or menu option. On the other hand, "open", "burned-in", "baked on", or "hard-coded" captions are visible to all viewers.
Most of the world does not distinguish captions from subtitles. In the United States and Canada, however, these terms do have different meanings. "Subtitles" assume the viewer can hear but cannot understand the language or accent, or the speech is not entirely clear, so they transcribe only dialogue and some on-screen text. "Captions" aim to describe to the deaf and hard of hearing all significant audio content — spoken dialogue and non-speech information such as the identity of speakers and, occasionally, their manner of speaking – along with any significant music or sound effects using words or symbols. Also the term "closed caption" has come to be used to also refer to the North American EIA-608 encoding that is used with NTSC-compatible video.
The United Kingdom, Ireland, and most other countries do not distinguish between subtitles and closed captions, and use "subtitles" as the general term. The equivalent of "captioning" is usually referred to as "subtitles for the hard of hearing". Their presence is referenced on screen by notation which says "Subtitles", or previously "Subtitles 888" or just "888" (the latter two are in reference to the conventional teletext channel for captions), which is why the term "subtitle" is also used to refer to the Ceefax-based Teletext encoding that is used with PAL-compatible video. The term "subtitle" has been replaced with "caption" in a number of PAL markets that still use Teletext – such as Australia and New Zealand – that purchase large amounts of imported US material, with much of that video having had the US CC logo already superimposed over the start of it. In New Zealand, broadcasters superimpose an ear logo with a line through it that represents subtitles for the hard of hearing, even though they are currently referred to as captions. In the UK, modern digital television services have subtitles for the majority of programs, so it is no longer necessary to highlight which have captioning and which do not.
History.
Open captioning.
Regular open captioned broadcasts began on PBS's "The French Chef" in 1972. WGBH began open captioning of the programs "Zoom", "ABC World News Tonight", and "Once Upon a Classic" shortly thereafter.
Technical development of closed captioning.
Closed captioning was first demonstrated at the First National Conference on Television for the Hearing Impaired in Nashville, Tennessee in
1971. A second demonstration of closed captioning was held at Gallaudet College (now Gallaudet University) on February 15, 1972 where ABC and the National Bureau of Standards demonstrated closed captions embedded within a normal broadcast of "The Mod Squad".
The closed captioning system was successfully encoded and broadcast in 1973 with the cooperation of PBS station WETA. As a result of these tests, the FCC in 1976 set aside line 21 for the transmission of closed captions. PBS engineers then developed the caption-editing consoles that would be used to caption prerecorded programs.
Real-time captioning, a process for captioning live broadcasts, was developed by the National Captioning Institute in 1982. In real-time captioning, court reporters trained to write at speeds of over 225 words per minute give viewers instantaneous access to live news, sports and entertainment. As a result, the viewer sees the captions within two to three seconds of the words being spoken, explaining spelling and grammatical errors and garbled characters.
Major US producers of captions are WGBH-TV, VITAC, CaptionMax and the National Captioning Institute. In the UK and Australasia, Red Bee Media, itfc and Independent Media Support are the major vendors.
Full-scale closed captioning.
The National Captioning Institute was created in 1979 in order to get the cooperation of the commercial television networks.
The first use of regularly scheduled closed captioning on American television occurred on March 16, 1980. Sears had developed and sold the Telecaption adapter, a decoding unit that could be connected to a standard television set. The first programs seen with captioning were a "Disney's Wonderful World" presentation of the film "Son of Flubber" on NBC, an "ABC Sunday Night Movie" airing of "Semi-Tough", and "Masterpiece Theatre" on PBS.
Legislative development in the U.S..
Until the passage of the Television Decoder Circuitry Act of 1990, television captioning was performed by a set-top box manufactured by Sanyo Electric and marketed by the National Captioning Institute (NCI). (At that time a set-top decoder cost about as much as a TV set itself, approximately $200.) Through discussions with the manufacturer it was established that the appropriate circuitry integrated into the television set would be less expensive than the stand-alone box, and Ronald May, then a Sanyo employee, provided the expert witness testimony on behalf of Sanyo and Gallaudet University in support of the passage of the bill. On January 23, 1991, the Television Decoder Circuitry Act of 1990 was passed by US Congress. This Act gave the Federal Communications Commission (FCC) power to enact rules on the implementation of Closed Captioning. This Act required all analog television receivers with screens of at least 13 inches or greater, either sold or manufactured, to have the ability to display closed captioning by July 1, 1993.
Also in 1990, the Americans with Disabilities Act (ADA) was passed to ensure equal opportunity for persons with disabilities. The ADA prohibits discrimination against persons with disabilities in public accommodations or commercial facilities. Title III of the ADA requires that public facilities – such as hospitals, bars, shopping centers and museums (but not movie theaters) – provide access to verbal information on televisions, films or slide shows.
The Telecommunications Act of 1996 expanded on the Decoder Circuity Act to place the same requirements on digital television receivers by July 1, 2002. All TV programming distributors in the U.S. are required to provide closed captions for Spanish language video programming as of January 1, 2010.
A bill, H.R. 3101, the Twenty-First Century Communications and Video Accessibility Act of 2010, was passed by the United States House of Representatives in July 2010. A similar bill, S. 3304, with the same name was passed by the United States Senate on August 5, 2010, by the House of Representatives on September 28, 2010, and was signed by President Barack Obama on October 8, 2010. The Act requires, in part, for ATSC-decoding set-top box remotes to have a button to turn on or off the closed captioning in the output signal. It also requires broadcasters to provide captioning for television programs redistributed on the Internet.
On February 20, 2014, the FCC unanimously approved the implementation of quality standards for closed captioning, addressing accuracy, timing, completeness, and placement. This is the first time the FCC has addressed quality issues in captions.
Legislative development in Australia.
The government of Australia provided seed funding in 1981 for the establishment of the Australian Caption Centre (ACC) and the purchase of equipment. Captioning by the ACC commenced in 1982 and a further grant from the Australian government enabled the ACC to achieve and maintain financial self-sufficiency. The ACC, now known as Media Access Australia, sold its commercial captioning division to Red Bee Media in December 2005. Red Bee Media continues to provide captioning services in Australia today.
Funding development in New Zealand.
In 1981, TVNZ held a telethon to raise funds for Teletext-encoding equipment used for the creation and editing of text-based broadcast services for the deaf. The service came into use in 1984 with caption creation and importing paid for as part of the public broadcasting fee until the creation of the NZ on Air taxpayer fund, which is used to provide captioning for NZ On Air content, TVNZ news shows and conversion of EIA-608 US captions to the preferred EBU STL format for only TV one, TV 2 and TV 3 with archived captions available to FOUR and select Sky programming. During the second half of 2012, TV3 and FOUR began providing non-Teletext DVB image-based captions on their HD service and used the same format on the satellite service, which has since caused major timing issues in relation to server load and the loss of captions from most SD DVB-S receivers, such as the ones Sky Television provides their customers. As of April 2, 2013 only the Teletext page 801 caption service will remain in use with the informational Teletext non-caption content being discontinued.
Application.
Closed captions were created for deaf or hard of hearing individuals to assist in comprehension. They can also be used as a tool by those learning to read, learning to speak a non-native language, or in an environment where the audio is difficult to hear or is intentionally muted. Captions can also be used by viewers who simply wish to read a transcript along with the program audio.
In the United States, the National Captioning Institute noted that English as a foreign or second language (ESL) learners were the largest group buying decoders in the late 1980s and early 1990s, before built-in decoders became a standard feature of US television sets. This suggested that the largest audience of closed captioning was people whose native language was not English. In the United Kingdom, of 7.5 million people using TV subtitles (closed captioning), 6 million have no hearing impairment.
Closed captions are also used in public environments, such as bars and restaurants, where patrons may not be able to hear over the background noise, or where multiple televisions are displaying different programs. In addition, online videos may be treated through digital processing of their audio content by various robotic algorithms (robots). Multiple chains of errors are the result. When a video is truly and accurately transcribed, then the closed-captioning publication serves a useful purpose, and the content is available for search engines to index and make available to users on the internet.
Some television sets can be set to automatically turn captioning on when the volume is muted.
Television and video.
For "live" programs, spoken words comprising the television program's soundtrack are transcribed by a human operator (a speech-to-text reporter) using stenotype or stenomask type of machines, whose phonetic output is instantly translated into text by a computer and displayed on the screen. This technique was developed in the 1970s as an initiative of the BBC's Ceefax teletext service. In collaboration with the BBC, a university student took on the research project of writing the first phonetics-to-text conversion program for this purpose. Sometimes, the captions of live broadcasts, like news bulletins, sports events, live entertainment shows, and other live shows fall behind by a few seconds. This delay is because the machine does not know what the person is going to say next, so after the person on the show says the sentence, the captions appear. Automatic computer speech recognition now works well when trained to recognize a single voice, and so since 2003, the BBC does live subtitling by having someone re-speak what is being broadcast. Live captioning is also a form of real-time text. Meanwhile, sport events on channels like ESPN are using court reporters, using a special (steno) keyboard and individually constructed "dictionaries."
In some cases, the transcript is available beforehand, and captions are simply displayed during the program after being edited. For programs that have a mix of pre-prepared and live content, such as news bulletins, a combination of the above techniques is used.
For prerecorded programs, commercials, and home videos, audio is transcribed and captions are prepared, positioned, and timed in advance.
For all types of NTSC programming, captions are "encoded" into line 21 of the vertical blanking interval – a part of the TV picture that sits just above the visible portion and is usually unseen. For ATSC (digital television) programming, three streams are encoded in the video: two are backward compatible "line 21" captions, and the third is a set of up to 63 additional caption streams encoded in EIA-708 format.
Captioning is modulated and stored differently in PAL and SECAM 625 line 25 frame countries, where teletext is used rather than in EIA-608, but the methods of preparation and the line 21 field used are similar. For home Betamax and VHS videotapes, a shift down of this line 21 field must be done due to the greater number of VBI lines used in 625 line PAL countries, though only a small minority of European PAL VHS machines support this (or any) format for closed caption recording. Like all teletext fields, teletext captions can't be stored by a standard 625 line VHS recorder (due to the lack of field shifting support), they are available on all professional S-VHS recordings due to all fields being recorded. Recorded Teletext caption fields also suffer from a higher number of caption errors due to increased number of bits and a low SNR especially on low bandwidth VHS. This is why Teletext captions used to be stored separately on floppy disk to the analogue master tape. DVDs have their own system for subtitles and/or captions that is digitally inserted in the data stream and encoded on playback in video field lines.
For older televisions, a set-top box or other decoder is usually required. In the US, since the passage of the Television Decoder Circuitry Act, manufacturers of most television receivers sold have been required to include closed captioning display capability. High-definition TV sets, receivers, and tuner cards are also covered, though the technical specifications are different (high-definition display screens, as opposed to high-definition TVs, may lack captioning). Canada has no similar law, but receives the same sets as the US in most cases.
During transmission, single byte errors can be replaced by a white space which can appear at the beginning of the program. More byte errors during EIA-608 transmission can affect the screen momentarily, by defaulting to a real-time mode such as the "roll up" style, type random letters on screen, and then revert to normal. Uncorrectable byte errors within the teletext page header will cause whole captions to be dropped. EIA-608 due to using only two characters per video frame sends these captions ahead of time storing them in a second buffer awaiting a command to display them, Teletext sends these in real-time.
The use of capitalization varies between caption provider, most providers caption use capitalize all words, while providers such as WGBH and non-US providers prefer to use mixed case letters.
There are two main styles of line 21 closed captioning:
Caption formatting.
TVNZ Access Services and Red Bee Media for BBC and Australia example:
UK IMS for ITV and Sky example:
US WGBH Access Services example:
US National Captioning Institute example:
US other provider example:
US in-house real-time roll-up example:
Non-US in-house real-time roll-up example:
Syntax.
For real-time captioning done outside of captioning facilities, the following syntax is used:
Styles of syntax that are used by various captioning producers:
Technical aspects.
There were many shortcomings in the original Line 21 specification from a typographic standpoint, since, for example, it lacked many of the characters required for captioning in languages other than English. Since that time, the core Line 21 character set has been expanded to include quite a few more characters, handling most requirements for languages common in North and South America such as French, Spanish, and Portuguese, though those extended characters are not required in all decoders and are thus unreliable in everyday use. The problem has been almost eliminated with a market specific full set of Western European characters and a private adopted Norpak extension for South Korean and Japanese markets. The full EIA-708 standard for digital television has worldwide character set support, but there has been little use of it due to EBU Teletext dominating DVB countries, which has its own extended character sets.
Captions are often edited to make them easier to read and to reduce the amount of text displayed onscreen. This editing can be very minor, with only a few occasional unimportant missed lines, to severe, where virtually every line spoken by the actors is condensed. The measure used to guide this editing is words per minute, commonly varying from 180 to 300, depending on the type of program. Offensive words are also captioned, but if the program is censored for TV broadcast, the broadcaster might not have arranged for the captioning to be edited or censored also. The "TV Guardian", a television set-top box, is available to parents who wish to censor offensive language of programs–the video signal is fed into the box and if it detects an offensive word in the captioning, the audio signal is bleeped or muted for that period of time.
Caption channels.
The Line 21 data stream can consist of data from several data channels multiplexed together. Odd field 1 can have four data channels: two separate synchronized captions (CC1, CC2) with caption-related text, such as website URLs (T1, T2). Even field 2 can have five additional data channels: two separate synchronized captions (CC3, CC4) with caption related text (T3, T4), and Extended Data Services (XDS) for Now/Next EPG details. XDS data structure is defined in CEA–608.
As CC1 and CC2 share bandwidth, if there is a lot of data in CC1, there will be little room for CC2 data and is generally only used for the primary audio captions. Similarly CC3 and CC4 share the second even field of line 21. Since some early caption decoders supported only single field decoding of CC1 and CC2, captions for SAP in a second language were often placed in CC2. This led to bandwidth problems, however, and the current U.S. Federal Communications Commission (FCC) recommendation is that bilingual programming should have the second caption language in CC3. Many Spanish television networks such as Univision and Telemundo, for example, provides English subtitles for many of its Spanish programs in CC3. Canadian broadcasters use CC3 for French translated SAPs, which is also a similar practice in South Korea and Japan.
Ceefax and Teletext can have a larger number of captions for other languages due to the use of multiple VBI lines. However, only European countries used a second subtitle page for second language audio tracks where either the NICAM dual mono or Zweikanalton were used.
HDTV interoperability issues.
Americas.
The US ATSC digital television system originally specified two different kinds of closed captioning datastream standards—the original analog compatible (available by Line 21) and the more modern digital only CEA-708 formats are delivered within the video stream. The US FCC mandates that broadcasters deliver (and generate, if necessary) both datastream formats with the CEA-708 format merely a conversion of the Line 21 format. The Canadian CRTC has not mandated that broadcasters either broadcast both datastream formats or exclusively in one format. Most broadcasters and networks to avoid large conversion cost outlays just provide EIA-608 captions along with a transcoded CEA-708 version encapsulated within CEA-708 packets.
Incompatibility issues with digital TV.
Many viewers find that when they acquire a digital television or set-top box they are unable to view closed caption (CC) information, even though the broadcaster is sending it and the TV is able to display it.
Originally, CC information was included in the picture ("line 21") via a composite video input, but there is no equivalent capability in the HDTV 720p/1080i interconnects (such as DVI, HDMI or component video) between the display and a "source". A "source", in this case, can be a DVD player or a terrestrial or cable digital television receiver. When CC information is encoded in the MPEG-2 data stream, only the device that decodes the MPEG-2 data (a source) has access to the closed caption information; there is no standard for transmitting the CC information to a display monitor separately. Thus, if there is CC information, the source device needs to overlay the CC information on the picture prior to transmitting to the display over the interconnect's video output.
Many source devices do not have the ability to overlay CC information, for controlling the CC overlay can be complicated. For example, the Motorola DCT-5xxx and -6xxx cable set-top receivers have the ability to decode CC information located on the MPEG-2 stream and overlay it on the picture, but turning CC on and off requires turning off the unit and going into a [//en.wikibooks.org/wiki/How_to_use_a_Motorola_DVR/Setup#Closed_Caption special setup menu] (it is not on the standard configuration menu and it cannot be controlled using the remote). Historically, DVD players, VCRs and set-top tuners did not need to do this overlaying, since they simply passed this information on to the TV, and they are not mandated to perform this overlaying.
Many modern digital television receivers can be directly connected to cables, but often cannot receive scrambled channels that the user is paying for. Thus, the lack of a standard way of sending CC information between components, along with the lack of a mandate to add this information to a picture, results in CC being unavailable to many hard-of-hearing and deaf users.
Europe/Australia.
The EBU Ceefax-based teletext systems are the source for closed captioning signals, thus when teletext is embedded into DVB-T or DVB-S the closed captioning signal is included. However, for DVB-T and DVB-S, it is not necessary for a teletext page signal to also be present (ITV1, for example, does not carry analogue teletext signals on Sky Digital, but does carry the embedded version, accessible from the "Services" menu of the receiver, or more recently by turning them off/on from a mini menu accessible from the "help" button).
New Zealand.
In New Zealand, captions use a EBU Ceefax-based teletext system on DVB broadcasts via satellite and cable television with the exception of MediaWorks New Zealand channels who completely switched to DVB RLE subtitles in 2012 on both Freeview satellite and UHF broadcasts, this decision was made based on the TVNZ practice of using this format on only DVB UHF broadcasts (aka Freeview HD). This made composite video connected TVs incapable of decoding the captions on their own. Also these pre-rendered subtitles use classic caption style opaque backgrounds with an overly large font size and obscure the picture more than the more modern, partially transparent backgrounds.
DTV standard captioning improvements.
The CEA-708 specification provides for dramatically improved captioning
As of 2009, however, most closed captioning for DTV environments is done using tools designed for analog captioning (working to the CEA-608 NTSC spec rather than the CEA-708 DTV spec). The captions are then run through transcoders made by companies like EEG Enterprises or Evertz, which convert the analog Line 21 caption format to the digital format. This means that none of the CEA-708 features are used unless they were also contained in CEA-608.
Uses of captioning in other mediums.
DVDs, BDs, & HD DVDs.
NTSC DVDs may carry closed captions in data packets of the MPEG-2 video streams inside of the Video-TS folder. Once played out of the analog outputs of a set top DVD player, the caption data is converted to the Line 21 format. They are output by the player to the composite video (or an available RF connector) for a connected TV's built-in decoder or a set-top decoder as usual. They can not be output on S-Video or component video outputs due to the lack of a colorburst signal on line 21. (Actually, regardless of this, if the DVD player is in interlaced rather than progressive mode, closed captioning "will" be displayed on the TV over component video input if the TV captioning is turned on and set to CC1.) When viewed on a personal computer, caption data can be viewed by software that can read and decode the caption data packets in the MPEG-2 streams of the DVD-Video disc. Windows Media Player (before Windows 7) in Vista supported only closed caption channels 1 and 2 (not 3 or 4). And Apple's DVD Player does not have the ability to read and decode Line 21 caption data which is recorded on a DVD made from an over-the-air broadcast. Apple's DVD Player can display some movie DVD captions.
In addition to Line 21 closed captions, video DVDs may also carry subtitles, which generally rendered from the EIA-608 captions as a bitmap overlay that can be turned on and off via a set top DVD player or DVD player software, just like the textual captions. This type of captioning is usually carried in a subtitle track labeled either "English for the hearing impaired" or, more recently, "SDH" (subtitled for the deaf and Hard of hearing). Many popular Hollywood DVD-Videos can carry both subtitles and closed captions (e.g. "Stepmom" DVD by Columbia Pictures). On some DVDs, the Line 21 captions may contain the same text as the subtitles; on others, only the Line 21 captions include the additional non-speech information (even sometimes song lyrics) needed for deaf and hard-of-hearing viewers. European Region 2 DVDs do not carry Line 21 captions, and instead list the subtitle languages available—English is often listed twice, one as the representation of the dialogue alone, and a second subtitle set which carries additional information for the deaf and hard-of-hearing audience. (Many deaf/ subtitle files on DVDs are reworkings of original teletext subtitle files.)
HD DVD and Blu-ray disc media cannot carry any VBI data such as Line 21 closed captioning due to the design of DVI-based High-Definition Multimedia Interface (HDMI) specifications that was only extended for synchronized digital audio replacing older analog standards, such as VGA, S-Video, component video and SCART. Both Blu-ray disc and HD DVD can use either PNG bitmap subtitles or 'advanced subtitles' to carry SDH type subtitling, the latter being an XML-based textual format which includes font, styling and positioning information as well as a unicode representation of the text. Advanced subtitling can also include additional media accessibility features such as "descriptive audio".
Movies.
There are several competing technologies used to provide captioning for movies in theaters. Cinema captioning falls into the categories of 'open' and 'closed.' The definition of "closed" captioning in this context is different from television, as it refers to any technology that allows as few as one member of the audience to view the captions.
Open captioning in a film theater can be accomplished through burned-in captions, projected text or bitmaps, or (rarely) a display located above or below the movie screen. Typically, this display is a large LED sign. In a digital theater, open caption display capability is built into the digital projector. Closed caption capability is also available, with the ability for 3rd-party closed caption devices to plug into the digital cinema server.
Probably the best-known closed captioning option for film theaters is the Rear Window Captioning System from the National Center for Accessible Media. Upon entering the theater, viewers requiring captions are given a panel of flat translucent glass or plastic on a gooseneck stalk, which can be mounted in front of the viewer's seat. In the back of the theater is an LED display that shows the captions in mirror image. The panel reflects captions for the viewer, but is nearly invisible to surrounding patrons. The panel can be positioned so that the viewer watches the movie through the panel, and captions appear either on or near the movie image. A company called Cinematic Captioning Systems has a similar reflective system called Bounce Back. A major problem for distributors has been that these systems are each proprietary, and require separate distributions to the theater to enable them to work. Proprietary systems also incur license fees.
For film projection systems, Digital Theater Systems, the company behind the DTS surround sound standard, has created a digital captioning device called the DTS-CSS (Cinema Subtitling System). It is a combination of a laser projector which places the captioning (words, sounds) anywhere on the screen and a thin playback device with a CD that holds many languages. If the Rear Window Captioning System is used, the DTS-CSS player is also required for sending caption text to the Rear Window sign located in the rear of the theater.
Special effort has been made to build accessibility features into digital projection systems (see digital cinema). Through SMPTE, standards now exist that dictate how open and closed captions, as well as hearing-impaired and visually impaired narrative audio, are packaged with the rest of the digital movie. This eliminates the proprietary caption distributions required for film, and the associated royalties. SMPTE has also standardized the communication of closed caption content between the digital cinema server and 3rd-party closed caption systems (the CSP/RPL protocol). As a result, new, competitive closed caption systems for digital cinema are now emerging that will work with any standards-compliant digital cinema server. These newer closed caption devices include cupholder-mounted electronic displays and wireless glasses which display caption text in front of the wearer's eyes. Bridge devices are also available to enable the use of Rear Window systems. As of mid-2010, the remaining challenge to the wide introduction of accessibility in digital cinema is the industry-wide transition to SMPTE DCP, the standardized packaging method for very highquality, secure distribution of digital movies.
Sports venues.
Captioning systems have also been adopted by some stadiums, typically through dedicated portions of their main scoreboards or as part of balcony fascia LED boards. These screens display captions of the public address announcer and other spoken content, such as those contained within in-game segments, public service announcements, and lyrics of songs played in-stadium. In some facilities, these systems were added as a result of discrimination lawsuits. Following a lawsuit under the Americans with Disabilities Act, FedEx Field added caption screens in 2006. After a similar lawsuit that declared special "deaf seating" areas with screen-mounted captioning, and later the use of smartphones to be insufficient due to the small size of text, University of Phoenix Stadium added dedicated caption displays in 2013.
Some stadiums utilize on-site captioners, while others outsource them to external providers who caption remotely. A prominent provider of in-arena captioning systems is Good Sport Captioning, founded by Patti White of St. Louis. White had worked as a stenographer at a courthouse near where Busch Stadium was being constructed, and reached a deal with the team to provide in-stadium captioning upon the stadium's 2006 opening—conducting her activity from her home. Patti later formed Good Sport Captioning to provide remote captioning for other teams and venues.
Video games.
Closed captioning of video games is becoming more common. One of the first video game companies to feature closed captioning was Bethesda Softworks in their 1990 release of "Hockey League Simulator" and "The Terminator 2029". Infocom also offered "Zork Grand Inquisitor" in 1997. Many games since then have at least offered subtitles for spoken dialog during cutscenes, and many include significant in-game dialog and sound effects in the captions as well; for example, with subtitles turned on in the "Metal Gear Solid" series of stealth games, not only are subtitles available during cut scenes, but any dialog spoken during real-time gameplay will be captioned as well, allowing players who can't hear the dialog to know what enemy guards are saying and when the main character has been detected. Also, in many of developer Valve's video games (such as "Half-Life 2" or "Left 4 Dead"), when closed captions are activated, dialog and nearly all sound effects either made by the player or from other sources (e.g. gunfire, explosions) will be captioned.
Video games don't offer Line 21 captioning, decoded and displayed by the television itself but rather a built-in subtitle display, more akin to that of a DVD. The game systems themselves have no role in the captioning either; each game must have its subtitle display programmed individually.
Reid Kimball, a game designer who is hearing impaired, is attempting to educate game developers about closed captioning for games. Reid started the Games[CC] group to closed caption games and serve as a research and development team to aid the industry. Kimball designed the Dynamic Closed Captioning system, writes articles, and speaks at developer conferences. Gamesfirst closed captioning project called Doom3[CC was nominated for an award as Best Doom3 Mod of the Year for IGDA's Choice Awards 2006 show.
Online video streaming.
Internet video streaming service YouTube offers captioning services in videos. The author of the video can upload a SubViewer (*.SUB), SubRip (*.SRT) or *.SBV file. As a beta feature, the site also added the ability to automatically transcribe and generate captioning on videos, with varying degrees of success based upon the content of the video.
However, the automatic captioning is often inaccurate on videos with background music and exaggerated emotion in speaking. Variations in volume can also result in nonsensical machine-generated captions. Additional problems arise with strong accents, sarcasm, differing contexts, or homonyms.
On June 30, 2010, YouTube announced a new "YouTube Ready" designation for professional caption vendors in the United States. The initial list included twelve companies who passed a caption quality evaluation administered by the Described and Captioned Media Project, have a website and a YouTube channel where customers can learn more about their services, and have agreed to post rates for the range of services that they offer for YouTube content.
Flash video also supports captions via the Distribution Exchange profile (DFXP) of W3C timed text format. The latest Flash authoring software adds free player skins and caption components that enable viewers to turn captions on/off during playback from a webpage. Previous versions of Flash relied on the Captionate 3rd party component and skin to caption Flash video. Custom Flash players designed in Flex can be tailored to support the timed-text exchange profile, Captionate .XML, or SAMI file (e.g. Hulu captioning). This is the preferred method for most US broadcast and cable networks that are mandated by the U.S. Federal Communications Commission to provide captioned on-demand content. The media encoding firms generally use software such as MacCaption to convert EIA-608 captions to this format.
The Silverlight Media Framework also includes support for the timed-text exchange profile for both download and adaptive streaming media.
Windows Media Video can support closed captions for both video on demand streaming or live streaming scenarios. Typically Windows Media captions support the SAMI file format but can also carry embedded closed caption data.
QuickTime video supports raw 608 caption data via proprietary Closed Caption Track, which are just EIA-608 byte pairs wrapped in a QuickTime packet container with different IDs for both line 21 fields. These captions can be turned on and off and appear in the same style as TV closed captions, with all the standard formatting (pop-on, roll-up, paint-on), and can be positioned and split anywhere on the video screen. QuickTime closed caption tracks can be viewed in Mac or Windows versions of QuickTime Player, iTunes (via QuickTime), iPod Nano, iPod Classic, iPod Touch, iPhone, and iPad.
Theatre.
Live plays can be open captioned by a captioner who displays lines from the script and including non-speech elements on a large display screen near the stage.
Telephones.
A captioned telephone is a telephone that displays real-time captions of the current conversation. The captions are typically displayed on a screen embedded into the telephone base.
Media monitoring services.
In the United States especially, most media monitoring services capture and index closed captioning text from news and public affairs programs, allowing them to search the text for client references. The use of closed captioning for television news monitoring was pioneered by Universal Press Clipping Bureau (Universal Information Services) in 1992, and later in 1993 by Tulsa-based NewsTrak of Oklahoma (later known as Broadcast News of Mid-America, acquired by video news release pioneer Medialink Worldwide Incorporated in 1997). US patent 7,009,657 describes a "method and system for the automatic collection and conditioning of closed caption text originating from multiple geographic locations" as used by news monitoring services.
Conversations.
Software programs are now available that automatically generate a closed-captioning of conversations. Examples of such conversations include discussions in conference rooms, classroom lectures, and/or religious services.
Non-linear video editing systems and closed captioning.
In 2010, Vegas Pro, the professional non-linear editor, was updated to support importing, editing, and delivering CEA-608 closed captions. Vegas Pro 10, released on October 11, 2010, added several enhancements to the closed captioning support. TV-like CEA-608 closed captioning can now be displayed as an overlay when played back in the Preview and Trimmer windows, making it easy to check placement, edits, and timing of CC information. CEA708 style Closed Captioning is automatically created when the CEA-608 data is created. Line 21 closed captioning is now supported, as well as HD-SDI closed captioning capture and print from AJA and Blackmagic Design cards. Line 21 support provides a workflow for existing legacy media. Other improvements include increased support for multiple closed captioning file types, as well as the ability to export closed caption data for DVD Architect, YouTube, RealPlayer, QuickTime, and Windows Media Player.
In mid-2009, Apple released Final Cut Pro version 7 and began support for inserting closed caption data into SD and HD tape masters via firewire and compatible video capture cards. Up until this time it was not possible for video editors to insert caption data with both CEA-608 and CEA-708 to their tape masters. The typical workflow included first printing the SD or HD video to a tape and sending it to a professional closed caption service company that had a stand-alone closed caption hardware encoder.
This new closed captioning workflow known as e-Captioning involves making a proxy video from the non-linear system to import into a third-party non-linear closed captioning software. Once the closed captioning software project is completed, it must export a closed caption file compatible with the non-linear editing system. In the case of Final Cut Pro 7, three different file formats can be accepted: a .SCC file (Scenarist Closed Caption file) for Standard Definition video, a QuickTime 608 closed caption track (a special 608 coded track in the .mov file wrapper) for standard-definition video, and finally a QuickTime 708 closed caption track (a special 708 coded track in the .mov file wrapper) for high-definition video output.
Alternatively, Matrox video systems devised another mechanism for inserting closed caption data by allowing the video editor to include CEA-608 and CEA-708 in a discrete audio channel on the video editing timeline. This allows real-time preview of the captions while editing and is compatible with Final Cut Pro 6 and 7.
Other non-linear editing systems indirectly support closed captioning only in Standard Definition line-21. Video files on the editing timeline must be composited with a line-21 VBI graphic layer known in the industry as a "blackmovie" with closed caption data. Alternately, video editors working with the DV25 and DV50 firewire workflows must encode their DV .avi or .mov file with VAUX data which includes CEA-608 closed caption data.
Logo.
The current and most familiar logo for closed captioning consists of two Cs (for "closed captioned") inside a television screen. It was created by WGBH. The other logo, trademarked by the National Captioning Institute, is that of a simple geometric rendering of a television set merged with the tail of a speech balloon; two such versions exist: one with a tail on the left, the other with a tail on the right.

</doc>
<doc id="40885" url="https://en.wikipedia.org/wiki?curid=40885" title="Closed-loop transfer function">
Closed-loop transfer function

A closed-loop transfer function in control theory is a mathematical expression (algorithm) describing the net result of the effects of a closed (feedback) loop on the input signal to the circuits enclosed by the loop.
Overview.
The closed-loop transfer function is measured at the output. The output signal waveform can be calculated from the closed-loop transfer function and the input signal waveform.
An example of a closed-loop transfer function is shown below:
The summing node and the "G"("s") and "H"("s") blocks can all be combined into one block, which would have the following transfer function:
Derivation.
We define an intermediate signal Z shown as follows:
Using this figure we write:

</doc>
<doc id="40888" url="https://en.wikipedia.org/wiki?curid=40888" title="Code conversion">
Code conversion

In telecommunication, the term code conversion has the following meanings: 
1. Conversion of signals, or groups of signals, in one code into corresponding signals, or groups of signals, in another code. 
2. A process for converting a code of some predetermined bit structure, such as 5, 7, or 14 bits per character interval, to another code with the same or a different number of bits per character interval. 
In code conversion, alphabetical order is not significant.

</doc>
<doc id="40890" url="https://en.wikipedia.org/wiki?curid=40890" title="Coded set">
Coded set

In telecommunication, a coded set is a set of elements onto which another set of elements has been mapped according to a code. 
Examples of coded sets include the list of names of airports that is mapped onto a set of corresponding three-letter representations of airport names, the list of classes of emission that is mapped onto a set of corresponding standard symbols, and the names of the months of the year mapped onto a set of two-digit decimal numbers.

</doc>
<doc id="40891" url="https://en.wikipedia.org/wiki?curid=40891" title="Code word">
Code word

In communication, a code word is an element of a standardized code or protocol. Each code word is assembled in accordance with the specific rules of the code and assigned a unique meaning. Code words are typically used for reasons of reliability, clarity, brevity, or secrecy.

</doc>
<doc id="40892" url="https://en.wikipedia.org/wiki?curid=40892" title="Coding">
Coding

Coding may refer to:

</doc>
<doc id="40893" url="https://en.wikipedia.org/wiki?curid=40893" title="Coherence length">
Coherence length

In physics, coherence length is the propagation distance over which a coherent wave (e.g. an electromagnetic wave) maintains a specified degree of coherence. Wave interference is strong when the paths taken by all of the interfering waves differ by less than the coherence length. A wave with a longer coherence length is closer to a perfect sinusoidal wave. Coherence length is important in holography and telecommunications engineering.
This article focuses on the coherence of classical electromagnetic fields. In quantum mechanics, there is a mathematically analogous concept of the quantum coherence length of a wave function.
Formulas.
In radio-band systems, the coherence length is approximated by
where "c" is the speed of light in a vacuum, "n" is the refractive index of the medium, and formula_2 is the bandwidth of the source.
In optical communications, assuming that the source has a Gaussian emission spectrum, the coherence length formula_3 is given by 
where formula_5 is the central wavelength of the source, formula_6 is the refractive index of the medium, and formula_7 is the spectral width of the source. If the source has a Gaussian spectrum with FWHM spectral width formula_7, then a path offset of ±formula_3 will reduce the fringe visibility to 50%.
"Coherence length" is usually applied to the optical regime.
The expression above is a frequently used approximation. Due to ambiguities in the definition of spectral width of a source, however, the following definition of coherence length has been suggested:
The coherence length can be measured using a Michelson interferometer and is the optical path length difference of a self-interfering laser beam which corresponds to a formula_10 fringe visibility, where the fringe visibility is defined as
where formula_12 is the fringe intensity.
In long-distance transmission systems, the coherence length may be reduced by propagation factors such as dispersion, scattering, and diffraction.
Lasers.
Multimode helium–neon lasers have a typical coherence length of 20 cm, while the coherence length of singlemode ones can exceed 100 m. Semiconductor lasers reach some 100 m. Singlemode fiber lasers with linewidths of a few kHz can have coherence lengths exceeding 100 km. Similar coherence lengths can be reached with optical frequency combs due to the narrow linewidth of each tooth. Non-zero visibility is present only for short intervals of pulses repeated after cavity length distances up to this long coherence length.

</doc>
<doc id="40894" url="https://en.wikipedia.org/wiki?curid=40894" title="Coherence time">
Coherence time

For an electromagnetic wave, the coherence time is the time over which a propagating wave (especially a laser or maser beam) may be considered coherent. In other words, it is the time interval within which its phase is, on average, predictable.
In long-distance transmission systems, the coherence time may be reduced by propagation factors such as dispersion, scattering, and diffraction.
Coherence time, "τ", is calculated by dividing the coherence length by the phase velocity of light in a medium; approximately given by
where "λ" is the central wavelength of the source, "Δν" and "Δλ" is the spectral width of the source in units of frequency and wavelength respectively, and "c" is the speed of light in vacuum.
A single mode fiber laser has a linewidth of a few kHz. The Schawlow-Townes limit for some cw lasers can be below 1 Hz. Hydrogen masers have linewidth around 1 Hz; their coherence length approximately corresponds to the distance from the Earth to the Moon.
, single electron spins show the longest room-temperature spin dephasing times ever observed in solid-state systems (1.8 ms).

</doc>
<doc id="40896" url="https://en.wikipedia.org/wiki?curid=40896" title="Collective routing">
Collective routing

Collective routing is routing in which a switching center automatically delivers messages to a specified list of destinations. 
Collective routing avoids the need to list each single address in the message heading. 
Major relay stations usually transmit messages bearing collective-routing indicators to tributary, minor, and other major relay stations.

</doc>
