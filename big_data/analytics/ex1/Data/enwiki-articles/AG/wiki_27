<doc id="41832" url="https://en.wikipedia.org/wiki?curid=41832" title="U interface">
U interface

The U interface or U reference point is a Basic Rate Interface (BRI) in the local loop of an Integrated Services Digital Network (ISDN). It is characterized by the use of a 2-wire transmission system that connects the network termination type 1 (NT1) on the customer's premises and the line termination (LT) in the carrier's local exchange. It is not as distance sensitive as a service using an S interface or T interface.
In America, the NT1 is customer premises equipment (CPE) which is purchased and maintained by the user, which makes the U interface a User–network interface (UNI). The American variant is specified by the American National Standards Institute (ANSI) in T1.601. In Europe, the NT1 belongs to the network operator, so the user doesn't have direct access to the U interface. The European variant is specified by the European Telecommunications Standards Institute (ETSI) in recommendation ETR 080. The ITU-T has issued recommendations G.960 and G.961 with world-wide scope, encompassing both the European and American variants of the U interface.
Logical interface.
Like all other ISDN basic rate interfaces, the U interface carries two B (bearer) channels at 64 kbit/s and one D (data) channel at 16 kbit/s for a combined bitrate of 144 kbit/s (2B+D).
Duplex transmission.
While in a four-wire interface such as the ISDN S and T-interfaces one wire pair is available for each direction of transmission, a two-wire interface needs to implement both directions on a single wire pair. To that end, ITU-T recommendation G.961 specifies two duplex transmission technologies for the ISDN U interface, either of which shall be used: Echo cancellation (ECH) and Time Compression Multiplex (TCM).
Echo cancellation (ECH).
When a transmitter applies a signal to the wire-pair, parts of the signal will be reflected as a result of imperfect balance of the hybrid and because of impedance discontinuities on the line. These reflections return to the transmitter as an echo and are indistinguishable from a signal transmitted at the far end. In the echo cancellation (ECH) scheme, the transmitter locally simulates the echo it expects to receive, and subtracts it from the received signal.
Time Compression Multiplex (TCM).
The Time Compression Multiplex (TCM) duplex method, also referred to as "burst mode", solves the echo problem indirectly. The line is operated at a rate at least twice the signal rate and both ends of the line take turns transmitting, in a time-division duplex fashion.
Line Systems.
ITU-T G.961 specifies four line systems for the ISDN U interface: MMS43, 2B1Q, TCM, and SU32. All line systems except TCM use echo cancellation for duplex operation. The American standard ANSI T1.601 specifies the 2B1Q line system, the European ETSI TR 080 recommendation specifies 2B1Q and MMS43.
MMMS43 (4B3T).
The Modified Monitoring State Code mapping 4 bits into 3 ternary symbols (MMS43), which is also referred to as 4B3T (four binary, three ternary) is a line system used in Europe and elsewhere in the world. 4B3T is a "block code" that uses Return-to-Zero states on the line. 4B3T converts each group of 4 data bits into 3 "ternary" line signal states (3 symbols). Echo cancellation techniques allow full-duplex operation on the line.
MMS43 is defined in Appendix I of G.961, Annex B of ETR 080, and other national standards, like Germany's 1TR220. 4B3T can be transmitted reliably at up to over cable or up to over cable. An internal termination impedance of is presented to the line at each end of the U-interface.
A 1 ms frame carrying 144 bits of 2B+D data is mapped to 108 ternary symbols. These symbols are scrambled, with different scrambling codes for the two transmission directions, in order reduce correlation between transmitted and received signal. To this frame, an 11-symbol preamble and a symbol from the CL channel are added, yielding a frame size of 120 ternary symbols and a symbol rate of 120 kilobaud. The CL channel is used to request activation or deactivation of a loopback in either the NT1 or a line regenerator.
In 4B3T coding, there are three states presented to line: a positive pulse (+), a negative pulse (-), or a zero-state (no pulse: 0). An analogy here is that operation is similar to B8ZS or HDB3 in T1/E1 systems, except that there is an actual gain in the information rate by coding 24=16 possible binary states to one of 33=27 ternary states. This added redundancy is used to generate a zero DC-bias signal.
One requirement for line transmission is that there should be no DC build-up on the line, so the accumulated DC build-up is monitored and the codewords are chosen accordingly. Of the 16 binary information words, some are always mapped to a DC-component free (ternary) code word, while others can be mapped to either one of two code words, one with a positive and the other with a negative DC-component. In the latter case, the transmitter chooses whether to send the code-word with negative or positive DC-component based on the accumulated DC-offset.
2B1Q.
2B1Q coding is the standard used in North America, Italy, and Switzerland. 2B1Q means that two bits are combined to form a single Quaternary line state (symbol). 2B1Q combines two bits at a time to be represented by one of four signal levels on the line. Echo cancellation techniques allow full-duplex operation on the line.
2B1Q coding is defined in Appendix II of G.961, ANSI T1.601, and Annex A of ETR 080. It can operate at distances up to about 18,000 feet () with loss up to . An internal termination impedance of 135 ohms is presented to the line at each end of the U-interface.
A 1.5 ms frame carrying 216 scrambled bits of 2B+D data is mapped to 108 quaternary symbols. To this frame, a 9-symbol preamble and 3 symbols from the CL channel are added, yielding a frame size of 120 quaternary symbols and a symbol rate of 80 kilobaud. The CL channel is used for communication between LT and NT1, a 12-bit cyclic redundancy check (CRC), and various other physical layer functions. The CRC covers one 12 ms multiframe (8×1.5 ms frames).
TCM / AMI.
The TCM / AMI ISDN line system, also referred to as TCM-ISDN, is used by Nippon Telegraph and Telephone in its "INS-Net 64" service.
Appendix III of G.961 specifies a line system based on the Time Compression Multiplex (TCM) duplex method and an alternate mark inversion (AMI) line code. The AMI line code maps one input bit to one ternary symbol. Like with MMS43, the ternary symbol can either be a positive (+), zero (0), or negative (-) voltage. A 0 bit is represented by a zero voltage, while a 1 bit is alternatingly represented by a positive and a negative voltage, resulting in a DC-bias free signal. In a 2.5 ms interval, each side can send a 1.178 ms frame representing 360 bits of 2B+D data. To the 2B+D data, an 8-bit preamble, 8 bits from the CL channel, as well as a parity bit are added, yielding a frame size of 377 bits and a baud rate of 320 kilobaud. The CL channel is used for operations and maintenance, as well transmitting a 12-bit CRC covering 4 frames.
SU32.
Appendix IV of G.961 specifies a line system based on echo cancellation and a substitutional 3B2T (SU32) line code, which maps three bits into 2 ternary symbols. As with MMS43 and AMI, the ternary symbol can either be a positive (+), zero (0), or negative (-) voltage. The mapping from 23=8 to 32=9 symbols leaves one unused symbol. When two subsequent input (binary) information words are identical, the (ternary) code word is substituted by the unused code word. A 0.75 ms frame carrying 108 bits of 2B+D data is mapped to 72 ternary symbols. To this frame, a 6-symbol preamble, one CRC symbol, and 2 symbols from the CL channel are added, yielding a frame size of 81 ternary symbols and a symbol rate of 108 kilobaud. The CL channel is used for supervisory and maintenance functions between the LT and NT1. The 15-bit CRC covers 16 frames.

</doc>
<doc id="41833" url="https://en.wikipedia.org/wiki?curid=41833" title="Unavailability">
Unavailability

Unavailability is the probability that an item will not operate correctly at a given time and under specified conditions. It opposes availability.
Numerical values associated with the calculation of availability are often awkward, consisting of a series of 9s before reaching any significant numerical information (e.g. 0.9999999654). For this reason, it is more convenient to use the complement measure of availability, namely, unavailability. Expressed mathematically, unavailability is 1 minus the availability. Therefore, a system with availability 0.9999999654 is more concisely described as having an unavailability of 3.46E-8. 
Unavailability may be expressed mathematically as the ratio,
where MTTR is the mean time to repair, and MTTF is the mean time to failure. Alternatively, this can be written as
where λ' is the critical failure rate (λ without the tick is the total failure rate) and μ' is the critical repair rate (rate at which critical failures are repaired).
In telecommunication, an unavailability is an expression of the degree to which a system, subsystem, or equipment is not operable and not in a committable state at the start of a mission, when the mission is called for at an unknown, i.e. random, time. The conditions determining operability and committability must be specified.

</doc>
<doc id="41834" url="https://en.wikipedia.org/wiki?curid=41834" title="Uninterruptible power supply">
Uninterruptible power supply

An uninterruptible power supply, also uninterruptible power source, UPS or battery/flywheel backup, is an electrical apparatus that provides emergency power to a load when the input power source, typically mains power, fails. A UPS differs from an auxiliary or emergency power system or standby generator in that it will provide near-instantaneous protection from input power interruptions, by supplying energy stored in batteries, supercapacitors, or flywheels. The on-battery runtime of most uninterruptible power sources is relatively short (only a few minutes) but sufficient to start a standby power source or properly shut down the protected equipment.
A UPS is typically used to protect hardware such as computers, data centers, telecommunication equipment or other electrical equipment where an unexpected power disruption could cause injuries, fatalities, serious business disruption or data loss. UPS units range in size from units designed to protect a single computer without a video monitor (around 200 volt-ampere rating) to large units powering entire data centers or buildings. The world's largest UPS, the 46-megawatt Battery Electric Storage System (BESS), in Fairbanks, Alaska, powers the entire city and nearby rural communities during outages.
Common power problems.
The primary role of any UPS is to provide short-term power when the input power source fails. However, most UPS units are also capable in varying degrees of correcting common utility power problems:
UPS units are divided into categories based on which of the above problems they address, and some manufacturers categorize their products in accordance with the number of power-related problems they address.
Technologies.
The three general categories of modern UPS systems are "on-line", "line-interactive" and "standby". An on-line UPS uses a "double conversion" method of accepting AC input, rectifying to DC for passing through the rechargeable battery (or battery strings), then inverting back to 120 V/230 V AC for powering the protected equipment. A line-interactive UPS maintains the inverter in line and redirects the battery's DC current path from the normal charging mode to supplying current when power is lost. In a standby ("off-line") system the load is powered directly by the input power and the backup power circuitry is only invoked when the utility power fails. Most UPS below 1 kVA are of the line-interactive or standby variety which are usually less expensive.
For large power units, Dynamic Uninterruptible Power Supplies (DUPS) are sometimes used. A synchronous motor/alternator is connected on the mains via a choke. Energy is stored in a flywheel. When the mains power fails, an eddy-current regulation maintains the power on the load as long as the flywheel's energy is not exhausted. DUPS are sometimes combined or integrated with a diesel generator that is turned on after a brief delay, forming a diesel rotary uninterruptible power supply (DRUPS).
A fuel cell UPS has been developed in recent years using hydrogen and a fuel cell as a power source, potentially providing long run times in a small space.
Offline/Standby.
The offline/standby UPS (SPS) offers only the most basic features, providing surge protection and battery backup. The protected equipment is normally connected directly to incoming utility power. When the incoming voltage falls below or rises above a predetermined level the SPS turns on its internal DC-AC inverter circuitry, which is powered from an internal storage battery. The UPS then mechanically switches the connected equipment on to its DC-AC inverter output. The switchover time can be as long as 25 milliseconds depending on the amount of time it takes the standby UPS to detect the lost utility voltage. The UPS will be designed to power certain equipment, such as a personal computer, without any objectionable dip or brownout to that device.
Line-interactive.
The line-interactive UPS is similar in operation to a standby UPS, but with the addition of a multi-tap variable-voltage autotransformer. This is a special type of transformer that can add or subtract powered coils of wire, thereby increasing or decreasing the magnetic field and the output voltage of the transformer. This is also known as a "Buck–boost transformer".
This type of UPS is able to tolerate continuous undervoltage brownouts and overvoltage surges without consuming the limited reserve battery power. It instead compensates by automatically selecting different power taps on the autotransformer. Depending on the design, changing the autotransformer tap can cause a very brief output power disruption, which may cause UPSs equipped with a power-loss alarm to "chirp" for a moment.
This has become popular even in the cheapest UPSs because it takes advantage of components already included. The main 50/60 Hz transformer used to convert between line voltage and battery voltage needs to provide two slightly different turns ratios: One to convert the battery output voltage (typically a multiple of 12 V) to line voltage, and a second one to convert the line voltage to a slightly higher battery charging voltage (such as a multiple of 14 V). The difference between the two voltages is because charging a battery requires a delta voltage (up to 13–14 V for charging a 12 V battery). Furthermore, it is easier to do the switching on the line-voltage side of the transformer because of the lower currents on that side.
To gain the "buck/boost" feature, all that is required is two separate switches so that the AC input can be connected to one of the two primary taps, while the load is connected to the other, thus using the main transformer's primary windings as an autotransformer. The battery can still be charged while "bucking" an overvoltage, but while "boosting" an undervoltage, the transformer output is too low to charge the batteries.
Autotransformers can be engineered to cover a wide range of varying input voltages, but this requires more taps and increases complexity, and expense of the UPS. It is common for the autotransformer to cover a range only from about 90 V to 140 V for 120 V power, and then switch to battery if the voltage goes much higher or lower than that range.
In low-voltage conditions the UPS will use more current than normal so it may need a higher current circuit than a normal device. For example, to power a 1000-W device at 120 V, the UPS will draw 8.33 A. If a brownout occurs and the voltage drops to 100 V, the UPS will draw 10 A to compensate. This also works in reverse, so that in an overvoltage condition, the UPS will need less current.
Online/double-conversion.
In an online UPS, the batteries are always connected to the inverter, so that no power transfer switches are necessary. When power loss occurs, the rectifier simply drops out of the circuit and the batteries keep the power steady and unchanged. When power is restored, the rectifier resumes carrying most of the load and begins charging the batteries, though the charging current may be limited to prevent the high-power rectifier from overheating the batteries and boiling off the electrolyte. The main advantage of an on-line UPS is its ability to provide an "electrical firewall" between the incoming utility power and sensitive electronic equipment.
The online UPS is ideal for environments where electrical isolation is necessary or for equipment that is very sensitive to power fluctuations. Although it was at one time reserved for very large installations of 10 kW or more, advances in technology have now permitted it to be available as a common consumer device, supplying 500 W or less. The initial cost of the online UPS may be higher, but its total cost of ownership is generally lower due to longer battery life. The online UPS may be necessary when the power environment is "noisy", when utility power sags, outages and other anomalies are frequent, when protection of sensitive IT equipment loads is required, or when operation from an extended-run backup generator is necessary.
The basic technology of the online UPS is the same as in a standby or line-interactive UPS. However it typically costs much more, due to it having a much greater current AC-to-DC battery-charger/rectifier, and with the rectifier and inverter designed to run continuously with improved cooling systems. It is called a "double-conversion" UPS due to the rectifier directly driving the inverter, even when powered from normal AC current.
Other designs.
Hybrid topology / double conversion on demand.
These hybrid Rotary UPS designs do not have official designations, although one name used by UTL is "double conversion on demand". This style of UPS is targeted towards high-efficiency applications while still maintaining the features and protection level offered by double conversion.
A hybrid (double conversion on demand) UPS operates as an off-line/standby UPS when power conditions are within a certain preset window. This allows the UPS to achieve very high efficiency ratings. When the power conditions fluctuate outside of the predefined windows, the UPS switches to online/double-conversion operation. In double-conversion mode the UPS can adjust for voltage variations without having to use battery power, can filter out line noise and control frequency. Examples of this hybrid/double conversion on demand UPS design are the HP R8000, HP R12000, HP RP12000/3 and the Eaton BladeUPS.
Ferro-resonant.
Ferro-resonant units operate in the same way as a standby UPS unit; however, they are online with the exception that a ferro-resonant transformer is used to filter the output. This transformer is designed to hold energy long enough to cover the time between switching from line power to battery power and effectively eliminates the transfer time. Many ferro-resonant UPSs are 82–88% efficient (AC/DC-AC) and offer excellent isolation.
The transformer has three windings, one for ordinary mains power, the second for rectified battery power, and the third for output AC power to the load.
This once was the dominant type of UPS and is limited to around the range. These units are still mainly used in some industrial settings (oil and gas, petrochemical, chemical, utility, and heavy industry markets) due to the robust nature of the UPS. Many ferro-resonant UPSs utilizing controlled ferro technology may not interact with power-factor-correcting equipment.
DC power.
A UPS designed for powering DC equipment is very similar to an online UPS, except that it does not need an output inverter. Also, if the UPS's battery voltage is matched with the voltage the device needs, the device's power supply will not be needed either. Since one or more power conversion steps are eliminated, this increases efficiency and run time.
Many systems used in telecommunications use an extra-low voltage "common battery" 48 V DC power, because it has less restrictive safety regulations, such as being installed in conduit and junction boxes. DC has typically been the dominant power source for telecommunications, and AC has typically been the dominant source for computers and servers.
There has been much experimentation with 48 V DC power for computer servers, in the hope of reducing the likelihood of failure and the cost of equipment. However, to supply the same amount of power, the current would be higher than an equivalent 115 V or 230 V circuit; greater current requires larger conductors, or more energy lost as heat.
A laptop computer is a classic example of a PC with a DC UPS built in.
High voltage DC (380 V) is finding use in some data center applications, and allows for small power conductors, but is subject to the more complex electrical code rules for safe containment of high voltages.
Rotary.
A rotary UPS uses the inertia of a high-mass spinning flywheel (flywheel energy storage) to provide short-term "ride-through" in the event of power loss. The flywheel also acts as a buffer against power spikes and sags, since such short-term power events are not able to appreciably affect the rotational speed of the high-mass flywheel. It is also one of the oldest designs, predating vacuum tubes and integrated circuits.
It can be considered to be "on line" since it spins continuously under normal conditions. However, unlike a battery-based UPS, flywheel-based UPS systems typically provide 10 to 20 seconds of protection before the flywheel has slowed and power output stops. It is traditionally used in conjunction with standby diesel generators, providing backup power only for the brief period of time the engine needs to start running and stabilize its output.
The rotary UPS is generally reserved for applications needing more than 10,000 W of protection, to justify the expense and benefit from the advantages rotary UPS systems bring. A larger flywheel or multiple flywheels operating in parallel will increase the reserve running time or capacity.
Because the flywheels are a mechanical power source, it is not necessary to use an electric motor or generator as an intermediary between it and a diesel engine designed to provide emergency power. By using a transmission gearbox, the rotational inertia of the flywheel can be used to directly start up a diesel engine, and once running, the diesel engine can be used to directly spin the flywheel. Multiple flywheels can likewise be connected in parallel through mechanical countershafts, without the need for separate motors and generators for each flywheel.
They are normally designed to provide very high current output compared to a purely electronic UPS, and are better able to provide inrush current for inductive loads such as motor startup or compressor loads, as well as medical MRI and cath lab equipment. It is also able to tolerate short-circuit conditions up to 17 times larger than an electronic UPS, permitting one device to blow a fuse and fail while other devices still continue to be powered from the rotary UPS.
Its life cycle is usually far greater than a purely electronic UPS, up to 30 years or more. But they do require periodic downtime for mechanical maintenance, such as ball bearing replacement. In larger systems redundancy of the system ensures the availability of processes during this maintenance. Battery-based designs do not require downtime if the batteries can be hot-swapped, which is usually the case for larger units. Newer rotary units use technologies such as magnetic bearings and air-evacuated enclosures to increase standby efficiency and reduce maintenance to very low levels.
Typically, the high-mass flywheel is used in conjunction with a motor-generator system. These units can be configured as:
In case No. 3 the motor generator can be synchronous/synchronous or induction/synchronous. The motor side of the unit in case Nos. 2 and 3 can be driven directly by an AC power source (typically when in inverter bypass), a 6-step double-conversion motor drive, or a 6-pulse inverter. Case No. 1 uses an integrated flywheel as a short-term energy source instead of batteries to allow time for external, electrically coupled gensets to start and be brought online. Case Nos. 2 and 3 can use batteries or a free-standing electrically coupled flywheel as the short-term energy source.
Form factors.
UPS systems come in several different forms and sizes. However, the two most common forms are tower and rack-mount.
Tower model.
Tower models stand upright on the ground or on a desk/shelf, and are typically used in network workstations or desktop computer applications.
Rack-mount model.
Rack-mount models can be mounted in standard 19" rack enclosures and can require anywhere from 1U to 12U (rack space). They are typically used in server and networking applications.
Applications.
N+1.
In large business environments where reliability is of great importance, a single huge UPS can also be a single point of failure that can disrupt many other systems. To provide greater reliability, multiple smaller UPS modules and batteries can be integrated together to provide redundant power protection equivalent to one very large UPS. "N+1" means that if the load can be supplied by N modules, the installation will contain N+1 modules. In this way, failure of one module will not impact system operation.
Multiple redundancy.
Many computer servers offer the option of redundant power supplies, so that in the event of one power supply failing, one or more other power supplies are able to power the load. This is a critical point – each power supply must be able to power the entire server by itself.
Redundancy is further enhanced by plugging each power supply into a different circuit (i.e. to a different circuit breaker).
Redundant protection can be extended further yet by connecting each power supply to its own UPS. This provides double protection from both a power supply failure and a UPS failure, so that continued operation is assured. This configuration is also referred to as 1+1 or 2N redundancy. If the budget does not allow for two identical UPS units then it is common practice to plug one power supply into mains power and the other into the UPS.
Outdoor use.
When a UPS system is placed outdoors, it should have some specific features that guarantee that it can tolerate weather without any effects on performance. Factors such as temperature, humidity, rain, and snow among others should be considered by the manufacturer when designing an outdoor UPS system. Operating temperature ranges for outdoor UPS systems could be around −40 °C to +55 °C.
Outdoor UPS systems can either be pole, ground (pedestal), or host mounted. Outdoor environment could mean extreme cold, in which case the outdoor UPS system should include a battery heater mat, or extreme heat, in which case the outdoor UPS system should include a fan system or an air conditioning system.
A solar inverter, or PV inverter, or solar converter, converts the variable direct current (DC) output of a photovoltaic (PV) solar panel into a utility frequency alternating current (AC) that can be fed into a commercial electrical grid or used by a local, off-grid electrical network. It is a critical BOS–component in a photovoltaic system, allowing the use of ordinary AC-powered equipment. Solar inverters have special functions adapted for use with photovoltaic arrays, including maximum power point tracking and anti-islanding protection.
Difficulties faced with generator use.
Power factor.
A problem in the combination of a "double conversion" UPS and a generator is the voltage distortion created by the UPS. The input of a double conversion UPS is essentially a big rectifier. The current drawn by the UPS is non-sinusoidal. This can cause the voltage from the AC mains or a generator to also become non-sinusoidal. The voltage distortion then can cause problems in all electrical equipment connected to that power source, including the UPS itself. It will also cause more power to be lost in the wiring supplying power to the UPS due to the spikes in current flow. This level of "noise" is measured as a percentage of "Total Harmonic Distortion of the current" (THD(i)). Classic UPS rectifiers have a THD(i) level of around 25–30%. To reduce voltage distortion, this requires heavier mains wiring or generators more than twice as large as the UPS.
There are several solutions to reduce the THD(i) in a double conversion UPS:
Passive power factor correction.
Classic solutions such as passive filters reduce THD(i) to 5–10% at full load. They are reliable, but big and only work at full load, and present their own problems when used in tandem with generators.
Active power factor correction.
An alternative solution is an active filter. Through the use of such a device, THD(i) can drop to 5% over the full power range. The newest technology in double conversion UPS units is a rectifier that doesn't use classic rectifier components (thyristors and diodes) but high frequency components. A double conversion UPS with an IGBT rectifier and inductor can have a THD(i) as small as 2%. This completely eliminates the need to oversize the generator (and transformers), without additional filters, investment cost, losses, or space.
Communication.
Power management (PM) requires
The basic computer-to-UPS control methods are intended for one-to-one signaling from a single source to a single target. For example, a single UPS may connect to a single computer to provide status information about the UPS, and allow the computer to control the UPS. Similarly, the USB protocol is also intended to connect a single computer to multiple peripheral devices.
In some situations it is useful for a single large UPS to be able to communicate with several protected devices. For traditional serial or USB control, a "signal replication" device may be used, which for example allows one UPS to connect to five computers using serial or USB connections. However, the splitting is typically only one direction from UPS to the devices to provide status information. Return control signals may only be permitted from one of the protected systems to the UPS.
As Ethernet has increased in common use since the 1990s, control signals are now commonly sent between a single UPS and multiple computers using standard Ethernet data communication methods such as TCP/IP. The status and control information is typically encrypted so that for example an outside hacker can not gain control of the UPS and command it to shut down.
Distribution of UPS status and control data requires that all intermediary devices such as Ethernet switches or serial multiplexers be powered by one or more UPS systems, in order for the UPS alerts to reach the target systems during a power outage. To avoid the dependency on Ethernet infrastructure, the UPSs can be connected directly to main control server by using GSM/GPRS channel also. The SMS or GPRS data packets sent from UPSs trigger software to shutdown the PCs to reduce the load.
Batteries.
The run-time for a battery-operated UPS depends on the type and size of batteries and rate of discharge, and the efficiency of the inverter. The total capacity of a lead–acid battery is a function of the rate at which it is discharged, which is described as Peukert's law.
Manufacturers supply run-time rating in minutes for packaged UPS systems. Larger systems (such as for data centers) require detailed calculation of the load, inverter efficiency, and battery characteristics to ensure the required endurance is attained.
Common battery characteristics and load testing.
When a lead–acid battery is charged or discharged, this initially affects only the reacting chemicals, which are at the interface between the electrodes and the electrolyte. With time, the charge stored in the chemicals at the interface, often called "interface charge", spreads by diffusion of these chemicals throughout the volume of the active material.
If a battery has been completely discharged (e.g. the car lights were left on overnight) and next is given a fast charge for only a few minutes, then during the short charging time it develops only a charge near the interface. The battery voltage may rise to be close to the charger voltage so that the charging current decreases significantly. After a few hours this interface charge will spread to the volume of the electrode and electrolyte, leading to an interface charge so low that it may be insufficient to start a car.
Due to the interface charge, brief UPS "self-test" functions lasting only a few seconds may not accurately reflect the true runtime capacity of a UPS, and instead an extended "recalibration" or "rundown" test that deeply discharges the battery is needed.
The deep discharge testing is itself damaging to batteries due to the chemicals in the discharged battery starting to crystallize into highly stable molecular shapes that will not re-dissolve when the battery is recharged, permanently reducing charge capacity. In lead acid batteries this is known as sulfation but also affects other types such as nickel cadmium batteries and lithium batteries. Therefore, it is commonly recommended that rundown tests be performed infrequently, such as every six months to a year.
Testing of strings of batteries/cells.
Multi-kilowatt commercial UPS systems with large and easily accessible battery banks are capable of isolating and testing individual cells within a "battery string", which consists of either combined-cell battery units (such as 12-V lead acid batteries) or individual chemical cells wired in series. Isolating a single cell and installing a jumper in place of it allows the one battery to be discharge-tested, while the rest of the battery string remains charged and available to provide protection.
It is also possible to measure the electrical characteristics of individual cells in a battery string, using intermediate sensor wires that are installed at every cell-to-cell junction, and monitored both individually and collectively. Battery strings may also be wired as series-parallel, for example two sets of 20 cells. In such a situation it is also necessary to monitor current flow between parallel strings, as current may circulate between the strings to balance out the effects of weak cells, dead cells with high resistance, or shorted cells. For example, stronger strings can discharge through weaker strings until voltage imbalances are equalized, and this must be factored into the individual inter-cell measurements within each string.
Series-parallel battery interactions.
Battery strings wired in series-parallel can develop unusual failure modes due to interactions between the multiple parallel strings. Defective batteries in one string can adversely affect the operation and lifespan of good or new batteries in other strings. These issues also apply to other situations where series-parallel strings are used, not just in UPS systems but also in electric vehicle applications.
Consider a series-parallel battery arrangement with all good cells, and one becomes shorted or dead:
The only way to prevent these subtle series-parallel string interactions is by not using parallel strings at all and using separate charge controllers and inverters for individual series strings.
Series new/old battery interactions.
Even just a single string of batteries wired in series can have adverse interactions if new batteries are mixed with old batteries. Older batteries tend to have reduced storage capacity, and so will both discharge faster than new batteries and also charge to their maximum capacity more rapidly than new batteries.
As a mixed string of new and old batteries is depleted, the string voltage will drop, and when the old batteries are exhausted the new batteries still have charge available. The newer cells may continue to discharge through the rest of the string, but due to the low voltage this energy flow may not be useful, and may be wasted in the old cells as resistance heating.
For cells that are supposed to operate within a specific discharge window, new cells with more capacity may cause the old cells in the series string to continue to discharge beyond the safe bottom limit of the discharge window, damaging the old cells.
When recharged, the old cells recharge more rapidly, leading to a rapid rise of voltage to near the fully charged state, but before the new cells with more capacity have fully recharged. The charge controller detects the high voltage of a nearly fully charged string and reduces current flow. The new cells with more capacity now charge very slowly, so slowly that the chemicals may begin to crystallize before reaching the fully charged state, reducing new cell capacity over several charge/discharge cycles until their capacity more closely matches the old cells in the series string.
For such reasons, some industrial UPS management systems recommend periodic replacement of entire battery arrays potentially using hundreds of expensive batteries, due to these damaging interactions between new batteries and old batteries, within and across series and parallel strings.

</doc>
<doc id="41835" url="https://en.wikipedia.org/wiki?curid=41835" title="Universal Time">
Universal Time

Universal Time (UT) is a time standard based on Earth's rotation. It is a modern continuation of Greenwich Mean Time (GMT), i.e., the mean solar time on the Prime Meridian at Greenwich, London, UK. In fact, the expression "Universal Time" is ambiguous (when accuracy of better than a few seconds is required), as there are several versions of it, the most commonly used being Coordinated Universal Time (UTC) and UT1 (see below). All of these versions of UT, except for UTC, are based on Earth's rotation relative to distant celestial objects (stars and quasars), but with a scaling factor and other adjustments to make them closer to solar time. UTC is based on International Atomic Time, with leap seconds added to keep it within 0.9 second of UT1.
Universal Time and standard time.
Prior to the introduction of standard time, each municipality throughout the civilized world set its official clock, if it had one, according to the local position of the Sun (see solar time). This served adequately until the introduction of rail travel in Britain, which made it possible to travel fast enough over long distances to require continuous re-setting of timepieces as a train progressed in its daily run through several towns. Greenwich Mean Time, where all clocks in Britain were set to the same time, was established to solve this problem. Chronometers or telegraphy was used to synchronize these clocks.
Standard time, as originally proposed by Scottish-Canadian Sir Sandford Fleming in 1879, divided the world into twenty-four time zones, each one covering 15 degrees of longitude. All clocks within each zone would be set to the same time as the others, but differed by one hour from those in the neighboring zones. The local time at the Royal Greenwich Observatory in Greenwich, England was adopted as standard on 22 October 1884 at the end of the International Meridian Conference, leading to the widespread use of Greenwich Mean Time to set local clocks. This location was chosen because by 1884 two-thirds of all nautical charts and maps already used it as their prime meridian. The conference did not adopt Fleming's time zones because they were outside the purpose for which it was called, which was to choose a basis for universal time (as well as a prime meridian).
During the period between 1848 to 1972, all of the major countries adopted time zones based on the Greenwich meridian.
In 1935, the term "Universal Time" was recommended by the International Astronomical Union as a more precise term than Greenwich Mean Time, because GMT could refer to either an astronomical day starting at noon or a civil day starting at midnight. The term "Greenwich Mean Time" persists, however, in common usage to this day in reference to civil timekeeping.
Measurement.
Based on the rotation of the Earth, time can be measured by observing celestial bodies crossing the meridian every day. Astronomers found that it was more accurate to establish time by observing stars as they crossed a meridian rather than by observing the position of the Sun in the sky. Nowadays, UT in relation to International Atomic Time (TAI) is determined by Very Long Baseline Interferometry (VLBI) observations of distant quasars, a method which can determine UT1 to within 4 milliseconds.
The rotation of the Earth is somewhat irregular, and is very gradually slowing due to tidal acceleration. Furthermore, the length of the second was determined from observations of the Moon between 1750 and 1890. All of these factors cause the mean solar day, on the average, to be slightly longer than the nominal 86,400 SI seconds, the traditional number of seconds per day. As UT is slightly irregular in its rate, astronomers introduced Ephemeris Time, which has since been replaced by Terrestrial Time (TT). Because Universal Time is synchronous with night and day, and more precise atomic-frequency standards drift away from this, however, UT is still used to produce a correction (called a leap second) to atomic time, in order to obtain a broadcast form of civil time that carries atomic frequency. Thus, civil broadcast standards for time and frequency usually follow International Atomic Time closely, but occasionally step (or "leap") in order to prevent them from drifting too far from mean solar time.
Barycentric Dynamical Time (TDB), a form of atomic time, is now used in the construction of the ephemerides of the planets and other solar system objects, for two main reasons. First, these ephemerides are tied to optical and radar observations of planetary motion, and the TDB time scale is fitted so that Newton's laws of motion, with corrections for general relativity, are followed. Next, the time scales based on Earth's rotation are not uniform and therefore, are not suitable for predicting the motion of bodies in our solar system.
Versions.
There are several versions of Universal Time:
Adoption in various countries.
The table shows the dates of adoption of time zones based on the Greenwich meridian, including half-hour zones.
Apart from the Nepal Time Zone (UTC+05:45) and the Chatham Standard Time Zone (UTC+12:45) used in New Zealand's Chatham Islands, all timezones in use are defined by an offset from UTC that is a multiple of half an hour, and in most cases a multiple of an hour.

</doc>
<doc id="41836" url="https://en.wikipedia.org/wiki?curid=41836" title="Abstract factory pattern">
Abstract factory pattern

The abstract factory pattern provides a way to encapsulate a group of individual factories that have a common theme without specifying their concrete classes. In normal usage, the client software creates a concrete implementation of the abstract factory and then uses the generic interface of the factory to create the concrete objects that are part of the theme. The client doesn't know (or care) which concrete objects it gets from each of these internal factories, since it uses only the generic interfaces of their products. This pattern separates the details of implementation of a set of objects from their general usage and relies on object composition, as object creation is implemented in methods exposed in the factory interface.
An example of this would be an abstract factory class codice_1 that provides interfaces to create a number of products (e.g. codice_2 and codice_3). The system would have any number of derived concrete versions of the codice_1 class like codice_5 or codice_6, each with a different implementation of codice_2 and codice_3 that would create a corresponding object like codice_9 or codice_10. Each of these products is derived from a simple abstract class like codice_11 or codice_12 of which the client is aware. The client code would get an appropriate instance of the codice_1 and call its factory methods. Each of the resulting objects would be created from the same codice_1 implementation and would share a common theme (they would all be fancy or modern objects). The client would only need to know how to handle the abstract codice_11 or codice_12 class, not the specific version that it got from the concrete factory.
A factory is the location of a concrete class in the code at which objects are constructed. The intent in employing the pattern is to insulate the creation of objects from their usage and to create families of related objects without having to depend on their concrete classes. This allows for new derived types to be introduced with no change to the code that uses the base class.
Use of this pattern makes it possible to interchange concrete implementations without changing the code that uses them, even at runtime. However, employment of this pattern, as with similar design patterns, may result in unnecessary complexity and extra work in the initial writing of code. Additionally, higher levels of separation and abstraction can result in systems which are more difficult to debug and maintain.
Definition.
The essence of the Abstract Factory Pattern is to "Provide an interface for creating families of related or dependent objects without specifying their concrete classes.".
Usage.
The "factory" determines the actual "concrete" type of object to be created, and it is here that the object is actually created (in C++, for instance, by the new operator). However, the factory only returns an "abstract" pointer to the created concrete object.
This insulates client code from object creation by having clients ask a factory object to create an object of the desired abstract type and to return an abstract pointer to the object.
As the factory only returns an abstract pointer, the client code (that requested the object from the factory) does not know — and is not burdened by — the actual concrete type of the object that was just created. However, the type of a concrete object (and hence a concrete factory) is known by the abstract factory; for instance, the factory may read it from a configuration file. The client has no need to specify the type, since it has already been specified in the configuration file. In particular, this means:
Structure.
Class Diagram.
The method codice_17 on the codice_18 interface returns objects of type codice_19. What implementation of codice_19 is returned depends on which implementation of codice_18 is handling the method call.
Pseudocode.
It should render a button in either a Windows style or Mac OS X style depending on which kind of factory was used. Note that the Application has no idea what kind of GUIFactory it is given or even what kind of Button that factory creates.
C# Example.
<syntaxhighlight lang="csharp">
interface IButton
interface IGUIFactory
class WinFactory : IGUIFactory
class OSXFactory : IGUIFactory
class WinButton : IButton
class OSXButton : IButton
class Program
</syntaxhighlight>

</doc>
<doc id="41837" url="https://en.wikipedia.org/wiki?curid=41837" title="Telecommunications link">
Telecommunications link

In telecommunications a link is a communications channel that connects two or more communicating devices. This link may be an actual physical link or it may be a logical link that uses one or more actual physical links.
A telecommunications link is generally one of several types of information transmission paths such as those provided by communication satellites, terrestrial radio communications infrastructure and computer networks to connect two or more points.
The term "link" is widely used in computer networking (see data link) to refer to the communications facilities that connect nodes of a network. When the link is a logical link the type of physical link should always be specified (e.g., data link, uplink, downlink, fiber optic link, point-to-point link, etc.)
Types.
Point-to-point.
A point-to-point link is a dedicated link that connects exactly two communication facilities (e.g., two nodes of a network, an intercom station at an entryway with a single internal intercom station, a radio path between two points, etc.).
Broadcast.
Broadcast links connect two or more nodes and support "broadcast transmission", where one node can transmit so that all other nodes can receive the same transmission. Ethernet is an example.
Multipoint.
Also known as a "multidrop" link, a multipoint link is a link that connects "two or more" nodes. Also known as general topology networks, these include ATM and Frame Relay links, as well as X.25 networks when used as links for a network layer protocol like IP.
Unlike broadcast links, there is no mechanism to efficiently send a single message to all other nodes without copying and retransmitting the message.
Point-to-multipoint.
A point-to-multipoint link (or simply a "multipoint") is a specific type of multipoint link which consists of a central connection endpoint (CE) that is connected to multiple peripheral CEs. Any transmission of data that originates from the central CE is received by all of the peripheral CEs while any transmission of data that originates from any of the peripheral CEs is only received by the central CE.
Private and public – accessibility and ownership.
Links are often referred to by terms which refer to the ownership and / or accessibility of the link.
Direction.
Forward link.
A forward link is the link from a fixed location (e.g., a base station) to a mobile user. If the link includes a communications relay satellite, the forward link will consist of both an uplink (base station to satellite) and a downlink (satellite to mobile user).
Reverse link.
The reverse link (sometimes called a "return channel") is the link from a mobile user to a fixed base station.
If the link includes a communications relay satellite, the reverse link will consist of both an uplink (mobile station to satellite) and a downlink (satellite to base station) which together constitute a half hop.

</doc>
<doc id="41842" url="https://en.wikipedia.org/wiki?curid=41842" title="User information bit">
User information bit

In telecommunication, a user information bit is a bit transferred from a source user to a telecommunications system for delivery to a destination user. 
User information bits do not include the overhead bits originated by, or having their primary functional effect within, the telecommunications system. 
User information bits are encoded to form channel bits.

</doc>
<doc id="41844" url="https://en.wikipedia.org/wiki?curid=41844" title="Validation">
Validation

Validation may refer to:

</doc>
<doc id="41845" url="https://en.wikipedia.org/wiki?curid=41845" title="Variable-length buffer">
Variable-length buffer

In telecommunication, a variable length buffer is a buffer into which data may be entered at one rate and removed at another rate without changing the data sequence. 
Most first-in first-out (FIFO) storage devices are variable-length buffers in that the input rate may be variable while the output rate is constant or the output rate may be variable while the input rate is constant. Various clocking and control systems are used to allow control of underflow or overflow conditions.

</doc>
<doc id="41847" url="https://en.wikipedia.org/wiki?curid=41847" title="Video teleconferencing unit">
Video teleconferencing unit

A video teleconferencing unit (VTU) is a piece of electrical equipment that performs videoconferencing functions, such as the coding and decoding of audio and video signals and multiplexing of video, audio, data, and control signals, and that usually does not include Input/Output (I/O) devices, cryptographic devices, network interface equipment, network connections, or the communications network to which the unit is connected.

</doc>
<doc id="41848" url="https://en.wikipedia.org/wiki?curid=41848" title="View">
View

View, or variants, may refer to:

</doc>
<doc id="41849" url="https://en.wikipedia.org/wiki?curid=41849" title="Viewdata">
Viewdata

Viewdata is a Videotex implementation. It is a type of information retrieval service in which a subscriber can access a remote database via a common carrier channel, request data and receive requested data on a video display over a separate channel. Samuel Fedida was credited as inventor of the system. Fedida had the idea for Viewdata in 1968. The first prototype became operational in 1974. The access, request and reception are usually via common carrier broadcast channels. This is in contrast with teletext.
Technology.
Originally Viewdata was accessed with a special purpose terminal (or emulation software) and a modem running at CCITT V.23 speed (1200 bit/s down, 75 bit/s up). By 2004 it was normally accessed over TCP/IP using Viewdata client software on a personal computer running Microsoft Windows, or using a Web-based emulator.
Travel industry.
As of 2015, Viewdata is still in use in the United Kingdom, mainly by the travel industry. Travel agents use it to look up the price and availability of package holidays and flights. Once they find what the customer is looking for they can place a booking. 
There are a number of factors still holding up a move to a Web based standard. Viewdata is regarded within the industry as low-cost and reliable, travel consultants have been trained to use Viewdata, they would need training to book holidays on the Internet, and tour operators cannot agree on a Web-based standard. 
Bulletin board systems.
It was made in the late 1970s and early 1980s to make it easier for travel consultants to check availability and make bookings for holidays.
A number of Viewdata bulletin board systems existed in the 1980s, predominantly in the UK due to the proliferation of the BBC Microcomputer, and a short-lived "Viewdata Revival" appeared in the late 1990s fuelled by the retrocomputing vogue. Some Viewdata boards still exist, with accessibility in the form of Java Telnet clients.

</doc>
<doc id="41850" url="https://en.wikipedia.org/wiki?curid=41850" title="Virtual call capability">
Virtual call capability

In telecommunication, a virtual call capability, sometimes called a virtual call facility, is a service feature in which:

</doc>
<doc id="41851" url="https://en.wikipedia.org/wiki?curid=41851" title="Virtual circuit">
Virtual circuit

A virtual circuit (VC) is a means of transporting data over a packet switched computer network in such a way that it appears as though there is a dedicated physical layer link between the source and destination end systems of this data. The term virtual circuit is synonymous with virtual connection and virtual channel. Before a connection or virtual circuit may be used, it has to be established, between two or more nodes or software applications, by configuring the relevant parts of the interconnecting network. After that, a bit stream or byte stream may be delivered between the nodes; hence, a virtual circuit protocol allows higher level protocols to avoid dealing with the division of data into segments, packets, or frames.
Virtual circuit communication resembles circuit switching, since both are connection oriented, meaning that in both cases data is delivered in correct order, and signalling overhead is required during a connection establishment phase. However, circuit switching provides a constant bit rate and latency, while these may vary in a virtual circuit service due to factors such as:
Many virtual circuit protocols, but not all, provide reliable communication service through the use of data retransmissions because of error detection and automatic repeat request (ARQ).
An alternate network configuration to virtual circuit is datagram.
Layer 4 virtual circuits.
Connection oriented transport layer datalink protocols such as TCP may rely on a connectionless packet switching network layer protocol such as IP, where different packets may be routed over different paths, and thus be delivered out of order. However, it is possible to use TCP as a virtual circuit, since TCP includes segment numbering that allows reordering on the receiver side to accommodate out-of-order delivery.
Layer 2/3 virtual circuits.
Datalink layer and network layer virtual circuit protocols are based on connection oriented packet switching, meaning that data is always delivered along the same network path, i.e., through the same nodes. Advantages with this over connectionless packet switching are: 
Examples of protocols that provide virtual circuits.
Examples of transport layer protocols that provide a virtual circuit:
Examples of network layer and datalink layer virtual circuit protocols, where data always is delivered over the same path:
Permanent and switched virtual circuits in ATM, frame relay, and X.25.
Switched virtual circuits (SVCs) are generally set up on a per-call basis and are disconnected when the call is terminated; however, a permanent virtual circuit (PVC) can be established as an option to provide a dedicated circuit link between two facilities. PVC configuration is usually preconfigured by the service provider. Unlike SVCs, PVC are usually very seldom broken/disconnected.
A switched virtual circuit (SVC) is a virtual circuit that is dynamically established on demand and is torn down when transmission is complete, for example after a phone call or a file download. SVCs are used in situations where data transmission is sporadic and/or not always between the same data terminal equipment (DTE) endpoints.
A permanent virtual circuit (PVC) is a virtual circuit established for repeated/continuous use between the same DTE. In a PVC, the long-term association is identical to the data transfer phase of a virtual call. Permanent virtual circuits eliminate the need for repeated call set-up and clearing.

</doc>
<doc id="41853" url="https://en.wikipedia.org/wiki?curid=41853" title="Virtual storage">
Virtual storage

Virtual storage can refer to:

</doc>
<doc id="41854" url="https://en.wikipedia.org/wiki?curid=41854" title="Virtual terminal">
Virtual terminal

In open systems, a virtual terminal (VT) is an application service that:
PuTTY is an example of a virtual terminal.
ITU-T defines a virtual terminal protocol based on the OSI application layer protocols. However, the virtual terminal protocol is not widely used on the Internet.
Virtual Terminals for payment card processing.
This term is also used to refer to web interfaces for processing card not present transactions. Such terminals allow call centre agents to enter a customer's credit card details to take a payment.

</doc>
<doc id="41855" url="https://en.wikipedia.org/wiki?curid=41855" title="Voice frequency">
Voice frequency

A voice frequency (VF) or voice band is one of the frequencies, within part of the audio range, that is used for the transmission of speech.
In telephony, the usable voice frequency band ranges from approximately 300 Hz to 3400 Hz. It is for this reason that the ultra low frequency band of the electromagnetic spectrum between 300 and 3000 Hz is also referred to as "voice frequency", being the electromagnetic energy that represents acoustic energy at baseband. The bandwidth allocated for a single voice-frequency transmission channel is usually 4 kHz, including guard bands, allowing a sampling rate of 8 kHz to be used as the basis of the pulse code modulation system used for the digital PSTN. Per the Nyquist–Shannon sampling theorem, the sampling frequency (8 kHz) must be at least twice the highest component of the voice frequency via appropriate filtering prior to sampling at discrete times (4 kHz) for effective reconstruction of the voice signal.
Fundamental frequency.
The voiced speech of a typical adult male will have a fundamental frequency from 85 to 180 Hz, and that of a typical adult female from 165 to 255 Hz. Thus, the fundamental frequency of most speech falls below the bottom of the "voice frequency" band as defined above. However, enough of the harmonic series will be present for the missing fundamental to create the impression of hearing the fundamental tone.

</doc>
<doc id="41856" url="https://en.wikipedia.org/wiki?curid=41856" title="Voice frequency primary patch bay">
Voice frequency primary patch bay

In telecommunication, a voice frequency primary patch bay (VF) is a patching facility that provides the first appearance of local-user VF circuits in the technical control facility (TCF). 
The VF primary patch bay provides patching, monitoring, and testing for all VF circuits. Signals will have various levels and signaling schemes depending on the user terminal equipment.

</doc>
<doc id="41858" url="https://en.wikipedia.org/wiki?curid=41858" title="Volt-ampere reactive">
Volt-ampere reactive

In electric power transmission and distribution, volt-ampere reactive (var) is a unit by which reactive power is expressed in an AC electric power system. Reactive power exists in an AC circuit when the current and voltage are not in phase. The correct symbol is var and not Var, VAr, or VAR, but all three terms are widely used, and VAR is widely used throughout the power industry infrastructure. The term "var" was proposed by the Romanian electrical engineer Constantin Budeanu and introduced in 1930 by the IEC in Stockholm, which has adopted it as the unit for reactive power.
Vars may be considered as either the imaginary part of apparent power, or the power flowing into a reactive load, where voltage and current are specified in volts and amperes. The two definitions are equivalent.
The unit "var" does not follow the recommended practice of the International System of Units, because the quantity the unit var represents is power, and SI practice is not to include information about the type of power being measured in the unit name. 
Reactive power.
A sinusoidally alternating voltage applied to a purely resistive load results in an alternating current that is fully in phase with the voltage. However, in many applications it is common for there to be a reactive component to the system, that is, the system possesses capacitance, inductance, or both. These electrical properties cause the current to change phase with respect to the voltage: capacitance tending the current to lead the voltage in phase, and inductance to lag it.
For sinusoid currents and voltages at the same frequency, reactive power in vars is the product of the RMS voltage and current, or the apparent power, multiplied by the sine of formula_1 (phase angle between the voltage and the current). The reactive power formula_2 (measured in units of volt-amperes reactive or var) is given by:
where formula_1 is the phase angle between the current and voltage.
Q refers to the maximum value of the instantaneous power absorbed by the reactive component of the load. 
Only effective power, the actual power delivered to or consumed by the load, is expressed in watts. The imaginary part is properly expressed in volt-amperes reactive.
Physical significance of reactive power.
Reactive power (measured in vars) is present in a system containing reactive (inductive or capacitive) components and can be either produced or consumed by different load/generation elements. The reactive power (the imaginary part) has great physical significance and is essential to the operation of the electrical system as a whole. While the real power P is used to supply the energy required to perform actual work (such as running a motor), the reactive power regulates the voltage in the system. If the reactive power is too low, inductive loads such as transformers will be unable to maintain voltages necessary for the generation of electromagnetic fields, leading to a "voltage collapse" that create blackouts. Transmission line impedances also make it necessary to provide reactive power to maintain voltage levels necessary for active power to flow through. Therefore reactive power is essential to move active power through transmission and distribution systems to the customer. However if reactive power in a system is too high, there is increased heat loss in transmission lines and loads as the current flowing through the system is much higher, creating a potentially hazardous breakdown situation. The power factor of a load tells us what fraction of the apparent power is in the form of real power and performs actual work. A high power factor is desirable since it minimizes the amount of reactive power needed by the load, reducing heat losses and maximizing efficiency.

</doc>
<doc id="41859" url="https://en.wikipedia.org/wiki?curid=41859" title="Voice-operated switch">
Voice-operated switch

In telecommunications, a voice operated switch, also known as VOX or Voice Operated eXchange, is a switch that operates when sound over a certain threshold is detected. It is usually used to turn on a transmitter or recorder when someone speaks and turn it off when they stop speaking. It is used instead of a push-to-talk button on transmitters or to save storage space on recording devices. On cell phones, it is used to save battery life. Intercom systems that use a speaker in a room as both a speaker and a microphone will often use VOX on the main console to switch the audio direction during a conversation. The circuit usually includes a delay between the sound stopping and switching direction, to avoid the circuit turning off during short pauses in speech.
A special case exists, if there is enough energy to power the system directly. For example, a microphone may send a voltage, high enough, to directly operate a transmitter. 
Comparison with push-to-talk.
Unlike manual push-to-talk (PTT) operation, VOX is
automatic; the user can keep his or her hands free while talking. But
VOX also has some significant disadvantages that explain why PTT is
still common.
Most VOX circuits have a sensitivity adjustment, but unwanted (and
sometimes undetected) VOX triggering can still occur on background
noise, heavy breathing or a side conversation. Conversely, it may not
activate when desired on speech that is too weak.
The VOX in a two-way radio can also be triggered by the loudspeaker
carrying the other side of the conversation. This problem can be
minimized with an "anti vox" feature to decrease VOX sensitivity when
the receiver is active.
Transmitters and recorders have short but finite activation times that
may clip the beginnings of phrases. Some modern VOX circuits eliminate
this problem by recording or transmitting a delayed version of the
input signal.
VOX uses a "hang" timer, typically 1-3 seconds, to remain engaged
during brief speech pauses. This means the last several seconds of
each transmission or recorded segment are always silence. A
VOX-activated recorder can delete the end of each segment but the user
of a VOX-activated half duplex radio must wait for the timer to expire before he or she can receive again.

</doc>
<doc id="41860" url="https://en.wikipedia.org/wiki?curid=41860" title="Wafer (electronics)">
Wafer (electronics)

A wafer, also called a slice or substrate, is a thin slice of semiconductor material, such as a crystalline silicon, used in electronics for the fabrication of integrated circuits and in photovoltaics for conventional, wafer-based solar cells. The wafer serves as the substrate for microelectronic devices built in and over the wafer and undergoes many microfabrication process steps such as doping or ion implantation, etching, deposition of various materials, and photolithographic patterning. Finally the individual microcircuits are separated (dicing) and packaged.
History.
By 1960, silicon wafers were being manufactured in the U.S. by companies such as MEMC/SunEdison. In 1965, American engineers Eric O. Ernst, Donald J. Hurd, and Gerard Seeley, while working under IBM, filed Patent US3423629A for the first high-capacity epitaxial apparatus.
Formation.
Wafers are formed of highly pure (99.9999999% purity), 
nearly defect-free single crystalline material. One process for forming crystalline wafers is known as Czochralski growth invented by the Polish chemist Jan Czochralski. In this process, a cylindrical ingot of high purity mono crystalline semiconductor, such as silicon or germanium, called a boule, is formed by pulling a seed crystal from a 'melt'. Donor impurity atoms, such as boron or phosphorus in the case of silicon, can be added to the molten intrinsic material in precise amounts in order to dope the crystal, thus changing it into n-type or p-type extrinsic semiconductor.
The boule is then sliced with a wafer saw (wire saw) and polished to form wafers. The size of wafers for photovoltaics is 100–200 mm square and the thickness is 200–300 μm. In the future, 160 μm will be the standard. Electronics use wafer sizes from 100–450 mm diameter. (The largest wafers made have a diameter of 450 mm but are not yet in general use.)
Cleaning, texturing and etching.
Wafers are cleaned with weak acids to remove unwanted particles, or repair damage caused during the sawing process. When used for solar cells, the wafers are textured to create a rough surface to increase their efficiency. The generated PSG (phosphosilicate glass) is removed from the edge of the wafer in the etching.
Wafer properties.
Standard wafer sizes.
Silicon wafers are available in a variety of diameters from 25.4 mm (1 inch) to 300 mm (11.8 inches). Semiconductor fabrication plants (also known as "fabs") are defined by the diameter of wafers that they are tooled to produce. The diameter has gradually increased to improve throughput and reduce cost with the current state-of-the-art fab using , with a proposal to adopt . Intel, TSMC and Samsung are separately conducting research to the advent of "prototype" (research) fabs, though serious hurdles remain.
Wafers grown using materials other than silicon will have different thicknesses than a silicon wafer of the same diameter. Wafer thickness is determined by the mechanical strength of the material used; the wafer must be thick enough to support its own weight without cracking during handling.
Historical increases of wafer size.
A unit wafer fabrication step, such as an etch step, can produce more chips proportional to the increase in wafer area, while the cost of the unit fabrication step goes up more slowly than the wafer area. This was the cost basis for increasing wafer size. Conversion to 300 mm wafers from 200 mm wafers began in earnest in 2000, and reduced the price per die about 30-40%.
However, this was not without significant problems for the industry.
Proposed 450mm transition.
There is considerable resistance to the 450 mm transition despite the possible productivity improvement, because of concern about insufficient return on investment. Higher cost semiconductor fabrication equipment for larger wafers increases the cost of 450 mm fabs (semiconductor fabrication facilities or factories). Lithographer Chris Mack claimed in 2012 that the overall price per die for 450 mm wafers would be reduced by only 10-20% compared to 300 mm wafers, because over 50% of total wafer processing costs are lithography-related. Converting to larger 450 mm wafers would reduce price per die only for process operations such as etch where cost is related to wafer count, not wafer area. Cost for processes such as lithography is proportional to wafer area, and larger wafers would not reduce the lithography contribution to die cost. Nikon plans to deliver 450-mm lithography equipment in 2015, with volume production in 2017. In November 2013 ASML paused development of 450-mm lithography equipment, citing uncertain timing of chipmaker demand.
The time-line for 450 mm has not been fixed. Mark Durcan, CEO of Micron Technology, said in February 2014 that he expects 450 mm adoption to be delayed indefinitely or discontinued. “I am not convinced that 450mm will ever happen but, to the extent that it does, it’s a long way out in the future. There is not a lot of necessity for Micron, at least over the next five years, to be spending a lot of money on 450mm. There is a lot of investment that needs to go on in the equipment community to make that happen. And the value at the end of the day – so that customers would buy that equipment – I think is dubious.” As of March 2014, Intel Corporation expects 450 mm deployment by 2020 (by the end of this decade). Mark LaPedus of semiengineering.com reported in mid-2014 that chipmakers had delayed adoption of 450 mm “for the foreseeable future.” According to this report some observers expect 2018 to 2020, while “G. Dan Hutcheson, chief executive of VLSI Research, doesn’t see 450mm fabs moving into production until 2020 to 2025.”
The step up to 300 mm required major changes, with fully automated factories using 300 mm wafers versus barely automated factories for the 200 mm wafers. These major investments were undertaken in the economic downturn following the dot-com bubble, resulting in huge resistance to upgrading to 450 mm by the original timeframe. On the ramp up to 450 mm are that the crystal ingots will be 3 times heavier (total weight a metric ton) and take 2-4 times longer to cool, and the process time will be double. All told, the development of 450 mm wafers requires significant engineering, time, and cost to overcome.
Analytical die count estimation.
In order to minimize the cost per die, manufacturers wish to maximize the number of dies that can be made from a single wafer; dies always have a square or rectangular shape due to the constraint of wafer dicing. In general, this is a computationally complex problem with no analytical solution, dependent on both the area of the dies as well as their aspect ratio (square or rectangular) and other considerations such as scribeline size and the space occupied by alignment and test structures. Note that gross DPW formulas account only for wafer area that is lost because it cannot be used to make physically complete dies; gross DPW calculations do "not" account for yield loss due to defects or parametric issues.
Nevertheless, the number of gross die per wafer (DPW) can be estimated starting with the first-order approximation or wafer-to-die area ratio,
where formula_2 is the wafer diameter (typically in mm) and formula_3 the size of each die (mm2). This formula simply states that the number of dies which can fit on the wafer cannot exceed the area of the wafer divided by the area of each individual die. It will always overestimate the true best-case gross DPW, since it includes the area of partially patterned dies which do not fully lie on the wafer surface (see figure). These partially patterned dies don't represent complete ICs, so they cannot be sold as functional parts.
Refinements of this simple formula typically add an edge correction, to account for partial dies on the edge, which in general will be more significant when the area of the die is large compared to the total area of the wafer. In the other limiting case (infinitesimally small dies or infinitely large wafers), the edge correction is negligible.
The correction factor or correction term generally takes one of the forms cited by De Vries,
Studies comparing these analytical formulas to brute-force computational results show that the formulas can be made more accurate, over practical ranges of die sizes and aspect ratios, by adjusting the coefficients of the corrections to values above or below unity, and by replacing the linear die dimension formula_7 with formula_8 (average side length) in the case of dies with large aspect ratio:
Crystalline orientation.
Wafers are grown from crystal having a regular crystal structure, with silicon having a diamond cubic structure with a lattice spacing of 5.430710 Å (0.5430710 nm). When cut into wafers, the surface is aligned in one of several relative directions known as crystal orientations. Orientation is defined by the Miller index with (100) or (111) faces being the most common for silicon.
Orientation is important since many of a single crystal's structural and electronic properties are highly anisotropic. Ion implantation depths depend on the wafer's crystal orientation, since each direction offers distinct paths for transport.
Wafer cleavage typically occurs only in a few well-defined directions. Scoring the wafer along cleavage planes allows it to be easily diced into individual chips ("dies") so that the billions of individual circuit elements on an average wafer can be separated into many individual circuits.
Crystallographic orientation notches.
Wafers under 200 mm diameter have "flats" cut into one or more sides indicating the crystallographic planes of the wafer (usually a {110} face). In earlier-generation wafers a pair of flats at different angles additionally conveyed the doping type (see illustration for conventions). Wafers of 200 mm diameter and above use a single small notch to convey wafer orientation, with no visual indication of doping type.
Impurity doping.
Silicon wafers are generally not 100% pure silicon, but are instead formed with an initial impurity doping concentration between 1013 and 1016 atoms per cm3 of boron, phosphorus, arsenic, or antimony which is added to the melt and defines the wafer as either bulk n-type or p-type. However, compared with single-crystal silicon's atomic density of 5×1022 atoms per cm3, this still gives a purity greater than 99.9999%. The wafers can also be initially provided with some interstitial oxygen concentration. Carbon and metallic contamination are kept to a minimum. Transition metals, in particular, must be kept below parts per billion concentrations for electronic applications.
Compound semiconductors.
While silicon is the prevalent material for wafers used in the electronics industry, other compound III-V or II-VI materials have also been employed. Gallium arsenide (GaAs), a III-V semiconductor produced via the Czochralski process, is also a common wafer material.

</doc>
<doc id="41861" url="https://en.wikipedia.org/wiki?curid=41861" title="Wide area information server">
Wide area information server

Wide Area Information Server (WAIS) is a client–server text searching system that uses the ANSI Standard Z39.50 Information Retrieval Service Definition and Protocol Specifications for Library Applications" (Z39.50:1988) to search index databases on remote computers. It was developed in the late 1980s as a project of Thinking Machines, Apple Computer, Dow Jones, and KPMG Peat Marwick. 
WAIS did not adhere to either the standard or its OSI framework (adopting instead TCP/IP) but created a unique protocol inspired by Z39.50:1988.
History.
The WAIS protocol and servers were primarily promoted by Thinking Machines Corporation (TMC) of Cambridge, Massachusetts. TMC produced WAIS servers which ran on their massively parallel CM-2 (Connection Machine) and SPARC-based CM-5 MP supercomputers. WAIS clients were developed for various operating systems and windowing systems including Microsoft Windows, Macintosh, NeXT, X, GNU Emacs, and character terminals. TMC, however, released a free open source version of WAIS to run on Unix in 1991. 
Inspired by the WAIS project on full text databases and emerging SGML projects Z39.50 version 2 or Z39.50:1992 was released. Unlike its 1988 predecessor it was a compatible superset of the ISO 10162/10163 work that had been done internationally.
With the advent of Z39.50:1992, the termination of support for the free WAIS from Thinking Machines and the establishment of WAIS Inc as a commercial venture, the U.S. National Science Foundation funded the Clearinghouse for Networked Information Discovery and Retrieval (CNIDR) to create a clearinghouse of information related to Internet search and discovery systems and to promote open source and standards. CNIDR created a new freely available open-source WAIS. This created first the freeWAIS package based on the wais-8-b5 codebase implemented by Thinking Machines Corp and then a wholly new software suite Isite based upon Z39.50:1992 with Isearch as its full text search engine.
Ulrich Pfeifer and Norbert Gövert of the computer science department of the University of Dortmund took the CNIDR freeWAIS code and extended it to become freeWAIS-sf: sf means structured fields and indicated its main improvement. Ulrich Pfeifer rewrote freeWAIS-sf in Perl where it became WAIT.
Inspired also by WAIS, especially its "Directory of Servers", Eliot Christian of USGS envisioned GILS: Government Information Locator Service. GILS (based upon Z39.50:1992 with some WAIS-like extensions) became a U.S. Federal mandate as part of the Paperwork Reduction Act of 1995 ().
Directory of Servers.
Thinking Machines Corp provided a service called the Directory of Servers. It was a WAIS server like any other information source but contained information about the other WAIS servers on the Internet. When one would create a WAIS server with the TMC WAIS code it would create a special kind of record containing metadata and some common words to describe the content of the index. It would be uploaded to the central server and indexed along with the records from other public servers. One could search the directory to find servers that might have content relevant to a specific field of interest. This model of searching for (WAIS) servers to search became the role model for GILS and Peter Deutsch's WHOIS++ distributed white pages directory.
People.
Two of the developers of WAIS, Brewster Kahle and Harry Morris, left Thinking Machines to found WAIS Inc in Menlo Park, California with Bruce Gilliat. WAIS Inc. was originally developed as a joint project between Apple Computer, Peat Markwick, Dow Jones, and Thinking Machines. In 1992, the presidential campaign of Ross Perot used WAIS as a campaign wide information system, connecting the field offices to the national office. Later, Perot Systems adopted WAIS to better access the information in its corporate databases. Other early clients were the Environmental Protection Agency, Library of Congress, and the Department of Energy and later the "Wall Street Journal" and "Encyclopædia Britannica".
WAIS Inc was sold to AOL in May 1995 for $15 million. Following the sale, Margaret St. Pierre left WAIS Inc to start Blue Angel Technologies. Her "WAIS variant" formed the basis of MetaStar. Georgios Papadopoulos left to found Atypon. François Schiettecatte left Human Genome Project at Johns Hopkins Hospital and started FS-Consult and developed his own variant of WAIS which eventually became ScienceServer, which was later sold to Elsevier Science. Kahle and Gilliat went on to found the Internet Archive and Alexa Internet.
WAIS and Gopher.
Public WAIS is often used as a full text search engine for individual Internet Gopher servers, supplementing the popular Veronica system which only searches the menu titles of Gopher sites. WAIS and Gopher share the World Wide Web's client–server architecture and a certain amount of its functionality. The WAIS protocol is influenced largely by the z39.50 protocol designed for networking library catalogs. It allows a text-based search, and retrieval following a search. Gopher provides a free text search mechanism, but principally uses menus. A menu is a list of titles, from which the user may pick one. While gopher space is a web containing many loops, the menu system gives the user the impression of a tree.
The Web's data model is similar to the gopher model, except that menus are generalized to hypertext documents. In both cases, simple file servers generate the menus or hypertext directly from the file structure of a server. The Web's hypertext model permits the author more freedom to communicate the options available to the reader, as it can include headings and various forms of list structure.

</doc>
<doc id="41862" url="https://en.wikipedia.org/wiki?curid=41862" title="Warner exemption">
Warner exemption

In telecommunication, a Warner exemption is a statutory exemption pertaining to the acquisition of telecommunications systems that meet the exclusionary criteria of the Warner Amendment, Public Law 97-86, 1 December 1981, which is also known as the Brooks Bill. 
Use of FTS2000 by U.S. Government agencies is mandatory when telecommunications are required. However, the Warner Amendment excludes the mandatory use of FTS2000 in instances related to maximum security.

</doc>
<doc id="41863" url="https://en.wikipedia.org/wiki?curid=41863" title="Waveguide">
Waveguide

A waveguide is a structure that guides waves, such as electromagnetic waves or sound waves. They enable a signal to propagate with minimal loss of energy by restricting expansion to one dimension or two. This is a similar effect to waves of water constrained within a canal, or why guns have barrels that restrict hot gas expansion to maximize energy transfer to their bullets. Without the physical constraint of a waveguide, signals will typically dissipate according to the inverse square law as they expand into three dimensional space.
There are different types of waveguides for each type of wave. The original and most common meaning is a hollow conductive metal pipe used to carry high frequency radio waves, particularly microwaves.
The geometry of a waveguide reflects its function. Slab waveguides confine energy to travel only in one dimension, fiber or channel waveguides for two dimensions. The frequency of the transmitted wave also dictates the shape of a waveguide: an optical fiber guiding high-frequency light will not guide microwaves of a much lower frequency. As a rule of thumb, the width of a waveguide needs to be of the same order of magnitude as the wavelength of the guided wave.
Some naturally occurring structures can also act as waveguides. The SOFAR channel layer in the ocean can guide the sound of whale song across enormous distances.
Principle of operation.
Waves propagate in all directions in open space as spherical waves. The power of the wave falls with the distance "R" from the source as the square of the distance (inverse square law). A waveguide confines the wave to propagate in one dimension, so that, under ideal conditions, the wave loses no power while propagating.
Due to total reflection at the walls, waves are confined to the interior of a waveguide. The propagation inside the waveguide, hence, can be described approximately as a "zigzag" between the walls. This description is exact for electromagnetic waves in a hollow metal tube with a rectangular or circular cross-section.
History.
The first structure for guiding waves was proposed by J. J. Thomson in 1893, and was first experimentally tested by Oliver Lodge in 1894. The first mathematical analysis of electromagnetic waves in a metal cylinder was performed by Lord Rayleigh in 1897.
For sound waves, Lord Rayleigh published a full mathematical analysis of propagation modes in his seminal work, “The Theory of Sound”.
The study of dielectric waveguides (such as optical fibers, see below) began as early as the 1920s, by several people, most famous of which are Rayleigh, Sommerfeld and Debye. 
Optical fiber began to receive special attention in the 1960s due to its importance to the communications industry.
Uses.
The uses of waveguides for transmitting signals were known even before the term was coined. The phenomenon of sound waves guided through a taut wire have been known for a long time, as well as sound through a hollow pipe such as a cave or medical stethoscope. Other uses of waveguides are in transmitting power between the components of a system such as radio, radar or optical devices. Waveguides are the fundamental principle of guided wave testing (GWT), one of the many methods of non-destructive evaluation.
Specific examples:
Propagation modes and cutoff frequencies.
A propagation mode in a waveguide is one solution of the wave equations, or, in other words, the form of the wave. Due to the constraints of the boundary conditions, there are only limited frequencies and forms for the wave function which can propagate in the waveguide. The lowest frequency in which a certain mode can propagate is the cutoff frequency of that mode. The mode with the lowest cutoff frequency is the basic mode of the waveguide, and its cutoff frequency is the waveguide cutoff frequency.
Impedance matching.
In circuit theory, the impedance is a generalization of electrical resistivity in the case of alternating current, and is measured in ohms (formula_1).
A waveguide in circuit theory is described by a transmission line having a length and self-impedance. In other words, the impedance is the resistance of the circuit component (in this case a waveguide) to the propagation of the wave. This description of the waveguide was originally intended for alternating current, but is also suitable for electromagnetic and sound waves, once the wave and material properties (such as pressure, density, dielectric constant) are properly converted into electrical terms (current and impedance for example).
Impedance matching is important when components of an electric circuit are connected (waveguide to antenna for example): The impedance ratio determines how much of the wave is transmitted forward and how much is reflected. In connecting a waveguide to an antenna a complete transmission is usually required, so their impedances are matched.
The reflection coefficient can be calculated using: formula_2, where formula_3 is the reflection coefficient (0 denotes full transmission, 1 full reflection, and 0.5 is a reflection of half the incoming voltage), formula_4 and formula_5 are the impedance of the first component (from which the wave enters) and the second component, respectively.
An impedance mismatch creates a reflected wave, which added to the incoming waves creates a standing wave. An impedance mismatch can be also quantified with the standing wave ratio (SWR or VSWR for voltage), which is connected to the impedance ratio and reflection coefficient by: formula_6, where formula_7 are the minimum and maximum values of the voltage absolute value, and the VSWR is the voltage standing wave ratio, which value of 1 denotes full transmission, without reflection and thus no standing wave, while very large values mean high reflection and standing wave pattern.
Electromagnetic waveguides.
Waveguides can be constructed to carry waves over a wide portion of the electromagnetic spectrum, but are especially useful in the microwave and optical frequency ranges. Depending on the frequency, they can be constructed from either conductive or dielectric materials. Waveguides are used for transferring both power and communication signals.
Optical waveguides.
Waveguides used at optical frequencies are typically dielectric waveguides, structures in which a dielectric material with high permittivity, and thus high index of refraction, is surrounded by a material with lower permittivity. The structure guides optical waves by total internal reflection. An example of an optical waveguide is optical fiber.
Other types of optical waveguide are also used, including photonic-crystal fiber, which guides waves by any of several distinct mechanisms. Guides in the form of a hollow tube with a highly reflective inner surface have also been used as light pipes for illumination applications. The inner surfaces may be polished metal, or may be covered with a multilayer film that guides light by Bragg reflection (this is a special case of a photonic-crystal fiber). One can also use small prisms around the pipe which reflect light via total internal reflection [http://www.physics.ubc.ca/ssp/research/lightpipe.htm]—such confinement is necessarily imperfect, however, since total internal reflection can never truly guide light within a "lower"-index core (in the prism case, some light leaks out at the prism corners).
Acoustic waveguides.
An "acoustic waveguide" is a physical structure for guiding sound waves. A duct for sound propagation also behaves like a transmission line. The duct contains some medium, such as air, that supports sound propagation.
Sound synthesis.
Sound synthesis uses digital delay lines as computational elements to simulate wave propagation in tubes of wind instruments and the vibrating strings of string instruments.

</doc>
<doc id="41864" url="https://en.wikipedia.org/wiki?curid=41864" title="Wave impedance">
Wave impedance

The wave impedance of an electromagnetic wave is the ratio of the transverse components of the electric and magnetic fields (the transverse components being those at right angles to the direction of propagation). For a transverse-electric-magnetic (TEM) plane wave traveling through a homogeneous medium, the wave impedance is everywhere equal to the intrinsic impedance of the medium. In particular, for a plane wave travelling through empty space, the wave impedance is equal to the impedance of free space. The symbol "Z" is used to represent it and it is expressed in units of ohms. The symbol η (eta) may be used instead of "Z" for wave impedance to avoid confusion with electrical impedance.
The wave impedance is given by
where formula_2 is the electric field and formula_3 is the magnetic field, in phasor representation. The impedance is in general a complex number.
In terms of the parameters of an electromagnetic wave and the medium it travels through, the wave impedance is given by
where μ is the magnetic permeability, ε is the electric permittivity and σ is the electrical conductivity of the material the wave is travelling through. In the equation, "j" is the imaginary unit, and ω is the angular frequency of the wave. In the case of a dielectric (where the conductivity is zero), the equation reduces to the real number
As usual for any electrical impedance, the ratio is defined only for the frequency domain and never in the time domain.
Wave impedance in free space.
In free space the wave impedance of plane waves is: 
and:
hence, to the same accuracy as the current definition of formula_8, the value in ohms is:
Wave impedance in an unbounded dielectric.
In a isotropic, homogeneous dielectric with negligible magnetic properties, i.e. formula_10 H/m and formula_11 F/m. So, the value of wave impedance in a perfect dielectric is
In a perfect dielectric, the wave impedance can be found by dividing "Z"0 by the square root of the (relative) dielectric constant.
Wave impedance in a waveguide.
For any waveguide in the form of a hollow metal tube, (such as rectangular guide, circular guide, or double-ridge guide), the wave impedance of a travelling wave is dependent on the frequency formula_13, but is the same throughout the guide. For transverse electric (TE) modes of propagation the wave impedance is:
where "f""c" is the cut-off frequency of the mode, and for transverse magnetic (TM) modes of propagation the wave impedance is:
Above the cut-off ("f" > "f""c"), the impedance is real (resistive) and the wave carries energy. Below cut-off the impedance is imaginary (reactive) and the wave is evanescent. These expressions neglect the effect of resistive loss in the walls of the waveguide. For a waveguide entirely filled with a homogeneous dielectric medium, similar expressions apply, but with the wave impedance of the medium replacing "Z"0. The presence of the dielectric also modifies the cut-off frequency "f""c".
For a waveguide or transmission line containing more than one type of dielectric medium (such as microstrip), the wave impedance will in general vary over the cross-section of the line.

</doc>
<doc id="41865" url="https://en.wikipedia.org/wiki?curid=41865" title="White facsimile transmission">
White facsimile transmission

In telecommunication, the term white facsimile transmission has the following meanings: 
Both of these terms became outmoded in the late 20th century except in specialist usage, as most fax machines now use the digital ITU-T fax standards, which encode the image digitally over a QAM-modulated signal. See also Black facsimile transmission.

</doc>
<doc id="41867" url="https://en.wikipedia.org/wiki?curid=41867" title="Wide Area Telephone Service">
Wide Area Telephone Service

Wide Area Telephone Service (WATS) was a flat-rate long distance service offering for customer dial-type telecommunications between a given customer phone (also known as a "station") and stations within specified geographic rate areas employing a single telephone line between the customer location and the serving central office. Each access line could be arranged for outward (OUT-WATS) or inward (IN-WATS) service, or both.
WATS was introduced by the Bell System in 1961 as a primitive long-distance flat-rate plan by which a business could obtain a special line with an included number of hours ('measured time' or 'full-time') of long-distance calling to a specified area. These lines were most often connected to private branch exchanges in large businesses. WATS lines were the basis for the first direct-dial toll-free +1-800 numbers (intrastate in 1966, interstate in 1967); by 1976, WATS brought AT&T a billion dollars in annual revenue.
For outbound calls, the 1984 AT&T divestiture brought multiple competitors offering similar services using standard business telephone lines; the special WATS line was ultimately supplanted by other flat-rate offerings. The requirement that an inbound toll-free number terminate at a special WATS line or fixed-rate service was also rendered obsolete by the 1980s due to intelligent network capability and technological improvement in the +1-800 service. A toll-free number may now terminate at a T carrier line, at any standard local telephone number or at one of multiple destinations based on time of day, call origin, cost or other factors.
Outbound WATS.
For Outbound WATS, the United States was divided into geographical Bands 0 through 5, relative to the purchaser. Band zero was intrastate calling and bands 1 through 5 (or 6) were interstate calls that were progressively further from the originating number. Historically, the higher band number carried a higher price per month or per minute. These lines could be used for outbound long-distance only; not local. In the U.S., interstate WATS lines could not be used for intrastate calls, and vice versa. With wider availability of inexpensive long distance using regular business lines, OutWATS service became obsolete late in the 20th century.
InWATS.
The original North American toll-free number was the Zenith number, published in one distant city (or a few cities) only. Published as "Zenith" and a five-digit number, these collect calls required operator assistance. The called party was charged for the operator-assisted call.
With "inward WATS", introduced for interstate calls by AT&T in 1967, subscribers were issued a toll-free telephone number in a designated toll-free area code. Unlike a standard collect call or a call to a Zenith number, +1-800- normally may be dialed directly with no live operator. Callers within a designated area could call without incurring a toll charge as the recipient paid for the calls at a fixed rate.
The introduction of InWATS fortuitously fell around the same time as the early centralized, automated national airline and hotel reservation systems, including Sabre (American Airlines, 1963), Holidex (Holiday Inn, 1965) and Reservatron (Sheraton, 1969). Hundreds of local reservation numbers for a major chain could be replaced with one central number, backed by a national computerized reservation system.
InWATS exchanges were assigned to Canada and other North American Numbering Plan countries, but the original InWATS in each country accepted domestic calls only. Initially +1-800-NN2-XXXX numbers were U.S. intrastate and specific prefixes (such as +1-800-387 Toronto and +1-800-267 Ottawa) were assigned to Canada. In the 1970s, AT&T's internal routing guides included separate U.S. and Canadian 1-800 exchange maps which looked much like area code maps as each geographic area code had one or more specific freephone exchange prefixes. Sheraton's 800-325-3535, one of the notable early adopters in late 1969, was hard-wired into St. Louis area code 314; 1-800-HOLIDAY at that time could not be a U.S. number if the 1-800-465 prefix was hard-wired to Thunder Bay's area code 807. Any attempt to call a foreign +1-800 gave a pre-recorded error, "the number you have dialed is not available from your calling area."
Like the OutWATS service, AT&T's InWATS was divided into intrastate and interstate, with interstate calls priced into five or six "bands" of calling. This favored placement of US national call centers in low-population Midwestern states such as Nebraska, whose central location meant a carefully situated "band 3" number reaching halfway across the US in every direction could potentially reach 47 states. A San Diego call center would be less fortunate; even with "band 6" (the most expensive lines), its 'national' number would be unreachable to millions as California is a populous state and intrastate calls needed a separate toll-free number.
The original InWATS system was supplanted by "Advanced 800 Service" in the 1980s. Modern systems eliminated requirements tying toll-free numbers to dedicated flat-rate inbound WATS lines. Direct inward dial, introduced in 1983, allowed one trunk to carry calls for multiple numbers. AT&T's monopoly on U.S. toll-free number routing ended in 1986, encouraging flexibility in order to match rivals Sprint and MCI. By 1989, fixed "bands" of coverage area had been largely replaced by distance-based billing, a growing number of 1-800 numbers were being terminated at standard local business or residence lines and one number could be sent to multiple locations based on call origin, least-cost routing or time of day routing. RespOrgs were established in the U.S. in 1993 and Canada in 1994 to provide toll free number portability using the Service Management System (SMS/800) database. Calls from Canada and the U.S., intrastate and interstate, could terminate at the same 1-800 number, even via different carriers. Vanity numbers became easier to obtain as a toll-free exchange prefix was no longer tied to a geographic location. By the 21st century, Voice over IP placed toll-free and foreign exchange numbers into the hands of even the smallest users, to whom dedicated inbound lines under the original InWATS model would have been prohibitively expensive.
WATS and the Civil Rights Movement.
During the Civil Rights Movement in the U.S., activist organizations such as SNCC used WATS as a convenient way for eyewitnesses on the ground to convey information quickly. Notes from these phone calls were compiled into "WATS Line Reports" and mailed to civil rights leaders, the media, the Justice Department, and others involved in the events. WATS was also how organizations communicated with local leaders across the country. A "Bay Area Friends of SNCC" newsletter in 1965 described WATS:
The WATS (Wide Area Telephone Service) line is the heart of all SNCC security and communications. For a flat monthly rate, an unlimited number of calls can be dialed directly to any place in the country — or the state — depending on what line one uses. The Jackson office has a state-wide line, the Atlanta office has the national WATS line. Both run on a 24-hour basis. A project worker can call in news of any incident, threat or major activity to the Jackson office. The WATS operator there takes down the details and relays it to Atlanta if the event is of national importance. In the case of a threat or incident involving Federal laws, Jackson will notify the FBI and the Justice Department. Atlanta uses its national WATS line to notify SNCC groups around the country.
See "External Links" for digitized WATS Line Reports. 

</doc>
<doc id="41868" url="https://en.wikipedia.org/wiki?curid=41868" title="Wideband modem">
Wideband modem

In telecommunication, the term wideband modem has the following meanings: 

</doc>
<doc id="41869" url="https://en.wikipedia.org/wiki?curid=41869" title="Wildcard character">
Wildcard character

The term wildcard character has several meanings.
Telecommunication.
In telecommunications, a wildcard is a character that may be substituted for any of a defined subset of all possible characters.
Computing.
In computer (software) technology, a wildcard character can be used to substitute for any other character or characters in a string.
File and directory patterns.
When specifying file names (or paths) in CP/M, DOS, Microsoft Windows, and Unix-like operating systems, the asterisk pattern character (, also called "star") matches zero or more characters.
In Unix-like and DOS operating systems, the question mark matches exactly one character; in DOS, if the question mark is placed at the end of the word, it will also match missing (zero) trailing characters. For example, in DOS, the pattern will match or , but not .
In Unix shells and Windows PowerShell, ranges of characters enclosed in square brackets ( and ) match a single character within the range; for example, matches any single uppercase or lowercase letter. Unix shells allow negation of the specified character set by using a leading (e.g., , which will match names like ). In shells that interpret as a history substitution, a leading can be used instead of to negate the character set.
The operation of matching of wildcard patterns to multiple file or path names is referred to as "globbing".
Databases.
In SQL, wildcard characters can be used in LIKE expressions; the percent sign matches zero or more characters, and underscore a single character. Transact-SQL also supports square brackets ( and ) to list sets and ranges of characters to match, a leading caret matches only a character not specified within the brackets. In Microsoft Access, wildcard characters can be used in LIKE expressions; the asterisk sign matches zero or more characters, the question mark matches a single character, the number sign matches a single digit (), and square brackets can be used to enclose sets or ranges of characters to match.
Regular expressions.
In regular expressions, the period (, also called "dot") is the wildcard pattern character that matches a single character. Combined with the asterisk operator it will match any number of characters.
In this case the asterisk is also known as the Kleene star.

</doc>
<doc id="41870" url="https://en.wikipedia.org/wiki?curid=41870" title="Wink pulsing">
Wink pulsing

Wink is used both in connection with DC signaling on a trunk, and with indicator lamps on a key telephone.
In telephone switching systems, wink pulsing is recurring pulsing in which the off-condition is relatively short compared to the on-condition. In Wink start trunks, the exchange at the originating end sends an off-hook to alert to a call. The terminating end indicates readiness to receive the dialed telephone number by sending an off-hook of approximately half a second duration, or "wink". Upon receiving this go ahead signal, the originating end uses multi-frequency or other address signalling to send the phone number.
On 1A2 key systems or similar key-operated telephone instruments, the hold position, "i.e.," the hold condition, of a line is often indicated by winking the associated lamp at 120 impulses per minute. During 6% of the pulse period the lamp is off and 94% of the period the lamp is on, "i.e.," 30 ms (milliseconds) off and 470 ms on.

</doc>
<doc id="41871" url="https://en.wikipedia.org/wiki?curid=41871" title="Wireless mobility management">
Wireless mobility management

Wireless mobility management: in Personal Communications Service (PCS), the assigning and controlling of wireless links for terminal network connections. Wireless mobility management provides an "alerting" function for call completion to a wireless terminal, monitors wireless link performance to determine when an automatic link transfer is required, and coordinates link transfers between wireless access interfaces.
One use of this is wireless push technology, by pushing data across wireless networks, this coordinates the link transfers and pushes data between the backend and wireless device only when an established connection is found.

</doc>
<doc id="41872" url="https://en.wikipedia.org/wiki?curid=41872" title="Work station">
Work station

A workstation or work station may refer to:

</doc>
<doc id="41873" url="https://en.wikipedia.org/wiki?curid=41873" title="X-dimension of recorded spot">
X-dimension of recorded spot

In fax systems, the X-dimension of recorded spot is the effective recorded spot dimension measured in the direction of the recorded line. By "effective recorded spot dimension" is meant the largest center-to-center spacing between recorded spots, which gives minimum peak-to-peak variation of density of the recorded line. "X-dimension of recorded spot" implies that the facsimile equipment response to a constant density in the object (original) is a succession of discrete recorded spots.

</doc>
<doc id="41877" url="https://en.wikipedia.org/wiki?curid=41877" title="Zero-dispersion wavelength">
Zero-dispersion wavelength

In a single-mode optical fiber, the zero-dispersion wavelength is the wavelength or wavelengths at which material dispersion and waveguide dispersion cancel one another. In all silica-based optical fibers, minimum material dispersion occurs naturally at a wavelength of approximately 1300 nm. Single-mode fibers may be made of silica-based glasses containing dopants that shift the material-dispersion wavelength, and thus, the zero-dispersion wavelength, toward the minimum-loss window at approximately 1550 nm. The engineering tradeoff is a slight increase in the minimum attenuation coefficient. Such fiber is called dispersion-shifted fiber.
Another way to alter the dispersion is changing the core size and the refractive indices of the material of core and cladding. Because fiber optic materials are already highly optimized for low scattering and high transparency alternative ways to change the refractive index were investigated. As a straight forward solution tapered fibers and holey fibers or photonic crystal fibers (PCF) were produced. Essentially they replace the cladding by air. This improves the contrast of refractive indices by a factor of 10. Therefore, the effective index is changed, especially for longer wavelengths. This type of refractive index change versus wavelength due to different geometry is called waveguide dispersion.
As these narrow waveguides (~1-3 µm core diameter) are combined with ultrashort pulses at the zero-dispersion wavelength pulses are not instantly destroyed by dispersion. After reaching a certain peak power within the pulse the non-linear refractive index starts to play an important role leading to frequency generation processes like self-phase modulation (SPM), modulational instability, soliton generation and soliton fission, cross phase modulation (XPM) and others. All these processes generate new frequency components, meaning that input light with narrow bandwidth expands into a wide range of new colours, through a process called supercontinuum generation.
The term is also used, more loosely, in multi-mode optical fiber. There, it refers to the wavelength at which the material dispersion is minimum, "i.e." "essentially" zero. This is more accurately called the minimum-dispersion wavelength.
Zero-dispersion slope.
The rate of change of dispersion with respect to wavelength at the zero-dispersion point is called the zero-dispersion slope. Doubly and quadruply clad single-mode fibers have two zero-dispersion points, and thus two zero-dispersion slopes.

</doc>
<doc id="41878" url="https://en.wikipedia.org/wiki?curid=41878" title="Zip-cord">
Zip-cord

Zip-cord is a type of electrical cable with two or more conductors held together by an insulating jacket that can be easily separated simply by pulling apart. The term is also used with optical fiber cables consisting of two optical fibers joined in a similar manner. The design of zip-cord makes it easy to keep conductors that carry related electrical or optical signals together and helps avoid tangling of cables. Typical uses include lamp cord and speaker wire. Conductors may be identified by a color tracer on the insulation, or by a ridge molded into the insulation of one wire, or by a colored tracer thread inside the insulation. Zip cords are intended for use on portable equipment, and the US and Canadian electrical codes do not permit their use for permanently installed wiring of line-voltage circuits. 

</doc>
<doc id="41879" url="https://en.wikipedia.org/wiki?curid=41879" title="Absolute pitch">
Absolute pitch

Absolute pitch (AP), widely referred to as perfect pitch, is a rare auditory phenomenon characterized by the ability of a person to identify or re-create a given musical note without the benefit of a reference tone.
AP can be demonstrated via linguistic labeling ("naming" a note), auditory imagery, or sensorimotor responses. For example, an AP possessor can accurately reproduce a heard tone on their musical instrument without "hunting" for the correct pitch. Researchers estimate the occurrence of AP to be 1 in 10,000 people.
Generally, absolute pitch implies some or all of the following abilities, achieved without a reference tone:
People may have absolute pitch along with the ability of relative pitch, and relative and absolute pitch work together in actual musical listening and practice, but strategies in using each skill vary. Those with absolute pitch may train their relative pitch, but there are no reported cases of an adult obtaining absolute pitch ability through musical training; adults who possess relative pitch but do not already have absolute pitch can learn "pseudo-absolute pitch" and become able to identify notes in a way that superficially resembles absolute pitch. Moreover, training pseudo-absolute pitch requires considerable motivation, time, and effort, and learning is not retained without constant practice and reinforcement.
Scientific studies.
History of study and terminologies.
Scientific study of absolute pitch appears to have commenced in the 19th century, focusing on the phenomenon of musical pitch and methods of measuring it. It would have been difficult for any notion of absolute pitch to have formed earlier, because of the fluidity of pitch references. For example, the note now known as 'A' varied in different local or national musical traditions between what would now be considered as G sharp and B flat before the standardisation of the late 19th century. While the term "absolute pitch", or "absolute ear", was in use by the late 19th century by both British and German researchers, its application was not universal; other terms such as "musical ear", "absolute tone consciousness", or "positive pitch" were also used to refer to the ability. The skill is not exclusively musical, or limited to human perception; absolute pitch has been demonstrated in animals such as bats, wolves, gerbils, and birds, for whom specific pitches facilitate identification of mates or meals.
Difference in cognition, not elementary sensation.
Physically and functionally, the auditory system of an absolute listener does not appear to be different from a non-absolute listener. Rather, "it reflects a particular ability to analyze frequency information, presumably involving high-level cortical processing." Absolute pitch is an act of cognition, needing memory of the frequency, a label for the frequency (such as "B-flat"), and exposure to the range of sound encompassed by that categorical label. Absolute pitch may be directly analogous to recognizing colors, phonemes (speech sounds) or other categorical perception of sensory stimuli. Just as most people have learned to recognize and name the color "blue" by the frequencies of the electromagnetic radiation that is perceived as light, it is possible that those who have been exposed to musical notes together with their names early in life will be more likely to identify, for example, the note C. Absolute pitch may also be related to certain genes, possibly an autosomal dominant genetic trait, though it "might be nothing more than a general human capacity whose expression is strongly biased by the level and type of exposure to music that people experience in a given culture."
Influence by music experience.
Absolute pitch sense appears to be influenced by cultural exposure to music, especially in the familiarization of the equal-tempered C-major scale. Most of the absolute listeners that were tested in this respect identified the C-major tones more reliably and, except for B, more quickly than the five "black key" tones, which corresponds to the higher prevalence of these tones in ordinary musical experience. One study of Dutch non-musicians also demonstrated a bias toward using C-major tones in ordinary speech, especially on syllables related to emphasis.
Linguistics.
Absolute pitch is more common among speakers of tonal languages such as most dialects of Chinese or Vietnamese, which often depend on pitch variation as the means of distinguishing words that otherwise sound the same; e.g. Mandarin with four possible pitch variations, Cantonese with six, Southern Min with seven or eight (depending on dialect), and Vietnamese with six. Speakers of Sino-Tibetan languages have been reported to speak a word in the same absolute pitch (within a quarter-tone) on different days; it has therefore been suggested that absolute pitch may be acquired by infants when they learn to speak a tonal language (and possibly also by infants when they learn to speak a pitch-accent language). However, the brains of tonal-language speakers do not naturally process musical sound as language; perhaps such speakers are more likely to acquire absolute pitch for musical tones when they later receive musical training. Many native speakers of a tone language, even those with little musical training, are observed to sing a given song consistently with regard to pitch. Among music students of East Asian ethnic heritage, those who speak a tone language very fluently have a much higher prevalence of absolute pitch than those who do not speak a tone language.
It is possible that African level-tone languages—such as Yoruba, with three pitch levels, and Mambila, with four—may be better suited to study the role of absolute pitch in speech than the pitch and contour tone languages of East Asia.
Speakers of European languages have been found to make subconscious use of an absolute pitch memory when speaking.
Perception.
Absolute pitch is the ability to perceive pitch class and to mentally categorize sounds according to perceived pitch class. "Pitch class" is a tonal quality which recurs among tones which share the relationship of an octave. While the boundaries of musical pitch categories vary among human cultures, the recognition of octave relationships is a natural characteristic of the mammalian auditory system. Accordingly, absolute pitch is not the ability to estimate a pitch value from the dimension of pitch evoking frequency (30–5000 Hz), but to identify a pitch class category within the dimension of pitch class (e.g., C-C-D ... B-C).
An absolute listener's sense of hearing is typically no keener than that of a non-absolute ("normal") listener. Absolute pitch does not depend upon a refined ability to perceive and discriminate gradations of sound frequencies, but upon detecting and categorizing a subjective perceptual quality typically referred to as "chroma". The two tasks— of identification (recognizing and naming a pitch) and discrimination (detecting changes or differences in rate of vibration)— are accomplished with different brain mechanisms.
Special populations.
The prevalence of absolute pitch is higher among those who are blind from birth as a result of optic nerve hypoplasia.
Absolute pitch is considerably more common among those whose early childhood was spent in East Asia. This might seem to be a genetic difference; but people of East Asian ancestry who are reared in North America are significantly less likely to develop absolute pitch than those raised in East Asia, so the difference is more probably explained by experience. The language that is spoken may be an important factor; many East Asians speak tonal languages such as Mandarin and Cantonese, while others (such as those in Japan and certain provinces of Korea) speak pitch-accent languages, and the prevalence of absolute pitch may be partly explained by exposure to pitches together with meaningful musical labels very early in life.
Absolute pitch ability has higher prevalence among those with Williams Syndrome and those with an autism spectrum disorder, with rates as high as 30% claimed, stating that the rate among musicians in general is far lower. A non-verbal piano-matching method resulted in a correlation of 97% between autism and absolute pitch, with a 53% correlation in non-autistic observers.
Nature vs. nurture.
Absolute pitch might be achievable by any human being during a critical period of auditory development, after which period cognitive strategies favor global and relational processing. Proponents of the critical-period theory agree that the presence of absolute pitch ability is dependent on learning, but there is disagreement about whether training causes absolute skills to occur or lack of training causes absolute perception to be overwhelmed and obliterated by relative perception of musical intervals.
There may be a genetic locus for absolute pitch ability, which locus would suggest a genetic basis for its presence or absence. A genetic basis, should it exist, might represent either a predisposition for learning the ability or signal the likelihood of its spontaneous occurrence.
Researchers have been trying to teach absolute pitch ability in laboratory settings for more than a century, and various commercial absolute-pitch training courses have been offered to the public since the early 1900s. However, no adult has ever been documented to have acquired absolute listening ability, because all adults who have been formally tested after AP training have failed to demonstrate "an unqualified level of accuracy... comparable to that of AP possessors".
Pitch memory related to musical context.
While very few people have the ability to name a pitch with no external reference, pitch memory can be activated by repeated exposure. People who are not skilled singers will often sing popular songs in the correct key, and can usually recognize when TV themes have been shifted into the wrong key. Members of the Venda culture in South Africa also sing familiar children's songs in the key in which the songs were learned.
This phenomenon is apparently unrelated to musical training. The skill may be associated more closely with vocal production. Violin students learning the Suzuki method are required to memorize each composition in a fixed key and play it from memory on their instrument, but they are not required to sing. When tested, these students did not succeed in singing the memorized Suzuki songs in the original, fixed key.
Possible problems.
Musicians with absolute perception may experience difficulties which do not exist for other musicians. Because absolute listeners are capable of recognizing that a musical composition has been transposed from its original key, or that a pitch is being produced at a nonstandard frequency (either sharp or flat), a musician with absolute pitch may become distressed upon perceiving tones they believe to be "wrong" or hearing a piece of music "in the wrong key." This can especially apply to Baroque music that is recorded in Baroque tuning (usually A = 415 Hz as opposed to 440 Hz, "i.e.", roughly a half step or semitone lower than standard concert pitch).
An absolute listener may also use absolute strategies for tasks which are more efficiently accomplished with relative strategies, such as transposition or producing harmony for tones whose frequencies do not match standard equal temperament. It is also possible for some musicians to have displaced absolute pitch, where all notes are slightly flat or slightly sharp of their respective pitch as defined by a given convention. This may arise from learning the pitch names from an instrument that was tuned to a concert pitch convention other than the one in use, "e.g.", A = 435 Hz (the Paris Opera convention of the late 19th and early 20th centuries) as opposed to the Anglo-American modern standard A = 440 Hz. When playing in groups with other musicians, this may lead to playing in a tonality that is slightly different from that of the rest of the group.
Synesthesia.
Absolute pitch shows a genetic overlap with synesthesia/ideasthesia, and some individuals with music-related synesthesia also have perfect pitch. They may associate certain notes or keys with different colors, enabling them to tell what any note or key is. It is unknown how many people with perfect pitch are also synesthetes.
Correlation with musical talent.
Absolute pitch is not a prerequisite for skilled musical performance or composition. However, musicians with absolute pitch tend to perform better on musical transcription tasks (controlling for age of onset and amount of musical training) compared with those without absolute pitch. It has been argued that musicians with absolute pitch perform worse than those without absolute pitch on recognition of musical intervals. However, experiments on which this conclusion was based contained an artifact, and when this artifact was removed, absolute pitch possessors were found to perform better than nonpossessors on recognition of musical intervals.
Owing to uncertainty in the historical record, it is often impossible to determine whether notable composers and musicians of the past had absolute pitch. Since absolute pitch is rare in European musical culture, claims that any particular musician possessed it are difficult to evaluate. Among composers of the Baroque and Classical eras, evidence is available only for Mozart, who is documented to have demonstrated the ability at age 7. Experts have only surmised that Beethoven had it, as indicated from some excerpts from his letters. By the 19th century, it became more common for the presence of absolute pitch to be recorded, identifying the ability to be present in musicians such as Camille Saint-Saëns and John Philip Sousa.
Relative pitch.
Many musicians have quite good relative pitch, a skill which can be learned through ear training. With practice, it is possible to listen to a single known pitch once (from a pitch pipe or a tuning fork) and then have stable, reliable pitch identification by comparing the notes heard to the stored memory of the tonic pitch. Unlike absolute pitch, this skill is dependent on a recently perceived tonal center.

</doc>
<doc id="41881" url="https://en.wikipedia.org/wiki?curid=41881" title="All About Eve">
All About Eve

All About Eve is a 1950 American drama film written and directed by Joseph L. Mankiewicz, and produced by Darryl F. Zanuck. It was based on the 1946 short story "The Wisdom of Eve" by Mary Orr, although screen credit was not given for it.
The film stars Bette Davis as Margo Channing, a highly regarded but aging Broadway star. Anne Baxter plays Eve Harrington, an ambitious young fan who insinuates herself into Channing's life, ultimately threatening Channing's career and her personal relationships. George Sanders, Celeste Holm, Hugh Marlowe, Barbara Bates, Gary Merrill, and Thelma Ritter also appear, and the film provided one of Marilyn Monroe's earliest important roles.
Praised by critics at the time of its release, "All About Eve" was nominated for 14 Academy Awards (a feat unmatched until the 1997 film "Titanic") and won six, including Best Picture. "All About Eve" is the only film in Oscar history to receive four female acting nominations (Davis and Baxter as Best Actress, Holm and Ritter as Best Supporting Actress). "All About Eve" was selected in 1990 for preservation in the United States National Film Registry and was among the first 50 films to be registered. "All About Eve" appeared at #16 on AFI's 1998 list of the 100 best American films.
Plot.
At an awards dinner, Eve Harrington (Anne Baxter)—the newest and brightest star on Broadway—is being presented the Sarah Siddons Award for her breakout performance as Cora in "Footsteps on the Ceiling". Theatre critic Addison DeWitt (George Sanders) observes the proceedings and, in a sardonic voiceover, recalls how Eve's star rose as quickly as it did.
The film flashes back a year. Margo Channing (Bette Davis) is one of the biggest stars on Broadway, but despite her success she is bemoaning her age, having just turned forty and knowing what that will mean for her career. After a performance one night, Margo's close friend Karen Richards (Celeste Holm), wife of the play's author Lloyd Richards (Hugh Marlowe), meets besotted fan Eve Harrington in the cold alley outside the stage door. Recognizing her from having passed her many times in the alley (as Eve claims to have seen every performance of Margo's current play, "Aged in Wood"), Karen takes her backstage to meet Margo. Eve tells the group gathered in Margo's dressing room—Karen and Lloyd, Margo's boyfriend Bill Sampson (Gary Merrill), a director who is eight years her junior, and Margo's maid Birdie (Thelma Ritter)—that she followed Margo's last theatrical tour to New York after seeing her in a play in San Francisco. She tells a moving story of growing up poor and losing her young husband in the recent war. Moved, Margo quickly befriends Eve, takes her into her home, and hires her as her assistant, leaving Birdie, who instinctively dislikes Eve, feeling put out.
Eve is gradually shown to be working to supplant Margo, scheming to become her understudy behind her back, driving wedges between her and Lloyd and Bill, and conspiring with an unsuspecting Karen to cause Margo to miss a performance. Eve, knowing in advance that she will be the one appearing that night, invites the city's theatre critics to attend that evening's performance, which is a triumph for her. Eve tries to seduce Bill, but he rejects her. Following a scathing newspaper column by Addison, Margo and Bill reconcile, dine with the Richardses, and decide to marry. That same night at the restaurant, Eve blackmails Karen into telling Lloyd to give her the part of Cora, by threatening to tell Margo of Karen's role in Margo's missed performance. Before Karen can talk with Lloyd, Margo announces to everyone's surprise that she does not wish to play Cora and would prefer to continue in "Aged in Wood". Eve secures the role and attempts to climb higher by using Addison, who is beginning to doubt her. Just before the premiere of her play at the Shubert in New Haven, Eve presents Addison with her next plan: to marry Lloyd, who, she claims, has come to her professing his love and his eagerness to leave his wife for her. Now, Eve exults, Lloyd will write brilliant plays showcasing "her". Unseen but mentioned in dialogue, Karen has begun to suspect Eve as a threat to her own marriage to Lloyd, and so she and Addison meet for lunch and help each other put the pieces about Eve together. Addison is infuriated that Eve has attempted to use him and reveals that he knows that her back story is all lies. Her real name is Gertrude Slojinski, she was never married, and she had been paid to leave her hometown over an affair with her boss, a brewer in Wisconsin. Addison blackmails Eve, informing her that she will not be marrying Lloyd or anyone else; in exchange for Addison's silence, she now "belongs" to him.
The film returns to the opening scene in which Eve, now a shining Broadway star headed for Hollywood, is presented with her award. In her speech, she thanks Margo, Bill, Lloyd and Karen with characteristic effusion, while all four stare back at her coldly. After the awards ceremony, Eve hands her award to Addison, skips a party in her honor, and returns home alone, where she encounters a young fan (Barbara Bates)—a high-school girl—who has slipped into her apartment and fallen asleep. The young girl professes her adoration and begins at once to insinuate herself into Eve's life, offering to pack Eve's trunk for Hollywood and being accepted. "Phoebe", as she calls herself, answers the door to find Addison returning with Eve's award. In a revealing moment, the young girl flirts daringly with the older man. Addison hands over the award to Phoebe and leaves without entering. Phoebe then lies to Eve, telling her it was only a cab driver who dropped off the award. While Eve rests in the other room, Phoebe dons Eve's elegant costume robe and poses in front of a multi-paned mirror, holding the award as if it were a crown. The mirrors transform Phoebe into multiple images of herself, and she bows regally, as if accepting the award to thunderous applause, while triumphant music plays.
Production.
Development.
The story of "All About Eve" originated in an anecdote related to Mary Orr by actress Elisabeth Bergner. While performing in "The Two Mrs. Carrolls" during 1943 and 1944, Bergner allowed a young fan to become part of her household and employed her as an assistant, but later regretted her generosity when the woman attempted to undermine her. Referring to her only as "the terrible girl", Bergner related the events to Orr, who used it as the basis for her short story "The Wisdom of Eve" (1946). In the story, Orr gives the girl a more ruthless character and allows her to succeed in stealing the older actress' career. Bergner later confirmed the basis of the story in her autobiography "Bewundert viel, und viel gescholten" ("Greatly Admired and Greatly Scolded").
In 1949, Mankiewicz was considering a story about an aging actress and, upon reading "The Wisdom of Eve", felt the conniving girl would be a useful added element. He sent a memo to Darryl F. Zanuck saying it "fits in with an original idea mine and can be combined. Superb starring role for Susan Hayward." Mankiewicz presented a film treatment of the combined stories under the title "Best Performance". He changed the main character's name from Margola Cranston to Margo Channing and retained several of Orr's characters — Eve Harrington, Lloyd and Karen Richards, and Miss Casswell — while removing Margo Channing's husband completely and replacing him with a new character, Bill Sampson. The intention was to depict Channing in a new relationship and allow Eve Harrington to threaten both Channing's professional and personal lives. Mankiewicz also added the characters Addison DeWitt, Birdie Coonan, Max Fabian, and Phoebe.
Zanuck was enthusiastic and provided numerous suggestions for improving the screenplay. In some sections, he felt Mankiewicz's writing lacked subtlety or provided excessive detail. He suggested diluting Birdie Coonan's jealousy of Eve so the audience would not recognize Eve as a villain until much later in the story. Zanuck reduced the screenplay by about 50 pages and chose the title "All About Eve" from the opening scenes in which Addison DeWitt says he will soon tell "more of Eve ... All about Eve, in fact."</ref>
Casting.
Among the actresses originally considered to play Margo Channing were Mankiewicz's original inspiration, Susan Hayward, who was rejected by Zanuck as "too young", Marlene Dietrich, dismissed as "too German", and Gertrude Lawrence, who was ruled out of contention when her lawyer insisted that Lawrence not have to drink or smoke in the film, and that the script would be rewritten to allow her to sing a torch song. Zanuck favored Barbara Stanwyck, but she was not available. Tallulah Bankhead and Ingrid Bergman were also considered, as was Joan Crawford, who was already working on the film "The Damned Don't Cry".
Eventually, the role went to Claudette Colbert, but when Colbert severely injured her back and was forced to withdraw shortly before filming began, Bette Davis was chosen to replace her. Davis, who had recently ended an 18-year association with Warner Bros. after several poorly received films, immediately accepted the role after realizing it was one of the best she had ever read. Channing had originally been conceived as genteel and knowingly humorous, but with the casting of Davis, Mankiewicz revised the character to be more abrasive. Mankiewicz praised Davis for both her professionalism and the calibre of her performance, but in later years continued to discuss how Colbert would have played the role.
Anne Baxter had spent a decade in supporting roles and had won the 1946 Academy Award for Best Supporting Actress for "The Razor's Edge". She got the role of Eve Harrington after the first choice, Jeanne Crain, became pregnant. Crain was at the height of her popularity and had established a career playing likable heroines; Zanuck believed she lacked the "bitch virtuosity" required by the part, and audiences would not accept her as a deceitful character.
The role of Bill Sampson was originally intended for John Garfield or Ronald Reagan. Reagan's future wife Nancy Davis was considered for Karen Richards and Jose Ferrer for Addison DeWitt. Zsa Zsa Gabor actively sought the role of Phoebe without realizing the producers were considering her, along with Angela Lansbury, for Miss Casswell.
Mankiewicz greatly admired Thelma Ritter and wrote the character of Birdie Coonan for her after working with her on "A Letter to Three Wives" in 1949. As Coonan was the only one immediately suspicious of Eve Harrington, he was confident Ritter would contribute a shrewd characterisation casting doubt on Eve and providing a counterpoint to the more "theatrical" personalities of the other characters. Marilyn Monroe, relatively unknown at the time, was cast as Miss Casswell, referred to by DeWitt as a "graduate of the Copacabana School of Dramatic Art". Monroe got the part after a lobbying campaign by her agent, despite Zanuck's initial antipathy and belief she was better suited to comedy. Angela Lansbury had been originally considered for the role. The inexperienced Monroe was cowed by Bette Davis, and took 11 takes to complete the scene in the theatre lobby with the star; when Davis barked at her, Monroe left the set to vomit. Smaller roles were filled by Gregory Ratoff as the producer Max Fabian, Barbara Bates as Phoebe, a young fan of Eve Harrington, and Walter Hampden as the master of ceremonies at an award presentation.
Response.
Critical reaction.
"All About Eve" received overwhelmingly positive reviews from critics upon its release on October 13, 1950 at a New York City premiere. The film's competitor, "Sunset Boulevard," released the same year, drew similar praise, and the two were often favorably compared. Film critic Bosley Crowther of "The New York Times" loved the film, stating it was "a fine Darryl Zanuck production, excellent music and on air ultra-class complete the superior satire".
Film critic Roger Ebert of the "Chicago Sun Times" praised the film, saying Bette Davis' character "veteran actress Margo Channing in "All About Eve" was her greatest role". A collection of reviews from the film's release are stored on the website Rottentomatoes.com, and "All About Eve" has garnered 100% positive reviews there, making it "Certified fresh". Boxoffice.com stated that it "is a classic of the American cinema – to this day the quintessential depiction of ruthless ambition in the entertainment industry, with legendary performances from Bette Davis, Anne Baxter and George Sanders anchoring one of the very best films from one of Hollywood's very best Golden Era filmmakers: Joseph L. Mankiewicz. It is a film that belongs on every collector's shelf – whether on video or DVD. It is a classic that deserves better than what Fox has given it."
Thematic content.
Critics and academics have delineated various themes in the film. Rebecca Flint Marx, in her "Allmovie" review, notes the antagonism that existed between Broadway and Hollywood at the time, stating that the "script summoned into existence a whole array of painfully recognizable theatre types, from the aging, egomaniacal grand dame to the outwardly docile, inwardly scheming ingenue to the powerful critic who reeks of malignant charm." Roger Ebert, in his review in "The Great Movies", says Eve Harrington is "a universal type", and focuses on the aging actress plot line, comparing the film to "Sunset Boulevard". Similarly, Marc Lee's 2006 review of the film for "The Daily Telegraph" describes a subtext "into the darker corners of show business, exposing its inherent ageism, especially when it comes to female stars." Kathleen Woodward's 1999 book, "Figuring Age: Women, Bodies, Generations (Theories of Contemporary Culture)", also discusses themes that appeared in many of the "aging actress" films of the 1950s and 1960s, including "All About Eve". She reasons that Margo has three options: "To continue to work, she can perform the role of a young woman, one she no longer seems that interested in. She can take up the position of the angry bitch, the drama queen who holds court (the deliberate camp that Sontag finds in this film). Or she can accept her culture's gendered discourse of aging which figures her as in her moment of fading. Margo ultimately chooses the latter option, accepting her position as one of loss."
Professor Robert J. Corber, who has studied homophobia within the cultural context of the Cold War in the United States, posits that the foundational theme in "All About Eve" is that the defense of the norms of heterosexuality, specifically in terms of patriarchal marriage, must be upheld in the face of challenges from female agency and homosexuality. The nurturing heterosexual relationships of Margo and Bill and of Karen and Lloyd serve to contrast with the loveless relationship predation and sterile careerism of the homosexual characters, Eve and Addison. Eve uses her physical femininity as a weapon to try to break up the marriages of both couples, and Addison's extreme cynicism serves as a model of Eve's future. Even film reviewer Kenneth Geist, despite being critical of the emphasis that Sam Staggs' book "All About All About Eve" places on the film's homosexual elements, nonetheless acknowledged that Eve's lesbianism seemed apparent; specifically, Geist states that "manifestations of Eve’s lesbianism are only twice briefly discernible". Geist asserted that Mankiewicz "was highly contemptuous of both male and female homosexuals", although Mankiewicz himself suggested otherwise in an interview in which he argued that society should "drop its vendetta against them".
Homosexuality was often linked to Communism during the Cold War's Lavender Scare and critics have written about film's subtle, yet central, Cold War narrative. The fair amount of subtlety employed in "All About Eve" is seen as primarily being due to Production Code restrictions on the depiction of homosexuals in the media during this time. However, notwithstanding those restrictions, Corber cites the film as but one example of a recurrent theme within American film of the homosexual as an emotionally bereft predator. The documentary "The Celluloid Closet" also affirms this theme to which Corber refers, including citing numerous other film examples from the same Production Code time period in which "All About Eve" was made.
Another important theme of the film, in terms of war politics and sexuality, involves the post-World War II pressure placed upon women to acquiesce agency. This pressure to resume "traditional" female roles is especially illustrated in this film in the contrast between Margo's mockery of Karen Richards for being a "happy little housewife" and her lengthy and inspired monologue, as a reformed woman later, about the virtuousness of marriage, including how a woman is not truly a woman without having a man beside her. This submissive and effeminate Margo is contrasted with the theatricality, combativeness, and egotism of the earlier career woman Margo, and the film's two homosexual characters. Margo quips that Eve should place her award "where her heart should be", and Eve is shown bereft at the end of the film. At dinner, the two married couples see Eve and Addison in a similarly negative light, with Margo wondering aloud what schemes Eve was constructing in her "feverish little brain". Additionally, Eve's utility as a personal assistant to Margo early in the film, which is a subtle construct of a same-sex intimate relationship, is decried by Birdie, the same working-class character who immediately detected the theatricality in Eve's story about her "husband". Birdie sees such agency as being unnatural, and the film contrasts its predatory nature ("studying you like a blueprint") with the love and warmth of her later reliance upon Bill. The pressure to acquiesce agency and more highly value patriarchy, following the return of men from the war, after having been shown propaganda promoting agency such as Rosie the Riveter and after having occupied traditionally male roles such as bomb-building factory worker, was deemed "the problem that has no name" by well-known feminist Betty Friedan.
Despite what critics such as Corber have described as the homophobia pervasive in the movie, "All About Eve" has long been a favored film among gay audiences, likely due to its campy overtones (in part due to the casting of Davis) and its general sophistication. Davis, who long had a strong gay fan base, expressed support for gay men in her 1972 interview with "The Advocate".
Awards and honors.
Later recognition and rankings.
In 1990, "All About Eve" was selected for preservation in the United States National Film Registry by the Library of Congress as being "culturally, historically, or aesthetically significant." The film received in 1997 a placement on the Producers Guild of America Hall of Fame. The film also earns a 100% rating on Rotten Tomatoes. The film has been selected by the American Film Institute for many of their 100 Years lists.
When AFI named Bette Davis # 2 on its list of the greatest female American screen legends, "All About Eve" was the film selected to highlight Davis' legendary career.
Sarah Siddons Award.
The film opens with the image of a fictitious award trophy, described by DeWitt as the "highest honor our theater knows: the Sarah Siddons Award for Distinguished Achievement." The statuette is modelled after the famous painting of Siddons costumed as the tragic Muse by Joshua Reynolds, a copy of which hangs in the entrance of Margo's apartment and often visible during the party scene. In 1952, a small group of distinguished Chicago theater-goers began to give an award with that name, which was sculpted to look like the one used in the film. It has been given annually, with past honorees including Bette Davis and Celeste Holm.
Adaptations.
The first radio adaptation was broadcast on the Lux Radio Theatre on NBC on October 1, 1951 starring Bette Davis, Gary Merrill and Anne Baxter.
A second radio version of "All About Eve" starring Tallulah Bankhead as Margo Channing was presented on NBC's "The Big Show" by the Theatre Guild of the Air on November 16, 1952. Bankhead and many contemporary critics felt that the characterization of Margo Channing was patterned on her, a long-rumored charge denied by both Mankiewicz and Davis, but attested by costume designer Edith Head. Additionally, Bankhead's rivalry with her understudy (Lizabeth Scott) during the production of "The Skin of Our Teeth" is cited as an alternative hypothesis for the origin of Mary Orr's "The Wisdom of Eve", the original short story that formed the basis for the film. Ironically, Bette Davis played three roles on film that had been originated by Tallulah Bankhead — "Dark Victory", "Jezebel" and "The Little Foxes", much to Bankhead's chagrin. Bankhead and Davis were considered to be somewhat similar in style. Ironically, several decades later Davis would call Channing "the essence of a Tallulah Bankhead kind of actress" in an interview with Barbara Walters. The production is notable in that Mary Orr, of "The Wisdom of Eve", played the role of Karen Richards. The cast also featured Alan Hewitt as Addison DeWitt (who narrated), Beatrice Pearson as Eve Harrington, Don Briggs as Lloyd Richards, Kevin McCarthy as Bill Samson, Florence Robinson as Birdie Coonan, and Stefan Schnabel as Max Fabian.
In 1970, "All About Eve" was the inspiration for the stage musical "Applause," with book by Betty Comden and Adolph Green, lyrics by Lee Adams, and music by Charles Strouse. The original production starred Lauren Bacall as Margo Channing, and it won the Tony Award for Best Musical that season. It ran for four previews and 896 performances at the Palace Theatre on Broadway. After Bacall left the production, she was replaced by Anne Baxter in the role of Margo Channing.

</doc>
<doc id="41882" url="https://en.wikipedia.org/wiki?curid=41882" title="MIL-STD-188">
MIL-STD-188

MIL-STD-188 is a series of U.S. military standards relating to telecommunications.
Purpose.
Faced with "past technical deficiencies in telecommunications systems and equipment and software…that were traced to basic inadequacies in the application of telecommunication standards and to the lack of a well defined…program for their review, control and implementation", the U.S. Department of Defense looked to develop a series of standards that would alleviate the problem.
By 1988, the U.S. Department of Defense (DoD) issued Instruction 4630.8 (reissued in 1992, 2002, 2004) stating its policy that "all forces for joint and combined operations be supported through compatible, interoperable, and integrated Command, Control, Communications, and Intelligence systems. …that all such systems developed for use by U.S. forces are considered to be for joint use." To achieve this the director of the Defense Information Systems Agency (DISA) is charged with "developing information technology standards to achieve interoperability and compatibility…ensure that all systems and equipment shall conform to technical and procedural standards for interface, interoperability, and compatibility".
The MIL-STD-188 standards were created to "address telecommunication design parameters based on proven technologies." To ensure interoperability, DISA made these standards mandatory for use in all new DoD systems and equipment, or major upgrades.
The mandatory use of these standards will aid significantly in achieving standardization and result in improvements in availability, maintainability, reliability, and supportability. This, in turn, will enhance lifecycle configuration management and logistic support with subsequent reductions in life cycle costs.
Evolution.
When first developed Military Standard 188 (MIL-STD-188) covered technical standards for tactical and long-haul communications, but as it was revised (MIL-STD-188A, MIL-STD-188B) it became a document applicable to tactical communications only (MIL-STD-188C 24 Nov 1969). The Defense Information Systems Agency published circulars which announced both standards and engineering criteria relating to the long-haul Defense Communications System and to the technical support of the Worldwide Military Command and Control System. In line with a decision by the Joint Chiefs of Staff, these standards are published in the MIL-STD-188 series of documents. This series is subdivided into "a MIL-STD-188-100 series covering common standards for tactical and long-haul communications, a MIL-STD-188-200 series covering standards for tactical communications only, and a MIL-STD-188-300 series covering standards for long-haul communications only."
The MIL-STD-188 series standards are encompassed by the DoD’s Joint Technical Architecture.
Deviations and waivers.
For any manufacturer seeking to deviate from the MIL–STD-188 series standards (prior to the manufacture of an item) they must request to do so with the Joint Steering Committee (JSC) which is constituted under the Defense Communications Agency. For any DoD Agency to get a waiver to receive an item that deviates from the standards they also must apply to the JSC.
Relation to other systems of standards.
According to DoD documents, "The MIL-STD-188 series may be based on, or make reference to, Joint Technical Architecture, American National Standards Institute (ANSI) standards, International Telecommunications Union - Telecommunication Standardization Sector (ITU-T) recommendations, North Atlantic Treaty Organization (NATO) Standardization Agreements (STANAG), and other standards wherever applicable."
Current development emphasis.
Currently the DoD is placing its emphasis "on the development of common standards for tactical and long-haul communications (the MIL-STD-188-100 series)."
Documents.
Note: The following list of documents are those that are presently active. Documents with three digit numbers followed by a letter of the alphabet indicate that they are revisions of an older version of that document.
MIL-STD-188-100 series.
According to the DoD the MIL-STD-188-100 series contains "technical standards and design objectives which are common to both the long haul and tactical communications systems."
The current articles in this series include:
MIL-STD-188-200 series.
According to the DoD the MIL-STD-188-200 series "contains current tactical communications, technical standards and design objectives…series includes appropriate unclassified design objectives and tactical communications systems technical standards…Appropriate communications-electronics systems standards and design objectives developed under joint projects…[which are integrated in the tactical communications standards."
The current articles in this segment include:
MIL-STD-188-300 series.
According to the DoD the MIL-STD-188–300 Series contains "communications system standards and design objectives applicable to the field of long haul and point-to–point communications in support of the Defense Communications System (DCS) and the National Military Command System (NMCS), and also to provide the necessary interface with non-DCS equipment."
The current articles in this series include:

</doc>
<doc id="41885" url="https://en.wikipedia.org/wiki?curid=41885" title="Weser">
Weser

The Weser () is a river in Northwestern Germany. Formed at Hannoversch Münden by the confluence of the rivers Fulda and Werra, it flows through Lower Saxony, then reaching the Hanseatic-town Bremen (see: Hanseatic League), before emptying further north at Bremerhaven into the North Sea. On the opposite (west) bank is the town of Nordenham at the foot of the Butjadingen Peninsula; thus, the mouth of the river is in Lower Saxony. The Weser has an overall length of . Together with its Werra tributary, which originates in Thuringia, its length is .
Etymology.
Linguistically, the name of both rivers, Weser and Werra, goes back to the same source, the differentiation being caused by the old linguistic border between Upper and Lower German, which touched the region of Hannoversch Münden.
The name "Weser" parallels the names of other rivers, such as the "Wear" in England and the "Vistula" in Poland, all of which are ultimately derived from the root *"weis-" "to flow", which gave Old English/Old Frisian "wāse" "mud, ooze", Old Norse "veisa" "slime, stagnant pool", Dutch "waas" "lawn", Old Saxon "waso" "wet ground, mire", and Old High German "wasal" "rain".
Course.
The Weser river is the longest river whose course reaches the sea and lies entirely within German national territory.
The upper part of its course leads through a hilly region called the Weserbergland. It extends from the confluence of the Fulda and the Werra to the Porta Westfalica, where it runs through a gorge between two mountain chains, the Wiehengebirge in the west and the Weserbergland in the east.
Between Minden and the North Sea, humans have largely canalised the river, permitting ships of up to 1,200 tons to navigate it. Eight hydroelectric dams stand along its length. It is linked to the Dortmund-Ems Canal via the Coastal Canal, and another canal links it at Bremerhaven to the Elbe River. A large reservoir on the Eder river, the main tributary of the Fulda, is used to regulate water levels on the Weser so as to ensure adequate depth for shipping throughout the year. The dam, built in 1914, was bombed and destroyed by British aircraft in May 1943, causing massive destruction and approximately 70 deaths downstream, but was rebuilt within four months. the Edersee reservoir, a major summer resort area, provides substantial hydroelectricity.
The Weser enters the North Sea in the southernmost part of the German Bight. In the North Sea, it splits up into two arms representing the ancient riverbed at the end of the last ice age. These sea-arms are called "Alte Weser" (old Weser) and "Neue Weser" (new Weser). They represent the major waterways for ships heading for the harbors of Bremerhaven, Nordenham and Bremen. The "Alte Weser" lighthouse marks the northernmost point of the Weser. This lighthouse replaced the historic and famous "Roter Sand" lighthouse in 1964.
Tributaries.
The largest tributary of the Weser is the Aller, which joins south of Bremen. The tributaries of the Weser and the Werra (from source to mouth) are:
Notable towns.
Towns along the Weser, from the confluence of Werra and Fulda to the mouth, include: Hann. Münden, Beverungen, Höxter, Holzminden, Bodenwerder, Hameln, Hessisch Oldendorf, Rinteln, Vlotho, Bad Oeynhausen, Porta Westfalica, Minden, Petershagen, Nienburg, Achim, Bremen, Brake, Nordenham, Bremerhaven.

</doc>
<doc id="41887" url="https://en.wikipedia.org/wiki?curid=41887" title="Felix Klein">
Felix Klein

Christian Felix Klein (25 April 1849 – 22 June 1925) was a German mathematician and mathematics educator, known for his work in group theory, complex analysis, non-Euclidean geometry, and on the connections between geometry and group theory. His 1872 Erlangen Program, classifying geometries by their underlying symmetry groups, was a hugely influential synthesis of much of the mathematics of the day.
Life.
Felix Klein was born on 25 April 1849 in Düsseldorf, to Prussian parents; his father, Caspar Klein (1809–1889), was a Prussian government official's secretary stationed in the Rhine Province. Klein's mother was Sophie Elise Klein (1819–1890, née Kayser). He attended the Gymnasium in Düsseldorf, then studied mathematics and physics at the University of Bonn, 1865–1866, intending to become a physicist. At that time, Julius Plücker held Bonn's chair of mathematics and experimental physics, but by the time Klein became his assistant, in 1866, Plücker's interest was geometry. Klein received his doctorate, supervised by Plücker, from the University of Bonn in 1868.
Plücker died in 1868, leaving his book on the foundations of line geometry incomplete. Klein was the obvious person to complete the second part of Plücker's "Neue Geometrie des Raumes", and thus became acquainted with Alfred Clebsch, who had moved to Göttingen in 1868. Klein visited Clebsch the following year, along with visits to Berlin and Paris. In July 1870, at the outbreak of the Franco-Prussian War, he was in Paris and had to leave the country. For a short time, he served as a medical orderly in the Prussian army before being appointed lecturer at Göttingen in early 1871.
Erlangen appointed Klein professor in 1872, when he was only 23. In this, he was strongly supported by Clebsch, who regarded him as likely to become the leading mathematician of his day. Klein did not build a school at Erlangen where there were few students, and so he was pleased to be offered a chair at Munich's Technische Hochschule in 1875. There he and Alexander von Brill taught advanced courses to many excellent students, including, Adolf Hurwitz, Walther von Dyck, Karl Rohn, Carl Runge, Max Planck, Luigi Bianchi, and Gregorio Ricci-Curbastro.
In 1875 Klein married Anne Hegel, the granddaughter of the philosopher Georg Wilhelm Friedrich Hegel.
After five years at the Technische Hochschule, Klein was appointed to a chair of geometry at Leipzig. There his colleagues included Walther von Dyck, Rohn, Eduard Study and Friedrich Engel. Klein's years at Leipzig, 1880 to 1886, fundamentally changed his life. In 1882, his health collapsed; in 1883–1884, he was plagued by depression. Nonetheless his research continued; his seminal work on hyperelliptic sigma functions dates from around this period, being published in 1886 and 1888.
Klein accepted a chair at the University of Göttingen in 1886. From then until his 1913 retirement, he sought to re-establish Göttingen as the world's leading mathematics research center. Yet he never managed to transfer from Leipzig to Göttingen his own role as the leader of a school of geometry. At Göttingen, he taught a variety of courses, mainly on the interface between mathematics and physics, such as mechanics and potential theory.
The research center Klein established at Göttingen served as a model for the best such centers throughout the world. He introduced weekly discussion meetings, and created a mathematical reading room and library. In 1895, Klein hired David Hilbert away from Königsberg; this appointment proved fateful, because Hilbert continued Göttingen's glory until his own retirement in 1932.
Under Klein's editorship, "Mathematische Annalen" became one of the very best mathematics journals in the world. Founded by Clebsch, only under Klein's management did it first rival then surpass "Crelle's Journal" based out of the University of Berlin. Klein set up a small team of editors who met regularly, making democratic decisions. The journal specialized in complex analysis, algebraic geometry, and invariant theory (at least until Hilbert killed the subject). It also provided an important outlet for real analysis and the new group theory.
In 1893 in Chicago, Klein was a keynote speaker at the International Mathematical Congress held as part of the World's Columbian Exposition. Thanks in part to Klein's efforts, Göttingen began admitting women in 1893. He supervised the first Ph.D. thesis in mathematics written at Göttingen by a woman; she was Grace Chisholm Young, an English student of Arthur Cayley's, whom Klein admired. In 1897 Klein became foreign member of the Royal Netherlands Academy of Arts and Sciences.
Around 1900, Klein began to take an interest in mathematical instruction in schools. In 1905, he played a decisive role in formulating a plan recommending that analytic geometry, the rudiments of differential and integral calculus, and the function concept be taught in secondary schools. This recommendation was gradually implemented in many countries around the world. In 1908, Klein was elected president of the International Commission on Mathematical Instruction at the Rome International Congress of Mathematicians. Under his guidance, the German branch of the Commission published many volumes on the teaching of mathematics at all levels in Germany.
The London Mathematical Society awarded Klein its De Morgan Medal in 1893. He was elected a member of the Royal Society in 1885, and was awarded its Copley Medal in 1912. He retired the following year due to ill health, but continued to teach mathematics at his home for some years more.
Klein bore the title of Geheimrat.
He died in Göttingen in 1925.
Work.
Klein's dissertation, on line geometry and its applications to mechanics, classified second degree line complexes using Weierstrass's theory of elementary divisors.
Klein's first important mathematical discoveries were made in 1870. In collaboration with Sophus Lie, he discovered the fundamental properties of the asymptotic lines on the Kummer surface. They went on to investigate W-curves, curves invariant under a group of projective transformations. It was Lie who introduced Klein to the concept of group, which was to play a major role in his later work. Klein also learned about groups from Camille Jordan.
Klein devised the bottle named after him, a one-sided closed surface which cannot be embedded in three-dimensional Euclidean space, but it may be immersed as a cylinder looped back through itself to join with its other end from the "inside". It may be embedded in Euclidean space of dimensions 4 and higher.
In the 1890s, Klein turned to mathematical physics, a subject from which he had never strayed far, writing on the gyroscope with Arnold Sommerfeld. In 1894 he launched the idea of an encyclopedia of mathematics including its applications, which became the Enzyklopädie der mathematischen Wissenschaften. This enterprise, which ran until 1935, provided an important standard reference of enduring value.
Erlangen program.
In 1871, while at Göttingen, Klein made major discoveries in geometry. He published two papers "On the So-called Non-Euclidean Geometry" showing that Euclidean and non-Euclidean geometries could be considered metric spaces determined by a Cayley-Klein metric. This insight had the corollary that non-Euclidean geometry was consistent if and only if Euclidean geometry was, putting Euclidean and non-Euclidean geometries on the same footing, and ending all controversy surrounding non-Euclidean geometry. Cayley never accepted Klein's argument, believing it to be circular.
Klein's synthesis of geometry as the study of the properties of a space that is invariant under a given group of transformations, known as the "Erlangen Program" (1872), profoundly influenced the evolution of mathematics. This program was set out in Klein's inaugural lecture as professor at Erlangen, although it was not the actual speech he gave on the occasion. The program proposed a unified approach to geometry that has become the accepted modern view. Klein showed how the essential properties of a given geometry could be represented by the group of transformations that preserve those properties. Thus the program's definition of geometry encompassed both Euclidean and non-Euclidean geometry.
Today the significance of Klein's contributions to geometry is more than evident, but not because those contributions are now seen as strange or wrong. On the contrary, those contributions have become so much a part of our present mathematical thinking that it is hard for us to appreciate their novelty, and the way in which they were not immediately accepted by all his contemporaries.
Complex analysis.
Klein saw his work on complex analysis as his major contribution to mathematics, specifically his work on:
Klein showed that the modular group moves the fundamental region of the complex plane so as to tessellate that plane. In 1879, he looked at the action of PSL(2,7), thought of as an image of the modular group, and obtained an explicit representation of a Riemann surface today called the Klein quartic. He showed that that surface was a curve in projective space, that its equation was "x"3"y" + "y"3"z" + "z"3"x" = 0, and that its group of symmetries was PSL(2,7) of order 168. His "Ueber Riemann's Theorie der algebraischen Funktionen und ihre Integrale" (1882) treats complex analysis in a geometric way, connecting potential theory and conformal mappings. This work drew on notions from fluid dynamics.
Klein considered equations of degree > 4, and was especially interested in using transcendental methods to solve the general equation of the fifth degree. Building on the methods of Hermite and Kronecker, he produced similar results to those of Brioschi and went on to completely solve the problem by means of the icosahedral group. This work led him to write a series of papers on elliptic modular functions.
In his 1884 book on the icosahedron, Klein set out a theory of automorphic functions, connecting algebra and geometry. However Poincaré published an outline of his theory of automorphic functions in 1881, which led to a friendly rivalry between the two men. Both sought to state and prove a grand uniformization theorem that would serve as a capstone to the emerging theory. Klein succeeded in formulating such a theorem and in sketching a strategy for proving it. But while doing this work his health collapsed, as mentioned above.
Klein summarized his work on automorphic and elliptic modular functions in a four volume treatise, written with Robert Fricke over a period of about 20 years.
Bibliography.
Primary:
Secondary

</doc>
<doc id="41890" url="https://en.wikipedia.org/wiki?curid=41890" title="Group theory">
Group theory

In mathematics and abstract algebra, group theory studies the algebraic structures known as groups. 
The concept of a group is central to abstract algebra: other well-known algebraic structures, such as rings, fields, and vector spaces, can all be seen as groups endowed with additional operations and axioms. Groups recur throughout mathematics, and the methods of group theory have influenced many parts of algebra. Linear algebraic groups and Lie groups are two branches of group theory that have experienced advances and have become subject areas in their own right.
Various physical systems, such as crystals and the hydrogen atom, may be modelled by symmetry groups. Thus group theory and the closely related representation theory have many important applications in physics, chemistry, and materials science. Group theory is also central to public key cryptography.
One of the most important mathematical achievements of the 20th century was the collaborative effort, taking up more than 10,000 journal pages and mostly published between 1960 and 1980, that culminated in a complete classification of finite simple groups.
Main classes of groups.
The range of groups being considered has gradually expanded from finite permutation groups and special examples of matrix groups to abstract groups that may be specified through a presentation by generators and relations.
Permutation groups.
The first class of groups to undergo a systematic study was permutation groups. Given any set "X" and a collection "G" of bijections of "X" into itself (known as "permutations") that is closed under compositions and inverses, "G" is a group acting on "X". If "X" consists of "n" elements and "G" consists of "all" permutations, "G" is the symmetric group S"n"; in general, any permutation group "G" is a subgroup of the symmetric group of "X". An early construction due to Cayley exhibited any group as a permutation group, acting on itself () by means of the left regular representation.
In many cases, the structure of a permutation group can be studied using the properties of its action on the corresponding set. For example, in this way one proves that for , the alternating group A"n" is simple, i.e. does not admit any proper normal subgroups. This fact plays a key role in the impossibility of solving a general algebraic equation of degree in radicals.
Matrix groups.
The next important class of groups is given by "matrix groups", or linear groups. Here "G" is a set consisting of invertible matrices of given order "n" over a field "K" that is closed under the products and inverses. Such a group acts on the "n"-dimensional vector space "K""n" by linear transformations. This action makes matrix groups conceptually similar to permutation groups, and the geometry of the action may be usefully exploited to establish properties of the group "G".
Transformation groups.
Permutation groups and matrix groups are special cases of transformation groups: groups that act on a certain space "X" preserving its inherent structure. In the case of permutation groups, "X" is a set; for matrix groups, "X" is a vector space. The concept of a transformation group is closely related with the concept of a symmetry group: transformation groups frequently consist of "all" transformations that preserve a certain structure.
The theory of transformation groups forms a bridge connecting group theory with differential geometry. A long line of research, originating with Lie and Klein, considers group actions on manifolds by homeomorphisms or diffeomorphisms. The groups themselves may be discrete or continuous.
Abstract groups.
Most groups considered in the first stage of the development of group theory were "concrete", having been realized through numbers, permutations, or matrices. It was not until the late nineteenth century that the idea of an abstract group as a set with operations satisfying a certain system of axioms began to take hold. A typical way of specifying an abstract group is through a presentation by "generators and relations",
A significant source of abstract groups is given by the construction of a "factor group", or quotient group, "G"/"H", of a group "G" by a normal subgroup "H". Class groups of algebraic number fields were among the earliest examples of factor groups, of much interest in number theory. If a group "G" is a permutation group on a set "X", the factor group "G"/"H" is no longer acting on "X"; but the idea of an abstract group permits one not to worry about this discrepancy.
The change of perspective from concrete to abstract groups makes it natural to consider properties of groups that are independent of a particular realization, or in modern language, invariant under isomorphism, as well as the classes of group with a given such property: finite groups, periodic groups, simple groups, solvable groups, and so on. Rather than exploring properties of an individual group, one seeks to establish results that apply to a whole class of groups. The new paradigm was of paramount importance for the development of mathematics: it foreshadowed the creation of abstract algebra in the works of Hilbert, Emil Artin, Emmy Noether, and mathematicians of their school.
Topological and algebraic groups.
An important elaboration of the concept of a group occurs if "G" is endowed with additional structure, notably, of a topological space, differentiable manifold, or algebraic variety. If the group operations "m" (multiplication) and "i" (inversion),
are compatible with this structure, i.e. are continuous, smooth or regular (in the sense of algebraic geometry) maps, then "G" becomes a topological group, a Lie group, or an algebraic group.
The presence of extra structure relates these types of groups with other mathematical disciplines and means that more tools are available in their study. Topological groups form a natural domain for abstract harmonic analysis, whereas Lie groups (frequently realized as transformation groups) are the mainstays of differential geometry and unitary representation theory. Certain classification questions that cannot be solved in general can be approached and resolved for special subclasses of groups. Thus, compact connected Lie groups have been completely classified. There is a fruitful relation between infinite abstract groups and topological groups: whenever a group "Γ" can be realized as a lattice in a topological group "G", the geometry and analysis pertaining to "G" yield important results about "Γ". A comparatively recent trend in the theory of finite groups exploits their connections with compact topological groups (profinite groups): for example, a single "p"-adic analytic group "G" has a family of quotients which are finite "p"-groups of various orders, and properties of "G" translate into the properties of its finite quotients.
Branches of group theory.
Finite group theory.
During the twentieth century, mathematicians investigated some aspects of the theory of finite groups in great depth, especially the local theory of finite groups and the theory of solvable and nilpotent groups. As a consequence, the complete classification of finite simple groups was achieved, meaning that all those simple groups from which all finite groups can be built are now known.
During the second half of the twentieth century, mathematicians such as Chevalley and Steinberg also increased our understanding of finite analogs of classical groups, and other related groups. One such family of groups is the family of general linear groups over finite fields. 
Finite groups often occur when considering symmetry of mathematical or
physical objects, when those objects admit just a finite number of structure-preserving transformations. The theory of Lie groups,
which may be viewed as dealing with "continuous symmetry", is strongly influenced by the associated Weyl groups. These are finite groups generated by reflections which act on a finite-dimensional Euclidean space. The properties of finite groups can thus play a role in subjects such as theoretical physics and chemistry.
Representation of groups.
Saying that a group "G" "acts" on a set "X" means that every element of "G" defines a bijective map on the set "X" in a way compatible with the group structure. When "X" has more structure, it is useful to restrict this notion further: a representation of "G" on a vector space "V" is a group homomorphism:
where GL("V") consists of the invertible linear transformations of "V". In other words, to every group element "g" is assigned an automorphism "ρ"("g") such that for any "h" in "G".
This definition can be understood in two directions, both of which give rise to whole new domains of mathematics. On the one hand, it may yield new information about the group "G": often, the group operation in "G" is abstractly given, but via "ρ", it corresponds to the multiplication of matrices, which is very explicit. On the other hand, given a well-understood group acting on a complicated object, this simplifies the study of the object in question. For example, if "G" is finite, it is known that "V" above decomposes into irreducible parts. These parts in turn are much more easily manageable than the whole "V" (via Schur's lemma).
Given a group "G", representation theory then asks what representations of "G" exist. There are several settings, and the employed methods and obtained results are rather different in every case: representation theory of finite groups and representations of Lie groups are two main subdomains of the theory. The totality of representations is governed by the group's characters. For example, Fourier polynomials can be interpreted as the characters of U(1), the group of complex numbers of absolute value "1", acting on the "L"2-space of periodic functions.
Lie theory.
A Lie group is a group that is also a differentiable manifold, with the property that the group operations are compatible with the smooth structure. Lie groups are named after Sophus Lie, who laid the foundations of the theory of continuous transformation groups. The term "groupes de Lie" first appeared in French in 1893 in the thesis of Lie’s student Arthur Tresse, page 3.
Lie groups represent the best-developed theory of continuous symmetry of mathematical objects and structures, which makes them indispensable tools for many parts of contemporary mathematics, as well as for modern theoretical physics. They provide a natural framework for analysing the continuous symmetries of differential equations (differential Galois theory), in much the same way as permutation groups are used in Galois theory for analysing the discrete symmetries of algebraic equations. An extension of Galois theory to the case of continuous symmetry groups was one of Lie's principal motivations.
Combinatorial and geometric group theory.
Groups can be described in different ways. Finite groups can be described by writing down the group table consisting of all possible multiplications . A more compact way of defining a group is by "generators and relations", also called the "presentation" of a group. Given any set "F" of generators {"g""i"}"i" ∈ "I", the free group generated by "F" subjects onto the group "G". The kernel of this map is called subgroup of relations, generated by some subset "D". The presentation is usually denoted by . For example, the group can be generated by one element "a" (equal to +1 or −1) and no relations, because never equals 0 unless "n" is zero. A string consisting of generator symbols and their inverses is called a "word".
Combinatorial group theory studies groups from the perspective of generators and relations. It is particularly useful where finiteness assumptions are satisfied, for example finitely generated groups, or finitely presented groups (i.e. in addition the relations are finite). The area makes use of the connection of graphs via their fundamental groups. For example, one can show that every subgroup of a free group is free.
There are several natural questions arising from giving a group by its presentation. The "word problem" asks whether two words are effectively the same group element. By relating the problem to Turing machines, one can show that there is in general no algorithm solving this task. Another, generally harder, algorithmically insoluble problem is the group isomorphism problem, which asks whether two groups given by different presentations are actually isomorphic. For example, the additive group Z of integers can also be presented by
it may not be obvious that these groups are isomorphic.
Geometric group theory attacks these problems from a geometric viewpoint, either by viewing groups as geometric objects, or by finding suitable geometric objects a group acts on. The first idea is made precise by means of the Cayley graph, whose vertices correspond to group elements and edges correspond to right multiplication in the group. Given two elements, one constructs the word metric given by the length of the minimal path between the elements. A theorem of Milnor and Svarc then says that given a group "G" acting in a reasonable manner on a metric space "X", for example a compact manifold, then "G" is quasi-isometric (i.e. looks similar from a distance) to the space "X".
Connection of groups and symmetry.
Given a structured object "X" of any sort, a symmetry is a mapping of the object onto itself which preserves the structure. This occurs in many cases, for example
The axioms of a group formalize the essential aspects of symmetry. Symmetries form a group: they are closed because if you take a symmetry of an object, and then apply another symmetry, the result will still be a symmetry. The identity keeping the object fixed is always a symmetry of an object. Existence of inverses is guaranteed by undoing the symmetry and the associativity comes from the fact that symmetries are functions on a space, and composition of functions are associative.
Frucht's theorem says that every group is the symmetry group of some graph. So every abstract group is actually the symmetries of some explicit object.
The saying of "preserving the structure" of an object can be made precise by working in a category. Maps preserving the structure are then the morphisms, and the symmetry group is the automorphism group of the object in question.
Applications of group theory.
Applications of group theory abound. Almost all structures in abstract algebra are special cases of groups. Rings, for example, can be viewed as abelian groups (corresponding to addition) together with a second operation (corresponding to multiplication). Therefore, group theoretic arguments underlie large parts of the theory of those entities.
Galois theory.
Galois theory uses groups to describe the symmetries of the roots of a polynomial (or more precisely the automorphisms of the algebras generated by these roots). The fundamental theorem of Galois theory provides a link between algebraic field extensions and group theory. It gives an effective criterion for the solvability of polynomial equations in terms of the solvability of the corresponding Galois group. For example, "S"5, the symmetric group in 5 elements, is not solvable which implies that the general quintic equation cannot be solved by radicals in the way equations of lower degree can. The theory, being one of the historical roots of group theory, is still fruitfully applied to yield new results in areas such as class field theory.
Algebraic topology.
Algebraic topology is another domain which prominently associates groups to the objects the theory is interested in. There, groups are used to describe certain invariants of topological spaces. They are called "invariants" because they are defined in such a way that they do not change if the space is subjected to some deformation. For example, the fundamental group "counts" how many paths in the space are essentially different. The Poincaré conjecture, proved in 2002/2003 by Grigori Perelman, is a prominent application of this idea. The influence is not unidirectional, though. For example, algebraic topology makes use of Eilenberg–MacLane spaces which are spaces with prescribed homotopy groups. Similarly algebraic K-theory relies in a way on classifying spaces of groups. Finally, the name of the torsion subgroup of an infinite group shows the legacy of topology in group theory.
Algebraic geometry and cryptography.
Algebraic geometry and cryptography likewise uses group theory in many ways. Abelian varieties have been introduced above. The presence of the group operation yields additional information which makes these varieties particularly accessible. They also often serve as a test for new conjectures. The one-dimensional case, namely elliptic curves is studied in particular detail. They are both theoretically and practically intriguing. Very large groups of prime order constructed in Elliptic-Curve Cryptography serve for public key cryptography. Cryptographical methods of this kind benefit from the flexibility of the geometric objects, hence their group structures, together with the complicated structure of these groups, which make the discrete logarithm very hard to calculate. One of the earliest encryption protocols, Caesar's cipher, may also be interpreted as a (very easy) group operation. In another direction, toric varieties are algebraic varieties acted on by a torus. Toroidal embeddings have recently led to advances in algebraic geometry, in particular resolution of singularities.
Algebraic number theory.
Algebraic number theory is a special case of group theory, thereby following the rules of the latter. For example, Euler's product formula
captures the fact that any integer decomposes in a unique way into primes. The failure of this statement for more general rings gives rise to class groups and regular primes, which feature in Kummer's treatment of Fermat's Last Theorem.
Harmonic analysis.
Analysis on Lie groups and certain other groups is called harmonic analysis. Haar measures, that is, integrals invariant under the translation in a Lie group, are used for pattern recognition and other image processing techniques.
Combinatorics.
In combinatorics, the notion of permutation group and the concept of group action are often used to simplify the counting of a set of objects; see in particular Burnside's lemma.
Music.
The presence of the 12-periodicity in the circle of fifths yields applications of elementary group theory in musical set theory.
Physics.
In physics, groups are important because they describe the symmetries which the laws of physics seem to obey. According to Noether's theorem, every continuous symmetry of a physical system corresponds to a conservation law of the system. Physicists are very interested in group representations, especially of Lie groups, since these representations often point the way to the "possible" physical theories. Examples of the use of groups in physics include the Standard Model, gauge theory, the Lorentz group, and the Poincaré group.
Chemistry and materials science.
In chemistry and materials science, groups are used to classify crystal structures, regular polyhedra, and the symmetries of molecules. The assigned point groups can then be used to determine physical properties (such as polarity and chirality), spectroscopic properties (particularly useful for Raman spectroscopy, infrared spectroscopy, circular dichroism spectroscopy, magnetic circular dichroism spectroscopy, UV/Vis spectroscopy, and fluorescence spectroscopy), and to construct molecular orbitals.
Molecular symmetry is responsible for many physical and spectroscopic properties of compounds and provides relevant information about how chemical reactions occur. In order to assign a point group for any given molecule, it is necessary to find the set of symmetry operations present on it. The symmetry operation is an action, such as a rotation around an axis or a reflection through a mirror plane. In other words, it is an operation that moves the molecule such that it is indistinguishable from the original configuration. In group theory, the rotation axes and mirror planes are called "symmetry elements". These elements can be a point, line or plane with respect to which the symmetry operation is carried out. The symmetry operations of a molecule determine the specific point group for this molecule.
In chemistry, there are five important symmetry operations. The identity operation (E) consists of leaving the molecule as it is. This is equivalent to any number of full rotations around any axis. This is a symmetry of all molecules, whereas the symmetry group of a chiral molecule consists of only the identity operation. Rotation around an axis (C"n") consists of rotating the molecule around a specific axis by a specific angle. For example, if a water molecule rotates 180° around the axis that passes through the oxygen atom and between the hydrogen atoms, it is in the same configuration as it started. In this case, , since applying it twice produces the identity operation. Other symmetry operations are: reflection, inversion and improper rotation (rotation followed by reflection).
Statistical Mechanics.
Group theory can be used to resolve the incompleteness of the statistical interpretations of mechanics developed by Willard Gibbs, relating to the summing of an infinite number of probabilities to yield a meaningful solution
History.
Group theory has three main historical sources: number theory, the theory of algebraic equations, and geometry. The number-theoretic strand was begun by Leonhard Euler, and developed by Gauss's work on modular arithmetic and additive and multiplicative groups related to quadratic fields. Early results about permutation groups were obtained by Lagrange, Ruffini, and Abel in their quest for general solutions of polynomial equations of high degree. Évariste Galois coined the term "group" and established a connection, now known as Galois theory, between the nascent theory of groups and field theory. In geometry, groups first became important in projective geometry and, later, non-Euclidean geometry. Felix Klein's Erlangen program proclaimed group theory to be the organizing principle of geometry.
Galois, in the 1830s, was the first to employ groups to determine the solvability of polynomial equations. Arthur Cayley and Augustin Louis Cauchy pushed these investigations further by creating the theory of permutation groups. The second historical source for groups stems from geometrical situations. In an attempt to come to grips with possible geometries (such as euclidean, hyperbolic or projective geometry) using group theory, Felix Klein initiated the Erlangen programme. Sophus Lie, in 1884, started using groups (now called Lie groups) attached to analytic problems. Thirdly, groups were, at first implicitly and later explicitly, used in algebraic number theory.
The different scope of these early sources resulted in different notions of groups. The theory of groups was unified starting around 1880. Since then, the impact of group theory has been ever growing, giving rise to the birth of abstract algebra in the early 20th century, representation theory, and many more influential spin-off domains. The classification of finite simple groups is a vast body of work from the mid 20th century, classifying all the finite simple groups.

</doc>
<doc id="41891" url="https://en.wikipedia.org/wiki?curid=41891" title="Stable nuclide">
Stable nuclide

Stable nuclides are nuclides that are not radioactive and so (unlike radionuclides) do not spontaneously undergo radioactive decay. When such nuclides are referred to in relation to specific elements, they are usually termed stable isotopes.
The 80 elements with one or more stable isotopes comprise a total of 254 nuclides that have not been known to decay using current equipment (see list at the end of this article). Of these elements, 26 have only one stable isotope; they are thus termed monoisotopic. The rest have more than one stable isotope. Tin has ten stable isotopes, the largest number known for an element.
Definition of stability, and naturally occurring nuclides.
Most naturally occurring nuclides are stable (about 254; see list at the end of this article); and about 34 more (total of 288) are known radioactives with sufficiently long half-lives (also known) to occur primordially. If the half-life of a nuclide is comparable to, or greater than, the Earth's age (4.5 billion years), a significant amount will have survived since the formation of the Solar System, and then is said to be primordial. It will then contribute in that way to the natural isotopic composition of a chemical element. Primordially present radioisotopes are easily detected with half-lives as short as 700 million years (e.g., 235U), although some primordial isotopes have been detected with half-lives as short as 80 million years (e.g., 244Pu). However, this is the present limit of detection, as the nuclide with the next-shortest half-life (niobium-92 with half-life 34.7 million years) has not yet been detected in nature.
Many naturally-occurring radioisotopes (another 51 or so, for a total of about 339) exhibit still shorter half-lives than 80 million years, but they are made freshly, as daughter products of decay processes of primordial nuclides (for example, radium from uranium) or from ongoing energetic reactions, such as cosmogenic nuclides produced by present bombardment of Earth by cosmic rays (for example, carbon-14 made from nitrogen).
Some isotopes that are classed as stable (i.e. no radioactivity has been observed for them) are predicted to have extremely long half-lives (sometimes as high as 1018 years or more). If the predicted half-life falls into an experimentally accessible range, such isotopes have a chance to move from the list of stable nuclides to the radioactive category, once their activity is observed. E.g. bismuth-209 and tungsten-180 were formerly classed as stable, but have been recently (2003) found to be alpha-active. However, such nuclides do not change their status as primordial when they are found to be radioactive.
Most stable isotopes in the earth are believed to have been formed in processes of nucleosynthesis, either in the Big Bang, or in generations of stars that preceded the formation of the solar system. However, some stable isotopes also show abundance variations in the earth as a result of decay from long-lived radioactive nuclides. These decay-products are termed radiogenic isotopes, in order to distinguish them from the much larger group of 'non-radiogenic' isotopes.
The so-called island of stability may reveal a number of long-lived or even stable atoms that are heavier (and with more protons) than lead.
Isotopes per element.
Of the known chemical elements, 80 elements have at least one stable nuclide. These comprise the first 82 elements from hydrogen to lead, with the two exceptions, technetium (element 43) and promethium (element 61), that do not have any stable nuclides. As of December 2011, there were a total of 254 known "stable" nuclides. In this definition, "stable" means a nuclide that has never been observed to decay against the natural background. Thus, these elements have half lives too long to be measured by any means, direct or indirect.
Stable isotopes:
These last 26 are thus called "monoisotopic elements". The mean number of stable isotopes for elements which have at least one stable isotope is 254/80 = 3.2.
"Magic numbers" and odd and even proton and neutron count.
Stability of isotopes is affected by the ratio of protons to neutrons, and also by presence of certain "magic numbers" of neutrons or protons which represent closed and filled quantum shells. These quantum shells correspond to a set of energy levels within the shell model of the nucleus; filled shells, such as the filled shell of 50 protons for tin, confers unusual stability on the nuclide. As in the case of tin, a magic number for Z, the atomic number, tends to increase the number of stable isotopes for the element.
Just as in the case of electrons, which have the lowest energy state when they occur in pairs in a given orbital, nucleons (both protons and neutrons) exhibit a lower energy state when their number is even, rather than odd. This stability tends to prevent beta decay (in two steps) of many even-even nuclides into another even-even nuclide of the same mass number but lower energy (and of course with two more protons and two fewer neutrons), because decay proceeding one step at a time would have to pass through an odd-odd nuclide of higher energy. This makes for a larger number of stable even-even nuclides, up to three for some mass numbers, and up to seven for some atomic (proton) numbers.
Conversely, of the 254 known stable nuclides, only five have both an odd number of protons "and" odd number of neutrons: hydrogen-2 (deuterium), lithium-6, boron-10, nitrogen-14, and tantalum-180m. Also, only four naturally occurring, radioactive odd-odd nuclides have a half-life over a billion years: potassium-40, vanadium-50, lanthanum-138, and lutetium-176. Odd-odd primordial nuclides are rare because most odd-odd nuclei are highly unstable with respect to beta decay, because the decay products are even-even, and are therefore more strongly bound, due to nuclear pairing effects.
Yet another effect of the instability of an odd number of either type of nucleons, is that odd-numbered elements tend to have fewer stable isotopes. Of the 26 monoisotopic elements that have only a single stable isotope, all but one have an odd atomic number — the single exception to both rules being beryllium. All of these elements also have an even number of neutrons, with the single exception again being beryllium.
Nuclear isomers, including a "stable" one.
The count of 254 known stable nuclides includes tantalum-180m, since even though its decay and instability is automatically implied by its notation of "metastable", still this has not yet been observed. All "stable" isotopes (stable by observation, not theory) are the ground states of nuclei, with the exception of tantalum-180m, which is a nuclear isomer or excited state. The ground state of this particular nucleus, Ta-180, is radioactive with a comparatively short half-life of 8 hours; in contrast, the decay of the excited nuclear isomer is extremely strongly forbidden by spin-parity selection rules. It has been reported experimentally by direct observation that the half-life of 180mTa to gamma decay must be more than 1015 years. Other possible modes of 180mTa decay (beta decay, electron capture and alpha decay) have also never been observed.
Still-unobserved decay.
It is expected that some continual improvement of experimental sensitivity will allow discovery of very mild radioactivity (instability) of some isotopes that are considered to be stable today. For an example of a recent discovery, it was not until 2003 that bismuth-209 (the only naturally-occurring isotope of bismuth) was shown to be very mildly radioactive. Prior to this discovery, there were theoretical predictions from nuclear physics that bismuth-209 would decay very slowly by alpha emission. These calculations were confirmed by the experimental observations in 2003.
The nuclides starting from with the isotope niobium-93 and extending to all higher atomic mass numbers, could theoretically experience spontaneous fission.
For processes other than spontaneous fission, other theoretical decay routes for heavier elements include:
These include all nuclides of mass 165 and greater. Argon-36 is presently the lightest known "stable" nuclide which is theoretically unstable.
The positivity of energy release in these processes means that they are allowed kinematically (they do not violate the conservation of energy) and, thus, in principle, can occur. They are not observed due to strong but not absolute suppression, by spin-parity selection rules (for beta decays and isomeric transitions) or by the thickness of the potential barrier (for alpha and cluster decays and spontaneous fission).--->
Summary table for numbers of each class of nuclides.
This is a summary table from List of nuclides. Note that numbers are not exact, and may change slightly in the future, as nuclides are observed to be radioactive, or new half-lives are determined to some precision.
List of stable nuclides.
Abbreviations for predicted unobserved decay:
A for alpha decay, B for beta decay, 2B for double beta decay, E for electron capture, 2E for double electron capture, IT for isomeric transition, SF for spontaneous fission.

</doc>
<doc id="41892" url="https://en.wikipedia.org/wiki?curid=41892" title="Terminal">
Terminal

Terminal may refer to:

</doc>
<doc id="41893" url="https://en.wikipedia.org/wiki?curid=41893" title="Teletype (disambiguation)">
Teletype (disambiguation)

Teletype may refer to:

</doc>
<doc id="41895" url="https://en.wikipedia.org/wiki?curid=41895" title="Craig Barrett">
Craig Barrett

Craig Barrett may refer to:

</doc>
<doc id="41896" url="https://en.wikipedia.org/wiki?curid=41896" title="Helmut Kohl">
Helmut Kohl

Helmut Josef Michael Kohl (; born 3 April 1930) is a German statesman, who served as Chancellor of Germany from 1982 to 1998 (of West Germany 1982–90 and of the reunited Germany 1990–98) and as the chairman of the Christian Democratic Union (CDU) from 1973 to 1998. From 1969 to 1976, Kohl was the 3rd Minister President of Rhineland-Palatinate.
Kohl's 16-year tenure was the longest of any German Chancellor since Otto von Bismarck, and by far the longest of any democratically-elected Chancellor. Kohl oversaw the end of the Cold War, and is widely regarded as the main architect of the German reunification. Together with French President François Mitterrand, Kohl is also considered to be the architect of the Maastricht Treaty, which established the European Union (EU).
Kohl has been described as "the greatest European leader of the second half of the 20th century" by U.S. Presidents George H. W. Bush and Bill Clinton.
Life.
Youth and education.
Helmut Kohl was born on 3 April 1930 in Ludwigshafen am Rhein (at the time part of Bavaria, now in Rhineland-Palatinate), Germany, the third child of Hans Kohl (1887–1975), a civil servant, and his wife, Cäcilie (née Schnur; 1890–1979). 
Kohl's family was conservative and Roman Catholic, and remained loyal to the Catholic Centre Party before and after 1933. His older brother died in the Second World War as a teenage soldier. At the age of ten, Kohl was obliged, like any child in Germany at the time, to join the "Deutsches Jungvolk", a section of the Hitler Youth. Aged 15, on 20 April 1945, Adolf Hitler's birthday, Kohl was sworn in to the Hitler Youth by leader Artur Axmann at Berchtesgaden, just days before the end of the war. Kohl was also drafted for military service in 1945; however, he was not involved in any combat, a fact he later referred to as the "mercy of late birth" (German: "Gnade der späten Geburt").
Kohl attended the Ruprecht Elementary School, and continued at the Max-Planck-Gymnasium. After graduating in 1950, Kohl began to study law in Frankfurt am Main, spending two semesters commuting between Ludwigshafen and Frankfurt. Here, Kohl heard lectures from Carlo Schmid and Walter Hallstein among others. In 1951, Kohl switched to the University of Heidelberg, where he majored in History and Political Science. Kohl was the first in his family to attend university.
Life before politics.
After graduating in 1956, Kohl became fellow at the Alfred Weber Institute of the University of Heidelberg under Dolf Sternberger where he was an active member of the student society AIESEC. In 1958, Kohl received his doctorate degree for his thesis "The Political Developments in the Palatinate and the Reconstruction of Political Parties after 1945". After that, Kohl entered business, first as an assistant to the director of a foundry in Ludwigshafen and, in April 1960, as a manager for the Industrial Union for Chemistry in Ludwigshafen. 
In 1960, Kohl married Hannelore Renner, after he had already asked for her hand in marriage in 1953, waiting with the ceremony until he was financially stable. Both had known each other since 1948, when they met in a dancing class. They had two sons.
Early political career.
In 1946, Kohl joined the recently founded CDU, becoming a full member once he turned 18 in 1948. In 1947, Kohl was one of the co-founders of the Junge Union-branch in Ludwigshafen, the CDU youth organisation. In 1953, Kohl joined the board of the Palatinate branch of the CDU. In 1954, Kohl became vice-chair of the Junge Union in Rhineland-Palatinate, being a member of the board until 1961. 
In January 1955, Kohl ran for a seat on the board of the Rhineland-Palatinate CDU, losing just narrowly to the state's Minister of Family Affairs, Franz-Josef Wuermeling. Kohl was however still able to take up a seat on the board, being sent there by his local party branch as a delegate. During his early years in the party, Kohl aimed to open it towards the young generation, turning away from a close relationship with the churches.
In early 1959, Kohl was elected chairman of the Ludwigshafen district branch of the CDU, as well as candidate for the upcoming state elections. On 19 April 1959, Kohl was elected as the youngest member of the state diet, the Landtag of Rhineland-Palatinate. In 1960, he was also elected into the municipal council of Ludwigshafen where he served as leader of the CDU party until 1969. When the chairman of the CDU parliamentary group in the Landtag, Wilhelm Boden, died in the fall of 1961, Kohl moved up into a deputy position. Following the next state election in 1963, he took over as chairman, a position he held until he became Minister-President in 1969. In 1966, Kohl and the incumbent minister-president and state party chairman, Peter Altmeier, agreed to share duties. In March 1966, Kohl was elected as chairman of the party in Rhineland-Palatinate, while Altmeier once again ran for minister-president in the state elections in 1967, agreeing to hand the post over to Kohl after two years, halfway into the legislative period.
Minister-President of Rhineland-Palatinate.
On 19 May 1969, Kohl was elected minister-president of Rhineland-Palatinate, as the successor to Peter Altmeier. As of 2015, he is the youngest person ever to be elected as head of government in a German "Bundesland". Just a few days after his election as minister-president, Kohl also became vice-chair of the federal CDU party. While in office, Kohl acted as a reformer, focusing on school and education. His government abolished school corporal punishment and the parochial school, topics that had been controversial with the conservative wing of his party. During his term, Kohl founded the University of Trier-Kaiserslautern and enacted territorial reform. He established two new ministries, one for economy and transportation and one for social matters, with the latter going to Heiner Geißler, who would work closely with Kohl for the next twenty years.
Federal party level, election as chairman of the CDU.
Kohl moved up into the federal board ("Vorstand") of the CDU in 1964. Two years later, shortly before his election as chairman of the party in Rhineland-Palatinate, he failed at an attempt to be voted into the executive committee ("Präsidium") of the party. After the CDU lost its involvement in the federal government for the first time since the end of World War II in the 1969 election, Kohl was elected into the committee. While former chancellor Kurt Georg Kiesinger remained chairman of the CDU until 1971, it was now parliamentary chairmen Rainer Barzel who led the opposition against the newly formed social-liberal coalition of Willy Brandt.
As a member of the board and the executive committee, Kohl pushed towards a party reform, supporting liberal stances in education and social policies, including employee participation. However, when a proposal by the board was put to vote at a party convention in early 1971 in Düsseldorf, Kohl was unable to prevail against protest coming from the conservative wing of the party around Alfred Dregger and the sister party CSU, costing him support at the liberal wing of the party. To make matters worse, in a mistake during the voting process, Kohl himself voted against the proposal, further angering his supporters, such as party treasurer Walther Leisler Kiep.
Nevertheless, when Kiesinger stepped down as party chairman in 1971, Kohl was a candidate for his succession. He was unsuccessful, losing the vote to Barzel 344 to 174. In April 1972, in the light of Brandt's Ostpolitik, the CDU aimed to dispose Brandt and his government in a constructive vote of no confidence, replacing him with Barzel. The attempt failed, as two members of the opposition voted against Barzel. After Barzel also lost the general election later that year, the path was free for Kohl to take over. After Barzel announced on 10 May 1973 that he would not run for the post of party chairman again, Kohl succeeded him at a party convention in Saarbrücken on 12 June 1973, amassing 520 of 600 votes, with him as the only candidate. He would hold the position until 1998.
The 1976 Bundestag election.
In the 1976 federal election, Kohl was the CDU/CSU's candidate for chancellor. The CDU/CSU coalition performed very well, winning 48.6% of the vote. However they were kept out of government by the centre-left cabinet formed by the Social Democratic Party of Germany and Free Democratic Party (Germany), led by Social Democrat Helmut Schmidt. Kohl then retired as minister-president of Rhineland-Palatinate to become the leader of the CDU/CSU in the Bundestag. He was succeeded by Bernhard Vogel.
Leader of the opposition.
In the 1980 federal elections, Kohl had to play second fiddle, when CSU-leader Franz Josef Strauß became the CDU/CSU's candidate for chancellor. Strauß was also unable to defeat the SPD/FDP alliance. Unlike Kohl, Strauß did not want to continue as the leader of the CDU/CSU and remained Minister-President of Bavaria. Kohl remained as leader of the opposition, under the third Schmidt cabinet (1980–82). On 17 September 1982, a conflict of economic policy occurred between the governing SPD/FDP coalition partners. The FDP wanted to radically liberalise the labour market, while the SPD preferred greater job security. The FDP began talks with the CDU/CSU to form a new government. 
Chancellor of West Germany.
Rise to power.
On 1 October 1982, the CDU proposed a constructive vote of no confidence which was supported by the FDP. The motion carried. Three days later, the Bundestag voted in a new CDU/CSU-FDP coalition cabinet, with Kohl as chancellor. Many of the important details of the new coalition had been hammered out on 20 September, though minor details were reportedly still being hammered out as the vote took place. Though Kohl's election was done according to the Basic Law, it came amid some controversy. The FDP had fought its 1980 campaign on the side of the SPD and even placed Chancellor Schmidt on some of their campaign posters. There were also doubts that the new government had the support of a majority of the people. In answer, the new government aimed at new elections at the earliest possible date. Polls suggested that a clear majority was indeed in reach. As the Basic Law only allows the dissolution of parliament after an unsuccessful confidence motion, Kohl had to take another controversial move: he called for a confidence vote only a month after being sworn in, in which members of his coalition abstained. President Karl Carstens then dissolved the Bundestag and called new elections.
The move was controversial, as the coalition parties denied their votes to the same man they had elected Chancellor a month before and whom they wanted to re-elect after the parliamentary election. However, this step was condoned by the German Federal Constitutional Court as a legal instrument and was again applied (by SPD Chancellor Gerhard Schröder and his Green allies) in 2005.
Second cabinet.
In the federal elections of March 1983, Kohl won a resounding victory. The CDU/CSU won 48.8%, while the FDP won 7.0%. Some opposition members of the Bundestag asked the Federal Constitutional Court to declare the whole proceeding unconstitutional. It denied their claim, but did set restrictions on a similar move in the future. The second Kohl cabinet pushed through several controversial plans, including the stationing of NATO midrange missiles, against major opposition from the peace movement. On 24 January 1984, Kohl spoke before the Israeli Knesset, as the first Chancellor of the post-war generation. In his speech, he used liberal journalist Günter Gaus' famous sentence that he had "the mercy of a late birth" ("Gnade der späten Geburt"). 
On 22 September 1984 Kohl met the French president François Mitterrand at Verdun, where the Battle of Verdun between France and Germany had taken place during World War I. Together, they commemorated the deaths of both World Wars. The photograph, which depicted their minutes long handshake became an important symbol of French-German reconciliation. Kohl and Mitterrand developed a close political relationship, forming an important motor for European integration. Together, they laid the foundations for European projects, like Eurocorps and Arte. This French-German cooperation also was vital for important European projects, like the Treaty of Maastricht and the Euro.
In 1985, Kohl and US President Ronald Reagan, as part of a plan to observe the 40th anniversary of V-E Day, saw an opportunity to demonstrate the strength of the friendship that existed between Germany and its former foe. During a November 1984 visit to the White House, Kohl appealed to Reagan to join him in symbolizing the reconciliation of their two countries at a German military cemetery. As Reagan visited Germany as part of the G6 conference in Bonn, the pair visited Bergen-Belsen concentration camp on 5 May, and more controversially, the German military cemetery at Bitburg, discovered to hold 49 members of the Waffen-SS. 
Third cabinet.
After the federal elections of 1987 Kohl won a slightly reduced majority and formed his third cabinet. The SPD's candidate for chancellor was the Minister-President of North Rhine-Westphalia, Johannes Rau. 
In 1987, Kohl received East German leader Erich Honecker – the first ever visit by an East German head of state to West Germany. This is generally seen as a sign that Kohl pursued "Ostpolitik", a policy of détente between East and West that had been begun by the SPD-led governments (and strongly opposed by Kohl's own CDU) during the 1970s. 
Domestic policy.
Kohl's chancellorship presided over a number of innovative policy measures. Extensions in unemployment benefit for older claimants were introduced, while the benefit for the young unemployed was extended to age 21. In 1986, a child-rearing allowance was introduced to benefit parents when at least one was employed. Informal carers were offered an attendance allowance together with tax incentives, both of which were established with the tax reforms of 1990, and were also guaranteed up to 25 hours a month of professional support, which was supplemented by four weeks of annual holiday relief. In 1984, an early retirement scheme was introduced that offered incentives to employers to replace elderly workers with applicants off the unemployment register. In 1989 a partial retirement plan was introduced under which elderly employees could work half-time and receive 70% of their former salary “and be credited with 90 per cent of the full social insurance entitlement.” In 1984, a Mother and Child Fund was established, providing discretionary grants “to forestall abortions on grounds of material hardship,” and in 1986 a 10 Mrd DM package of Erziehungsgeld (childcare allowance) was introduced, although according to various studies, this latter initiative was heavily counterbalanced by cuts. In 1989, special provisions were introduced for the older unemployed.
Kohl's time as Chancellor, however, also saw some controversial decisions in the field of social policy. Student aid was made reimbursable to the state while the Health Care Reform Act of 1989 introduced the concept by which patients pay up front and are reimbursed, while increasing patient co-payments for hospitalisation, spa visits, dental prostheses, and prescription drugs. In addition, while a 1986 Baby-Year Pensions reform granted women born after 1921 one year of work-credit per child, lawmakers were forced by public protest to phase in supplementary pension benefits for mothers who were born before the cut-off year.
Road to reunification.
Following the breach of the Berlin Wall and the collapse of the East German Communist regime in 1989, Kohl's handling of the East German issue would become the turning point of his chancellorship. Kohl, like most West Germans, was initially caught unaware when the Socialist Unity Party was toppled in late 1989. However, well aware of his constitutional mandate to seek German unity, he immediately moved to make it a reality. Taking advantage of the historic political changes occurring in East Germany, Kohl presented a ten-point plan for "Overcoming of the division of Germany and Europe" without consulting his coalition partner, the FDP, or the Western Allies. In February 1990, he visited the Soviet Union seeking a guarantee from Mikhail Gorbachev that the USSR would allow German reunification to proceed. One month later, the Party of Democratic Socialism — the renamed SED — was roundly defeated by a grand coalition headed by the East German counterpart of Kohl's CDU, which ran on a platform of speedy reunification.
On 18 May 1990, Kohl signed an economic and social union treaty with East Germany. This treaty stipulated that when reunification took place, it would be under the quicker provisions of Article 23 of the Basic Law. That article stated that any new states could adhere to the Basic Law by a simple majority vote. The alternative would have been the more protracted route of drafting a completely new constitution for the newly reunified country, as provided by Article 146 of the Basic Law. However, an Article 146 reunification would have opened up contentious issues in West Germany, and would have been impractical in any case since by then East Germany was in a state of utter collapse. In contrast, an Article 23 reunification could be completed in as little as six months.
Over the objections of Bundesbank president Karl Otto Pöhl, he allowed a 1:1 exchange rate for wages, interest and rent between the West and East Marks. In the end, this policy would seriously hurt companies in the new federal states. Together with Foreign Minister Hans-Dietrich Genscher, Kohl was able to resolve talks with the former Allies of World War II to allow German reunification. He received assurances from Gorbachev that a reunified Germany would be able to choose which international alliance it wanted to join, although Kohl made no secret that he wanted the reunified Germany to inherit West Germany's seats at NATO and the EC.
A reunification treaty was signed on 31 August 1990, and was overwhelmingly approved by both parliaments on 20 September 1990. On 3 October 1990, East Germany officially ceased to exist, and its territory joined the Federal Republic as the five states of Brandenburg, Mecklenburg-Vorpommern, Saxony, Saxony-Anhalt and Thuringia. These states had been the original five states of East Germany before being abolished in 1952, and had been reconstituted in August. East and West Berlin were reunited as the capital of the enlarged Federal Republic. After the fall of the Berlin Wall, Kohl confirmed that historically German territories east of the Oder-Neisse line were definitively part of Poland, thereby relinquishing any claim Germany had to them. In 1993, Kohl confirmed, via treaty with the Czech Republic, that Germany would no longer bring forward territorial claims as to the pre-1945 ethnic German so-called Sudetenland. This treaty was a disappointment for the German Heimatvertriebene ("displaced persons").
Chancellor of reunified Germany.
Reunification placed Kohl in a momentarily unassailable position. In the 1990 electionsthe first free, fair and democratic all-German elections since the Weimar Republic eraKohl won by a landslide over opposition candidate and Minister-President of Saarland, Oskar Lafontaine. He then formed his fourth cabinet.
After the federal elections of 1994 Kohl was reelected with a somewhat reduced majority, defeating Minister-President of Rhineland-Palatinate Rudolf Scharping. The SPD was however able to win a majority in the Bundesrat, which significantly limited Kohl's power. In foreign politics, Kohl was more successful, for instance getting Frankfurt am Main as the seat for the European Central Bank. In 1997, Kohl received the Vision for Europe Award for his efforts in the unification of Europe.
By the late 1990s, the aura surrounding Kohl had largely worn off amid rising unemployment. He was heavily defeated in the 1998 federal elections by the Minister-President of Lower Saxony, Gerhard Schröder.
Retirement and legal troubles.
A red-green coalition government led by Schröder replaced Kohl's government on 27 October 1998. He immediately resigned as CDU leader and largely retired from politics. However, he remained a member of the Bundestag until he decided not to run for reelection in the 2002 election.
CDU finance affair.
Kohl's life after political office in the beginning was dominated by the CDU-party finance scandal. The party financing scandal became public in 1999, when it was discovered that the CDU had received and kept illegal donations during Kohl's leadership.
Life after politics.
In 2002, Kohl left the Bundestag and officially retired from politics. In recent years, Kohl has been largely rehabilitated by his party again. After taking office, Angela Merkel invited her former patron to the Chancellor's Office and Ronald Pofalla, the Secretary-General of the CDU, announced that the CDU will cooperate more closely with Kohl, "to take advantage of the experience of this great statesman", as Pofalla put it. On 5 July 2001, his wife, Hannelore, committed suicide, due to suffering from photodermatitis for many years. On 4 March 2004, he published the first of his memoirs, called "Memories 1930–1982", covering the period 1930 to 1982, when he became chancellor. The second part, published on 3 November 2005, included the first half of his chancellorship (from 1982–90). On 28 December 2004, he was air-lifted by the Sri Lankan Air Force, after having been stranded in a hotel by the 2004 Indian Ocean earthquake. Kohl is a member of the Club of Madrid.
As reported in the German press, he also gave his name to the soon-to-be launched Helmut Kohl Centre for European Studies (currently Centre for European Studies), which is the new political foundation of the European People's Party. In late February 2008, Kohl suffered a stroke in combination with a fall which caused serious head injuries and required his hospitalization, since when he has been reported as bound to a wheelchair due to partial paralysis and with difficulty speaking. He has remained in intensive care since, marrying his 43-year-old partner, Maike Richter, on 8 May 2008, while still in hospital. In 2010, he had a gall bladder operation in Heidelberg, and heart surgery in 2012. He was reportedly in "critical condition" in June 2015, following intestinal surgery following a hip-replacement procedure.
In 2011, Kohl, in spite of his frail health, began giving a number of interviews and issued statements in which he sharply condemned his successor Angela Merkel, whom he had formerly mentored, on her policies in favor of strict austerity in the European debt crisis and later also towards Russia in the Ukrainian crisis, which he sees as opposed to his politics of peaceful bi-lateral European integration during his time as chancellor. He has published the book "Aus Sorge um Europa" ("Out of Concern for Europe") outlining these criticisms of Merkel (while also attacking his immediate successor Gerhard Schröder's Euro policy) and was widely quoted in the press as saying, ""Die macht mir mein Europa kaputt."" ("She's destroying the Europe that I have built."). Kohl thus joined former German chancellors Gerhard Schröder and Helmut Schmidt in their similar criticisms of Merkel's policies in these two fields. On 19 April 2016, Kohl was visited in his Oggersheim residence by Hungarian Prime Minister Viktor Orbán. The two had a one-hour conversation and released a joint press statement regarding the European migrant crisis, saying that both doubted that Europe was capable of continuing to absorb refugees indefinitely. Before the meeting, it had widely been interpreted as criticism of Angela Merkel's handling of the crisis, but eventually, Kohl and Orban refrained from attacking the chancellor directly, writing: "It is about a good future for Europe and peace in the world. The efforts of point in the same direction."
Political views.
Kohl was committed to European integration, maintaining close relations with the French president Mitterrand. Parallel to this he was committed to German reunification. Although he continued the Ostpolitik of his social-democratic predecessors, Kohl supported Reagan's more aggressive policies in order to weaken the USSR. 
Media portrayals.
Kohl faced stiff opposition from the West German political left, and mocked for his provincial background, physical stature and simple language. Similar to historical French cartoons of Louis-Philippe of France, Hans Traxler depicted Kohl as a "pear" in the left-leaning satirical journal "Titanic". The German word "Birne" ("pear") became a widespread nickname and symbol for the Chancellor.
Honors and awards.
Helmut Kohl has received numerous awards and accolades, as well as honorary titles such as doctorates and citizenships. Among others, he was joint recipient of the Charlemagne Prize with French President François Mitterrand for their contribution to Franco-German friendship and European Union. In 1996, Kohl received the Prince of Asturias Award in International Cooperation from Felipe of Spain. In 1998, Kohl was named Honorary Citizen of Europe by the European heads of state or government for his extraordinary work for European integration and cooperation, an honor previously only bestowed on Jean Monnet.

</doc>
<doc id="41901" url="https://en.wikipedia.org/wiki?curid=41901" title="Helmut Schmidt">
Helmut Schmidt

Helmut Heinrich Waldemar Schmidt (; 23 December 1918 – 10 November 2015) was a German statesman and member of the Social Democratic Party of Germany (SPD), who served as Chancellor of the Federal Republic of Germany from 1974 to 1982.
Before becoming Chancellor, he had served as Minister of Defence (1969–1972) and as Minister of Finance (1972–1974). In the latter role he gained credit for his financial policies. He had also served briefly as Minister of Economics and as acting Foreign Minister. As Chancellor, he focused on international affairs, seeking "political unification of Europe in partnership with the United States". He was an energetic diplomat who sought European co-operation and international economic co-ordination. He was re-elected chancellor in 1976 and 1980, but his coalition fell apart in 1982 with the switch by his coalition allies, the Free Democratic Party.
He retired from Parliament in 1986, after clashing with the SPD's left wing, who opposed him on defence and economic issues. In 1986 he was a leading proponent of European monetary union and a European Central Bank.
Background, family, early life and education.
Helmut Schmidt was born as the eldest of two sons of teachers Ludovica Koch and Gustav Ludwig Schmidt (d. 1981) in Barmbek, a rough working-class district of Hamburg, in 1918. Schmidt studied at Hamburg Lichtwark School, graduating in 1937. Schmidt's father was born the biological son of a German Jewish banker, Ludwig Gumpel, and a Christian waitress, Friederike Wenzel, and then covertly adopted, although this was kept a family secret for many years. This was confirmed publicly by Schmidt in 1984, after Valéry Giscard d'Estaing revealed the fact to journalists, apparently with Schmidt's assent. Schmidt himself was a non-practising Lutheran.
Schmidt was a group leader (Scharführer) in the Hitler Youth organization until 1936, when he was demoted and sent on leave because of his anti-Nazi views. On 27 June 1942, he married his childhood sweetheart Hannelore "Loki" Glaser (3 March 1919 – 21 October 2010). They had two children: Helmut Walter (26 June 1944 – February 1945, died of meningitis), and Susanne (b. 1947), who works in London for Bloomberg Television. Schmidt resumed his education in Hamburg after the war, graduating in economics and political science in 1949.
Military service.
Schmidt was conscripted into military service in 1937, and began serving with an anti-aircraft battery at Vegesack near Bremen during World War II. After brief service on the Eastern Front during the invasion of the Soviet Union in 1941, including the Siege of Leningrad, he returned to Germany in 1942 to work as a trainer and advisor at the Ministry of Aviation. During his service in World War II, Schmidt was awarded the Iron Cross 2nd Class. He attended the People's Court as a military spectator at some of the show trials for officers involved in the 20 July plot, where an unsuccessful attempt was made to assassinate Hitler at Rastenburg, and was disgusted by Roland Freisler's conduct. Toward the end of the war, from December 1944 onwards, he served as an Oberleutnant in the Flakartillery on the Western Front during the Battle of the Bulge and the Ardennes Offensive. He was captured by the British in April 1945 on Lüneburg Heath, and was a prisoner of war until August of that year in Belgium.
Political career.
Early years.
Schmidt joined the Social Democratic Party of Germany (SPD) in 1946, and from 1947 to 1948 was the leader of the Socialist German Student League, the student organisation of the SPD. Upon graduating from the University of Hamburg, where he read economics, he worked for the government of the city-state of Hamburg, working in the department of economic policy. Beginning in 1952, under Karl Schiller, he was a senior figure heading up the "Behörde für Wirtschaft und Verkehr" (the Hamburg State Ministry for Economy and Transport).
He was elected to the "Bundestag" in 1953, and in 1957 he became a member of the SPD parliamentary party executive. A vocal critic of conservative government policy, his outspoken rhetoric in parliament earned him the nickname "Schmidt-Schnauze" ("Schmidt the Lip"). In 1958, he joined the national board of the SPD ("Bundesvorstand"), and campaigned against nuclear weapons and the equipping of the "Bundeswehr" with such devices. He alarmed some in his party by taking part in manoeuvres as a reserve officer in the newly formed Bundeswehr. In 1962, he gave up his seat in parliament to concentrate on his tasks in Hamburg.
Senator.
The government of the city-state of Hamburg is known as the Senate of Hamburg, and from 1961 to 1965, Schmidt was the "Innensenator": the senator of the interior. He gained a reputation as a "Macher" (doer) – someone who gets things done regardless of obstacles – by his effective management during the emergency caused by the 1962 flood, during which 300 people drowned. Schmidt used all means at his disposal to alleviate the situation, even when that meant overstepping his legal authority, including employing the federal police and army units (ignoring the German constitution's prohibition on using the army for "internal affairs"; a clause excluding disasters was not added until 1968). Describing his actions, Schmidt said, "I wasn't put in charge of these units – I took charge of them!" He saved a further 1,000 lives and swiftly managed the re-housing of thousands of the homeless.
Return to federal politics.
In 1965, he was re-elected to the Bundestag. In 1967, after the formation of the Grand Coalition between the SPD and the Christian Democratic Union (CDU), he became chairman of the Social Democratic parliamentary party, a post he held until the elections of 1969. In 1968, he was elected deputy party chairman, a post that he held until 1984. Between 1968 to 1983, Schmidt was deputy chairman of the SPD. Unlike Willy Brandt and Gerhard Schröder, he never became chairman of the party.
In October 1969, he entered the government of Willy Brandt as defense minister. During his term in office, the military conscription time was reduced from 18 to 15 months, while at the same time increasing the number of young men being conscripted. Additionally, Schmidt decided to introduce the Bundeswehr universities in Hamburg and Munich to broaden the academic education of the German officer corps, and the situation of non-commissioned officers was improved. In July 1972, he succeeded Karl Schiller as Minister for Economics and Finances, but in November 1972, he relinquished the Economics department, which was again made a separate ministry. Schmidt remained Minister of Finances, where he faced the prospect of rising inflation. Shortly before the Oil Shock of 1973, which rattled Britain and United States, Schmidt agreed that European currencies should be floated against the US Dollar. He remained in charge of finance until May 1974.
Chancellor.
Schmidt became Chancellor of West Germany on 16 May 1974, after Brandt's resignation in the wake of an espionage scandal. The worldwide economic recession was the main concern of his administration, and Schmidt took a tough and disciplined line, reducing public spending. Schmidt was also active in improving relations with France. Together with the French President Valéry Giscard d'Estaing, he was one of the fathers of the world economic summits, the first of which assembled in 1975. In 1975, he was a signatory of the Helsinki Accords to create the Conference for Security and Co-operation in Europe, the precursor of today's OSCE. In 1978 he helped set up the European Monetary System (EMS), known as the "Snake in the Tunnel".
He remained chancellor after the 1976 elections, in coalition with the liberal Free Democratic Party (FDP). He adopted a tough, uncompromising line with the indigenous Red Army Faction (RAF) extremists. In October 1977, he ordered an anti-terrorist unit of Bundesgrenzschutz soldiers to end the Palestinian terrorist hijacking of a Lufthansa aircraft named "Landshut", staged to secure the release of imprisoned RAF leaders, after it landed in Mogadishu, Somalia. Three of the four kidnappers were killed during the assault on the plane, but all 86 passengers were rescued unharmed.
Schmidt was re-elected as chancellor in November 1980. Concerned about the Soviet invasion of Afghanistan, and the Soviet superiority regarding missiles in Central Europe, Schmidt issued proposals resulting in the NATO Double-Track Decision, concerning the deployment of medium-range nuclear missiles in Western Europe, should the Soviets not disarm. This decision was unpopular with the German public. A mass demonstration against the deployment mobilized 400,000 people in October 1981.
At the beginning of his period as chancellor, Schmidt was a proponent of Keynesian economics, and pursued expansionary monetary and fiscal policies during his time as chancellor. Between 1979 and 1982, the Schmidt administration pursued such policies in an effort to reduce unemployment. These were moderately successful, as the fiscal measures introduced after 1977, with reductions in income and wealth taxes and an increase in the medium-term public investment programme, were estimated to have created 160,000 additional jobs in 1978–79, or 300,000 if additional public sector employment was included in the figure. The small fall in the unemployment rate, however, was achieved at the cost of a larger budget deficit (which rose from 31.2 billion DM to 75.7 billion DM in 1981), brought about by fiscal expansion.
During the 1970s, West Germany was able to weather the global financial storm far better than almost all the other developed countries, with unemployment and inflation kept at comparatively low levels. During the 1976 election campaign, the SPD/FDP coalition was able to win the battle of statistics, whether the figures related to employees' incomes, strikes, unemployment, growth, or public sector debts. Amongst other social improvements, retirement pensions had been doubled between 1969 and 1976, and unemployment pay increased to 68% of previous earnings.
While visiting Saudi Arabia in April 1981, Schmidt made some unguarded remarks about the Israel-Palestine conflict that succeeded in aggravating the always-delicate relations between Israel and West Germany. Asked by a reporter about the moral aspect of German-Israeli relations, he stated that Israel was not in a position to criticize Germany due to its handling of Palestinians, and "That won't do. And in particular, it won't do for a German living in a divided nation and laying moral claim to the right of self-determination for the German people. One must then recognize the moral claim of the Palestinian people to the right of self-determination." On 3 May, Israeli prime minister Menachem Begin denounced Schmidt as "unprincipled, avaricious, heartless, and lacking in human feeling", and stated that he had "willingly served in the German armies that murdered millions". Begin was also upset over remarks he (Schmidt) had made on West German television the previous week, in which he spoke apologetically about the suffering Germany inflicted on various nations during World War II, but made no mention of the Jews. While flying home from Riyadh, Schmidt told his advisers that war guilt could not continue to affect Germany's foreign relations.
Schmidt was the first world leader to call upon newly elected French president François Mitterrand, who visited Bonn in July 1981. The two found themselves in "complete agreement" on foreign policy matters and relations with the United States and the Soviet Union, but differed on trade and economic issues.
By the end of his term, however, Schmidt had turned away from deficit spending, due to a deteriorating economic situation, and a number of welfare cuts were carried out, including smaller increases in child benefits and higher unemployment and health contributions. Large sections of the SPD increasingly opposed his security policy, while most of the FDP politicians strongly supported that policy. While representatives of the left wing of the Social Democratic Party opposed reduction of the state expenditures, the FDP began proposing a monetarist economic policy. In February 1982, Schmidt won a motion of confidence; however on 17 September 1982, the coalition broke apart, with the four FDP ministers leaving his cabinet. Schmidt continued to head a minority government composed only of SPD members, while the FDP negotiated a coalition with the CDU/CSU. During this time, Schmidt also headed the Ministry of Foreign Affairs. On 1 October 1982, parliament approved of a Vote of No-Confidence and elected the CDU chairman Helmut Kohl as the new chancellor. This was the only time in the history of the Federal Republic that a chancellor was ousted from office in this way.
In October 1974, a Rehabilitation Benefits Alignment Act was passed, with the intention of promoting rehabilitation of the handicapped by extending certain benefits to them. To meet the need for more uniform medical treatment in rural areas and on the peripheral of cities due to a lack of panel doctors in those areas, a bill was passed in December 1976 which improved the possibilities of panel doctors' associations by ensuring that panel doctors were available to provide treatment, while also providing for planning according to need and the participation of the sickness insurances. An Act of August 1975 on criminal law reform introduced "other forms of assistance" such as medical advice on contraception, together with assistance pertaining to sterilisation and abortion. New assistance benefits were created in 1975 for family planning and maternity consultations, whilst a constant attendance allowance was increased. Housing renovation and energy savings legislation was introduced in 1977, while a constitutional reform of 1981 increased federal powers in health and education.
In July 1974, special benefits were introduced to compensate for wages not paid as a result of bankruptcy for a maximum of up three months. Increases in income-limits for housing allowances were carried out, together with housing allowance rates, while major improvements were made in welfare provision for the elderly. By 1982, the purchasing power of the average pension was 2.5% better than in 1975. In 1975, tax allowances were replaced by child benefits, while payment for the first child was introduced. A tax relief act reduced income taxes and provided additional tax benefits for housing allowances. The Schmidt administration also introduced social policy legislation in the late 1970s, which increased family allowances (though by a smaller amount than in 1974) and maternity leave benefits. The increases in benefits under the Schmidt administration arguably had a positive impact on reducing inequalities, with the percentage of West Germans living in poverty (according to one measurement) falling between 1978 and 1982.
Under the law of June 1974, the residents could participate in the management of the establishment through a consultative committee. A law of June 1975 amended the Employment Protection Law and the Law on the provision of temporary workers which improved the legal protection of temporary migrants workers in West Germany. A law of December 1975 gave the right to claim under the sickness insurance scheme for medical consultations for family planning purposes. A law of May 1975 extended social security to handicapped persons according to various procedures.
Socialist measures for Youth and Unemployment.
A law of April 1976 on youth employment limited working hours to 40 hours in a 5-day week, raised the minimum working age from 14 to 15, increased leave, improved conditions for release from work for day attendance at vocational training school and for periods of weeks under the block release system, and improved protection at work by restrictions on employment in dangerous or unhealthy work. The general section of the Social Code, which came into effect in January 1976, introduced basic measures concerning the social services. It laid down an obligation to establish the services and institutions needed by the population and to provide them with information and advice on their social rights. These provisions had already had certain effects, in particular a considerable growth in home help services and social centres. A regulation in application of a 1974 law on old people's homes and adult hostels was introduced, according to which compulsory consultative committees could be set up by the residents to ensure their participation in the running of these establishments in a greater measure than in the past. A law passed in August 1974 supplemented the protection provided for handicapped people, under a law passed during the Brandt Administration in April 1974, by providing that, henceforth, the benefits for the purposes of medical and occupational rehabilitation would be the same for all the categories of persons concerned: war victims, the sick, the victims of industrial accidents, congenitally handicapped persons: a total of about 4 million persons in all.
Various measures were also carried out to mitigate the effects of unemployment. Employment creation schemes were introduced to help young workers. The Training Opportunities Act (1976) helped (over a four-year period) to increase the number of vocational training places from 450,000 to 630,000 a year. In 1976, a provisional law was introduced to boost the number of apprentices, which reduced the numbers of young people out of work. An experimental retraining programme was launched on the shop floor (lasting from 1979 to 1981), which benefited 45,680 people.
A special programme was introduced, specially designed for young people who, because of their poor level of education and language ability, were unable to find a suitable job or training place. The young people were offered a one-year full-time course of training to qualify them for a training place or job, and in September 1980, approximately 15,000 young people were participating in these courses. From 1980 onwards, parents could deduct the cost of day care for their children (in day nurseries and nursery schools in particular) from their taxable income up to an annual maximum of DM 600 or DM 1,200, depending on whether the income of a single parent or that of a married couple was involved. Major additions were also made to the regulations on dangerous substances, while comprehensive new regulations concerning installations requiring supervision were introduced. The Federal Ministry for Youth, Family Affairs and Health gave particular attention to assisting parents in assuming their educational responsibilities towards their children. For instance, special 'letters to parents' were distributed free of charge to parents of children under 8, with some 3 million sent in 1979. A determined effort was also made to provide better education for socially disadvantaged children by supporting pilot schemes and research projects. Public funds had been allocated from 1979 onwards to a pilot scheme entitled 'Aid to children in need' under which children's communities were set up in Berlin and Giitersloh to protect and care for children who had been or were at risk of being ill-treated by their parents, while at the same time the family education and advisory services were assigned the task of educating these parents.
Enshrining constitutional rights for workers.
The Fifth Amendment of July 1979 to the Employment Promotion Law provided, among other measures, for an improvement in conditions governing financial support towards basic vocational training for unemployed young people with at least one year's vocational experience, the expansion of training activities for jobs in which there is a shortage of skilled workers and easier access to further vocational training facilities for problem groups (such as the unskilled, the unemployed, and women generally). In 1979, the Federal Minister for Education and Science made funds available for a new further education establishment to train instructors. Under a law amending the law respecting technical working media and the Industrial Code of August 1979, machines and equipment which had been voluntarily submitted for testing and passed by an established body may bear the marking 'GS' (=safety-tested). For medical equipment, the Federal Minister of Labour and Social Affairs was authorized to issue orders containing further safety provisions, while the resale of hazardous equipment and its display at exhibitions may be prohibited in future by factory inspectors even in the case of trading companies.
Urban housing developments.
The 1976 Act for the Promotion of Urban Development and the 1977 Housing Modernisation Act, together with the 1971 Act for the Promotion of Urban Development passed by the Brandt Administration, enabled most West German cities by the end of the 1970s to introduce programmes aimed at renovating their pre-war residential areas. Additional tax reforms were introduced that lowered the tax burden on low-income households, and which played an important role "in pre-empting a real decline in the income and purchasing power of workers". A law was passed to encourage low-income home ownership, while 250 million marks was provided in 1978 for the promotion of sports and physical education. That same year, entitlement to educational allowances was extended to all tenth-grade pupils in vocational education. In 1979, DM 219 million was set aside for about 80,000 dwellings under the modernisation programme for dwellings worthy of preservation run jointly by the Federal authorities and the individual Lander (50% of this money was earmarked for modernization priority areas). In addition, DM 2,350 million was made available under a five-year programme to improve the housing stock. Loans and higher tax rebates were also used to encourage modernisation of dwellings and energy-saving measures. 577 slum clearance and urban development schemes in 459 municipalities were also accorded financial support amounting to DM 183.5 million, under a law on the promotion of urban development. A law of October 1979 granted a lump-sum allowance for the winter of 1979/80 to help low-income groups to meet the additional outlay incurred by the rise in fuel costs. In August 1979, a programme was adopted for foreign refugees, with resources allocated for aid concerning information, legal advice, psycho-social and medical assistance and for measures to facilitate the integration of refugees or their emigration to other countries. In 1981, DM 340 million were set aside for subsidies and DM 148 million for low-interest loans, which enabled financial assistance to be granted towards the modernization of some 80 000 dwellings.
Industrial compliance standards.
An amendment to a law of September 1980 on air traffic, adopted in January 1981, prohibited the transport of radioactive substances by air without a special permit. Existing safety regulations were considerably extended and modified by the technical committees responsible for individual specialist areas. Regarding installations requiring supervision, the technical regulations for pressure containers (19 January 1982) and steam boilers (26 January 18 March and 8 June 1982) were extended and revised, with their most important provisions concerning the oil- and gas-firing of steam boilers. A Directive on connecting lines designed to carry dangerous fluids (11 June 1982) was issued, together with technical regulations on pressure gases (11 June and 9 July 1982) The existing technical regulations on flammable fluids were also modified and by means of new regulations and directives extended (19 April 1982). Other modifications were made to the technical regulations on high-pressure gas pipelines (22 June and 10 September 1982) and on installations. where acetylene is present and calcium carbide is stored (30 September 1982), while new recommended levels for dangerous working substances were incorporated into the regulations governing these substances (10 May 1982).
Workers' pension reform.
The Introductory Tax Reform Law (1974) increased bad weather payments, part-time workers' benefits and insurance benefits to 68% of net wages, fixed special benefits during vocational training at 90% of net earnings, increased assistance benefits to 58% of net earnings, and abolished special family benefits "in favour of the inclusion of the unemployed under general child allowance scheme". A special tax credit was introduced in 1978 in cases of particular financial burden due to children, while a substantial increase in the child allowance was made in 1979. Several policy changes were carried out between 1976 and 1982, such as tax credits and family allowances, which compensated unions for wage restraint and "guaranteed the maintenance of a constant income level for employed persons and their families". Increases were made in child benefits, which rose on a regular basis (particularly for families with more than one child) for most of the years that the Schmidt Administration was in office.
In terms of workplace rights, a "parity" system was introduced (although in a weakened form) on the supervisory boards of all companies employing over 2,000 workers, a reform which West German trade unions had long fought for. This law improved employee representation on the supervisory boards of companies outside the steel and coal industries. The main provision of this new piece of legislation was that in the 650 major companies that accounted for 70% of West Germany's output, employee representation on the supervisory boards rose from one-third to one-half. In 1976, the Young Persons (Protection of Employment) Act was passed, which forbade the employment of children and young persons required to attend full-time education, with minor exceptions.
Product liability and consumer protection.
In June 1974, a reformed food law was passed into law, which aimed to safeguard consumers from physical harm. The Students' Sickness Insurance Law (1975) extended compulsory coverage to students (medical benefits only), while the Artists' Social Insurance Law (1981) introduced compulsory insurance for artists below a certain income-limit. The Detergents Law (1975) and the Effluency Levies Act (1978) were passed to encourage environmental protection. In 1975, the allowable duration of unemployment benefit payment was extended to 24 months during periods of general recession. The 1976 law on standard terms of sale gave consumer groups the right to file suits against companies employing unfair terms of sale. The Higher Education Framework Act of 1976 pronounced that scientific continuing education was a task to be implemented by the institutions of the system of higher education, thus exceeding their traditional tasks of research and lecturing. In 1977, an "investment programme for the future" was decided upon by the Schmidt Administration, which provided DM 16 thousand million for the improvement of the transport system, an efficient and ecological energy supply, provisions for water supply, vocational training, and the safeguarding of the environment.
What was the meaning of social protection?
The social protection of civil servants and judges (Bund and Lander) was standardised and improved by a law of August 1974. Under a law of May 1976, victims of acts of violence and their survivors would in future have the right to compensation in respect of the physical and economic consequences in the same manner as protection for war victims. In 1977, DM 8 million was made available by the federal government to welfare bodies to build and modernise holiday homes for families. That same year, the conditions for investment in the privately financed construction of rented dwellings were improved by the reintroduction of decreasing depreciation for buildings. In order to take the situation of the unemployed into account to the maximum possible extent in asset formation policy, certain legal provisions were amended so that in the event of unemployment, personal payments could be made to continue savings plans which entailed employers contributions. In addition, workers who had been unemployed for a year or more could unblock savings plans before the end of the freeze without losing the financial benefits offered by the State. A new special programme with funds of DM 100 million was launched at the start of 1978 to improve training and job opportunities for the handicapped. The budget of the Federal Labour Office was increased exceptionally by more than 20%, whilst special emphasis was placed on measures to promote vocational training, job creation, advanced training and retraining. The aim was to reduce the high proportion of unemployed persons lacking training and increase the chances of this group to obtain employment.
Under a regulation of December 1976, four new occupational diseases were recognised. To expand training opportunities for girls, a pilot scheme was launched in 1978 to open up certain skilled industrial and technical occupations to them. Laws restricting the access of migrant workers to certain regions were repealed in 1977, and the existing provisions were made more flexible in order to allow the children of migrant workers who had entered the Federal Republic of Germany in 1975/76 access to employment. Legislation governing old people's homes and adult assistance establishments was further supplemented by two regulations, one imposing minimum requirements concerning premises, and the other laying down rules for financial management to ensure that residents were not financially exploited.
Under a law of July 1980, a farmer's surviving spouse wishing to continue working on the farm could obtain a helper or temporary aid from the agricultural pension fund. Any spouse choosing not to do so was entitled to a survivor's allowance if he or she was no longer able to find suitable paid employment either for reasons of age (over 45) or because there were children to bring up. In other cases, the allowance was designed to facilitate reintegration into working life. This allowance guaranteed the spouse protection under the agricultural sickness insurance scheme, which also covered self-employed fishermen and beekeepers.
Divorce reform.
A wide range of social liberal reforms were also carried out during Schmidt's time in office. A marriage and divorce law of 1976 instituted the principle of maintenance obligations of each economically stronger partner, That same year, A reform of naming for partners after marriage was carried out, together with a reform of marriage law, which eliminated "moral guilt" as a criterion for alimony payment obligations. The First Marriage Reform Law of 1976 stated that pension entitlements acquired during marriage must be shared with the economically weaker spouse following divorce. In 1977, a law was introduced which enabled married women to enter employment without the permission of their husbands, while prison reforms guaranteed inmates access to courts for any violations of their rights, limited sentences in all but the gravest cases to 15 years, and proclaimed rehabilitation to be the objective of incarceration. In 1977, a Sex Discrimination Act was passed. In 1981, a legal aid system was established to facilitate access to courts of law.
Implications for a socialist Europe.
An amendment to the legal code for residency permits was made in 1978, which granted foreign residents the right to unlimited residence permits after five years of continuous residency. The amendment also stated that legal residents would be eligible for a residence entitlement after eight years if certain conditions were met, such as language fluency. In 1979, paid parental leave was extended from 2 to 6 months, while the European directive on equal treatment for women in paid employment was adopted that same year. The Maintenance Security Law of 1979 introduced public advance payments for single parents "not in receipt of maintenance payments from the liable parent". These benefits were made payable up to 36 months, and private claims against a parent not meeting a maintenance liability were taken over by the state. In that same year, four months paid parental leave were introduced for working mothers, while job-protected leave after childbirth was increased from 8 weeks to 6 months. In 1980, a "compliance law" was passed that covered discrimination in hiring, promotion and dismissal, and measures to promote equal pay.
Life after politics.
In 1982, along with his friend Gerald Ford, he co-founded the annual AEI World Forum. The following year he joined the nationwide weekly "Die Zeit" newspaper as co-publisher, also acting as its director from 1985 to 1989. In 1985, he became managing director. With Takeo Fukuda he founded the Inter Action Councils in 1983. He retired from the "Bundestag" in 1986. In December 1986, he was one of the founders of the committee supporting the EMU and the creation of the European Central Bank.
Contrary to the line of his party, Schmidt was a determined opponent of Turkey's bid to join the EU. He also opposed phasing out nuclear energy, something that the Red-Green coalition of Gerhard Schröder supported. In 2007, Schmidt described the climate debate as "hysterically overheated". When asked about social media, Schmidt said he perceived the internet as "threatening". He was particularly concerned about the superficiality of communication on the web.
In 2014, Schmidt said the situation in Ukraine was dangerous, because "Europe, the Americans and also Russia are behaving in a way that Christopher Clark, described in his book "The Sleepwalkers: How Europe Went to War in 1914" that's very much worth reading, as the beginning of World War I: like sleepwalkers."
Schmidt was the author of numerous books on his political life, on foreign policy, and political ethics. He made appearances in numerous television talk shows, and remained one of the most renowned political publicists in Germany until his death.
Friendships.
Schmidt described the assassinated Egyptian president Anwar Sadat as one of his friends from the world of politics, and maintained a friendship with ex-president Valéry Giscard d'Estaing of France. His circle also included former U.S. Secretary of State Henry Kissinger, who went on record as stating that he wished to predecease Helmut Schmidt, because he would not wish to live in a world without him.
He was also good friends with former Canadian Prime Minister Pierre Trudeau. In 2011, Schmidt made a pilgrimage to the Trudeau family vault in St-Rémi-de-Napierville Cemetery, accompanied by Jean Chrétien and Tom Axworthy.
Personal life.
Schmidt admired the philosopher Karl Popper, and contributed a foreword to the 1982 Festschrift in Popper's honor.
Schmidt was a talented pianist, and recorded piano concertos of both Mozart and Bach with German pianist and conductor Christoph Eschenbach. Schmidt recorded Mozart's piano concerto for three pianos, K. 242, with the London Philharmonic Orchestra directed by Eschenbach in 1982 with pianists Eschenbach and Justus Frantz for EMI Records (CDC 7 47473 2). In that recording, according to the CD's liner notes, Schmidt played the part written for Countess Antonia Lodron's youngest daughter Giuseppina, "almost a beginner" who commissioned the work. The part brilliantly "enables any reasonably practiced amateur to participate in a performance". The same musical notes also indicate that Schmidt and Frantz had played duets during Frantz's student days. In 1990 Schmidt joined Eschenbach, Frantz, Gerhard Oppitz and the Hamburg Philharmonic Orchestra in Deutsche Grammophon's recording of Bach's Concerto in A minor for four harpsichords, BWV 1065.
All his adult life, Schmidt was a heavy smoker. He was well known for lighting up during TV interviews and talk shows. In October 1981, Schmidt was fitted with a cardiac pacemaker.
In January 2008, German police launched an inquiry after an anti-smoking initiative charged that Schmidt was defying the recently introduced smoking ban. The initiative claimed that the ex-chancellor had been flagrantly ignoring anti-smoking laws. Despite pictures in the press, the case was subsequently dropped after the public prosecutor's office ruled that Schmidt's actions had not been a threat to public health.
On 6 April 2010, with a lifespan of 33,342 days, he surpassed Konrad Adenauer in terms of longevity, and at the time of his death was the oldest former chancellor in German history.
His wife of 68 years, Loki Schmidt, died on 21 October 2010, aged 91.
At the beginning of August 2012, Schmidt gave an interview on German television and revealed that at 93 years of age, he had fallen in love again. His new life-partner was his long-standing associate Ruth Loah, 79.
Death and state funeral.
In September 2015, Schmidt had to undergo surgery for a blood clot in his leg. After initial improvement, his condition worsened again in November, with his doctor saying he "feared for the worst". Schmidt died in his Hamburg home on the afternoon of 10 November 2015, aged 96.
A state funeral for Schmidt was held on 23 November at the Protestant (Lutheran) St. Michael's Church, Hamburg, where Loki Schmidt's funeral had been held. German Chancellor Angela Merkel, in remarks to mourners, said, "He will be missed. He was an astute observer and commentator, and it was with good reason that he had a reputation for dependability." Others who spoke included former U.S. Secretary of State Henry Kissinger. Speaking in German, he lauded Schmidt for "vision and courage", based on the principles of "reason, law, peace and faith," and said Schmidt had been "a kind of world conscience."
Among the 1,800 who attended were German President Joachim Gauck and former French President Valéry Giscard d'Estaing, whose tenure in office paralleled Schmidt's as German chancellor. Other guests included former chancellor Gerhard Schröder, former presidents Christian Wulff, Horst Köhler, Roman Herzog and Hamburg's mayor Olaf Scholz. A flag-draped coffin containing the remains of the former chancellor, also a former German defense minister, was escorted by the German Army's Wachbataillon from St. Michael's to Ohlsdorf Cemetery for a private interment ceremony. Helmut Schmidt's remains were buried there one day later, in the family grave alongside the remains of his parents and his wife, Loki.
Honours and awards.
Helmut Schmidt received a number of accolades, among them was the Grand Cross Order of Merit of the Federal Republic of Germany, which he chose not to accept in Hanseatic tradition, in order to refuse any decoration presented for merely fulfilling one's duty.
In 2003, the university of Germany's federal armed forces in Hamburg was renamed Helmut Schmidt University – University of the Federal Armed Forces Hamburg in 2003, in honour of the politician whoas minister of defensehad introduced mandatory academic education for German career officers.
Honorary degrees.
Throughout his tenure as chancellor, and even thereafter, Helmut Schmidt received 24 honorary degrees. They include degrees from the British universities Oxford and Cambridge, Paris Sorbonne, the American Harvard and Johns Hopkins universities, the Belgian Katholieke Universiteit Leuven, and the Keio University in Japan.

</doc>
<doc id="41902" url="https://en.wikipedia.org/wiki?curid=41902" title="Ludwig Erhard">
Ludwig Erhard

Ludwig Wilhelm Erhard (; 4 February 1897 – 5 May 1977) was a German politician affiliated with the CDU and the 2nd Chancellor of the Federal Republic of Germany (West Germany) from 1963 until 1966. He is often famed for leading German postwar economic reforms and economic recovery (""Wirtschaftswunder,"" German for "economic miracle") in his role as Minister of Economics under Chancellor Konrad Adenauer from 1949 to 1963. During that period he promoted the concept of the social market economy ("soziale Marktwirtschaft"), on which Germany's economic policy in the 21st century continues to be based. In his tenure as chancellor, however, Erhard failed to win confidence in his handling of a budget deficit and his direction of foreign policy, and his popularity waned. He resigned his chancellorship on 1 December 1966.
Life and work.
Born in Fürth, Kingdom of Bavaria, Erhard was a commercial apprentice from 1913 to 1916. After his apprenticeship he worked as retail salesman in his father's draper's shop.
In 1916, during World War I, he joined the German forces as an artilleryman. He fought in Romania and was seriously injured near Ypres in 1918. Because of his injury he could no longer work as a draper and started learning economics. He received his Ph.D. from the University of Frankfurt in 1925, for a dissertation written under Franz Oppenheimer.
During his time in Frankfurt he married Luise Lotter (1893–1975), widow Schuster, on 11 December 1923. After his graduation they moved to Fürth and he became executive in his parents' company in 1925. After three years he became assistant at the "Institut für Wirtschaftsbeobachtung der deutschen Fertigware", a marketing research institute. Later, he became deputy director of the institute.
During World War II, he worked on concepts for a postwar peace; however, officially such studies were forbidden by the Nazis, who had declared "total war". As a result, Erhard lost his job in 1942 but continued to work on the subject by order of the "Reichsgruppe Industrie." In 1944 he wrote "War Finances and Debt Consolidation" (orig: "Kriegsfinanzierung und Schuldenkonsolidierung"). In this study he assumed that Germany had already lost the war. He sent his thoughts to Carl Friedrich Goerdeler, a central figure in the German resistance against the Nazi government, who recommended Erhard to his comrades. Erhard discussed his concept with Otto Ohlendorf, deputy secretary of state in the Reichsministerium für Wirtschaft, as well. Ohlendorf himself spoke out for "active and courageous entrepreneurship (aktives und wagemutiges Unternehmertum)", which was intended to replace bureaucratic state planning of the economy after the war. Erhard was an outsider who supported the resistance, who personally and professionally rejected Nazism, and who endorsed efforts to effect a sensitive, intelligent approach to economic revival during the approaching postwar period.
Postwar.
After the war Erhard became an economic consultant. Under the Bizone established by the American and British administration in 1947, he led the "Sonderstelle Geld und Kredit" (Special Office for Money and Credit), an expert commission preparing the currency reform in Germany's western zones of occupation. The commission began its deliberations in October 1947, and in April 1948 produced the so-called Homburg plan, elements of which were adopted by the Allies in the currency reform that set the stage for the recovery of the economy.
In April 1948 Erhard was elected director of economics by the Bizonal Economic Council. On 20 June 1948, the Deutsche Mark was introduced. Erhard abolished the price-fixing and production controls that had been enacted by the military administration. This exceeded his authority, but he succeeded with this step.
Minister of Economics.
In the first free elections following the Nazi era, Erhard stood for election in a Baden-Württemberg district and was elected. He was appointed Minister for Economics, a position he would hold for the next 14 years; from 1957 to 1963 he was also the 2nd Vice-Chancellor of Germany.
A staunch believer in economic liberalism, Erhard joined the Mont Pelerin Society in 1950 and used this influential body of liberal economic and political thinkers to test his ideas for the reorganization of the West German economy. Some of the society's members were members of the Allied High Commission and Erhard was able to make his case directly to them. The Mont Pélerin Society welcomed Erhard because this gave its members a welcome opportunity to have their ideas tested in real life. Late in the 1950s, Erhard's ministry became involved in the struggle within the society between the European and the Anglo-American factions, and sided with the former. Erhard viewed the market itself as social and supported only a minimum of welfare legislation. However Erhard suffered a series of decisive defeats in his effort to create a free, competitive economy in 1957; he had to compromise on such key issues as the anti-cartel legislation. Thereafter, the West German economy evolved into a conventional welfare state from the basis that had been already laid in the 1880s by Bismarck. According to Alfred Mierzejewski the generally accepted view is that Germany has a social market economy, that the post-war German economy has evolved since 1948, but the fundamental characteristics of that economic system have not changed, while in his opinion the social market economy had begun to fade in 1957, disappearing entirely by the late 1960s.
In July 1948, a group of southwest German businessmen had attacked the restrictive credit policy of Erhard as Economic Director. While Erhard had designed this policy to assure currency stability and stimulate the economy via consumption, business feared the scarcity of investment capital would retard economic recovery. Erhard was also deeply critical of a bureaucratic-institutional integration of Europe on the model of the European Coal and Steel Community.
Erhard decided, as economic director for the British and American occupation zones, to lift many price controls in 1948, despite opposition from both the social democratic opposition and Allied authorities. Erhard's financial and economic policies soon proved widely popular as the German economy made a miracle recovery to rapid growth and widespread prosperity in the 1950s, overcoming wartime destruction and successfully integrating millions of refugees from the east.
Chancellor.
After the resignation of Adenauer in 1963, Erhard was elected chancellor with 279 against 180 votes in the Bundestag on 16 October. In 1965, he was re-elected. From 1966 to 1967, he also headed the Christian Democratic Union as "de facto" chairman, despite the fact that he was never a member of that party (which made his election to the chairmanship irregular and void "de jure"), as he never formally filed a membership application despite pressures from Chancellor Adenauer. The reasons for Erhard's reluctance are unknown, but it is probable that they stemmed from Erhard's general scepticism about party politics. However, Erhard was regarded and treated as a long-time CDU member and as the party chairman by almost everyone in Germany at the time, including the vast majority of the CDU itself. The fact that he was not a member was known only to a very small circle of party leaders at the time, and it did not become known to the public until the year 2007, when the silence was finally broken by Erhard's close advisor Horst Wünsche.
In domestic policy, a number of progressive reforms were carried out during Erhard's time as chancellor. In education, the number of years of compulsory education was extended, spending on schools was significantly increased, and a standardisation of the school system among the Lander (Hamburg Agreement) was carried out. In the field of social security, Housing Benefit was introduced in 1965, while federally funded child allowances for two or more children were introduced a year earlier.
Foreign policy.
Erhard explored using money to make possible reunification of Germany. Despite Washington's reluctance, Erhard envisaged offering Nikita Khrushchev, the leader in Moscow, massive economic aid in exchange for more political liberty in East Germany and eventually for reunification. Erhard believed that if West Germany were to offer a "loan" worth $25 billion US to the Soviet Union (which Erhard did not expect to be repaid), then the Soviet Union would permit German reunification. The acting American Secretary of State George Wildman Ball described Erhard's plan to essentially buy East Germany from the Soviet Union as "half-baked and unrealistic." Erhard's objective corresponded in time with Khrushchev rethinking his relations to West Germany. The Soviet leader secretly encouraged Erhard to present a realistic proposal for a 'modus vivendi' and officially accepted the chancellor's invitation to visit Bonn. However, Khrushchev fell from power in October 1964, and nothing developed. Perhaps more importantly, by late 1964, the Soviet Union had received a vast series of loans from the international money markets, and no longer felt the need for Erhard's money.
Erhard believed the major world problems were soluble through free trade and the economic unity of Europe (as a prerequisite for political unification); he alienated French president Charles de Gaulle, who wanted the opposite. Support for the American role in the Vietnam War proved fatal for Erhard's coalition. Through his endorsement of the American goal of military victory in Vietnam, Erhard sought closer collaboration with Washington and less with Paris. Erhard's policy complicated Allied initiatives toward German unification, a dilemma that the United States placed on the back burner as it focused on Southeast Asia. Erhard failed to understand that American global interests—not Europe's needs—dictated policy in Washington, D.C., and he rejected Adenauer's policy of fostering good relations with both the United States and France in the pursuit of West German national interest. Faced with a dangerous budget deficit in the 1966–1967 recession, Erhard fell from office in part because of concessions that he made during a visit to U.S. President Lyndon B. Johnson.
In 1961, while vice president, Johnson had hosted Konrad Adenauer some two years before the German statesman vacated the chancellorship of the German Federal Republic. In December 1963, less than a month after he had assumed the American presidency upon the assassination of John F. Kennedy, Johnson staged the first ever presidential barbecue in Erhard's honor. The event was held in and about the Stonewall Elementary School gymnasium in Stonewall in the Texas Hill Country. Among the entertainers was the internationally known concert pianist Van Cliburn, who appeared in a business suit, rather than his usual formal wear. As a member of the Texas House of Representatives, Samuel Ealy Johnson, Jr., Johnson's father, been sensitive to his German-American constituency and had opposed the Creel Committee's attempt to disparage German culture and isolate German-Americans during World War I. Adenauer and Erhard had also stayed at Johnson's ranch in Gillespie County.
Erhard's fall suggested that progress on German unification required a broader approach and a more active foreign policy. Chancellor Willy Brandt in the late 1960s abandoned the Hallstein Doctrine of previous chancellors and employed a new "Ostpolitik," seeking improved relations with the Soviet Union and Eastern Europe and thereby laying the groundwork for détente and coexistence between East and West. In the 1980s Chancellor Helmut Kohl, however, reverted to Erhard's approach in collaborating with the Reagan administration in its hard-line anti-Soviet policy.
Resignation and retirement.
On 26 October 1966, Minister Walter Scheel (FDP) resigned, protesting against the budget released the day before. The other ministers who were members of the FDP followed his example — the coalition was broken. On 1 December, Erhard resigned. His successor was Kurt Georg Kiesinger (CDU), who formed a grand coalition with the SPD.
Erhard continued his political work by remaining a member of the West German parliament until his death in Bonn on 5 May 1977. He was buried in Gmund, near the Tegernsee. The Ludwig Erhard-Berufsschule (professional college) in Paderborn, Fürth and Münster are named in his honour.
Erhard's First Ministry.
Erhard's first ministry was from 16 October 1963 – 26 October 1965.
Erhard's Second Ministry.
Erhard's Second Ministry occurred from 26 October 1965 – 1 December 1966.

</doc>
<doc id="41904" url="https://en.wikipedia.org/wiki?curid=41904" title="Storage">
Storage

Storage may refer to:

</doc>
<doc id="41906" url="https://en.wikipedia.org/wiki?curid=41906" title="Al Pacino">
Al Pacino

Alfredo James "Al" Pacino (; born April 25, 1940) is an American actor of stage and screen, filmmaker, and screenwriter. Pacino has had a career spanning fifty years, during which time he has received numerous accolades and honors both competitive and honorary, among them an Academy Award, two Tony Awards, two Primetime Emmy Awards, a British Academy Film Award, four Golden Globe Awards, the Lifetime Achievement Award from the American Film Institute, the Golden Globe Cecil B. DeMille Award, and the National Medal of Arts. He is also one of few performers to have won a competitive Oscar, an Emmy and a Tony Award for acting, dubbed the "Triple Crown of Acting".
A method actor and former student of the Herbert Berghof Studio and the Actors Studio in New York City, where he was taught by Charlie Laughton and Lee Strasberg, Pacino made his feature film debut with a minor role in "Me, Natalie" (1969) and gained favourable notices for his lead role as a heroin addict in "The Panic in Needle Park" (1971). He achieved international acclaim and recognition for his breakthrough role as Michael Corleone in Francis Ford Coppola's "The Godfather" (1972). He received his first Oscar nomination and would reprise the role in sequels "Part II" (1974) and "Part III" (1990). Pacino's performance as Corleone is now regarded as one of the greatest screen performances in film history.
Pacino received his first Best Actor Oscar nomination for "Serpico" (1973); he was also nominated for "The Godfather Part II", "Dog Day Afternoon" (1975) and "...And Justice for All" (1979) and won the award in 1993 for his performance as a blind Lieutenant Colonel in "Scent of a Woman" (1992). For his performances in "The Godfather", "Dick Tracy" (1990) and "Glengarry Glen Ross" (1992), Pacino was nominated for the Academy Award for Best Supporting Actor. Other notable roles include Tony Montana in "Scarface" (1983), Carlito Brigante in "Carlito's Way" (1993), Lieutenant Vincent Hanna in "Heat" (1995), Benjamin Ruggiero in "Donnie Brasco" (1997), Lowell Bergman in "The Insider" (1999) and Detective Will Dormer in "Insomnia" (2002). In television, Pacino has acted in several productions for HBO including the miniseries "Angels in America" (2003) and the Jack Kevorkian biopic "You Don't Know Jack" (2010), both of which won him the Primetime Emmy Award for Outstanding Lead Actor in a Miniseries or a Movie.
In addition to his work in film, Pacino has had an extensive career on stage and is a two-time Tony Award winner, in 1969 and 1977, for his performances in "Does a Tiger Wear a Necktie?" and "The Basic Training of Pavlo Hummel" respectively. A lifelong fan of Shakespeare, Pacino directed and starred in "Looking for Richard" (1996), a documentary film about the play "Richard III", a role which Pacino had earlier portrayed on-stage in 1977. He has also acted as Shylock in a 2004 feature film adaptation and a 2010 production of "The Merchant of Venice". Having made his filmmaking debut with "Looking for Richard", Pacino has also directed and starred in the independent film "Chinese Coffee" (2000) and the films "Wilde Salomé" (2011) and "Salomé" (2013), about the play "Salomé" by Oscar Wilde. Since 1994, Pacino has been the joint president of the Actors Studio with Ellen Burstyn and Harvey Keitel.
Early life and education.
Pacino was born in New York City (East Harlem), to Sicilian-American parents Salvatore Pacino and Rose, who divorced when he was two years old. His mother moved near the Bronx Zoo to live with her parents, Kate and James Gerardi, who, coincidentally, had come from a town in Sicily named Corleone. His father, who was from San Fratello in the Province of Messina, moved to Covina, California, and worked as an insurance salesman and restaurateur.
In his teen years "Sonny", as he was known to his friends, aimed to become a baseball player, and was also nicknamed "The Actor". Pacino dropped out of many classes, but not English. He dropped out of school at age 17. His mother disagreed with his decision; they argued and he left home. He worked at low-paying jobs, messenger, busboy, janitor, and postal clerk, to finance his acting studies. He once worked in the mail room for "Commentary" magazine.
He began smoking and drinking at age nine, and took up casual cannabis use at age 13, but never used hard drugs. His two closest friends died from drug abuse at the ages of 19 and 30. Growing up in The Bronx, he got into occasional fights and was considered something of a troublemaker at school.
He acted in basement plays in New York's theatrical underground but was rejected for the Actors Studio while a teenager. Pacino then joined the Herbert Berghof Studio (HB Studio), where he met acting teacher Charlie Laughton (not to be confused with the British actor Charles Laughton), who became his mentor and best friend. In this period, he was often unemployed and homeless, and sometimes slept on the street, in theaters, or at friends' houses.
In 1962, his mother died at the age of 43. The following year, Pacino's grandfather James Gerardi, one of the most influential people in his life, also died.
Actors Studio training.
After four years at HB Studio, Pacino successfully auditioned for the Actors Studio. The Actors Studio is a membership organization of professional actors, theatre directors and playwrights in the Hell's Kitchen neighborhood of Manhattan in New York City. Pacino studied "method acting" under acting coach Lee Strasberg, who later appeared with Pacino in the films "The Godfather Part II" and in "...And Justice for All".
During later interviews he spoke about Strasberg and the Studio's effect on his career. "The Actors Studio meant so much to me in my life. Lee Strasberg hasn't been given the credit he deserves ... Next to Charlie, it sort of launched me. It really did. That was a remarkable turning point in my life. It was directly responsible for getting me to quit all those jobs and just stay acting."
In another interview he added, "It was exciting to work for him Strasberg because he was so interesting when he talked about a scene or talked about people. One would just want to hear him talk, because things he would say, you'd never heard before ... He had such a great understanding ... he loved actors so much."
Pacino is currently co-president, along with Ellen Burstyn and Harvey Keitel, of the Actors Studio.
Stage career.
In 1967, Pacino spent a season at the Charles Playhouse in Boston, performing in Clifford Odets' "Awake and Sing!" (his first major paycheck: $125 a week); and in Jean-Claude Van Itallie's "America, Hurrah", where he met actress Jill Clayburgh on this play. They had a five-year romance and moved back together to New York City.
In 1968, Pacino starred in Israel Horovitz's "The Indian Wants the Bronx" at the Astor Place Theater, playing Murph, a street punk. The play opened January 17, 1968, and ran for 177 performances; it was staged in a double bill with Horovitz's "It's Called the Sugar Plum", starring Clayburgh. Pacino won an Obie Award for Best Actor for his role, with John Cazale winning for Best Supporting actor and Horowitz for Best New Play. Martin Bregman saw the play and became Pacino's manager, a partnership that became fruitful in the years to come, as Bregman encouraged Pacino to do "The Godfather", "Serpico" and "Dog Day Afternoon". "Martin Bregman discovered me off Broadway. I was 26, 25. And he discovered me and became my manager. And that's why I'm here. I owe it to Marty, I really do," Pacino himself has recently stated about his own career.
Pacino and this production of "The Indian Wants the Bronx" traveled to Italy for a performance at the Festival dei Due Mondi in Spoleto. It was Pacino's first journey to Italy; he later recalled that "performing for an Italian audience was a marvelous experience". Pacino and Clayburgh were cast in "Deadly Circle of Violence", an episode of the ABC television series "NYPD", premiering November 12, 1968. Clayburgh at the time was also appearing on the soap opera "Search for Tomorrow", playing the role of Grace Bolton. Her father would send the couple money each month to help.
On February 25, 1969, Pacino made his Broadway debut in Don Petersen's "Does a Tiger Wear a Necktie?" at the Belasco Theater produced by A&P Heir Huntington Hartford. It closed after 39 performances on March 29, 1969, but Pacino received rave reviews and won the Tony Award on April 20, 1969. Pacino continued performing onstage in the 1970s, winning a second Tony Award for "The Basic Training of Pavlo Hummel" and performing the title role in "Richard III". In the 1980s, Pacino again achieved critical success on stage while appearing in David Mamet's "American Buffalo," for which Pacino was nominated for a Drama Desk Award. Since 1990, Pacino's stage work has included revivals of Eugene O'Neill's "Hughie", Oscar Wilde's "Salome" and in 2005 Lyle Kessler's "Orphans".
Pacino made his return to the stage in summer 2010, as Shylock in a Shakespeare in the Park production of "The Merchant of Venice". The acclaimed production moved to Broadway at the Broadhurst Theatre in October, earning US$1 million at the box office in its first week. The performance also garnered him a Tony Award nomination for Best Leading Actor in a Play. In October 2012 Pacino starred in the 30th anniversary Broadway revival of David Mamet's classic play, "Glengarry Glen Ross", which ran through January 20, 2013.
Presently, he is starring on Broadway in "China Doll", a play written for him by David Mamet. It is a limited run of 87 performances, after acclaimed reviews of 4 performances in October 2015. The play is currently running as of November 2015.
Film career.
Early film career.
Pacino found acting enjoyable and realized he had a gift for it while studying at The Actors Studio. However, his early work was not financially rewarding. After his success on stage, Pacino made his movie debut in 1969 with a brief appearance in "Me, Natalie", an independent film starring Patty Duke. In 1970, Pacino signed with the talent agency Creative Management Associates (CMA).
1970s.
It was the 1971 film "The Panic in Needle Park", in which he played a heroin addict, that brought Pacino to the attention of director Francis Ford Coppola, who cast him as Michael Corleone in the blockbuster Mafia film "The Godfather" (1972). Although several established actorsincluding Jack Nicholson, Robert Redford, Warren Beatty, and little-known Robert De Niroalso tried out for the part, Coppola selected the relatively unknown Pacino, to the dismay of studio executives.
Pacino's performance earned him an Academy Award nomination, and offered a prime example of his early acting style, described by Halliwell's Film Guide as "intense" and "tightly clenched". Pacino boycotted the Academy Award ceremony, insulted at being nominated for the Supporting Acting award, noting that he had more screen time than co-star and Best Actor winner Marlon Brandowho also boycotted the awards, but for unrelated reasons.
In 1973, he co-starred in "Scarecrow", with Gene Hackman, and won the Palme d'Or at the Cannes Film Festival. That same year, Pacino was nominated for an Academy Award for Best Actor after starring in "Serpico", based on the true story of New York City policeman Frank Serpico, who went undercover to expose the corruption of fellow officers. In 1974, Pacino reprised his role as Michael Corleone in the sequel "The Godfather Part II", which was the first sequel to win the Best Picture Oscar; Pacino, meanwhile, was nominated for his third Oscar.
"Newsweek" has described his performance in "The Godfather Part II" as "arguably cinema's greatest portrayal of the hardening of a heart". In 1975, he enjoyed further success with the release of "Dog Day Afternoon", based on the true story of bank robber John Wojtowicz. It was directed by Sidney Lumet, who had directed him in "Serpico" a few years earlier, and Pacino was again nominated for Best Actor.
In 1977, Pacino starred as a race-car driver in "Bobby Deerfield", directed by Sydney Pollack, and received a Golden Globe nomination for Best Actor – Motion Picture Drama for his portrayal of the title role. His next film was the courtroom drama "...And Justice for All", which again saw Pacino lauded by critics for his wide range of acting abilities, and nominated for the Best Actor Oscar for a fourth time. However he lost out that year to Dustin Hoffman in "Kramer vs. Kramer"—a role that Pacino had declined.
During the 1970s, Pacino had four Oscar nominations for Best Actor, for his performances in "Serpico", "The Godfather Part II", "Dog Day Afternoon", and "...And Justice for All".
1980s.
Pacino's career slumped in the early 1980s; his appearances in the controversial "Cruising", a film that provoked protests from New York's gay community, and the comedy-drama "Author! Author!", were critically panned. However, 1983's "Scarface", directed by Brian De Palma, proved to be a career highlight and a defining role. Upon its initial release, the film was critically panned due to violent content, but later received critical acclaim. The film did well at the box office, grossing over US$45 million domestically. Pacino earned a Golden Globe nomination for his role as Cuban drug lord Tony Montana.
In 1985, Pacino worked on his personal project, "The Local Stigmatic", a 1969 Off Broadway play by the English writer Heathcote Williams. He starred in the play, remounting it with director David Wheeler and the Theater Company of Boston in a 50-minute film version. The film was not released theatrically, but was later released as part of the "Pacino: An Actor's Vision" box set in 2007.
His 1985 film "Revolution" about a fur trapper during the American Revolutionary War, was a commercial and critical failure, which Pacino blamed on a rushed production, resulting in a four-year hiatus from films. At this time Pacino returned to the stage. He mounted workshop productions of "Crystal Clear", "National Anthems" and other plays; he appeared in "Julius Caesar" in 1988 in producer Joseph Papp's New York Shakespeare Festival. Pacino remarked on his hiatus from film: "I remember back when everything was happening, '74, '75, doing "The Resistible Rise of Arturo Ui" on stage and reading that the reason I'd gone back to the stage was that my movie career was waning! That's been the kind of ethos, the way in which theater's perceived, unfortunately." Pacino returned to film in 1989's "Sea of Love", when he portrayed a detective hunting a serial killer who finds victims through the singles column in a newspaper. The film earned solid reviews.
1990s.
Pacino received an Academy Award nomination for playing Big Boy Caprice in the box office hit "Dick Tracy" in 1990, of which critic Roger Ebert described Pacino as "the scene-stealer". Later in the year he followed this up in a return to one of his most famous characters, Michael Corleone, in "The Godfather Part III" (1990). The film received mixed reviews, and had problems in pre-production due to script rewrites and the withdrawal of actors shortly before production.
In 1991, Pacino starred in "Frankie and Johnny" with Michelle Pfeiffer, who co-starred with Pacino in "Scarface". Pacino portrays a recently paroled cook who begins a relationship with a waitress (Pfeiffer) in the diner where they work. It was adapted by Terrence McNally from his own Off-Broadway play "Frankie and Johnny in the Clair de Lune" (1987), that featured Kenneth Welsh and Kathy Bates. The film received mixed reviews, although Pacino later said he enjoyed playing the part. Janet Maslin in "The New York Times" wrote, "Mr. Pacino has not been this uncomplicatedly appealing since his "Dog Day Afternoon" days, and he makes Johnny's endless enterprise in wooing Frankie a delight. His scenes alone with Ms. Pfeiffer have a precision and honesty that keep the film's maudlin aspects at bay."
In 1992, Pacino won the Academy Award for Best Actor, for his portrayal of the blind U.S. Army Lieutenant Colonel Frank Slade in Martin Brest's "Scent of a Woman". That year, he was also nominated for Best Supporting Actor for "Glengarry Glen Ross", making Pacino the first male actor ever to receive two acting nominations for two movies in the same year, and to win for the lead role.
Pacino starred alongside Sean Penn in the crime drama "Carlito's Way" in 1993, in which he portrayed a gangster released from prison with the help of his lawyer (Penn) and vows to go straight. Pacino starred in Michael Mann's "Heat" (1995), in which he and Robert De Niro appeared on-screen together for the first time (though both Pacino and De Niro starred in "The Godfather Part II", they did not share any scenes).
In 1996, Pacino starred in his theatrical docudrama "Looking for Richard", a performance of selected scenes of Shakespeare's "Richard III" and a broader examination of Shakespeare's continuing role and relevance in popular culture. The cast brought together for the performance included Alec Baldwin, Kevin Spacey, and Winona Ryder. Pacino played Satan in the supernatural thriller "The Devil's Advocate" (1997) which co-starred Keanu Reeves. The film was a success at the box office, taking US$150 million worldwide. Roger Ebert wrote in the "Chicago Sun-Times", "The satanic character is played by Pacino with relish bordering on glee."
In "Donnie Brasco", Pacino played mafia gangster "Lefty", the true story of undercover FBI agent Donnie Brasco (Johnny Depp) and his work in bringing down the mafia from the inside. Pacino also starred as real life "60 Minutes" producer Lowell Bergman in the multi-Oscar nominated "The Insider" opposite Russell Crowe, before starring in Oliver Stone's "Any Given Sunday" in 1999.
2000s.
Pacino has not received another Academy Award nomination since winning for "Scent of a Woman", but has won three Golden Globes since the year 2000, the first being the Cecil B. DeMille Award in 2001 for lifetime achievement in motion pictures.
In 2000, Pacino released a low-budget film adaptation of Ira Lewis' play "Chinese Coffee" to film festivals. Shot almost exclusively as a one-on-one conversation between two main characters, the project took nearly three years to complete and was funded entirely by Pacino. "Chinese Coffee" was included with Pacino's two other rare films he was involved in producing, "The Local Stigmatic" and "Looking for Richard", on a special DVD box set titled "Pacino: An Actor's Vision", which was released in 2007. Pacino produced prologues and epilogues for the discs containing the films.
Pacino turned down an offer to reprise his role as Michael Corleone in the computer game version of '. As a result, Electronic Arts was not permitted to use Pacino's likeness or voice in the game, although his character does appear in it. He did allow his likeness to appear in the video game adaptation of 1983's "Scarface", quasi-sequel titled '.
Director Christopher Nolan worked with Pacino on "Insomnia", a remake of the Norwegian film of the same name, co-starring Robin Williams. "Newsweek" stated that "he can play small as rivetingly as he can play big, that he can implode as well as explode". The film and Pacino's performance were well received, gaining a favorable rating of 93 percent on the review aggregation website Rotten Tomatoes. The film did moderately well at the box office, taking in $113 million worldwide. His next film, "S1m0ne", did not gain much critical praise or box office success.
He played a publicist in "People I Know", a small film that received little attention despite Pacino's well-received performance. Rarely taking a supporting role since his commercial breakthrough, he accepted a small part in the box office flop "Gigli", in 2003, as a favor to director Martin Brest. "The Recruit", released in 2003, featured Pacino as a CIA recruiter and co-stars Colin Farrell. The film received mixed reviews, and has been described by Pacino as something he "personally couldn't follow". Pacino next starred as lawyer Roy Cohn in the 2003 HBO miniseries "Angels in America", an adaptation of Tony Kushner's Pulitzer Prize winning play of the same name. For this performance, Pacino won his third Golden Globe, for Best Performance by an Actor, in 2004.
Pacino starred as Shylock in Michael Radford's 2004 film adaptation of "The Merchant of Venice", choosing to bring compassion and depth to a character traditionally played as a villainous caricature. In "Two for the Money", Pacino portrays a sports gambling agent and mentor for Matthew McConaughey, alongside Rene Russo. The film was released on October 8, 2005, to mixed reviews. Desson Thomson wrote in "The Washington Post", "Al Pacino has played the mentor so many times, he ought to get a kingmaker's award ... the fight between good and evil feels fixed in favor of Hollywood redemption."
On October 20, 2006, the American Film Institute named Pacino the recipient of the 35th AFI Life Achievement Award. On November 22, 2006, the University Philosophical Society of Trinity College, Dublin awarded Pacino the Honorary Patronage of the Society.
Pacino played a spoof role in Steven Soderbergh's "Ocean's Thirteen", alongside George Clooney, Brad Pitt, Matt Damon, Elliott Gould and Andy García, as the villain Willy Bank, a casino tycoon targeted by Danny Ocean and his crew. The film received generally favorable reviews.
"88 Minutes" was released on April 18, 2008, in the United States, after having been released in various other countries in 2007. The film co-starred Alicia Witt and was critically panned, although critics found fault with the plot, and not Pacino's acting. In "Righteous Kill", Pacino and Robert De Niro co-star as New York detectives searching for a serial killer. The film was released to theaters on September 12, 2008. While it was an anticipated return for the two stars, it was not well received by critics. Lou Lumenick of the "New York Post" gave "Righteous Kill" one star out of four, saying: "Al Pacino and Robert De Niro collect bloated paychecks with intent to bore in "Righteous Kill", a slow-moving, ridiculous police thriller that would have been shipped straight to the remainder bin at Blockbuster if it starred anyone else."
2010s.
Pacino played Dr. Jack Kevorkian in an HBO Films biopic entitled "You Don't Know Jack", which premiered April 2010. The film is about the life and work of the physician-assisted suicide advocate. The performance earned Pacino his second Emmy Award for lead actor and his fourth Golden Globe award.
It was announced in May 2011 that Pacino was to be honored with the "Glory to the Film-maker" award at the 68th Venice International Film Festival. The award was presented ahead of the premiere of his film "Wilde Salome", the third film Pacino has directed. Pacino, who plays the role of Herod in the film, describes it as his "most personal project ever".
The United States premiere of "Wilde Salomé" took place on the evening of March 21, 2012, before a full house at the 1,400-seat Castro Theatre in San Francisco's Castro District. Marking the 130th anniversary of Oscar Wilde's visit to San Francisco, the event was a benefit for the GLBT Historical Society.
Pacino most recently starred in a 2013 HBO biographical picture about record producer Phil Spector's murder trial, titled "Phil Spector".
Pacino and Robert De Niro were reportedly set to star in the upcoming project "The Irishman", to be directed by Martin Scorsese and co-star Joe Pesci. It was announced in January 2013 that Pacino would play the late former Penn State University football coach Joe Paterno in the movie tentatively titled "Happy Valley" and based on a 2012 biography of Paterno by sportswriter Joe Posnanski.
Personal life.
Although he has never married, Pacino has three children. The eldest, Julie Marie (born 1989), is his daughter with acting coach Jan Tarrant. He also has twins, son Anton James and daughter Olivia Rose (born January 25, 2001), with actress Beverly D'Angelo, with whom he had a relationship from 1996 until 2003. Pacino had a relationship with Diane Keaton, his co-star in the "Godfather" trilogy. The on-again, off-again relationship ended following the filming of "The Godfather Part III". He has had relationships with Tuesday Weld, Jill Clayburgh, Marthe Keller, Kathleen Quinlan and Lyndall Hobbs.
The Internal Revenue Service filed a tax lien against Pacino, claiming he owes the government a total of $188,000 for 2008 and 2009. A representative for Pacino blamed his former business manager Kenneth Starr for the discrepancy.
Awards and nominations.
Pacino has been nominated and has won many awards during his acting career, including eight Oscar nominations (winning one), 15 Golden Globe nominations (winning four), five BAFTA nominations (winning two), two Primetime Emmy Awards for his work on television, and two Tony Awards for his stage work. In 2007, the American Film Institute awarded Pacino with a lifetime achievement award and, in 2003, British television viewers voted Pacino as the greatest film star of all time in a poll for Channel 4.

</doc>
<doc id="41907" url="https://en.wikipedia.org/wiki?curid=41907" title="Dick Tracy">
Dick Tracy

Dick Tracy is an American comic strip featuring Dick Tracy (originally Plainclothes Tracy), a tough and intelligent police detective created by Chester Gould. The strip made its debut on October 4, 1931 in the "Detroit Mirror". It was distributed by the Chicago Tribune New York News Syndicate. Gould wrote and drew the strip until 1977. Since that time, various artists and writers have continued the strip, which still runs in newspapers today. Dick Tracy has also been the hero in a number of films, notably one in which Warren Beatty played the crime fighter in 1990.
Comic strip.
Characters and story.
Tracy uses forensic science, advanced gadgetry, and wits, in an early example of the police procedural mystery story—although stories often end in gunfights just the same. Stories typically follow a criminal committing a crime and Tracy's relentless pursuit of the criminal. The strip's most popular villain was Flattop Jones, a freelance hitman hired by black marketeers to murder Tracy. When Flattop was killed, fans went into public mourning, and the Flattop Story was reprinted in DC's series of Oversize Comic Reprints in the 1970s. The villains' small crimes led to bigger, out of control situations, reflecting film noir. Similarly, innocent witnesses were frequently killed, and Tracy's paramour Tess Trueheart was often endangered by the villains. As the story progressed, Tracy adopted an orphan under the name Dick Tracy Jr., or "Junior" for short, who appeared in investigations until becoming a police forensic artist in his father's precinct. He also cultivated a professional partner, ex-steel worker Pat Patton, who gradually became a detective of skill and courage enough to satisfy Tracy's requirements.
Tracy characters were often caricatures of celebrities. There was Breathless Mahoney, modeled after Veronica Lake. Likewise, B.O. Plenty was inspired by George "Gabby" Hayes (with perhaps a nod to Al St. John also), Vitamin Flintheart by John Barrymore, and Spike Dyke by Spike Jones. Others include villains like Rughead (Robert Montgomery), Oodles (Jackie Gleason) and Mumbles (Bing Crosby). Gould even parodied himself as the out-of-shape Pear Shape.
Evolution of the strip.
On January 13, 1946, the 2-Way Wrist Radio became one of the strip's most immediately recognizable icons, worn as a wristwatch by Tracy and members of the police force, and may have inspired later smartwatches. The 2-Way Wrist Radio was upgraded to a 2-Way Wrist TV in 1964. This development also led to the introduction of an important supporting character, Diet Smith, an eccentric industrialist who financed the development of this equipment. In a conspicuous coincidence, the idea of a radio built into a wrist watch played an important role in the story line of "Superman – The Talking Cat" broadcast on the Mutual Broadcasting System on January 9 through 28, 1946 (episodes 878 through 891).
In late 1948, a botched security detail led to the death of the semi-regular character Brilliant, the blind inventor of the 2-Way Wrist Radio (among other devices). Whereupon, Chief Brandon resigned in shame, Dick Tracy's superior on the police force and a presence in the strip since 1931, and Pat Patton was promoted to police chief in Brandon's place, previously Tracy's buffoonish partner. A new character was introduced named Sam Catchem to take Patton's place as Tracy's sidekick.
The 1950s.
Gould introduced topical story lines about television, juvenile delinquency, graft, organized crime, and other developments in American life during the 1950s; and elements of soap opera depicted Dick, Tess, and Junior (along with the Tracys' baby daughter Bonnie Braids) at home as a family. Depictions of family life alternated with the story's crime drama, as in the kidnapping of Bonnie Braids by fugitive Crewy Lou, or Junior's girlfriend Model being accidentally killed by her brother.
Gould incurred some controversy when he had Tracy live in an unaccountably ostentatious manner on a police officer's salary, and responded with a story wherein Tracy was accused of corruption and had to explain the origin of his possessions in detail. In his book-length examination of the strip, "Dick Tracy – The Official Biography", Jay Maeder suggested that Gould's critics were unsatisfied by his explanation. Nevertheless, the controversy eventually faded, and the cartoonist reduced exposure to Tracy's home life.
Tracy's cases generally incriminated independent operators rather than organized crime—with a few exceptions, such as Big Boy, a fictionalized version of Al Capone and the strip's first villain. Tracy opposed a series of big-time mobsters in the 1950s, such as the King, George "Mr. Crime" Alpha, Odds Zonn, and Willie "The Fifth" Millyun, after events like the Kefauver Hearings. As Tess faded into the background, Tracy assumed as assistant the rookie policewoman Lizz Worthington.
From 1956 to 1964, the "Dick Tracy" Sunday page was accompanied by a topper humor strip called "The Gravies" and drawn by Gould and his assistants.
Space period.
As technology progressed, the methods that Tracy and the police used to track and capture criminals took the form of increasingly fanciful atomic-powered gadgets developed by Diet Smith Industries. This eventually led to the 1960s advent of the Space Coupe, a spacecraft with a magnetic propulsion system. This marked the beginning of the strip's "Space Period," which saw Tracy and friends having adventures on the Moon and meeting Moon Maid, the daughter of the leader of a race of humanoid people living in "Moon Valley" in 1964. After an eventual sharing of technological information, Moon technology became standard issue on Tracy's police force, including air cars, flying cylindrical vehicles. The villains became even more exaggerated in power, resulting in an escalating series of stories that no longer resembled the urban crime drama roots of the strip. During this period, Tracy met famed cartoonist Chet Jade, creator of the comic strip "Sawdust", in which the only characters are talking dots.
One of the new characters, Mr. Intro, was only manifested as a disembodied voice. His goal was world domination in the vein of a James Bond villain. Tracy eventually used an atomic laser beam to annihilate Intro and his island base.
Junior married Moon Maid in October 1964. Their daughter Honey Moon Tracy had antennae and magnetic hands. In the spring of 1969, Tracy was offered the post of Chief of Police in Moon Valley. However, he ended up back on Earth when the Apollo 11 mission in 1969 showed that the moon was barren of all life. Many of the accoutrements of the space period stories remained for many years afterward, such as the Space Coupe and much of the high-tech gadgetry. Moon Maid receded from the storyline.
The stories of this period took an increasingly condemnatory tone pertaining to contemporary court decisions concerning the rights of the accused, which often involved Tracy being frustrated by legal technicalities. For example, having caught a gang of diamond thieves red-handed, Tracy was forced to let them walk because he could not "prove" beyond reasonable doubt that the diamonds were stolen. As he saw the thieves get off without penalty, Tracy was heard to grumble, "Yes, under today's interpretation of the laws, it seems it's the police who are handcuffed!"
1970s.
In the 1970s, Gould modernized Tracy by giving him a longer hair style and mustache, and added a hippie sidekick, Groovy Grove. Groovy's first appearance in print, as it happened, occurred during the same week as the Kent State shootings. Groovy remained with the strip, off and on until his death in 1984.
Shortly before his retirement, Gould drew a strip in which Sam, Lizz, and Groovy held Tracy down to shave off his mustache.
At this time, the standard publication size and space of newspaper comics was sharply reduced; for example, the "Dick Tracy" Sunday strip, which had traditionally been a full-page episode containing 12 panels, was cut in size to a half-page format that offered, at most, eight panels—these new restrictions created challenges for all comic artists. During the late 1970s the strip was thought to have been drawn by a few other artists due to an ailing Gould.
Plenty family.
The Plenty family was a group of goofy redneck yokels headed by the former villain Bob Oscar ("B.O."), along with Gertrude ("Gravel Gertie") Plenty. Gravel Gertie was introduced as the unwitting dupe (accessory) of the villain The Brow, who was on the run from Dick Tracy. The family provided a humorous counterpoint to Tracy's adventures. The Plenty sub-story was decades long, and saw Sparkle Plenty grow from an infant to a young married lady, eventually becoming a beautiful fashion model. Sparkle Plenty's May 30, 1947 birth became a significant mainstream media event, with spinoff merchandising and magazine coverage.
The Plenty family appeared with Tracy in a story that occurred in a bank, where "B.O." found a way to prevent thieves from snatching an envelope of money from a counter.
In the April 24, 2011 strip, B.O. and Gertie had a second child, Attitude, a boy who is as ugly as Sparkle is beautiful. His face has yet to be shown.
Crimestoppers' Textbook.
Beginning in the early 1950s, the Sunday strip included a frame devoted to a page from the "Crimestoppers' Textbook", a series of handy illustrated hints for the amateur crime-fighter. This was named after a short-lived youth group seen in the strip during the late 1940s, led by Junior Tracy, called "Dick Tracy's Crimestoppers." This feature ended when Gould retired from the strip in 1977, but Max Allan Collins reinstated it, and it is still part of the comic strip. After Gould's retirement, Collins initially replaced the Textbook with "Dick Tracy's Rogues Gallery," a salute to memorable "Tracy" villains of the past.
Later years.
Chester Gould retired from comics in 1977; his last "Dick Tracy" strip appeared in print on Sunday, December 25 of that year. The following Monday, "Dick Tracy" was taken over by Max Allan Collins and longtime Gould assistant Rick Fletcher. Gould's name remained in the byline for a few years after his retirement as a story consultant.
In one of Max Allan Collins' first stories as the strip's writer, the gangster known as "Big Boy" learned that he was dying and had less than a year to live. Big Boy was still seeking revenge on the plainclothesman who sent him up the river, and he wanted to live just long enough to see Tracy's death. He put out an open contract on Tracy's head worth one million dollars, knowing that every small-time hood in the City would take a crack at the famous cop for that amount of money. One of the would-be collectors rigged Tracy's car to explode, but inadvertently killed Moon Maid instead of Tracy in the explosion. A funeral strip for Moon Maid explicitly stated that this officially severed all ties between Earth and the Moon in the strip, thus eliminating the last remnants of the Space Period. Honey Moon received a new hairstyle that covered her antennae, and she was ultimately phased out of the strip. Junior later married Sparkle Plenty (the daughter of B.O. and Gravel Gertie Plenty), and had a daughter named Sparkle Plenty Jr. In the 1990s, Tracy's son Joseph Flintheart Tracy took on a role similar to Junior's in the earlier strips.
In addition, Collins removed other Gould creations of the 1960s and 1970s (including Groovy Grove, who was gravely wounded in the line of duty and later died in the hospital; Lizz married him before his death). On a more philosophical level, Collins took a generally less cynical view of the justice system than Gould; Tracy came to accept its limitations and requirements as a normal part of the process which he could manage. Extreme technology was phased out, such as the Space Coupe, in favor of more realistic advanced tools such as the 2-Way Wrist Computer in 1987.
New semi-regular characters introduced by Collins and Fletcher included: Dr. Will Carver, a plastic surgeon with underworld ties who often worked on known felons; Wendy Wichel, a smarmy newspaper reporter/editorialist with a strong anti-Tracy bias in her articles; and Lee Ebony, an African-American female detective. Vitamin Flintheart reappeared occasionally as a comic-relief figure, the aged ham actor created by Gould in 1944 who had not been seen in the strip for almost three decades. The Plenty family (B.O., Gravel Gertie, and Sparkle) were also brought back as semi-regulars; Junior and Sparkle were married following the death of Moon Maid, and soon gave birth to their own daughter Sparkle Plenty, Jr.
Original villains seen during this period included Angeltop (revenge-seeking, psychopathic daughter of the slain Flattop), Torcher (whose scheme was arson-for-profit), and Splitscreen (a video pirate). Collins brought back at least one "classic" Gould villain or revenge-seeking family member per year. The revived Gould villains were often provided with full names, and marriages, children, and other family connections were developed, bringing more humanity to many of the originally grotesque brutes. "Flattop", particularly, had a number of relatives, all with his characteristic head structure and facial attributes, who turned up one by one to avenge their ancestor on Tracy.
Rick Fletcher died in 1983 and was succeeded by editorial cartoonist Dick Locher, who had assisted Gould on the strip in the late 1950s and early 1960s. Locher was assisted by his son John, who died in 1986.
Max Allan Collins was fired from the strip in 1992, following a financial reorganization of their comic strip holdings, and "Tribune" staff writer and columnist Mike Kilian took over the writing. Kilian was paid less than half of what Collins was making per strip , but continued until his death on October 27, 2005. Locher was both author and artist for over three years, beginning on January 9, 2006. On March 16, 2009, Jim Brozman began collaborating with Locher, taking over the drawing duties while Locher continued to write the strip.
In 2005, Tracy was a guest at Blondie and Dagwood's 75th anniversary party in the comic strip "Blondie". Later, Dick Tracy appeared in the comic strip "Gasoline Alley".
On January 19, 2011, Tribune Media Services announced that Locher was retiring from the strip and handing the reins to artist Joe Staton and writer Mike Curtis. The new creative team has previously worked together on "Scooby-Doo", "Richie Rich", and" Casper the Friendly Ghost". Their first Dick Tracy strip was published March 14, 2011. Staton and Curtis are assisted by Shelley Pleger, who inks and letters Staton's drawings, along with Shane Fisher, who provides the coloring on the Sunday strips, and Chicago-area policeman Jim Doherty, who provides "Crimestopper" captions for the Sunday strips and acts as the feature's technical advisor. Doherty also introduced a new feature, "Tracy's Hall of Fame" (which replaces the "Crimestopper" panel approximately once each month), in which a real-life police officer is profiled and honored.
They reintroduced many of the characters of the forties through the sixties, including Mr. Crime and a reformed Mole, while introducing more deformed and grotesque villains such as Abner Kadaver, Panda, and The Jumbler. They have also brought back all the gadgets and plot elements of the 1960s space era, starting in early 2013. They have also done crossovers, with cameos from "Popeye" and "Brenda Starr", and a long sequence involving "Little Orphan Annie".
Awards and honors.
Chester Gould won the Reuben Award for the strip in 1959 and 1977.
The Mystery Writers of America honored Gould and his work with a Special Edgar Award in 1980. This was the first time MWA ever honored a comic strip.
In 1995, the strip was one of 20 included in the Comic Strip Classics series of commemorative postage stamps and postcards.
On May 2, 2011, the Tennessee Senate passed Resolution 30, congratulating Mike Curtis and Joe Staton on their professional accomplishments, including "Dick Tracy".
On September 7, 2013, at the Baltimore Comics Convention, "Dick Tracy" was awarded the Harvey in the "Best Syndicated Strip or Panel" category. "Tracy" was simultaneously the oldest continually running strip, and the first adventure strip ever to win the Harvey Award in this category. On September 6, 2014, "Tracy" was awarded a second Harvey Award in the newspaper strip category, becoming one of only three strips to win in this category in consecutive years. On September 26, 2015, "Tracy" won a third Harvey in the same category, becoming one of only three strips to win in three consecutive years.
Other media depictions.
Radio.
"Dick Tracy" had a long run on radio, from 1934 weekdays on NBC's New England stations to the ABC network in 1948. Bob Burlen was the first radio Tracy in 1934, and others heard in the role during the 1930s and 1940s were Barry Thomson, Ned Wever and Matt Crowley. The early shows all had 15-minute episodes.
On CBS, with Sterling Products as sponsor, the serial aired four times a week from February 4, 1935 to July 11, 1935, moving to Mutual from September 30, 1935 to March 24, 1937 with Bill McClintock doing the sound effects. NBC's weekday afternoon run from January 3, 1938 to April 28, 1939 had sound effects by Keene Crockett and was sponsored by Quaker Oats, which brought "Dick Tracy" into primetime (Saturdays at 7 pm and, briefly, Mondays at 8 pm) with 30-minute episodes from April 29, 1939 to September 30, 1939. The series returned to 15-minute episodes on the ABC Blue Network from March 15, 1943 to July 16, 1948, sponsored by Tootsie Roll, which used the music theme of "Toot Toot, Tootsie" for its 30-minute Saturday ABC series from October 6, 1945 to June 1, 1946. Sound effects on ABC were supplied by Walt McDonough and Al Finelli.
On February 15, 1945, Command Performance broadcast the musical comedy Dick Tracy in B-Flat with Bing Crosby as Tracy, Bob Hope as Flattop, Dinah Shore as Tess Trueheart, among the cast. Dick Tracy's wedding is repeatedly interrupted as Tracy chases after one villain after another. In the strip, his marriage wasn't until 1950 and his honeymoon was disrupted by his going after Wormy.
Recordings.
Jim Ameche portrayed Tracy in a two-record set recorded by Mercury Records in 1947. The record sleeves were illustrated with Sunday strips reprinted in black-and-white for children to color.
Film serials.
Dick Tracy made his film debut in "Dick Tracy" (1937), a 15-chapter movie serial by Republic Pictures starring Ralph Byrd. The Spider Gang was on the loose, tired of Dick Tracy's cunning skills. Through the 15-chapter serial, 15 different cases were solved, all plots by the Spider Gang. Dick Tracy was also in search for his missing brother, Gordon Tracy (Carleton Young). The Dick Tracy character proved very popular, and a second serial, "Dick Tracy Returns", appeared in 1938 (reissued in 1948). "Dick Tracy's G-Men" was released in 1939 (reissued in 1955). The last was "Dick Tracy vs. Crime Inc." in 1941 (reissued as "Dick Tracy vs. the Phantom Empire" in 1952).
The sequels were produced under an interpretation of the contract for the first "Dick Tracy" serial, which gave license for "a series or serial". As a result, Chester Gould received no further money for the sequel serials.
In these serials, Dick Tracy is portrayed as an FBI agent, or "G-Man", based in California, rather than as a detective in the police force of a Midwestern city resembling Chicago, and, aside from himself and Junior, no characters from the strip appear in any of the four films.
However, comic relief sidekick "Mike McGurk" bears some resemblance to Tracy's partner from the strip, Pat Patton; Tracy's secretary, Gwen Andrews (played by several actresses in the course of the series, including Jennifer Jones under a variation of her real name, Phyllis Isley), provides the same kind of feminine interest as Tess Trueheart; and FBI Director Clive Anderson (Francis X. Bushman and others) is the same kind of avuncular superior as Chief Brandon.
The first serial, "Dick Tracy", is now in the public domain.
Early feature films.
Six years after the release of the final Republic serial, Dick Tracy headlined four feature films, produced by RKO Radio Pictures. "Dick Tracy" (aka "Dick Tracy, Detective") (1945) was followed by "Dick Tracy vs. Cueball" in 1946, both with Morgan Conway as Tracy. Ralph Byrd returned for the last two features, both released in 1947: "Dick Tracy's Dilemma" and "Dick Tracy Meets Gruesome". "Gruesome" is probably the best known of the four, with the villain portrayed by Boris Karloff. All four movies had many of the visual features associated with film noir: dramatic, shadowy photographic compositions, with many exterior scenes filmed at night (at the RKO Encino movie ranch). Lyle Latell co-starred in all four films as Pat Patton. Anne Jeffreys played Tess Trueheart in the first two, succeeded by Kay Christopher and finally Anne Gwynne; Ian Keith joined the cast as the actor Vitamin Flintheart for two films; Joseph Crehan played Chief Brandon. RKO stocked the films with familiar faces, creating a veritable rogues' gallery of characters: Mike Mazurki as Splitface, Dick Wessel as Cueball, Esther Howard as Filthy Flora, Jack Lambert as hook-handed villain The Claw; baldheaded, pop-eyed Milton Parsons, mild-mannered Byron Foulger, dangerous Trevor Bardette, pockmarked, gently sinister Skelton Knaggs.
Television.
The strip has had limited exposure on television with one early live-action series, two animated series, one unsold pilot that was never picked up, and a proposed TV series currently held up in litigation.
First live-action series.
Ralph Byrd, who had played the square-jawed sleuth in all four Republic movie serials, and in two of the RKO feature-length films, reprised his role in a short-lived live-action "Dick Tracy" series that ran on ABC from 1950 to 1951. Additional episodes intended for first-run syndication continued to be produced into 1952. Produced by P. K. Palmer, who also wrote many of the scripts, the series often featured Gould-created villains such as Flattop, Shaky, the Mole, Breathless Mahoney, Heels Beals, and Influence, all of whom appeared on film for the first time on this series. Other cast members included Joe Devlin as Sam Catchem, Angela Greene as Tess Tracy (née Trueheart), Martin Dean as Junior, and Pierre Watkin as Chief Patton. Criticized for its violence, the series remained popular. It ended, not in response to criticism, but because of Byrd's unexpected, premature death in 1952. The series was filmed on a low budget, with many long hours and a rushed shooting schedule. Many episodes of this series have been released on various Public Domain TV Detective DVD sets.
Animated cartoons.
The first cartoon series was produced from 1960 to 1961 by UPA. Tracy employed a series of cartoon-like subordinate flatfoots to fight crime each week, contacting them on his two-way wrist radio. Everett Sloane voiced Tracy and supporting characters and villains were voiced by Jerry Hausner, Mel Blanc, Benny Rubin, Johnny Coons, Paul Frees, and others. These subordinates included "Go-Go" Gomez, Joe Jitsu, Hemlock Holmes, and Heap O'Calorie. 130 five-minute cartoons were designed and packaged for syndication, usually intended for local children's shows.
UPA was also the production company behind the Mr. Magoo cartoons, so it was possible for them to arrange a meeting between Tracy and Magoo in a 1965 episode of the season-long TV series "The Famous Adventures of Mr. Magoo". In that episode "Dick Tracy and the Mob," Tracy persuades Magoo (a well-known actor in the context of the "Famous Adventures" series) to impersonate an international hit man whom he resembles and infiltrate a gang of criminals made up of Flattop, Pruneface, Itchy, Mumbles, and others. Unlike the earlier animated Tracy shorts, this longer episode was played relatively straight, with Tracy getting much more screen time. Pitting Tracy against a coalition of several of his foes was adopted more than two decades later in the 1990 film mentioned below.
A second cartoon series was produced in 1971 and was a feature in "Archie's TV Funnies", produced by Filmation. It adhered more closely to the comic strip, although it was hampered by cruder animation than the UPA shorts, typical of the studio's production standards.
Live-action television pilot.
William Dozier produced a pilot for a live-action Dick Tracy series in 1967 starring Ray MacDonnell in the title role. (Dozier was the producer responsible for the 1966 "Batman" television series.) The pilot was "The Plot To Kill NATO", featuring "Special Guest Villain" Victor Buono as 'Mr. Memory'. The quality was slightly above-average, but the series was not purchased by either ABC or NBC, as ratings for the "Batman" series were dropping and a similar series featuring "The Green Hornet" had recently flopped. To the networks, the "Hero Camp" or "Batmania" craze was dying, and they chose not to take a risk on another series.
The pilot is notable for the non-appearance of the future Jan Brady (Eve Plumb) as Bonnie Braids. She was cast in the role but only appears in the title credits at the opening of the show.
1990 film.
In 1990, Warren Beatty directed and starred as the title character in a live action all-star cast film, along with Al Pacino, Dustin Hoffman, and Madonna.
Comic books.
Tracy made his first comic book appearance in 1936 as one of the features included in the first issue of Dell's "Popular Comics". These were reprints from the newspaper strip, reconfigured to fit the pages of a comic book, as was the case with most Tracy comic book appearances. Tracy remained a regular feature in "Popular Comics" through the publication's 21st issue.
The first comic book to feature Tracy exclusively was the "Dick Tracy Feature Book", published in May 1937 by David McKay Publications. McKay's Feature Books were magazines that rotated several popular characters from comics strips through 1938. Three more of McKay's Feature Books starred Tracy in the following months.
In 1939, Dell started a comic magazine series called "Black and White Comics," essentially identical to McKay's "Feature Books." Six of the 15 issues featured Tracy. In 1941, Dell's "Black and White" series was replaced by the "Large Feature Books," the third issue of which featured Tracy. As with the McKay series, the Dell "Black and White" and "Large Feature" series were abridged reprints of the strip.
In 1938, "Tracy" became one of several regular newspaper strips featured in Dell's regular monthly "Super Comics", remaining a regular part of that publication until 1948. In 1939, "Tracy" was the sole feature in the very first issue of Dell's "Four-Color Comics", which put out more than 1,300 issues starring hundreds of characters between 1939 and 1962. Tracy was featured in seven more "Four-Color" issues throughout the 1940s.
Tracy was frequently featured in comic books used as promotional items by various companies. In 1947, for example, Sig Feuchtwanger produced a comic book that was a giveaway prize in boxes of Quaker Puffed Wheat cereal, sponsor of the popular "Dick Tracy" radio series.
In January 1948, Dell began the first regular "Dick Tracy" comic book series, "Dick Tracy Monthly". This series ultimately ran for 145 issues, the first 24 of which were published by Dell, after which it was picked up by Harvey Comics. Continuing the same numbering, Harvey published the series until 1961. As with most previous Tracy comic book incarnations, these were, with the exception of the last few Dell issues which featured original material, slightly abridged and reconfigured reprints of the newspaper strips.
"Dick Tracy" was revived in 1986 by Blackthorne Publishing and ran for 99 issues. Disney produced a series of three issues as a tie-in for their 1990 film. This miniseries, "True Hearts and Tommy Guns", was drawn by Kyle Baker and edited by Len Wein. The third issue was a direct adaptation of the film.
Recent events.
Media outlets reported a legal battle being waged over rights to the Dick Tracy character. Warren Beatty announced plans to make a sequel to his 1990 movie. At the same time, television producers announced plans for a new "Dick Tracy" TV series. Both sides claimed that they were the legal owners of the rights to Dick Tracy. In May 2005, Beatty sued the Tribune Company, claiming he has owned the rights to the Dick Tracy character since 1985. Pressure from Beatty led to the cancellation of a proposed collaboration between artist Mike Oeming and writer Brian Bendis on a new serialized Dick Tracy comic.
The lawsuit was resolved in Beatty's favor, with a US District judge ruling that Beatty did everything contractually required of him to keep the rights to the character.
Books.
Over the years, many reprints of "Dick Tracy" newspaper strips have been published. Beginning in 2007, IDW Publishing reprinted the complete strip in hardcover volumes.
Other collections include:
Other editions:
Licensed products.
In the 1960s, Aurora produced a plastic model kit of Dick Tracy sliding down a fire escape ladder into an alley, in hot pursuit with gun drawn. A Dick Tracy Space Coupe model came next.
Also in the market were Mattel's Dick Tracy range of toy weapons.
In 1990, Playmates Toys released a line of action figures called Dick Tracy: Coppers and Gangsters to coincide with the Dick Tracy movie. The figures were 5" tall, stylized with exaggerated comicy looks and came with lots of accessories. Two figures in the line had limited availability; Steve the Tramp (called "The Tramp" on the package front) was pulled from the assortment after complaints of portrayal of a homeless person as a criminal. The figure of "The Blank" was added to the assortment well after the film's release to keep the secret of the identity of the character. As a result, only limited quantities made it to store shelves.
The Dick Tracy video game was developed by Titus Software in 1990. It was ported to many platforms including Amiga, Commodore and MS-DOS. Dick Tracy is a side scrolling action shooting game. Player controls Dick Tracy through five stages.
There were also games made for the Nintendo Entertainment System (1990), Sega Master System (1990), Sega Genesis (1990), and the Game Boy (1991).
In 2009, Shocker Toys released a monochromatic Dick Tracy action figure as an exclusive product for the San Diego Comic-Con. The figure appears in a suit with two-way wrist radio. There was also a variant figure released of Dick Tracy in his signature trench coat and fedora with a tommy gun accessory.
References.
Notes
Bibliography

</doc>
<doc id="41908" url="https://en.wikipedia.org/wiki?curid=41908" title="Key Word in Context">
Key Word in Context

KWIC is an acronym for Key Word In Context, the most common format for concordance lines. The term KWIC was first coined by Hans Peter Luhn. The system was based on a concept called "keyword in titles" which was first proposed for Manchester libraries in 1864 by Andrea Crestadoro.
A KWIC index is formed by sorting and aligning the words within an article title to allow each word (except the stop words) in titles to be searchable alphabetically in the index. It was a useful indexing method for technical manuals before computerized full text search became common.
For example, a search query including all of the words in the title statement of this article ("KWIC is an acronym for Key Word In Context, the most common format for concordance lines") and the in English ("the free encyclopedia"), searched against this very webpages, might yield a KWIC index as follows. A KWIC index usually uses a wide layout to allow the display of maximum 'in context' information (not shown in the following example).
A KWIC index is a special case of a permuted index. This term refers to the fact that it indexes all cyclic permutations of the headings. Books composed of many short sections with their own descriptive headings, most notably collections of manual pages, often ended with a permuted index section, allowing the reader to easily find a section by any word from its heading. This practice, also known as KWOC (“Key Word Out of Context”), is no longer common.
References in Literature.
"Note: The first reference does not show the KWIC index unless you pay to view the paper. The second reference does not even list the paper at all."

</doc>
<doc id="41909" url="https://en.wikipedia.org/wiki?curid=41909" title="Accrual bond">
Accrual bond

An accrual bond is a fixed-interest bond that is issued at its face value and repaid at the end of the maturity period together with the accrued interest. In Germany, the accrued interest is compounded. In contrast to zero-coupon bonds, accrual bonds have a clearly stated coupon rate.

</doc>
<doc id="41910" url="https://en.wikipedia.org/wiki?curid=41910" title="Aftermarket">
Aftermarket

Aftermarket may refer to:

</doc>
<doc id="41912" url="https://en.wikipedia.org/wiki?curid=41912" title="Allotment">
Allotment

Allotment may refer to:

</doc>
<doc id="41913" url="https://en.wikipedia.org/wiki?curid=41913" title="Subscription (finance)">
Subscription (finance)

Subscription refers to the process of investors signing up and committing to invest in a financial instrument, before the actual closing of the purchase.
The term comes from the Latin word "subscribere".
Historical.
Praenumeration.
An early form was praenumeration, a common business practice in the 18th-century book trade in Germany. The publisher offered to sell a book that was planned but had not yet been printed, usually at a discount, so as to cover their costs in advance. The business practice was particularly common with magazines, helping to determine in advance how many subscribers there would be. Praenumeration is similar to the recent crowdfunding financing model.
New issues.
Subscription agreement.
Subscription to new issues can be covered by a subscription agreement, legally committing the investor to invest in the financial instrument, and committing the company to certain obligations and warranties. In some jurisdictions, it is possible for the issuer and subscriber to use a template subscription agreement as the basis of this agreement, although bespoke contract drafting by a qualified specialist may be required in more complex cases.
Subscription period.
When a new security is to be issued, investors typically have two weeks to submit their subscription orders. At the end of this "subscription period", the issuer announces the offering price and the method of allotment.
Allotment.
Allotment is a method of distributing securities to investors when an issue has been oversubscribed. At the end of the subscription period, the demand for a new issue can exceed the number of shares or bonds being issued. In such cases, the underwriting bank allots the securities with the approval of the issuer, either by lottery or on the basis of a formula. An allotment formula usually takes into account the issuer's preferred target investor groups.
Oversubscription.
A funding round is oversubscribed when the company has obtained for funding commitments from investors that in aggregate amount to more money than the company needs or intends to raise. It may be used informally to describe a state where there is more money available than the company needs.
Oversubscription adjustment.
A company can adjust oversubscription money with his receivable allotment money and calls money. Rest money will be returned to shareholders.

</doc>
<doc id="41914" url="https://en.wikipedia.org/wiki?curid=41914" title="Capital market">
Capital market

Capital markets are financial markets for the buying and selling of long-term debt or equity-backed securities. These markets channel the wealth of savers to those who can put it to long-term productive use, such as companies or governments making long-term investments. Capital markets are defined as markets in which money is provided for periods longer than a year.
Financial regulators, such as the UK's Bank of England (BoE) or the U.S. Securities and Exchange Commission (SEC), oversee the capital markets in their jurisdictions to protect investors against fraud, among other duties.
Modern capital markets are almost invariably hosted on computer-based electronic trading systems; most can be accessed only by entities within the financial sector or the treasury departments of governments and corporations, but some can be accessed directly by the public. There are many thousands of such systems, most serving only small parts of the overall capital markets. Entities hosting the systems include stock exchanges, investment banks, and government departments. Physically the systems are hosted all over the world, though they tend to be concentrated in financial centres like London, New York, and Hong Kong.
A key division within the capital markets is between the primary markets and secondary markets. In primary markets, new stock or bond issues are sold to investors, often via a mechanism known as underwriting. The main entities seeking to raise long-term funds on the primary capital markets are governments (which may be municipal, local or national) and business enterprises (companies). Governments tend to issue only bonds, whereas companies often issue either equity or bonds. The main entities purchasing the bonds or stock include pension funds, hedge funds, sovereign wealth funds, and less commonly wealthy individuals and investment banks trading on their own behalf. In the secondary markets, existing securities are sold and bought among investors or traders, usually on an exchange, over-the-counter, or elsewhere. The existence of secondary markets increases the willingness of investors in primary markets, as they know they are likely to be able to swiftly cash out their investments if the need arises.
A second important division falls between the stock markets (for equity securities, also known as shares, where investors acquire ownership of companies) and the bond markets (where investors become creditors).
Difference between money markets and capital markets.
The money markets are used for the raising of short term finance, sometimes for loans that are expected to be paid back as early as overnight. Whereas the "capital markets" are used for the raising of long term finance, such as the purchase of shares, or for loans that are not expected to be fully paid back for at least a year.
Funds borrowed from the "money markets" are typically used for general operating expenses, to cover brief periods of liquidity. For example, a company may have inbound payments from customers that have not yet cleared, but may wish to immediately pay out cash for its payroll. When a company borrows from the primary "capital markets", often the purpose is to invest in additional physical capital goods, which will be used to help increase its income. It can take many months or years before the investment generates sufficient return to pay back its cost, and hence the finance is long term.
Together, "money markets" and "capital markets" form the financial markets as the term is narrowly understood. The capital market is concerned with long term finance. In the widest sense, it consists of a series of channels through which the savings of the community are made available for industrial and commercial enterprises and public authorities.
Difference between regular bank lending and capital markets.
Regular bank lending is not usually classed as a capital market transaction, even when loans are extended for a period longer than a year. A key difference is that with a regular bank loan, the lending is not securitised (i.e., it doesn't take the form of resalable security like a share or bond that can be traded on the markets). A second difference is that lending from banks and similar institutions is more heavily regulated than capital market lending. A third difference is that bank depositors and shareholders tend to be more risk averse than capital market investors. The previous three differences all act to limit institutional lending as a source of finance. Two additional differences, this time favoring lending by banks, are that banks are more accessible for small and medium companies, and that they have the ability to create money as they lend. In the 20th century, most company finance apart from share issues was raised by bank loans. But since about 1980 there has been an ongoing trend for disintermediation, where large and credit worthy companies have found they effectively have to pay out less in interest if they borrow direct from capital markets rather than banks. The tendency for companies to borrow from capital markets instead of banks has been especially strong in the US. According to Lena Komileva writing for "The Financial Times", Capital Markets overtook bank lending as the leading source of long term finance in 2009 - this reflects the additional risk aversion and regulation of banks following the 2008 financial crisis.
Examples of capital market transactions.
A government raising money on the primary markets.
When a government wants to raise long term finance it will often sell bonds to the capital markets. In the 20th and early 21st century, many governments would use investment banks to organize the sale of their bonds. The leading bank would underwrite the bonds, and would often head up a syndicate of brokers, some of whom might be based in other investment banks. The syndicate would then sell to various investors. For developing countries, a multilateral development bank would sometimes provide an additional layer of underwriting, resulting in risk being shared between the investment bank(s), the multilateral organization, and the end investors. However, since 1997 it has been increasingly common for governments of the larger nations to bypass investment banks by making their bonds directly available for purchase over the Internet. Many governments now sell most of their bonds by computerized auction. Typically large volumes are put up for sale in one go; a government may only hold a small number of auctions each year. Some governments will also sell a continuous stream of bonds through other channels. The biggest single seller of debt is the US Government; there are usually several transactions for such sales every second, which corresponds to the continuous updating of the US real time debt clock.
A company raising money on the primary markets.
When a company wants to raise money for long-term investment, one of its first decisions is whether to do so by issuing bonds or shares. If it chooses shares, it avoids increasing its debt, and in some cases the new shareholders may also provide non monetary help, such as expertise or useful contacts. On the other hand, a new issue of shares can dilute the ownership rights of the existing shareholders, and if they gain a controlling interest, the new shareholders may even replace senior managers. From an investor's point of view, shares offer the potential for higher returns and capital gains if the company does well. Conversely, bonds are safer if the company does poorly, as they are less prone to severe falls in price, and in the event of bankruptcy, bond owners are usually paid before shareholders.
When a company raises finance from the primary market, the process is more likely to involve face-to-face meetings than other capital market transactions. Whether they choose to issue bonds or shares, companies will typically enlist the services of an investment bank to mediate between themselves and the market. A team from the investment bank often meets with the company's senior managers to ensure their plans are sound. The bank then acts as an underwriter, and will arrange for a network of brokers to sell the bonds or shares to investors. This second stage is usually done mostly through computerized systems, though brokers will often phone up their favored clients to advise them of the opportunity. Companies can avoid paying fees to investment banks by using a direct public offering, though this is not a common practice as it incurs other legal costs and can take up considerable management time.
Trading on the secondary markets.
Most capital market transactions take place on the secondary market. On the primary market, each security can be sold only once, and the process to create batches of new shares or bonds is often lengthy due to regulatory requirements. On the secondary markets, there is no limit on the number of times a security can be traded, and the process is usually very quick. With the rise of strategies such as high-frequency trading, a single security could in theory be traded thousands of times within a single hour. Transactions on the secondary market don't directly help raise finance, but they do make it easier for companies and governments to raise finance on the primary market, as investors know if they want to get their money back in a hurry, they will usually be easily able to re-sell their securities. Sometimes however secondary capital market transactions can have a negative effect on the primary borrowers - for example, if a large proportion of investors try to sell their bonds, this can push up the yields for future issues from the same entity. An extreme example occurred shortly after Bill Clinton began his first term as President of the United States; Clinton was forced to abandon some of the spending increases he'd promised in his election campaign due to pressure from the bond markets. In the 21st century, several governments have tried to lock in as much as possible of their borrowing into long dated bonds, so they are less vulnerable to pressure from the markets. Following the financial crisis of 2007–08, the introduction of Quantitative easing further reduced the ability of private actors to push up the yields of government bonds, at least for countries with a Central bank able to engage in substantial Open market operations.
A variety of different players are active in the secondary markets. Regular individuals account for a small proportion of trading, though their share has slightly increased; in the 20th century it was mostly only a few wealthy individuals who could afford an account with a broker, but accounts are now much cheaper and accessible over the internet. There are now numerous small traders who can buy and sell on the secondary markets using platforms provided by brokers which are accessible via web browsers. When such an individual trades on the capital markets, it will often involve a two-stage transaction. First they place an order with their broker, then the broker executes the trade. If the trade can be done on an exchange, the process will often be fully automated. If a dealer needs to manually intervene, this will often mean a larger fee. Traders in investment banks will often make deals on their bank's behalf, as well as executing trades for their clients. Investment banks will often have a division (or department) called "capital markets": staff in this division try to keep aware of the various opportunities in both the primary and secondary markets, and will advise major clients accordingly. Pension and sovereign wealth funds tend to have the largest holdings, though they tend to buy only the highest grade (safest) types of bonds and shares, and often don't trade all that frequently. According to a 2012 "Financial Times" article, hedge funds are increasingly making most of the short term trades in large sections of the capital market (like the UK and US stock exchanges), which is making it harder for them to maintain their historically high returns, as they are increasingly finding themselves trading with each other rather than with less sophisticated investors.
There are several ways to invest in the secondary market without directly buying shares or bonds. A common method is to invest in mutual funds or exchange-traded funds. It's also possible to buy and sell derivatives that are based on the secondary market; one of the most common being contract for difference - these can provide rapid profits, but can also cause buyers to lose more money than they originally invested.
Size of the global capital markets.
All figures given are in Billions of US$ and are sourced to the IMF. There is no universally recognized standard for measuring all of these figures, so other estimates may vary. A GDP column is included as a comparison.
Capital controls.
Capital controls are measures imposed by a state's government aimed at managing capital account transactions - in other words, capital market transactions where one of the counter-parties involved is in a foreign country. Whereas domestic regulatory authorities try to ensure that capital market participants trade fairly with each other, and sometimes to ensure institutions like banks don't take excessive risks, capital controls aim to ensure that the macroeconomic effects of the capital markets don't have a net negative impact on the nation in question. Most advanced nations like to use capital controls sparingly if at all, as in theory allowing markets freedom is a win-win situation for all involved: investors are free to seek maximum returns, and countries can benefit from investments that will develop their industry and infrastructure. However, sometimes capital market transactions can have a net negative effect - for example, in a financial crisis, there can be a mass withdrawal of capital, leaving a nation without sufficient foreign currency to pay for needed imports. On the other hand, if too much capital is flowing into a country, it can push up inflation and the value of the nation's currency, making its exports uncompetitive. Some nations such as India have also used capital controls to ensure that their citizens' money is invested at home, rather than abroad.

</doc>
<doc id="41915" url="https://en.wikipedia.org/wiki?curid=41915" title="Primary market">
Primary market

The primary market is the part of the capital market that deals with issuing of new securities. Companies, governments or public sector institutions can obtain funds through the sale of a new stock or bond issues through primary market. This is typically done through an investment bank or finance syndicate of securities dealers.
The process of selling new issues to investors is called underwriting. In the case of a new stock issue, this sale is an initial public offering (IPO). Dealers earn a commission that is built into the price of the security offering, though it can be found in the prospectus. Primary markets create long term instruments through which corporate entities borrow from capital market.
Once issued the securities typically trade on a secondary market such as a stock exchange, bond market or derivatives exchange.
Features.
Features of primary markets are:

</doc>
<doc id="41916" url="https://en.wikipedia.org/wiki?curid=41916" title="Financial market">
Financial market

A financial market is a market in which people trade financial securities, commodities, and other fungible items of value at low transaction costs and at prices that reflect supply and demand. Securities include stocks and bonds, and commodities include precious metals or agricultural products.
In economics, typically, the term "market" means the aggregate of possible buyers and sellers of a certain good or service and the transactions between them.
The term "market" is sometimes used for what are more strictly "exchanges", organizations that facilitate the trade in financial securities, e.g., a stock exchange or commodity exchange. This may be a physical location (like the NYSE, BSE, NSE) or an electronic system (like NASDAQ). Much trading of stocks takes place on an exchange; still, corporate actions (merger, spinoff) are outside an exchange, while any two companies or people, for whatever reason, may agree to sell stock from the one to the other without using an exchange.
Trading of currencies and bonds is largely on a bilateral basis, although some bonds trade on a stock exchange, and people are building electronic systems for these as well, similar to stock exchanges.
Types of financial markets.
Within the financial sector, the term "financial markets" is often used to refer just to the markets that are used to raise finance: for long term finance, the "Capital markets"; for short term finance, the "Money markets". Another common use of the term is as a catchall for all the markets in the financial sector, as per examples in the breakdown below.
The capital markets may also be divided into primary markets and secondary markets. Newly formed (issued) securities are bought or sold in primary markets, such as during initial public offerings. Secondary markets allow investors to buy and sell existing securities. The transactions in primary markets exist between issuers and investors, while secondary market transactions exist among investors.
Liquidity is a crucial aspect of securities that are traded in secondary markets. Liquidity refers to the ease with which a security can be sold without a loss of value. Securities with an active secondary market mean that there are many buyers and sellers at a given point in time. Investors benefit from liquid securities because they can sell their assets whenever they want; an illiquid security may force the seller to get rid of their asset at a large discount.
Raising capital.
Financial markets attract funds from investors and channel them to corporations—they thus allow corporations to finance their operations and achieve growth. Money markets allow firms to borrow funds on a short term basis, while capital markets allow corporations to gain long-term funding to support expansion (known as maturity transformation).
Without financial markets, borrowers would have difficulty finding lenders themselves. Intermediaries such as banks, Investment Banks, and Boutique Investment Banks can help in this process. Banks take deposits from those who have money to save. They can then lend money from this pool of deposited money to those who seek to borrow. Banks popularly lend money in the form of loans and mortgages.
More complex transactions than a simple bank deposit require markets where lenders and their agents can meet borrowers and their agents, and where existing borrowing or lending commitments can be sold on to other parties. A good example of a financial market is a stock exchange. A company can raise money by selling shares to investors and its existing shares can be bought or sold.
The following table illustrates where financial markets fit in the relationship between lenders and borrowers:
Lenders.
The lender temporarily gives money to somebody else, on the condition of getting back the principal amount together with some interest/profit or charge.
Individuals & Doubles.
Many individuals are not aware that they are lenders, but almost everybody does lend money in many ways. A person lends money when he or she:
Companies.
"Companies" tend to be lenders of capital. When companies have surplus cash that is not needed for a short period of time, they may seek to make money from their cash surplus by lending it via short term markets called money markets. Alternatively, such companies may decide to return the cash surplus to their shareholders (e.g. via a share repurchase or dividend payment).
Borrowers.
Governments borrow by issuing bonds. In the UK, the government also borrows from individuals by offering bank accounts and Premium Bonds. Government debt seems to be permanent. Indeed, the debt seemingly expands rather than being paid off. One strategy used by governments to reduce the "value" of the debt is to influence "inflation".
"Municipalities and local authorities" may borrow in their own name as well as receiving funding from national governments. In the UK, this would cover an authority like Hampshire County Council.
"Public Corporations" typically include nationalized industries. These may include the postal services, railway companies and utility companies.
Many borrowers have difficulty raising money locally. They need to borrow internationally with the aid of Foreign exchange markets.
Borrowers having similar needs can form into a group of borrowers. They can also take an organizational form like Mutual Funds. They can provide mortgage on weight basis. The main advantage is that this lowers the cost of their borrowings.
Derivative products.
During the 1980s and 1990s, a major growth sector in financial markets was the trade in so called derivative products, or derivatives for short.
In the financial markets, stock prices, bond prices, currency rates, interest rates and dividends go up and down, creating "risk". Derivative products are financial products which are used to "control" risk or paradoxically "exploit" risk. It is also called financial economics.
Derivative products or instruments help the issuers to gain an unusual profit from issuing the instruments. For using the help of these products a contract has to be made. Derivative contracts are mainly 4 types:
Seemingly, the most obvious buyers and sellers of currency are importers and exporters of goods. While this may have been true in the distant past, when international trade created the demand for currency markets, importers and exporters now represent only 1/32 of foreign exchange dealing, according to the Bank for International Settlements.
The picture of foreign currency transactions today shows:
Analysis of financial markets.
Much effort has gone into the study of financial markets and how prices vary with time. Charles Dow, one of the founders of Dow Jones & Company and The Wall Street Journal, enunciated a set of ideas on the subject which are now called Dow theory. This is the basis of the so-called technical analysis method of attempting to predict future changes. One of the tenets of "technical analysis" is that market trends give an indication of the future, at least in the short term. The claims of the technical analysts are disputed by many academics, who claim that the evidence points rather to the random walk hypothesis, which states that the next change is not correlated to the last change. The role of human psychology in price variations also plays a significant factor. Large amounts of volatility often indicate the presence of strong emotional factors playing into the price. Fear can cause excessive drops in price and greed can create bubbles. In recent years the rise of algorithmic and high-frequency program trading has seen the adoption of momentum, ultra-short term moving average and other similar strategies which are based on technical as opposed to fundamental or theoretical concepts of market Behaviour.
The scale of changes in price over some unit of time is called the volatility.
It was discovered by Benoît Mandelbrot that changes in prices do not follow a Gaussian distribution, but are rather modeled better by Lévy stable distributions. The scale of change, or volatility, depends on the length of the time unit to a power a bit more than 1/2. Large changes up or down are more likely than what one would calculate using a Gaussian distribution with an estimated standard deviation.
Role (Financial system and the economy).
One of the important sustainability requisite for the accelerated development of an economy is the existence of a dynamic financial market. A financial market helps the economy in the following manner. 
Constituents of Financial Market.
Based on market levels.
Simply put, primary market is the market where the newly started company issued shares to the public for the first time through IPO (initial public offering). Secondary market is the market where the second hand securities are sold (securitCommodity Marketies).

</doc>
<doc id="41918" url="https://en.wikipedia.org/wiki?curid=41918" title="Bond">
Bond

Bond, bonds, bonded, and bonding may refer to:

</doc>
<doc id="41919" url="https://en.wikipedia.org/wiki?curid=41919" title="Bonds">
Bonds

Bonds can refer to any of several things:

</doc>
<doc id="41920" url="https://en.wikipedia.org/wiki?curid=41920" title="Compact Disc Digital Audio">
Compact Disc Digital Audio

Compact Disc Digital Audio (CDDA or CD-DA) is the standard format for audio compact discs. The standard is defined in the "Red Book", one of a series of "Rainbow Books" (named for their binding colors) that contain the technical specifications for all CD formats.
Standard.
The "Red Book" specifies the physical parameters and properties of the CD, the optical "stylus" parameters, deviations and error rate, modulation system (eight-to-fourteen modulation, EFM) and error correction facility (cross-interleaved Reed–Solomon coding, CIRC), and the eight subcode channels. These parameters are common to all compact discs and used by all logical formats, such as CD-ROM. The standard also specifies the form of digital audio encoding: 2-channel signed 16-bit Linear PCM sampled at 44,100 Hz. Although rarely used, the specification allows for discs to be mastered with a form of emphasis.
The first edition of the "Red Book" was released in 1980 by Philips and Sony; it was adopted by the Digital Audio Disc Committee and ratified by the International Electrotechnical Commission Technical Committee 100, as an International Standard in 1987 with the reference IEC 60908. The second edition of IEC 60908 was published in 1999 and it cancels and replaces the first edition, amendment 1 (1992) and the corrigendum to amendment 1. The IEC 60908 however does not contain all the information for extensions that is available in the Red Book, such as the details for CD-Text, CD+G and CD+EG.
The standard is not freely available and must be licensed. It is available from Philips and the IEC. , Philips outsources licensing of the standard to Adminius, which charges for the "Red Book", plus each for the "Subcode Channels R-W" and "CD Text Mode" annexes.
Audio format.
The audio contained in a CD-DA consists of two-channel signed 16-bit Linear PCM sampled at 44,100 Hz.
Sample rate.
The sampling rate is adapted from that attained when recording digital audio on a PAL (or NTSC) videotape with a PCM adaptor, an earlier way of storing digital audio. An audio CD can represent frequencies up to 22.05 kHz, the Nyquist frequency of the 44.1 kHz sample rate.
The selection of the sample rate was based primarily on the need to reproduce the audible frequency range of 20–20,000 Hz (20 kHz). The Nyquist–Shannon sampling theorem states that a sampling rate of more than twice the maximum frequency of the signal to be recorded is needed, resulting in a required rate of at least 40 kHz. The exact sampling rate of 44.1 kHz was inherited from a method of converting digital audio into an analog video signal for storage on U-matic video tape, which was the most affordable way to transfer data from the recording studio to the CD manufacturer at the time the CD specification was being developed. The device that converts an analog audio signal into PCM audio, which in turn is changed into an analog video signal is called a PCM adaptor. This technology could store six samples (three samples per stereo channel) in a single horizontal line. 60 field/s black and white video (not 59.94 color) was required, and in NTSC countries (USA/Japan) that video signal has 245 usable lines per field, which works out to be (245 * 60 * 3) 44,100 samples/s/stereo channel. Similarly, PAL has 294 lines and 50 fields, which gives 44,100 samples/s/stereo channel. This system could store 14-bit samples with some error correction, or 16-bit samples with almost no error correction.
There was a long debate over the use of 14-bit (Philips) or 16-bit (Sony) quantization, and 44,056 or 44,100 samples/s (Sony) or approximately 44,000 samples/s (Philips). When the Sony/Philips task force designed the Compact Disc, Philips had already developed a 14-bit D/A converter (DAC), but Sony insisted on 16-bit. In the end, 16 bits and 44.1 kilosamples per second prevailed. Philips found a way to produce 16-bit quality using its 14-bit DAC by using four times oversampling.
Pre-emphasis.
Some CDs are mastered with pre-emphasis, an artificial boost of high audio frequencies. The pre-emphasis improves the apparent signal-to-noise ratio by making better use of the channel's dynamic range. On playback, the player applies a de-emphasis filter to restore the frequency response curve to an overall flat one. Pre-emphasis time constants are 50µs and 15µs (9.49 dB boost at 20kHz), and a binary flag in the disc subcode instructs the player to apply de-emphasis filtering if appropriate. Playback of such discs in a computer or 'ripping' to wave files typically does not take into account the pre-emphasis, so such files play back with a distorted frequency response.
Storage capacity and playing time.
The creators of the CD originally aimed at a playing time of 60 minutes with a disc diameter of 100 mm (Sony) or 115 mm (Philips). Sony vice-president Norio Ohga suggested extending the capacity to 74 minutes to accommodate Wilhelm Furtwängler's recording of Ludwig van Beethoven's Symphony No. 9 from the 1951 Bayreuth Festival. The additional 14-minute playing time subsequently required changing to a 120 mm disc. Kees Schouhamer Immink, Philips' chief engineer, however, denies this, claiming that the increase was motivated by technical considerations, and that even after the increase in size, the Furtwängler recording would not have fit on one of the earliest CDs.
According to a "Sunday Tribune" interview, the story is slightly more involved. In 1979, Philips owned PolyGram, one of the world's largest distributors of music. PolyGram had set up a large experimental CD plant in Hannover, Germany, which could produce huge numbers of CDs having a diameter of 115 mm. Sony did not yet have such a facility. If Sony had agreed on the 115-mm disc, Philips would have had a significant competitive edge in the market. The long playing time of Beethoven's Ninth Symphony imposed by Ohga was used to push Philips to accept 120 mm, so that Philips' PolyGram lost its edge on disc fabrication.
The 74-minute playing time of a CD, which was longer than the 22 minutes per side typical of long-playing (LP) vinyl albums, was often used to the CD's advantage during the early years when CDs and LPs vied for commercial sales. CDs would often be released with one or more bonus tracks, enticing consumers to buy the CD for the extra material. However, attempts to combine double LPs onto one CD occasionally resulted in the opposite situation in which the CD would actually offer fewer tracks than the equivalent LP, though bonus tracks were also added to CD re-releases of double LPs as well.
Playing times beyond 74 minutes are achieved by decreasing track pitch beyond the original "Red Book" standard. Most players can accommodate the more closely spaced data.
Current manufacturing processes allow an audio CD to contain up to 80 minutes (variable from one replication plant to another) without requiring the content creator to sign a waiver releasing the plant owner from responsibility if the CD produced is marginally or entirely unreadable by some playback equipment. Thus, in current practice, maximum CD playing time has crept higher by reducing minimum engineering tolerances; by and large, this has not unacceptably reduced reliability.
This table shows the progression in the maximum duration of released audio CDs:
Technical specifications.
Data encoding.
Each audio sample is a signed 16-bit two's complement integer, with sample values ranging from −32768 to +32767. The source audio data is divided into frames, containing twelve samples each (six left and right samples, alternating), for a total of 192 bits (24 bytes) of audio data per frame.
This stream of audio frames, as a whole, is then subjected to CIRC encoding, which segments and rearranges the data and expands it with parity bits in a way that allows occasional read errors to be detected and corrected. CIRC encoding also interleaves the audio frames throughout the disc over several consecutive frames so that the information will be more resistant to burst errors. Therefore, a physical frame on the disc will actually contain information from multiple logical audio frames. This process adds 64 bits of error correction data to each frame. After this, 8 bits of subcode or subchannel data are added to each of these encoded frames, which is used for control and addressing when playing the CD.
CIRC encoding plus the subcode byte generate 33-bytes long frames, called "channel-data" frames. These frames are then modulated through eight-to-fourteen modulation (EFM), where each 8-bit word is replaced with a corresponding 14-bit word designed to reduce the number of transitions between 0 and 1. This reduces the density of physical pits on the disc and provides an additional degree of error tolerance. Three "merging" bits are added before each 14-bit word for disambiguation and synchronization. In total there are 33 × (14 + 3) = 561 bits. A 27-bit word (a 24-bit pattern plus 3 merging bits) is added to the beginning of each frame to assist with synchronization, so the reading device can locate frames easily. With this, a frame ends up containing 588 bits of "channel data" (which are decoded to only 192 bits music).
The frames of channel data are finally written to disc physically in the form of pits and lands, with each pit or land representing a series of zeroes, and with the transition points—the edge of each pit—representing 1.
A Red Book-compatible CD-R has pit-and-land-shaped spots on a layer of organic dye instead of actual pits and lands; a laser creates the spots by altering the reflective properties of the dye.
Data structure.
The audio data stream in an audio CD is continuous, but has three parts. The main portion, which is further divided into playable audio tracks, is the "program area". This section is preceded by a "lead-in" track and followed by a "lead-out" track. The lead-in and lead-out tracks encode only silent audio, but all three sections contain subcode data streams.
The lead-in's subcode contains repeated copies of the disc's Table Of Contents (TOC), which provides an index of the start positions of the tracks in the program area and lead-out. The track positions are referenced by absolute timecode, relative to the start of the program area, in MSF format: minutes, seconds, and fractional seconds called "frames". Each timecode frame is one seventy-fifth of a second, and corresponds to a block of 98 channel-data frames—ultimately, a block of 588 pairs of left and right audio samples. Timecode contained in the subchannel data allows the reading device to locate the region of the disc that corresponds to the timecode in the TOC. The TOC on discs is analogous to the partition table on hard drives. Nonstandard or corrupted TOC records are abused as a form of CD/DVD copy protection, in e.g. the key2Audio scheme.
Tracks.
The largest entity on a CD is called a track. A CD can contain up to 99 tracks (including a data track for mixed mode discs). Each track can in turn have up to 100 indexes, though players which handle this feature are rarely found outside of pro audio, particularly radio broadcasting. The vast majority of songs are recorded under index 1, with the pre-gap being index 0. Sometimes hidden tracks are placed at the end of the last track of the disc, often using index 2 or 3. This is also the case with some discs offering "101 sound effects", with 100 and 101 being indexed as two and three on track 99. The index, if used, is occasionally put on the track listing as a decimal part of the track number, such as 99.2 or 99.3. (Information Society's "Hack" was one of very few CD releases to do this, following a release with an equally obscure CD+G feature.) The track and index structure of the CD were carried forward to the DVD format as title and chapter, respectively.
Tracks, in turn, are divided into timecode frames (or sectors), which are further subdivided into channel-data frames.
Frames and timecode frames.
The smallest entity in a CD is a channel-data "frame", which consists of 33 bytes and contains six complete 16-bit stereo samples: 24 bytes for the audio (two bytes × two channels × six samples = 24 bytes), eight CIRC error-correction bytes, and one subcode byte. As described in the "Data encoding" section, after the EFM modulation the number of bits in a frame totals 588.
On a "Red Book" audio CD, data is addressed using the "MSF scheme", with timecodes expressed in minutes, seconds and another type of "frames" (mm:ss:ff), where one frame corresponds to 1/75th of a second of audio: 588 pairs of left and right samples. This timecode frame is distinct from the 33-byte channel-data frame described above, and is used for time display and positioning the reading laser. When editing and extracting CD audio, this timecode frame is the smallest addressable time interval for an audio CD; thus, track boundaries only occur on these frame boundaries. Each of these structures contains 98 channel-data frames, totaling 98 × 24 = 2,352 bytes of music. The CD is played at a speed of 75 frames (or sectors) per second, thus 44,100 samples or 176,400 bytes per second.
In the 1990s, CD-ROM and related Digital Audio Extraction (DAE) technology introduced the term "sector" to refer to each timecode frame, with each sector being identified by a sequential integer number starting at zero, and with tracks aligned on sector boundaries. An audio CD sector corresponds to 2,352 bytes of decoded data. The "Red Book" does not refer to sectors, nor does it distinguish the corresponding sections of the disc's data stream except as "frames" in the MSF addressing scheme.
The following table shows the relation between tracks, timecode frames (sectors) and channel-data frames:
Bit rate.
The audio bit rate for a "Red Book" audio CD is 1,411,200 bits per second or 176,400 bytes per second; 2 channels × 44,100 samples per second per channel × 16 bits per sample. Audio data coming in from a CD is contained in sectors, each sector being 2,352 bytes, and with 75 sectors containing 1 second of audio. For comparison, the bit rate of a "1×" CD-ROM is defined as 2,048 bytes per sector × 75 sectors per second = 153,600 bytes per second. The remaining 304 bytes in a sector are used for additional data error correction.
Data access from computers.
Unlike on a DVD or CD-ROM, there are no "files" on a "Red Book" audio CD; there is only one continuous stream of LPCM audio data, and a parallel, smaller set of 8 subcode data streams. Computer operating systems, however, may provide access to an audio CD as if it contains files. For example, Windows represents the CD's Table of Contents as a set of Compact Disc Audio track (CDA) files, each file containing indexing information, not audio data.
In a process called ripping, digital audio extraction software can be used to read CD-DA audio data and store it in files. Common audio file formats for this purpose include WAV and AIFF, which simply preface the LPCM data with a short header; FLAC, ALAC, and Windows Media Audio Lossless, which compress the LPCM data in ways that conserve space yet allow it to be restored without any changes; and various lossy, perceptual coding formats like MP3 and AAC, which modify and compress the audio data in ways that irreversibly change the audio, but that exploit features of human hearing to make the changes difficult to discern.
Format variations.
Recording publishers have created CDs that violate the "Red Book" standard. Some do so for the purpose of copy prevention, using systems like Copy Control. Some do so for extra features such as DualDisc, which includes both a CD layer and a DVD layer whereby the CD layer is much thinner, 0.9 mm, than required by the "Red Book", which stipulates a nominal 1.2 mm, but at least 1.1 mm. Philips and many other companies have stated that including the Compact Disc Digital Audio logo on such non-conforming discs may constitute trademark infringement. Either in anticipation or in response, recent copy-protected CDs bear stickers and warnings that the CD is not standard and may not play in all CD players, and no longer display the long-familiar logo.
Super Audio CD was a standard published in 1999 that aimed to provide better audio quality in CDs, but it never became very popular. DVD Audio, an advanced version of the audio CD, emerged in 1999. The format was designed to feature audio of higher fidelity. It applies a higher sampling rate and used 650 nm lasers.
Copyright issues.
There have been moves by the recording industry to make audio CDs (Compact Disc Digital Audio) unplayable on computer CD-ROM drives, to prevent the copying of music. This is done by intentionally introducing errors onto the disc that the embedded circuits on most stand-alone audio players can automatically compensate for, but which may confuse CD-ROM drives. Consumer rights advocates as of October 2001 pushed to require warning labels on compact discs that do not conform to the official Compact Disc Digital Audio standard (often called the Red Book) to inform consumers which discs do not permit full fair use of their content.
In 2005, Sony BMG Music Entertainment was criticised when a copy protection mechanism known as Extended Copy Protection (XCP) used on some of their audio CDs automatically and surreptitiously installed copy-prevention software on computers (see 2005 Sony BMG CD copy protection scandal). Such discs are not legally allowed to be called CDs or Compact Discs because they break the Red Book standard governing CDs, and Amazon.com for example describes them as "copy protected discs" rather than "compact discs" or "CDs".

</doc>
<doc id="41924" url="https://en.wikipedia.org/wiki?curid=41924" title="Mnemonic major system">
Mnemonic major system

The Major System (also called the phonetic number system, phonetic mnemonic system, or Herigone's mnemonic system) is a mnemonic technique used to aid in memorizing numbers.
The system works by converting numbers into consonant sounds, then into words by adding vowels. The system works on the principle that images can be remembered more easily than numbers.
The system.
Each numeral is associated with one or more consonants. Vowels and the consonants "w", "h", and "y" are ignored. These can be used as "fillers" to make sensible words from the resulting consonant sequences. A standard mapping is:
The groups of similar sounds and the rules for applying the mappings are almost always fixed, but other hooks and mappings can be used as long as the person using the system can remember them and apply them consistently.
Each numeral maps to a set of similar sounds with similar mouth and tongue positions. The link is phonetic, that is to say, it is the consonant sounds that matter, not the spelling. Therefore, a word like "action" would encode the number "762" (/k/-/ʃ/-/n/), not "712" ("k"-"t"-"n"). Double letters are disregarded when not pronounced separately, e.g. "muddy" encodes "31" (/m/-/d/), not "311", but "midday" encodes "311" (/m/-/d/-/d/) while "accept" encodes "7091" (/k/-/s/-/p/-/t/) since the "d"s and "c"s are pronounced separately. "x" encodes "70" when pronounced as /ks/ or /gz/ (e.g. in "fax" and "exam") and "76" when pronounced /kʃ/ or /gʒ/ (e.g. in "action" or "luxury"); "z" encodes "10" when pronounced /ts/ (e.g. in "pizza"). In "ghost" ("701", /g/-/s/-/t/) and "enough" ("28", /n/-/f/), "gh" is being encoded by different numerals. Usually, a rhotic accent is assumed, e.g. "fear" would encode "84" (/f/-/r/) rather than "8" (/f/).
Often the mapping is compact. "Hindquarters", for example, translates unambiguously to "2174140" (/n/-/d/-/k/-/r/-/t/-/r/-/z/), which amounts to seven digits encoded by eight letters, and can be easily visualized.
Each numeral maps to a set of similar sounds with similar mouth and tongue positions. 
For most people it would be easier to remember "3.1415927" (an approximation of the mathematical constant pi) as:
Short term visual memory of imagined scenes allows large numbers of digits to be memorized with ease, though usually only for a short time.
Whilst this is unwieldy at first, with practice it can become a very effective technique. Longer-term memory may require the formulation of more object-related mnemonics with greater logical connection, perhaps forming grammatical sentences that apply to the matter rather than just strings of images.
The system can be employed with phone numbers. One would typically make up multiple words, preferably a sentence, or an ordered sequence of images featuring the owner of the number.
The Major System can be combined with a peg system for remembering lists, and is sometimes used also as a method of generating the pegs. It can also be combined with other memory techniques such as rhyming, substitute words, or the method of loci. Repetition and concentration using the ordinary memory is still required.
An advantage of the major system is that it is possible to use a computer to automatically translate the number into a set of words. One can then pick the best of several alternatives. Such programs include "Numzi" "Rememberg" "Fonbee", the freeware "2Know", and the website "pinfruit".
Example words.
Some of these example words may belong to more than one word category.
History.
A different memory system, the method of loci was taught to schoolchildren for centuries, at least until 1584, "when Puritan reformers declared it unholy for encouraging bizarre and irreverent images." The same objection can be made over the major system, with or without the method of loci. Mental images may be easier to remember if they are insulting, violent, or obscene (see Von Restorff effect).
Pierre Hérigone (1580–1643) was a French mathematician and astronomer and devised the earliest version of the major system. The major system was further developed by Stanislaus Mink von Wennsshein 300 years ago. It was later elaborated upon by other users. In 1730, Richard Grey set forth a complicated system that used both consonants and vowels to represent the digits. In 1808 Gregor von Feinaigle introduced the improvement of representing the digits by consonant sounds (but reversed the values of 8 and 9 compared to those listed above).
In 1825 Aimé Paris published the first known version of the major system in its modern form.
In 1844 Francis Fauvel Gouraud (1808-1847) delivered a series of lectures introducing his mnemonic system which was based on Aimé Paris' version. The lectures drew some of the largest crowds ever assembled to hear lectures of a "scientific" nature up to that time. This series of lectures was later published as "Phreno-Mnemotechny or The Art of Memory" in 1845 and his system received wide acclaim. According to Gouraud, Richard Grey indicated that a discussion on Hebrew linguistics in William Beveridge (bishop)'s "Institutionum chronotogicarum libri duo, una cum totidem arithmetices chronologicæ libellis" (London, 1669) inspired him to create his system of mnemotechniques which later evolved in to the major system.
The system described in this article would again be popularized by Harry Lorayne, a best selling contemporary author on memory.
The name "major system" refers to Major Beniowski, who published a version of the system in his book, "The Anti-Absurd or Phrenotypic English Pronouncing and Orthographical Dictionary".
There is a reasonable historical possibility that the roots of the Major System are entangled with older systems of Shorthand. It is certainly the case that the underlying structure of the Major System has a direct overlap with Gregg shorthand, which was a popular shorthand system in the late 1800s and early 1900s.
Phonetic number memorization systems also occur in other parts of the world, such as the Katapayadi system going back to at least the 7th Century in India.
Practice.
Memory feats centred around numbers can be performed by experts who have learned a 'vocabulary' of at least 1 image for every 1 and 2 digit number which can be combined to form narratives. To learn a vocabulary of 3 digit numbers is harder because for each extra digit 10 times more images need to be learned, but many mnemonists use a set of 1000 images. Combination of images into a narrative is easier to do rapidly than is forming a coherent, grammatical sentence. This pre-memorisation and practice at forming images reduces the time required to think up a good imaginary object and create a strong memorable impression of it. The best words for this purpose are usually nouns, especially those for distinctive objects which make a strong impression on a variety of senses (e.g. a "Lime" for 53, its taste, its smell, its colour and even its texture are distinctive) or which move (like an "arrow" for 4).
For basic proficiency a large vocabulary of image words isn't really necessary since, when the table above is reliably learned, it is easy to form your own words ad hoc.
Indexing Sequences.
Mnemonics often centre around learning a complete sequence where all objects in that sequence that come before the one you are trying to recall must be recalled first. For instance, if you were using the mnemonic "Richard of York gave battle in vain" for the colours of the rainbow; (red, orange, yellow, green, blue, indigo and violet) to remember what colour comes after indigo you would have to recall the whole sequence. For a short sequence this may be trivial; for longer lists, it can become complicated and error-prone.
A good example would be in recalling what is the 53rd element of the periodic table. It might be possible for some people to construct and then learn a string of 53 or more items which you have substituted for the elements and then to recall them one by one, counting them off as you go, but it would be a "great deal easier" and less laborious/tedious to "directly" associate element 53 with, for example, a lime (a suitable mnemonic for 53) recalling some prior imagining of yours regarding a mishap where lime juice gets into one's eye - "eye" sounding like "I", the symbol for "Iodine". This allows for random access directly to the item, without the need for recalling any previous items.
If you were remembering element 53 in the process of recalling the periodic table you could then recall an image for 54, for instance thinking of a friend called "Laura" (54) in the lotus position looking very Zen-like in order to remind yourself that element 54 is "Xenon".
This is an example of combining the Major System with the peg system.

</doc>
<doc id="41926" url="https://en.wikipedia.org/wiki?curid=41926" title="Nearest neighbour algorithm">
Nearest neighbour algorithm

The nearest neighbour algorithm was one of the first algorithms used to determine a solution to the travelling salesman problem. In it, the salesman starts at a random city and repeatedly visits the nearest city until all have been visited. It quickly yields a short tour, but usually not the optimal one.
Example with the travelling salesman problem.
Below is the application of nearest neighbour algorithm to the travelling salesman problem.
These are the steps of the algorithm:
The sequence of the visited vertices is the output of the algorithm.
The nearest neighbour algorithm is easy to implement and executes quickly, but it can sometimes miss shorter routes which are easily noticed with human insight, due to its "greedy" nature. As a general guide, if the last few stages of the tour are comparable in length to the first stages, then the tour is reasonable; if they are much greater, then it is likely that there are much better tours. Another check is to use an algorithm such as the lower bound algorithm to estimate if this tour is good enough.
In the worst case, the algorithm results in a tour that is much longer than the optimal tour. To be precise, for every constant r there is an instance of the traveling salesman problem such that the length of the tour computed by the nearest neighbour algorithm is greater than r times the length of the optimal tour. Moreover, for each number of cities there is an assignment of distances between the cities for which the nearest neighbor heuristic produces the unique worst possible tour.
The nearest neighbour algorithm may not find a feasible tour at all, even when one exists.
References.
G. Gutin, A. Yeo and A. Zverovich, Traveling salesman should not be greedy: domination analysis of
greedy-type heuristics for the TSP. Discrete Applied Mathematics 117 (2002), 81-86. 
J. Bang-Jensen, G. Gutin and A. Yeo, When the greedy
algorithm fails. Discrete Optimization 1 (2004), 121-127. 
G. Bendall and F. Margot, Greedy Type Resistance of
Combinatorial Problems, Discrete Optimization 3 (2006), 288-298.

</doc>
<doc id="41927" url="https://en.wikipedia.org/wiki?curid=41927" title="Signal generator">
Signal generator

Signal generators, also known variously as function generators, RF and microwave signal generators, pitch generators, arbitrary waveform generators, digital pattern generators or frequency generators, are electronic devices that generate repeating or non-repeating electronic signals (in either the analog or digital domains). They are generally used in designing, testing, troubleshooting, and repairing electronic or electroacoustic devices; though they often have artistic uses as well.
There are many different types of signal generators, with different purposes and applications (and at varying levels of expense); in general, no device is suitable for all possible applications.
Traditionally, signal generators have been embedded hardware units, but since the age of multimedia-PCs, flexible, programmable software tone generators have also been available.
General purpose signal generators.
Function generators.
A function generator is a device which produces simple repetitive waveforms. Such devices contain an electronic oscillator, a circuit that is capable of creating a repetitive waveform. (Modern devices may use digital signal processing to synthesize waveforms, followed by a digital to analog converter, or DAC, to produce an analog output). The most common waveform is a sine wave, but sawtooth, step (pulse), square, and triangular waveform oscillators are commonly available as are arbitrary waveform generators (AWGs). If the oscillator operates above the audio frequency range (>20 kHz), the generator will often include some sort of modulation function such as amplitude modulation (AM), frequency modulation (FM), or phase modulation (PM) as well as a second oscillator that provides an audio frequency modulation waveform.
Arbitrary waveform generators.
Arbitrary waveform generators, or AWGs, are sophisticated signal generators which allow the user to generate arbitrary waveforms, within published limits of frequency range, accuracy, and output level. Unlike function generators, which are limited to a simple set of waveforms, an AWG allows the user to specify a source waveform in a variety of different ways. AWGs are generally more expensive than function generators, and are often more highly limited in available bandwidth; as a result, they are generally limited to higher-end design and test applications.
RF and microwave signal generators.
RF (radio frequency) and microwave signal generators are used for testing components, receivers and test systems in a wide variety of applications including cellular communications, WiFi, WiMAX, GPS, audio and video broadcasting, satellite communications, radar and electronic warfare. RF and microwave signal generators normally have similar features and capabilities, but are differentiated by frequency range. RF signal generators typically range from a few kHz to 6 GHz, while microwave signal generators cover a much wider frequency range, from less than 1 MHz to at least 20 GHz. Some models go as high as 70 GHz with a direct coaxial output, and up to hundreds of GHz when used with external waveguide source modules. RF and microwave signal generators can be classified further as analog or vector signal generators.
Analog signal generators.
Analog signal generators based on a sine-wave oscillator were common before the inception of digital electronics, and are still used. There was a sharp distinction in purpose and design of radio-frequency and audio-frequency signal generators.
RF signal generators are capable of producing CW (continuous wave) tones. The output frequency can usually be tuned anywhere in their frequency range. Many models offer various types of analog modulation, either as standard equipment or as an optional capability to the base unit. This could include AM, FM, ΦM (phase modulation) and pulse modulation. Another common feature is a built-in attenuator which makes it possible to vary the signal’s output power. Depending on the manufacturer and model, output powers can range from -135 to +30 dBm. A wide range of output power is desirable, since different applications require different amounts of signal power. For example, if a signal has to travel through a very long cable out to an antenna, a high output signal may be needed to overcome the losses through the cable and still have sufficient power at the antenna. But when testing receiver sensitivity, a low signal level is required to see how the receiver behaves under low signal-to-noise conditions.
RF signal generators are available as benchtop instruments, rackmount instruments, embeddable modules and in card-level formats. Mobile, field-testing and airborne applications benefit from lighter, battery-operated platforms. In automated and production testing, web-browser access, which allows multi-source control, and faster frequency switching speeds improve test times and throughput.
RF signal generators are required for servicing and setting up analog radio receivers, and are used for professional RF applications.
Audio-frequency signal generators generate signals in the audio-frequency range and above. An early example was the HP200A Audio Oscillator, the first product sold by the Hewlett-Packard Company in 1939. Applications include checking frequency response of audio equipment, and many uses in the electronic laboratory.
Equipment distortion can be measured using a very-low-distortion audio generator as the signal source, with appropriate equipment to measure output distortion harmonic-by-harmonic with a wave analyser, or simply total harmonic distortion. A distortion of 0.0001% can be achieved by an audio signal generator with a relatively simple circuit.
Vector signal generators.
With the advent of digital communications systems, it is no longer possible to adequately test these systems with traditional analog signal generators. This has led to the development of vector signal generators, also known as digital signal generators. These signal generators are capable of generating digitally-modulated radio signals that may use any of a large number of digital modulation formats such as QAM, QPSK, FSK, BPSK, and OFDM. In addition, since modern commercial digital communication systems are almost all based on well-defined industry standards, many vector signal generators can generate signals based on these standards. Examples include GSM, W-CDMA (UMTS), CDMA2000, LTE, Wi-Fi (IEEE 802.11), and WiMAX (IEEE 802.16). In contrast, military communication systems such as JTRS, which place a great deal of importance on robustness and information security, typically use very proprietary methods. To test these types of communication systems, users will often create their own custom waveforms and download them into the vector signal generator to create the desired test signal.
Logic signal generators.
Also known as 'data pattern generator' or more often 'digital pattern generator', this type of signal generators produces logic types of signals - that is logic 1s and 0s in the form of conventional voltage levels. The usual voltage standards are: LVTTL, LVCMOS.
As such, they must be distinguished from 'pulse/pattern generators', which refers to signal generators able to generate logic pulses with different analog characteristics (such as pulse rise/fall time, high level length, ...).
Logic signal generators (digital pattern generators) are used as stimulus source for digital integrated circuits and embedded systems - for functional validation and testing.
Special purpose signal generators.
In addition to the above general-purpose devices, there are several classes of signal generators designed for specific applications.
Pitch generators and audio generators.
A pitch generator is a type of signal generator optimized for use in audio and acoustics applications. Pitch generators typically include sine waves over the audio frequency range (20 Hz–20 kHz). Sophisticated pitch generators will also include sweep generators (a function which varies the output frequency over a range, in order to make frequency-domain measurements), multipitch generators (which output several pitches simultaneously, and are used to check for intermodulation distortion and other non-linear effects), and tone bursts (used to measure response to transients). Pitch generators are typically used in conjunction with sound level meters, when measuring the acoustics of a room or a sound reproduction system, and/or with oscilloscopes or specialized audio analyzers. 
Many pitch generators operate in the digital domain, producing output in various digital audio formats such as AES3, or SPDIF. Such generators may include special signals to stimulate various digital effects and problems, such as clipping, jitter, bit errors; they also often provide ways to manipulate the metadata associated with digital audio formats.
The term synthesizer is used for a device that generates audio signals for music, or that uses slightly more intricate methods.
Computer programs.
Computer programs can be used to generate arbitrary waveforms on a general-purpose computer and output the waveform via an output interface. Such programs may be provided commercially or be freeware. Simple systems use a standard computer sound card as output device, limiting the accuracy of the output waveform and limiting frequency to lie within the audio-frequency band.
Video signal generators.
A video signal generator is a device which outputs predetermined video and/or television waveforms, and other signals used to stimulate faults in, or aid in parametric measurements of, television and video systems. There are several different types of video signal generators in widespread use. Regardless of the specific type, the output of a video generator will generally contain synchronization signals appropriate for television, including horizontal and vertical sync pulses (in analog) or sync words (in digital). Generators of "composite" video signals (such as NTSC and PAL) will also include a colorburst signal as part of the output. Video signal generators are available for a wide variety of applications, and for a wide variety of digital formats; many of these also include audio generation capability (as the audio track is an important part of any video or television program or motion picture).
Technical trends driving the ARB industry.
New high-speed DACs provide up to 16-bit resolution at sample rates in excess of 1 GS/s. These devices provide the foundation for an AWG with the bandwidth and dynamic range to address modern radio and communication applications. In combination with a quadrature modulator and advanced digital signal processing, high-speed DACs can be applied to create a full-featured vector signal generator with very high modulation bandwidth. Example applications include commercial wireless standards such as Wi-Fi (IEEE 802.11), WiMAX (IEEE 802.16) and LTE, in addition to military standards such as those specified in the Joint Tactical Radio System (JTRS) initiative. Also, broad modulation bandwidth allows multi-carrier signal generation, necessary for testing receiver adjacent channel rejection.

</doc>
<doc id="41928" url="https://en.wikipedia.org/wiki?curid=41928" title="Klein four-group">
Klein four-group

In mathematics, the Klein four-group (or just Klein group or Vierergruppe (), often symbolized by the letter V or as K4) is the group , the direct product of two copies of the cyclic group of order 2. It was named "Vierergruppe" by Felix Klein in 1884. The Klein group's Cayley table is given by:
The Klein four-group is also defined by the group presentation
All non-identity elements of the Klein group have order 2, thus any two non-identity elements can serve as generators in the above presentation. The Klein four-group is the smallest non-cyclic group. It is however an abelian group, and isomorphic to the dihedral group of order (cardinality) 4; it is the only dihedral group that is abelian.
The Klein four-group is also isomorphic to the direct sum , so that it can be represented as the pairs under component-wise addition modulo 2 (or equivalently the bit strings under bitwise XOR); with (0,0) being the group's identity element. The Klein four-group is thus an example of an elementary abelian 2-group, which is also called a Boolean group. The Klein four-group is thus also the group generated by the symmetric difference as the binary operation on the subsets of a powerset of a set with two elements, i.e. over a field of sets with four elements, e.g. formula_2; the empty set is the group's identity element in this case.
Another numerical construction of the Klein four-group is the set with the operation being multiplication modulo 8. Here "a" is 3, "b" is 5, and is .
Geometrically, in two dimensions the Klein four-group is the symmetry group of a rhombus and of a rectangle which are not squares, the four elements being the identity, the vertical reflection, the horizontal reflection, and a 180 degree rotation.
In three dimensions there are three different symmetry groups that are algebraically the Klein four-group V:
The three elements of order 2 in the Klein four-group are interchangeable: the automorphism group of "V" is the group of permutations of these three elements. 
The Klein four-group's permutations of its own elements can be thought of abstractly as its permutation representation on 4 points:
In this representation, V is a normal subgroup of the alternating group A4
(and also the symmetric group S4) on 4 letters. In fact, it is the kernel of a surjective group homomorphism from S4 to S3.
According to Galois theory, the existence of the Klein four-group (and in particular, this representation of it) explains the existence of the formula for calculating the roots of quartic equations in terms of radicals, as established by Lodovico Ferrari:
the map corresponds to the resolvent cubic, in terms of Lagrange resolvents.
The Klein four-group as a subgroup of A4 is not the automorphism group of any simple graph. It is, however, the automorphism group of a two-vertex graph where the vertices are connected to each other with "two" edges, making the graph non-simple. It is also the automorphism group of the following simple graph, but in the permutation representation where the points are labeled top-left, bottom-left, top-right, bottom-right:
In the construction of finite rings, eight of the eleven rings with four elements have the Klein four-group as their additive substructure.
If R× denotes the multiplicative group of non-zero reals and R+ the multiplicative group of positive reals, R× × R× is the group of units of the ring , and is a subgroup of (in fact it is the component of the identity of ). The quotient group is isomorphic to the Klein four-group. In a similar fashion, the group of units of the split-complex number ring, when divided by its identity component, also results in the Klein four-group.
In music composition the four-group is the basic group of permutations in the twelve-tone technique. In that instance the Cayley table is written;

</doc>
<doc id="41930" url="https://en.wikipedia.org/wiki?curid=41930" title="Dividend">
Dividend

A dividend is a payment made by a corporation to its shareholders, usually as a distribution of profits. When a corporation earns a profit or surplus, it can re-invest it in the business (called retained earnings), and pay a fraction of the profit as a dividend to shareholders. Distribution to shareholders can be in cash (usually a deposit into a bank account) or, if the corporation has a dividend reinvestment plan, the amount can be paid by the issue of further shares or share repurchase.
A dividend is allocated as a fixed amount per share, with shareholders receiving a dividend in proportion to their shareholding. For the joint-stock company, paying dividends is not an expense; rather, it is the division of after tax profits among shareholders. Retained earnings (profits that have not been distributed as dividends) are shown in the shareholders' equity section on the company's balance sheet - the same as its issued share capital. Public companies usually pay dividends on a fixed schedule, but may declare a dividend at any time, sometimes called a special dividend to distinguish it from the fixed schedule dividends. Cooperatives, on the other hand, allocate dividends according to members' activity, so their dividends are often considered to be a pre-tax expense.
The word "dividend" comes from the Latin word ""dividendum"" ("thing to be divided").
Forms of payment.
Cash dividends are the most common form of payment and are paid out in currency, usually via electronic funds transfer or a printed paper check. Such dividends are a form of investment income and are usually taxable to the recipient in the year they are paid. This is the most common method of sharing corporate profits with the shareholders of the company. For each share owned, a declared amount of money is distributed. Thus, if a person owns 100 shares and the cash dividend is 50 cents per share, the holder of the stock will be paid $50. Dividends paid are not classified as an expense, but rather a deduction of retained earnings. Dividends paid does not show up on an income statement but does appear on the balance sheet.
Stock or scrip dividends are those paid out in the form of additional stock shares of the issuing corporation, or another corporation (such as its subsidiary corporation). They are usually issued in proportion to shares owned (for example, for every 100 shares of stock owned, a 5% stock dividend will yield 5 extra shares).
Nothing tangible will be gained if the stock is split because the total number of shares increases, lowering the price of each share, without changing the market capitalization, or total value, of the shares held. (See also Stock dilution.)
Stock dividend distributions are issues of new shares made to limited partners by a partnership in the form of additional shares. Nothing is split, these shares increase the market capitalization and total value of the company at the same time reducing the original cost basis per share.
Stock dividends are not includable in the gross income of the shareholder for US income tax purposes. Because the shares are issued for proceeds equal to the pre-existing market price of the shares; there is no negative dilution in the amount recoverable.
Property dividends or dividends in specie (Latin for "in kind") are those paid out in the form of assets from the issuing corporation or another corporation, such as a subsidiary corporation. They are relatively rare and most frequently are securities of other companies owned by the issuer, however they can take other forms, such as products and services.
Interim dividends are dividend payments made before a company's Annual General Meeting (AGM) and final financial statements. This declared dividend usually accompanies the company's interim financial statements.
Other dividends can be used in structured finance. Financial assets with a known market value can be distributed as dividends; warrants are sometimes distributed in this way. For large companies with subsidiaries, dividends can take the form of shares in a subsidiary company. A common technique for "spinning off" a company from its parent is to distribute shares in the new company to the old company's shareholders. The new shares can then be traded independently.
Reliability of dividends.
Two metrics are commonly used to examine a firm's dividend policy.
"Payout ratio" is calculated by dividing the company's dividend by the earnings per share. A payout ratio greater than 1 means the company is paying out more in dividends for the year than it earned.
"Dividend cover" is calculated by dividing the company's cash flow from operations by the dividend. This ratio is apparently popular with analysts of income trusts in Canada.
Dividends are payments made by a corporation to its shareholder members. It is the portion of corporate profits paid out to stockholders.
Dividend dates.
A dividend that is declared must be approved by a company's board of directors before it is paid. For public companies, four dates are relevant regarding dividends:
Declaration date — the day the board of directors announces its intention to pay a dividend. On that day, a liability is created and the company records that liability on its books; it now owes the money to the stockholders.
In-dividend date — the last day, which is one trading day before the "ex-dividend date", where the stock is said to be "cum dividend" ('with [including] dividend'). In other words, existing holders of the stock and anyone who buys it on this day will receive the dividend, whereas any holders selling the stock lose their right to the dividend. After this date the stock becomes "ex dividend".
Ex-dividend date — the day on which shares bought and sold no longer come attached with the right to be paid the most recently declared dividend. In the United States, it is typically 2 trading days before the "record date". This is an important date for any company that has many stockholders, including those that trade on exchanges, to enable reconciliation of who is entitled to be paid the dividend. Existing holders of the stock will receive the dividend even if they sell the stock on or after that date, whereas anyone who bought the stock will not receive the dividend. It is relatively common for a stock's price to decrease on the ex-dividend date by an amount roughly equal to the dividend paid. This reflects the decrease in the company's assets resulting from the declaration of the dividend.
Book closure date —when a company announces a dividend, it will also announce a date on which the company will ideally temporarily close its books for fresh transfers of stock, which is also usually the record date.
Record date — shareholders registered in the company's record as at the record date will be paid the dividend. Shareholders who are not registered as of this date will not receive the dividend. Registration in most countries is essentially automatic for shares purchased before the ex-dividend date.
Payment date — the day on which the dividend cheques will actually be mailed to shareholders or credited to their bank account.
Dividend-reinvestment.
Some companies have dividend reinvestment plans, or DRIPs, not to be confused with scrips. DRIPs allow shareholders to use dividends to systematically buy small amounts of stock, usually with no commission and sometimes at a slight discount. In some cases, the shareholder might not need to pay taxes on these re-invested dividends, but in most cases they do.
Dividend taxation.
Most countries impose a corporate tax on the profits made by a company. A dividend paid by a company is not an expense of the company, but is income of the shareholder. The tax treatment of this dividend income varies considerably between countries:
United States and Canada.
The United States and Canada impose a lower tax rate on dividend income than ordinary income, on the basis that company profits had already been taxed as corporate tax.
Australia and New Zealand.
Australia and New Zealand have a dividend imputation system, wherein companies can attach franking credits or imputation credits to dividends. These franking credits represent the tax paid by the company upon its pre-tax profits. One dollar of company tax paid generates one franking credit. Companies can attach any proportion of franking up to a maximum amount that is calculated from the prevailing company tax rate: for each dollar of dividend paid, the maximum level of franking is the company tax rate divided by (1 - company tax rate). At the current 30% rate, this works out at 0.30 of a credit per 70 cents of dividend, or 42.857 cents per dollar of dividend. The shareholders who are able to use them, apply these credits against their income tax bills at a rate of a dollar per credit, thereby effectively eliminating the double taxation of company profits.
United Kingdom.
The UK's taxation system is unique in its treatment of dividends: when a shareholder receives a dividend, a tax credit of 10% is deemed to already have been paid on that dividend. This ensures that double taxation does not take place, and has the effect that basic rate taxpayers pay no additional tax. Corporate taxpayers have their tax liability satisfied by the deemed tax credit, with no further tax liability. Non-taxpaying entities such as certain trusts, charities and pension funds are unaffected by the tax credit as this is only a deemed payment. The tax credit is unavailable for repayment.
India.
In India, companies declaring or distributing dividend, are required to pay a Corporate Dividend Tax in addition to the tax levied on their income. The dividend received by the shareholders is then exempt in their hands.
Effect on stock price.
After a stock goes ex-dividend (i.e. when a dividend has just been paid, so there is no anticipation of another imminent dividend payment), the stock price should drop.
To calculate the amount of the drop, the traditional method is to view the financial effects of the dividend from the perspective of the company. Since the company has paid say £x in dividends per share out of its cash account on the left hand side of the balance sheet, the equity account on the right side should decrease an equivalent amount. This means that a £x dividend should result in a £x drop in the share price.
A more accurate method of calculating this price is to look at the share price and dividend from the after-tax perspective of a share holder. The after-tax drop in the share price (or capital gain/loss) should be equivalent to the after-tax dividend. For example, if the tax of capital gains Tcg is 35%, and the tax on dividends Td is 15%, then a £1 dividend is equivalent to £0.85 of after tax money. To get the same financial benefit from a capital loss, the after tax capital loss value should equal £0.85. The pre-tax capital loss would be £0.85/(1-Tcg) = £0.85/(1-35%) = £0.85/65% = £1.30. In this case, a dividend of £1 has led to a larger drop in the share price of £1.30, because the tax rate on capital losses is higher than the dividend tax rate.
Finally, security analysis that does not take dividends into account may mute the decline in share price, for example in the case of a Price–earnings ratio target that does not back out cash; or amplify the decline, for example in the case of Trend following.
Criticism.
Some believe that company profits are best re-invested back into the company: research and development, capital investment, expansion, etc. Proponents of this view (and thus critics of dividends per se) suggest that an eagerness to return profits to shareholders may indicate the management having run out of good ideas for the future of the company. Some studies, however, have demonstrated that companies that pay dividends have higher earnings growth, suggesting that dividend payments may be evidence of confidence in earnings growth and sufficient profitability to fund future expansion.
Taxation of dividends is often used as justification for retaining earnings, or for performing a stock buyback, in which the company buys back stock, thereby increasing the value of the stock left outstanding.
When dividends are paid, individual shareholders in many countries suffer from double taxation of those dividends:
In many countries, the tax rate on dividend income is lower than for other forms of income to compensate for tax paid at the corporate level.
Capital gains should not be confused with dividends. Capital gains assume an increase in a stock's value. Dividend is merely parsing out a share of the profits, and is taxed at the dividend tax rate. If there is an increase of value of stock, and a shareholder chooses to sell the stock, the shareholder will pay a tax on capital gains (often taxed at a lower rate than ordinary income). If a holder of the stock chooses to not participate in the buyback, the price of the holder's shares could rise (as well as it could fall), but the tax on these gains is delayed until the actual sale of the shares.
Certain types of specialized investment companies (such as a REIT in the U.S.) allow the shareholder to partially or fully avoid double taxation of dividends.
Shareholders in companies that pay little or no cash dividends can reap the benefit of the company's profits when they sell their shareholding, or when a company is wound down and all assets liquidated and distributed amongst shareholders. This, in effect, delegates the dividend policy from the board to the individual shareholder. Payment of a dividend can increase the borrowing requirement, or leverage, of a company. 
Other corporate entities.
Cooperatives.
Cooperative businesses may retain their earnings, or distribute part or all of them as dividends to their members. They distribute their dividends in proportion to their members' activity, instead of the value of members' shareholding. Therefore, co-op dividends are often treated as pre-tax expenses. In other words, local tax or accounting rules may treat a dividend as a form of customer rebate or a staff bonus to be deducted from turnover before profit (tax profit or operating profit) is calculated.
Consumers' cooperatives allocate dividends according to their members' trade with the co-op. For example, a credit union will pay a dividend to represent interest on a saver's deposit. A retail co-op store chain may return a percentage of a member's purchases from the co-op, in the form of cash, store credit, or equity. This type of dividend is sometimes known as a patronage dividend or patronage refund, as well as being informally named "divi" or "divvy".
Producer cooperatives, such as worker cooperatives, allocate dividends according to their members' contribution, such as the hours they worked or their salary.
Trusts.
In real estate investment trusts and royalty trusts, the distributions paid often will be consistently greater than the company earnings. This can be sustainable because the accounting earnings do not recognize any increasing value of real estate holdings and resource reserves. If there is no economic increase in the value of the company's assets then the excess distribution (or dividend) will be a return of capital and the book value of the company will have shrunk by an equal amount. This may result in capital gains which may be taxed differently from dividends representing distribution of earnings.
Mutuals.
The distribution of profits by other forms of mutual organization also varies from that of joint-stock companies, though may not take the form of a dividend.
In the case of mutual insurance, for example, in the United States, a distribution of profits to holders of participating life policies is called a "dividend".
These profits are generated by the investment returns of the insurer's general account, in which premiums are invested and from which claims are paid.
The participating dividend may be used to decrease premiums, or to increase the cash value of the policy.
Some life policies pay nonparticipating dividends.
As a contrasting example, in the United Kingdom, the surrender value of a with-profits policy is increased by a "bonus", which also serves the purpose of distributing profits.
Life insurance dividends and bonuses, while typical of mutual insurance, are also paid by some joint stock insurers.
Insurance dividend payments are not restricted to life policies. For example, general insurer State Farm Mutual Automobile Insurance Company can distribute dividends to its vehicle insurance policyholders.

</doc>
<doc id="41932" url="https://en.wikipedia.org/wiki?curid=41932" title="Accuracy and precision">
Accuracy and precision

Precision is a description of "random errors", a measure of statistical variability.
Accuracy has two definitions:
Common definition.
In the fields of science, engineering and statistics, the accuracy of a measurement system is the degree of closeness of measurements of a quantity to that quantity's true value. The precision of a measurement system, related to reproducibility and repeatability, is the degree to which repeated measurements under unchanged conditions show the same results. Although the two words precision and accuracy can be synonymous in colloquial use, they are deliberately contrasted in the context of the scientific method.
A measurement system can be accurate but not precise, precise but not accurate, neither, or both. For example, if an experiment contains a systematic error, then increasing the sample size generally increases precision but does not improve accuracy. The result would be a consistent yet inaccurate string of results from the flawed experiment. Eliminating the systematic error improves accuracy but does not change precision.
A measurement system is considered "valid" if it is both "accurate" and "precise". Related terms include "bias" (non-random or directed effects caused by a factor or factors unrelated to the independent variable) and "error" (random variability).
The terminology is also applied to indirect measurements—that is, values obtained by a computational procedure from observed data.
In addition to accuracy and precision, measurements may also have a measurement resolution, which is the smallest change in the underlying physical quantity that produces a response in the measurement.
In numerical analysis, accuracy is also the nearness of a calculation to the true value; while precision is the resolution of the representation, typically defined by the number of decimal or binary digits.
Statistical literature prefers to use the terms "bias" and "variability" instead of accuracy and precision: bias is the amount of inaccuracy and variability is the amount of imprecision.
Quantification.
In industrial instrumentation, accuracy is the measurement tolerance, or transmission of the instrument and defines the limits of the errors made when the instrument is used in normal operating conditions.
Ideally a measurement device is both accurate and precise, with measurements all close to and tightly clustered around the true value. The accuracy and precision of a measurement process is usually established by repeatedly measuring some traceable reference standard. Such standards are defined in the International System of Units (abbreviated SI from French: Système international d'unités) and maintained by national standards organizations such as the National Institute of Standards and Technology in the United States.
This also applies when measurements are repeated and averaged. In that case, the term standard error is properly applied: the precision of the average is equal to the known standard deviation of the process divided by the square root of the number of measurements averaged. Further, the central limit theorem shows that the probability distribution of the averaged measurements will be closer to a normal distribution than that of individual measurements.
With regard to accuracy we can distinguish:
A common convention in science and engineering is to express accuracy and/or precision implicitly by means of significant figures. Here, when not explicitly stated, the margin of error is understood to be one-half the value of the last significant place. For instance, a recording of 843.6 m, or 843.0 m, or 800.0 m would imply a margin of 0.05 m (the last significant place is the tenths place), while a recording of 8,436 m would imply a margin of error of 0.5 m (the last significant digits are the units).
A reading of 8,000 m, with trailing zeroes and no decimal point, is ambiguous; the trailing zeroes may or may not be intended as significant figures. To avoid this ambiguity, the number could be represented in scientific notation: 8.0 × 103 m indicates that the first zero is significant (hence a margin of 50 m) while 8.000 × 103 m indicates that all three zeroes are significant, giving a margin of 0.5 m. Similarly, it is possible to use a multiple of the basic measurement unit: 8.0 km is equivalent to 8.0 × 103 m. In fact, it indicates a margin of 0.05 km (50 m). However, reliance on this convention can lead to false precision errors when accepting data from sources that do not obey it.
Precision is sometimes stratified into:
ISO Definition (ISO 5725).
A shift in the meaning of these terms appeared with the publication of the ISO 5725 series of standards in 1994, which is also reflected in the 2008 issue of the "BIPM International Vocabulary of Metrology" (VIM), items 2.13 and 2.14.
According to ISO 5725-1, the general term "accuracy" is used to describe the closeness of a measurement to the true value. When the term is applied to sets of measurements of the same "measurand", it involves a component of random error and a component of systematic error. In this case trueness is the closeness of the mean of a set of measurement results to the actual (true) value and precision is the closeness of agreement among a set of results.
ISO 5725-1 and VIM also avoid the use of the term "bias", previously specified in BS 5497-1, because it has different connotations outside the fields of science and engineering, as in medicine and law.
In binary classification.
"Accuracy" is also used as a statistical measure of how well a binary classification test correctly identifies or excludes a condition.
That is, the accuracy is the proportion of true results (both true positives and true negatives) among the total number of cases examined. To make the context clear by the semantics, it is often referred to as the "Rand accuracy" or "Rand index". It is a parameter of the test.
An accuracy of 100% means that the measured values are exactly the same as the given values.
On the other hand, precision or positive predictive value is defined as the proportion of the true positives against all the positive results (both true positives and false positives)
Also see Sensitivity and specificity.
Accuracy may be determined from sensitivity and specificity, provided prevalence is known, using the equation:
The accuracy paradox for predictive analytics states that predictive models with a given level of accuracy may have greater predictive power than models with higher accuracy. It may be better to avoid the accuracy metric in favor of other metrics such as precision and recall. In situations where the minority class is more important, F-measure may be more appropriate, especially in situations with very skewed class imbalance.
Another useful performance measure is the "balanced accuracy" which avoids inflated performance estimates on imbalanced datasets. It is defined as the arithmetic mean of sensitivity and specificity, or the average accuracy obtained on either class:
If the classifier performs equally well on either class, this term reduces to the conventional accuracy (i.e., the number of correct predictions divided by the total number of predictions). In contrast, if the conventional accuracy is above chance "only" because the classifier takes advantage of an imbalanced test set, then the balanced accuracy, as appropriate, will drop to chance. A closely related chance corrected measure is:
A direct approach to debiasing and renormalizing Accuracy is Cohen's kappa, whilst Informedness has been shown to be a Kappa-family debiased renormalization of Recall. Informedness and Kappa have the advantage that chance level is defined to be 0, and they have the form of a probability. Informedness has the stronger property that it is the probability that an informed decision is made (rather than a guess), when positive. When negative this is still true for the absolutely value of Informedness, but the information has been used to force an incorrect response.
In psychometrics and psychophysics.
In psychometrics and psychophysics, the term "accuracy" is interchangeably used with validity and "constant error". "Precision" is a synonym for reliability and "variable error". The validity of a measurement instrument or psychological test is established through experiment or correlation with behavior. Reliability is established with a variety of statistical techniques, classically through an internal consistency test like Cronbach's alpha to ensure sets of related questions have related responses, and then comparison of those related question between reference and target population.
In logic simulation.
In logic simulation, a common mistake in evaluation of accurate models is to compare a logic simulation model to a transistor circuit simulation model. This is a comparison of differences in precision, not accuracy. Precision is measured with respect to detail and accuracy is measured with respect to reality.
In information systems.
The concepts of accuracy and precision have also been studied in the context of databases, information systems and their sociotechnical context. The necessary extension of these two concepts on the basis of theory of science suggests that they (as well as data quality and information quality) should be centered on accuracy defined as the closeness to the true value seen as the degree of agreement of readings or of calculated values of one same conceived entity, measured or calculated by different methods, in the context of maximum possible disagreement.

</doc>
<doc id="41933" url="https://en.wikipedia.org/wiki?curid=41933" title="Cold-blooded">
Cold-blooded

Cold-blooded is an informal term for one or more of a group of characteristics that determine an animal's thermophysiology. These include:
"Cold blooded" and "Coldblooded" may also refer to:

</doc>
<doc id="41935" url="https://en.wikipedia.org/wiki?curid=41935" title="Oldenburg (Oldenburg)">
Oldenburg (Oldenburg)

Oldenburg (Oldb) or simply Oldenburg (; Low German: "Ollnborg"; Saterland Frisian: "Ooldenbuurich") is an independent city in the state of Lower Saxony, Germany. During the French annexation (1811–1813) in the wake of the Napoleonic war against Britain, it was also known as "Le Vieux-Bourg" in French. The city is situated at the Rivers Hunte and Haaren, in the northwestern region between the cities of Bremen in the east and Groningen (Netherlands) in the west. It has a population of 160,907 (December 2014).
The city is the place of origin of the House of Oldenburg. Before the end of the German Empire (1918), it was the administrative centre and residence of the monarchs of Oldenburg.
History.
Archaeological finds point to a settlement dating back to the 8th century. The place was first mentioned in 1108 as "Aldenburg" in connection with Elimar I (also known as Egilmar I) who is now commonly seen as the first count of Oldenburg. The town gained importance due to its location at a ford of the navigable Hunte river. Oldenburg became the capital of the County of Oldenburg (later Duchy, Grand Duchy, and Free State), a small state in the shadow of the much more powerful Hanseatic city of Bremen.
In the 17th century, Oldenburg was a wealthy town in a time of war and turmoil and its population and power grew considerably. In 1667, the town was struck by a disastrous plague epidemic and, shortly after, a fire destroyed Oldenburg. The Danish kings, who were also counts of Oldenburg at the time, were not much interested in the condition of the town and it lost most of its former importance. In 1773, Danish rule ended. It was only then that the destroyed buildings in the city were rebuilt in a neoclassicist style. (In German, the ‘neoclassicist style’ of that period would usually be called "klassizistisch", while "neoklassizistisch" specifically refers to the classicist style of the early 20th century.)
After German Emperor Wilhelm II was forced to abdicate following the exhaustion and defeat of the German Empire in World War I, monarchic rule ended in Oldenburg as well with the abdication of Grand Duke Frederick Augustus II of Oldenburg "(Friedrich August II von Oldenburg)" on 11 November 1918. The Grand Duchy of Oldenburg now became the Free State of Oldenburg "(Freistaat Oldenburg)", the city remained the capital.
In the 1928 city elections, the Nazi Party received 9.8% of the vote, enough for a seat on the Oldenburg city council. In the September 1930 Oldenburg state elections, the Nazi Party's share of the vote rose to 27.3%, and on May 29, 1932, the Nazi Party received 48.4% of the state election, enough to put the Nazi party in charge of forming a state government and, significantly, making Oldenburg the first state in the country to put the Nazis in power based on electoral turnout. By that fall, a campaign of Aryanization began, forcing the sale of formerly Jewish-owed properties at steep discounts.
In 1945, after World War II, the State of Oldenburg was part of the British zone of occupation. The British military government of the Oldenburg region resided in the city. Several displaced persons camps were set up in the city that had suffered only 1.4% destruction during the bombing campaigns of World War II. About 42,000 refugees migrated into Oldenburg, which raised the number of residents to over 100,000. In 1946, the Free State of Oldenburg was dissolved, and the area became the 'Administrative District' of Oldenburg "(Verwaltungsbezirk Oldenburg)" as part of the newly formed federal German state of Lower Saxony "(Niedersachsen)". The city was now capital of the district. In 1978, the district was dissolved and succeeded by the newly formed Weser-Ems administrative region "(Regierungsbezirk Weser-Ems)", again with the city as administrative capital. The State of Lower Saxony dissolved all regierungsbezirks by the end of 2004 in the course of administrative reforms.
City government.
Local elections take place every five years. The city council "(Stadtrat)" has 50 seats. The lord mayor is elected directly by the citizens.
Economy and infrastructure.
Agriculture.
The city is surrounded by large agricultural areas, about 80% of which is grassland. There are farms near and even a few within city limits. Predominant agricultural activities of the region are the cultivation of livestock, especially dairy cows and other grazing animals, crops such as grains for food and animal feed, as well as asparagus, corn, and kale.
Cultural life.
Jewish community.
The history of the Jewish community of Oldenburg dates back to the 14th century. Towards and during the 19th century, the Jews in Oldenburg were always around 1% of the total population, and by that time had acquired their own synagogue, cemetery and school. Most of them were merchants and businessmen. On 1938 Kristallnacht, the town men were led to Sachsenhausen concentration camp, among them Leo Trepp, the community Rabbi who survived and later became an honorary citizen of Oldenburg and honored by a street named after him. Since 1981 an annual commemoration walk (Erinnerungsgang) has been held by Oldenburg citizens in memory of the deportation of the Oldenburg Jews on November 10, 1938. Those who remained after 1938 immigrated to Canada, USA, United Kingdom, Holland or Palestine. 
After WWII, a group of survivors returned to the city and maintained a small community until it was dissolved during the seventies. Nevertheless, due to Jewish emigration from the former USSR to Germany in the nineties, a community of about 340 people is now maintaining its own synagogue, cemetery and other facilities. The old Jewish cemetery, which is no longer active after the opening of a new one, was desecrated twice in 2011 and 2013.
Education.
Tertiary education.
There are two public universities in Oldenburg:
Privately managed institutions of higher education:
Other:
Events.
Oldenburg hosted the 2007 Fistball World Championship.
International relations.
Oldenburg is twin towns with following cities and districts:

</doc>
<doc id="41938" url="https://en.wikipedia.org/wiki?curid=41938" title="Bloomsbury Group">
Bloomsbury Group

The Bloomsbury Group—or Bloomsbury Set—was an influential group of associated English writers, intellectuals, philosophers and artists, the best known members of which included Virginia Woolf, John Maynard Keynes, E. M. Forster and Lytton Strachey. This loose collective of friends and relatives lived, worked or studied together near Bloomsbury, London, during the first half of the 20th century. According to Ian Ousby, "although its members denied being a group in any formal sense, they were united by an abiding belief in the importance of the arts". Their works and outlook deeply influenced literature, aesthetics, criticism, and economics as well as modern attitudes towards feminism, pacifism, and sexuality.
Origins.
The male members of the Bloomsbury Group, except Duncan Grant, were educated at Cambridge at either Trinity or King’s College. Most of them, except Clive Bell and the Stephen brothers, were members of "the exclusive Cambridge society, the 'Apostles'". At Trinity in 1899 Lytton Strachey, Leonard Woolf, Saxon Sydney-Turner and Clive Bell became good friends with Thoby Stephen, and it was through Thoby and Adrian Stephen's sisters Vanessa and Virginia that the men met the women of Bloomsbury when they came down to London.
In 1905 Vanessa began the "Friday Club" and Thoby ran "Thursday Evenings", which became the basis for the Bloomsbury Group, which to some was really "Cambridge in London". Thoby's premature death in 1906 brought them more firmly together and they became what is now known as the "Old Bloomsbury" group who met in earnest beginning in 1912. In the 1920s and 1930s the group shifted when the original members died and the next generation had reached adulthood.
The Bloomsbury Group, mostly from upper middle-class professional families, formed part of "an intellectual aristocracy which could trace itself back to the Clapham Sect". It was an informal network of an influential group of artists, art critics, writers and an economist — many of whom lived in the West Central 1 district of London known as Bloomsbury. They were "spiritually" similar to the Clapham group who supported its members' careers: "The Bloomsberries promoted one another's work and careers just as the original Claphamites did, as well as the intervening generations of their grandparents and parents."
A historical feature of these friends and relations is that their close relationships all pre-dated their fame as writers, artists, and thinkers.
Membership.
Members.
The group had ten core members:
In addition to these ten, Leonard Woolf, in the 1960s, listed as 'Old Bloomsbury' Adrian and Karin Stephen, Saxon Sydney-Turner, and Molly MacCarthy, with Julian Bell, Quentin Bell and Angelica Bell, and David Garnett as later additions". Except for Forster, who published three novels before the highly successful "Howards End" in 1910, the group were late developers.
There were stable marriages and varied and complicated affairs among the individual members. Lytton Strachey and his cousin and lover Duncan Grant became close friends of the Stephen sisters, Vanessa Bell and Virginia Woolf. Duncan Grant had affairs with siblings Vanessa Bell and Adrian Stephen, as well as David Garnett, Maynard Keynes, and James Strachey. Clive Bell married Vanessa in 1907, and Leonard Woolf returned from the Ceylon Civil Service to marry Virginia in 1912. Cambridge Apostle friendships brought into the group Desmond MacCarthy, his wife Molly, and E. M. Forster.
The group met not only in their homes in Bloomsbury, central London, but also at countryside retreats. There are two significant ones near Lewes in Sussex: Charleston Farmhouse, where Vanessa Bell and Duncan Grant moved in 1916, and Monk's House (now owned by the National Trust), in Rodmell, owned by Virginia and Leonard Woolf from 1919.
The Ascension Parish Burial Ground in Cambridge is the final resting place of two of the Bloomsbury Group, Sir Desmond and Lady Molly McCarthy. It also houses nine other Apostles, including the philosopher G.E. Moore, who was a great influence on the Bloomsberries.
Others.
Much about Bloomsbury appears to be controversial, including its membership and name: indeed, some would maintain that "the three words 'the Bloomsbury group' have been so much used as to have become almost unusable".
Close friends, brothers, sisters, and even sometimes partners of the friends were not necessarily members of Bloomsbury: Keynes’s wife Lydia Lopokova was only reluctantly accepted into the group, and there were certainly "writers who were at some time close friends of Virginia Woolf, but who were distinctly not 'Bloomsbury': T. S. Eliot, Katherine Mansfield, Hugh Walpole". Another is Vita Sackville-West, who became "Hogarth Press's best-selling author". Members cited in "other lists might include Ottoline Morrell, or Dora Carrington, or James and Alix Strachey".
Shared ideas.
The lives and works of the group members show an overlapping, interconnected similarity of ideas and attitudes that helped to keep the friends and relatives together, reflecting in large part the influence of G. E. Moore: "the essence of what Bloomsbury drew from Moore is contained in his statement that 'one's prime objects in life were love, the creation and enjoyment of aesthetic experience and the pursuit of knowledge'".
Philosophy and ethics.
Through the Apostles they also encountered the analytic philosophers G. E. Moore and Bertrand Russell who were revolutionizing British philosophy at the start of the 20th century. Distinguishing between ends and means was a commonplace of ethics, but what made Moore's "Principia Ethica" (1903) so important for the philosophical basis of Bloomsbury thought was Moore's conception of "intrinsic worth" as distinct from "instrumental value". As with the distinction between love (an intrinsic state) and monogamy (a behavior), Moore's differentiation between intrinsic and instrumental value allowed the Bloomsburies to maintain an ethical high-ground based on intrinsic merit, independent of, and without reference to, the consequences of their actions. For Moore, intrinsic value depended on an indeterminable intuition of good and a concept of complex states of mind whose worth as a whole was not proportionate to the sum of its parts. For both Moore and Bloomsbury, the greatest ethic goods were "the importance of personal relationships and the private life", as well as aesthetic appreciation: "art for art's sake".
Rejection of bourgeois habits.
Bloomsbury reacted against current social rituals, "the bourgeois habits ... the conventions of Victorian life" with their emphasis on public achievement, in favour of a more informal and private focus on personal relationships and individual pleasure. E. M. Forster for example approved of "the decay of smartness and fashion as factors, and the growth of the idea of enjoyment", and asserted that "if I had to choose between betraying my country and betraying my friend, I hope I should have the guts to betray my country".
The Group "believed in pleasure ... They tried to get the maximum of pleasure out of their personal relations. If this meant triangles or more complicated geometric figures, well then, one accepted that too". Yet at the same time, they shared a sophisticated, civilized, and highly articulated ideal of pleasure. As Virginia Woolf put it, their "triumph is in having worked out a view of life which was not by any means corrupt or sinister or merely intellectual; rather ascetic and austere indeed; which still holds, and keeps them dining together, and staying together, after 20 years".
Politics.
Politically, Bloomsbury held mainly left-liberal stances (opposed to militarism, for example); but its "clubs and meetings were not activist, like the political organisations to which many of Bloomsbury's members also belonged", and they would be criticised for that by their 1930s successors, who by contrast were "heavily touched by the politics which Bloomsbury had rejected".
The campaign for women’s suffrage added to the controversial nature of Bloomsbury, as Virginia Woolf represented the group in the fictional "The Years" and "Night and Day" works about the suffrage movement.
Art.
Roger Fry joined the group in 1910. His post-impressionist exhibitions of 1910 and 1912 involved Bloomsbury in a second revolution following on the Cambridge philosophical one. This time the Bloomsbury painters were much involved and influenced. Fry and other Bloomsbury artists rejected the traditional distinction between fine and decorative art.
These "Bloomsbury assumptions" are reflected in members' criticisms of materialistic realism in painting and fiction, influenced above all by Clive Bell's "concept of 'Significant Form', which separated and elevated the concept of form above content in works of art": it has been suggested that, with their "focus on form ...Bell's ideas have come to stand in for, perhaps too much so, the aesthetic principles of the Bloomsbury Group".
The establishment's hostility to post-impressionism made Bloomsbury controversial, and controversial they have remained. Clive Bell polemicized post-impressionism in his widely read book "Art" (1914), basing his aesthetics partly on Roger Fry’s art criticism and G. E. Moore's moral philosophy; and as the war came he argued that "in these days of storm and darkness, it seemed right that at the shrine of civilization - in Bloomsbury, I mean - the lamp should be tended assiduously".
World War I.
Old Bloomsbury’s development was inevitably impacted on, along with just about everything else in modernist culture, by the First World War: indeed, "the small world of Bloomsbury was later said by some on its outskirts to have been irretrivably shattered", though in fact its friendships "survived the upheavals and dislocations of war, in many ways were even strengthened by them". Most but not all of them were conscientious objectors, which of course added to the group’s controversies. Politically the members of Bloomsbury had liberalism and socialism leanings.
Though the war dispersed Old Bloomsbury, the individuals continued to develop their careers. E. M. Forster followed his successful novels with "Maurice" which he could not publish because it treated homosexuality untragically. In 1915 Virginia Woolf brought out her first novel, "The Voyage Out". And in 1917 the Woolfs founded their Hogarth Press, which would publish T. S. Eliot, Katherine Mansfield, and many others including Virginia herself along with the standard English translations of Freud. Then in 1918 Lytton Strachey published his critique of Victorianism in the shape of four ironic biographies in "Eminent Victorians," which added to the arguments about Bloomsbury that continue to this day, and "brought him the triumph he had always longed for ... The book was a sensation".
The following year came J. M. Keynes’s influential attack on the Versailles Peace Treaty: ""The Economic Consequences of the Peace"" immediately established Maynard as an economist of international eminence".
Later Bloomsbury.
The 1920s were in a number of ways the blooming of Bloomsbury. Virginia Woolf was writing and publishing her most widely read modernist novels and essays, E. M. Forster completed "A Passage to India", which remains the most highly regarded novel on British imperialism in India . Forster wrote no more novels but he became one of England’s most influential essayists. Duncan Grant, and then Vanessa Bell, had single-artist exhibitions. Lytton Strachey wrote his biographies of two queens, Victoria then Elizabeth (and Essex). Desmond MacCarthy and Leonard Woolf engaged in friendly rivalry as literary editors, respectively of the "New Statesman" and "The Nation and Athenaeum", thus fuelling animosities that saw Bloomsbury dominating the cultural scene. Roger Fry wrote and lectured widely on art; meanwhile, Clive Bell applied Bloomsbury values to his book "Civilization" (1928), which Leonard Woolf saw as limited and elitist, describing Clive as a "wonderful organiser of intellectual greyhound racing tracks".
In the darkening 1930s, Bloomsbury began to die: "Bloomsbury itself was hardly any longer a focus". A year after publishing a collection of brief lives, "Portraits in Miniature" (1931), Lytton Strachey died; shortly afterwards Carrington shot herself. Roger Fry, who had become England’s greatest art critic, died in 1934. Vanessa and Clive's eldest son, Julian Bell, was killed in 1937 during the Spanish Civil War. Virginia Woolf wrote , but with the coming of war again her mental instability recurred, and she drowned herself in 1941. In the previous decade she had become one of the century's most famous feminist writers with three more novels, and a series of essays including the moving late memoir "A Sketch of the Past". It was also in the 1930s that Desmond MacCarthy became perhaps the most widely read—and heard—literary critic with his columns in "The Sunday Times" and his broadcasts with the BBC. John Maynard Keynes's "The General Theory of Employment, Interest, and Money" (1936) made him one of the century's most influential economists. He died in 1946 after being much involved in monetary negotiations with the United States.
The diversity yet collectivity of Later Bloomsbury's ideas and achievements can be summed up in a series of credos that were done in 1938, the year of the Munich Agreement. Virginia Woolf published her radical feminist polemic "Three Guineas" that shocked some of her fellow members, including Keynes who had enjoyed the gentler "A Room of One's Own" (1929). Keynes read his famous but decidedly more conservative memoir "My Early Beliefs" to The Memoir Club. Clive Bell published an appeasement pamphlet (he later supported the war), and E. M. Forster wrote an early version of his famous essay "What I Believe" with its choice, still shocking for some, of personal relations over patriotism: his quiet assertion in the face of the increasingly totalitarian claims of both left and right that "personal relations ... love and loyalty to an individual can run counter to the claims of the State".
Memoir Club.
In March 1920 Molly MacCarthy began the Memoir Club to help Desmond and herself write their memoirs; and also "for their friends to regroup after the war (with the proviso that they should always tell the truth)". It met until 1956 or 1964.
Criticism.
If "the contempt or suspicion—the environment that a person or group creates around itself—is always a kind of alter ego, an essential and revealing part of the production", there is perhaps much to be learnt from the (extensive) criticism that the Bloomsbury Group aroused. Early complaints focused on a perceived cliquiness: "on personal mannerisms—the favourite phrases ('ex-quisitely civilized', and 'How "simply too" extraordinary!'), the incredulous, weirdly emphasised Strachey voice". After World War I, as the members of the Group "began to be famous, the execration increased, and the caricature of an idle, snobbish and self-congratulatory rentier class, promoting its own brand of high culture began to take shape": as Forster self-mockingly put it, "In came the nice fat dividends, up rose the lofty thoughts".
The growing threats of the 1930s brought new criticism from younger writers of "what the last lot had done (Bloomsbury, Modernism, Eliot) in favour of what they thought of as urgent hard-hitting realism"; while "Wyndham Lewis's "The Apes of God", which called Bloomsbury élitist, corrupt and talentless, caused a stir" of its own. The most telling criticism, however, came perhaps from within the Group's own ranks, when on the eve of war Keynes gave a "nostalgic and disillusioned account of the pure sweet air of G. E. Moore, that belief in undisturbed individualism, that Utopianism based on a belief in human reasonableness and decency, that refusal to accept the idea of civilisation as 'a thin and precarious crust' ... Keynes's fond, elegiac repudiation of his "early beliefs", in the light of current affairs ("We completely misunderstood human nature, including our own")".
More recent criticism comes from American philosopher Martha Nussbaum, quoted in 1999 as saying "I don't like anything that sets itself up as an in-group or an elite, whether it is the Bloomsbury group or Derrida".

</doc>
