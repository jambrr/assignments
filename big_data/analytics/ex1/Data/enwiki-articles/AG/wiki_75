<doc id="45524" url="https://en.wikipedia.org/wiki?curid=45524" title="Pseudorandom number generator">
Pseudorandom number generator

A pseudorandom number generator (PRNG), also known as a deterministic random bit generator (DRBG), is an algorithm for generating a sequence of numbers whose properties approximate the properties of sequences of random numbers. The PRNG-generated sequence is not truly random, because it is completely determined by a relatively small set of initial values, called the PRNG's "seed" (which may include truly random values). Although sequences that are closer to truly random can be generated using hardware random number generators, "pseudorandom" number generators are important in practice for their speed in number generation and their reproducibility.
PRNGs are central in applications such as simulations (e.g. for the Monte Carlo method), electronic games (e.g. for procedural generation), and cryptography. Cryptographic applications require the output not to be predictable from earlier outputs, and more elaborate algorithms, which do not inherit the linearity of simpler PRNGs, are needed.
Good statistical properties are a central requirement for the output of a PRNG. In general, careful mathematical analysis is required to have any confidence that a PRNG generates numbers that are sufficiently close to random to suit the intended use. John von Neumann cautioned about the misinterpretation of a PRNG as a truly random generator, and joked that "Anyone who considers arithmetical methods of producing random digits is, of course, in a state of sin."
Periodicity.
A PRNG can be started from an arbitrary initial state using a seed state. It will always produce the same sequence when initialized with that state. The "period" of a PRNG is defined thus: the maximum, over all starting states, of the length of the repetition-free prefix of the sequence. The period is bounded by the number of the states, usually measured in bits. However, since the length of the period potentially doubles with each bit of "state" added, it is easy to build PRNGs with periods long enough for many practical applications.
If a PRNG's internal state contains "n" bits, its period can be no longer than 2n results, and may be much shorter. For some PRNGs, the period length can be calculated without walking through the whole period. Linear Feedback Shift Registers (LFSRs) are usually chosen to have periods of exactly 2n−1. Linear congruential generators have periods that can be calculated by factoring. Although PRNGs will repeat their results after they reach the end of their period, a repeated result does not imply that the end of the period has been reached, since its internal state may be larger than its output; this is particularly obvious with PRNGs with a one-bit output.
Most PRNG algorithms produce sequences which are uniformly distributed by any of several tests. It is an open question, and one central to the theory and practice of cryptography, whether there is any way to distinguish the output of a high-quality PRNG from a truly random sequence without knowing the algorithm(s) used and the state with which it was initialized. The security of most cryptographic algorithms and protocols using PRNGs is based on the assumption that it is infeasible to distinguish use of a suitable PRNG from use of a truly random sequence. The simplest examples of this dependency are stream ciphers, which (most often) work by exclusive or-ing the plaintext of a message with the output of a PRNG, producing ciphertext. The design of cryptographically adequate PRNGs is extremely difficult, because they must meet additional criteria (see below). The size of its period is an important factor in the cryptographic suitability of a PRNG, but not the only one.
Potential problems with deterministic generators.
In practice, the output from many common PRNGs exhibit artifacts that cause them to fail statistical pattern-detection tests. These include:
Defects exhibited by flawed PRNGs range from unnoticeable (and unknown) to very obvious. An example was the RANDU random number algorithm used for decades on mainframe computers. It was seriously flawed, but its inadequacy went undetected for a very long time.
In many fields, much research work prior to the 21st century that relied on random selection or on Monte Carlo simulations, or in other ways relied on PRNGs, is much less reliable than it might have been as a result of using poor-quality PRNGs. Even today, caution is sometimes required, as illustrated by the following warning, which is given in the "International Encyclopedia of Statistical Science" (2010).
As an illustration, consider the widely used programming language Java. As of 2015, Java still relies on a linear congruential generator (LCG) for its (non-cryptographically-secure) PRNG; yet LCGs are of low quality—see further below.
The first PRNG to avoid major problems and still run fairly quickly was the Mersenne Twister (discussed below), which was published in 1998. Other high-quality PRNGs have since been developed.
Generators based on linear recurrences.
In the second half of the 20th century, the standard class of algorithms used for PRNGs comprised linear congruential generators. The quality of LCGs was known to be inadequate, but better methods were unavailable. Press et al. (2007) described the result thus: "If all scientific papers whose results are in doubt because of and related were to disappear from library shelves, there would be a gap on each shelf about as big as your fist".
A major advance in the construction of pseudorandom generators was the introduction of techniques based on linear recurrences on the two-element field; such generators are related to linear feedback shift registers.
The 1997 invention of the Mersenne Twister, in particular, avoided many of the problems with earlier generators. The Mersenne Twister has a period of 219937−1 iterations (≈4.3), is proven to be equidistributed in (up to) 623 dimensions (for 32-bit values), and at the time of its introduction was running faster than other statistically reasonable generators.
Subsequently, the WELL family of generators was developed. The WELL generators in some ways improves on the quality of the Mersenne Twister—which has a too-large state space and a very slow recovery from state spaces with a large number of zeros.
In 2003, George Marsaglia introduced the family of xorshift generators, again based on a linear recurrence. Such generators are extremely fast and, combined with a nonlinear operation, they pass strong statistical tests.
Cryptographically secure pseudorandom number generators.
A PRNG suitable for cryptographic applications is called a "cryptographically secure PRNG" (CSPRNG). A requirement for a CSPRNG is that an adversary not knowing the seed has only negligible advantage in distinguishing the generator's output sequence from a random sequence. In other words, while a PRNG is only required to pass certain statistical tests, a CSPRNG must pass all statistical tests that are restricted to polynomial time in the size of the seed. Though such property cannot be proven, strong evidence may be provided by reducing the CSPRNG to a problem that is assumed to be hard, such as integer factorization. In general, years of review may be required before an algorithm can be certified as a CSPRNG.
Some classes of CSPRNGs include the following:
It has been shown to be likely that the NSA has inserted an asymmetric backdoor into the NIST certified pseudorandom number generator Dual_EC_DRBG.
BSI evaluation criteria.
The German Federal Office for Information Security ("Bundesamt für Sicherheit in der Informationstechnik", BSI) has established four criteria for quality of deterministic random number generators. They are summarized here:
For cryptographic applications, only generators meeting the K3 or K4 standard are acceptable.
Mathematical definition.
Given
We call a function formula_19 (where formula_20 is the set of positive integers) a pseudo-random number generator for formula_1 given formula_4 taking values in formula_11 iff
It can be shown that if formula_28 is a pseudo-random number generator for the uniform distribution on formula_29 and if formula_30 is the CDF of some given probability distribution formula_1, then formula_32 is a pseudo-random number generator for formula_1, where formula_34 is the percentile of formula_1, i.e. formula_36. Intuitively, an arbitrary distribution can be simulated from a simulation of the standard uniform distribution.
Early approaches.
An early computer-based PRNG, suggested by John von Neumann in 1946, is known as the middle-square method. The algorithm is as follows: take any number, square it, remove the middle digits of the resulting number as the "random number", then use that number as the seed for the next iteration. For example, squaring the number "1111" yields "1234321", which can be written as "01234321", an 8-digit number being the square of a 4-digit number. This gives "2343" as the "random" number. Repeating this procedure gives "4896" as the next result, and so on. Von Neumann used 10 digit numbers, but the process was the same.
A problem with the "middle square" method is that all sequences eventually repeat themselves, some very quickly, such as "0000". Von Neumann was aware of this, but he found the approach sufficient for his purposes, and was worried that mathematical "fixes" would simply hide errors rather than remove them.
Von Neumann judged hardware random number generators unsuitable, for, if they did not record the output generated, they could not later be tested for errors. If they did record their output, they would exhaust the limited computer memories then available, and so the computer's ability to read and write numbers. If the numbers were written to cards, they would take very much longer to write and read. On the ENIAC computer he was using, the "middle square" method generated numbers at a rate some hundred times faster than reading numbers in from punched cards.
The middle-square method has since been supplanted by more elaborate generators.
Non-uniform generators.
Numbers selected from a non-uniform probability distribution can be generated using a uniform distribution PRNG and a function that relates the two distributions.
First, one needs the cumulative distribution function formula_37 of the target distribution formula_38:
Note that formula_40. Using a random number "c" from a uniform distribution as the probability density to "pass by", we get
so that
is a number randomly selected from distribution formula_38.
For example, the inverse of cumulative Gaussian distribution
formula_44 with an ideal uniform PRNG with range (0, 1) as input formula_45 would produce a sequence of (positive only) values with a Gaussian distribution; however
Similar considerations apply to generating other non-uniform distributions such as Rayleigh and Poisson.

</doc>
<doc id="45527" url="https://en.wikipedia.org/wiki?curid=45527" title="Linear congruential generator">
Linear congruential generator

A linear congruential generator (LCG) is an algorithm that yields a sequence of pseudo-randomized numbers calculated with a discontinuous piecewise linear equation. The method represents one of the oldest and best-known pseudorandom number generator algorithms. The theory behind them is relatively easy to understand, and they are easily implemented and fast, especially on computer hardware which can provide modulo arithmetic by storage-bit truncation.
The generator is defined by the recurrence relation:
where formula_2 is the sequence of pseudorandom values, and
are integer constants that specify the generator. If "c" = 0, the generator is often called a multiplicative congruential generator (MCG), or Lehmer RNG. If "c" ≠ 0, the method is called a "mixed congruential generator".
Period length.
The period of a general LCG is at most "m", and for some choices of factor "a" much less than that. The LCG will have a full period for all seed values if and only if:
These three requirements are referred to as the Hull-Dobell Theorem. While LCGs are capable of producing pseudorandom numbers which can pass formal tests for randomness, this is extremely sensitive to the choice of the parameters "c", "m", and "a".
Historically, poor choices had led to ineffective implementations of LCGs. A particularly illustrative example of this is RANDU, which was widely used in the early 1970s and led to many results which are currently being questioned because of the use of this poor LCG.
Parameters in common use.
The most efficient LCGs have an "m" equal to a power of 2, most often "m" = 232 or "m" = 264, because this allows the modulus operation to be computed by merely truncating all but the rightmost 32 or 64 bits. The following table lists the parameters of LCGs in common use, including built-in "rand()" functions in runtime libraries of various compilers.
As shown above, LCGs do not always use all of the bits in the values they produce. For example, the Java implementation operates with 48-bit values at each iteration but returns only their 32 most significant bits. This is because the higher-order bits have longer periods than the lower-order bits (see below). LCGs that use this truncation technique produce statistically better values than those that do not.
The Knuth representation for 3 variables is as below:
"X""n"+1 = (8121 "X""n" + 28411) mod 134456
Because there are only 134456 distinct possible values, according to the parameter definition, it tends to make it a bit more predictable. If "X""n" is even then "X""n"+1 will be odd, and vice versa, so the lowest order of bit oscillates at each step. This makes the generator to produce bits in each number that are usually not equally random.
Advantages and disadvantages of LCGs.
LCGs are fast and require minimal memory (typically 32 or 64 bits) to retain state. This makes them valuable for simulating multiple independent streams.
LCGs should not be used for applications where high-quality randomness is critical. For example, it is not suitable for a Monte Carlo simulation because of the serial correlation (among other things). They also must not be used for cryptographic applications; see cryptographically secure pseudo-random number generator for more suitable generators. If a linear congruential generator is seeded with a character and then iterated once, the result is a simple classical cipher called an affine cipher; this cipher is easily broken by standard frequency analysis.
LCGs tend to exhibit some severe defects. For instance, if an LCG is used to choose points in an n-dimensional space, the points will lie on, at most, (n!m)1/n hyperplanes (Marsaglia's Theorem, developed by George Marsaglia). This is due to serial correlation between successive values of the sequence "X""n". The spectral test, which is a simple test of an LCG's quality, is based on this fact.
A further problem of LCGs is that the lower-order bits of the generated sequence have a far shorter period than the sequence as a whole if "m" is set to a power of 2. In general, the "n"th least significant digit in the base "b" representation of the output sequence, where "b""k" = "m" for some integer "k", repeats with at most period "b""n".
Yet another problem is that LCGs are not suitable for parallel programming. Multiple threads may access the currently stored state simultaneously causing a race condition. In implementations which use same initialization for different threads, equal sequences of random numbers may occur on simultaneously executing threads. Random number generators, particularly for parallel computers, should not be trusted. It is strongly recommended to check the results of simulation with more than one RNG to check if bias is introduced. Among the recommended generators for use on a parallel computer include combined linear congruential generators using sequence splitting and lagged Fibonacci generators using independent sequences.
Nevertheless, for some applications LCGs may be a good option. For instance, in an embedded system, the amount of memory available is often severely limited. Similarly, in an environment such as a video game console taking a small number of high-order bits of an LCG may well suffice. The low-order bits of LCGs when m is a power of 2 should never be relied on for any degree of randomness whatsoever. Indeed, simply substituting 2"n" for the modulus term reveals that the low order bits go through very short cycles. In particular, any full-cycle LCG when m is a power of 2 will produce alternately odd and even results.
Comparison with other PRNGs.
Generators based on linear recurrences (such as <samp>xorshift*</samp>) or on good avalanching functions (such as SplitMix64[http://prng.di.unimi.it/splitmix64.c]) outperform linear congruential generators even at small state sizes. Moreover, linear generators can generate very long sequences, and when suitably perturbed at the output, they pass strong statistical tests. Several examples can be found in the Xorshift family. 
The Mersenne twister algorithm provides a very long period (219937 − 1) and variate uniformity, but it fails some statistical tests. A common Mersenne twister implementation, interestingly enough, uses an LCG to generate seed data.
Linear congruential generators have the problem that all of the bits in each number are usually not equally random. A linear feedback shift register PRNG produces a stream of pseudo-random bits, each of which are truly pseudo-random, and can be implemented with essentially the same amount of memory as a linear congruential generator, albeit with a bit more computation.
The linear feedback shift register has a strong relationship to linear congruential generators.
Given a few values in the sequence, some techniques can predict the following values in the sequence for not only linear congruent generators but any other polynomial congruent generator.

</doc>
<doc id="45528" url="https://en.wikipedia.org/wiki?curid=45528" title="Opportunity cost">
Opportunity cost

In microeconomic theory, the opportunity cost of a choice is the value of the best alternative forgone where, given limited resources, a choice needs to be made between several mutually exclusive alternatives. Assuming the best choice is made, it is the "cost" incurred by not enjoying the "benefit" that would have been had by taking the second best available choice. The "New Oxford American Dictionary" defines it as "the loss of potential gain from other alternatives when one alternative is chosen." Opportunity cost is a key concept in economics, and has been described as expressing "the basic relationship between scarcity and choice." The notion of opportunity cost plays a crucial part in attempts to ensure that scarce resources are used efficiently. Thus, opportunity costs are not restricted to monetary or financial costs: the real cost of output forgone, lost time, pleasure or any other benefit that provides utility should also be considered opportunity costs.
History.
The term was coined in 1914 by Austrian economist Friedrich von Wieser in his book .• </ref> The idea had been anticipated by previous writers including Benjamin Franklin and Frédéric Bastiat. Franklin coined the phrase "Time is Money", and spelt out the associated opportunity cost reasoning in his “Advice to a Young Tradesman” (1746): “Remember that Time is Money. He that can earn Ten Shillings a Day by his Labour, and goes abroad, or sits idle one half of that Day, tho’ he spends but Sixpence during his Diversion or Idleness, ought not to reckon That the only Expence; he has really spent or rather thrown away Five Shillings besides.”
Bastiat's 1848 essay "What Is Seen and What Is Not Seen" used opportunity cost reasoning in his critique of the broken window fallacy, and of what he saw as spurious arguments for public expenditure.
Opportunity costs in production.
Explicit costs.
Explicit costs are opportunity costs that involve direct monetary payment by producers. The explicit opportunity cost of the factors of production not already owned by a producer is the price that the producer has to pay for them. For instance, if a firm spends $100 on electrical power consumed, its explicit opportunity cost is $100. This cash expenditure represents a lost opportunity to purchase something else with the $100.
Implicit costs.
Implicit costs (also called implied, imputed or notional costs) are the opportunity costs not reflected in cash outflow but implied by the failure of the firm to allocate its existing (owned) resources, or factors of production to the best alternative use. For example: a manufacturer has previously purchased 1000 tons of steel and the machinery to produce a widget. The implicit part of the opportunity cost of producing the widget is the revenue lost by not selling the steel and not renting out the machinery instead of using them for production.
One example of opportunity cost is in the evaluation of "foreign" (to the USA) buyers and their allocation of cash assets in real estate or other types of investment vehicles. With the recent downturn (circa June/July 2015) of the Chinese stock market, more and more Chinese investors from Hong Kong and Taiwan are turning to the United States as an alternative vessel for their investment dollars; the opportunity cost of leaving their money in the Chinese stock market or Chinese real estate market is too high relative to yields available in the USA real estate market
Evaluation.
Note that opportunity cost is not the "sum" of the available alternatives when those alternatives are, in turn, mutually exclusive to each other – it is the "next best" alternative given up selecting the best option. The opportunity cost of a city's decision to build the hospital on its vacant land is the loss of the land for a sporting center, or the inability to use the land for a parking lot, or the money which could have been made from selling the land. Use for any one of those purposes would preclude the possibility to implement any of the other.

</doc>
<doc id="45534" url="https://en.wikipedia.org/wiki?curid=45534" title="Aldo Rossi">
Aldo Rossi

Aldo Rossi (3 May 1931 – 4 September 1997) was an Italian architect and designer who accomplished the unusual feat of achieving international recognition in four distinct areas: theory, drawing, architecture and product design.
He was the first Italian to receive the Pritzker Prize for architecture.
Early life.
He was born in Milan, Italy. After early education by the Somascan Religious Order and then at Alessandro Volta College in Lecco, in 1949 he went to the school of architecture at the Polytechnic University of Milan. His thesis advisor was Piero Portaluppi and he graduated in 1959.
In 1955 he had started writing for, and from 1959 was one of the editors of, the architectural magazine Casabella-Continuità, with editor in chief Ernesto Nathan Rogers. Rossi left in 1964, when the chief editorship went to Gian Antonio Bernasconi. Rossi went on to work for Società magazine and Il_contemporaneo, making Rossi one of the most active participants in the fervent cultural debate of the time.
His early articles cover architects such as Alessandro Antonelli, Mario Ridolfi, Auguste Perret and Emil Kaufmann and much of this material became part of his second book, "Scritti scelti sull'architettura e la città 1956-1972" ("Selected writings on architecture and the city from 1956 to 1972"). He married the Swiss actress Sonia Gessner, who introduced him to the world of film and theater. Culture and his family became central to his life. His son Fausto was active in movie-making both in front of and behind the camera and his daughter Vera was involved with theatre.
Career.
He began his professional career at the studio of Ignazio Gardella in 1956, moving on to the studio of Marco Zanuso. In 1963 also he began teaching, firstly as an assistant to Ludovico Quaroni (1963) at the school of urban planning in Arezzo, then to Carlo Aymonino at the Institute of Architecture in Venice. In 1965 he was appointed lecturer at the Polytechnic University of Milan and the following year he published "The architecture of the city" which soon became a classic of architectural literature.
His professional career, initially dedicated to architectural theory and small building work took a huge leap forward when Aymonino allowed Rossi to design part of the Monte Amiata complex in the Gallaratese quarter of Milan. In 1971 he won the design competition for the extension of the San Cataldo Cemetery in Modena, which made him internationally famous.
After suspension from teaching in Italy in those politically troubled times, he moved to ETH Zurich, occupying the chair in architectural design from 1971 to 1975.
In 1973 he was director of the International Architecture Section at the XV Milan Triennial Exhibition of Decorative Arts and Modern Architecture, where he presented, among others, his student Arduino Cantafora. Rossi's design ideas for the exhibition are explained in the International Architecture Catalogue and in a 16mm documentary "Ornament and crime" directed by Luigi Durissi and produced along with Gianni Braghieri and Franco Raggi. In 1975, Rossi returned to the teaching profession in Italy, teaching architectural composition in Venice.
In 1979 he was made a member of the prestigious Academy of Saint Luke. Meanwhile, there was international interest in his skills. He taught at several universities in the United States, including Cooper Union in New York City and Cornell University in Ithaca (New York State). At Cornell he participated in the "Institute for Architecture and Urban Studies" joint venture with New York's Museum of Modern Art, travelling to China and Hong Kong and attending conferences in South America.
In 1981 he published his autobiography, "A scientific autobiography". In this work the author, "in discrete disorder", brings back memories, objects, places, forms, literature notes, quotes, and insights and tries to "... go over things or impressions, describe, or look for ways to describe." In the same year he won first prize at the international competition for the design of an apartment block on the corner of Kochstraße and Wilhelmstraße in central Berlin.
In 1984 together with Ignazio Gardella and Fabio Reinhart, he won the competition for the renovation of the Teatro Carlo Felice in Genoa, which was not fully completed until 1991. In 1985 and 1986 Rossi was director of the 3rd (respectively 4th) International Architecture Exhibition at the Venice Biennale including further away display spaces such as Villa Farsetti in Santa Maria di Sala.
In 1987 he won two international competitions: one for a site at the Parc de la Villette in Paris, the other for the Deutsches Historisches Museum in Berlin, which was never brought to fruition. In 1989 he continued product design work for Unifor (now part of Molteni Furniture) and Alessi. His espresso maker "La Cupola", designed for Alessi came out in 1988.
In 1990 he was awarded the Pritzker Prize. The city of Fukuoka in Japan honoured him for his work on the hotel complex "The Palace" and he won the 1991 Thomas Jefferson Medal in Public Architecture from the American Institute of Architects. These prestigious awards were followed by exhibitions at the Centre Georges Pompidou in Paris, the Beurs van Berlage in Amsterdam, the Berlinische Galerie in Berlin and the Museum of Contemporary Art in Ghent, Belgium.
In 1996 he became an honorary member of the American Academy of Arts and Letters and the following year he received their special cultural award in architecture and design. He died in Milan on 4 September 1997, following a car accident. Posthumously he received the "Torre Guinigi" prize for his contribution to urban studies and the "Seaside Prize" of the Seaside Institute, Florida, where he had built a detached family home in 1995.
On appeal his proposals won the 1999 competition for the restoration of the Teatro La Fenice, Venice and it reopened in 2004. In 1999 the Faculty of Architecture of the University of Bologna, based in Cesena, was named after him.
Work.
His earliest works of the 1960s were mostly theoretical and displayed a simultaneous influence of 1920s Italian modernism ("see Giuseppe Terragni"), classicist influences of Viennese architect Adolf Loos, and the reflections of the painter Giorgio de Chirico. A trip to the Soviet Union to study Stalinist architecture also left a marked impression.
In his writings Rossi criticized the lack of understanding of the city in current architectural practice. He argued that a city must be studied and valued as something constructed over time; of particular interest are urban artifacts that withstand the passage of time. Rossi held that the city remembers its past (our "collective memory"), and that we use that memory through monuments; that is, monuments give structure to the city.
He became extremely influential in the late 1970s and 1980s as his body of built work expanded and for his theories promoted in his books "The Architecture of the City" ("L'architettura della città", 1966) and "A Scientific Autobiography" ("Autobiografia scientifica", 1981).The largest of Rossi's projects in terms of scale was the San Cataldo Cemetery, in Modena, Italy, which began in 1971 but is yet to be completed. Rossi referred to it as a "city of the dead".
The distinctive independence of his buildings is reflected in the micro-architectures of the products designed by Rossi. In the 1980s Rossi designed stainless steel cafetières and other products for Alessi, Pirelli, and others.
Exhibits.
For the Venice Biennale in 1979 Rossi designed a floating "Teatro del Mondo" that seated 250 people. For the Venice Biennale in 1984, he designed a triumphal arch at the entrance to the exhibition site. In 2006 two pylons based on an original 1989 design by Aldo Rossi were erected in front of the Bonnefanten Museum in Maastricht by the Delft architectural firm Ufo Architecten.
Awards.
Aldo Rossi won the prestigious Pritzker Prize for architecture in 1990. Ada Louise Huxtable, architectural critic and Pritzker juror, has described Rossi as "a poet who happens to be an architect."
Product design.
In addition to architecture, Rossi, created product designs, including:

</doc>
<doc id="45535" url="https://en.wikipedia.org/wiki?curid=45535" title="Alessi (Italian company)">
Alessi (Italian company)

Alessi is a housewares and kitchen utensil company in Italy, producing everyday items from plastic and metal, created by famous designers.
From the 1990s onward, Alessi has been associated with the notion of "designer" objects — otherwise ordinary tools and objects executed as high design, particularly in a post-modern mode, from designers such as Philippe Starck. Many of the early memorable "designer kettles", "designer toothbrushes", "designer kitchenware", and so on were Alessi products, though competition in this product category has greatly increased since then.
History.
1920s to 1940s.
Alessi was founded in 1921 by Giovanni Alessi. The firm began as a workshop in Valle Strona near Lake Orta in the Italian Alps near Switzerland. An area known for its tradition in making small objects of wood or metal for in the house and in the kitchen in general. Alessi started with producing a wide range of tableware items in nickel, chromium and silver-plated brass. The company’s intention was to produce hand-crafted items with the aid of machines. Design in the current sense of the term began when Carlo Alessi (born 1916), son of Giovanni, was named chief designer. Carlo was trained as an industrial designer. Between 1935 and 1945 he developed virtually all of the products Alessi produced. In 1945 he ascended to chief executive and designed the coffee service.
1950s and 1960s.
In the 1950s the company was under the leadership of Carlo Alessi. It was his brother Ettore Alessi who introduced the collaboration with external designers in 1955. With some architects, he designed a number of items which were created for the hotel needs. Through his intervention caused many individual objects, which were best-sellers, such as the historical series of "wire baskets". One of the key designs of this period is the "shaker" from 1957 by Luigi Massoni and Carlo Mazzeri. This was designed in a series with an "Ice bucket" and "Ice tongs" as part of the Program 4 for the 11 triennale in Milan. This was the first time that the Alessi products got shown with manufactured goods. The 1950s were a difficult time to sell designer objects, as it was only a few years after World War II, and many people could not afford designer objects.
1970s and 1980s.
In 1970 Alberto Alessi was responsible for the third transformation of the company. Alessi was considered one of the "Italian Design Factories". In this decade under the leadership of Alberto Alessi the company collaborated with some design maestros like Achille Castiglioni, Richard Sapper, Alessandro Mendini and Ettore Sottsass, who are now all icons of the 1970s. In the '70s Alessi produced the "Condiment set" (salt, pepper and toothpicks) by Ettore Sottsass, the "Espressomaker" by Sapper.
The 1980s marked a period in which Italian design factories had to compete with mass production. These movements had a different view on design, for the Italian design factories the design and therefore the designer was the most important part of the process while for the mass production the design had to be functional and easy to be reproduced. Also in the 1980s, they changed their marketing image from factory to industrial research lab, meaning that it is a place for research and production. For Alessi the '80s are marked with some iconic designs like the "Two tone kettle" by Sapper, their first cutlery set "Dry" by Castiglioni. Alessi collaborated with new designers like Aldo Rossi, Michael Graves and Philippe Starck who have been responsible for the some of Alessi's all time bestseller like the "kettle" with a bird whistle by Graves.
1990s to the present.
In recent decades as the "designer houseware" market greatly expanded, Alessi faced increasing competition from other international manufacturers, especially in lower-cost products mass-produced for retailers such as Target Corporation and J. C. Penney.
In the 1990s Alessi started to work more with plastics, at the request of designers who found it an easier material to work with than metal, offering more design freedom and innovative possibilities. The 1990s were marked by the theme "Family Follows Fiction" with playful and imaginative objects. Artists designing for this theme included Stefano Giovannoni and Alessandro Mendini, who designed "Fruit Mama" and the bestseller "Anna G." Metal still remained a popular material, for example the "Girotondo" family by King Kong. 
During the 2000s Alessi collaborated with several architects for the "coffee and tea towers", with a new generation of architects such as Wiel Arets, Zaha Hadid, Toyo Ito, Tom Kovac, Greg Lynn, MVRDV, Jean Nouvel, and UN Studio. These sets had a limited production of 99 copies. 
Another remarkable design in the 2000s is the "Blow Up" series by Fratelli Campana. The brothers played with form and shape to create baskets and other objects that look like they would fall apart when touched.
In 2006 the company reclassified its products under three lines: "A di Alessi", "Alessi" and "Officina Alessi". "A di Alessi" is more ‘democratic’ and more ‘pop’. This product line is the lower price range of Alessi. "Alessi" is the historic brand company and continues to develop the best of industrial mass production industry from the view of quality and design in a medium/high price range. The "Officina Alessi" is more exclusive, innovative and experimental, this is marked by small batch production series and limited series.
Alessi products are on display in museums worldwide like Museum of Modern Art New York, Metropolitan Museum of Art, Victoria and Albert Museum, Pompidou Centre, and Stedelijk Museum Italy.
Designers and their designs.
From 1945 until today Alessi has collaborated with designers and even other brands or companies for their products. Some key designs and their designers:
A collaboration with the National Palace Museum of Taiwan, produced a collection of various kitchenware products with Asian themes.

</doc>
<doc id="45537" url="https://en.wikipedia.org/wiki?curid=45537" title="Ustad Isa">
Ustad Isa

Ustad Isa Shirazi ( translation "Master Isa") was a Persian architect, often described as the chief architect of the Taj Mahal in Agra, India.
The lack of complete and reliable information as to whom the credit for the design belongs, led to innumerable speculations. Scholars suggest the story of Ustad Isa was born of the eagerness of the British in the 19th century to believe that such a beautiful building should be credited to a European architect. Local informants were reported to have sated British curiosity regarding the origins of the Taj by also supplying them with fictitious lists of workmen and materials from all over Asia. Typically, he is described as a Persian architect.
Recent research suggests the Persian architect, Ustad Ahmad Lahauri was the most likely candidate as the chief architect of the Taj, an assertion based on a claim made in writings by Lahauri's son Lutfullah Muhandis.

</doc>
<doc id="45538" url="https://en.wikipedia.org/wiki?curid=45538" title="Mersenne Twister">
Mersenne Twister

The Mersenne Twister is a pseudorandom number generator (PRNG). It is by far the most widely used general-purpose PRNG. Its name derives from the fact that its period length is chosen to be a Mersenne prime.
The Mersenne Twister was developed in 1997 by and . It was designed specifically to rectify most of the flaws found in older PRNGs. It was the first PRNG to provide fast generation of high-quality pseudorandom integers.
The most commonly used version of the Mersenne Twister algorithm is based on the Mersenne prime 219937−1. The standard implementation of that, MT19937, uses a 32-bit word length. There is another implementation that uses a 64-bit word length, MT19937-64; it generates a different sequence.
Adoption in software systems.
The Mersenne Twister PRNG is available by default for the following software systems:
R,
PHP,
Python,
Ruby,
CMU Common Lisp,
Embeddable Common Lisp,
Steel Bank Common Lisp,
Free Pascal,
GLib,
SageMath,
Maple,
MATLAB,
GAUSS,
IDL,
Julia,
Scilab,
Stata,
GNU Octave,
the GNU Scientific Library,
the GNU Multiple Precision Arithmetic Library,
and Microsoft Visual C++.
It is also available in standard C++ (since C++11)
and Apache.
Add-on implementations are provided in many program libraries, including the Boost C++ Libraries
and the NAG Numerical Library.
The Mersenne Twister is one of two PRNGs in SPSS: the other generator is kept only for compatibility with older programs, and the Mersenne Twister is stated to be "more reliable".
The Mersenne Twister is similarly one of the PRNGs in SAS: the other generators are older and deprecated.
Advantages.
The commonly-used version of Mersenne Twister, MT19937, which produces a sequence of 32-bit integers, has the following desirable properties:
Disadvantages.
The large state space comes with a performance cost: the 2.5 KiB state buffer will place a load on the memory caches. In 2011, Saito & Matsumoto proposed a version of the Mersenne Twister to address this issue. The tiny version, TinyMT, uses just 127 bits of state space.
By today's standards, the Mersenne Twister is somewhat slow, unless the SFMT implementation is used (see section below).
It passes most, but not all, of the stringent TestU01 randomness tests.
Multiple Mersenne Twister instances that differ only in seed value (but not other parameters) are not generally appropriate for Monte-Carlo simulations that require independent random number generators, though there exists a method for choosing multiple sets of parameter values.
It can take a long time to start generating output that passes randomness tests, if the initial state is highly non-random—particularly if the initial state has many zeros. A consequence of this is that two instances of the generator, started with initial states that are almost the same, will usually output nearly the same sequence for many iterations, before eventually diverging. The 2002 update to the MT algorithm has improved initialization, so that beginning with such a state is very unlikely.
"k"-distribution.
A pseudorandom sequence "xi" of "w"-bit integers of period "P" is said to be "k"-distributed to "v"-bit accuracy if the following holds.
The general algorithm is characterized by the following quantities (some of these explanations make sense only after reading the rest of the algorithm):
with the restriction that 2"nw" − "r" − 1 is a Mersenne prime. This choice simplifies the primitivity test and "k"-distribution test that are needed in the parameter search.
The series x is defined as a series of "w"-bit quantities with the recurrence relation:
where formula_5 denotes the bitwise or, formula_6 the bitwise exclusive or (XOR), formula_7 means the upper formula_8 bits of formula_9, and formula_10 means the lower formula_11 bits of formula_12. The twist transformation "A" is defined in rational normal form as:
formula_13
with "I""n" − 1 as the ("n" − 1) × ("n" − 1) identity matrix. The rational normal form has the benefit that multiplication by "A" can be efficiently expressed as: (remember that here matrix multiplication is being done in "F"2, and therefore bitwise XOR takes the place of addition)
formula_14
where "x"0 is the lowest order bit of "x".
As like TGFSR(R), the Mersenne Twister is cascaded with a tempering transform to compensate for the reduced dimensionality of equidistribution (because of the choice of "A" being in the rational normal form). Note that this is equivalent to using the matrix "A where "A = "T"−1"AT" for "T" an invertible matrix, and therefore the analysis of characteristic polynomial mentioned below still holds.
As with "A", we choose a tempering transform to be easily computable, and so do not actually construct "T" itself. The tempering is defined in the case of Mersenne Twister as
where x is the next value from the series, y a temporary intermediate value, z the value returned from the algorithm, with Â«, Â» as the bitwise left and right shifts, and & as the bitwise and. The first and last transforms are added in order to improve lower-bit equidistribution. From the property of TGFSR, formula_15 is required to reach the upper bound of equidistribution for the upper bits.
The coefficients for MT19937 are:
Note that 32-bit implementations of the Mersenne Twister generally have "d" = FFFFFFFF16. As a result, the "d" is occasionally omitted from the algorithm description, since the bitwise and with "d" in that case has no effect.
The coefficients for MT19937-64 are:
Initialization.
As should be apparent from the above description, the state needed for a Mersenne Twister implementation is an array of "n" values of "w" bits each. To initialize the array, a "w"-bit seed value is used to supply "x"0 through "x""n" − 1 by setting "x"0 to the seed value and thereafter setting
for "i" from 1 to "n"-1. The first value the algorithm then generates is based on "x""n". The constant "f" forms another parameter to the generator, though not part of the algorithm proper. The value for "f" for MT19937 is 1812433253 and for MT19937-64 is 6364136223846793005.
Comparison with classical GFSR.
In order to achieve the 2"nw" − "r" − 1 theoretical upper limit of the period in a TGFSR, "φ""B"("t") must be a primitive polynomial, "φ""B"("t") being the characteristic polynomial of
formula_16
formula_17
The twist transformation improves the classical GFSR with the following key properties:
Pseudocode.
The following piece of pseudocode implements the general Mersenne Twister algorithm. The constants w, n, m, r, a, u, d, s, b, t, c, l, and f are as in the algorithm description above. It is assumed that int represents a type sufficient to hold values with w bits:
Python implementation.
This python implementation hard-codes the constants for MT19937:
SFMT.
SFMT, the single instruction, multiple data-oriented fast Mersenne Twister, is a variant of Mersenne Twister, introduced in 2006, designed to be fast when it runs on 128-bit SIMD.
Intel SSE2 and PowerPC AltiVec are supported by SFMT. It is also used for games with the Cell BE in the PlayStation 3.
MTGP.
MTGP is a variant of Mersenne Twister optimised for graphics processing units published by Mutsuo Saito and Makoto Matsumoto. The basic linear recurrence operations are extended from MT and parameters are chosen to allow many threads to compute the recursion in parallel, while sharing their state space to reduce memory load. The paper claims improved equidistribution over MT and performance on a high specification GPU (Nvidia GTX260 with 192 cores) of 4.7 ms for 5×107 random 32-bit integers.

</doc>
<doc id="45541" url="https://en.wikipedia.org/wiki?curid=45541" title="Social Darwinism">
Social Darwinism

Social Darwinism is a name given to various theories of society which emerged in the United Kingdom, North America, and Western Europe in the 1870s, and which claim to apply biological concepts of natural selection and survival of the fittest to sociology and politics. According to their critics, at least, social Darwinists argue that the strong should see their wealth and power increase while the weak should see their wealth and power decrease. Different social-Darwinist groups have differing views about which groups of people are considered to be "the strong" and which groups of people are considered to be "the weak", and they also hold different opinions about the precise mechanisms that should be used to reward strength and punish weakness. Many such views stress competition between individuals in "laissez-faire" capitalism, while others are claimed to have motivated ideas of authoritarianism, eugenics, racism, imperialism, fascism, Nazism, and struggle between national or racial groups.
The term "Social Darwinism" gained widespread currency when used after 1944 by opponents of these earlier concepts. The majority of those who have been categorised as social Darwinists did not identify themselves by such a label.
Creationists have often maintained that social Darwinism—leading to policies designed to reward the most competitive—is a logical consequence of "Darwinism" (the theory of natural selection in biology).
Biologists and historians have stated that this is a fallacy of appeal to nature, since the theory of natural selection is merely intended as a description of a biological phenomenon and should not be taken to imply that this phenomenon is "good" or that it ought to be used as a moral guide in human society. While most scholars recognize some historical links between the popularisation of Darwin's theory and forms of social Darwinism, they also maintain that social Darwinism is not a necessary consequence of the principles of biological evolution.
Scholars debate the extent to which the various social Darwinist ideologies reflect Charles Darwin's own views on human social and economic issues. His writings have passages that can be interpreted as opposing aggressive individualism, while other passages appear to promote it. Some scholars argue that Darwin's view gradually changed and came to incorporate views from other theorists such as Herbert Spencer. Spencer published his Lamarckian evolutionary ideas about society before Darwin first published his theory in 1859, and both Spencer and Darwin promoted their own conceptions of moral values. Spencer supported "laissez-faire" capitalism on the basis of his Lamarckian belief that struggle for survival spurred self-improvement which could be inherited.
Origin of the term.
The term first appeared in Europe in 1877, and around this time it was used by sociologists opposed to the concept. The term was popularized in the United States in 1944 by the American historian Richard Hofstadter who used it in the ideological war effort against fascism to denote a reactionary creed which promoted competitive strife, racism and chauvinism. Hofstadter later also recognized (what he saw as) the influence of Darwinist and other evolutionary ideas upon those with collectivist views, enough to devise a term for the phenomenon, "Darwinist collectivism". Before Hofstadter's work the use of the term "social Darwinism" in English academic journals was quite rare. In fact,
The term "social Darwinism" has rarely been used by advocates of the supposed ideologies or ideas; instead it has almost always been used pejoratively by its opponents. The term draws upon the common use of the term "Darwinism", which has been used to describe a range of evolutionary views, but in the late 19th century was applied more specifically to natural selection as first advanced by Charles Darwin to explain speciation in populations of organisms. The process includes competition between individuals for limited resources, popularly but inaccurately described by the phrase "survival of the fittest", a term coined by sociologist Herbert Spencer.
While the term has been applied to the claim that Darwin's theory of evolution by natural selection can be used to understand the social endurance of a nation or country, social Darwinism commonly refers to ideas that predate Darwin's publication of "On the Origin of Species". Others whose ideas are given the label include the 18th century clergyman Thomas Malthus, and Darwin's cousin Francis Galton who founded eugenics towards the end of the 19th century.
Theories and origins.
The term Darwinism had been coined by Thomas Henry Huxley in his April 1860 review of "On the Origin of Species", and by the 1870s it was used to describe a range of concepts of evolutionism or development, without any specific commitment to Charles Darwin's own theory.
The first use of the phrase "social Darwinism" was in Joseph Fisher's 1877 article on "The History of Landholding in Ireland" which was published in the "Transactions of the Royal Historical Society". Fisher was commenting on how a system for borrowing livestock which had been called "tenure" had led to the false impression that the early Irish had already evolved or developed land tenure;
Despite the fact that social Darwinism bears Charles Darwin's name, it is also linked today with others, notably Herbert Spencer, Thomas Malthus, and Francis Galton, the founder of eugenics. In fact, Spencer was not described as a social Darwinist until the 1930s, long after his death.
Darwin himself gave serious consideration to Galton's work, but considered the ideas of "hereditary improvement" impractical. Aware of weaknesses in his own family, Darwin was sure that families would naturally refuse such selection and wreck the scheme. He thought that even if compulsory registration was the only way to improve the human race, this illiberal idea would be unacceptable, and it would be better to publicize the "principle of inheritance" and let people decide for themselves.
In "The Descent of Man, and Selection in Relation to Sex" of 1882 Darwin described how medical advances meant that the weaker were able to survive and have families, and as he commented on the effects of this, he cautioned that hard reason should not override sympathy and considered how other factors might reduce the effect:
Thus the weak members of civilized societies propagate their kind. No one who has attended to the breeding of domestic animals will doubt that this must be highly injurious to the race of man. It is surprising how soon a want of care, or care wrongly directed, leads to the degeneration of a domestic race; but excepting in the case of man himself, hardly any one is so ignorant as to allow his worst animals to breed.The aid which we feel impelled to give to the helpless is mainly an incidental result of the instinct of sympathy, which was originally acquired as part of the social instincts, but subsequently rendered, in the manner previously indicated, more tender and more widely diffused. Nor could we check our sympathy, even at the urging of hard reason, without deterioration in the noblest part of our nature. The surgeon may harden himself whilst performing an operation, for he knows that he is acting for the good of his patient; but if we were intentionally to neglect the weak and helpless, it could only be for a contingent benefit, with an overwhelming present evil.
... We must therefore bear the undoubtedly bad effects of the weak surviving and propagating their kind; but there appears to be at least one check in steady action, namely that the weaker and inferior members of society do not marry so freely as the sound; and this check might be indefinitely increased by the weak in body or mind refraining from marriage, though this is more to be hoped for than expected.
Social Darwinists.
Herbert Spencer's ideas, like those of evolutionary progressivism, stemmed from his reading of Thomas Malthus, and his later theories were influenced by those of Darwin. However, Spencer's major work, "Progress: Its Law and Cause" (1857), was released two years before the publication of Darwin's "On the Origin of Species", and "First Principles" was printed in 1860.
In "The Social Organism" (1860), Spencer compares society to a living organism and argues that, just as biological organisms evolve through natural selection, society evolves and increases in complexity through analogous processes.
In many ways, Spencer's theory of cosmic evolution has much more in common with the works of Lamarck and Auguste Comte's positivism than with Darwin's.
Jeff Riggenbach argues that Spencer's view was that culture and education made a sort of Lamarckism possible and notes that Herbert Spencer was a proponent of private charity.
Spencer's work also served to renew interest in the work of Malthus. While Malthus's work does not itself qualify as social Darwinism, his 1798 work "An Essay on the Principle of Population", was incredibly popular and widely read by social Darwinists. In that book, for example, the author argued that as an increasing population would normally outgrow its food supply, this would result in the starvation of the weakest and a Malthusian catastrophe.
According to Michael Ruse, Darwin read Malthus' famous "Essay on a Principle of Population" in 1838, four years after Malthus' death. Malthus himself anticipated the social Darwinists in suggesting that charity could exacerbate social problems.
Another of these social interpretations of Darwin's biological views, later known as eugenics, was put forth by Darwin's cousin, Francis Galton, in 1865 and 1869. Galton argued that just as physical traits were clearly inherited among generations of people, the same could be said for mental qualities (genius and talent). Galton argued that social morals needed to change so that heredity was a conscious decision in order to avoid both the over-breeding by less fit members of society and the under-breeding of the more fit ones.
In Galton's view, social institutions such as welfare and insane asylums were allowing inferior humans to survive and reproduce at levels faster than the more "superior" humans in respectable society, and if corrections were not soon taken, society would be awash with "inferiors". Darwin read his cousin's work with interest, and devoted sections of "Descent of Man" to discussion of Galton's theories. Neither Galton nor Darwin, though, advocated any eugenic policies restricting reproduction, due to their Whiggish distrust of government.
Friedrich Nietzsche's philosophy addressed the question of artificial selection, yet Nietzsche's principles did not concur with Darwinian theories of natural selection. Nietzsche's point of view on sickness and health, in particular, opposed him to the concept of biological adaptation as forged by Spencer's "fitness". Nietzsche criticized Haeckel, Spencer, and Darwin, sometimes under the same banner by maintaining that in specific cases, sickness was necessary and even helpful. Thus, he wrote:
Wherever progress is to ensue, deviating natures are of greatest importance. Every progress of the whole must be preceded by a partial weakening. The strongest natures retain the type, the weaker ones help to advance it.
Something similar also happens in the individual. There is rarely a degeneration, a truncation, or even a vice or any physical or moral loss without an advantage somewhere else. In a warlike and restless clan, for example, the sicklier man may have occasion to be alone, and may therefore become quieter and wiser; the one-eyed man will have one eye the stronger; the blind man will see deeper inwardly, and certainly hear better. To this extent, the famous theory of the survival of the fittest does not seem to me to be the only viewpoint from which to explain the progress of strengthening of a man or of a race. 
Ernst Haeckel's recapitulation theory was not Darwinism, but rather attempted to combine the ideas of Goethe, Lamarck and Darwin. It was adopted by emerging social sciences to support the concept that non-European societies were "primitive" in an early stage of development towards the European ideal, but since then it has been heavily refuted on many fronts Haeckel's works led to the formation of the Monist League in 1904 with many prominent citizens among its members, including the Nobel Prize winner Wilhelm Ostwald.
The simpler aspects of social Darwinism followed the earlier Malthusian ideas that humans, especially males, require competition in their lives in order to survive in the future. Further, the poor should have to provide for themselves and not be given any aid. However, amidst this climate, most social Darwinists of the early twentieth century actually supported better working conditions and salaries. Such measures would grant the poor a better chance to provide for themselves yet still distinguish those who are capable of succeeding from those who are poor out of laziness, weakness, or inferiority.
Darwinism and hypotheses of social change.
"Social Darwinism" was first described by Oscar Schmidt of the University of Strasbourg, reporting at a scientific and medical conference held in Munich in 1877. He noted how socialists, although opponents of Darwin's theory, used it to add force to their political arguments. Schmidt's essay first appeared in English in "Popular Science" in March 1879. There followed an anarchist tract published in Paris in 1880 entitled "Le darwinisme social" by Émile Gautier. However, the use of the term was very rare—at least in the English-speaking world (Hodgson, 2004)—until the American historian Richard Hofstadter published his influential "Social Darwinism in American Thought" (1944) during World War II.
Hypotheses of social evolution and cultural evolution were common in Europe. The Enlightenment thinkers who preceded Darwin, such as Hegel, often argued that societies progressed through stages of increasing development. Earlier thinkers also emphasized conflict as an inherent feature of social life. Thomas Hobbes's 17th century portrayal of the state of nature seems analogous to the competition for natural resources described by Darwin. Social Darwinism is distinct from other theories of social change because of the way it draws Darwin's distinctive ideas from the field of biology into social studies.
Darwin, unlike Hobbes, believed that this struggle for natural resources allowed individuals with certain physical and mental traits to succeed more frequently than others, and that these traits accumulated in the population over time, which under certain conditions could lead to the descendants being so different that they would be defined as a new species.
However, Darwin felt that "social instincts" such as "sympathy" and "moral sentiments" also evolved through natural selection, and that these resulted in the strengthening of societies in which they occurred, so much so that he wrote about it in "Descent of Man":
The following proposition seems to me in a high degree probable—namely, that any animal whatever, endowed with well-marked social instincts, the parental and filial affections being here included, would inevitably acquire a moral sense or conscience, as soon as its intellectual powers had become as well, or nearly as well developed, as in man. For, firstly, the social instincts lead an animal to take pleasure in the society of its fellows, to feel a certain amount of sympathy with them, and to perform various services for them.
United States.
Spencer proved to be a popular figure in the 1880s primarily because his application of evolution to areas of human endeavor promoted an optimistic view of the future as inevitably becoming better. In the United States, writers and thinkers of the gilded age such as Edward L. Youmans, William Graham Sumner, John Fiske, John W. Burgess, and others developed theories of social evolution as a result of their exposure to the works of Darwin and Spencer.
In 1883, Sumner published a highly influential pamphlet entitled "What Social Classes Owe to Each Other", in which he insisted that the social classes owe each other nothing, synthesizing Darwin's findings with free enterprise Capitalism for his justification. According to Sumner, those who feel an obligation to provide assistance to those unequipped or under-equipped to compete for resources, will lead to a country in which the weak and inferior are encouraged to breed more like them, eventually dragging the country down. Sumner also believed that the best equipped to win the struggle for existence was the American businessman, and concluded that taxes and regulations serve as dangers to his survival. This pamphlet makes no mention of Darwinism, and only refers to Darwin in a statement on the meaning of liberty, that "There never has been any man, from the primitive barbarian up to a Humboldt or a Darwin, who could do as he had a mind to."
Sumner never fully embraced Darwinian ideas, and some contemporary historians do not believe that Sumner ever actually believed in social Darwinism. The great majority of American businessmen rejected the anti-philanthropic implications of the theory. Instead they gave millions to build schools, colleges, hospitals, art institutes, parks and many other institutions. Andrew Carnegie, who admired Spencer, was the leading philanthropist in the world (1890–1920), and a major leader against imperialism and warfare.
H. G. Wells was heavily influenced by Darwinist thoughts, and novelist Jack London wrote stories of survival that incorporated his views on social Darwinism. Film director Stanley Kubrick has been quoted to have held social Darwinist opinions.
Japan.
Social Darwinism has influenced political, public health and social movements in Japan since the late 19th and early 20th century. Social Darwinism was originally brought to Japan through the works of Francis Galton and Ernst Haeckel as well as United States, British and French Lamarkian eugenic written studies of the late 19th and early 20th centuries. Eugenism as a science was hotly debated at the beginning of the 20th century, in "Jinsei-Der Mensch", the first eugenics journal in the empire. As Japan sought to close ranks with the west, this practice was adopted wholesale along with colonialism and its justifications.
China.
Social Darwinism was formally introduced to China through the translation by Yan Fu of Huxley's "Evolution and Ethics", in the course of an extensive series of translations of influential Western thought. Yan's translation strongly impacted Chinese scholars because he added national elements not found in the original. He understood Spencer's sociology as "not merely analytical and descriptive, but prescriptive as well", and saw Spencer building on Darwin, whom Yan summarized thus:
By the 1920s, social Darwinism found expression in the promotion of eugenics by the Chinese sociologist Pan Guangdan.
When Chiang Kai-shek started the New Life movement in 1934, he
Nazi Germany.
Nazi Germany's justification for its aggression was regularly promoted in Nazi propaganda films depicting scenes such as beetles fighting in a lab setting to demonstrate the principles of "survival of the fittest" as depicted in "Alles Leben ist Kampf" (English translation: "All Life is Struggle"). Hitler often refused to intervene in the promotion of officers and staff members, preferring instead to have them fight amongst themselves to force the "stronger" person to prevail—"strength" referring to those social forces void of virtue or principle. Key proponents were Alfred Rosenberg, who was hanged later at Nuremberg. Such ideas also helped to advance euthanasia in Germany, especially Action T4, which led to the murder of mentally ill and disabled people in Germany.
The argument that Nazi ideology was strongly influenced by social Darwinist ideas is often found in historical and social science literature. For example, the philosopher and historian Hannah Arendt analysed the historical development from a politically indifferent scientific Darwinism via social Darwinist ethics to racist ideology.
By 1985, creationists were taking up the argument that Nazi ideology was directly influenced by Darwinian evolutionary theory.
Such claims have been presented by creationists such as Jonathan Sarfati. Intelligent design creationism supporters have promoted this position as well. For example, it is a theme in the work of Richard Weikart, who is a historian at California State University, Stanislaus, and a senior fellow for the Center for Science and Culture of the Discovery Institute.
It is also a main argument in the 2008 intelligent-design/creationist movie "". These claims are widely criticized. The Anti-Defamation League has rejected such attempts to link Darwin's ideas with Nazi atrocities, and has stated that "Using the Holocaust in order to tarnish those who promote the theory of evolution is outrageous and trivializes the complex factors that led to the mass extermination of European Jewry."
Similar criticisms are sometimes applied (or misapplied) to other political or scientific theories that resemble social Darwinism, for example criticisms leveled at evolutionary psychology. For example, a critical reviewer of Weikart's book writes that "(h)is historicization of the moral framework of evolutionary theory poses key issues for those in sociobiology and evolutionary psychology, not to mention bioethicists, who have recycled many of the suppositions that Weikart has traced."
Another example is recent scholarship that portrays Ernst Haeckel's Monist League as a mystical progenitor of the Völkisch movement and, ultimately, of the Nazi Party of Adolf Hitler. Scholars opposed to this interpretation, however, have pointed out that the Monists were freethinkers who opposed all forms of mysticism, and that their organizations were immediately banned following the Nazi takeover in 1933 because of their association with a wide variety of causes including feminism, pacifism, human rights, and early gay rights movements.
Criticism and controversy.
Multiple incompatible definitions.
Social Darwinism has many definitions, and some of them are incompatible with each other. As such, social Darwinism has been criticized for being an inconsistent philosophy, which does not lead to any clear political conclusions. For example, "The Concise Oxford Dictionary of Politics" states:
Part of the difficulty in establishing sensible and consistent usage is that commitment to the biology of natural selection and to 'survival of the fittest' entailed nothing uniform either for sociological method or for political doctrine. A 'social Darwinist' could just as well be a defender of laissez-faire as a defender of state socialism, just as much an imperialist as a domestic eugenist.
Nazism, Eugenics, Fascism, Imperialism.
Social Darwinism was predominantly found in laissez-faire societies where the prevailing view was that of an individualist order to society. As such, social Darwinism supposed that human progress would generally favor the most individualistic races, which were those perceived as stronger. A different form of social Darwinism was part of the ideological foundations of Nazism and other fascist movements. This form did not envision survival of the fittest within an individualist order of society, but rather advocated a type of racial and national struggle where the state directed human breeding through eugenics. Names such as "Darwinian collectivism" or "Reform Darwinism" have been suggested to describe these views, in order to differentiate them from the individualist type of social Darwinism.
Some pre-twentieth century doctrines subsequently described as social Darwinism appear to anticipate state imposed eugenics and the race doctrines of Nazism. Critics have frequently linked evolution, Charles Darwin and social Darwinism with racialism, nationalism, imperialism and eugenics, contending that social Darwinism became one of the pillars of fascism and Nazi ideology, and that the consequences of the application of policies of "survival of the fittest" by Nazi Germany eventually created a very strong backlash against the theory.
As mentioned above, social Darwinism has often been linked to nationalism and imperialism. During the age of New Imperialism, the concepts of evolution justified the exploitation of "lesser breeds without the law" by "superior races". To elitists, strong nations were composed of white people who were successful at expanding their empires, and as such, these strong nations would survive in the struggle for dominance. With this attitude, Europeans, except for Christian missionaries, seldom adopted the customs and languages of local people under their empires.
Peter Kropotkin – "Mutual Aid: A Factor of Evolution".
Peter Kropotkin argued in his 1902 book "" that Darwin did not define the fittest as the strongest, or most clever, but recognized that the fittest could be those who cooperated with each other. In many animal societies, "struggle is replaced by co-operation".
It may be that at the outset Darwin himself was not fully aware of the generality of the factor which he first invoked for explaining one series only of facts relative to the accumulation of individual variations in incipient species. But he foresaw that the term which he was introducing into science would lose its philosophical and its only true meaning if it were to be used in its narrow sense only—that of a struggle between separate individuals for the sheer means of existence. And at the very beginning of his memorable work he insisted upon the term being taken in its "large and metaphorical sense including dependence of one being on another, and including (which is more important) not only the life of the individual, but success in leaving progeny." [Quoting "Origin of Species," chap. iii, p. 62 of first edition.
While he himself was chiefly using the term in its narrow sense for his own special purpose, he warned his followers against committing the error (which he seems once to have committed himself) of overrating its narrow meaning. In "The Descent of Man" he gave some powerful pages to illustrate its proper, wide sense. He pointed out how, in numberless animal societies, the struggle between separate individuals for the means of existence disappears, how struggle is replaced by co-operation, and how that substitution results in the development of intellectual and moral faculties which secure to the species the best conditions for survival. He intimated that in such cases the fittest are not the physically strongest, nor the cunningest, but those who learn to combine so as mutually to support each other, strong and weak alike, for the welfare of the community. "Those communities", he wrote, "which included the greatest number of the most sympathetic members would flourish best, and rear the greatest number of offspring" (2nd edit., p. 163). The term, which originated from the narrow Malthusian conception of competition between each and all, thus lost its narrowness in the mind of one who knew Nature.
Noam Chomsky discussed briefly Kropotkin's views in a July 8, 2011 YouTube video from Renegade Economist, in which he said Kropotkin argued
... the exact opposite Social Darwinism. He argued that on Darwinian grounds, you would expect cooperation and mutual aid to develop leading towards community, workers' control and so on. Well, you know, he didn't prove his point. It's at least as well argued as Herbert Spencer is ...

</doc>
<doc id="45547" url="https://en.wikipedia.org/wiki?curid=45547" title="Refugee">
Refugee

A refugee, according to the Geneva Convention on Refugees is a person who is outside their country of citizenship because they have well-founded grounds for fear of persecution because of their race, religion, nationality, membership of a particular social group or political opinion, and is unable to obtain sanctuary from their home country or, owing to such fear, is unwilling to avail themselves of the protection of that country; or in the case of not having a nationality and being outside their country of former habitual residence as a result of such event, is unable or, owing to such fear, is unwilling to return to their country of former habitual residence. Such a person may be called an "asylum seeker" until considered with the status of "refugee" by the Contracting State where they formally make a claim for sanctuary or right of asylum.
In UN parlance, the definition of the word has been expanded to include descendants of refugees in the case of two specific groups, viz. Palestinian refugees and Sahrawi refugees. As a result, the vast majority of registered refugees within these two groups are not themselves refugees. Rather, they have inherited their refugee status and hence their eligibility for aid and services, provided they meet certain criteria established by the UN and/or aid agencies. The UN does not consider refugee status to be hereditary for any other group, but may still assist relatives of refugees in some cases.
At the end of 2014, there were 19.5 million refugees worldwide (14.4 million under UNHCR's mandate, plus 5.1 million Palestinian refugees under UNRWA's mandate). The 14.4 million refugees under UNHCR's mandate were around 2.7 million more than at the end of 2013 (+23%), the highest level since 1995. Among them, Syrian refugees became the largest refugee group in 2014 (3.9 million, 1.55 million more than the previous year), overtaking Afghan refugees (2.6 million), who had been the largest refugee group for three decades. As of March 2016, Turkey has become world's biggest refugee hosting country having 2.7 million Syrian and 300.000 Iraqi refugees and had spent more than US$7.6 billion on direct assistance to refugees. Pakistan is second, hosting 1.6 million Afghan refugees. According to the UNHCR there are 200,000 to 500,000 Rohingya refugees in Bangladesh and only 32,355 of them are registered. The religious, sectarian and denominational affiliation has been an important feature of debate in refugee-hosting nations.
Definition.
The 1951 United Nations Convention Relating to the Status of Refugees has adopted the following term "refugee" to apply to any person who (in Article 1.A.2): "owing to well-founded fear of being persecuted for reasons of race, religion, nationality, membership of a particular social group or political opinion, is outside the country of his nationality and is unable or, owing to such fear, is unwilling to avail himself of the protection of that country; or who, not having a nationality and being outside the country of his former habitual residence as a result of such events, is unable or, owing to such fear, is unwilling to return to it."
This original definition with all its legacies has been criticized as based on three political framings:
The concept of a refugee was expanded by the Convention's 1967 Protocol and by regional conventions in Africa and Latin America to include persons who had fled war or other violence in their home country. European Union's minimum standards definition of refugee, underlined by Art. 2 (c) of Directive No. 2004/83/EC, essentially reproduces the narrow definition of refugee offered by the UN 1951 Convention; nevertheless, by virtue of articles 2 (e) and 15 of the same Directive, persons who have fled a war-caused generalized violence are, at certain conditions, eligible for a complementary form of protection, called subsidiary protection. The same form of protection is foreseen for people who, without being refugees, are nevertheless exposed, if returned to their countries of origin, to death penalty, torture or other inhuman or degrading treatments.
The term refugee is often used to include displaced persons who may fall outside the legal definition in the Convention, either because they have left their home countries because of war and not because of a fear of persecution, or because they have been forced to migrate within their home countries. The Convention Governing the Specific Aspects of Refugee Problems in Africa, adopted by the Organization of African Unity in 1969, accepted the definition of the 1951 Refugee Convention and expanded it to include people who left their countries of origin not only because of persecution but also due to acts of external aggression, occupation, domination by foreign powers or serious disturbances of public order.
Refugees were defined as a legal group in response to the large numbers of people fleeing Eastern Europe following World War II. The lead international agency coordinating refugee protection is the Office of the United Nations High Commissioner for Refugees (UNHCR), which counted 8,400,000 refugees worldwide at the beginning of 2006. This was the lowest number since 1980. The major exception is the 4,600,000 Palestinian refugees under the authority of the United Nations Relief and Works Agency for Palestine Refugees in the Near East (UNRWA). By the end of 2014, the UNHCR estimated the number of refugees to 19.5 million. Research finds that refugees have historically tended to flee to nearby countries with ethnic kin populations and a history of accepting other co-ethnic refugees. The "durable solutions" to refugee populations, as defined by UNHCR and governments, are: voluntary repatriation to the country of origin; local integration into the country of asylum; and resettlement to a third country.
Although similar and frequently confused with refugees, internally displaced persons have a different legal definition and are essentially refugees who have not crossed any international border. At the end of 2012, the Office of the United Nations High Commissioner for Refugees (UNHCR), the United Nations' refugee agency, reported that there were 15.4 million refugees worldwide. By contrast there were 38.2 million (about twice as many) IDPs at the end of 2014.
Law.
Refugee law encompasses both customary law, peremptory norms, and international legal instruments. These include:
History.
The idea that a person who sought sanctuary in a holy place could not be harmed without inviting divine retribution was familiar to the ancient Greeks and ancient Egyptians. However, the right to seek asylum in a church or other holy place was first codified in law by King Æthelberht of Kent in about AD 600. Similar laws were implemented throughout Europe in the Middle Ages. The related concept of political exile also has a long history: Ovid was sent to Tomis; Voltaire was sent to England. By the 1648 Peace of Westphalia, nations recognized each other's sovereignty. However, it was not until the advent of romantic nationalism in late 18th-century Europe that nationalism gained sufficient prevalence for the phrase "country of nationality" to become practically meaningful, and for people crossing borders to be required to provide identification.
The term "refugee" is sometimes applied to people who might fit the definition outlined by the 1951 Convention, were it to be applied retroactively. There are many candidates. For example, after the Edict of Fontainebleau in 1685 outlawed Protestantism in France, hundreds of thousands of Huguenots fled to England, the Netherlands, Switzerland, South Africa, Germany and Prussia. The repeated waves of pogroms that swept Eastern Europe in the 19th and early 20th centuries prompted mass Jewish emigration (more than 2 million Russian Jews emigrated in the period 1881–1920). Beginning in the 19th century, Muslim people emigrated to Turkey from Europe. The Balkan Wars of 1912–1913 caused 800,000 people to leave their homes. Various groups of people were officially designated refugees beginning in World War I.
League of Nations.
The first international co-ordination of refugee affairs came with the creation by the League of Nations in 1921 of the High Commission for Refugees and the appointment of Fridtjof Nansen as its head. Nansen and the Commission were charged with assisting the approximately 1,500,000 people who fled the Russian Revolution of 1917 and the subsequent civil war (1917–1921), most of them aristocrats fleeing the Communist government. It is estimated that about 800,000 Russian refugees became stateless when Lenin revoked citizenship for all Russian expatriates in 1921.
In 1923, the mandate of the Commission was expanded to include the more than one million Armenians who left Turkish Asia Minor in 1915 and 1923 due to a series of events now known as the Armenian Genocide. Over the next several years, the mandate was expanded further to cover Assyrians and Turkish refugees. In all of these cases, a refugee was defined as a person in a group for which the League of Nations had approved a mandate, as opposed to a person to whom a general definition applied.
The 1923 population exchange between Greece and Turkey involved about two million people (around 1.5 million Anatolian Greeks and 500,000 Muslims in Greece) most of whom were forcibly repatriated and denaturalized from homelands of centuries or millennia (and guaranteed the nationality of the destination country) by a treaty promoted and overseen by the international community as part of the Treaty of Lausanne.
The U.S. Congress passed the Emergency Quota Act in 1921, followed by the Immigration Act of 1924. The Immigration Act of 1924 was aimed at further restricting the Southern and Eastern Europeans, especially Jews, Italians and Slavs, who had begun to enter the country in large numbers beginning in the 1890s. Most of the European refugees (principally Jews and Slavs) fleeing Stalin, the Nazis and World War II were barred from going to the United States.
In 1930, the Nansen International Office for Refugees (Nansen Office) was established as a successor agency to the Commission. Its most notable achievement was the Nansen passport, a refugee travel document, for which it was awarded the 1938 Nobel Peace Prize. The Nansen Office was plagued by problems of financing, an increase in refugee numbers, and a lack of co-operation from some member states, which led to mixed success overall.
However, it managed to lead fourteen nations to ratify the 1933 Refugee Convention, an early, and relatively modest, attempt at a human rights charter, and in general assisted around one million refugees worldwide.
1933 (rise of Nazism) to 1944.
The rise of Nazism led to such a very large increase in the number of refugees from Germany that in 1933 the League created a High Commission for Refugees Coming from Germany. Besides other measures by the Nazis which created fear and flight, Jews were stripped of German citizenship by the "Reich Citizenship Law" of 1935. On 4 July 1936 an agreement was signed under League auspices that defined a refugee coming from Germany as "any person who was settled in that country, who does not possess any nationality other than German nationality, and in respect of whom it is established that in law or in fact he or she does not enjoy the protection of the Government of the Reich" (article 1). 
The mandate of the High Commission was subsequently expanded to include persons from Austria and Sudetenland, which Germany annexed after 1 October 1938 in accordance with the Munich Agreement. According to the Institute for Refugee Assistance, the actual count of refugees from Czechoslovakia on 1 March 1939 stood at almost 150,000. Between 1933 and 1939, about 200,000 Jews fleeing Nazism were able to find refuge in France, while at least 55,000 Jews were able to find refuge in Palestine before the British authorities closed that destination in 1939.
On 31 December 1938, both the Nansen Office and High Commission were dissolved and replaced by the Office of the High Commissioner for Refugees under the Protection of the League. This coincided with the flight of several hundred thousand Spanish Republicans to France after their defeat by the Nationalists in 1939 in the Spanish Civil War.
The conflict and political instability during World War II led to massive numbers of refugees (see World War II evacuation and expulsion). In 1943, the Allies created the United Nations Relief and Rehabilitation Administration (UNRRA) to provide aid to areas liberated from Axis powers, including parts of Europe and China. By the end of the War, Europe had more than 40 million refugees. UNRRA was involved in returning over seven million refugees, then commonly referred to as displaced persons or DPs, to their country of origin and setting up displaced persons camps for one million refugees who refused to be repatriated. Even two years after the end of War, some 850,000 people still lived in DP camps across Western Europe. After the establishment of Israel in 1948, Israel accepted more than 650,000 refugees by 1950. By 1953, over 250,000 refugees were still in Europe, most of them old, infirm, crippled, or otherwise disabled.
Post-World War II population transfers.
After the Soviet armed forces captured eastern Poland from the Germans in 1944, the Soviets unilaterally declared a new frontier between the Soviet Union and Poland approximately at the Curzon Line, despite the protestations from the Polish government-in-exile in London and the western Allies at the Teheran Conference and the Yalta Conference of February 1945. After the German surrender on 7 May 1945, the Allies occupied the remainder of Germany, and the Berlin declaration of 5 June 1945 confirmed the division of Allied-occupied Germany according to the Yalta Conference, which stipulated the continued existence of the German Reich as a whole, which would include its eastern territories as of 31 December 1937. This did not impact on Poland's eastern border, and Stalin refused to be removed from these eastern Polish territories.
In the last months of World War II, about five million German civilians from the German provinces of East Prussia, Pomerania and Silesia fled the advance of the Red Army from the east and became refugees in Mecklenburg, Brandenburg and Saxony. Since the spring of 1945 the Poles had been forcefully expelling the remaining German population in these provinces. When the Allies met in Potsdam on 17 July 1945 at the Potsdam Conference, a chaotic refugee situation faced the occupying powers. The Potsdam Agreement, signed on 2 August 1945, defined the Polish western border as that of 1937, (Article VIII) placing one fourth of Germany's territory under the Provisional Polish administration. Article XII ordered that the remaining German populations in Poland, Czechoslovakia and Hungary be transferred west in an "orderly and humane" manner. (See Flight and expulsion of Germans (1944–50).)
Although not approved by Allies at Potsdam, hundreds of thousands of ethnic Germans living in Yugoslavia and Romania were deported to slave labour in the Soviet Union, to Allied-occupied Germany, and subsequently to the German Democratic Republic (East Germany), Austria and the Federal Republic of Germany (West Germany). This entailed the largest population transfer in history. In all 15 million Germans were affected, and more than two million perished during the expulsions of the German population. (See Flight and expulsion of Germans (1944–1950).) Between the end of War and the erection of the Berlin Wall in 1961, more than 563,700 refugees from East Germany traveled to West Germany for asylum from the Soviet occupation.
During the same period, millions of former Russian citizens were forcefully repatriated against their will into the USSR. On 11 February 1945, at the conclusion of the Yalta Conference, the United States and United Kingdom signed a Repatriation Agreement with the USSR. The interpretation of this Agreement resulted in the forcible repatriation of all Soviets regardless of their wishes. When the war ended in May 1945, British and United States civilian authorities ordered their military forces in Europe to deport to the Soviet Union millions of former residents of the USSR, including many persons who had left Russia and established different citizenship decades before. The forced repatriation operations took place from 1945 to 1947.
At the end of World War II, there were more than 5 million "displaced persons" from the Soviet Union in Western Europe. About 3 million had been forced laborers (Ostarbeiters) in Germany and occupied territories. The Soviet POWs and the Vlasov men were put under the jurisdiction of SMERSH (Death to Spies). Of the 5.7 million Soviet prisoners of war captured by the Germans, 3.5 million had died while in German captivity by the end of the war. The survivors on their return to the USSR were treated as traitors (see Order No. 270). Over 1.5 million surviving Red Army soldiers imprisoned by the Nazis were sent to the Gulag.
Poland and Soviet Ukraine conducted population exchanges following the imposition of a new Poland-Soviet border at the Curzon Line in 1944. About 2,100,000 Poles were expelled west of the new border (see Repatriation of Poles), while about 450,000 Ukrainians were expelled to the east of the new border. The population transfer to Soviet Ukraine occurred from September 1944 to May 1946 (see Repatriation of Ukrainians). A further 200,000 Ukrainians left southeast Poland more or less voluntarily between 1944 and 1945.
The International Refugee Organization (IRO) was founded on 20 April 1946, and took over the functions of the United Nations Relief and Rehabilitation Administration, which was shut down in 1947. While the handover was originally planned to take place at the beginning of 1947, it did not occur until July 1947. The International Refugee Organization was a temporary organization of the United Nations (UN), which itself had been founded in 1945, with a mandate to largely finish the UNRRA's work of repatriating or resettling European refugees. It was dissolved in 1952 after resettling about one million refugees. The definition of a refugee at this time was an individual with either a Nansen passport or a "Certificate of identity" issued by the International Refugee Organization.
The Constitution of the International Refugee Organization, adopted by the United Nations General Assembly on 15 December 1946, specified the agency's field of operations. Controversially, this defined "persons of German ethnic origin" who had been expelled, or were to be expelled from their countries of birth into the postwar Germany, as individuals who would "not be the concern of the Organization." This excluded from its purview a group that exceeded in number all the other European displaced persons put together. Also, because of disagreements between the Western allies and the Soviet Union, the IRO only worked in areas controlled by Western armies of occupation.
US position on persecution of religious minorities.
The US government position on refugees states that there is repression of religious minorities in the Middle East and in Pakistan such as Christians, Hindus, as well as Ahmadi, and Zikri denominations of Islam. In Sudan, where Islam is the state religion, Muslims dominate the government and restrict activities of Christians, practitioners of traditional African indigenous religions and other non-Muslims. The question of Jewish, Christian and other refugees from Arab and Muslim countries was introduced in March 2007 in the US Congress.
UNHCR.
Headquartered in Geneva, Switzerland, the Office of the United Nations High Commissioner for Refugees (UNHCR) (established 14 December 1950) protects and supports refugees at the request of a government or the United Nations and assists in their return or resettlement. All refugees in the world are under the UNHCR mandate except Palestinian refugees who fled the current state of Israel between 1947 and 1949, as a result of the 1948 Palestine War, and their descendants, who are assisted by the United Nations Relief and Works Agency (UNRWA). However, Palestinian Arabs who fled the West Bank and Gaza after 1949 (for example, during the 1967 Six Day war) are under the jurisdiction of the UNHCR.
UNHCR provides protection and assistance not only to refugees, but also to other categories of displaced or needy people. These include asylum seekers, refugees who have returned home but still need help in rebuilding their lives, local civilian communities directly affected by the movements of refugees, stateless people and so-called internally displaced people (IDPs). IDPs are civilians who have been forced to flee their homes, but who have not reached a neighboring country and therefore, unlike refugees, are not protected by international law and may find it hard to receive any form of assistance. As the nature of war has changed in the last few decades, with more and more internal conflicts replacing interstate wars, the number of IDPs has increased significantly to an estimated 5 million people worldwide. According to Bogumil Terminski the stabilization of refugee problem worldwide is the main cause of the development of the studies on internal displacement.
The agency is mandated to lead and co-ordinate international action to protect refugees and resolve refugee problems worldwide. Its primary purpose is to safeguard the rights and well-being of refugees. It strives to ensure that everyone can exercise the right to seek asylum and find safe refuge in another State, with the option to return home voluntarily, integrate locally or to resettle in a third country.
UNHCR's mandate has gradually been expanded to include protecting and providing humanitarian assistance to what it describes as other persons "of concern", including internally displaced persons (IDPs) who would fit the legal definition of a refugee under the 1951 Refugee Convention and 1967 Protocol, the 1969 Organization for African Unity Convention, or some other treaty if they left their country, but who presently remain in their country of origin. UNHCR thus has missions in Colombia, Democratic Republic of the Congo, Serbia and Montenegro and Ivory Coast to assist and provide services to IDPs.
Asia – 8,603,600
Africa – 5,169,300
Europe – 3,666,700
Latin America and Caribbean – 2,513,000
North America – 716,800
Oceania – 82,500.
International attitude.
World Refugee Day.
World Refugee Day occurs on 20 June. The day was created in 2000 by a special United Nations General Assembly Resolution. 20 June had previously been commemorated as African Refugee Day in a number of African countries. 
In the United Kingdom World Refugee Day is celebrated as part of Refugee week. Refugee Week is a nationwide festival designed to promote understanding and to celebrate the cultural contributions of refugees, and features many events such as music, dance and theatre.
In the Roman Catholic Church, the World Day of Migrants and Refugees is celebrated in January each year. It was instituted in 1914 by Pope Pius X.
Asylum seeker.
Signatories to the Refugee convention recognize persecution "on account of race, religion, nationality, political opinion, or membership in a particular social group" as grounds for seeking asylum. Until a request for refuge has been accepted, the person is referred to as an "asylum seeker". Only after the recognition of the asylum seeker's protection needs is he or she is officially referred to as a refugee and enjoys refugee status. This carries certain rights and obligations according to the legislation of the receiving country.
The practical determination of whether a person is a refugee or not is most often left to certain government agencies within the host country. This can lead to a situation where the country will neither recognize the refugee status of the asylum seekers nor see them as legitimate migrants and treat them as illegal aliens.
Signatories to the refugee convention create their own policies for assessing the protection status of asylum seekers, and the proportion of asylum applicants who are rejected varies from country to country and year to year. Failed asylum applicants are most often placed in immigration detention before they can be deported. The burden of substantiating an asylum claim lies with the claimant, who must establish that they qualify for protection. In many countries, asylum applicants can challenge a rejection by challenging the decision in a court or migration review panel. In the United Kingdom, more than one in four decisions to refuse an asylum seeker protection are overturned by immigration judges.
A claim for asylum may also be made onshore, usually after making an unauthorized arrival. Some governments are tolerant and accepting of onshore asylum claims; other governments arrest or detain those who attempt to seek asylum; sometimes while processing their claims.
Non-governmental organizations concerned with refugees and asylum seekers have pointed out difficulties for displaced persons to seek asylum in industrialized countries. As their immigration policy often focuses on the fight of irregular migration and the strengthening of border controls it deters displaced persons from entering territory in which they could lodge an asylum claim. The lack of opportunities to legally access the asylum procedures can force asylum seekers to undertake often expensive and hazardous attempts at illegal entry.
In many countries, Country of Origin Information is used by migration officials as part of the assessment of asylum claims, and governments commission research into the accuracy of their country reports. Some countries have studied the rejection rates of their migration officials making decisions, finding that individuals reject more applicants than others assessing similar cases - and migration officials are required to standardise the reasons for accepting or rejecting claims, so that the decision of one adjudicator is consistent with what their colleagues decide.
Statistics.
An interactive graphic of the size of the world's refugee populations in a country, or from a country, based on UNHCR data from years 1975-2000 is available for use.
Reasons for refugee crises.
Environment and climate.
Although they do not fit the definition of refugees set out in the UN Convention, people displaced by the effects of climate change have often been termed "climate refugees" or "climate change refugees". The term 'environmental refugee' is also commonly used and an estimated 25 million people can currently be classified as such. The alarming predictions by the UN, charities and some environmentalists, that between 200 million and 1 billion people could flood across international borders to escape the impacts of climate change in the next 40 years are realistic. Case studies from Bolivia, Senegal and Tanzania, three countries extremely prone to climate change, show that people affected by environmental degradation rarely move across borders. Instead, they adapt to new circumstances by moving short distances for short periods, often to cities. Millions of people live in places that are vulnerable to the effects of climate change. They face extreme weather conditions such as droughts or floods. Their lives and livelihoods might be threatened in new ways and create new vulnerabilities. Migration is in many developing countries a coping strategy to mitigate poverty and is already happening independent of the effects of climate change and environmental degradation. It is a selective process and the poorest and most vulnerable people are often excluded as they will find it almost impossible to move due to a lack of necessary funds or social support.
Economic migrants.
Not all migrants seeking shelter in another country fall under the definition of "refugee" according to article 1A of the Geneva Convention. In 1951, when the text of the Convention was discussed, the parties of the treaty had the idea that slavery was a thing from the past: therefore escaped and fleeing slaves are a group not mentioned in the definition, as well as a category that later emerged: the climate refugee ("environmental migrant") (see below).
In 2008-2009, the humanitarian nature of the mass movement of Zimbabweans to neighbouring Southern African blurred the distinction between what is a "refugee" and an "economic migrant". Such people fit neither category perfectly and have more general needs, rights and responsibilities, that fall outside the specific mandate of the UNHCR. They fall between the cracks, according to the report "Zimbabwean Migration into Southern Africa: New Trends and Responses", released in November 2009 by the Forced Migration Studies Programme (FMSP) at the University of the Witwatersrand, South Africa. According to the researchers, a lack of protection of migrants in the region was based on a ""false distinction"" between a forced and an economic migrant, instead of focusing on the real and urgent needs some of these migrants have. The report suggested that a better term would be "forced humanitarian migrants", who moved for the purpose of their and their dependents' basic survival.
To emphasize the importance of a common humanitarian position on the outflow of Zimbabweans into the region the Regional Office for Southern Africa of the UN Office for the Coordination of Humanitarian Affairs coined the term "migrants of humanitarian concern" in 2008.
Official responses to Zimbabwean migration in Botswana, Malawi, Zambia and Mozambique are still premised on the original definition from the 1951 Convention, and so were said to be failing to protect both Zimbabweans and their own citizens". Those crossing the border were neither refugees – most did not even apply for refugee status – and, given the extent of economic collapse at home, nor they could hardly be considered as "voluntary" economic migrants. So many of them were not legally protected, nor do they receive humanitarian support, as they fell outside the mandates of the support structures offered by government and non-government institutions. In Botswana, Zambia and Malawi, asylum is available to Zimbabweans; in Mozambique, the few applicants for asylum had been rejected due to the state's decision to consider Zimbabweans as 'economic' and not forced humanitarian migrants.
Except for South Africa, protection and access to services in most countries in the region is contingent on receiving the refugee status, and require asylum seekers to stay in isolated camps, unable to work or travel, and thus send money to relatives that stayed behind in Zimbabwe. South Africa was considering the introduction of a special permit for Zimbabweans, but the policy was still under review.
Migratory routes and methods of fleeing.
Boat people.
The term "boat people" came into common use in the 1970s with the mass exodus of Vietnamese refugees following the Vietnam War. It is a widely used form of migration for people migrating from Cuba, Haiti, Morocco, Vietnam or Albania. They often risk their lives on dangerously crude and overcrowded boats to escape oppression or poverty in their home nations. Events resulting from the Vietnam War led many people in Cambodia, Laos, and especially Vietnam to become refugees in the late 1970s and 1980s. In 2001, 353 asylum seekers sailing from Indonesia to Australia drowned when their vessel sank.
The main danger to a boat person is that the boat he or she is sailing in may actually be anything that floats and is large enough for passengers. Although such makeshift craft can result in tragedy, in 2003 a small group of 5 Cuban refugees attempted (unsuccessfully, but un-harmed) to reach Florida in a 1950s pickup truck made buoyant by oil barrels strapped to its sides.
Boat people are frequently a source of controversy in the nation they seek to immigrate to, such as the United States, New Zealand, Germany, France, Russia, Canada, Italy, Japan, South Korea, Spain and Australia. Boat people are often forcibly prevented from landing at their destination, such as under Australia's Pacific Solution (which operated from 2001 until 2008), or they are subjected to mandatory detention after their arrival.
Balkan routes.
Since 2015 more than 700.000 refugees and other migrants used these routes (i.e. the Eastern Balkan route and the Western Balkan route) from Greece through the Balkan to enter central european countries. Since March 2016 this route is almost closed.
Temporary protection and maintenance.
Refugee camps.
A refugee camp is a place built by governments or NGOs (such as the International Committee of the Red Cross) to receive refugees. People may stay in these camps, receiving emergency food, education and medical aid. If it becomes safer they can make use of voluntary repatriation programmes and return home. In some cases, often after several years, other countries decide it will never be safe to return these people, and they may be resettled in "third countries" However, more often than not, refugees are neither resettled nor naturalised. In the meantime, they are at risk for disease, child soldier recruitment, terrorist recruitment, and physical and sexual violence. There are estimated to be 700 refugee camp locations.
Urban refugees.
Not all refugees that are supported by the UNHCR live in refugee camps. A significant number, more than half, live in urban settings, such as the around 60.000 Iraqi refugees in Damascus (Syria), the around 30.000 Sudanese refugees in Cairo (Egypt), or the around 50.000 Somali, Ethiopian and Congolese refugees in Nairobi (Kenya).
Durable solutions.
Rather than only safeguarding the rights and the well-being of refugees in the camps or in urban settings on a temporary basis the UNHCR's ultimate goal is to find one of the three durable solutions for refugees: integration, repatriation, resettlement.
Naturalisation and integration.
In 2014 Tanzania granted citizenship to 162,000 refugees from Burundi and in 1982 to 32,000 Rwandan refugees. Mexico naturalised 6,200 Guatemalan refugees in 2001.
In the context of the Arab-Israeli conflict, the State of Israel has guaranteed asylum and citizenship to Jewish refugees. Many countries, such as Syria and Kenya, rule out the integration of refugees in their country.
Voluntary return.
In the last couple of years parts of or even whole refugee populations were able to return to their home countries: e.g. 120,000 Congolese refugees returned from the Republic of Congo to the DRC, many Somalis are returning from Kenya, 30,000 Angolans returned home from the DRC and Botswana, Ivorian refugees returned from Liberia, Afgans from Pakistan and Iraqis from Syria. The UNHCR and the IOM offer assistance to refugees who want to return voluntarily to their home countries. Many developed countries also have Assisted Voluntary Return (AVR) programmes for asylum seekers who want to go back or were refused asylum.
Third country resettlement.
Resettlement involves the assisted movement of refugees who are unable to return home to safe third countries. The UNHCR has traditionally seen resettlement as the least preferable of the "durable solutions" to refugee situations. However, in April 2000 the then UN High Commissioner for Refugees, Sadako Ogata, stated "Resettlement can no longer be seen as the least-preferred durable solution; in many cases it is the "only" solution for refugees." Politicians in some western countries have shown a preference for Christian refugees over those of other religions.
Resettlement involves a number of difficulties, most of them involving the often extreme cultural transition needed to adapt to life in the country of resettlement. For the many refugees going from rural undeveloped countries to life in urban centers, public transport, education, health care systems, job applications, and even grocery shopping can be difficult to navigate. Language barriers also frequently pose a problem. Even aside from material problems, resettled refugees can struggle with issues of identity and belonging, as societal integration can be very difficult in a completely different culture, and discrimination frequently further inhibits the process.
The UNHCR does recognize benefits to resettlement as well, however. On their website, they bring attention to the fact that refugees have much to bring to the countries in which they are resettled in terms of culture and labor, going as far as to say that "both refugee resettlement and general migration are now recognized as critical factors in the economic success of a number of industrialized countries." According to the UNHCR, resettlement serves three primary functions: securing fundamental human rights such as "life, liberty, safety, health," etc.for refugees who are at risk in camps, providing a long-term solution to the issue of displacement for large numbers of refugees, and alleviating the burden on countries offering asylum to such displaced peoples. Frequently, these countries of asylum are some of the world's poorest nations and cannot handle the large influx of persons that occur when war, persecution, or other events drive refugees across their borders into their country.
Refugee rights.
Right of return.
Even in a supposedly "post-conflict" environment, it is not a simple process for refugees to return home. The UN Pinheiro Principles are guided by the idea that people not only have the right to return home, but also the right to the same property. It seeks to return to the pre-conflict status quo and ensure that no one profits from violence. Yet this is a very complex issue and every situation is different; conflict is a highly transformative force and the pre-war status-quo can never be reestablished completely, even if that were desirable (it may have caused the conflict in the first place). Therefore, the following are of particular importance to the right to return:
Refugees who were resettled to a third country will likely lose the indefinite leave to remain in this country if they return to their country of origin or the country of first asylum.
Right to non-refoulement.
Non-refoulement is the right not to be returned to a place of persecution and is the foundation for international refugee law, as outlined in the 1951 Convention Relating to the Status of Refugees. The right to non-refoulement is distinct from the right to asylum. In order to respect the right to asylum states must not deport genuine refugees. In contrast, the right to non-refoulement allows states to transfer genuine refugees to third party countries with respectable human rights records. The portable procedural model, proposed by political philosopher Andy Lamey, emphasizes the right to non-refoulement by guaranteeing refugees three procedural rights (to a verbal hearing, to legal counsel, and to judicial review of detention decisions) and ensuring those rights in the constitution. This proposal attempts to strike a balance between the interest of national governments and the interests of refugees.
Right to family reunification.
Family reunification, which is also a form of resettlement, is a recognized reason for immigration in many countries because of the presence of one or more family members in a certain country, therefore, enables the rest of the family to immigrate to that country as well.
Right to travel.
The states which are signed the 1951 Convention Relating to the Status of Refugees are obliged to issue travel documents to refugees lawfully resident in their territory.
Modern and contemporary crises.
Movements in Africa.
Since the 1950s, many nations in Africa have suffered civil wars and ethnic strife, thus generating a massive number of refugees of many different nationalities and ethnic groups. The number of refugees in Africa increased from 860,000 in 1968 to 6,775,000 by 1992. By the end of 2004, that number had dropped to 2,748,400 refugees, according to the United Nations High Commission for Refugees. (That figure does not include internally displaced persons, who do not cross international borders and so do not fit the official definition of refugee.)
Many refugees in Africa cross into neighboring countries to find haven; often, African countries are simultaneously countries of origin for refugees and countries of asylum for other refugees. The Democratic Republic of Congo, for instance, was the country of origin for 462,203 refugees at the end of 2004, but a country of asylum for 199,323 other refugees.
Countries in Africa from where 5,000 or more refugees originated as of the end of 2004 are listed below. The largest number of refugees are from Sudan and have fled either the longstanding and recently concluded Sudanese Civil War or the War in Darfur and are located mainly in Chad, Uganda, Ethiopia, and Kenya.
Angola.
Decolonisation during the 1960s and 1970s often resulted in the mass exodus of European-descended settlers out of Africa – especially from North Africa (1.6 million European "pieds noirs"), Congo, Mozambique and Angola. By the mid-1970s, the Portugal's African territories were lost, and nearly one million Portuguese or persons of Portuguese descent left those territories (mostly Portuguese Angola and Mozambique) as destitute refugees – the "retornados".
The Angolan Civil War (1975–2002), one of the largest and deadliest Cold War conflicts, erupted shortly after and spread out across the newly independent country. At least one million people were killed, four million were displaced internally and another half million fled as refugees.
Uganda.
In the 1970s Uganda and other East African nations implemented racist policies that targeted the Asian population of the region. Uganda under Idi Amin's leadership was particularly most virulent in its anti-Asian policies, eventually resulting in the expulsion and ethnic cleansing of Uganda's Asian minority. Uganda's 80,000 Asians were mostly Indians born in the country. India had refused to accept them. Most of the expelled Indians eventually settled in the United Kingdom, Canada and in the United States.
The Lord's Resistance Army insurgency forced many civilians to live in internally displaced person camps.
Great Lakes crisis.
In the aftermath of the 1994 Rwandan Genocide, over two million people fled into neighboring countries, in particular Zaire. The refugee camps were soon controlled by the former government and Hutu militants who used the camps as bases to launch attacks against the new government in Rwanda. Little action was taken to resolve the situation and the crisis did not end until Rwanda-supported rebels forced the refugees back across the border at the beginning of the First Congo War.
Darfur.
An estimated 2.5 million people, roughly one-third the population of the Darfur area, have been forced to flee their homes after attacks by Janjaweed Arab militia backed by Sudanese troops during the ongoing war in Darfur in western Sudan since roughly 2003.
Nigeria.
Following Boko Haram's violence thousands of Nigerian's fled to Niger and Cameroon.
Central African Republic.
African refugees in Israel.
Since 2003, an estimated 70,000 immigrants arrived illegally from various African countries into Israel. Some 600 refugees from the Darfur region of Sudan have been granted temporary resident status that is to be renewed every year, although not official refugee status. Another 2,000 refugees from the conflict between Eritrea and Ethiopia have been granted temporary resident status on humanitarian grounds. Israel prefers not to recognize them as refugees so as not to offend Eritrea and Ethiopia. The Sudanese, who are from an enemy state, are also not recognized as refugees. In effect, Israeli politicians, including the current prime minister Benjamin Netanyahu, have referred to the refugees as a threat to Israel's "Jewish character". African refugees are sometimes subject to racism and racial riots, as well as physical assaults. These assaults have been occurring in Israel, especially in southern Tel Aviv since mid-2012.
Over the past years, conflicts have occurred between Israelis and African immigrants in southern Tel-Aviv, mostly due to poverty issues on both sides. Locals accuse African immigrants of rape, Stealing and assault, making racial issues emerge in the southern part of Tel-Aviv, which became an immigrant-populated area.
In 2012, Reuters reported that Israel may jail "illegal immigrants" for up to three years under a law put into effect to stem the flow of Africans across the desert border with Egypt. Netanyahu said in effect that, "If we don't stop their entry, the problem that currently stands at 60,000 could grow to 600,000, and that threatens our existence as a uniquely Jewish and democratic state."
Sudanese refugees.
There are tens of thousands of Sudanese refugees in Egypt, most of them seeking refuge from ongoing military conflicts in their home country of Sudan. Their official status as refugees is highly disputed, and they have been subject to racial discrimination and police violence. They live among a much larger population of Sudanese migrants in Egypt, more than two million people of Sudanese nationality (by most estimates; a full range is 750,000 to 4 million (FMRS 2006:5) who live in Egypt. The U.S. Committee for Refugees and Immigrants believes many more of these migrants are in fact refugees, but see little benefit in seeking recognition.
Western Sahara conflict.
It is estimated that between 165,000 – 200,000 Sahrawis – people from the disputed territory of Western Sahara – have lived in five large refugee camps near Tindouf in the Algerian part of the Sahara Desert since 1975. The UNHCR and WFP are presently engaged in supporting what they describe as the "90,000 most vulnerable" refugees, giving no estimate for total refugee numbers.
Libyan Civil War.
Refugees of the 2011 Libyan civil war are the people, predominantly of Libyan nationality, who fled or were expelled from their homes during the 2011 Libyan civil war, from within the borders of Libya to the neighbouring states of Tunisia, Egypt and Chad, as well as to European countries, across the Mediterranean, as Boat people. The majority of Libyan refugees are Arabs and Berbers, though many of other ethnicities, temporarily living in Libya, originated from sub-Saharan Africa, were also among the first refugee waves to exit the country. The total Libyan refugee numbers are estimated at near one million as of June 2011. About half of them had returned to Libyan territory during summer 2011, though large refugee camps on Tunisian and Chad border kept being overpopulated.
Movements in the Americas.
Latin Americans.
More than one million Salvadorans were displaced during the Salvadoran Civil War from 1975 to 1982. About half went to the United States, most settling in the Los Angeles area. There was also a large exodus of Guatemalans during the 1980s, trying to escape from the civil war there as well. These people went to Southern Mexico and the U.S.
From 1991 through 1994, following the military coup d'état against President Jean-Bertrand Aristide, thousands of Haitians fled violence and repression by boat. Although most were repatriated to Haiti by the U.S. government, others entered the United States as refugees. Haitians were primarily regarded as economic migrants from the grinding poverty of Haiti, the poorest nation in the Western Hemisphere.
The victory of the forces led by Fidel Castro in the Cuban Revolution led to a large exodus of Cubans between 1959 and 1980. Thousands of Cubans yearly continue to risk the waters of the Straits of Florida seeking better economic and political conditions in the U.S. In 1999 the highly publicized case of six-year-old Elián González brought the covert migration to international attention. Measures by both governments have attempted to address the issue. The U.S. government instituted a wet feet, dry feet policy allowing refuge to those travelers who manage to complete their journey, and the Cuban government has periodically allowed for mass migration by organizing leaving posts. The most famous of these agreed migrations was the Mariel boatlift of 1980.
Colombia has one of the world's largest populations of internally displaced persons (IDPs), with estimates ranging from 2.6 to 4.3 million people, due to the ongoing Colombian armed conflict. The larger figure is cumulative since 1985. It is now estimated by the U.S. Committee for Refugees and Immigrants that there are about 150,000 Colombians in "refugee-like situations" in the United States, not recognized as refugees or subject to any formal protection.
United States.
During the Vietnam War, many U.S. citizens who were conscientious objectors and wished to avoid the draft sought political asylum in Canada. President Jimmy Carter issued an amnesty. Since 1975, the U.S. has resettled approximately 2.6 million refugees, with nearly 77% being either Indochinese or citizens of the former Soviet Union. Since the enactment of the Refugee Act of 1980, annual admissions figures have ranged from a high of 207,116 in 1980 to a low of 27,100 in 2002.
Currently, nine national voluntary agencies resettle refugees nationwide on behalf of the U.S. government: Church World Service, Ethiopian Community Development Council, Episcopal Migration Ministries, Hebrew Immigrant Aid Society, International Rescue Committee, U.S. Committee for Refugees and Immigrants, Lutheran Immigration and Refugee Service, United States Conference of Catholic Bishops, and World Relief.
Jesuit Refugee Service/USA (JRS/USA) has worked to help resettle Bhutanese refugees in the United States. The mission of JRS/USA is to accompany, serve and defend the rights of refugees and other forcibly displaced persons. JRS/USA is one of 10 geographic regions of Jesuit Refugee Service, an international Catholic organization sponsored by the Society of Jesus. In coordination with JRS's International Office in Rome, JRS/USA provides advocacy, financial and human resources for JRS regions throughout the world.
The U.S. Office of Refugee Resettlement (ORR) funds a number of organizations that provide technical assistance to voluntary agencies and local refugee resettlement organizations. RefugeeWorks, headquartered in Baltimore, Maryland, is ORR's training and technical assistance arm for employment and self-sufficiency activities, for example. This nonprofit organization assists refugee service providers in their efforts to help refugees achieve self-sufficiency. RefugeeWorks publishes white papers, newsletters and reports on refugee employment topics.
In 2005, as a result of hurricane Katrina, New Orleans citizens were referred to be the media as "refugees". Many New Orleanians consider the term refugee to be an insult. Resident Joseph Melancon explains, "And they had the nerve to call us refugees! When I heard they called us refugees, I couldn’t do nothing but drop my head cause I said I’m a United States citizen!" Actor Wendell Pierce says, "Damn, when the storm came it blew away our citizenship too?" Such narratives regarding the loss of citizenship are used to illustrate the trauma endured and the degradation citizens inflicted during the storm and they are also to show the federal government failing to uphold some contractual responsibility. In Sanctuary: African Americans and Empire, Waligora Davis remarks that, "The problem of the refugee, the stateless, the semi-colonial that DuBois names the black American, is a problem of the refugees relationship to the law and the state. Collectively, such persons signify a community outside the precincts of laws, they remain marginalized as a result of their loss of withheld citizenship." 
Yet, these narratives and Waligora-Davis’ definition does not fully engage the ways in which ‘refugee’ can be deployed as a diasporic trope for empowerment, similar to how The Fugees utilize the term in the diaspora. Here, I would then like to incorporate Alexander Weyheliye's "Sounding Diasporic Citizenship" to consider the possibility for the term "refugee" to be a liberating call to build community and redress trauma throughout the New Orleans diaspora using the example of the hip hop group, the Fugees.
Movements in Asia.
Afghanistan.
From the Soviet invasion of Afghanistan in 1979 until the late 2001 US-led invasion, about six million Afghan refugees have fled to neighboring Pakistan (mainly NWFP) and Iran, making Afghanistan the largest refugee-producing country. Since early 2002, more than 5 million Afghan refugees have repatriated through the UNHCR from both Pakistan and Iran back to their native country, Afghanistan. Approximately 3.5 million from Pakistan while the remaining 1.5 million from Iran. Since 2007 the Iranian government has forcibly deported mostly unregistered (and some registered) Afghan refugees back to Afghanistan, with 362,000 being deported in 2008.
As of March 2009, some 1.7 million registered Afghan refugees still remain in Pakistan. This include the many who were born in Pakistan during the last 30 years but still counted as citizens of Afghanistan. They are allowed to work and study until the end of 2012. 935,600 registered Afghans are living in Iran, which also include the ones born inside Iran.
The 2011 industrialized country asylum data notes a 30% increase in applications from Afghans from 2010 to 2011, primarily towards Germany and Turkey. As of November 2012, there were still 1.8 million Afghans living in Pakistan given both security and economic instability in their home country. However, the country that for decades has hosted Afghan refugees has become the site of extensive military activity that has displaced Pakistanis internally as well as back and forth into Afghanistan. In recent years political momentum has also been building in Pakistan to compel Afghan refugees to repatriate. In July 2012, the Pakistani government announced it would not renew the ID cards of registered Afghan refugees, and as of January 2013, will treat them as illegal immigrants.
Pakistan.
Since the beginning US military intervention against the Taliban in Pakistan over 1.2 million people have been displaced in across the country, joined by a further 555,000 Pakistanis uprooted by fighting since August 2008.
Dissolution of the British Raj, The Partition of 1947 and Independence.
The partition of the British Raj provinces of Panjab and Bengal and the subsequent independence of Pakistan and one day later of India in 1947 resulted in the largest human movement in history. In this population exchange, approximately 7 million Hindus and Sikhs from Bangladesh and Pakistan moved to India while approximately 7 million Muslims from India moved to Pakistan. Approximately one million Muslims, Hindus and Sikhs died during this event.
Bangladeshis in India in 1971.
As a result of the Bangladesh Liberation War, on 27 March 1971, Prime Minister of India, Indira Gandhi, expressed full support of her Government to the Bangladeshi struggle for freedom. The Bangladesh-India border was opened to allow panic-stricken Bangladeshis' safe shelter in India. The governments of West Bengal, Bihar, Assam, Meghalaya and Tripura established refugee camps along the border. Exiled Bangladeshi army officers and the Indian military immediately started using these camps for recruitment and training members of Mukti Bahini. During the Bangladesh War of Independence around 10 million Bangladeshis fled the country to escape the killings and atrocities committed by the Pakistan Army.
Bangladeshi refugees are known as '"Chakmas"' in India.other than chakmas there are Bengali Hindu refugee are also there who remain in India after war.
Pakistani Biharis in Bangladesh after 1971.
During the period of united Pakistan (1947–1971), the Urdu-speaking Biharis did not assimilated themselves into the society of Bangladesh and remained a distinct cultural-linguistic group ever since. Due to being a different linguistic group they were assaulted by Bengalis after the Bangladesh Liberation War the 1971 war because of their active participation along with the Palistani armed forces in committing genocide over the local populace. Some atrocities took place against Biharis but even after 1971 they are still living in Bangladesh while opting to be a repatriated to Pakistan. At the end of the war many Biharis took shelter in refugee camps in different cities, the biggest being the Geneva Camp in Dhaka. It is estimated that about 250,000 Biharis are living in those camps and in Rangpur and Dinajpur districts today.
Rohingyas in Bangladesh and Pakistan from Burma.
Bangladesh hosts more than 250,000 Muslim Rohingya refugees forced from western Burma (Myanmar) who fled in 1991-92 to escape persecution by the Burmese military junta. Many have lived there for close to twenty years. The Bangladeshi government divides the Rohingya into two categories – recognized refugees living in official camps and unrecognized refugees living in unofficial sites or among Bangladeshi communities. Around 30,000 Rohingyas are residing in two camps in Nayapara and Kutupalong area of Cox's Bazar district in Bangladesh. These camp residents have access to basic services, those outside do not. With no changes inside Burma in sight, Bangladesh must come to terms with the long-term needs of all the Rohingya refugees in the country, and allow international organizations to expand services that benefit the Rohingya as well as local communities.
The agency has been supporting Rohingya refugees staying in the camps. On the other hand, it is not receiving applications for refugee status from the newly arrived Rohingyas. This amounts to compromising of its mandate.
The brutal campaign of ethnic cleansing of Muslims in Arakan State by the Burmese military in 1991-92 thousands of people have been detained in crowded refugee camps in Bangladesh and tens of thousands have been repatriated to Burma to face further repression. There are widespread allegations of religious persecution, use of forced labor and denial of citizenship of many Rohingya forced to return to Burma since 1996.
Many have fled again to Bangladesh to seek work or shelter, or flee from Burmese military oppression, and some are forced across the border by Burmese security forces. In the past few months, abuses against Rohingya in Arakan State has continued, including strict registration laws that continue to deny Rohingya citizenship, restrictions on movement, land confiscation and forced evictions to make way for Buddhist Burmese settlements, widespread forced labor in infrastructure projects and closure of some mosques, including nine in North Buthidaung Township of Western Arakan State in the last half of 2006.
An estimated 90,000 people have been displaced in the 2012 sectarian violence between Rohingya Muslims and Buddhists in Burma's western Rakhine State.
There are also large number of Muslim Rohingya refugees in Pakistan. Most of them have made perilous journey across Bangladesh and India and have settled in Karachi.
Himalayas.
After the 1959 Tibetan exodus, there are more than 150,000 Tibetans who live in India, many in settlements in Dharamsala and Mysore, and Nepal. These include people who have escaped over the Himalayas from Tibet, as well as their children and grandchildren. In India the overwhelming majority of Tibetans born in India are still stateless and carry a document called an Identity Card issued by the Indian government in lieu of a passport. This document states the nationality of the holder as Tibetan. It is a document that is frequently rejected as a valid travel document by many customs and immigrations departments. The Tibetan refugees also own a Green Book issued by the Tibetan Government in Exile for rights and duties towards this administration.
In 1991–92, Bhutan expelled roughly 100,000 ethnic Nepalis known as Lhotshampas from the southern part of the country. Most of them have been living in seven refugee camps run by UNHCR in eastern Nepal ever since; some of them resettled in India. In March 2008, this population began a multiyear resettlement to third countries including the United States, New Zealand, Denmark, Canada, Norway and Australia. At present, the United States is working towards resettling more than 60,000 of these refugees in the US as a third country settlement programme.
Meanwhile, as many as 200,000 Nepalese were displaced during the Maoist insurgency and Nepalese Civil War which ended in 2006.
By 2009, more than 3 million civilians had been displaced by the War in North-West Pakistan (2004–present).
Sri Lanka.
The civil war in Sri Lanka, from 1983 to 2009 had generated thousands of internally displaced people as well as refugees most of them being the Tamils. Many Sri Lankans have fled to neighbourly India and western countries such as Canada, France, Denmark, the United Kingdom, and Germany.
While successive policies of discrimination and intimidation of the Tamils drove thousands to flee seeking asylum, the brutal end to the Civil War and the ongoing repression have forced a wave of thousands of refugees migrate, to countries like Canada, the UK and especially Australia. Australia in particular, receives hundreds of refugees every month.
About 69,000 Sri Lankan Tamil refugees live in 112 camps in the southern Indian state of Tamil Nadu.
Jammu and Kashmir.
According to the National Human Rights Commission (NHRC), about 300,000 Hindu Kashmiri Pandits have been forced to leave the state of Jammu and Kashmir due to Islamic militancy and religious discrimination from the Muslim majority, making them refugees in their own country. Some have found refuge in Jammu and its adjoining areas, while others in camps in Delhi and others in other states of India and other countries too. Kashmiri groups peg the number of migrants closer to 500,000.
Tajikistan civil war.
Since 1991, much of the country's non-Muslim population, including non-ethnic Tajikistan's Russians and Bukharian Jews, have fled Tajikistan due to severe poverty, instability and Tajikistan Civil War (1992–1997). In 1992, most of the country's Jewish population was evacuated to Israel. Most of the ethnic Russian population fled to Russia. By the end of the civil war Tajikistan was in a state of complete devastation. Around 1.2 million people were refugees inside and outside of the country. Due to severe poverty a lot of Tajiks had to migrate to Russia.47% of Tajikistan's GDP comes from immigrant remittances (from Tajiks working in Russia).
Uzbekistan.
In 1989, after bloody pogroms against the Meskhetian Turks in Central Asia's Ferghana Valley, nearly 90,000 Meskhetian Turks left Uzbekistan.
The 2010 ethnic violence in Kyrgyzstan left some 300,000 people internally displaced, and around 100,000 sought refuge in Uzbekistan.
Southeast Asia (Vietnam War).
Following the communist takeovers in Vietnam, Cambodia, and Laos in 1975, about three million people attempted to escape in the subsequent decades. With massive influx of refugees daily, the resources of the receiving countries were severely strained. The plight of the boat people became an international humanitarian crisis. The United Nations High Commissioner for Refugees (UNHCR) set up refugee camps in neighboring countries to process the boat people. The budget of the UNHCR increased from $80 million in 1975 to $500 million in 1980. Partly for its work in Indochina, the UNHCR was awarded the 1981 Nobel Peace Prize.
Movements in Europe.
World War II refugee issues.
Jewish refugees.
Between the first and second world wars, hundreds of thousands of European Jews, mainly from Germany and Austria attempted to flee 
the German government's anti-semitic policies which culminated in the Holocaust and the mass murder of millions of European Jews. These Jews were often found it difficult or impossible to immigrate to other European countries. The 1938 Evian Conference, the 1943 Bermuda Conference and other attempts failed to resolve the problem of Jewish refugees, a fact widely used in Nazi propaganda (see also MS "St. Louis").
Since its founding at the beginning of the 1900s Jewish immigration to the British Mandate for Palestine was encouraged by the nascent Zionist movement, but immigration was restricted by the British government, under the pressure from Palestinian Arabs. Following its formation in 1948, according to 1947 UN Partition Plan, Israel adopted the Law of Return, granting Israeli citizenship to any Jewish immigrant. Mass rioting and attacks on Jews throughout the Muslim World following the creation of the state of Israel led to the Jewish exodus from Arab and Muslim countries, in which 850,000 Jews from Arab and Muslim countries fled to Israel between 1948 and the early 1970s.
European Union.
According to the European Council on Refugees and Exiles, a network of European refugee-assisting non-governmental organizations (NGOs), huge differences exist between national asylum systems in Europe, making the asylum system a 'lottery' for refugees. For example, Iraqis who flee their home country and end up in Germany have an 85% chance of being recognised as a refugee and those who apply for asylum in Slovenia do not get a protection status at all.
United Kingdom.
In the United Kingdom the Asylum Support Partnership was created to enable all the agencies working to support and assist Asluym Seekers in making Asylum claims was established in 2012 and is part funded by the home office.
France.
In 2010, President Nicolas Sarkozy began the systematic dismantling of illegal Romani camps and squats in France, deporting thousands of Roma residing in France illegally to Romania, Bulgaria or elsewhere.
Hungary.
In 1956–57 following the Hungarian Revolution of 1956 nearly 200,000 persons, about two percent of the population of Hungary, fled as refugees to Austria and West Germany.
Czechoslovakia.
The Warsaw Pact invasion of Czechoslovakia in 1968 was followed by a wave of emigration, unseen before. It stopped shortly after (estimate: 70,000 immediately, 300,000 in total).
Southeastern Europe.
Following the Greek Civil War (1946–1949) hundreds of thousands of Greeks and Ethnic Macedonians were expelled or fled the country. The number of refugees ranged from 35,000 to over 213,000. Over 28,000 children were evacuated by the Partisans to the Eastern Bloc and the Socialist Republic of Macedonia. This left thousands of Greeks and Aegean Macedonians spread across the world.
The forced assimilation campaign of the late 1980s directed against ethnic Turks resulted in the emigration of some 300,000 Bulgarian Turks to Turkey.
Beginning in 1991, political upheavals in Southeastern Europe such as the breakup of Yugoslavia, displaced about 2,700,000 people by mid-1992, of which over 700,000 of them sought asylum in European Union member states. In 1999, about one million Albanians escaped from Serbian persecution.
Today there are still thousands of refugees and internally displaced persons in Southeastern Europe who cannot return to their homes. Most of them are Serbs who cannot return to Kosovo, and who still live in refugee camps in Serbia today. Over 200,000 Serbs and other non-Albanian minorities fled or were expelled from Kosovo after the Kosovo War in 1999.
In 2009, between 7% and 7.5% of Serbia's population were refugees and IDPs. Around 500,000 refugees, mainly from Croatia and Bosnia and Herzegovina, arrived following the Yugoslav wars. The IDPs were primarily from Kosovo. , Serbia had the largest refugee population in Europe.
Chechnya.
From 1992 ongoing conflict has taken place in Chechenya, Caucasus due to independence proclaimed by this republic in 1991 which is not accepted by the Russian Federation or any other state in the world. As a consequence about 2 million people have been displaced and still cannot return to their homes. At the end of the Soviet era, ethnic Russians comprised about 23% of the population (269,000 in 1989). Due to widespread lawlessness and ethnic cleansing under the government of Dzhokhar Dudayev most non-Chechens (and many Chechens as well) fled the country during the 1990s or were killed.
Nagorno Karabakh.
The Nagorno Karabakh conflict has resulted in the displacement of 528,000 Azerbaijanis (this figure does not include new born children of these IDPs) from Armenian occupied territories including Nagorno Karabakh, and 220,000 Azeris and 18,000 Kurds fled from Armenia to Azerbaijan from 1988 to 1989. 280,000 persons—virtually all ethnic Armenians—fled Azerbaijan during the 1988–1993 war over the disputed region of Nagorno-Karabakh. By the time both Azerbaijan and Armenia had finally agreed to a ceasefire in 1994, an estimated 17,000 people had been killed, 50,000 had been injured, and over a million had been displaced.
Georgia.
More than 250,000 people, mostly Georgians but some others too, were the victims of forcible displacement and ethnic-cleansing from Abkhazia during the War in Abkhazia between 1992 and 1993, and afterwards in 1993 and 1998.
As a result of 1991–1992 South Ossetia War, about 100,000 ethnic Ossetians fled South Ossetia and Georgia proper, most across the border into Russian North Ossetia. A further 23,000 ethnic Georgians fled South Ossetia and settled in other parts of Georgia.
The United Nations estimated 100,000 Georgians have been uprooted as a result of the 2008 South Ossetia war; some 30,000 residents of South Ossetia fled into the neighboring Russian province of North Ossetia.
Ukraine.
According to the United Nations (UNHCR's European director Vincent Cochetel), 814,000 Ukrainians have fled to Russia since the beginning of 2014, including those who did not register as asylum seekers, and 260,000 left to other parts of Ukraine. 
However, also quoting UNHCR, Deutsche Welle says 197,000 Ukrainians fled to Russia by 20 August 2014 and not less than 190,000 have fled to other parts of Ukraine, 14,000 to Belarus and 14,000 to Poland. In Russia many were resettled in specially built refugee villages in Siberia. Russia also registered 2 million new citizens of Ukraine in October 2015, who had arrived since January 1 2014.
Movements in the Near and Middle East.
Population exchange between Greece and Turkey.
The 1923 population exchange between Greece and Turkey was stemmed from the "Convention Concerning the Exchange of Greek and Turkish Populations" signed at Lausanne, Switzerland, on 30 January 1923, by the governments of Greece and the Republic of Turkey. It involved approximately 2 million people (around 1.5 million Anatolian Greeks and 500,000 Muslims in Greece), most of whom were forcibly made refugees and "de jure" denaturalized from their homelands.
By the end of 1922, the vast majority of native Asia Minor Greeks had already fled the Greek genocide (1914–1922) and Greece's later defeat in the Greco-Turkish War (1919–1922). According to some calculations, during the autumn of 1922, around 900,000 Greeks arrived in Greece. The population exchange was envisioned by Turkey as a way to formalize, and make permanent, the exodus of Greeks from Turkey, while initiating a new exodus of a smaller number of Muslims from Greece to supply settlers for occupying the newly depopulated regions of Turkey, while Greece saw it as a way to supply its masses of new propertyless Greek refugees from Turkey with lands to settle from the exchanged Muslims of Greece.
This major compulsory population exchange, or agreed mutual expulsion, was based not on language or ethnicity, but upon religious identity, and involved nearly all the Orthodox Christian citizens of Turkey, including its native Turkish-speaking Orthodox citizens, and most of the Muslim citizens of Greece, including its native Greek-speaking Muslim citizens.
Palestinians.
A heavy exodus of the non-Jewish population of Palestine took place in 1948. Though usually described as byproduct of the 1948 Palestine war, the first and largest wave of Palestinian refugees took place in early 1948, shortly after the Deir Yassin massacre—preceding, therefore, said war, with expulsions of Palestinians continuing to happen for some years thereafter. According to files belonging to the Israeli army that came under the attention of Israeli historians such as Benny Morris, the overwhelming majority (about 73%) of Palestinian refugees left as a result of actions undertaken by Zionist militias and Jewish authorities, with a smaller percentage, about 5%, leaving voluntarily. By the end of 1948, there were about 700,000 Palestinian refugees.
Following the departure of refugees, properties, lands, money, and bank accounts belonging to Palestinians were frozen and confiscated. Jewish ownership of the land, which by late 1947 accounted for less than 6% of historic Palestine and less than 10% of the territory the UN allotted to the Jewish state, swelled.
Dispossession and displacement of Palestinians continued in the decades after Israel's independence, and renewal of conflicts between Israel and its neighbors. During the 1967 war, about 400,000 Palestinians, half of whom were 1948 refugees, fled their lands in the West Bank following advances by the Israeli army and settled in Jordan. In the 2000s, Israel blacklisted the refugees from that war to impede them from returning and reclaiming their properties and lands, which have been allocated to Jewish-only settlements and Israeli military bases. Israel has also admitted to revoking the residency rights of 250,000 Palestinians in the occupied territories in the period between 1967 and 1994, the year of the establishment of the Palestinian Authority, after they left temporarily to study and work abroad.
Palestinian refugees and their descendents spread throughout the Arab world; the largest populations are found in neighboring Levantine countries—Syria, Lebanon and Jordan. The populations of the West Bank and Gaza are also composed to a large extent of refugees and their descendents. 
Until 1967, the West Bank and Gaza were officially ruled, respectively, by Jordan and Egypt. Jordan's Hashemite Kingdom was the only Arab government to have granted citizenship to Palestinian refugees.
Palestinian refugees from 1948 and their descendants do not come under the 1951 UN Convention Relating to the Status of Refugees, but under the UN Relief and Works Agency for Palestine Refugees in the Near East, which created its own criteria for refugee classification. The great majority of Palestinian refugees have kept the refugee status for generations, under a special decree of the UN, and legally defined to include descendants of refugees, as well as others who might otherwise be considered internally displaced persons.
As of December 2005, the World Refugee Survey of the U.S. Committee for Refugees and Immigrants estimates the total number of Palestinian refugees and their descendants to be 2,966,100. Palestinian refugees number almost half of Jordan's population, however they have assimilated into Jordanian society, having a full citizenship. In Syria, though not officially becoming citizens, most of the Palestinian refugees were granted resident rights and issued travel documents. Following the Oslo Agreements, attempts were made to integrate the displaced Palestinians and their descendants into the Palestinian community. In addition, Israel granted permissions for family reunions and return of only about 10,000 Fatah members to the West Bank. The refugee situation and the presence of numerous refugee camps continues to be a point of contention in the Israeli-Palestinian conflict.
Jews of Arab and Muslim countries.
Following the Jewish exodus from Arab and Muslim countries, the combined population of Jewish communities of the Middle East (excluding Israel) and North Africa was reduced from about 900,000 in 1948 to less than 8,000 today. The history of the exodus is politicized, given its proposed relevance to a final settlement Israeli-Palestinian peace negotiations. When presenting the history, those who view the Jewish exodus as equivalent to the 1948 Palestinian exodus, such as the Israeli government and NGOs such as JJAC and JIMENA, emphasize "push factors", such as cases of anti-Jewish violence and forced expulsions, and refer to those affected as "refugees". Those who argue that the exodus does not equate to the Palestinian exodus emphasize "pull factors", such as the actions of local Zionist agents aiming to fulfil the One Million Plan, highlight good relations between the Jewish communities and their country's governments, emphasize the impact of other push factors such as the decolonization in the Maghreb and the Suez War and Lavon Affair in Egypt, and argue that many or all of those who left were not refugees.
Israel absorbed approximately 600,000 Jews from Arab and Muslim countries, many of whom were temporarily settled in tent cities called "Ma'abarot". They were eventually absorbed into Israeli society, and the last "Ma'abarah" was dismantled in 1958. By contrast European Jews were quickly settled in Israel. Their descendants, and those of Iranian and Turkish Jews, now number 3.06 million of Israel's 5.4 to 5.8 million Jewish citizens.
In 2007, both the US Senate and House of Representatives passed simple resolutions and to Make clear that the United States Government supports the position that, as an integral part of any comprehensive peace, the issue of refugees and the mass violations of human rights of minorities in Arab and Muslim countries throughout the Middle East, North Africa, and the Persian Gulf must be resolved in a manner that includes (A) consideration of the legitimate rights of all refugees displaced from Arab and Muslim countries throughout the Middle East, North Africa, and the Persian Gulf; and (B) recognition of the losses incurred by Jews, Christians, and other minority groups as a result of the Arab-Israeli conflict.
The resolutions had been written together with lobbyist group JJAC, whose founder Stanley Urman described the resolution in 2009 as "perhaps our most significant accomplishment". The House of Representatives resolution was sponsored by AIPAC-member Jerrold Nadler. Michael Fischbach explain the resolutions as "a tactic to help the Israeli government deflect Palestinian refugee claims in any final Israeli-Palestinian peace deal, claims that include Palestinian refugees’ demand for the "right of return" to their pre-1948 homes in Israel."
Other Israeli academics and leaders state that Oriental Jews did not come to Israel as refugees, pointing out that many decided to migrate despite leading comfortable lives in the Arab world and arrived to Israel under the directive of underground Zionist activists acting on behalf of the Israeli state.
Some Arab countries, like Iraq, did take a number of measures against Jews who left the country, including the confiscation of assets left behind, though the initial act of Jewish departure was, according to Israeli-Iraqi historian Avi Shlaim, undertaken voluntarily.
Internally displaced Syrians from the Golan Heights.
After the 1967 war, when Israel launched pre-emptive attacks on Egypt and Syrian and annexed the Golan Heights. Israel destroyed 139 Syrian villages in the occupied territory of the Golan Heights and 130,000 of its residents fled or were expelled from their lands, which now serve the purpose of settlements and military bases. About 9,000 Syrians, all of whom of the Druze ethno-religious group, were allowed to remain in their lands.
Cyprus crisis of 1974.
It is estimated that 40% of the Greek population of Cyprus, as well as over half of the Turkish Cypriot population, were displaced during the Turkish invasion of Cyprus in 1974. The figures for internally displaced Cypriots varies, the United Peacekeeping force in Cyprus (UNFICYP) estimates 165,000 Greek Cypriots and 45,000 Turkish Cypriots. The UNHCR registers slightly higher figures of 200,000 and 65,000 respectively, being partly based on official Cypriot statistics which register children of displaced families as refugees. The separation of the two communities via the UN patrolled Green Line prohibited the return of all internally displaced people.
Lebanon Civil War crisis.
It is estimated that some 900,000 people, representing one-fifth of the pre-war population, were displaced from their homes during the Lebanese Civil War (1975–90).
Kurdish population displacement due to Turkish conflict.
Between 1984 and 1999, the Turkish Armed Forces and various groups claiming to represent the Kurdish people have engaged in open war, and much of the countryside in the southeast was depopulated, with Kurdish civilians moving to local defensible centers such as Diyarbakır, Van, and Şırnak, as well as to the cities of western Turkey and even to western Europe. The causes of the depopulation included Kurdistan Workers' Party atrocities against Kurdish clans they could not control, the poverty of the southeast, and the Turkish state's military operations. Human Rights Watch has documented many instances where the Turkish military forcibly evacuated villages, destroying houses and equipment to prevent the return of the inhabitants. An estimated 3,000 Kurdish villages in Turkey were virtually wiped from the map, representing the displacement of more than 378,000 people.
Iran-Iraq war.
The Iran–Iraq War from 1980 to 1988, the 1990 Iraqi invasion of Kuwait, the first Gulf War and subsequent conflicts all generated hundreds of thousands if not millions of refugees. Iran also provided asylum for 1,400,000 Iraqi refugees who had been uprooted as a result of the 1991 uprisings in Iraq (1990–91). At least one million Iraqi Kurds were displaced during the Al-Anfal Campaign (1986–1989).
Refugees of the Gulf War.
The Palestinian exodus from Kuwait took place during and after the Gulf War. There were 400,000 Palestinians in Kuwait before the Gulf War. During the Gulf War, more than 200,000 Palestinians fled Kuwait during the Iraqi occupation of Kuwait due to harassment and intimidation by Iraqi security forces, in addition to getting fired from work by Iraqi authority figures in Kuwait. After the Gulf War in 1991, Kuwaiti authorities pressured nearly 200,000 Palestinians to leave Kuwait. The policy which partly led to this exodus was a response to the alignment of PLO leader Yasser Arafat with Saddam Hussein.
Iraq War (2003–today).
The Iraq war has generated millions of refugees and internally displaced persons. As of 2007 more Iraqis have lost their homes and become refugees than the population of any other country. Over 4,700,000 people, more than 16% of the Iraqi population, have become uprooted. Of these, about 2 million have fled Iraq and flooded other countries, and 2.7 million are estimated to be refugees inside Iraq, with nearly 100,000 Iraqis fleeing to Syria and Jordan each month. Only 1% of the total Iraqi displaced population was estimated to be in the Western countries.
Roughly 40% of Iraq's middle class is believed to have fled, the U.N. said. Most are fleeing systematic persecution and have no desire to return. All kinds of people, from university professors to bakers, have been targeted by militias, insurgents and criminals. An estimated 331 school teachers were slain in the first four months of 2006, according to Human Rights Watch, and at least 2,000 Iraqi doctors have been killed and 250 kidnapped since the 2003 U.S. invasion. Iraqi refugees in Syria and Jordan live in impoverished communities with little international attention
to their plight and little legal protection. In Syria alone an estimated 50,000 Iraqi girls and women, many of them widows, are forced into prostitution just to survive.
According to Washington-based Refugees International, out of the 4.2 million refugees fewer than 800 have been allowed into the US since the 2003 invasion. Sweden had accepted 18,000 and Australia had resettled almost 6,000. By 2006 Sweden had granted protection to more Iraqis than all the other EU Member States combined. However, and following repeated unanswered calls to its European partners for greater solidarity, July 2007 saw Sweden introduce a more restrictive policy towards Iraqi asylum seekers, which is expected to reduce the recognition rate in 2008.
As of September 2007 Syria had decided to implement a strict visa regime to limit the number of Iraqis entering the country at up to 5,000 per day, cutting the only accessible escape route for thousands of refugees fleeing the civil war in Iraq. A government decree that took effect on 10 September 2007 bars Iraqi passport holders from entering Syria except for businessmen and academics. Until then, the Syria was the only country that had resisted strict entry regulations for Iraqis.
In June 2014, More than 500,000 people fled Mosul to escape from the advancing Islamic State of Iraq and Syria (ISIS).
Mandaeans and Yazidis.
Furthermore, the small Mandaean and Yazidi communities are at the risk of elimination due to ethnic cleansing by Islamic militants. Entire neighborhoods in Baghdad were ethnically cleansed by Shia and Sunni Militias. Satellite shows ethnic cleansing in Iraq was key factor in "surge" success.
Refugees in Jordan.
Jordan has one of the world's largest immigrant populations with some sources putting the immigrant percentage to being 60%. Iraqi refugees number between 750,000 and 1 million in Jordan with most living in Amman. Jordan also has Armenian, Chechen, Circassian minorities, and about half of its population is said to be of Palestinian refugees and their descendants.
Syrian refugees.
To escape the violence, nearly 4,088,078 Syrian refugees have fled the country to neighboring Jordan, Lebanon, Turkey and Iraq.
Refugee issues.
Protracted displacement.
Displacement is a long lasting reality for most refugees. Two-thirds of all refugees around the world have been displaced for over three years, which is known as being in 'protracted displacement'. 50% of refugees - around 10 million people - have been displaced for over ten years. Research from the Overseas Development Institute has found that aid programmes for refugees need to move from short-term models of assistance (such as food or cash handouts) to more sustainable long-term programmes that help refugees become more self-reliant. This can involve tackling difficult legal and economic environments, by improving social services, job opportunities and laws.
Medical problems.
Apart from physical wounds or starvation, a large percentage of refugees develop symptoms of post-traumatic stress disorder (PTSD) or depression. These long-term mental problems can severely impede the functionality of the person in everyday situations; it makes matters even worse for displaced persons who are confronted with a new environment and challenging situations. They are also at high risk for suicide.
Among other symptoms, post-traumatic stress disorder involves anxiety, over-alertness, sleeplessness, chronic fatigue syndrome, motor difficulties, failing short term memory, amnesia, nightmares and sleep-paralysis. Flashbacks are characteristic to the disorder: the patient experiences the traumatic event, or pieces of it, again and again. Depression is also characteristic for PTSD-patients and may also occur without accompanying PTSD.
PTSD was diagnosed in 34.1% of Palestinian children, most of whom were refugees, males, and working. The participants were 1,000 children aged 12 to 16 years from governmental, private, and United Nations Relief Work Agency UNRWA schools in East Jerusalem and various governorates in the West Bank.
Another study showed that 28.3% of Bosnian refugee women had symptoms of PTSD three or four years after their arrival in Sweden. These women also had significantly higher risks of symptoms of depression, anxiety, and psychological distress than Swedish-born women. For depression the odds ratio was 9.50 among Bosnian women.
A study by the Department of Pediatrics and Emergency Medicine at the Boston University School of Medicine demonstrated that twenty percent of Sudanese refugee minors living in the United States had a diagnosis of post-traumatic stress disorder. They were also more likely to have worse scores on all the Child Health Questionnaire subscales.
Many more studies illustrate the problem. One meta-study was conducted by the psychiatry department of Oxford University at Warneford Hospital in the United Kingdom. Twenty surveys were analyzed, providing results for 6,743 adult refugees from seven countries. In the larger studies, 9% were diagnosed with post-traumatic stress disorder and 5% with major depression, with evidence of much psychiatric co-morbidity. Five surveys of 260 refugee children from three countries yielded a prevalence of 11% for post-traumatic stress disorder. According to this study, refugees resettled in Western countries could be about ten times more likely to have PTSD than age-matched general populations in those countries. Worldwide, tens of thousands of refugees and former refugees resettled in Western countries probably have post-traumatic stress disorder.
Exploitation.
Refugee populations consist of people who are terrified and are away from familiar surroundings. There can be instances of exploitation at the hands of enforcement officials, citizens of the host country, and even United Nations peacekeepers. Instances of
human rights violations, child labor, mental and physical trauma/torture, violence-related trauma, and sexual exploitation, especially of children, are not entirely unknown. In many refugee camps in three war-torn West African countries, Sierra Leone, Guinea, and Liberia, young girls were found to be exchanging sex for money, a handful of fruit, or even a bar of soap. Most of these girls were between 13 and 18 years of age. In most cases, if the girls had been forced to stay, they would have been forced into marriage. They became pregnant around the age of 15 on average. This happened as recently as in 2001. Parents tended to turn a blind eye because sexual exploitation had become a "mechanism of survival" in these camps.
Security threats.
Very rarely, refugees have been used and recruited as refugee warriors, and the humanitarian aid directed at refugee relief has very rarely been utilized to fund the acquisition of arms. Support from a refugee-receiving state has rarely been used to enable refugees to mobilize militarily, enabling conflict to spread across borders.

</doc>
<doc id="45548" url="https://en.wikipedia.org/wiki?curid=45548" title="Ozieri">
Ozieri

Its cathedral of the Immacolata is the episcopal see of the Roman Catholic Diocese of Ozieri. 
Ozieri is the centre of the earliest known archaeological culture on Sardinia (known as Ozieri Culture).
Transportation.
Ozieri can be reached from Sassari through the SS.597 National road, and by Olbia (SS.199).
The city has a railways station located in the "frazione" of Chilivani (lines to Olbia, Porto Torres and Cagliari).
See also.
<BR>

</doc>
<doc id="45549" url="https://en.wikipedia.org/wiki?curid=45549" title="West Timor">
West Timor

West Timor () is the western and Indonesian portion of the island of Timor and part of the province of East Nusa Tenggara ().
During the colonial period it was known as Dutch Timor and was a centre of Dutch loyalists during the Indonesian National Revolution (1945–1949).
From 1949 to 1975 it was known as Indonesian Timor.
History.
European colonization of Timor began in the 16th century. Although the Portuguese claimed the island of Timor in 1520, the Dutch (in the form of the Dutch East India Company) settled West Timor in 1640, forcing the Portuguese out to East Timor. The subsequent collapse of the company meant that in 1799, the area returned to official Dutch rule. Finally, in 1914, the border between East and West Timor was finalized by a treaty between Portugal and the Netherlands that was originally signed in 1859 and modified in 1893.
West Timor had the status of "residentie" within the Dutch East Indies.
Japan conquered the island during World War II in early 1942. Upon Indonesian independence, West Timor became part of the new Republic of Indonesia.
On 6 September 2000, Pero Simundza from Croatia, Carlos Caceres-Collazio from Puerto Rico and Samson Aregahegn from Ethiopia – all UNHCR staff members – were killed in an attack by 5,000 members of a pro-Indonesian militia, armed with machetes, on the office of UNHCR in the town of Atambua, which is in the vicinity of the border with East Timor and where the main refugee camp was located. (See attacks on humanitarian workers.)
Geography.
West Timor is a political region that comprises the western half of Timor island with the exception of Oecusse district (which is politically part of East Timor) and forms a part of the Indonesian province of Nusa Tenggara Timur (NTT, or East Nusa Tenggara). The land area of West Timor is . The highest point of West Timor is Mount Mutis, at .
Rote Island, the southernmost island of Indonesia, is southwest of West Timor.
West Timor's largest town and chief port is Kupang, the capital of Nusa Tenggara Timur province.
Administration.
West Timor is part of the East Nusa Tenggara province. It was formerly split into the City of Kupang (a "kabupaten" or regency-level administrative area) and four regencies (kabupaten); from west to east these are: Kupang, Timor Tengah Selatan (South Central Timor), Timor Tengah Utara (North Central Timor) and Belu. However, a fifth regency – Malaka – was in 2012 formed from the southern half of Belu Regency. Note the administrative area has shrunk as Rote Ndao Regency (Rote and Ndoa islands to the southwest) and Sabu Raijua Regency (the Savu Islands further west) were split off in 2002 and 2009 respectively from Kupang Regency. The island accounts for 35.5% of the provincial population.
Population.
There were approximately 1.8 million inhabitants in 2008, some of them refugees who had fled the 1999 violence in East Timor.
Languages.
In addition to the national language, Indonesian, native languages belonging to the Fabronic Stock of the Austronesian group of languages are spoken in West Timor, the others in East Timor. These languages include Uab Meto, Tetum, Ndaonese, Rotinese and Helong.
Knowledge of Dutch (the colonial language) is now limited to the older generations.
Religion.
West Timor's main religions are Catholicism (56%), Protestantism (35%) and Islam (8%). From the Catholic missionary Apostolic Vicariate of Dutch Timor stem the Metropolitan Archdiocese of Kupang and its suffragan Diocese of Atambua.
Economy.
West Timor has an average unemployment rate of 2.39%. 30% of the population lived below the poverty line in 1998; as of 2012, it remained at 30%. The economy is mainly agricultural, using slash-and-burn methods to produce corn, rice, coffee, copra and fruit. Some timber harvesting is undertaken, producing eucalyptus, sandalwood, teak, bamboo and rosewood.

</doc>
<doc id="45550" url="https://en.wikipedia.org/wiki?curid=45550" title="Porto Torres">
Porto Torres

Porto Torres (, ) is a "comune" and city in northern Sardinia, in the Province of Sassari.
It is situated on the north-west coast about east of the Gorditanian promontory (Capo del Falcone), and on the spacious bay of the Gulf of Asinara.
Geography.
Porto Torres' territory is situate on north-west part of Sardinian Coast, inside the gulf called "Gulf of Asinara".
The extension of municipality is almost 10200 hectare and is suddivided in two part with almost the same portion of land.
One part is the part where is situated the city, the industry and the romanic ruins, the latter is Asinara island with the smaller Isola Piana. This part of territory is form 1997 a national parc.
The morphology of "city part" is flat, in fact Porto Torres and the rest part of north-west Sardinia is characterized by a Nurra flat, there are some hill formation in the middle of the flat, part of this hill formation is in Porto Torres' territory and the highest elevation of it is Monte Alvaro with it thikness of 342m.
Furthemore the land territory is crossed by two rivers, Rio Mannu and Fiume Santo, the firt draws the edge of Porto Torres territory on west instead the latter is pretty near the city and was used like a fluvial way from the roman age.
History.
In ancient times, Turris Libyssonis was one of the most considerable cities in Sardinia. It was probably of purely Roman origin, founded apparently by Julius Caesar, as it bore the title "Colonia Julia". Pliny described it as a colony, the only on the island in his time, suggesting that there was previously no town on the spot, but merely a fort or "castellum". It is noticed also by Ptolemy and in the Itineraries, but without any indication that it was a place of any importance.
The ancient remains still existing prove that it must have been a considerable town under the Roman Empire. According from the inscriptions on ancient milestones, the principal road through the island ran directly from Caralis (Cagliari) to Turris, a sufficient proof that the latter was a place much frequented. Indeed, two roads, which diverged at Othoca (modern Santa Giusta) connected Caralis to Turris, the more important keeping inland and the other following the west coast. It was also an episcopal see during the early part of the Middle Ages.
The existing port at Porto Torres, which is almost wholly artificial, is based in great part on Roman foundations; and there exist also the remains of a temple (which, as we learn from an inscription, was dedicated to Fortune, and restored in the reign of Philip), of "thermae", of a basilica and an aqueduct, as well as a bridge over the adjoining small river, still called the "Fiume Turritano". The ancient city continued to be inhabited till the 11th century, when the greater part of the population migrated to Sassari, about inland, and situated on a hill. It was partly under Genoese hands before, in the early 15th century, it was conquered by the Aragonese. After the Spanish rule it was part of the Kingdom of Sardinia.
Torres was separated from the comune of Sassari in 1842. At the time the area which had been built around the basilica of San Gavino joined the fishermen's community near the port to form the new "Porto Torres".
Transportation and industry.
The port is connected by ferries with Genoa, Marseille, Toulon, Barcelona, Civitavecchia, Propriano, Expressway SS131/E25 to Sassari and Cagliari, and a national road to Santa Teresa Gallura (SS200).
A railway operated by Trenitalia connects the town with Sassari, and the rest of the island.
Chemical industries support the modern economy of Porto Torres. Fiume Santo, a 1,040 MW power station owned by E.ON, is west from the port, in the municipality of Sassari.
Plans related to industrial conversion are in progress in Porto Torres, where seven research centres are developing the transformation from traditional fossil fuel related industry to an integrated production chain from vegetable oil using oleaginous seeds to bio-plastics.

</doc>
<doc id="45551" url="https://en.wikipedia.org/wiki?curid=45551" title="Alghero">
Alghero

Alghero (; , , ; ; ), is a town and episcopal see of about 44,000 inhabitants in the Italian insular province of Sassari in northwestern Sardinia, next to the Mediterranean Sea.
The name Alghero comes from the medieval Latin "Aleguerium", meaning stagnation of algae ("Posidonia oceanica". The Catalan language is co-official in the city, unique in Italy.
History.
The area of today's Alghero has been settled since pre-historic times. The Ozieri culture was present here in the 4th millennium BC (Necropolis of Anghelu Ruju), while the Nuraghe civilization settled in the area around 1,500 BC.
The Phoenicians had arrived by the 8th century BC and the metalworking town of Sant'Imbenia, with a mixed Phoenician and Nuragic population, engaged in trade with the Etruscans on the Italian mainland.
Due to its strategic position on the Mediterranean Sea, Alghero had been developed into a fortified port town by 1102, built by the Genoese Doria family. The Dorias ruled Alghero for centuries, apart from a brief period under the rule of Pisa between 1283–84. In 1353 it was captured by the forces of the Crown of Aragon under Bernardo de Cabrera; in 1372, following several revolts, the Genoese and indigenous Sardinian population were expelled. Alghero's population later grew because of the arrival of Catalan colonists. In the early 16th century Alghero received papal recognition as a bishopric and the status of King's City ("ciutat de l'Alguer") and developed economically.
The Aragonese were followed by the Spanish Habsburgs, who ruled until 1702, brought some stylish elegance to the town.
In 1720 Alghero, along with the rest of Sardinia, was handed over to the Piedmont-based House of Savoy. In 1821 a famine led to a revolt by the population, which was bloodily suppressed. At the end of the same century Alghero was de-militarised.
During the Fascist era, part of the surrounding marshes were reclaimed and the suburbs of Fertilia and S.M. La Palma were founded. During World War II (1943) Alghero was bombed, and its historical centre suffered heavy damage. The presence of malaria in the countryside was finally overcome in the 1950s.
Since then, Alghero has become a popular tourist resort.
Language.
A dialect of Catalan is spoken in Alghero, introduced when Catalonians settled in the town. Catalan was replaced as the official language of the Island by Spanish in the 17th century, then by Italian. The most recent linguistic research showed that 22% of the population speak Algherese Catalan as a first language and around 90% have some understanding of the language. Currently, there has been a revival of the arts in Algherese Catalan, with singers such as Franca Masu performing original compositions in the language.
Following a rural exodus from the surrounding villages towards the city, much of the population speaks or has some proficiency in Sardinian, in addition to Italian and Catalan. Historically, the spread of Catalan was limited to the city and part of the coast, as the surrounding countryside has always been populated by Sardinian speaking people.
Archeology.
Some 100 remains from the Nuraghe era and a Phoenician necropolis can be seen in the neighbouring area of Sant'Imbenia and Roman remains have been found near the Alghero airport).
Books.
In the 1930s the Swedish writer Amelie Posse Brazdova wrote a book entitled "Sardinia Side Show", where she told the complete story of two years she spent "interned" in Alghero old town during World War I.
External links.
<br>

</doc>
<doc id="45552" url="https://en.wikipedia.org/wiki?curid=45552" title="United Nations Trust Territories">
United Nations Trust Territories

United Nations trust territories were the successors of the remaining League of Nations mandates, and came into being when the League of Nations ceased to exist in 1946. All of the trust territories were administered through the United Nations Trusteeship Council. The one territory not turned over was South-West Africa, which South Africa insisted remained under the League of Nations Mandate. It eventually gained independence in 1990 as Namibia. The main objection was that the trust territory guidelines required that the lands be prepared for independence and majority rule.
The concept is distinct from a territory temporarily and directly governed by the United Nations. 
Trust territories (and administering powers).
Former German Schutzgebiete.
All these territories previously were League of Nations mandates.
Former German and/or Japanese colonies.
These territories were also former League of Nations mandates.

</doc>
<doc id="45553" url="https://en.wikipedia.org/wiki?curid=45553" title="Portuguese Timor">
Portuguese Timor

Portuguese Timor was the name of East Timor when it was under Portuguese control. During most of this period, Portugal shared the island of Timor with the Dutch East Indies.
The first Europeans to arrive in the region were Portuguese in 1515. Dominican friars established a presence on the island in 1556, and the territory was declared a Portuguese colony in 1702. Following a Lisbon-instigated decolonisation process, Indonesia invaded East Timor in 1975. However, the invasion was never accepted by other countries, so Portuguese Timor existed officially until independence in 2002.
Early colonialists.
Prior to the arrival of European colonial powers, the island of Timor was part of the trading networks that stretched between India and China and incorporating Maritime Southeast Asia. The island's large stands of fragrant sandalwood were its main commodity. The first European powers to arrive in the area were the Portuguese in the early sixteenth century followed by the Dutch in the late sixteenth century. Both came in search of the fabled Spice Islands of Maluku. In 1515, Portuguese first landed near modern Pante Macassar. Portuguese merchants exported sandalwood from the island, until the tree nearly became extinct. In 1556 a group of Dominican friars established the village of Lifau.
In 1613, the Dutch take control of the Western part of the island. Over the following three centuries, the Dutch would come to dominate the Indonesian archipelago with the exception of the eastern half of Timor, which would become Portuguese Timor. The Portuguese introduced maize as a food crop and coffee as an export crop. Timorese systems of tax and labour control were preserved, through which taxes were paid through their labour and a portion of the coffee and sandalwood crop. The Portuguese introduced mercenaries into Timor communities and Timor chiefs hired Portuguese soldiers for wars against neighbouring tribes. With the use of the Portuguese musket, Timorese men became deer hunters and suppliers of deer horn and hide for export.
The Portuguese introduced Roman Catholicism to East Timor, the Latin writing system, the printing press, and formal schooling. Two groups of people were introduced to East Timor: Portuguese men, and Topasses. Portuguese language was introduced into church and state business, and Portuguese Asians used Malay in addition to Portuguese. Under colonial policy, Portuguese citizenship was available to men who assimilated Portuguese language, literacy, and religion; by 1970, 1,200 East Timorese, largely drawn from the aristocracy, Dili residents, or larger towns, had obtained Portuguese citizenship. By the end of the colonial administration in 1974, 30 percent of Timorese were practising Roman Catholics while the majority continued to worship spirits of the land and sky.
Establishment of the colonial state.
In 1702, Lisbon sent its first governor successfully, António Coelho Guerreiro, to Lifau, which became capital of all Portuguese dependencies on Lesser Sunda Islands. Former capitals were Solor and Larantuka. Portuguese control over the territory was tenuous particularly in the mountainous interior. Dominican friars, the occasional Dutch raid, and the Timorese themselves competed with Portuguese merchants. The control of colonial administrators was largely restricted to the Dili area, and they had to rely on traditional tribal chieftains for control and influence.
The capital was moved to Dili in 1769, due to attacks from the Topasses, who became rulers of several local kingdoms (Liurai). At the same time, the Dutch were colonising the west of the island and the surrounding archipelago that is now Indonesia. The border between Portuguese Timor and the Dutch East Indies was formally decided in 1859 with the Treaty of Lisbon. In 1913, the Portuguese and Dutch formally agreed to split the island between them. The definitive border was drawn by the Permanent Court of Arbitration in 1916, and it remains the international boundary between the modern states of East Timor and Indonesia.
For the Portuguese, East Timor remained little more than a neglected trading post until the late nineteenth century. Investment in infrastructure, health, and education was minimal. Sandalwood remained the main export crop with coffee exports becoming significant in the mid-nineteenth century. In places where Portuguese rule was asserted, it tended to be brutal and exploitative.
Twentieth century.
At the beginning of the twentieth century, a faltering home economy prompted the Portuguese to extract greater wealth from its colonies, resulting in increased resistance to Portuguese rule in East Timor. In 1910–12, a Timorese rebellion was quashed after Portugal brought in troops from its colonies in Mozambique and Macau, resulting in the deaths of 3,000 East Timorese.
In the 1930s, the Japanese semi-governmental "Nan’yō Kōhatsu" development company, with the secret sponsorship of the Imperial Japanese Navy invested heavily in a joint-venture with the primary plantation company of Portuguese Timor, SAPT. The joint-venture effectively controlled imports and exports into the island by the mid-1930s and the extension of Japanese interests greatly concerned the British, Dutch and Australian authorities.
Although Portugal was neutral during World War II, in December 1941, Portuguese Timor was occupied by a small British, Australian and Dutch force, to preempt a Japanese invasion. However, the Japanese did invade in the Battle of Timor in February 1942. Under Japanese occupation, the borders of the Dutch and Portuguese were overlooked with Timor island being made a single Japanese army administration zone. 400 Australian and Dutch commandos trapped on the island by the Japanese invasion waged a guerrilla campaign, which tied up Japanese troops and inflicted over 1,000 casualties. Timorese and the Portuguese helped the guerillas but following the Allies' eventual evacuation, Japanese retribution from their soldiers and Timorese militia raised in West Timor was severe. By the end of the War, an estimated 40–60,000 Timorese had died, the economy was in ruins, and famine widespread. (see Battle of Timor).
Following World War II, the Portuguese promptly returned to reclaim their colony, while West Timor became part of Indonesia, which secured its independence in 1949. To rebuild the economy, colonial administrators forced local chiefs to supply labourers which further damaged the agricultural sector. The role of the Catholic Church in East Timor grew following the Portuguese government handing over the education of the Timorese to the Church in 1941. In post-war Portuguese Timor, primary and secondary school education levels significantly increased, albeit on a very low base. Although illiteracy in 1973 was estimated at 93 per cent of the population, the small educated elite of East Timorese produced by the Church in the 1960s and 70s, became the independence leaders during the Indonesian occupation.
End of Portuguese rule.
Following a 1974 coup (the "Carnation Revolution"), the new government of Portugal favoured a gradual decolonisation process for Portuguese territories in Asia and Africa. When East Timorese political parties were first legalised in April 1974, three major players emerged. The Timorese Democratic Union (UDT), was dedicated to preserving East Timor as a protectorate of Portugal and in September announced its support for independence. Fretilin endorsed "the universal doctrines of socialism", as well as "the right to independence", and later declared itself "the only legitimate representative of the people". A third party, APODETI emerged advocating East Timor's integration with Indonesia expressing concerns that an independent East Timor would be economically weak and vulnerable.
On 28 November 1975, Fretilin unilaterally declared the territory's independence.
Nine days later, Indonesia invaded the territory declaring it Indonesia's 27th province Timor Timur in 1976. The United Nations, however, did not recognise the annexation. The last governor of Portuguese Timor was Mário Lemos Pires from 1974–75. Following the end of Indonesian occupation in 1999, and a United Nations administered transition period, East Timor became formally independent in 2002.
The first Timorese currency was the Portuguese Timor pataca (introduced 1894), and after 1959 the Portuguese Timor escudo, linked to the Portuguese escudo, was used. In 1975 the currency ceased to exist as East Timor was annexed by Indonesia and began using the Indonesian rupiah.

</doc>
<doc id="45556" url="https://en.wikipedia.org/wiki?curid=45556" title="United States Postmaster General">
United States Postmaster General

The Postmaster General of the United States is the chief executive officer of the United States Postal Service. The office, in one form or another, is older than both the United States Constitution and the United States Declaration of Independence. Benjamin Franklin was appointed by the Continental Congress as the first Postmaster General in 1775, serving just over 15 months.
Until 1971, the postmaster general was the head of the Post Office Department (or simply "Post Office" until the 1820s). From 1829 to 1971, he was a member of the President's Cabinet.
The Cabinet post of Postmaster General was often given to a new President's campaign manager or other key political supporter, and was considered something of a sinecure. The Postmaster General was in charge of the governing party's patronage, and was a powerful position which held much influence within the party.
In 1971, the Post Office Department was re-organized into the United States Postal Service, an independent agency of the executive branch. Thus, the Postmaster General is no longer a member of the Cabinet and is no longer in Presidential succession.
The Postmaster General is the second-highest paid U.S. government official, based on publicly available salary information, after the President of the United States.
Living former Postmasters General.
, there are six living former Postmasters General, the oldest being W. Marvin Watson (1968–1969, born 1924). The most recent Postmaster General to pass away was Preston Robert Tisch (1986–1988), on November 15, 2005.

</doc>
<doc id="45557" url="https://en.wikipedia.org/wiki?curid=45557" title="Sartène">
Sartène

Sartène (; , ), is a commune in the Corse-du-Sud department of France on the island of Corsica.
Its history dates back to medieval times and granite buildings from the early 16th century still line some of the streets. One of the main incidents in the town's history was an attack by pirates from Algiers in 1583, after which 400 people were taken away. These attacks continued into the 18th century.
The town is centred on the Place de la Liberation (previously the Place Porta), at the edge of which is the church of Sainte Marie. The town allows good views across the valley. Sartene wine is appreciated by wine connoisseurs for its good quality.
Sartene has given its name to one of the southern Corsican dialects that are most like the sardinian Gallurese dialect.
Sights.
Genoese towers in the commune of Sartène:
There are numerous archaeological sites in the commune or Sartène:

</doc>
<doc id="45558" url="https://en.wikipedia.org/wiki?curid=45558" title="United States Secretary of Housing and Urban Development">
United States Secretary of Housing and Urban Development

The United States Secretary of Housing and Urban Development is the head of the United States Department of Housing and Urban Development, a member of the President's Cabinet, and Twelfth in the Presidential line of succession. The post was created with the formation of the Department of Housing and Urban Development on September 9, 1965, by President Lyndon B. Johnson's signing of the Department of Housing and Urban Development Act () into law. The Department's mission is "to increase homeownership, support community development and increase access to affordable housing free from discrimination."
Robert C. Weaver became the first African American Cabinet member by being appointed to the position. The department was also the first Cabinet department to be headed by an African American woman, Patricia Roberts Harris, in 1977. Henry Cisneros became the first Hispanic HUD Secretary in 1993.
Julian Castro was confirmed by Senate on July 9, 2014 and assumed office on July 28, 2014, succeeding Shaun Donovan who was nominated to be the next Director of the Office of Management and Budget.
Living former Secretaries of Housing and Urban Development.
As of , there are eight living former Secretaries of Housing and Urban Development, the oldest being Maurice E. Landrieu (served 1979-1981, born 1930). The most recent Secretary of Housing and Urban Development to die was James T. Lynn (served 1973-1975, born 1927), on December 6, 2010.

</doc>
<doc id="45559" url="https://en.wikipedia.org/wiki?curid=45559" title="United States Secretary of Transportation">
United States Secretary of Transportation

The United States Secretary of Transportation is the head of the United States Department of Transportation, a member of the President's Cabinet, and thirteenth in the Presidential Line of Succession. The post was created with the formation of the Department of Transportation on October 15, 1966, by President Lyndon B. Johnson's signing of the Department of Transportation Act. The Department's mission is "to develop and coordinate policies that will provide an efficient and economical national transportation system, with due regard for need, the environment, and the national defense." The Secretary of Transportation oversees eleven agencies, including the Federal Aviation Administration, the Federal Highway Administration, and the National Highway Traffic Safety Administration. In April 2008, Mary Peters launched the official blog of the Secretary of Transportation called The Fast Lane.
The first Secretary of Transportation was Alan Stephenson Boyd, nominated to the post by Democratic President Lyndon B. Johnson. Ronald Reagan's second Secretary of Transportation, Elizabeth Dole, was the first female holder, and Mary Peters was the second. Gerald Ford's nominee William Thaddeus Coleman, Jr. was the first African American to serve as Transportation Secretary, and Federico Peña, serving under Bill Clinton, was the first Hispanic to hold the position, subsequently becoming Secretary of Energy. Japanese American Norman Mineta, who had previously been Secretary of Commerce, is the longest-serving Secretary, holding the post for over five and a half years, and Andrew Card is the shortest-serving Secretary, serving only eleven months. Neil Goldschmidt was the youngest secretary, taking office at age thirty-nine, while Norman Mineta was the oldest, retiring at age seventy-four. On January 23, 2009, the sixteenth secretary Ray LaHood took office, serving under the administration of Democrat Barack Obama; he had previously been a Republican Congressman from Illinois for fourteen years. The salary of the Secretary of Transportation is $199,700.
Anthony Foxx, then Mayor of Charlotte, North Carolina was nominated by President Barack Obama on April 29, 2013, to succeed Ray LaHood. On June 27, 2013 the Senate confirmed his appointment by a vote of 100-0.
Line of succession.
The line of succession regarding who would act as Secretary of Transportation in the event of a vacancy or incapacitation is as follows:
Living former Secretaries of Transportation.
As of , there are twelve living former Secretaries of Transportation, the oldest being William T. Coleman, Jr. (served 1975-1977, born 1920). The most recent Secretary of Transportation to die was Andrew L. Lewis (served 1981-1983, born 1931), on February 10, 2016. 
References.
The order of succession of the U.S. Department of Transportation is as follows <http://edocket.access.gpo.gov/2008/pdf/E8-5543.pdf>:

</doc>
<doc id="45561" url="https://en.wikipedia.org/wiki?curid=45561" title="United States Secretary of Energy">
United States Secretary of Energy

The United States Secretary of Energy is the head of the U.S. Department of Energy, a member of the Cabinet of the United States, and Fourteenth in the presidential line of succession. The position was formed on October 1, 1977 with the creation of the Department of Energy when President Jimmy Carter signed the Department of Energy Organization Act. Originally the post focused on energy production and regulation. The emphasis soon shifted to developing technology for better and more efficient energy sources as well as energy education. After the end of the Cold War, the department's attention also turned toward radioactive waste disposal and maintenance of environmental quality. The current Secretary of Energy is Ernest Moniz.
Former Secretary of Defense James Schlesinger was the first Secretary of Energy, who was a Republican nominated to the post by Democratic President Jimmy Carter, the only time a president has appointed someone of another party to the post. Schlesinger is also the only secretary to be dismissed from the post. Hazel O'Leary, Bill Clinton's first Secretary of Energy, was first female and African-American holder. The first Hispanic to serve as Energy Secretary was Clinton's second, Federico Peña. Steven Chu became the first Asian American to hold the position on January 20, 2009, serving under the administration of Barack Obama. He is also the first and only Nobel Prize winner to be a Cabinet secretary and the longest-serving Secretary of Energy. Spencer Abraham became the first Arab American to hold the position on November 15, 2004, serving under the administration of George W. Bush.
Living former Secretaries of Energy.
As of , there are nine living former Secretaries of Energy, the oldest being Charles Duncan, Jr. (served 1979-1981, born 1926). The most recent Secretary of Energy to die was James B. Edwards (served 1981–1982, born 1927), on December 26, 2014. 

</doc>
<doc id="45562" url="https://en.wikipedia.org/wiki?curid=45562" title="Cartagena Protocol on Biosafety">
Cartagena Protocol on Biosafety

The Cartagena Protocol on Biosafety to the Convention on Biological Diversity is an international agreement on biosafety as a supplement to the Convention on Biological Diversity effective since 2003. The Biosafety Protocol seeks to protect biological diversity from the potential risks posed by genetically modified organisms resulting from modern biotechnology.
The Biosafety Protocol makes clear that products from new technologies must be based on the precautionary principle and allow developing nations to balance public health against economic benefits. It will for example let countries ban imports of genetically modified organisms if they feel there is not enough scientific evidence that the product is safe and requires exporters to label shipments containing genetically altered commodities such as corn or cotton.
The required number of 50 instruments of ratification/accession/approval/acceptance by countries was reached in May 2003. In accordance with the provisions of its Article 37, the Protocol entered into force on 11 September 2003. As of March 2015, the Protocol had 170 parties, which includes 167 United Nations member states, the State of Palestine, and the European Union.
Objective.
In accordance with the precautionary approach, contained in Principle 15 of the Rio Declaration on Environment and Development, the objective of the Protocol is to contribute to ensuring an adequate level of protection in the field of the safe transfer, handling and use of 'living modified organisms resulting from modern biotechnology' that may have adverse effects on the conservation and sustainable use of biological diversity, taking also into account risks to human health, and specifically focusing on transboundary movements (Article 1 of the Protocol, SCBD 2000).
Living modified organisms (LMOs).
The protocol defines a 'living modified organism' as any living organism that possesses a novel combination of genetic material obtained through the use of modern biotechnology, and 'living organism' means any biological entity capable of transferring or replicating genetic material, including sterile organisms, viruses and viroids. 'Modern biotechnology' is defined in the Protocol to mean the application of in vitro nucleic acid techniques, or fusion of cells beyond the taxonomic family, that overcome natural physiological reproductive or recombination barriers and are not techniques used in traditional breeding and selection. 'Living modified organism (LMO) Products' are defined as processed material that are of living modified organism origin, containing detectable novel combinations of replicable genetic material obtained through the use of modern biotechnology (for instance, flour from GM maize). 'Living modified organism intended for direct use as food or feed, or for processing (LMO-FFP)' are agricultural commodities from GM crops. Overall the term 'living modified organisms' is equivalent to genetically modified organism – the Protocol did not make any distinction between these terms and did not use the term 'genetically modified organism.'
Precautionary approach.
One of the outcomes of the United Nations Conference on Environment and Development (also known as the Earth Summit) held in Rio de Janeiro, Brazil, in June 1992, was the adoption of the Rio Declaration on Environment and Development, which contains 27 principles to underpin sustainable development. Commonly known as the precautionary principle, Principle 15 states that "In order to protect the environment, the precautionary approach shall be widely applied by States according to their capabilities. Where there are threats of serious or irreversible damage, lack of full scientific certainty shall not be used as a reason for postponing cost-effective measures to prevent environmental degradation."
Elements of the precautionary approach are reflected in a number of the provisions of the Protocol, such as:
Application.
The Protocol applies to the transboundary movement, transit, handling and use of all living modified organisms that may have adverse effects on the conservation and sustainable use of biological diversity, taking also into account risks to human health (Article 4 of the Protocol, SCBD 2000).
Parties and non-parties.
The governing body of the Protocol is called the Conference of the Parties to the Convention serving as the meeting of the Parties to the Protocol (also the COP-MOP). The main function of this body is to review the implementation of the Protocol and make decisions necessary to promote its effective operation. Decisions under the Protocol can only be taken by Parties to the Protocol. Parties to the Convention that are not Parties to the Protocol may only participate as observers in the proceedings of meetings of the COP-MOP.
The Protocol addresses the obligations of Parties in relation to the transboundary movements of LMOs to and from non-Parties to the Protocol. The transboundary movements between Parties and non-Parties must be carried out in a manner that is consistent with the objective of the Protocol. Parties are required to encourage non-Parties to adhere to the Protocol and to contribute information to the Biosafety Clearing-House.
Relationship with the WTO.
A number of agreements under the World Trade Organization (WTO), such as the Agreement on the Application of Sanitary and Phytosanitary Measures (SPS Agreement) and the Agreement on Technical Barriers to Trade (TBT Agreement), and the Agreement on Trade-Related Aspects of Intellectual Property Rights (TRIPs), contain provisions that are relevant to the Protocol. The Protocol states in its preamble that parties:
Main features.
Overview of features.
The Protocol promotes biosafety by establishing rules and procedures for the safe transfer, handling, and use of LMOs, with specific focus on transboundary movements of LMOs. It features a set of procedures including one for LMOs that are to be intentionally introduced into the environment called the advance informed agreement procedure, and one for LMOs that are intended to be used directly as food or feed or for processing. Parties to the Protocol must ensure that LMOs are handled, packaged and transported under conditions of safety. Furthermore, the shipment of LMOs subject to transboundary movement must be accompanied by appropriate documentation specifying, among other things, identity of LMOs and contact point for further information. These procedures and requirements are designed to provide importing Parties with the necessary information needed for making informed decisions about whether or not to accept LMO imports and for handling them in a safe manner.
The Party of import makes its decisions in accordance with scientifically sound risk assessments. The Protocol sets out principles and methodologies on how to conduct a risk assessment. In case of insufficient relevant scientific information and knowledge, the Party of import may use precaution in making their decisions on import. Parties may also take into account, consistent with their international obligations, socio-economic considerations in reaching decisions on import of LMOs.
Parties must also adopt measures for managing any risks identified by the risk assessment, and they must take necessary steps in the event of accidental release of LMOs.
To facilitate its implementation, the Protocol establishes a Biosafety Clearing-House for Parties to exchange information, and contains a number of important provisions, including capacity-building, a financial mechanism, compliance procedures, and requirements for public awareness and participation.
Procedures for moving LMOs across borders.
Advance Informed Agreement.
The "Advance Informed Agreement" (AIA) procedure applies to the first intentional transboundary movement of LMOs for intentional introduction into the environment of the Party of import. It includes four components: notification by the Party of export or the exporter, acknowledgment of receipt of notification by the Party of import, the decision procedure, and opportunity for review of decisions. The purpose of this procedure is to ensure that importing countries have both the opportunity and the capacity to assess risks that may be associated with the LMO before agreeing to its import. The Party of import must indicate the reasons on which its decisions are based (unless consent is unconditional). A Party of import may, at any time, in light of new scientific information, review and change a decision. A Party of export or a notifier may also request the Party of import to review its decisions.
However, the Protocol's AIA procedure does not apply to certain categories of LMOs:
While the Protocol's AIA procedure does not apply to certain categories of LMOs, Parties have the right to regulate the importation on the basis of domestic legislation. There are also allowances in the Protocol to declare certain LMOs exempt from application of the AIA procedure.
LMOs intended for food or feed, or for processing.
LMOs intended for direct use as food or feed, or processing (LMOs-FFP) represent a large category of agricultural commodities. The Protocol, instead of using the AIA procedure, establishes a more simplified procedure for the transboundary movement of LMOs-FFP. Under this procedure, A Party must inform other Parties through the Biosafety Clearing-House, within 15 days, of its decision regarding domestic use of LMOs that may be subject to transboundary movement.
Decisions by the Party of import on whether or not to accept the import of LMOs-FFP are taken under its domestic regulatory framework that is consistent with the objective of the Protocol. A developing country Party or a Party with an economy in transition may, in the absence of a domestic regulatory framework, declare through the Biosafety Clearing-House that its decisions on the first import of LMOs-FFP will be taken in accordance with risk assessment as set out in the Protocol and time frame for decision-making.
Handling, transport, packaging and identification.
The Protocol provides for practical requirements that are deemed to contribute to the safe movement of LMOs. Parties are required to take measures for the safe handling, packaging and transportation of LMOs that are subject to transboundary movement. The Protocol specifies requirements on identification by setting out what information must be provided in documentation that should accompany transboundary shipments of LMOs. It also leaves room for possible future development of standards for handling, packaging, transport and identification of LMOs by the meeting of the Parties to the Protocol.
Each Party is required to take measures ensuring that LMOs subject to intentional transboundary movement are accompanied by documentation identifying the LMOs and providing contact details of persons responsible for such movement. The details of these requirements vary according to the intended use of the LMOs, and, in the case of LMOs for food, feed or for processing, they should be further addressed by the governing body of the Protocol. (Article 18 of the Protocol, SCBD 2000).
The first meeting of the Parties adopted decisions outlining identification requirements for different categories of LMOs (Decision BS-I/6, SCBD 2004). However, the second meeting of the Parties failed to reach agreement on the detailed requirements to identify LMOs intended for direct use as food, feed or for processing and will need to reconsider this issue at its third meeting in March 2006.
Biosafety Clearing-House.
The Protocol established a Biosafety Clearing-House (BCH), in order to facilitate the exchange of scientific, technical, environmental and legal information on, and experience with, living modified organisms; and to assist Parties to implement the Protocol (Article 20 of the Protocol, SCBD 2000). It was established in a phased manner, and the first meeting of the Parties approved the transition from the pilot phase to the fully operational phase, and adopted modalities for its operations (Decision BS-I/3, SCBD 2004).

</doc>
<doc id="45564" url="https://en.wikipedia.org/wiki?curid=45564" title="United States Secretary of Veterans Affairs">
United States Secretary of Veterans Affairs

The United States Secretary of Veterans Affairs is the head of the U.S. Department of Veterans Affairs, the department concerned with veterans' benefits, health care, and national veterans' memorials and cemeteries. The Secretary is a member of the Cabinet and second to last at sixteenth in the line of succession to the presidency (the position was last until the addition of the United States Department of Homeland Security in 2006). To date, all appointees and acting appointees to the post have been United States military veterans, but that is not a requirement to fill the position.
When the post of Secretary is vacant, the United States Deputy Secretary of Veterans Affairs or any other person designated by the President serves as Acting Secretary until the President nominates and the United States Senate confirms a new Secretary.
On December 8, 2008, U.S. President Barack Obama announced he would nominate retired U.S. Army general Eric Shinseki to be the seventh Secretary of Veterans Affairs. He was unanimously confirmed by the United States Senate on January 20, 2009. General Shinseki resigned as Secretary of Veterans Affairs on May 30, 2014, making deputy secretary Sloan Gibson the acting secretary. On June 29, 2014, President Obama nominated former Procter & Gamble CEO Robert A. McDonald to serve as VA secretary. The United States Senate confirmed McDonald on July 29, 2014.
Living former Secretaries of Veterans Affairs.
As of , there are five living former Secretaries of Veterans Affairs, the oldest being Jim Nicholson (served 2005-2007, born 1938). The most recent Secretary of Veterans Affairs to die was Ed Derwinski (served 1989-1992, born 1926), on January 15, 2012.

</doc>
<doc id="45567" url="https://en.wikipedia.org/wiki?curid=45567" title="Portuguese Mozambique">
Portuguese Mozambique

Portuguese Mozambique or Portuguese East Africa are the common terms by which Mozambique is designated when referring to the historic period when it was a Portuguese overseas territory. Former Portuguese Mozambique constituted a string of Portuguese colonies and later a single Portuguese overseas province along the south-east African coast, which now form the Republic of Mozambique.
Portuguese trading settlements and, later, colonies were formed along the coast from 1498, when Vasco da Gama first reached the Mozambican coast. Lourenço Marques explored the area that is now Maputo Bay in 1544. He settled permanently in present-day Mozambique, where he spent most of his life, and his work was followed by other Portuguese explorers, sailors and traders. Some of these colonies were handed over in the late 19th century for rule by chartered companies such as the ' and the '. In 1951 the colonies were combined into a single overseas province under the name as an integral part of Portugal. Most of the original colonies have given their names to the modern provinces of Mozambique.
Mozambique, according to official policy, was not a colony at all but rather a part of the "pluricontinental and multiracial nation" of Portugal. Portugal sought in Mozambique, as it did in all its colonies, to Europeanise the local population and assimilate them into Portuguese culture. Lisbon also wanted to retain the colonies as trading partners and markets for its goods. African inhabitants of the colony were ultimately supposed to become full citizens with full political rights through a long development process. To that end, segregation in Mozambique was minimal compared to that in neighbouring South Africa. However, paid forced labour, to which all Africans were liable if they failed to pay head taxes, was not abolished until the early 1960s.
Designation.
During its history, under Portuguese dominion, present-day Mozambique had the following formal designations: 
Overview.
Until the 20th century the land and peoples of Mozambique were barely affected by the Europeans who came to its shores and entered its major rivers. As the Muslim traders, mostly Swahili, were displaced from their coastal centres and routes to the interior by the Portuguese, migrations of Bantu peoples continued and tribal federations formed and reformed as the relative power of local chiefs changed. For four centuries the Portuguese presence was meagre. Coastal and river trading posts were built, abandoned, and built again. Governors sought personal profits to take back to Portugal, and colonists were not attracted to the distant area with its relatively unattractive climate; those who stayed were traders who married local women and successfully maintained relations with local chiefs.
In Portugal, however, Mozambique was considered to be a vital part of a world empire. Periodic recognition of the relative insignificance of the revenues it could produce was tempered by the mystique which developed regarding the mission of the Portuguese to bring their civilization to the African territory. It was believed that through missionary activity and other direct contact between Africans and Europeans, the Africans could be taught to appreciate and participate in Portuguese culture.
In the last decade of the 19th century and the first part of the 20th century, integration of Mozambique into the structure of the Portuguese nation was begun. After all of the area of the present province had been recognized by other European powers as belonging to Portugal, pacification of the tribes of the interior was completed and the traditional holders of political power were subordinated to the Portuguese. Civil administration was established throughout the area, the building of an infrastructure was begun, and agreements regarding the transit trade of Mozambique's land-locked neighbours to the west were made.
Portugal never officially had a racist policy or sanctioned discrimination based on race. Its concept of what it called a "multiracial society" envisaged complete racial integration, including intermarriage, as well as cultural adaptation. The historically determined position of the Portuguese as conquerors and governors of the Africans, however, resulted in barriers to the formation of this ideal. The fact that most Africans were not "cultivated" in the Portuguese sense, and that many participated in what were considered by the Portuguese to be pagan beliefs and uncivilized behaviour, tended to create a low opinion of Africans as a group. The uneducated Portuguese immigrant peasants in urban areas were frequently in direct competition with Africans for jobs and demonstrated jealousies and prejudices with racial overtones.
The society was divided into two peripherally interrelated sectors. The urban-based modern sector, comprising altogether between 2 and 2.5 percent of the population, consisting mostly of Europeans but including a few thousand Europeanised Africans, Indians, and Chinese, was dominant in the economic, political, and social realms. Communication between this sector and the large majority of rural Africans was limited; only a small proportion of the Africans could speak Portuguese, the language of the administration and the modern economic sector. Communication between members of the ten different major ethnolinguistic groups was also difficult.
Economically and socially, all but a few educated and Europeanised Africans were at a disadvantage vis-à-vis the Europeans. Access to education above the primary level was limited by lack of means, by age limitations, or by lack of sufficient preparations. Access to economic opportunity was limited by lack of adequate training.
Between the modern urban and traditional rural sectors of the society was a steadily increasing group of Africans who were loosening their ties with the village and starting to participate in the money economy, to settle in suburbs, and to adopt new customs. This transitional group included individuals who had acquired a modicum of education or skills and some of the aspirations associated with a modern European way of life. Many of them, especially those who had an education beyond the primary level, were more alert politically than the majority of the population, who are either unaware of or uninterested in political issues. It was members of this group, allied with forward-looking European leaders and intellectuals, who had shown the greatest interest in reforms and benefits for the African population. Some among them left the country to become active participants in the independence movement.
History.
When Portuguese explorers reached East Africa in 1498, Swahili commercial settlements had existed along the coast and outlying islands for several centuries. From about 1500, Portuguese trading posts and forts became regular ports of call on the new route to the east.
The voyage of Vasco da Gama around the Cape of Good Hope into the Indian Ocean in 1498 marked the Portuguese entry into trade, politics, and society in the Indian Ocean world. The Portuguese gained control of the Island of Mozambique and the port city of Sofala in the early 16th century. Vasco da Gama having visited Mombasa in 1498, was then successful in reaching India and this permitted the Portuguese to trade with the Far East directly by sea, thus challenging older trading networks of mixed land and sea routes, such as the spice trade routes that used the Persian Gulf, Red Sea and caravans to reach the eastern Mediterranean.
The Republic of Venice had gained control over much of the trade routes between Europe and Asia. After traditional land routes to India had been closed by the Ottoman Turks, Portugal hoped to use the sea route pioneered by da Gama to break the Venetian trading monopoly. Initially, Portuguese rule in East Africa focused mainly on a coastal strip centred in Mombasa. With voyages led by Vasco da Gama, Francisco de Almeida and Afonso de Albuquerque, the Portuguese dominated much of southeast Africa's coast, including Sofala and Kilwa, by 1515. Their main goal was to dominate trade with India. As the Portuguese settled along the coast, they made their way into the hinterland as ' (backwoodsmen). These ' lived alongside Swahili traders and even took up service among Shona kings as interpreters and political advisors. One such "" managed to travel through almost all the Shona kingdoms, including the Mutapa Empire's (Mwenemutapa) metropolitan district, between 1512 and 1516.
By the 1530s, small groups of Portuguese traders and prospectors penetrated the interior regions seeking gold, where they set up garrisons and trading posts at Sena and Tete on the Zambezi River and tried to gain exclusive control over the gold trade. The Portuguese finally entered into direct relations with the Mwenemutapa in the 1560s.
They recorded a wealth of information about the Mutapa kingdom as well as its predecessor, Great Zimbabwe. According to Swahili traders whose accounts were recorded by the Portuguese historian João de Barros, Great Zimbabwe was an ancient capital city built of stones of marvellous size without the use of mortar. And while the site was not within Mutapa's borders, the Mwenemutapa kept noblemen and some of his wives there.
The Portuguese attempted to legitimate and consolidate their trade and settlement positions through the creation of ' (land grants) tied to Portuguese settlement and administration. While ' were originally developed to be held by Portuguese, through intermarriage they became African Portuguese or African Indian centres defended by large African slave armies known as "Chikunda". Historically, within Mozambique, there was slavery. Human beings were bought and sold by African tribal chiefs, Arab traders, and the Portuguese. Many Mozambican slaves were supplied by tribal chiefs who raided warring tribes and sold their captives to the "".
Although Portuguese influence gradually expanded, its power was limited and exercised through individual settlers and officials who were granted extensive autonomy. The Portuguese were able to wrest much of the coastal trade from Arabs between 1500 and 1700, but, with the Arab seizure of Portugal's key foothold at Fort Jesus on Mombasa Island (now in Kenya) in 1698, the pendulum began to swing in the other direction. As a result, investment lagged while Lisbon devoted itself to the more lucrative trade with India and the Far East and to the colonisation of Brazil. During the 18th and 19th centuries, the Mazrui and Omani Arabs reclaimed much of the Indian Ocean trade, forcing the Portuguese to retreat south.
Many "" had declined by the mid-19th century, but several of them survived. During the 19th century other European powers, particularly the British and the French, became increasingly involved in the trade and politics of the region. In the Island of Mozambique, the hospital, a majestic neo-classical building constructed in 1877 by the Portuguese, with a garden decorated with ponds and fountains, was for many years the biggest hospital south of the Sahara. By the early 20th century the Portuguese had shifted the administration of much of Mozambique to large private companies, like the Mozambique Company, the Zambezia Company and the Niassa Company, controlled and financed mostly by the British, which established, with the Portuguese, railroad lines to neighbouring countries. The companies, granted a charter by the Portuguese government to foster economic development and maintain Portuguese control in the territory's provinces, would lose their purpose when the territory was transferred to the control of the Portuguese colonial government between 1929 and 1942.
Although slavery had been legally abolished in Mozambique by the Portuguese authorities, at the end of the 19th century the Chartered companies enacted a forced labour policy and supplied cheap – often forced – African labour to the mines and plantations of the nearby British colonies and South Africa. The Zambezia Company, the most profitable chartered company, took over a number of smaller "" holdings, and requested Portuguese military outposts to protect its property. The chartered companies and the Portuguese administration built roads and ports to bring their goods to market including a railroad linking present day Zimbabwe with the Mozambican port of Beira. However, the development's administration gradually started to pass directly from the trading companies to the Portuguese government itself.
Because of their unsatisfactory performance and because of the shift, under the "" regime of Oliveira Salazar, towards a stronger Portuguese control of the Portuguese Empire's economy, the companies' concessions were not renewed when they ran out. This was what happened in 1942 with the Mozambique Company, which however continued to operate in the agricultural and commercial sectors as a corporation, and had already happened in 1929 with the termination of the Niassa Company's concession.
In the 1950s, the Portuguese overseas colony was rebranded an overseas province of Portugal, and by the early 1970s it was officially upgraded to the status of Portuguese non-sovereign state, by which it would remain a Portuguese territory but with a wider administrative autonomy. The Front for the Liberation of Mozambique (FRELIMO), initiated a guerrilla campaign against Portuguese rule in September 1964. This conflict, along with the two others already initiated in the other Portuguese colonies of Angola and Guinea, became part of the so-called Portuguese Colonial War (1961–74). From a military standpoint, the Portuguese regular army held the upper hand during all of the conflicts against the independentist guerrilla forces, which created favourable conditions for social development and economic growth until the end of the conflict in 1974.
After ten years of sporadic warfare and after Portugal's return to democracy through a leftist military coup in Lisbon which replaced Portugal's ' regime in favor of a military junta (the Carnation Revolution of April 1974), FRELIMO took control of the territory. The talks that led to an agreement on Mozambique's independence, signed in Lusaka, were started. Within a year, almost all ethnic Portuguese population had left, many fleeing in fear (in mainland Portugal they were known as '); others were expelled by the ruling power of the newly independent territory. Mozambique became independent from Portugal on 25 June 1975.
Government.
At least since the early 19th century, the legal status of Mozambique always considered it as much a part of Portugal as Lisbon, but as an overseas province enjoyed special derogations to account for its distance from Europe.
From 1837, the highest government official in the province of Mozambique has always been the governor-general, who reported directly to the Government in Lisbon, usually through the minister of the Overseas. During some periods in the late 19th and the early 20th century, the governors-general of Mozambique received the status of royal commissioners or of high commissioners, which gave them extended executive and legislative powers, equivalent to those of a government minister.
In the 20th century, the province was also subject to the authoritarian "" regime that ruled Portugal from 1933 to 1974, until the military coup at Lisbon, known as the Carnation Revolution. Most members of the government of Mozambique were from Portugal, but a few were Africans. Nearly all members of the bureaucracy were from Portugal, as most Africans did not have the necessary qualifications to obtain positions.
The government of Mozambique, as it was in Portugal, was highly centralized. Power was concentrated in the executive branch, and all elections where they occurred were carried out using indirect methods. From the Prime Minister's office in Lisbon, authority extended down to the most remote posts and "" of Mozambique through a rigid chain of command. The authority of the government of Mozambique was residual, primarily limited to implementing policies already decided in Europe. In 1967, Mozambique also sent seven delegates to the National Assembly in Lisbon.
The highest official in the province was the governor-general, appointed by the Portuguese cabinet on recommendation of the Overseas Minister. The governor-general had both executive and legislative authority. A Government Council advised the governor-general in the running of the province. The functional cabinet consisted of five secretaries appointed by the Overseas Minister on the advice of the governor. A Legislative Council had limited powers and its main activity was approving the provincial budget. Finally, an Economic and Social Council had to be consulted on all draft legislation, and the governor-general had to justify his decision to Lisbon if he ignored its advice.
Mozambique was divided into nine districts, which were further subdivided into 61 municipalities (') and 33 circumscriptions ('). Each subdivision was then made up of three or four individual posts, 166 in all with an average of 40,000 Africans in each. Each district, except Lourenço Marques which was run by the governor-general, was overseen by a governor. Most Africans only had contact with the Portuguese through the post administrator, who was required to visit each village in his domain at least once a year.
The lowest level of administration was the ', settlements inhabited by Africans living according to customary law. Each ' was run by a ', an African or Portuguese official chosen on the recommendation of local residents. Under the ', each village had its own African headman.
Each level of government could also have an advisory board or council. They were established in municipalities with more than 500 electors, in smaller municipalities or circumscriptions with more than 300 electors, and in posts with more than 20 electors. Each district also had its own board as well.
Two legal systems were in force — Portuguese civil law and African customary law. As part of its policy of assimilation, the Portuguese sought to break down the African legal system and did not study or codify much of it. Until 1961, Africans were considered to be "" or natives, rather than citizens. After 1961, the previous native laws were repealed and Africans gained "de facto" Portuguese citizenship. From then on, the status of Africans depended merely on whether or not they chose to be governed by civil law, and the number of Africans that made the choice was very small.
Geography.
Portuguese East Africa was located in south-eastern Africa. It was a long coastal strip with Portuguese strongholds, from current day Tanzania and Kenya, to the south of current-day Mozambique.
In 1900, the part of modern Mozambique northwest of the Zambezi and Shire Rivers was called ; the rest of it was . Various districts existed, and even issued stamps, during the first part of the century, including Inhambane, , Mozambique Colony, Mozambique Company, Nyassa Company, Quelimane, Tete, and . The Nyassa Company territory is now and .
In the early- and mid-20th century, a number of changes occurred. Firstly, on 28 June 1919, the Treaty of Versailles transferred the Kionga Triangle, a territory south of the Rovuma River from German East Africa to Mozambique.
During World War II, the Charter of the Mozambique Company expired, on 19 July 1942; its territory, known as Manica and Sofala, became a district of Mozambique. Mozambique was constituted as four districts on 1 January 1943 — Manica and Sofala, , (South of the Save River), and .
On 20 October 1954, administrative reorganization caused and Mozambique districts to be split from . At the same time, the district was divided into Gaza, Inhambane and , while the district was split from Manica and Sofala.
By the early 1970s, Mozambique was bordering the Mozambique Channel, bordering the countries of Malawi, Rhodesia, South Africa, Swaziland, Tanzania, and Zambia. Covering a total area of . With a tropical to subtropical climate, the Zambezi flows through the north-central and most fertile part of the country. Its coastline had , with of land boundaries, its highest point at Monte Binga (). The Gorongosa National Park, founded in 1920, was the main natural park in the territory.
The districts with its respective capitals were:
Other important urban centres included Sofala, Nacala, António Enes, Island of Mozambique and Vila Junqueiro.
Demographics.
By 1970, the Portuguese Overseas Province of Mozambique had about 8,168,933 inhabitants. Nearly 300,000 were white ethnic Portuguese. There was a number of mulattoes, from both European and African ancestry, living across the territory. However, the majority of the population belonged to local tribal groups which included the Makua–Lomwe, the Shona and the Tsonga. Other ethnic minorities included British, Greeks, Chinese and Indians. Most inhabitants were black indigenous Africans with a diversity of ethnic and cultural backgrounds, ranging from Shangaan and Makonde to Yao or Shona peoples. The Makua were the largest ethnic group in the north. The Sena and Shona (mostly Ndau) were prominent in the Zambezi valley, and the Shangaan (Tsonga) dominated in the south. In addition, several other minority groups lived a tribal lifestyle across the territory.
Mozambique had around 250,000 Europeans in 1974 that made up around 3% of the population. Mozambique was cosmopolitan as it had around Indian, Chinese, Greek and Anglophone communities living there too (over 25,000 Indians and 5,000 Chinese by the early 1970s). The white population was more influenced from South Africa. The capital of Portuguese Mozambique, Lourenço Marques (Maputo), had a population of 355,000 in 1970 with around 100,000 Europeans. Beira had around 115,000 inhabitants at the time with around 30,000 Europeans. Most of the other cities ranged from 10 to 15% in the number of Europeans, while Portuguese Angola cities had European majorities ranging from 50% to 60%.
Society.
The establishment of a dual, racialized civil society was formally recognized in ' (The Statute of Indigenous Populations) adopted in 1929, and was based in the subjective concept of civilization versus tribalism. Portugal's colonial authorities were totally committed to develop a fully multiethnic "civilized" society in its African colonies, but that goal or "civilizing mission", would only be achieved after a period of Europeanization or enculturation of the native black tribes and ethnocultural groups. It was a policy which had already been stimulated in the former Portuguese colony of Brazil and in Portuguese Angola. The ' established a distinction between the "colonial citizens", subject to the Portuguese laws and entitled to all citizenship rights and duties effective in the "metropole", and the ' (natives), subjected to colonial legislation and, in their daily lives, to their customary, tribal native laws. Between the two groups there was a third small group, the ', comprising native blacks, mulatos, Asians, and mixed-race people, who had at least some formal education, were not subjected to paid forced labor, were entitled to some citizenship rights, and held a special identification card that differed from the one imposed on the immense mass of the African population (the '), a card that the colonial authorities conceived of as a means of controlling the movements of forced labor (CEA 1998). The ' were subject to the traditional authorities, who were gradually integrated into the colonial administration and charged with solving disputes, managing the access to land, and guaranteeing the flows of workforce and the payment of taxes. As several authors have pointed out (Mamdani 1996; Gentili 1999; O'Laughlin 2000), the "" regime was the political system that subordinated the immense majority of Mozambicans to local authorities entrusted with governing, in collaboration with the lowest echelon of the colonial administration, the "native" communities described as tribes and assumed to have a common ancestry, language, and culture. The colonial use of traditional law and structures of power was thus an integral part of the process of colonial domination (Young 1994; Penvenne 1995; O'Laughlin 2000) obsessed with the maximization of economic development and growth through the use of idle or unproductive African workforce.
In the 1940s, the integration of traditional authorities into the colonial administration was deepened, a level of social integration, miscegenation and social promotion based in skill and human qualities of each individual, rather than in the ethnic background, which was coined ' and had been a major feature of the Portuguese Empire throughout history. The Portuguese colony was divided into ' (municipalities), in urban areas, governed by colonial and metropolitan legislation, and ' (localities), in rural areas. The ' were led by a colonial administrator and divided into ' (subdivisions of circunscrições), headed by ' (tribal chieftains), the embodiment of traditional authorities. Provincial Portuguese Decree No. 5.639, of July 29, 1944, attributed to ' and their assistants, the ', the status of ' (administrative assistants). Gradually, these "traditional" titles lost some of their content, and the ' and ' came to be viewed as an effective part of the colonial state, remunerated for their participation in the collection of taxes, recruitment of the labor force, and agricultural production in the area under their control. Within the areas of their jurisdiction, the ' and ' also controlled the distribution of land and settled conflicts according to customary norms (Geffray 1990; Alexander 1994; Dinerman 1999). To exercise their power, the ' and ' had their own police force. This system of indirect rule illustrates what the disjunction between political and administrative control. In major urban areas, most notoriously the cosmopolitan provincial ports of and Beira, racial integration and socioeconomic opportunities for all kind of skilled citizens were already very deep. It continued after the ' system was abolished in the early 1960s after the Portuguese colony of Mozambique has been rebranded the Overseas Province of Mozambique in the 1950s. From then on, all Africans were considered Portuguese citizens, and racial discrimination became a sociological rather than a legal feature of colonial society. The rule of traditional authorities was indeed integrated more than before in the colonial administration.
Ethnic African inhabitants of the Portuguese overseas provinces were ultimately supposed to become full citizens with full political rights through a long development process. To that end, by the 1960s and 1970s, segregation in Mozambique was minimal compared to that in neighbouring South Africa.
Urban centres.
The largest coastal cities, the first founded or settled by Portuguese people since the 16th century, like the capital , Beira, Quelimane, Nacala and Inhambane were modern cosmopolitan ports and a melting pot of several cultures, with a strong South African influence. The Southeast African and Portuguese cultures were dominant, but the influence of Arab, Indian, and Chinese cultures were also felt. The cuisine was diverse, owing especially to the Portuguese cuisine and Muslim heritage, and seafood was also quite abundant.
Lourenço Marques had always been a point of interest for artistic and architectural development since the first days of its urban expansion and this strong artistic spirit was responsible for attracting some of the world's most forward architects at the turn of the 20th century. The city was home to masterpieces of building work by, Pancho Guedes, Herbert Baker and Thomas Honney amongst others. The earliest architectural efforts around the city focused on classical European designs such as the Central Train Station (CFM) designed by architects Alfredo Augusto Lisboa de Lima, Mario Veiga and Ferreira da Costa and built between 1913 and 1916 (sometimes mistaken with the work of Gustav Eiffel), and the Hotel Polana designed by Herbert Baker.
As the 1960s and 1970s approached, was yet again at the center of a new wave of architectural influences made most popular by Pancho Guedes. The designs of the 1960s and 1970s were characterized by modernist movements of clean, straight and functional structures. However, prominent architects such as Pancho Guedes fused this with local art schemes giving the city's buildings a unique Mozambican theme. As a result, most of the properties erected during the second construction boom take on these styling cues.
Economy.
Since the 15th century, Portugal founded settlements, trading posts, forts and ports in the Sub-Saharan Africa's coast. Cities, towns and villages were founded all over East African territories by the Portuguese, especially since the 19th century, like Lourenço Marques, Beira, Vila Pery, Vila Junqueiro, Vila Cabral and Porto Amélia. Others were expanded and developed greatly under Portuguese rule, like Quelimane, Nampula and Sofala. By this time, Mozambique had become a Portuguese colony, but administration was left to the trading companies (like Mozambique Company and Niassa Company) who had received long-term leases from Lisbon. By the mid-1920s, the Portuguese succeeded in creating a highly exploitative and coercive settler economy, in which African natives were forced to work on the fertile lands taken over by Portuguese settlers. Indigenous African peasants mainly produced cash crops designated for sale in the markets of the colonial metropole (the center, i.e. Portugal). Major cash crops included cotton, cashews, tea and rice. This arrangement ended in 1932 after the takeover in Portugal by the new António de Oliveira Salazar's government — the "". Thereafter, Mozambique, along with other Portuguese colonies, was put under the direct control of Lisbon. In 1951, it became an overseas province. The economy expanded rapidly during the 1950s and 1960s, attracting thousands of Portuguese settlers to the country. It was around this time that the first nationalist guerrilla groups began to form in Tanzania and other African countries. The strong industrial and agricultural development that did occur throughout the 1950s, 1960s and early 1970s was based on Portuguese development plans, and also included British and South African investment.
In 1959–60, Mozambique's major exports included cotton, cashew nuts, tea, sugar, copra and sisal. Other major agricultural productions included rice and coconut. The expanding economy of the Portuguese overseas province was fuelled by foreign direct investment, and public investment which included ambitious state-managed development plans. British capital owned two of the large sugar concessions (the third was Portuguese), including the famous Sena states. The Matola Oil Refinery, Procon, was controlled by Britain and the United States. In 1948 the petroleum concession was given to the Mozambique Gulf Oil Company. At Maotize coal was mined; the industry was chiefly financed by Belgian capital. 60% of the capital of the ' was held by the ', 30% by the Mozambique Company, and the remaining 10% by the Government of the territory. Three banks were in operation, the , Portuguese, Barclays Bank, D.C.O., British, and the (a partnership between Standard Bank of South Africa and mainland's ). Nine out of the twenty-three insurance companies were Portuguese. 80% of life assurance was in the hands of foreign companies which testifies to the openness of the economy.
The Portuguese overseas province of Mozambique was the first territory of Portugal, including the European mainland, to distribute Coca Cola. Lately the Oil Refinery was established by the "" (SONAREP) — a Franco-Portuguese syndicate. In the sisal plantations Swiss capital was invested, and in copra concerns, a combination of Portuguese, Swiss and French capital was invested. The large availability of capital from both Portuguese and international origin, allied to the wide range of natural resources and the growing urban population, lead to an impressive growth and development of the economy.
From the late stages of this notable period of high growth and huge development effort started in the 1950s, was the construction of Cahora Bassa dam by the Portuguese, which started to fill in December 1974 after construction was commenced in 1969. In 1971 construction work of the Massingir Dam began. At independence, Mozambique's industrial base was well-developed by Sub-Saharan Africa standards, thanks to a boom in investment in the 1960s and early 1970s. Indeed, in 1973, value added in manufacturing was the sixth highest in Sub-Saharan Africa.
Economically, Mozambique was a source of agricultural raw materials and an earner of foreign exchange. It also provided a market for Portuguese manufacturers which were protected from local competition. Transportation facilities had been developed to exploit the transit trade of South Africa, Swaziland, Rhodesia, Malawi, and Zambia, agricultural production for export purposes had been encouraged, and profitable arrangements for the export of labour had been made with neighbouring countries. Industrial production had been relatively insignificant, but did begin to increase in the 1960s. The economic structure generally favoured the taking of profits to Portugal rather than their reinvestment in Mozambique. The Portuguese interests which dominate in banking, industry, and agriculture, exerted a powerful influence on policy.
Education.
Mozambique's rural black populations were largely illiterate, as were a majority of Portugal's peasantry. However, a number of natives from diverse tribal backgrounds were educated in Portuguese language and history by several missionary schools established across the vast countryside areas. In mainland Portugal, the homeland of the colonial authorities which ruled Mozambique from the 16th century until 1975, by the end of the 19th century the illiteracy rates were at over 80 percent and higher education was reserved for a small percentage of the population. 68.1 percent of mainland Portugal's population was still classified as illiterate by the 1930 census. Mainland Portugal's literacy rate by the 1940s and early 1950s was low for North American and Western European standards at the time. Only in the mid-1960s did the country make public education available for all children between the ages of six and twelve, and the overseas territories in Africa profited from this new educational developments and change in policy at Lisbon. Starting in the early 1950s, the access to basic, secondary and technical education was expanded and its availability was being increasingly opened to both the African ' and the European Portuguese of the African territories. A comprehensive network of secondary schools (the ') and technical or vocational education schools were implemented across the cities and main towns of the territory. In 1962, the first Mozambican university was founded by the Portuguese authorities in the provincial capital, , the "", awarding a wide range of degrees from engineering to medicine, during a time that in the European Portuguese mainland only four public universities were in operation.
Sports.
The Portuguese-ruled territory was introduced to several popular European and North American sports disciplines since the early urbanistic and economic booms of the 1920s and 1940s. This period was a time of city and town expansion and modernization that included the construction of several sports facilities for football, rink hockey, basketball, volleyball, handball, athletics, gymnastics and swimming. Several sports clubs were founded across the entire territory, among them were some of the largest and oldest sports organizations of Mozambique like established in 1920. Other major sports clubs were founded in the following years like (1921), (1924), (1928), (1943), (1943), and (1955). Several sportsmen, especially football players, that achieved wide notability in Portuguese sports were from Mozambique. Eusébio and Mário Coluna were examples of that, and excelled in the Portugal national football team. Since the 1960s, with the latest developments on commercial aviation, the highest ranked football teams of Mozambique and the other African overseas provinces of Portugal, started to compete in the (the Portuguese Cup). There were also several facilities and organizations for golf, tennis and wild hunting.
The nautical sports were also well developed and popular, especially in , home to the . The largest stadium was the , located near . Opened in 1968, it was at the time the most advanced in Mozambique conforming to standards set by both FIFA and the Union Cycliste Internationale (UCI). The cycling track could be adjusted to allow for 20,000 more seats.
Beginning in the 1950s, motorsport was introduced to Mozambique. At first race cars would compete in areas around the city, Polana and along the ' but as funding and interest increased, a dedicated race track was built in the Costa Do Sol area along and behind the ' with the ocean to the east with a length of . The initial surface of the new track, named did not provide enough grip and an accident in the late 1960s killed 8 people and injured many more. Therefore, in 1970, the track was renovated and the surface changed to meet the highest international safety requirements that were needed at large events with many spectators. The length then increased to . The city became host to several international and local events beginning with the inauguration on 26 November 1970.
Carnation Revolution and independence.
As communist and anti-colonial ideologies spread out across Africa, many clandestine political movements were established in support of Mozambique's independence. Regardless of whether it was exaggerated anti-Portuguese / anti-"Colonial" propaganda, a dominant tendency in Mozambique, or a mix of both, these movements claimed that since policies and development plans were primarily designed by the ruling authorities for the benefit of the Mozambican ethnic Portuguese population, little attention was paid to local tribal integration and the development of its native communities. According to the official guerrilla statements, this affected a majority of the indigenous population who suffered both state-sponsored discrimination and enormous social pressure. Many felt they had received too little opportunity or resources to upgrade their skills and improve their economic and social situation to a degree comparable to that of the Europeans. Statistically, Portuguese Mozambique's whites were indeed wealthier and more skilled than the black indigenous majority, but the late 1950s, the 1960s and the early 1970s, were being testimony of a gradual change based in new socio-economic developments and egalitarian policies regarding underprivileged rural black communities.
The Front for the Liberation of Mozambique (FRELIMO), headquartered in Tanzania, initiated a guerrilla campaign against Portuguese rule in September 1964. This conflict, along with the two others already initiated in the other Portuguese overseas territories of Angola and Portuguese Guinea, became part of the Portuguese Colonial War (1961–74). Several African territories under European rule had achieved independence in recent decades. Oliveira Salazar attempted to resist this tide and maintain the integrity of the Portuguese empire. By 1970, the anti-guerrilla war in Africa was consuming an important part of the Portuguese budget and there was no sign of a final solution in sight. This year was marked by a large-scale military operation in northern Mozambique, the Gordian Knot Operation, which displaced the FRELIMO's bases and destroyed much of the guerrillas' military capacity. At a military level, a part of Guinea-Bissau was de facto independent since 1973, but the capital and the major towns were still under Portuguese control. In Angola and Mozambique, independence movements were only active in a few remote countryside areas from where the Portuguese Army had retreated. However, their impending presence and the fact that they wouldn't go away dominated public anxiety. Throughout the war period Portugal faced increasing dissent, arms embargoes and other punitive sanctions imposed by most of the international community. For the Portuguese society the war was becoming even more unpopular due to its length and financial costs, the worsening of diplomatic relations with other United Nations members, and the role it had always played as a factor of perpetuation of the regime. It was this escalation that would led directly to the mutiny of members of the FAP in the Carnation Revolution in 1974 — an event that would led to the independence of the former Portuguese colonies in Africa. A leftist military coup in Lisbon on 24 April 1974 by the "" (MFA), overthrow the Estado Novo regime headed by Prime Minister Marcelo Caetano.
As one of the objectives of the MFA, all the Portuguese overseas territories in Africa were offered independence. FRELIMO took complete control of the Mozambican territory after a transition period, as agreed in the Lusaka Accord which recognized Mozambique's right to independence and the terms of the transfer of power.
Within a year of the Portuguese military coup at Lisbon, almost all Portuguese population had left the African territory as refugees (in mainland Portugal they were known as "") – some expelled by the new ruling power of Mozambique, some fleeing in fear. A parade and a state banquet completed the independence festivities in the capital, which was expected to be renamed Can Phumo, or "Place of Phumo", after a Shangaan chief who lived in the area before the Portuguese navigator founded the city in 1545 and gave his name to it. Most city streets, named for Portuguese heroes or important dates in Portuguese history, had their names changed.
Portuguese population's rapid exodus left the Mozambican economy in disarray. In addition, after the independence day on 25 June 1975, the eruption of the Mozambican Civil War (1977–92) destroyed the remaining wealth and left the former Portuguese Overseas Province in a state of absolute disrepair.
References.
Herrick, Allison and others (1969). "Area Handbook for Mozambique", US Government Printing Office.

</doc>
<doc id="45568" url="https://en.wikipedia.org/wiki?curid=45568" title="The Abyss">
The Abyss

The Abyss is a 1989 American science fiction film written and directed by James Cameron, starring Ed Harris, Mary Elizabeth Mastrantonio, and Michael Biehn. When an American submarine sinks in the Caribbean, the US search and recovery team works with an oil platform crew, racing against Russian vessels to recover the boat. Deep in the ocean, they encounter something unexpected.
This movie released on August 8, 1989 and received positive reviews, but underperformed at box office.
Plot.
In 1988, the US submarine USS "Montana" has an encounter with an unidentified submerged object and sinks near the Cayman Trough. With Soviet ships moving in to try to salvage the sub and a hurricane moving over the area, the US government opts to send a SEAL team to "Deep Core", a privately owned experimental underwater drilling platform near the Cayman Trough to use as a base of operations. The platform's designer, Dr. Lindsey Brigman, insists on coming along with the SEAL team, despite her estranged husband Virgil "Bud" Brigman being the current foreman.
During initial investigation of the "Montana", a power outage in the team's submersibles leads to Lindsey seeing a strange light circling the sub. At the same time, one of "Deep Core's" crew, "Jammer", damages his breathing apparatus in an apparent panic, and falls into a coma. This prompts the admiral in charge of the operation to send Lt. Coffey, the SEAL team leader, to take one of the mini-subs and recover a Trident missile warhead from the "Montana", just as the storm hits above. The "Benthic Explorer", which "Deep Core" is tethered to, is rocked by the storm, and the cable crane is torn from the ship. The crane falls into the trench and, without the mini-sub to disconnect the cable, "Deep Core" is dragged towards the trench, stopping just short of it. The rig is partially flooded, killing several crew members and damaging its power systems. Coffey shows little remorse when they return to the damaged base.
Lindsey is sent in dive gear to retrieve some oxygen bottles from a damaged portion of the rig to give the crew enough time to wait out the storm. While working, she's accosted by a small, maneuverable pink/purple device, followed by a much larger one. Before she can take a picture as proof, the large craft zooms downward into the trench, leaving her to take fuzzy, smeared pictures of the smaller one following it. She coins the term "non-terrestrial intelligence", or "NTI". As the crew struggles against the cold, they find an NTI has formed a living column of water and is exploring the base. Though they treat it with curiosity, Coffey is agitated by it and cuts it in half by closing a pressure bulkhead on it, causing it to retreat. The crew becomes convinced that Coffey is suffering paranoia from high-pressure nervous syndrome. Spying on him through a remote operated vehicle, they find he and another SEAL are arming the warhead to attack the NTIs, and race to stop him. Bud fights Coffey but Coffey escapes in a mini-sub with the primed warhead, and Bud and Lindsay give chase in the other sub. Coffey is able to launch to the warhead into the trench, but his sub is damaged and drifts over the edge of the trough, and he is crushed when the sub implodes from high pressures. The other mini-sub is also damaged and is taking on water; with only one functional diving suit, Lindsay opts to enter deep hibernation when the ocean's cold water engulfs her, and Bud swims back with her body to the platform. There, he and the crew administer CPR and revive her. Bud and Lindsay reaffirm their lost love.
One SEAL, unaware of Coffey's plan at the time, helps to locate the warhead, stopped on a ledge several thousand feet down the trench. Bud volunteers to use an experimental diving suit equipped with a liquid breathing apparatus to survive to that depth, though he will only be able to communicate through a keypad on the suit. Bud begins his dive, assisted by Lindsay's voice keeping him coherent against the effects of the mounting pressure, and reaches the warhead. The SEAL guides him in disarming it, but his only light source is yellow, making two high-contrast striped wires appear identical, forcing him to make a 50-50 choice on which wire to cut. With nearly no oxygen left in the system, Bud types out that he knew this was a one-way trip, and tells Lindsay he loves her. As he waits for death, an NTI approaches Bud and takes his hand. He is guided to an alien ship deeper in the trench. Deep inside, the NTI creates an atmospheric pocket for Bud, allowing him to breathe normally. The NTI plays back Bud's message to his wife and the two look at each other with understanding.
On "Deep Core" the crew is waiting for rescue when they see a message from Bud that he met some friends and warning them to hold on. The base shakes and lights from the trench bring the arrival of the alien ship. It rises to the ocean's surface, with "Deep Core" and several of the surface ships run aground on its hull. The crew of "Deep Core" leave the platform, surprised they aren't suffering from decompression sickness, when they see Bud walking out of the alien ship. Lindsay races to hug Bud.
Special Edition.
In the extended version, the events in the film are played against a backdrop of conflict between the United States and the Soviet Union, with the potential for all-out war; the sinking of the "Montana" fueling the aggression. There is more conflict between Bud and Lindsay in regards to their former relationship. The primary addition is the ending: when Bud is taken to the alien ship, they start by showing him images of war and aggression from news sources around the globe. The aliens then create massive tidal waves that threaten the world's coasts, but stop them short before they hit. They then show Bud his message to Lindsay, and let the tidal waves disperse without damage, as a warning to humanity.
Production.
H. G. Wells was the first to introduce the notion of a sea alien in his 1897 short story "In the Abyss". The idea for "The Abyss" came to James Cameron when, at age 17 and in high school, he attended a science lecture about deep sea diving by a man, Francis J. Falejczyk, who was the first human to breathe fluid through his lungs in experiments conducted by Dr. Johannes A. Kylstra. He subsequently wrote a short story that focused on a group of scientists in a laboratory at the bottom of the ocean. The basic idea did not change, but many of the details evolved over the years. Once Cameron arrived in Hollywood, he quickly realized that a group of scientists was not that commercial and changed it to a group of blue-collar workers. While making "Aliens", Cameron saw a "National Geographic" film about remote operated vehicles operating deep in the North Atlantic Ocean. These images reminded him of his short story. He and producer Gale Anne Hurd decided that "The Abyss" would be their next film. Cameron wrote a treatment combined with elements of a shooting script, which generated a lot of interest in Hollywood. He then wrote the script, basing the character of Lindsey on Hurd and finished it by the end of 1987. Cameron and Hurd were married before "The Abyss", separated during pre-production, and divorced in February 1989, two months after principal photography.
Pre-production.
The cast and crew trained for underwater diving for one week in the Cayman Islands. This was necessary because 40% of all live-action principal photography took place underwater. Furthermore, Cameron's production company had to design and build experimental equipment and develop a state-of-the-art communications system that allowed the director to talk underwater to the actors and dialogue to be recorded directly onto tape for the first time.
Cameron had originally planned to shoot on location in the Bahamas where the story was set but quickly realized that he needed to have a completely controlled environment because of the stunts and special visual effects involved. He considered shooting the film in Malta, which had the largest unfiltered tank of water, but it was not adequate for Cameron's needs. Underwater sequences for the film were shot at a unit of the Gaffney Studios, situated outside Gaffney, South Carolina, which had been abandoned by Duke Power officials after previously spending $700 million constructing the Cherokee Nuclear Power Plant.
Two specially constructed tanks were used. The first one, based on the abandoned plant's primary reactor containment vessel, held of water, was 55 feet (18 m) deep and 209 feet (70 m) across. At the time, it was the largest fresh-water filtered tank in the world. Additional scenes were shot in the second tank, an unused turbine pit, which held of water. As the production crew rushed to finish painting the main tank, millions of gallons of water poured in and took five days to fill. The Deepcore rig was anchored to a 90-ton concrete column at the bottom of the large tank. It consisted of six partial and complete modules that took over half a year to plan and build from scratch.
Can-Dive Services Ltd., a Canadian commercial diving company that specialized in “saturation” diving systems and underwater technology, specially manufactured the two working craft (Flatbed and Cab One) for the film. Two million dollars was spent on set construction.
Filming was also done at the largest underground lake in the world — a mine in Bonne Terre, Missouri, which was the background for several underwater shots.
Principal photography.
The main tank was not ready in time for the first day of principal photography. Cameron delayed filming for a week and pushed the smaller tank's schedule forward, demanding that it be ready weeks ahead of schedule. Filming eventually began on August 15, 1988, but there were still problems. On the first day of shooting in the main water tank, it sprang a leak and of water a minute rushed out. The studio brought in dam-repair experts to seal it. In addition, enormous pipes with elbow fittings had been improperly installed. There was so much water pressure in them that the elbows blew off.
Cameron's choice of cinematographer on the movie was Mikael Salomon - a US-based Danish national who would go on to work on other blockbusters such as "Backdraft" and "Arachnophobia" before moving into the director's slot on a myriad of films and TV shows including two editions of the acclaimed HBO WW2 series "Band of Brothers". Salomon used three cameras in watertight housings that were specially designed. Another special housing was designed for scenes that went from above-water dialogue to below-water dialogue. The filmmakers had to figure out how to keep the water clear enough to shoot and dark enough to look realistic at 2,000 feet (700 m), which was achieved by floating a thick layer of plastic beads in the water and covering the top of the tank with an enormous tarpaulin. Cameron wanted to see the actors' faces and hear their dialogue, and thus hired Western Space and Marine to engineer helmets which would remain optically clear underwater and installed state-of-the-art aircraft quality microphones into each helmet. Safety conditions were also a major factor with the installation of a decompression chamber on site, along with a diving bell and a safety diver for each actor.
The breathing fluid used in the film actually exists but has only been thoroughly investigated in animals. Over the previous 20 years it had been tested on several animals, who survived. The rat shown in the film was actually breathing fluid and survived unharmed, although the scene was censored in Britain for perceived cruelty to animals.
Ed Harris did not actually breathe the fluid. He held his breath inside a helmet full of liquid while being towed 30 feet (10 m) below the surface of the large tank. He recalled that the worst moments were being towed with fluid rushing up his nose and his eyes swelling up. Actors played their scenes at 33 feet (11 m), too shallow a depth for them to need decompression, and rarely stayed down for more than an hour at a time. Cameron and the 26-person underwater diving crew sank to 50 feet (17 m) and stayed down for five hours at a time. To avoid decompression sickness, they would have to hang from hoses halfway up the tank for as long as two hours, breathing pure oxygen.
The cast and crew endured over six months of grueling six-day, 70-hour weeks on an isolated set. At one point, Mary Elizabeth Mastrantonio had a physical and emotional breakdown on the set and on another occasion, Ed Harris burst into spontaneous sobbing while driving home. Cameron himself admitted, "I knew this was going to be a hard shoot, but even I had no idea just how hard. I don't "ever" want to go through this again". For example, for the scene where portions of the rig are flooded with water, he realized that he initially did not know how to minimize the sequence's inherent danger. It took him more than four hours to set up the shot safely. Actor Leo Burmester said, "Shooting "The Abyss" has been the hardest thing I've ever done. Jim Cameron is the type of director who pushes you to the edge, but he doesn't make you do anything he wouldn't do himself." A lightning storm caused a 200-foot (65 m) tear in the black tarpaulin covering the main tank. Repairing it would have taken too much time, so the production began shooting at night. In addition, blooming algae often reduced visibility to 20 feet (6 m) within hours. Over-chlorination led to divers' skin burning and exposed hair being stripped off or turning white.
As production went on, the slow pace and daily mental and physical strain of filming began to wear on the cast and crew. Mary Elizabeth Mastrantonio remembered, "We never started and finished any one scene in any one day". At one point, Cameron told the actors to relieve themselves in their wetsuits to save time between takes. While filming one of many takes of Mastrantonio's character's resuscitation scene- in which she was soaking wet, topless and repeatedly being slapped and pounded on the chest- the camera ran out of film, prompting Mastrantonio to storm off the set yelling, "We are not animals!" For some shots in the scene that focus on Ed Harris, he was yelling at thin air because Mastrantonio refused to film the scene again. Michael Biehn also grew frustrated by the waiting. He claimed that he was in South Carolina for five months and only acted for three to four weeks. He remembered one day being ten meters underwater and "suddenly the lights went out. It was so black I couldn't see my hand. I couldn't surface. I realized I might not get out of there." Harris recalled: "One day we were all in our dressing rooms and people began throwing couches out the windows and smashing the walls. We just had to get our frustrations out." Cameron responded to these complaints, saying, "For every hour they spent trying to figure out what magazine to read, we spent an hour at the bottom of the tank breathing compressed air." After 140 days and $4 million over budget, filming finally wrapped on December 8, 1988. Before the film's release, there were reports from South Carolina that Ed Harris was so upset by the physical demands of the film and Cameron's dictatorial directing style that he said he would refuse to help promote the motion picture. Harris later denied this rumor and helped promote the film. But after its release and initial promotion, Harris publicly refused to ever again discuss the film, saying "I'm never talking about it and never will." Mary Elizabeth Mastrantonio has also since brushed off the film, commenting, ""The Abyss" was a lot of things. Fun to make is not one of them."
Post-production.
To create the alien water tentacle, Cameron initially considered cel animation or a tentacle sculpted in clay and then animated via stop-motion techniques with water reflections projected onto it. Phil Tippett suggested Cameron contact Industrial Light & Magic. The special visual effects work was divided up among seven FX divisions with motion control work by Dream Quest Images and computer graphics and opticals by ILM. ILM designed a program to produce surface waves of differing sizes and kinetic properties for the pseudopod. For the moment where it mimics Bud and Lindsey's faces, Ed Harris had eight of his facial expressions scanned while twelve of Mastrantonio's were scanned via software used to create computer-generated sculptures. The set was photographed from every angle and digitally recreated so that the pseudopod could be accurately composited into the live-action footage. The company spent six months to create 75 seconds of computer graphics needed for the creature. The film was to have opened on July 4, 1989, but its release was delayed for more than a month by production and special effects problems.
Studio executives were nervous about the film's commercial prospects when preview audiences laughed at scenes of serious intent. Industry insiders said that the release delay was because nervous executives ordered the film's ending completely re-shot. There was also a question of the size of the film's budget. One executive claimed $47 million while "The Wall Street Journal" reported a figure of $60 million. Box office revenue tracker site "The Numbers" lists the production budget at $70 million. When promoting the film on "Late Night with David Letterman", Cameron himself said the production budget was $43 million. None of these figures include marketing or distribution costs.
Reception.
Box office.
"The Abyss" was a box office disappointment, released on August 9, 1989, in 1,533 theaters, where it grossed $9,319,797 on its opening weekend ranking #2 at the box office. It went on to make $54,461,047 in North America and $35,539,051 throughout the rest of the world for a worldwide total of $90,000,098, above 
its estimated $70 million production budget.
Critical response.
On Rotten Tomatoes, a review aggregator, "The Abyss" has "Certified Fresh" score of 89% based on 44 reviews with an average rating of 7.2 out of 10. The critical consensus states: "The utterly gorgeous special effects frequently overshadow the fact that "The Abyss" is also a totally gripping, claustrophobic thriller, complete with an interesting crew of characters." On Metacritic, the film has an average score of 62 out of 100, based on 14 critics indicating "generally favorable reviews".
"Newsweek" magazine's David Ansen, summarizing the theatrical release, wrote, "The payoff to "The Abyss" is pretty damn silly — a portentous "deus ex machina" that leaves too many questions unanswered and evokes too many other films." In her review for "The New York Times", Caryn James claimed that the film had "at least four endings," and "by the time the last ending of this two-and-a-quarter-hour film comes along, the effect is like getting off a demon roller coaster that has kept racing several laps after you were ready to get off." Chris Dafoe, in his review for "The Globe and Mail", wrote, "At its best, "The Abyss" offers a harrowing, thrilling journey through inky waters and high tension. In the end, however, this torpedo turns out to be a dud - it swerves at the last minute, missing its target and exploding ineffectually in a flash of fantasy and fairy-tale schtick."
While praising the film's first two hours as "compelling", the "Toronto Star" remarked, "But when Cameron takes the adventure to the next step, deep into the heart of fantasy, it all becomes one great big deja boo. If we are to believe what Cameron finds way down there, E.T. didn't really phone home, he went surfing and fell off his board." "USA Today" gave the film three out of four stars and wrote, "Most of this underwater blockbuster is 'good,' and at least two action set pieces are great. But the dopey wrap-up sinks the rest 20,000 leagues." In her review for "The Washington Post", Rita Kempley wrote that the film "asks us to believe that the drowned return to life, that the comatose come to the rescue, that driven women become doting wives, that Neptune cares about landlubbers. I'd sooner believe that Moby Dick could swim up the drainpipe." "Halliwell's Film Guide" claimed the film was "despite some clever special effects, a tedious, overlong fantasy that is more excited by machinery than people." Conversely, "Rolling Stone" magazine's Peter Travers enthused, "["The Abyss" is] the greatest underwater adventure ever filmed, the most consistently enthralling of the summer blockbusters...one of the best pictures of the year."
The reviews tallied therein contain reviews for both the theatrical release and the Special Edition. The release of the Special Edition in 1993 garnered much praise. Each giving it thumbs up, Siskel remarked, ""The Abyss" has been improved," and Ebert added, "It makes the film seem more well rounded." In the book "Reel Views 2", James Berardinelli comments, "James Cameron's "The Abyss" may be the most extreme example of an available movie that demonstrates how the vision of a director, once fully realized on screen, can transform a good motion picture into a great one."
Accolades.
"The Abyss" won the 1990 Oscar for Best Visual Effects (John Bruno, Dennis Muren, Hoyt Yeatman, and Dennis Skotak). It was also nominated for:
The studio unsuccessfully lobbied hard to get Michael Biehn nominated for the Academy Award for Best Supporting Actor.
Many other film organizations, such as the Academy of Science Fiction, Fantasy & Horror Films and the American Society of Cinematographers, also nominated "The Abyss". The film ended up winning a total of three other awards from these organizations.
Soundtrack.
The soundtrack to "The Abyss" was released on August 22, 1989.
Deluxe Edition.
Varese Sarabande, which released the original album, issued a limited edition (3000 copies) two-disc album in 2014 featuring the complete score.
"Disc One":
"Disc Two": Tracks 10-19 are bonus tracks.
History of the Special Edition.
Even as the film was in the first weeks of its 1989 theatrical release, rumors were circulating of a wave sequence missing from the film's end. As chronicled in the 1993 laserdisc Special Edition release and later in the 2000 DVD, the pressure to cut the film's running time stemmed from both distribution concerns and Industrial Light & Magic's then-inability to complete the required sequences. From the distributor's perspective, the looming three-hour length limited the number of times the film could be shown each day, assuming that audiences would be willing to sit through the entire film, though 1990's "Dances with Wolves" would shatter both industry-held notions. Further, test audience screenings revealed a surprisingly mixed reaction to the sequences as they appeared in their unfinished form; in post-screening surveys, they dominated both the "Scenes I liked most" and "Scenes I liked least" fields. Contrary to speculation, studio meddling was not the cause of the shortened length; Cameron held final cut as long as the film met a running time of roughly two hours and 15 minutes. He later noted, "Ironically, the studio brass were horrified when I said I was cutting the wave."
What emerges in the winnowing process is only the best stuff. And I think the overall caliber of the film is improved by that. I cut only two minutes of "Terminator". On "Aliens", we took out much more. I even reconstituted some of that in a special (TV) release version.
The sense of something being missing on "Aliens" was greater for me than on "The Abyss", where the film just got consistently better as the cut got along. The film must function as a dramatic, organic whole. When I cut the film together, things that read well on paper, on a conceptual level, didn't necessarily translate to the screen as well. I felt I was losing something by breaking my focus. Breaking the story's focus and coming off the main characters was a far greater detriment to the film than what was gained. The film keeps the same message intact at a thematic level, not at a really overt level, by working in a symbolic way.
Cameron elected to remove the sequences along with other, shorter scenes elsewhere in the film, reducing the running time from roughly two hours and 50 minutes to two hours and 20 minutes and diminishing his signature themes of nuclear peril and disarmament. Subsequent test audience screenings drew substantially better reactions.
Star Mary Elizabeth Mastrantonio publicly expressed regret about some of the scenes selected for removal from the film's theatrical cut: "There were some beautiful scenes that were taken out. I just wish we hadn't shot so much that isn't in the film."
Shortly after the film's premiere, Cameron and video editor Ed Marsh created a longer video cut of "The Abyss" for their own use that incorporated dailies. With the tremendous success of Cameron's "" in 1991, Lightstorm Entertainment secured a five-year, $500 million financing deal with 20th Century Fox for films produced, directed or written by Cameron. The contract allocated roughly $500,000 of the amount to complete "The Abyss". ILM was commissioned to finish the work they had started three years earlier, with many of the same people who had worked on it originally.
The CGI tools developed for "Terminator 2: Judgment Day" allowed ILM to complete the rumored tidal-wave sequence, as well as correcting flaws in rendering for all their other work done for the film.
The tidal wave sequence had originally been designed by ILM as a physical effect, using a plastic wave, but Cameron was dissatisfied with the end result, and the sequence was scrapped. By the time Cameron was ready to revisit "The Abyss", ILM's CGI prowess had finally progressed to an appropriate level, and the wave was rendered as a CGI effect. "Terminator 2: Judgment Day" screenwriter and frequent Cameron collaborator William Wisher had a cameo in the scene as a reporter in Santa Monica who catches the first tidal wave on camera.
When it was discovered that original production sound recordings had been lost, new dialogue and foley were recorded, but since Captain Kidd Brewer had died of a self-inflicted gunshot before he could return to re-loop his dialog, producers and editors had to lift his original dialogue tracks from the remaining optical-sound prints of the dailies. The Special Edition was therefore dedicated to his memory as a result.
As Alan Silvestri was not available to compose new music for the restored scenes, Robert Garrett, who had composed temporary music for the film's initial cutting in 1989, was chosen to create new music. The Special Edition was completed in December 1992, with 28 minutes added to the film, and saw a limited theatrical release in New York City and Los Angeles on February 26, 1993, and expanded to key cities nationwide in the following weeks.
On home video, in addition to the conventional two-tape VHS release, the first THX-certified LaserDisc title of the Special Edition Box Set was released in May 1993 and was a best seller for the rest of the year. Both the theatrical and SE editions remain available on DVD; however all available DVDs are non-anamorphic, with the exception of the Chinese DVD produced for Region 6 by Excel Media.
Adaptations.
Science-fiction author Orson Scott Card was hired to write a novelization of the film based on the screenplay and discussions with Cameron. He wrote back-stories for Bud, Lindsey and Coffey as a means not only of helping the actors define their roles, but also to justify some of their behavior and mannerisms in the film. Card also wrote the aliens as a colonizing species which preferentially sought high-pressure deep-water worlds to build their ships as they traveled further into the galaxy (their mothership was in orbit on the far side of the moon). The NTIs' knowledge of neuroanatomy and nanoscale manipulation of biochemistry was responsible for many of the "deus ex machina" aspects of the film.
A licensed interactive fiction video game based on the script was being developed for Infocom by Bob Bates, but was cancelled when Infocom was shut down by its then-parent company Activision. Sound Source Interactive later created an action video game entitled "The Abyss: Incident at Europa". The game takes place a few years after the film, where the player must find a cure for a deadly virus.

</doc>
<doc id="45569" url="https://en.wikipedia.org/wiki?curid=45569" title="Dedekind cut">
Dedekind cut

In mathematics, a Dedekind cut, named after Richard Dedekind, is a partition of the rational numbers into two non-empty sets "A" and "B", such that all elements of "A" are less than all elements of "B", and "A" contains no greatest element. Dedekind cuts are one method of construction of the real numbers.
The set "B" may or may not have a smallest element among the rationals. If "B" has a smallest element among the rationals, the "cut" corresponds to that rational. Otherwise, that cut defines a unique irrational number which, loosely speaking, fills the "gap" between "A" and "B". In other words, "A" contains every rational number "less than" the cut, and "B" contains every rational number "greater than or equal to" the cut. An irrational cut is equated to an irrational number which is in neither set. Every real number, rational or not, is equated to one and only one cut of rationals.
More generally, a Dedekind cut is a partition of a totally ordered set into two non-empty parts "A" and "B", such that "A" is closed downwards (meaning that for all "a" in "A", "x" ≤ "a" implies that "x" is in "A" as well) and "B" is closed upwards, and "A" contains no greatest element. See also completeness (order theory).
It is straightforward to show that a Dedekind cut among the real numbers is uniquely defined by the corresponding cut among the rational numbers. Similarly, every cut of reals is identical to the cut produced by a specific real number (which can be identified as the smallest element of the "B" set). In other words, the number line where every real number is defined as a Dedekind cut of rationals is a complete continuum without any further gaps.
Dedekind used the German word "" (cut) in a visual sense rooted in Euclidean geometry. His theorem asserting the completeness of the real number system is nevertheless a theorem about numbers and not geometry. Classical Euclidean geometry lacked a treatment of continuity (although Eudoxus did construct a sophisticated theory of incommensurable quantities such as formula_1 ): thus the very first proposition of the very first book of Euclid's geometry (constructing an equilateral triangle) was criticised by Pappus of Alexandria on the grounds that there was nothing in the axioms that asserted two intersecting circles in fact intersect in points. In David Hilbert's axiom system, continuity is provided by the Axiom of Archimedes, while in Alfred Tarski's system continuity is provided by what is essentially Dedekind's section. In mathematical logic, the identification of the real numbers with the real number line is provided by the Cantor–Dedekind axiom.
Representations.
It is more symmetrical to use the ("A","B") notation for Dedekind cuts, but each of "A" and "B" does determine the other. It can be a simplification, in terms of notation if nothing more, to concentrate on one "half" — say, the lower one — and call any downward closed set "A" without greatest element a "Dedekind cut".
If the ordered set "S" is complete, then, for every Dedekind cut ("A", "B") of "S", the set "B" must have a minimal element "b", 
hence we must have that "A" is the interval ( −∞, "b"), and "B" the interval ["b", +∞).
In this case, we say that "b" "is represented by" the cut ("A","B").
The important purpose of the Dedekind cut is to work with number sets that are "not" complete. The cut itself can represent a number not in the original collection of numbers (most often rational numbers). The cut can represent a number "b", even though the numbers contained in the two sets "A" and "B" do not actually include the number "b" that their cut represents.
For example if "A" and "B" only contain rational numbers, they can still be cut at √2 by putting every negative rational number in "A", along with every non-negative number whose square is less than 2; similarly "B" would contain every positive rational number whose square is greater than or equal to 2. Even though there is no rational value for √2, if the rational numbers are partitioned into "A" and "B" this way, the partition itself represents an irrational number.
Ordering of cuts.
Regard one Dedekind cut ("A", "B") as "less than" another Dedekind cut ("C", "D") (of the same superset) if "A" is a proper subset of "C". Equivalently, if "D" is a proper subset of "B", the cut ("A", "B") is again "less than" ("C", "D"). In this way, set inclusion can be used to represent the ordering of numbers, and all other relations ("greater than", "less than or equal to", "equal to", and so on) can be similarly created from set relations.
The set of all Dedekind cuts is itself a linearly ordered set (of sets). Moreover, the set of Dedekind cuts has the least-upper-bound property, i.e., every nonempty subset of it that has any upper bound has a "least" upper bound. Thus, constructing the set of Dedekind cuts serves the purpose of embedding the original ordered set "S", which might not have had the least-upper-bound property, within a (usually larger) linearly ordered set that does have this useful property.
Construction of the real numbers.
A typical Dedekind cut of the rational numbers is given by
This cut represents the irrational number √2 in Dedekind's construction. To establish this truly, one must show that this really is a cut and that it is the square root of two. However, neither claim is immediate. Showing that it is a cut requires showing that for any positive rational "x" with "x"2 < 2, there is a rational "y" with "x" < "y" and "y"2 < 2. The choice formula_4 works. Then we have a cut and it has a square no larger than 2, but to show equality requires showing that if "r" is any rational number less than 2, then there is positive "x" in "A" with "r" < "x"2.
Note that the equality "b"2 = 2 cannot hold since √2 is not rational.
Generalizations.
A construction similar to Dedekind cuts is used for the construction of surreal numbers.
Partially ordered sets.
More generally, if "S" is a partially ordered set, a "completion" of "S" means a complete lattice "L" with an order-embedding of "S" into "L". The notion of "complete lattice" generalizes the least-upper-bound property of the reals.
One completion of "S" is the set of its "downwardly closed" subsets, ordered by inclusion. A related completion that preserves all existing sups and infs of "S" is obtained by the following construction: For each subset "A" of "S", let "A"u denote the set of upper bounds of "A", and let "A"l denote the set of lower bounds of "A". (These operators form a Galois connection.) Then the Dedekind–MacNeille completion of "S" consists of all subsets "A" for which ("A"u)l = "A"; it is ordered by inclusion. The Dedekind-MacNeille completion is the smallest complete lattice with "S" embedded in it.

</doc>
<doc id="45570" url="https://en.wikipedia.org/wiki?curid=45570" title="DNA vaccination">
DNA vaccination

DNA vaccination is a technique for protecting an animal against disease by injecting it with genetically engineered DNA so cells directly produce an antigen, resulting in a protective immunological response. Several DNA vaccines have been released for veterinary use, and there has been promising research using the vaccines for viral, bacterial and parasitic diseases, as well as to several tumour types. Although only one DNA vaccine has been approved for human use, DNA vaccines may have a number of potential advantages over conventional vaccines, including the ability to induce a wider range of immune response types.
History.
Many believe vaccines are among the greatest achievements of modern medicine – in industrial nations, they have eliminated naturally occurring cases of smallpox, and nearly eliminated polio, while other diseases, such as typhus, rotavirus, hepatitis A and B and others are well controlled. Conventional vaccines, however, only cover a small number of diseases, and infections that lack effective vaccines kill millions of people every year, with AIDS, hepatitis C and malaria being particularly common.
"First generation" vaccines are whole-organism vaccines – either live and weakened, or killed forms. Live, attenuated vaccines, such as smallpox and polio vaccines, are able to induce killer T-cell (TC or CTL) responses, helper T-cell (TH) responses and antibody immunity. However, there is a small risk that attenuated forms of a pathogen can revert to a dangerous form, and may still be able to cause disease in immunocompromised vaccine recipients (such as those with AIDS). While killed vaccines do not have this risk, they cannot generate specific killer T cell responses, and may not work at all for some diseases.
"Second generation vaccines" were developed to reduce the risks from live vaccines. These are subunit vaccines, consisting of defined protein antigens (such as tetanus or diphtheria toxoid) or recombinant protein components (such as the hepatitis B surface antigen). These, too, are able to generate TH and antibody responses, but not killer T cell responses.
DNA vaccines are "third generation vaccines", and contain DNA coding specific proteins (antigens) from a pathogen. The vaccine DNA is injected into the cells of the body, where the "inner machinery" of the host cells "reads" the DNA and uses it to synthesize the pathogen's proteins. Because these proteins are recognised as foreign, when they are processed by the host cells and displayed on their surface, the immune system is alerted, which then triggers a range of immune responses.
Alternatively, the DNA maybe encapsulated in protein to facilitate entry into cells. If this capsid protein is also included in the DNA, the resulting vaccine hopes to combine the potency of a live vaccine without any reversion risks. In 1983, Enzo Paoletti and Dennis Panicali at the New York Department of Health devised a strategy to produce recombinant DNA vaccines by using genetic engineering techniques to transform ordinary smallpox vaccine into vaccines that may be able to prevent other diseases. They altered the DNA of cowpox virus by inserting a gene from other viruses (namely Herpes simplex virus, hepatitis B and influenza).
Current use.
No DNA vaccines have been approved for human use in the United States. Thus far, few experimental trials have evoked a response strong enough to protect against disease, and the usefulness of the technique, while tantalizing, remains to be conclusively proven in humans. As of June 2015 only one human DNA vaccine has been approved for human use, the single-dose Japanese encephalitis vaccine called IMOJEV, released in 2010. However, a veterinary DNA vaccine to protect horses from West Nile virus has been approved. In August 2007, a preliminary study in DNA vaccination against multiple sclerosis was reported as being effective.
Plasmid vectors for use in vaccination.
Vector design.
DNA vaccines elicit the best immune response when highly active expression vectors are used. These are plasmids which usually consist of a strong viral promoter to drive the in vivo transcription and translation of the gene (or complementary DNA) of interest. Intron A may sometimes be included to improve mRNA stability and hence increase protein expression. Plasmids also include a strong polyadenylation/transcriptional termination signal, such as bovine growth hormone or rabbit beta-globulin polyadenylation sequences. Multicistronic vectors are sometimes constructed to express more than one immunogen, or to express an immunogen and an immunostimulatory protein.
Because the plasmid is the “vehicle” from which the immunogen is expressed, optimising vector design for maximal protein expression is essential. One way of enhancing protein expression is by optimising the codon usage of pathogenic mRNAs for eukaryotic cells. Pathogens often have different AT-contents than the species being immunized, so altering the gene sequence of the immunogen to reflect the codons more commonly used in the target species may improve its expression.
Another consideration is the choice of promoter. The SV40 promoter was conventionally used until research showed that vectors driven by the Rous Sarcoma Virus (RSV) promoter had much higher expression rates. More recently, expression rates have been further increased by the use of the cytomegalovirus (CMV) immediate early promoter. Inclusion of the Mason-Pfizer monkey virus (MPV)-CTE with/without rev increased envelope expression. Furthermore the CTE+rev construct was significantly more immunogenic than CTE-alone vector. Additional modifications to improve expression rates have included the insertion of enhancer sequences, synthetic introns, adenovirus tripartite leader (TPL) sequences and modifications to the polyadenylation and transcriptional termination sequences. An example of DNA vaccine plasmid is pVAC, it uses SV40 promoter.
Structural instability phenomena are of particular concern for plasmid manufacture, DNA vaccination and gene therapy. Accessory regions pertaining to the plasmid backbone may engage in a wide range of structural instability phenomena. Well-known catalysts of genetic instability include direct, inverted, and tandem repeats, which are known to be conspicuous in a large number of commercially available cloning and expression vectors. Therefore, the reduction or complete elimination of extraneous noncoding backbone sequences would pointedly reduce the propensity for such events to take place, and consequently, the overall recombinogenic potential of the plasmid.
Mechanism of plasmids.
Once the plasmid inserts itself into the nucleus of the transfected cell, it starts to encode for a gene resulting in production of peptide string of a foreign antigen. The cell on its surface displays the foreign antigen with both histocompatibility complex (MHC) classes I and class II molecule. The antigen-presenting cell then travels to the lymph nodes and presents the antigen peptide and costimulatory molecule signaled by T-cell results in initiation of the immune response.
Vaccine insert design.
Immunogens can be targeted to various cellular compartments in order to improve antibody or cytotoxic T-cell responses. Secreted or plasma membrane-bound antigens are more effective at inducing antibody responses than cytosolic antigens, while cytotoxic T-cell responses can be improved by targeting antigens for cytoplasmic degradation and subsequent entry into the major histocompatibility complex (MHC) class I pathway. This is usually accomplished by the addition of N-terminal ubiquitin signals.
The conformation of the protein can also have an effect on antibody responses, with “ordered” structures (like viral particles) being more effective than unordered structures. Strings of minigenes (or MHC class I epitopes) from different pathogens are able to raise cytotoxic T-cell responses to a number of pathogens, especially if a TH epitope is also included.
Delivery methods.
DNA vaccines have been introduced into animal tissues by a number of different methods. These delivery methods are briefly reviewed in Table 2, with the advantages and disadvantages of the most commonly used methods summarised in Table 3.
The two most popular approaches are injection of DNA in saline, using a standard hypodermic needle, and gene gun delivery. A schematic outline of the construction of a DNA vaccine plasmid and its subsequent delivery by these two methods into a host is illustrated at Scientific American. Injection in saline is normally conducted intramuscularly (IM) in skeletal muscle, or intradermally (ID), with DNA being delivered to the extracellular spaces. This can be assisted by electroporation; by temporarily damaging muscle fibres with myotoxins such as bupivacaine; or by using hypertonic solutions of saline or sucrose. Immune responses to this method of delivery can be affected by many factors, including needle type, needle alignment, speed of injection, volume of injection, muscle type, and age, sex and physiological condition of the animal being injected.
Gene gun delivery, the other commonly used method of delivery, ballistically accelerates plasmid DNA (pDNA) that has been adsorbed onto gold or tungsten microparticles into the target cells, using compressed helium as an accelerant.
Alternative delivery methods have included aerosol instillation of naked DNA on mucosal surfaces, such as the nasal and lung mucosa, and topical administration of pDNA to the eye and vaginal mucosa. Mucosal surface delivery has also been achieved using cationic liposome-DNA preparations, biodegradable microspheres, attenuated "Shigella" or "Listeria" vectors for oral administration to the intestinal mucosa, and recombinant adenovirus vectors. Another alternative vector is a hybrid vehicle composed of bacteria cell and synthetic polymers. An "E. coli" inner core and poly(beta-amino ester) outer coat function synergistically to increase the gene delivery efficiency by addressing barriers associated with antigen-presenting cell gene delivery which include cellular uptake and internalization, phagosomal escape and intracellular cargo concentration. Tested in mice, the hybrid vector was found to induce immune response.
The method of delivery determines the dose of DNA required to raise an effective immune response. Saline injections require variable amounts of DNA, from 10 μg-1 mg, whereas gene gun deliveries require 100 to 1000 times less DNA than intramuscular saline injection to raise an effective immune response. Generally, 0.2 μg – 20 μg are required, although quantities as low as 16 ng have been reported. These quantities vary from species to species, with mice, for example, requiring approximately 10 times less DNA than primates. Saline injections require more DNA because the DNA is delivered to the extracellular spaces of the target tissue (normally muscle), where it has to overcome physical barriers (such as the basal lamina and large amounts of connective tissue, to mention a few) before it is taken up by the cells, while gene gun deliveries bombard DNA directly into the cells, resulting in less “wastage”.
Another approach to DNA vaccination is expression library immunization (ELI). Using this technique, potentially all the genes from a pathogen can be delivered at one time, which may be useful for pathogens which are difficult to attenuate or culture. ELI can be used to identify which of the pathogen’s genes induce a protective response. This has been tested with "Mycoplasma pulmonis", a murine lung pathogen with a relatively small genome, and it was found that even partial expression libraries can induce protection from subsequent challenge.
Immune response raised by DNA vaccines.
Helper T cell responses.
DNA immunization is able to raise a range of TH responses, including lymphoproliferation and the generation of a variety of cytokine profiles. A major advantage of DNA vaccines is the ease with which they can be manipulated to bias the type of T-cell help towards a TH1 or TH2 response. Each type of response has distinctive patterns of lymphokine and chemokine expression, specific types of immunoglobulins expressed, patterns of lymphocyte trafficking, and types of innate immune responses generated.
Raising of different types of T-cell help.
The type of T-cell help raised is influenced by the method of delivery and the type of immunogen expressed, as well as the targeting of different lymphoid compartments. Generally, saline needle injections (either IM or ID) tend to induce TH1 responses, while gene gun delivery raises TH2 responses. This is true for intracellular and plasma membrane-bound antigens, but not for secreted antigens, which seem to generate TH2 responses, regardless of the method of delivery.
Generally the type of T-cell help raised is stable over time, and does not change when challenged or after subsequent immunizations which would normally have raised the opposite type of response in a naïve animal. However, Mor "et al.". (1995) immunized and boosted mice with pDNA encoding the circumsporozoite protein of the mouse malarial parasite "Plasmodium yoelii" (PyCSP) and found that the initial TH2 response changed, after boosting, to a TH1 response.
Mechanistic basis for different types of T-cell help.
It is not understood how these different methods of DNA immunization, or the forms of antigen expressed, raise different profiles of T-cell help. It was thought that the relatively large amounts of DNA used in IM injection were responsible for the induction of TH1 responses. However, evidence has shown no differences in TH type due to dose. It has been postulated that the type of T-cell help raised is determined by the differentiated state of antigen presenting cells. Dendritic cells can differentiate to secrete IL-12 (which supports TH1 cell development) or IL-4 (which supports TH2 responses). pDNA injected by needle is endocytosed into the dendritic cell, which is then stimulated to differentiate for TH1 cytokine production, while the gene gun bombards the DNA directly into the cell, thus bypassing TH1 stimulation.
Practical uses of polarised T-cell help.
This polarisation in T-cell help is useful in influencing allergic responses and autoimmune diseases. In autoimmune diseases, the goal would be to shift the self-destructive TH1 response (with its associated cytotoxic T cell activity) to a non-destructive TH2 response. This has been successfully applied in predisease priming for the desired type of response in preclinical models and somewhat successful in shifting the response for an already established disease.
Cytotoxic T-cell responses.
One of the greatest advantages of DNA vaccines is that they are able to induce cytotoxic T lymphocytes (CTL) without the inherent risk associated with live vaccines. CTL responses can be raised against immunodominant and immunorecessive CTL epitopes, as well as subdominant CTL epitopes, in a manner which appears to mimic natural infection. This may prove to be a useful tool in assessing CTL epitopes of an antigen, and their role in providing immunity.
Cytotoxic T-cells recognise small peptides (8-10 amino acids) complexed to MHC class I molecules (Restifo et al., 1995). These peptides are derived from endogenous cytosolic proteins which are degraded and delivered to the nascent MHC class I molecule within the endoplasmic reticulum (ER). Targeting gene products directly to the ER (by the addition of an amino-terminal insertion sequence) should thus enhance CTL responses. This has been successfully demonstrated using recombinant vaccinia viruses expressing influenza proteins, but the principle should be applicable to DNA vaccines too. Targeting antigens for intracellular degradation (and thus entry into the MHC class I pathway) by the addition of ubiquitin signal sequences, or mutation of other signal sequences, has also been shown to be effective at increasing CTL responses.
CTL responses can also be enhanced by co-inoculation with co-stimulatory molecules such as B7-1 or B7-2 for DNA vaccines against influenza nucleoprotein, or GM-CSF for DNA vaccines against the murine malaria model "P. yoelii". Co-inoculation with plasmids encoding co-stimulatory molecules IL-12 and TCA3 have also been shown to increase CTL activity against HIV-1 and influenza nucleoprotein antigens.
Humoral (antibody) response.
Antibody responses elicited by DNA vaccinations are influenced by a number of variables, including type of antigen encoded; location of expressed antigen (i.e. intracellular vs. secreted); number, frequency and dose of immunizations; site and method of antigen delivery, to name a few.
Kinetics of antibody response.
Humoral responses after a single DNA injection can be much longer-lived than after a single injection with a recombinant protein. Antibody responses against hepatitis B virus (HBV) envelope protein (HBsAg) have been sustained for up to 74 weeks without boost, while lifelong maintenance of protective response to influenza haemagglutinin has been demonstrated in mice after gene gun delivery. Antibody-secreting cells migrate to the bone marrow and spleen for long-term antibody production, and are generally localised there after one year.
Comparisons of antibody responses generated by natural (viral) infection, immunization with recombinant protein and immunization with pDNA are summarised in Table 4. DNA-raised antibody responses rise much more slowly than when natural infection or recombinant protein immunization occurs. It can take as long as 12 weeks to reach peak titres in mice, although boosting can increase the rate of antibody production. This slow response is probably due to the low levels of antigen expressed over several weeks, which supports both primary and secondary phases of antibody response.
DNA vaccine expressing HBV small and middle envelope protein was injected into adults with chronic hepatitis. The vaccine resulted in specific interferon gamma cell production. Also specific T-cells for middle envelop proteins antigens were developed. The immune response of the patients was not robust enough to control HBV infection (Mancini - Bourgine et al.)
Additionally, the titres of specific antibodies raised by DNA vaccination are lower than those obtained after vaccination with a recombinant protein. However, DNA immunization-induced antibodies show greater affinity to native epitopes than recombinant protein-induced antibodies. In other words, DNA immunization induces a qualitatively superior response. Antibody can be induced after just one vaccination with DNA, whereas recombinant protein vaccinations generally require a boost. As mentioned previously, DNA immunization can be used to bias the TH profile of the immune response, and thus the antibody isotype, which is not possible with either natural infection or recombinant protein immunization. Antibody responses generated by DNA are useful not just in vaccination but as a preparative tool, too. For example, polyclonal and monoclonal antibodies can be generated for use as reagents.
Mechanistic basis for DNA raised immune responses.
DNA uptake mechanism.
When DNA uptake and subsequent expression was first demonstrated "in vivo" in muscle cells, it was thought that these cells were unique in this ability because of their extensive network of T-tubules. Using electron microscopy, it was proposed that DNA uptake was facilitated by caveolae (or, non-clathrin coated pits). However, subsequent research revealed that other cells (such as keratinocytes, fibroblasts and epithelial Langerhans cells) could also internalize DNA. This phenomenon has not been the subject of much research, so the actual mechanism of DNA uptake is not known.
Two theories are currently popular – that "in vivo" uptake of DNA occurs non-specifically, in a method similar to phago- or pinocytosis, or through specific receptors. These might include a 30kDa surface receptor, or macrophage scavenger receptors. The 30kDa surface receptor binds very specifically to 4500-bp genomic DNA fragments (which are then internalised) and is found on professional APCs and T-cells. Macrophage scavenger receptors bind to a variety of macromolecules, including polyribonucleotides, and are thus also candidates for DNA uptake. Receptor mediated DNA uptake could be facilitated by the presence of polyguanylate sequences. Further research into this mechanism might seem pointless, considering that gene gun delivery systems, cationic liposome packaging, and other delivery methods bypass this entry method, but understanding it might be useful in reducing costs (e.g. by reducing the requirement for cytofectins), which will be important in the food animals industry.
Antigen presentation by bone marrow-derived cells.
Studies using chimeric mice have shown that antigen is presented by bone-marrow derived cells, which include dendritic cells, macrophages and specialised B-cells called professional antigen presenting cells (APC). After gene gun inoculation to the skin, transfected Langerhans cells migrate to the draining lymph node to present antigen. After IM and ID injections, dendritic cells have also been found to present antigen in the draining lymph node and transfected macrophages have been found in the peripheral blood.
Besides direct transfection of dendritic cells or macrophages, cross priming is also known to occur following IM, ID and gene gun deliveries of DNA. Cross priming occurs when a bone marrow-derived cell presents peptides from proteins synthesised in another cell in the context of MHC class 1. This can prime cytotoxic T-cell responses and seems to be important for a full primary immune response.
Role of the target site.
IM and ID delivery of DNA initiate immune responses differently. In the skin, keratinocytes, fibroblasts and Langerhans cells take up and express antigen, and are responsible for inducing a primary antibody response. Transfected Langerhans cells migrate out of the skin (within 12 hours) to the draining lymph node where they prime secondary B- and T-cell responses. In skeletal muscle, on the other hand, striated muscle cells are most frequently transfected, but seem to be unimportant in mounting an immune response. Instead, IM inoculated DNA “washes” into the draining lymph node within minutes, where distal dendritic cells are transfected and then initiate an immune response. Transfected myocytes seem to act as a “reservoir” of antigen for trafficking professional APCs.
Maintenance of immune response.
DNA vaccination generates an effective immune memory via the display of antigen-antibody complexes on follicular dendritic cells (FDC), which are potent B-cell stimulators. T-cells can be stimulated by similar, germinal centre dendritic cells. FDC are able to generate an immune memory because antibodies production “overlaps” long-term expression of antigen, allowing antigen-antibody immunocomplexes to form and be displayed by FDC.
Interferons.
Both helper and cytotoxic T-cells can control viral infections by secreting interferons. Cytotoxic T cells usually kill virally infected cells. However, they can also be stimulated to secrete antiviral cytokines such as IFN-γ and TNF-α, which don’t kill the cell but place severe limitations on viral infection by down-regulating the expression of viral components. DNA vaccinations can thus be used to curb viral infections by non-destructive IFN-mediated control. This has been demonstrated for the hepatitis B virus. IFN-γ is also critically important in controlling malaria infections, and should be taken into consideration when developing anti-malarial DNA vaccines.
Modulation of the immune response.
Cytokine modulation.
For a vaccine to be effective, it must induce an appropriate immune response for a given pathogen, and the ability of DNA vaccines to polarise T-cell help towards TH1 or TH2 profiles, and generate CTL and/or antibody when required, is a great advantage in this regard. This can be accomplished by modifications to the form of antigen expressed (i.e. intracellular vs. secreted), the method and route of delivery, and the dose of DNA delivered. However, it can also be accomplished by the co-administration of plasmid DNA encoding immune regulatory molecules, i.e. cytokines, lymphokines or co-stimulatory molecules. These “genetic adjuvants” can be administered a number of ways:
In general, co-administration of pro-inflammatory agents (such as various interleukins, tumor necrosis factor, and GM-CSF) plus TH2 inducing cytokines increase antibody responses, whereas pro-inflammatory agents and TH1 inducing cytokines decrease humoral responses and increase cytotoxic responses (which is more important in viral protection, for example). Co-stimulatory molecules like B7-1, B7-2 and CD40L are also sometimes used.
This concept has been successfully applied in topical administration of pDNA encoding IL-10. Plasmid encoding B7-1 (a ligand on APCs) has successfully enhanced the immune response in anti-tumour models, and mixing plasmids encoding GM-CSF and the circumsporozoite protein of P. yoelii (PyCSP) has enhanced protection against subsequent challenge (whereas plasmid-encoded PyCSP alone did not). It was proposed that GM-CSF may cause dendritic cells to present antigen more efficiently, and enhance IL-2 production and TH cell activation, thus driving the increased immune response. This can be further enhanced by first priming with a pPyCSP and pGM-CSF mixture, and later boosting with a recombinant poxvirus expressing PyCSP. However, co-injection of plasmids encoding GM-CSF (or IFN-γ, or IL-2) and a fusion protein of "P. chabaudi" merozoite surface protein 1 (C-terminus)-hepatitis B virus surface protein (PcMSP1-HBs) actually abolished protection against challenge, compared to protection acquired by delivery of pPcMSP1-HBs alone.
The advantages of using genetic adjuvants are their low cost and simplicity of administration, as well as avoidance of unstable recombinant cytokines and potentially toxic, “conventional” adjuvants (such as alum, calcium phosphate, monophosphoryl lipid A, cholera toxin, cationic and mannan-coated liposomes, QS21, carboxymethylcellulose and ubenimix). However, the potential toxicity of prolonged cytokine expression has not been established, and in many commercially important animal species, cytokine genes still need to be identified and isolated. In addition, various plasmid encoded cytokines modulate the immune system differently according to the time of delivery. For example, some cytokine plasmid DNAs are best delivered after the immunogen pDNA, because pre- or co-delivery can actually decrease specific responses, and increase non-specific responses.
Immunostimulatory CpG motifs.
Plasmid DNA itself appears to have an adjuvant effect on the immune system. Bacterially derived DNA has been found to trigger innate immune defence mechanisms, the activation of dendritic cells, and the production of TH1 cytokines. This is due to recognition of certain CpG dinucleotide sequences which are immunostimulatory. CpG stimulatory (CpG-S) sequences occur twenty times more frequently in bacterially derived DNA than in eukaryotes. This is because eukaryotes exhibit “CpG suppression” – i.e. CpG dinucleotide pairs occur much less frequently than expected. Additionally, CpG-S sequences are hypomethylated. This occurs frequently in bacterial DNA, while CpG motifs occurring in eukaryotes are all methylated at the cytosine nucleotide. In contrast, nucleotide sequences which inhibit the activation of an immune response (termed CpG neutralising, or CpG-N) are over represented in eukaryotic genomes. The optimal immunostimulatory sequence has been found to be an unmethylated CpG dinucleotide flanked by two 5’ purines and two 3’ pyrimidines. Additionally, flanking regions outside this immunostimulatory hexamer must be guanine-rich to ensure binding and uptake into target cells.
The innate system works synergistically with the adaptive immune system to mount a response against the DNA encoded protein. CpG-S sequences induce polyclonal B-cell activation and the upregulation of cytokine expression and secretion. Stimulated macrophages secrete IL-12, IL-18, TNF-α, IFN-α, IFN-β and IFN-γ, while stimulated B-cells secrete IL-6 and some IL-12.
Manipulation of CpG-S and CpG-N sequences in the plasmid backbone of DNA vaccines can ensure the success of the immune response to the encoded antigen, and drive the immune response toward a TH1 phenotype. This is useful if a pathogen requires a TH response for protection. CpG-S sequences have also been used as external adjuvants for both DNA and recombinant protein vaccination with variable success rates. Other organisms with hypomethylated CpG motifs have also demonstrated the stimulation of polyclonal B-cell expansion. However, the mechanism behind this may be more complicated than simple methylation – hypomethylated murine DNA has not been found to mount an immune response.
Most of the evidence for the existence of immunostimulatory CpG sequences comes from murine studies. Clearly, extrapolation of this data to other species should be done with caution – different species may require different flanking sequences, as binding specificities of scavenger receptors differ between species. Additionally, species such as ruminants may be insensitive to immunostimulatory sequences due to the large gastrointestinal load they exhibit. Further research may be useful in the optimisation of DNA vaccination, especially in the food animal industry.
Alternative boosts.
DNA-primed immune responses can be boosted by the administration of recombinant protein or recombinant poxviruses. “Prime-boost” strategies with recombinant protein have successfully increased both neutralising antibody titre, and antibody avidity and persistence, for weak immunogens, such as HIV-1 envelope protein. Recombinant virus boosts have been shown to be very efficient at boosting DNA-primed CTL responses. Priming with DNA focuses the immune response on the required immunogen, while boosting with the recombinant virus provides a larger amount of expressed antigen, leading to a large increase in specific CTL responses.
Prime-boost strategies have been successful in inducing protection against malarial challenge in a number of studies. Primed mice with plasmid DNA encoding "Plasmodium yoelii" circumsporozoite surface protein (PyCSP), then boosted with a recombinant vaccinia virus expressing the same protein had significantly higher levels of antibody, CTL activity and IFN-γ, and hence higher levels of protection, than mice immunized and boosted with plasmid DNA alone. This can be further enhanced by priming with a mixture of plasmids encoding PyCSP and murine GM-CSF, before boosting with recombinant vaccinia virus. An effective prime-boost strategy for the simian malarial model "P. knowlesi" has also been demonstrated. Rhesus monkeys were primed with a multicomponent, multistage DNA vaccine encoding two liver-stage antigens - the circumsporozoite surface protein (PkCSP) and sporozoite surface protein 2 (PkSSP2) - and two blood stage antigens - the apical merozoite surface protein 1 (PkAMA1) and merozoite surface protein 1 (PkMSP1p42). They were then boosted with a recombinant canarypox virus encoding all four antigens (ALVAC-4). Immunized monkeys developed antibodies against sporozoites and infected erythrocytes, and IFN-γ-secreting T-cell responses against peptides from PkCSP. Partial protection against sporozoite challenge was achieved, and mean parasitemia was significantly reduced, compared to control monkeys. These models, while not ideal for extrapolation to "P. falciparum" in humans, will be important in pre-clinical trials.
Additional methods of enhancing DNA-induced immune responses.
Formulations of DNA.
The efficiency of DNA immunization can be improved by stabilising DNA against degradation, and increasing the efficiency of delivery of DNA into antigen presenting cells. This has been demonstrated by coating biodegradable cationic microparticles (such as poly(lactide-co-glycolide) formulated with cetyltrimethylammonium bromide) with DNA. Such DNA-coated microparticles can be as effective at raising CTL as recombinant vaccinia viruses, especially when mixed with alum. Particles 300 nm in diameter appear to be most efficient for uptake by antigen presenting cells.
Alphavirus vectors.
Recombinant alphavirus-based vectors have also been used to improve DNA vaccination efficiency. The gene encoding the antigen of interest is inserted into the alphavirus replicon, replacing structural genes but leaving non-structural replicase genes intact. The Sindbis virus and Semliki Forest virus have been used to build recombinant alphavirus replicons. Unlike conventional DNA vaccinations, however, alphavirus vectors kill transfected cells, and are only transiently expressed. Also, alphavirus replicase genes are expressed in addition to the vaccine insert. It is not clear how alphavirus replicons raise an immune response, but it is thought that this may be due to the high levels of protein expressed by this vector, replicon-induced cytokine responses, or replicon-induced apoptosis leading to enhanced antigen uptake by dendritic cells.

</doc>
<doc id="45575" url="https://en.wikipedia.org/wiki?curid=45575" title="Southern Schleswig">
Southern Schleswig

Southern Schleswig ( or "Landesteil Schleswig", ) is the southern half of the former Duchy of Schleswig in Germany on the Jutland Peninsula. The geographical area today covers the large area between the Eider river in the south and the Flensburg Fjord in the north, where it borders Denmark. Northern Schleswig, congruent with the former South Jutland County. The area belonged to the Crown of Denmark until the Prussians and Austrian declared war on Denmark in 1864. Denmark wanted to give away the German speaking Holsten and set the new border at the small river Ejderen. This was a reason for war, did Prussian chancellor Otto von Bismarck conclude, and even proclaimed it as a "holy war". The German chancellor also turned himself to the Emperor of Austria, Franz Joseph I of Austria for help. A similar war in 1848 had got all wrong for the Prussians. With help of both the Austrians and the Danish born General Moltke was the Danish army destroyed or forced to make disordered retreat. And the Prussian - Danish border was moved from the Elbe up in Jutland to the creek "Kongeåen".
After the First world war did two referendums decide a new border 
The northern part went back to Denmark as "Nordslesvig" (North Slesvig). But the middle and southern part including Schleswig's only city, Flensburg, remained in what now was German hands, rather than Prussian ones. In Denmark did the loss of "Flensborg" cause a political crisis, "Påskekrisen" or the Easter Crisis, as it happened during the Easter of 1920.
But Südschleswig including the city of Flensburg remained as a part of Prussia in the new German republic, the Weimar Republic. Also after the Second world war did the area remain as German territory, and together with Holstein was the new Federal Republic of Schleswig-Holstein founded along with the Federal Republic of Germany (or Western Germany) in 1948. Although Hitler occupied Denmark between 9.April.1940 and the late evening of 4.May.1945
(with exception of the Baltic island of Bornholm, which was occupied by USSR-forces between 9.May.1945 and 5.April.1946), did he never annex any part of Denmark. Perhaps Hitler told the truth, when he wrote "The new Germany must grow toward the east". But the Belgium cities of Eupen and Malmedy was re-annexed to the Third Reich in the summer of 1940. After the First world war had they been given to Belgium as a part of the Versailles Treaty.
History.
The Schleswig lands north of the Eider river and the Bay of Kiel had been a fief of the Danish Crown since the Early Middle Ages. The southern Holstein region belonged to Francia and later to the Holy Roman Empire, it was however held as an Imperial fief by the Danish kings since the 1460 Treaty of Ribe.
The Schleswig-Holstein Question at first culminated in the course of the Revolutions of 1848, when from 1848 to 1851 revolting German-speaking National liberals backed by Prussia fought for the separation of Schleswig and Holstein from Denmark in the First Schleswig War. Though the "status quo" was restored, the conflict lingered on and on 1 February 1864 the Prussian and Austrian troops crossed the Eider sparking off the Second Schleswig War, after which Denmark had to cede Schleswig and Holstein according to the Treaty of Vienna. After the Austro-Prussian War of 1866, victorious Prussia took control over all Schleswig and Holstein but was obliged by the Peace of Prague to hold a referendum in predominantly Danish-speaking Northern Schleswig, which it never did.
After the German defeat in World War I the Schleswig Plebiscites were decreed by the Treaty of Versailles, in which the present-day German-Danish border was drawn taking effect on 15 June 1920, dividing Schleswig in a southern and northern part and leaving a considerable Danish and German minority on both sides 
Today.
Southern Schleswig is part of the German state ("Bundesland") of Schleswig-Holstein, therefore its denotation as "Landesteil Schleswig". It does not however form an administrative entity, but consists of the districts ("Landkreise") of Schleswig-Flensburg, Nordfriesland, the urban district ("Kreisfreie Stadt") of Flensburg and the northern part of Rendsburg-Eckernförde.
Beside Standard German, Low Saxon dialects (Schleswigsch) are spoken, as well as Danish (Standard Danish or South Schleswig Danish) and its South Jutlandic variant, furthermore North Frisian in the west. Danish and North Frisian are official minority languages. Many of the inhabitants who only speak German and not Danish do not consider the region any different from the rest of Schleswig-Holstein. This notion is disputed by those defining themselves as Danes, South Schleswigans or Schleswigans, particularly historians and people organised in the institutions of the Danish minority of Southern Schleswig, such as the South Schleswig Voter Federation. Many of the Last names found in the region are very often of Scandinavian or Danish form, with the "-sen" endings like Petersen.
The major cities of Southern Schleswig are Flensburg, Rendsburg, the city of Schleswig, and Husum.

</doc>
<doc id="45576" url="https://en.wikipedia.org/wiki?curid=45576" title="County Donegal">
County Donegal

County Donegal (pronounced or ; ) is a county in Ireland. It is part of the Border Region and is in the province of Ulster. It is named after the town of Donegal ("Dún na nGall") in the south of the county. Donegal County Council is the local council for the county and Lifford serves as the county town. The population of the county was 161,137 according to the 2011 census. It has also been known as (County) Tyrconnell ("Tír Chonaill"), after the historic territory of the same name.
Geography and political subdivisions.
In terms of size and area, it is the largest county in Ulster and the fourth-largest county in all of Ireland. Uniquely, County Donegal shares a small border with only one other county in the Republic of Ireland – County Leitrim. The greater part of its land border is shared with three counties of Northern Ireland: County Londonderry, County Tyrone and County Fermanagh. This geographic isolation from the rest of the Republic has led to Donegal people maintaining a distinct cultural identity and has been used to market the county with the slogan "Up here it's different". While Lifford is the county town, Letterkenny is by far the largest town in the county with a population of 19,588. Letterkenny and the nearby city of Derry form the main economic axis of the northwest of Ireland. Indeed, what became the City of Derry was officially part of County Donegal up until 1610.
Baronies.
There are eight historic baronies in the county:
Informal districts.
The county may be informally divided into a number of traditional districts. There are two Gaeltacht districts in the west: The Rosses (), centred on the town of Dungloe (), and Gweedore (). Another Gaeltacht district is located in the north-west: Cloughaneely (), centred on the town of Falcarragh (). The most northerly part of the island of Ireland is the location for three peninsulas of outstanding natural beauty: Inishowen, Fanad and Rosguill. The main population centre of Inishowen, Ireland's largest peninsula, is Buncrana. In the east of the county lies the Finn Valley (centred on Ballybofey). The Laggan district (not to be confused with the more famous Lagan Valley in the south of County Antrim) is centred on the town of Raphoe. 
Demographics.
According to the 1841 Census, County Donegal had a population of 296,000 people. As a result of famine and emigration, the population had reduced by 41,000 by 1851 and further reduced by 18,000 by 1861. By the time of the 1951 Census the population was only 44% of what it had been in 1841. The 2006 Census, undertaken by the State's Central Statistics Office, had County Donegal's population standing at 147,264. According to the 2011 Census, the county's population had grown to 161,137.
Physical geography.
The county is the most mountainous in Ulster consisting chiefly of two ranges of low mountains; the Derryveagh Mountains in the north and the Bluestack Mountains in the south, with Mount Errigal at the highest peak. It has a deeply indented coastline forming natural sea loughs, of which both Lough Swilly and Lough Foyle are the most notable. The Slieve League cliffs are the sixth-highest sea cliffs in Europe, while Malin Head is the most northerly point on the island of Ireland.
The climate is temperate and dominated by the Gulf Stream, with warm, damp summers and mild wet winters. Two permanently inhabited islands, Arranmore and Tory Island, lie off the coast, along with a large number of islands with only transient inhabitants. Ireland's second longest river, the Erne, enters Donegal Bay near the town of Ballyshannon. The River Erne, along with other Donegal waterways, has been dammed to produce hydroelectric power. The River Foyle separates part of County Donegal from parts of both counties Londonderry and Tyrone.
Botany.
A survey of the macroscopic marine algae of County Donegal was published in 2003. The survey was compiled using the algal records held in the herbaria of the following institutions: the Ulster Museum, Belfast; Trinity College, Dublin; National University of Ireland, Galway, and the Natural History Museum, London.
Records of flowering plants include: "Dactylorhiza purpurella" (Stephenson and Stephenson) Soó.
Zoology.
The animals included in the county include Badgers ("Meles meles" L.)
There are habitats for the rare Corncrake in the county.
History.
At various times in its history, it has been known as County Tirconaill, County Tirconnell or County Tyrconnell (). The former was used as its official name during 1922–1927. This is in reference to both the old "túath" of Tír Chonaill and the earldom that succeeded it.
County Donegal is famous for being the home of the once mighty Clann Dálaigh, whose most famous branch were the Clann Ó Domhnaill, better known in English as the O'Donnell Clan. Until around 1600, the O'Donnells were one of Ireland's richest and most powerful "Gaelic" (native Irish) ruling-families. Within the Province of Ulster only the Clann Uí Néill (known in English as the O'Neill Clan) of modern County Tyrone were more powerful. The O'Donnells were Ulster's second most powerful "clan" or ruling-family from the early 13th-century through to the start of the 17th-century. For several centuries the O'Donnells ruled Tír Chonaill, a Gaelic kingdom in West Ulster that covered almost all of modern County Donegal. The head of the O'Donnell family had the titles "An Ó Domhnaill" (meaning "The O'Donnell" in English) and "Rí Thír Chonaill" (meaning "King of Tír Chonaill" in English). Based at Donegal Castle in "Dún na nGall" (modern Donegal Town), the O'Donnell "Kings of Tír Chonaill" were traditionally "inaugurated" at Doon Rock near Kilmacrenan. O'Donnell royal or "chiefly" power was finally ended in what was then the newly created County Donegal in September 1607, following the Flight of the Earls from near Rathmullan. The modern "County Arms of Donegal" (dating from the early 1970s) was influenced by the design of the old O'Donnell royal arms. The "County Arms" is the official coat of arms of both County Donegal and Donegal County Council.
The modern County Donegal was shired by order of the English Crown in 1585. The English authorities at Dublin Castle formed the new county by amalgamating the old Kingdom of Tír Chonaill with the old Lordship of Inishowen. However, the English authorities were unable to establish control over Tír Chonaill and Inishowen until after the Battle of Kinsale in 1602. Full control over the new County Donegal was only achieved after the Flight of the Earls in September 1607. It was the centre of O'Doherty's Rebellion of 1608 with the key Battle of Kilmacrennan taking place there. The county was one of those 'planted' during the Plantation of Ulster from around 1610 onwards. What became the City of Derry was officially part of County Donegal up until 1610.
County Donegal was one of the worst affected parts of Ulster during the Great Famine of the late 1840s in Ireland. Vast swathes of the county were devastated by this catastrophe, many areas becoming permanently depopulated. Vast numbers of County Donegal's people emigrated at this time, chiefly through Londonderry Port.
The Partition of Ireland in the early 1920s had a massive direct impact on County Donegal. Partition cut the county off, economically and administratively, from Derry, which had acted for centuries as the county's main port, transport hub and financial centre. Derry, together with west Tyrone, was henceforward in a new, different jurisdiction officially called Northern Ireland. Partition also meant that County Donegal was now almost entirely cut off from the rest of the jurisdiction in which it now found itself, the new dominion called the Irish Free State, which in April 1949 became the Republic of Ireland. Only a few miles of the county is physically connected by land to the rest of the Republic. The existence of a border cutting Donegal off from her natural hinterlands in Derry City and West Tyrone greatly exacerbated the economic difficulties of the county after partition. The county's economy is particularly susceptible, just like that of Derry City, to the currency fluctuations of the Euro against sterling.
Added to all this, in the late 20th century County Donegal was adversely affected by The Troubles in Northern Ireland. The county suffered several bombings and assassinations. In June 1987, Constable Samuel McClean, a Donegal man who was a serving member of the Royal Ulster Constabulary, was shot dead by the Provisional Irish Republican Army at his family home near Drumkeen. In May 1991, the prominent Sinn Féin politician Councillor Eddie Fullerton was assassinated by the Ulster Defence Association at his home in Buncrana. This added further to the economic and social difficulties of the county. However, the greater economic and administrative integration following the Good Friday Agreement of April 1998 has been of benefit to the county.
It has been labelled the 'forgotten county' by its own politicians, owing to the perception that it is ignored by the Irish Government, even in times of crisis.
Irish language.
Much of the county is seen as being a bastion of Gaelic culture and the Irish language, the Donegal Gaeltacht being the second-largest in the country. The version of the Irish language spoken in County Donegal is Ulster Irish.
Of the Gaeltacht population of 24,744, 16% of the county's total, 17,132 say they can speak Irish. There are three Irish-speaking parishes: Gweedore, The Rosses and Cloughaneely. Other Irish-speaking areas include Gaeltacht an Láir: Glencolmcille, Fanad and Rosguill, the islands of Aranmore, Tory Island and Inishbofin. Gweedore is the largest Irish-speaking parish, with over 5,000 inhabitants. All schools in the region use Irish as the language of instruction. One of the constituent colleges of NUI Galway, Acadamh na hOllscolaíochta Gaeilge, is based in Gweedore.
There are 1,005 students attending the five Gaelscoileanna and two Gaelcholáistí in the rest of the county. According to the 2006 Census, there are also 7,218 people who identify as being daily Irish speakers outside the Gaeltacht in the rest of the county.
Government and politics.
Donegal County Council (which has officially been in existence since 1899) has responsibility for local administration, and is headquartered at the County House in Lifford. Until 2014, there were also Town Councils in Letterkenny, Bundoran, Ballyshannon and Buncrana. The Town Councils were abolished in June 2014 when the Local Government Reform Act 2014 was implemented and their functions were taken over by Donegal County Council. Elections to the County Council take place every five years. Thirty seven councillors are elected using the system of Proportional representation-Single Transferable Vote (STV). For the purpose of elections the county is divided into 5 Muncipal Districts comprising the following local electoral areas: Donegal (6), Glenties (6), Inishowen (9), Letterkenny (10) and Stranorlar (6).
For general elections, the county-wide constituency elects five representatives to Dáil Éireann. For elections to the European Parliament, the county is part of the Midlands–North-West constituency.
Voters have a reputation nationally for being "conservative and contrarian", the county having achieved prominence for having rejected the Fiscal Treaty in 2012 and both the Lisbon Treaty votes.
Freedom of Donegal.
The Freedom of Donegal is an award that is given to people who have been recognised for outstanding achievements on behalf of the people and county of Donegal. Such people include Daniel O' Donnell, Phil Coulter, Shay Given, Packie Bonner, Paddy Crerand and the Brennan family.
In 2009 the members of the 28th Infantry Battalion of the Irish Defence Forces were also awarded the Freedom of the County from Donegal County Council ""in recognition of their longstanding service to the County of Donegal"."
Access.
An extensive rail network used to exist throughout the county and was mainly operated by the County Donegal Railways Joint Committee and the Londonderry and Lough Swilly Railway Company (known as the L. & L.S.R. or the Lough Swilly Company for short). Unfortunately all these lines were laid to a 3-foot gauge where the connecting lines were all laid to the Irish standard gauge of . This meant that all goods had to be transhipped at Derry and Strabane. Like all narrow gauge railways this became a major handicap after World War 1 when road transport began to seriously erode the railways goods traffic. By 1953 the Lough Swilly had closed its entire railway system and become a bus and road haulage concern. The County Donegal lasted until 1960 as it had largely dieselised its passenger trains by 1951. By the late 1950s major work was required to upgrade the track and the Irish Government was unwilling to supply the necessary funds, so 'the Wee Donegal', as it was affectionally known, was closed in 1960. The Great Northern Railway (Ireland) L.t.d. (the G.N.R.) also ran a line from Strabane through The Laggan, a district in the east of the county, along the River Foyle into Derry. However, the railway network within County Donegal was completely closed by 1960. Today, the closest railway station to the county is Waterside Station in the City of Derry, which is operated by Northern Ireland Railways (N.I.R.). Train services along the Belfast-Derry railway line run, via Coleraine, to Belfast Central and Belfast Great Victoria Street.
County Donegal is served by both Donegal Airport, located at Carrickfinn in The Rosses in the west of the county, and by City of Derry Airport, located at Eglinton to the east. The nearest main international airport to the county is Belfast International Airport (popularly known as Aldergrove Airport), which is located to the east at Aldergrove, near Antrim Town, in County Antrim, from Derry City and from Letterkenny.
Culture.
The variant of the Irish language spoken in Donegal shares many traits with Scottish Gaelic. The Irish spoken in the Donegal Gaeltacht (Irish-speaking area) is of the Ulster dialect, while Inishowen (parts of which only became English-speaking in the early 20th century) used the East Ulster dialect. Ulster Scots is often spoken in both the Finn Valley and The Laggan district of East Donegal. Donegal Irish has a strong influence on learnt Irish across Ulster.
Like other areas on the western seaboard of Ireland, Donegal has a distinctive fiddle tradition which is of world renown. Donegal is also well known for its songs which have, like the instrumental music, a distinctive sound. Donegal musical artists such as the bands Clannad, The Pattersons, and Altan and solo artist Enya, have had international success with traditional or traditional flavoured music. Donegal music has also influenced people not originally from the county including folk and pop singers Paul Brady and Phil Coulter. Singer Daniel O'Donnell has become a popular ambassador for the county. Popular music is also common, the county's most acclaimed rock artist being the Ballyshannon-born Rory Gallagher. Other famous acts to come out of Donegal include folk-rock band Goats Don't Shave, Eurovision contestant Mickey Joe Harte and indie rock group The Revs and in more recent years bands such as In Their Thousands and Mojo Gogo have featured on the front page of "Hot Press" magazine.
Donegal has a long literary tradition in both Irish and English. The famous Irish navvy-turned-novelist Patrick MacGill, author of many books about the experiences of Irish migrant itinerant labourers in Britain at around the start of the 20th century, such as "The Rat Pit" and the autobiographical "Children of the Dead End", is from the Glenties area. There is a literary summer school in Glenties named in his honour. The novelist and socialist politician Peadar O'Donnell hailed from The Rosses in west Donegal. The poet William Allingham was also from Ballyshannon. Modern exponents include the Inishowen playwright and poet Frank McGuinness and the playwright Brian Friel. Many of Friel's plays are set in the fictional Donegal town of Ballybeg.
Authors in Donegal have been creating works, like the "Annals of the Four Masters", in Gaelic and Latin since the Early Middle Ages. The Irish philosopher John Toland was born in Inishowen in 1670. He was thought of as the original freethinker by George Berkeley. Toland was also instrumental in the spread of freemasonry throughout Continental Europe. In modern Irish Donegal has produced famous, and sometimes controversial, authors such as the brothers Séamus Ó Grianna and Seosamh Mac Grianna from The Rosses and the contemporary (and controversial) Irish-language poet Cathal Ó Searcaigh from Gortahork in Cloughaneely, and where he is known to locals as "Gúrú na gCnoc" ('the Guru of the Hills').
Donegal is known for the beauty of its textiles, whose unique woolen blends are made of short threads with tiny bits of color blended in for a heathered effect. Sometimes they are woven in a rustic herringbone format and other times in more of a box weave of varied colors. These weaves are known as donegal tweeds (with a small 'd') and are world renowned.
Although approximately 85% of its population is Roman Catholic, County Donegal also has a sizeable Protestant minority. Many Donegal Protestants trace their ancestors to settlers who arrived during the Plantation of Ulster in the early 17th-century. The Church of Ireland is the largest Protestant denomination but is closely rivalled by a large number of Presbyterians. The areas of Donegal with the highest percentage of Protestants are The Laggan area of East Donegal around Raphoe, the Finn Valley and areas around Ramelton, Milford and Dunfanaghy – where their proportion reaches up to 30–45 percent. There is also a large Protestant population between Donegal Town and Ballyshannon in the south of the county. In absolute terms, Letterkenny has the largest number of Protestants (over 1000) and is the most Presbyterian town (among those settlements with more than 3000 people) in the Republic of Ireland.
The Earagail Arts Festival is held within the county each July.
People from Donegal have also contributed to culture elsewhere. Francis Alison was one of the founders of the College of Philadelphia, which would later become the University of Pennsylvania. Francis Makemie (originally from Ramelton) founded the Presbyterian Church in America. David Steele, from Upper Creevaugh, was a prominent Reformed Presbyterian, or Covenanter, minister who emigrated to the United States in 1824. The Right Reverend Dr Charles Inglis, who was the first Church of England Bishop of the Diocese of Nova Scotia, was the third son of The Reverend Archibald Inglis, the Rector in Glencolumbkille.
Places of interest.
County Donegal is a favoured destination for many travellers. One of the attractions is Glenveagh National Park (formerly part of the Glenveagh Estate), as yet (March 2012) the only official "national park" anywhere in the Province of Ulster. The park is a 140 km² (about 35,000 acre) nature reserve with scenery of mountains, raised boglands, lakes and woodlands. At its heart is Glenveagh Castle, a late Victorian 'folly' that was originally built as a summer residence.
The Donegal Gaeltacht (Irish-speaking district) also attracts young people to County Donegal each year during the school summer holidays. The three-week-long summer Gaeltacht courses give young Irish people from other parts of the country a chance to learn the Irish language and traditional Irish cultural traditions that are still prevalent in parts of Donegal. The Donegal Gaeltacht has traditionally been a very popular destination each summer for young people from Northern Ireland. Scuba diving is also very popular with a club being located in Donegal Town.
Education.
Third-level education within the county is provided by Letterkenny Institute of Technology (L.Y.I.T.; popularly known locally as 'the Regional'), established in the 1970s in Letterkenny. In addition, many young people from the county attend third-level institutions elsewhere in Ireland, especially in Derry and also at the University of Ulster at Coleraine (U.U.C.), the University of Ulster at Jordanstown (U.U.J.), The Queen's University of Belfast ('Queen's'), and NUI Galway. Many Donegal students also attend the Limavady Campus of the North West Regional College (popularly known as Limavady Tech) and the Omagh Campus of South West College (popularly known as Omagh Tech or Omagh College).
Sport.
Gaelic football and hurling.
The Gaelic Athletic Association (G.A.A.) sport of Gaelic football is very popular in County Donegal. Donegal's inter-county football team have won the All-Ireland Senior Football Championship title twice (in 1992 and 2012). Donegal emerged victorious from the 2012 All-Ireland Senior Football Championship Final on 23 September 2012 to take the Sam Maguire Cup for only the second time, with early goals from Michael Murphy and Colm McFadden setting up victory of 2–11 to 0–13 over Mayo. In 2007, Donegal won only their second national title by winning the National Football League. On 24 April 2011, Donegal added their third national title when they defeated Laois to capture the National Football League Division Two. There are 16 clubs in the Donegal Senior Football Championship, with many others playing at a lower level.
Hurling, handball and rounders are also played but are less widespread, as in other parts of northwestern Ireland. The Donegal county senior hurling won the Lory Meagher Cup in 2011 and the Nicky Rackard Cup in 2013
Rugby Union.
There are several rugby teams in the county. These include Ulster Qualifying League Two side Letterkenny RFC, whose ground is named after Dave Gallaher, the captain of the 1905 New Zealand All Blacks touring team, who have since become known as The Originals. He was born in nearby Ramelton.
Ulster Qualifying League Three sides include Ballyshannon RFC, Donegal Town RFC and Inishowen RFC. Finn Valley RFC and Tir Chonaill RFC both compete in the Ulster Minor League North.
Association football.
Finn Harps plays in the League of Ireland and won promotion to the Premier Division in 2007 following a 6–3 aggregate win in the playoff final. They are now back alongside their arch-rivals Derry City F.C., with whom they contest Ireland's "North-West Derby". There are numerous other clubs in Donegal, but none has achieved the status of Finn Harps.
Golf.
Many people travel to Donegal for the superb golf links—long sandy beaches and extensive dune systems are a feature of the county, and many links courses have been developed.
Golf is a very popular sport within the county, including world class golf courses such as Ballyliffin (Glashedy), Ballyliffin (Old), both of which are located in the Inishowen peninsula. Other courses to note are Murvagh (located outside Donegal Town) and Rosapenna (Sandy Hills) located in Downings (near Carrigart). The Glashedy Links has been ranked 6th in a recent ranking taken by Golf Digest on the best courses in Ireland. The Old links was ranked 28th, Murvagh 36th and Sandy Hills 38th.
Mountain Biking.
Because of Donegal's hilly and mountain landscape, Mountain Biking has become a significant and growing interest. The Donegal Mountain Bike Club is the newest Mountain Bike club in Donegal, and held its first race on 31 August 2011. The 'Bogman Race' was entered by more than 50 people from different backgrounds of cycling. Due to the overwhelming popularity of their first ever race, the club plans to organise more races in the near future over different seasons, and aims to make it a major tourist attraction throughout Donegal.
Cricket.
Cricket is also played in County Donegal. This sport is chiefly confined to The Laggan district and the Finn Valley in the east of the county. The town of Raphoe and the nearby village of St. Johnston, both in The Laggan, are the traditional strongholds of cricket within the county. The game is mainly played and followed by members of County Donegal's Protestant community.
Other sports.
Donegal's rugged landscape lends itself to active sports like climbing, hillwalking, surfing and kite-flying.
Rock climbing is of very high quality and still under-developed in the county. There is a wealth of good quality climbs in the county, from granite rocks in the south to quartzite and dolerite in the north; from long mountain routes in the Poisoned Glen to boulder challenges of excellent quality in the west and in the Inishowen Peninsula. The current Donegal rock climbing guidebook contains over 2800 recorded rock climbs covering the entire county including the Donegal Sea Stacks, Malin Beg, Gola, and Cruit Islands.
Surfing on Donegal's Atlantic coast is considered to be as good as any in Ireland. The seaside resort of Bundoran, located in the very south of the county, along with nearby Rossnowlagh, have been 'reborn' as the centre of surfing in County Donegal. Indeed, these areas are renowned as the main surfing centres in Ulster.
Greyhound racing based in Lifford is home to the Lifford Greyhound Racing Stadium, a state of the art stadium built at a cost of €12 million and also the East Donegal Coursing Club.
People.
A
B
C
D
E
G
H
I
J
L
Mac/Mc
M
O
P
R
S
T
Surnames.
The most common surnames in County Donegal at the time of the United Kingdom Census of 1901 were:

</doc>
<doc id="45579" url="https://en.wikipedia.org/wiki?curid=45579" title="Queens">
Queens

Queens is the easternmost and largest in area of the five boroughs of New York City. It is geographically adjacent to the borough of Brooklyn at the southwestern end of Long Island, and to Nassau County further east on Long Island; in addition, Queens shares water borders with the boroughs of Manhattan and the Bronx. Coterminous with Queens County since 1899, the borough of Queens is the second-largest in population (after Brooklyn), with a census-estimated 2,339,150 residents in 2015, approximately 48% of them foreign-born. Queens County is also the second-most populous county in New York, behind the neighboring borough of Brooklyn, which is coterminous with Kings County. Queens is the fourth-most densely populated county among New York City's boroughs, as well as in the United States. If each New York City borough were an independent city, Queens would also be the nation's fourth most populous city, after Los Angeles, Chicago and Brooklyn. Queens is the most ethnically diverse urban area in the world.
Queens was established in 1683, as one of the original 12 counties of New York and was named for the Portuguese Princess Catherine of Braganza (1638–1705), Queen of England, Scotland, and Ireland. It became a borough of New York City in 1898, and from 1683 until 1899, the County of Queens included what is now Nassau County.
Queens has the most diversified economy of the five boroughs of New York City and is home to JFK International Airport and LaGuardia Airport. These airports are among the busiest in the world, causing the airspace above Queens to be the most congested in the country. Attractions in Queens include Flushing Meadows Park—home to the New York Mets baseball team and the US Open tennis tournament—Kaufman Astoria Studios, Silvercup Studios, and Aqueduct Racetrack. The borough has diverse housing, ranging from high-rise apartment buildings in the urban areas of western and central Queens, such as Jackson Heights, Flushing, Astoria, and Long Island City, to suburban neighborhoods in the eastern part of the borough such as Little Neck, Douglaston, and Bayside.
History.
Colonial and post-colonial history.
European colonization brought Dutch and English settlers, as a part of the New Netherland colony. First settlements occurred in 1635 followed by early colonizations at Maspeth in 1642,
and Vlissingen (now Flushing) in 1643.
Other early settlements included Newtown (now Elmhurst) and Jamaica. However, these towns were mostly inhabited by English settlers from New England via eastern Long Island (Suffolk County) subject to Dutch law. After the capture of the colony by the English and its renaming as New York in 1664, the area (and all of Long Island) became known as Yorkshire.
The Flushing Remonstrance signed by colonists in 1657 is considered a precursor to the United States Constitution's provision on freedom of religion in the Bill of Rights. The signers protested the Dutch colonial authorities' persecution of Quakers in what is today the borough of Queens.
Originally, Queens County included the adjacent area now comprising Nassau County. It was an original county of New York State, one of twelve created on November 1, 1683. The county was named after Catherine of Braganza, since she was queen of England at the time (she was Portugal's royal princess Catarina daughter of King John IV of Portugal). The county was founded alongside Kings County (Brooklyn, which was named after her husband, King Charles II), and Richmond County (Staten Island, named after his illegitimate son, the 1st Duke of Richmond).
On October 7, 1691, all counties in the Colony of New York were redefined. Queens gained North Brother Island, South Brother Island, and Huletts Island (today known as Rikers Island).
On December 3, 1768, Queens gained other islands in Long Island Sound that were not already assigned to a county but that did not abut on Westchester County (today's Bronx County).
Queens played a minor role in the American Revolution, as compared to Brooklyn, where the Battle of Long Island was largely fought. Queens, like the rest of Long Island, remained under British occupation after the Battle of Long Island in 1776 and was occupied throughout most of the rest of the Revolutionary War. Under the Quartering Act, British soldiers used, as barracks, the public inns and uninhabited buildings belonging to Queens residents. Even though many local people were against unannounced quartering, sentiment throughout the county remained in favor of the British crown. The quartering of soldiers in private homes, except in times of war, was banned by the Third Amendment to the United States Constitution. Nathan Hale was captured by the British on the shore of Flushing Bay in Queens before being executed by hanging in Manhattan for gathering intelligence.
From 1683 until 1784, Queens County consisted of five towns: Flushing, Hempstead, Jamaica, Newtown, and Oyster Bay. On April 6, 1784, a sixth town, the Town of North Hempstead, was formed through secession by the northern portions of the Town of Hempstead.
The seat of the county government was located first in Jamaica,
but the courthouse was torn down by the British during the American Revolution to use the materials to build barracks.
After the war, various buildings in Jamaica temporarily served as courthouse and jail until a new building was erected about 1787 (and later completed) in an area near Mineola (now in Nassau County) known then as Clowesville.
The 1850 census was the first in which the population of the three western towns exceeded that of the three eastern towns that are now part of Nassau County. Concerns were raised about the condition and distance of the old courthouse, and several sites were in contention for the construction of a new one.
In 1870, Long Island City split from the Town of Newtown, incorporating itself as a city, consisting of what had been the Village of Astoria and some unincorporated areas within the Town of Newtown. Around 1874, the seat of county government was moved to Long Island City from Mineola.
On March 1, 1860, the eastern border between Queens County (later Nassau County) and Suffolk County was redefined with no discernible change.
On June 8, 1881, North Brother Island was transferred to New York County.
On May 8, 1884, Rikers Island was transferred to New York County.
In 1885, Lloyd Neck, which was part of the Town of Oyster Bay and was earlier known as Queens Village, seceded from Queens and became part of the Town of Huntington in Suffolk County.
On April 16, 1964, South Brother Island was transferred to Bronx County.
Incorporation as borough.
The New York City Borough of Queens was authorized on May 4, 1897, by a vote of the New York State Legislature after an 1894 referendum on consolidation. The eastern of Queens that became Nassau County was partitioned on January 1, 1899.
Queens Borough was established on January 1, 1898. Long Island City, the towns of Newtown, Flushing, and Jamaica, and the Rockaway Peninsula portion of the Town of Hempstead were merged to form the new borough, dissolving all former municipal governments (Long Island City, the county government, all towns, and all villages) within the new borough. The areas of Queens County that were not part of the consolidation plan, consisting of the towns of North Hempstead and Oyster Bay, and the major remaining portion of the Town of Hempstead, remained part of Queens County until they seceded to form the new Nassau County on January 1, 1899. At this point, the boundaries of Queens County and the Borough of Queens became coterminous. With consolidation, Jamaica once again became the county seat, though county offices now extend to nearby Kew Gardens also.
The borough's administrative and court buildings are presently located in Kew Gardens and downtown Jamaica respectively, two neighborhoods that were villages of the former Town of Jamaica.
From 1905 to 1908 the Long Island Rail Road in Queens became electrified. Transportation to and from Manhattan, previously by ferry or via bridges in Brooklyn, opened up with the Queensboro Bridge finished in 1909, and with railway tunnels under the East River in 1910. From 1915 onward, much of Queens was connected to the New York City Subway system. With the 1915 construction of the Steinway Tunnel carrying the IRT Flushing Line between Queens and Manhattan, and the robust expansion of the use of the automobile, the population of Queens more than doubled in the 1920s, from 469,042 in 1920 to 1,079,129 in 1930.
In later years, Queens was the site of the 1939 New York World's Fair and the 1964 New York World's Fair. LaGuardia Airport, in northern Queens, opened in 1939. Idlewild Airport, in southern Queens and now called JFK Airport, opened in 1948. American Airlines Flight 587 took off from the latter airport on November 12, 2001, but ended up crashing in Queens' Belle Harbor area, killing 265 people. In late October 2012, much of Queens's Breezy Point area was destroyed by a massive six-alarm fire caused by Hurricane Sandy.
Geography.
Queens is located on the far western portion of geographic Long Island and includes a few smaller islands, most of which are in Jamaica Bay, forming part of the Gateway National Recreation Area, which in turn is one of the National Parks of New York Harbor. According to the U.S. Census Bureau, Queens County has a total area of , of which is land and (39%) is water.
Brooklyn, the only other New York City borough on geographic Long Island, lies just south and west of Queens, with Newtown Creek, an estuary that flows into the East River, forming part of the border. To the west and north is the East River, across which is Manhattan to the west and The Bronx to the north. Nassau County is east of Queens on Long Island. Staten Island is southwest of Brooklyn, and shares only a 3-mile-long water border (in the Outer Bay) with Queens.
The Rockaway Peninsula, the most southernly part of all of Long Island, sits between Jamaica Bay and the Atlantic Ocean, featuring the most prominent public beaches in Queens. Flushing Bay and the Flushing River are in the north, connecting to the East River. The East River opens into Long Island Sound. The midsection of Queens is crossed by the Long Island straddling terminal moraine created by the Wisconsin Glacier.
Climate.
Under the Köppen climate classification, using the coldest month (January) isotherm, Queens and the rest of New York City have a humid subtropical climate (Cfa) with partial shielding from the Appalachian Mountains and moderating influences from the Atlantic Ocean. Queens receives plentiful precipitation all year round with 44.8 inches yearly. Extremes range from 107 °F (41.6 °C) to -3 °F (-19.4 °C). Winters are relatively mild compared to other areas of New York State, though snow is common and blizzards occur about every 4–6 years. Springs are unpredictable and can be chilly to very warm. Summers are hot, humid, and wet. Autumn is similar to spring, while snowfall generally begins in December.
Neighborhoods.
Four United States Postal Service postal zones serve Queens, based roughly on those serving the towns in existence at the consolidation of the five boroughs into New York City: Long Island City (ZIP codes starting with 111), Jamaica (114), Flushing (113), and Far Rockaway (116). In addition, the Floral Park post office (110), based in Nassau County, serves a small part of northeastern Queens. Each of these main post offices have neighborhood stations with individual ZIP codes, and unlike the other boroughs, these station names are often used in addressing letters. These ZIP codes do not always reflect traditional neighborhood names and boundaries; "East Elmhurst", for example, was largely coined by the USPS and is not an official community. Most neighborhoods have no solid boundaries. The Forest Hills and Rego Park neighborhoods, for instance, overlap.
Residents of Queens often closely identify with their neighborhood rather than with the borough or city. The borough is a patchwork of dozens of unique neighborhoods, each with its own distinct identity:
Demographics.
Population estimates.
Since 2010, the population of Queens was estimated by the United States Census Bureau to have increased 4.9% to 2,339,150, as of 2015 – Queens' estimated population represented 27.4% of New York City's population of 8,550,405; 29.8% of Long Island's population of 7,838,722; and 11.8% of New York State's population of 19,795,791.
According to 2012 census estimates, 27.2% of the population was Non-Hispanic White, 20.9% Black or African American, 24.8% Asian, 12.9% from some other race, and 2.7% of two or more races. 27.9% of Queens's population was of Hispanic or Latino origin (of any race).
The New York City Department of City Planning was alarmed by the negligible reported increase in population between 2000 and 2010, and it considers estimated increases for Queens for 2012 and 2013 to be questionable. Areas with high proportions of immigrants and undocumented aliens are traditionally undercounted for a variety of reasons. New housing and transit statistics suggest otherwise but corrective formulas were not applied. The racial breakdown of the population is similarly suspect. Foreign born people frequently do not interpret racial definitions as the Census suggests.
As of the most stable census of , there were 2,229,379 people, 782,664 households, and 537,690 families residing in the county. The population density was 20,409.0 inhabitants per square mile (7,879.6/km²). There were 817,250 housing units at an average density of 7,481.6 per square mile (2,888.5/km²). The racial makeup of the county was 44.08% White, 20.01% Black or African American, 0.50% Native American, 17.56% Asian, 0.06% Pacific Islander, 11.68% from other races, and 6.11% from two or more races. 24.97% of the population were Hispanic or Latino of any race.
Ethnic groups.
In Queens, approximately 48.5% of the population was foreign-born as of 2010. Of that, 49.5% were born in Latin America, 33.5% in Asia, 14.8% in Europe, 1.8% in Africa, and 0.4% in North America. Roughly 2.1% of the population was born in Puerto Rico, a U.S. territory, or abroad to American parents. In addition, 51.2% of the population was born in the United States. Approximately 44.2% of the population over 5 years of age speak English at home; 23.8% speak Spanish at home. Also, 16.8% of the populace speak other Indo-European languages at home. Another 13.5% speak an Asian language at home.
Among the Asian population, people of Chinese ethnicity make up the largest ethnic group at 10.2% of Queens' population, with about 237,484 people; the other East and Southeast Asian groups are: Koreans (2.9%), Filipinos (1.7%), Japanese (0.3%), Thais (0.2%), Vietnamese (0.2%), and Indonesians and Burmese both make up 0.1% of the population. People of South Asian descent make up 7.8% of Queens' population: Indians (5.3%), Bangladeshi (1.5%), Pakistanis (0.7%), and Nepali (0.2%).
Among the Hispanic population, Puerto Ricans make up the largest ethnic group at 4.6%, next to Mexicans, who make up 4.2% of the population, and Dominicans at 3.9%. Central Americans make up 2.4% and are mostly Salvadorans. South Americans constitute 9.6% of Queens's population, mainly of Ecuadorian (4.4%) and Colombian descent (3.2%).
Some main European ancestries in Queens as of 2000 include:
The Hispanic or Latino population increased by 61% to 597,773 between 1990 and 2006 and now accounts for 26.5% of the borough's population. Queens is now home to hundreds of thousands of Latinos and Hispanics:
Queens is home to 49.6% of the city's Asian population. Among the five boroughs, Queens has the largest population of Chinese, Indian, Korean, Filipino, Bangladeshi and Pakistani Americans. Queens has the largest Asian American population by county outside the Western United States; according to the 2006 American Community Survey, Queens ranks fifth among US counties with 477,772 (21.18%) Asian Americans, behind Los Angeles County, California, Honolulu County, Hawaii, Santa Clara County, California, and Orange County, California.
The borough is also home to one of the highest concentrations of Indian Americans in the nation, with an estimated population of 144,896 in 2014 (6.24% of the 2014 borough population), as well as Pakistani Americans, who number at 15,604. Queens has the second largest Sikh population in the nation after California.
In 2010, Queens held a disproportionate share of several Asian communities within New York City, relative to its overall population, as follows:
Queens has the third largest Bosnian population in the United States behind only St. Louis and Chicago, numbering more than 15,000.
A 2011 UJA/Federation of New York study found that Queens was home to 198,000 Jewish Americans, up from 186,000 in 2002.
There were 782,664 households out of which 31.5% had children under the age of 18 living with them, 46.9% were married couples living together, 16.0% had a female householder with no husband present, and 31.3% were non-families. 25.6% of all households were made up of individuals and 9.7% had someone living alone who was 65 years of age or older. The average household size was 2.81 and the average family size was 3.39.
In the county the population was spread out with 22.8% under the age of 18, 9.6% from 18 to 24, 33.1% from 25 to 44, 21.7% from 45 to 64, and 12.7% who were 65 years of age or older. The median age was 35 years. For every 100 females there were 92.9 males. For every 100 females age 18 and over, there were 89.6 males.
The median income for a household in the county was $37,439, and the median income for a family was $42,608. Males had a median income of $30,576 versus $26,628 for females. The per capita income for the county was $19,222. About 16.9% of families and 24.7% of the population were below the poverty line, including 18.8% of those under age 18 and 13.0% of those age 65 or over. In Queens, the black population earns more than whites on average.
Many of these African Americans live in quiet, middle class suburban neighborhoods near the Nassau County border, such as Laurelton and Cambria Heights which have large black populations whose family income is higher than average. Those areas are known for their well kept homes, suburban feel, and low crime rate. The migration of European Americans from parts of Queens has been long ongoing with departures from Ozone Park, Woodhaven, Bellerose, Floral Park, and Flushing, etc. (most of the outgoing population has been replaced with Asian Americans). Neighborhoods such as Whitestone, College Point, North Flushing, Auburndale, Bayside, Middle Village, Little Neck, and Douglaston have not had a substantial exodus of white residents, but have seen an increase of Asian population, mostly Chinese and Korean. Queens has recently experienced a real estate boom making most of its neighborhoods very desirable for people who want to reside near Manhattan in a less urban setting. According to a 2001 Claritas study, Queens is the most diverse county in the United States among counties of 100,000+ population.
Culture.
While Queens has not been the center of any major artistic movements, it has been the home of such notable artists as Tony Bennett, Francis Ford Coppola, Paul Simon, and Robert Mapplethorpe. The current poet laureate of Queens is Paolo Javier.
Queens has notably fostered African-American culture, with establishments such as The Afrikan Poetry Theatre and the Black Spectrum Theater Company catering specifically to African Americans in Queens. In the 1940s, Queens was an important center of jazz; such jazz luminaries as Louis Armstrong, Charlie Parker, and Ella Fitzgerald took up residence in Queens, seeking refuge from the segregation they found elsewhere in New York. Additionally, many notable hip-hop acts hail from Queens, including Nas, Run-D.M.C., Kool G Rap, A Tribe Called Quest, LL Cool J, Mobb Deep, 50 Cent, Nicki Minaj, and Heems of Das Racist.
Queens hosts various museums and cultural institutions that serve its diverse communities. They range from the historical (such as the John Bowne House) to the scientific (such as the New York Hall of Science), from conventional art galleries (such as the Noguchi Museum) to unique graffiti exhibits (such as 5 Pointz). Queens's cultural institutions include, but are not limited to:
The travel magazine "Lonely Planet" also named Queens the top destination in the country for 2015 for its cultural and culinary diversity. Stating that Queens is "quickly becoming its hippest" but that "most travelers haven’t clued in… yet," the "Lonely Planet" stated that "nowhere is the image of New York as the global melting pot truer than Queens."
Languages.
There are 138 languages spoken in the borough. As of 2010, 43.84% (905,890) of Queens residents age 5 and older spoke English at home as a primary language, while 23.88% (493,462) spoke Spanish, 8.06% (166,570) Chinese, 3.44% (71,054) various Indic languages, 2.74% (56,701) Korean, 1.67% (34,596) Russian, 1.56% (32,268) Italian, 1.54% (31,922) Tagalog, 1.53% (31,651) Greek, 1.32% (27,345) French Creole, 1.17% (24,118) Polish, 0.96% (19,868) Hindi, 0.93% (19,262) Urdu, 0.92% (18,931) other Asian languages, 0.80% (16,435) other Indo-European languages, 0.71% (14,685) French, 0.61% (12,505) Arabic, 0.48% (10,008) Serbo-Croatian, and Hebrew was spoken as a main language by 0.46% (9,410) of the population over the age of five. In total, 56.16% (1,160,483) of Queens's population age 5 and older spoke a mother language other than English.
Food.
The cuisine available in Queens reflects its vast cultural diversity. The cuisine of a particular neighborhood often represents its demographics; for example, Astoria hosts many Greek restaurants, in keeping with its traditionally Greek population. Jackson Heights is known for its prominent Indian cuisine and also many Latin American eateries.
Government.
Since New York City's consolidation in 1898, Queens has been governed by the New York City Charter that provides for a strong mayor-council system. The centralized New York City government is responsible forpublic education, correctional institutions, public safety, recreational facilities, sanitation, water supply, and welfare services in Queens. The Queens Library is governed by a 19-member Board of Trustees, who are appointed by the Mayor of New York City and the Borough President of Queens.
Since 1990 the Borough President has acted as an advocate for the borough at the mayoral agencies, the City Council, the New York state government, and corporations. Queens' Borough President is Melinda Katz, elected in November 2013 as a Democrat with 80.3% of the vote . Queens Borough Hall is the seat of government and is located in Kew Gardens.
The Democratic Party holds most public offices. Sixty-three percent of registered Queens voters are Democrats. Local party platforms center on affordable housing, education and economic development. Controversial political issues in Queens include development, noise, and the cost of housing.
Each of the city's five counties has its own criminal court system and District Attorney, the chief public prosecutor who is directly elected by popular vote. Richard A. Brown, who ran on both the Republican and Democratic Party tickets, has been the District Attorney of Queens County since 1991.
Queens has 12 seats on the New York City Council, the second largest number among the five boroughs. It is divided into 14 community districts, each served by a local Community Board. Community Boards are representative bodies that field complaints and serve as advocates for local residents.
Although Queens is heavily Democratic, it is considered a swing county in New York politics. Republican political candidates who do well in Queens usually win citywide or statewide elections. Republicans such as former Mayors Rudolph Giuliani and Michael Bloomberg won majorities in Queens. Republican State Senator Serphin Maltese represented a district in central and southern Queens for twenty years until his defeat in 2008 by Democratic City Councilman Joseph Addabbo. In 2002, Queens voted against incumbent Republican Governor of New York George Pataki in favor of his Democratic opponent, Carl McCall by a slim margin.
However, Queens has not voted for a Republican candidate in a presidential election since 1972, when Queens voters chose Richard Nixon over George McGovern. Since the 1996 presidential election, Democratic presidential candidates have received over 70% of the popular vote in Queens.
Economy.
Queens has the second-largest economy of New York City's five boroughs, following Manhattan. In 2004, Queens had 15.2% (440,310) of all private sector jobs in New York City and 8.8% of private sector wages. Queens has the most diversified economy of the five boroughs, with occupations spread relatively evenly across the health care, retail trade, manufacturing, construction, transportation, and film and television production sectors, such that no single sector is overwhelmingly dominant.
The diversification in Queens' economy is reflected in the large amount of employment in the export-oriented portions of its economy—such as transportation, manufacturing, and business services—that serve customers outside the region. This accounts for more than 27% of all Queens jobs and offers an average salary of $43,727, 14% greater than that of jobs in the locally oriented sector.
The borough's largest employment sector—trade, transportation, and utilities—accounted for nearly 30% of all jobs in 2004. Queens is home to two of the three major New York City area airports, JFK International Airport and LaGuardia Airport. These airports are among the busiest in the world, leading the airspace above Queens to be the most congested in the country. This airline industry is particularly important to the economy of Queens, providing almost one quarter of the sector's employment and more than 30% of the sector's wages.
Education and health services is the next largest sector in Queens and comprised almost 24% of the borough's jobs in 2004. The manufacturing and construction industries in Queens are the largest of the City and account for nearly 17% of the borough's private sector jobs. Comprising almost 17% of the jobs in Queens is the information, financial activities, and business and professional services sectors.
, Queens had almost 40,000 business establishments. Small businesses act as an important part of the borough's economic vitality with two thirds of all business employing between one and four people.
Several large companies have their headquarters in Queens, including watchmaker Bulova, based in East Elmhurst; internationally renowned piano manufacturer Steinway & Sons in Astoria; Glacéau, the makers of Vitamin Water, headquartered in Whitestone; and JetBlue Airways, an airline based in Long Island City.
Long Island City is a major manufacturing and back office center. Flushing is a major commercial hub for Chinese American and Korean American businesses, while Jamaica is the major civic and transportation hub for the borough.
Sports.
Citi Field, home ballpark of the New York Mets of Major League Baseball is located in Flushing Meadows-Corona Park. Shea Stadium, the former home of the Mets and the New York Jets of the National Football League, as well as the temporary home of the New York Yankees and the New York Giants Football Team stood where Citi Field's parking lot is now located. The US Open tennis tournament is played at the USTA Billie Jean King National Tennis Center, located just south of Citi Field. Arthur Ashe Stadium is the biggest tennis stadium in the world. The US Open was formerly played at the West Side Tennis Club in Forest Hills. Queens is also the home of Aqueduct Racetrack, located in Ozone Park.
Transportation.
Queens has crucial importance in international and interstate air traffic. Two of the New York metropolitan area's three major airports are located there; LaGuardia Airport is in northern Queens, while John F. Kennedy International Airport is to the south on the shores of Jamaica Bay.
According to the 2000 Census, 37.7% of all Queens households did not own a car. The citywide rate is 55%. Therefore, mass transit is also used.
Public transport.
Twelve New York City Subway routes traverse Queens, serving 81 stations on seven main lines. The , , , and routes connect Queens to Brooklyn without going through Manhattan first. The , M, , , and trains connect Queens and Brooklyn via Manhattan, while the and / trains connect Queens to Manhattan only. Trains on the M service go through Queens twice in the same trip; both of its full-length termini, in Middle Village and Forest Hills, are in Queens.
A commuter train system, the Long Island Rail Road, operates 22 stations in Queens with service to Manhattan, Brooklyn, and Long Island. Jamaica station is a hub station where all the lines in the system but one (the Port Washington Branch) converge. It is the busiest commuter rail hub in the United States. Sunnyside Yard is used as a staging area by Amtrak and NJ Transit for intercity and commuter trains from Penn Station in Manhattan. 61st Street – Woodside acts as one of the many LIRR connections to the New York City Subway. The elevated AirTrain people mover system connects JFK International Airport to the New York City Subway and the Long Island Rail Road; a separate AirTrain system is planned alongside the Grand Central Parkway to connect LaGuardia Airport to these transit systems. Plans were announced in July 2015 to entirely rebuild LaGuardia Airport itself in a multibillion-dollar project to replace its aging facilities, and this project would accommodate the new AirTrain connection.
About 100 local bus routes operate within Queens, and another 15 express routes shuttle commuters between Queens and Manhattan, under the MTA New York City Bus and MTA Bus brands.
A streetcar line connecting Queens with Brooklyn was proposed by the city in February 2016, with the planned timeline calling for service to begin around 2024.
Roads.
Highways.
Queens is traversed by three trunk east-west highways. The Long Island Expressway (Interstate 495) runs from the Queens Midtown Tunnel on the west through the borough to Nassau County on the east. The Grand Central Parkway, whose western terminus is the Triborough Bridge, extends east to the Queens/Nassau border, where its name changes to the Northern State Parkway. The Belt Parkway begins at the Gowanus Expressway in Brooklyn, and extends east into Queens, past Aqueduct Racetrack and JFK Airport. On its eastern end at the Queens/Nassau border, it splits into the Southern State Parkway which continues east, and the Cross Island Parkway which turns north.
There are also several major north-south highways in Queens, including the Brooklyn-Queens Expressway (Interstate 278), the Van Wyck Expressway (Interstate 678), the Clearview Expressway (Interstate 295), and the Cross Island Parkway.
Streets.
The streets of Queens are laid out in a semi-grid system, with a numerical system of street names (similar to Manhattan and the Bronx). Nearly all roadways oriented north-south are "Streets", while east-west roadways are "Avenues", beginning with the number 1 in the west for Streets and in the north for Avenues. In some parts of the borough, several consecutive streets may share numbers (for instance, 72nd Street followed by 72nd Place and 72nd Lane, or 52nd Avenue followed by 52nd Road, 52nd Drive, and 52nd Court), often causing confusion for non-residents. In addition, incongruous alignments of street grids, unusual street paths due to geography, or other circumstances often lead to the skipping of numbers (for instance, on Ditmars Boulevard, 70th Street is followed by Hazen Street which is followed by 49th Street). Numbered roads tend to be residential, although numbered commercial streets are not rare. A fair number of streets that were country roads in the 18th and 19th centuries (especially major thoroughfares such as Northern Boulevard, Queens Boulevard, Hillside Avenue, and Jamaica Avenue) carry names rather than numbers, typically though not uniformly called "Boulevards" or "Parkways".
The structure of a Queens address was designed to provide convenience in locating the address itself; the first half of a number in a Queens address refers to the nearest cross street, the second half refers to the house or lot number from where the street begins from that cross street, followed by the name of the street itself. For example, to find an address in Queens, 14-01 120th Street, one could ascertain from the address structure itself that the listed address is at the intersection of 14th Avenue and 120th Street, and that the address must be closest to 14th Avenue rather than 15th Avenue, as it is the first lot on the block. This pattern doesn't stop when a street is named, assuming that there is an existing numbered cross-street. For example, Queens College is situated at 65–30 Kissena Boulevard, and is so named because the cross-street closest to the entrance is 65th Avenue. Th
Many of the village street grids of Queens had only worded names, some were numbered according to local numbering schemes, and some had a mix of words and numbers. In the early 1920s a "Philadelphia Plan" was instituted to overlay one numbered system upon the whole borough. The Topographical Bureau, Borough of Queens, worked out the details. Subway stations were only partly renamed, and some, including those along the IRT Flushing Line (), now share dual names after the original street names. In 2012, some numbered streets in the Douglaston Hill Historic District were renamed to their original names, with 43rd Avenue becoming Pine Street.
The Rockaway Peninsula does not follow the same system as the rest of the borough and has its own numbering system. Streets are numbered in ascending order heading west from near the Nassau County border, and are prefixed with the word "Beach." Streets at the easternmost end, however, are nearly all named. Streets in Bayswater, which is on Jamaica Bay, has its numbered streets prefixed with the word "Bay" rather than "Beach". Another deviation from the norm is Broad Channel; it maintains the north-south numbering progression but uses only the suffix "Road," as well as the prefixes "West" and "East," depending on location relative to Cross Bay Boulevard, the neighborhood's major through street. Broad Channel's streets were a continuation of the mainland Queens grid in the 1950s; formerly the highest numbered avenue in Queens was 208th Avenue rather today's 165th Avenue in Howard Beach & Hamilton Beach. The other exception is the neighborhood of Ridgewood, which for the most part shares a grid and house numbering system with the Brooklyn neighborhood of Bushwick. The grid runs east-west from the LIRR Bay Ridge Branch right-of-way to Flushing Avenue; and north-south from Forest Avenue in Ridgewood to Bushwick Avenue in Brooklyn before adjusting to meet up with the Bedford-Stuyvesant grid at Broadway. All streets on the grid have names.
Bridges and tunnels.
Queens is connected to the Bronx by the Bronx–Whitestone Bridge, the Throgs Neck Bridge, the Triborough (Robert F. Kennedy) Bridge, and the Hell Gate Bridge. Queens is connected to Manhattan Island by the Robert F. Kennedy Bridge, the Queensboro Bridge, and the Queens Midtown Tunnel, as well as to Roosevelt Island by the Roosevelt Island Bridge.
While most of the Queens/Brooklyn border is on land, the Kosciuszko Bridge crosses the Newtown Creek connecting Maspeth to Greenpoint, Brooklyn. The Pulaski Bridge connects McGuinness Boulevard in Greenpoint to 11th Street, Jackson Avenue, and Hunters Point Avenue in Long Island City. The J. J. Byrne Memorial Bridge (a.k.a. Greenpoint Avenue Bridge) connects the sections of Greenpoint Avenue in Greenpoint and Long Island City. A lesser bridge connects Grand Avenue in Queens to Grand Street in Brooklyn.
The Cross Bay Veterans Memorial Bridge traverses Jamaica Bay to connect the Rockaway Peninsula to the rest of Queens. Marine Parkway–Gil Hodges Memorial Bridge links the western part of the Peninsula with Flatbush Avenue, Brooklyn's longest thoroughfare. Both crossings were built and continue to be operated by what is now known as MTA Bridges and Tunnels. The IND Rockaway Line parallels the Cross Bay, has a mid-bay station at Broad Channel which is just a short walk from the Jamaica Bay Wildlife Refuge, now part of Gateway National Recreation Area and a major stop on the Atlantic Flyway.
Waterways.
One year-round scheduled ferry service connects Queens and Manhattan. New York Water Taxi operates service across the East River from Hunters Point in Long Island City to Manhattan at 34th Street and south to Pier 11 at Wall Street. In 2007, limited weekday service was begun between Breezy Point, the westernmost point in the Rockaways, to Pier 11 via the Brooklyn Army Terminal. Summertime weekend service provides service from Lower Manhattan and southwest Brooklyn to the peninsula's Gateway beaches.
In the aftermath of Hurricane Sandy on October 29, 2012, massive infrastructure damage to the IND Rockaway Line () south of the Howard Beach – JFK Airport station severed all direct subway connections between the Rockaway Peninsula and Broad Channel, Queens and the Queens mainland for many months. Ferry operator SeaStreak began running a city-subsidized ferry service between a makeshift ferry slip at Beach 108th Street and Beach Channel Drive in Rockaway Park, Queens, and Pier 11/Wall Street, then continuing on to the East 34th Street Ferry Landing. In August 2013, a stop was added at Brooklyn Army Terminal. Originally intended as just a stopgap alternative transportation measure until subway service was restored to the Rockaways, the ferry proved to be popular with both commuters and tourists and was extended several times, as city officials evaluated the ridership numbers to determine whether to establish the service on a permanent basis. Between its inception and December 2013, the service had carried close to 200,000 riders. When the city government announced its budget in late June 2014 for the upcoming fiscal year beginning July 1, the ferry only received a $2 million further appropriation, enough to temporarily extend it again through October, but did not receive the approximately $8 million appropriation needed to keep the service running for the full fiscal year. Despite last-minute efforts by local transportation advocates, civic leaders and elected officials, ferry service ended on October 31, 2014. They promised to continue efforts to have the service restored.
Education.
Elementary and secondary education.
Elementary and secondary school education in Queens is provided by a vast number of public and private institutions. Public schools in the borough are managed by the New York City Department of Education, the largest public school system in the United States. Most private schools are affiliated to or identify themselves with the Roman Catholic or Jewish religious communities. Townsend Harris High School is a Queens public magnet high school for the humanities consistently ranked as among the top 100 high schools in the United States.
Queens Library.
The Queens Borough Public Library is the public library system for the borough and one of three library systems serving New York City. Dating back to the foundation of the first Queens library in Flushing in 1858, the Queens Borough Public Library is one of the largest public library systems in the United States. Separate from the New York Public Library, it is composed of 63 branches throughout the borough. In fiscal year 2001, the Library achieved a circulation of 16.8 million. First in circulation in New York State since 1985, the Library has maintained the highest circulation of any city library in the country since 1985 and the highest circulation of any library in the nation since 1987. The Library maintains collections in many languages, including Spanish, Chinese, Korean, Russian, Haitian Creole, Polish, and six Indic languages, as well as smaller collections in 19 other languages.
Notable people.
Various public figures have grown up or lived in Queens. Musicians who have lived in the borough include singer Nadia Ali, rappers LL Cool J, Nas, Ja Rule, 50 Cent, Run–D.M.C., Nicki Minaj, Rich The Kid, Simon & Garfunkel and Johnny Ramone. Actors such as Adrien Brody, and Lucy Liu and Idina Menzel have been born and/or raised in Queens. Porn star Ron Jeremy was born in Queens. Actor Mae West has also lived in Queens. Physician Joshua Prager was born in Whitestone. Mafia boss John Gotti lived in Queens for many years. Donald Trump, a real estate billionaire, socialite, and 2016 U.S. presidential candidate for the Republican Party, was born in Queens and raised in Jamaica Estates.
Queens has also been home to athletes such as professional basketball player Rafer Alston Basketball players Kareem Abdul-Jabbar and Metta World Peace were both born in Queens. Olympic Athlete Bob Beamon. Tennis star John McEnroe was born in Douglaston. Fictional Marvel Comics character Spider-Man is also portrayed as a native of Queens.

</doc>
<doc id="45582" url="https://en.wikipedia.org/wiki?curid=45582" title="Duchy of Schleswig">
Duchy of Schleswig

The Duchy of Schleswig (; ; Low German: "Sleswig"; North Frisian: "Slaswik") was a principality in Southern Jutland ("Sønderjylland") covering the area about 60 km north and 70 km south of the current border between Germany and Denmark; the territory has been divided between the two countries since 1920, with Northern Schleswig in Denmark and Southern Schleswig in Germany. The region is also called Sleswick in English.
The area's traditional significance lies in the transfer of goods between the North Sea and the Baltic Sea, connecting the trade route through Russia with the trade routes along the Rhine and the Atlantic coast (see also Kiel Canal).
History.
Early history.
Roman sources place the homeland of the Jute tribe north of the river Eider and that of the Angles to its south, who in turn abutted the neighbouring Saxons. By the early Middle Ages the population of Schleswig consisted of Danes to the north of Danevirke and Schlei and on the peninsula Schwansen, North Frisians on the west coast below a line slightly south of the present border and on the islands, and Saxon (or Low German) in the far South. During the 14th century the population on Schwansen began to speak German beside Danish, but otherwise the ethnic borders remained remarkably stable until around 1800 with the exception of the population in the towns that became increasingly German from the 14th century onwards.
During the early Viking Age, Haithabu - Scandinavia's biggest trading centre - was located in this region, which is also the location of the interlocking fortifications known as the "Danewerk". Its construction, and in particular its great expansion around 737, has been interpreted as an indication of the emergence of a unified Danish state. In May 1931 scientists of the National Museum of Denmark announced the finding of eighteen Viking graves with the remains of eighteen men in them. The discovery came during excavations in Schleswig. The skeletons indicated that the men were bigger proportioned than twentieth-century Danish men. Each of the graves was laid out from east to west. Researchers surmised that the bodies were entombed in wooden coffins originally, but only the iron nails remained. Towards the end of the Early Middle Ages, Schleswig formed part of the historical Lands of Denmark as Denmark unified out of a number of petty chiefdoms in the 8th to 10th centuries (the puissant of the Viking incursions).
The southern boundary of Denmark in the region of the Eider River and the Danevirke was a source of continuous dispute. The Treaty of Heiligen was signed in 811 between the Danish King Hemming and Charlemagne, by which the border was established at the Eider. During the 10th century there were several wars between East Francia and Denmark. In 1027, Conrad II and Canute the Great again settled their mutual border at the Eider.
In 1115, king Niels created his nephew Canute Lavard - a son of his predecessor Eric I - Earl of Schleswig, a title used for only a short time before the recipient began to style himself Duke.
In 1230s, Southern Jutland (Duchy of Slesvig) was allotted as an appanage to Abel Valdemarsen, Canute's great-grandson, a younger son of Valdemar II of Denmark. Abel, having wrested the Danish throne to himself for a brief period, left his duchy to his sons and their successors, who pressed claims to the throne of Denmark for much of the next century, so that the Danish kings were at odds with their cousins, the dukes of Slesvig. Feuds and marital alliances brought the Abel dynasty into a close connection with the German Duchy of Holstein by the 15th century. The latter was a fief subordinate to the Holy Roman Empire, while Schleswig remained a Danish fief. These dual loyalties were to become a main root of the dispute between the German states and Denmark in the 19th century, when the ideas of romantic nationalism and the nation-state won popular support.
Early modern times.
The title Duke of Schleswig was inherited in 1460 by the hereditary kings of Norway who were also regularly elected kings of Denmark simultaneously, and their sons (unlike Denmark which was not hereditary). This was an anomaly – a king holding a ducal title, which he as king was the fount of and its liege lord. The title and anomaly survived presumably because it was already co-regally held by the king's sons. Between 1544 and 1713/20 the ducal reign had become a condominium, with the royal House of Oldenburg and its cadet branch House of Holstein-Gottorp jointly holding the stake. A third branch in the condominium, the short-lived House of Haderslev, was already extinct in 1580 by the time of John the Elder.
Following the Protestant Reformation when Latin was replaced as the medium of church service by the vernacular languages, the diocese of Schleswig was divided and an autonomous archdeaconry of Haderslev created. On the west coast the Danish diocese of Ribe stopped about 5 km north of the present border. This created a new cultural dividing line in the duchy because German was used for church services and teaching in the diocese of Schleswig and Danish was used in the diocese of Ribe and the archdeaconry of Haderslev. This line corresponds remarkably well with the present border.
In the 17th century a series of wars between Denmark and Sweden—which Denmark lost—devastated the region economically. However the nobility responded with a new agricultural system that restored prosperity. In the period 1600 to 1800 the region experienced the growth of manorialism of the sort common in the rye-growing regions of eastern Germany. The manors were large holdings with the work done by feudal peasant farmers. They specialized in high quality dairy products. Feudal lordship was combined with technical modernization, and the distinction between unfree labour and paid work was often vague. The feudal system was gradually abolished in the late 18th century, starting with the crown lands in 1765 and later the estates of the nobility. In 1805 all serfdom was abolished and land tenure reforms allowed former peasants to own their own farms.
19th century and the rise of nationalism.
From around 1800 to 1840 the Danish speaking population on the Angeln peninsula between Schleswig and Flensburg began to switch to Low German and in the same period many North Frisians also switched to Low German. This linguistic change created a new de facto dividing line between German and Danish speakers north of Tønder and south of Flensburg.
From around 1830 large segments of the population began to identify with either German or Danish nationality and mobilized politically. In Denmark, the National Liberal Party used the Schleswig Question as part of their agitation and demanded that the Duchy be incorporated in the Danish kingdom under the slogan "Denmark to the Eider". This caused a conflict between Denmark and the German states over Schleswig and Holstein which led to the Schleswig-Holstein Question of the 19th century. When the National Liberals came to power in Denmark, in 1848, it provoked an uprising of ethnic Germans who supported Schleswig's ties with Holstein. This led to the First War of Schleswig. Denmark was victorious and the Prussian troops were ordered to pull out of Schleswig and Holstein following the London Protocol of 1852.
Denmark again attempted to integrate Schleswig, by creating a new common constitution (the so-called November Constitution) for Denmark and Schleswig in 1863, but the German Confederation, led by Prussia and Austria, defeated the Danes in the Second War of Schleswig the following year. Prussia and Austria then assumed administration of Schleswig and Holstein respectively under the Gastein Convention of 14 August 1865. However, tensions between the two powers culminated in the Austro-Prussian War of 1866. In the Peace of Prague, the victorious Prussians annexed both Schleswig and Holstein, creating the province of Schleswig-Holstein. Provision for the cession of northern Schleswig to Denmark was made pending a popular vote in favour of this. In 1878, however, Austria went back on this provision, and Denmark, in a Treaty of 1907, with Germany, recognized that, by the agreement between Austria and Prussia, the frontier between Prussia and Denmark had finally been settled.
Modern times.
The Treaty of Versailles provided for plebiscites to determine the ownership of the region. Thus, two referendums were held in 1920, resulting in the partition of the region. Northern Schleswig voted by an 75% Danish Majority, joined Denmark, whereas Central Schleswig voted, by an 80% German majority, to remain part of Germany. In Southern Schleswig, no referendum was held, as the likely outcome was apparent. The name Southern Schleswig is now used for all of German Schleswig. This decision left substantial minorities on both sides of the new border.
Following the Second World War, a substantial part of the German population in Southern Schleswig changed their nationality and declared themselves as Danish. This change was caused by a number of factors, most importantly the German defeat and an influx of a large number of refugees from eastern Germany, whose culture and appearance differed from the local Germans, who were mostly descendents of Danish families that had changed their nationality in the 19th century. The change created a temporary Danish majority in the region and a demand for a new referendum from the Danish population in South Schleswig and some Danish politicians, including prime minister Knud Kristensen. But the majority in the Danish parliament refused to support a referendum in South Schleswig, fearing that the "new Danes" were not genuine in their change of nationality. This proved to be the case and, from 1948 the Danish population began to shrink again. By the early 1950s, it nevertheless had stabilised at a level four times higher than the pre-war number.
In the Copenhagen-Bonn declaration of 1955, Germany and Denmark promised to uphold the rights of each other's minority population. Today, both parts co-operate as a Euroregion, despite a national border dividing the former duchy. As Denmark and Germany are both part of the Schengen Area, there are no controls at the border.
Name and naming dispute.
In the 19th century, there was a naming dispute concerning the use of "Schleswig" or "Slesvig" and "Sønderjylland" (Southern Jutland). Originally the duchy was called "Sønderjylland" (Southern Jutland) but in the late 14th century the name of the city Slesvig (now Schleswig) started to be used for the whole territory. The term "Sønderjylland" was hardly used between the 16th and 19th centuries, and in this period the name "Schleswig" had no special political connotations. But around 1830, some Danes started to re-introduce the archaic term Sønderjylland to emphasize the area's history before its association with Holstein and its connection with the rest of Jutland. Its revival and widespread use in the 19th Century therefore had a clear Danish nationalist connotation of laying a claim to the territory and objecting to the German claims. "Olsen's Map", published by the Danish cartographer Olsen in the 1830s that used this term, aroused a storm of protests by the duchy's German inhabitants. Even though many Danish nationalists, such as the National Liberal ideologue and agitator Orla Lehmann, used the name "Schleswig", it began to assume a clear German nationalist character in the mid 19th century – especially when included in the combined term "Schleswig-Holstein". A central element of the German nationalistic claim was the insistence on Schleswig and Holstein being a single, indivisible entity. Since Holstein was legally part of the German Confederation, and ethnically entirely German with no Danish population, use of that name implied that both provinces should belong to Germany and that their connection with Denmark should be weakened or altogether severed. 
After the German conquest in 1864, the term Sønderjylland became increasingly dominant among the Danish population, even though most Danes still had no objection to the use of "Schleswig" as such (it is etymologically of Danish origin) and many of them still used it, themselves, in its Danish version "Slesvig". An example is the founding of De Nordslesvigske Landboforeninger (The North Schleswig Farmers Association). In 1866 Schleswig and Holstein were legally merged into the Prussian province of Schleswig-Holstein. 
The naming dispute was resolved with the 1920 plebiscites and partition, each side applying its preferred name to the part of the territory remaining in its possession – though both terms can, in principle, still refer to the entire region. Northern Schleswig was, after the 1920 plebiscites, officially named The Southern Jutland districts ("de sønderjyske landsdele"), while Southern Schleswig then remained a part of the Prussian province, which became the German state of Schleswig-Holstein in 1946.

</doc>
<doc id="45584" url="https://en.wikipedia.org/wiki?curid=45584" title="Biodefense">
Biodefense

Biodefense refers to short term, local, usually military measures to restore biosecurity to a given group of persons in a given area who are, or may be, subject to biological warfare—in the civilian terminology, it is a very robust biohazard response. It is technically possible to apply biodefense measures to protect animals or plants, but this is generally uneconomic. However, protection of water supplies and food supplies are often a critical part of biodefense. Various definitions of biosafety emerged in different professions to guarantee non-human health.
Biodefense is most often discussed in the context of biowar or bioterrorism, and is generally considered a military or emergency response term.
Biodefense applies to two distinct target populations: civilian non-combatant and military combatant (troops in the field).
Military.
Biodefense of troops in the field.
Military biodefense in the United States began with the United States Army Medical Unit (USAMU) at Fort Detrick, Maryland, in 1956. (In contrast to the U.S. Army Biological Warfare Laboratories [1943–1969], also at Fort Detrick, the USAMU's mission was purely to develop defensive measures against bio-agents, as opposed to weapons development.) The USAMU was disestablished in 1969 and succeeded by today's United States Army Medical Research Institute of Infectious Diseases (USAMRIID).
The United States Department of Defense (or "DoD") has focused since at least 1998 on the development and application of vaccine-based biodefenses. In a July 2001 report commissioned by the DoD, the "DoD-critical products" were stated as vaccines against anthrax (AVA and Next Generation), smallpox, plague, tularemia, botulinum, ricin, and equine encephalitis. Note that two of these targets are toxins (botulinum and ricin) while the remainder are infectious agents.
Civilian.
Role of public health and disease surveillance.
It is important to note that all of the classical and modern biological weapons organisms are animal diseases, the only exception being smallpox. Thus, in any use of biological weapons, it is highly likely that animals will become ill either simultaneously with, or perhaps earlier than humans.
Indeed, in the largest biological weapons accident known–the anthrax outbreak in Sverdlovsk (now Yekaterinburg) in the Soviet Union in 1979, sheep became ill with anthrax as far as 200 kilometers from the release point of the organism from a military facility in the southeastern portion of the city (known as Compound 19 and still off limits to visitors today, see Sverdlovsk anthrax leak).
Thus, a robust surveillance system involving human clinicians and veterinarians may identify a bioweapons attack early in the course of an epidemic, permitting the prophylaxis of disease in the vast majority of people (and/or animals) exposed but not yet ill.
For example, in the case of anthrax, it is likely that by 24–36 hours after an attack, some small percentage of individuals (those with compromised immune system or who had received a large dose of the organism due to proximity to the release point) will become ill with classical symptoms and signs (including a virtually unique chest X-ray finding, often recognized by public health officials if they receive timely reports). By making these data available to local public health officials in real time, most models of anthrax epidemics indicate that more than 80% of an exposed population can receive antibiotic treatment before becoming symptomatic, and thus avoid the moderately high mortality of the disease.
Identification of bioweapons.
The goal of biodefense is to integrate the sustained efforts of the national and homeland security, medical, public health, intelligence, diplomatic, and law enforcement communities. Health care providers and public health officers are among the first lines of defense. In some countries private, local, and provincial (state) capabilities are being augmented by and coordinated with federal assets, to provide layered defenses against biological weapons attacks. During the first Gulf War the United Nations activated a biological and chemical response team, Task Force Scorpio, to respond to any potential use of weapons of mass destruction on civilians.
The traditional approach toward protecting agriculture, food, and water: focusing on the natural or unintentional introduction of a disease is being strengthened by focused efforts to address current and anticipated future biological weapons threats that may be deliberate, multiple, and repetitive.
The growing threat of biowarfare agents and bioterrorism has led to the development of specific field tools that perform on-the-spot analysis and identification of encountered suspect materials. One such technology, being developed by researchers from the Lawrence Livermore National Laboratory (LLNL), employs a "sandwich immunoassay", in which fluorescent dye-labeled antibodies aimed at specific pathogens are attached to silver and gold nanowires.
The U.S. National Institute of Allergy and Infectious Diseases (NIAID) also participates in the identification and prevention of biowarfare and first released a strategy for biodefense in 2002, periodically releasing updates as new pathogens are becoming topics of discussion. Within this list of strategies, responses for specific infectious agents are provided, along with the classification of these agents. NIAID provides countermeasures after the U.S. Department of Homeland Security details which pathogens hold the most threat.
Planning and response.
Planning may involve the development of biological identification systems.Until recently in the United States, most biological defense strategies have been geared to protecting soldiers on the battlefield rather than ordinary people in cities. Financial cutbacks have limited the tracking of disease outbreaks. Some outbreaks, such as food poisoning due to "E. coli" or "Salmonella", could be of either natural or deliberate origin.
Preparedness<br>
Biological agents are relatively easy to obtain by terrorists and are becoming more threatening in the U.S., and laboratories are working on advanced detection systems to provide early warning, identify contaminated areas and populations at risk, and to facilitate prompt treatment. Methods for predicting the use of biological agents in urban areas as well as assessing the area for the hazards associated with a biological attack are being established in major cities. In addition, forensic technologies are working on identifying biological agents, their geographical origins and/or their initial son. Efforts include decontamination technologies to restore facilities without causing additional environmental concerns.
Early detection and rapid response to bioterrorism depend on close cooperation between public health authorities and law enforcement; however, such cooperation is currently lacking. National detection assets and vaccine stockpiles are not useful if local and state officials do not have access to them.
Biosurveillance<br>
In 1999, the University of Pittsburgh's Center for Biomedical Informatics deployed the first automated bioterrorism detection system, called RODS (Real-Time Outbreak Disease Surveillance). RODS is designed to draw collect data from many data sources and use them to perform signal detection, that is, to detect a possible bioterrorism event at the earliest possible moment. RODS, and other systems like it, collect data from sources including clinic data, laboratory data, and data from over-the-counter drug sales. In 2000, Michael Wagner, the codirector of the RODS laboratory, and Ron Aryel, a subcontractor, conceived the idea of obtaining live data feeds from "non-traditional" (non-health-care) data sources. The RODS laboratory's first efforts eventually led to the establishment of the National Retail Data Monitor, a system which collects data from 20,000 retail locations nationwide.
On February 5, 2002, George W. Bush visited the RODS laboratory and used it as a model for a $300 million spending proposal to equip all 50 states with biosurveillance systems. In a speech delivered at the nearby Masonic temple, Bush compared the RODS system to a modern "DEW" line (referring to the Cold War ballistic missile early warning system).
The principles and practices of biosurveillance, a new interdisciplinary science, were defined and described in the "Handbook of Biosurveillance", edited by Michael Wagner, Andrew Moore and Ron Aryel, and published in 2006. Biosurveillance is the science of real-time disease outbreak detection. Its principles apply to both natural and man-made epidemics (bioterrorism).
Data which potentially could assist in early detection of a bioterrorism event include many categories of information. Health-related data such as that from hospital computer systems, clinical laboratories, electronic health record systems, medical examiner record-keeping systems, 911 call center computers, and veterinary medical record systems could be of help; researchers are also considering the utility of data generated by ranching and feedlot operations, food processors, drinking water systems, school attendance recording, and physiologic monitors, among others. Intuitively, one would expect systems which collect more than one type of data to be more useful than systems which collect only one type of information (such as single-purpose laboratory or 911 call-center based systems), and be less prone to false alarms, and this appears to be the case.
In Europe, disease surveillance is beginning to be organized on the continent-wide scale needed to track a biological emergency. The system not only monitors infected persons, but attempts to discern the origin of the outbreak.
Researchers are experimenting with devices to detect the existence of a threat:
New research shows that ultraviolet avalanche photodiodes offer the high gain, reliability and robustness needed to detect anthrax and other bioterrorism agents in the air. The fabrication methods and device characteristics were described at the 50th Electronic Materials Conference in Santa Barbara on June 25, 2008. Details of the photodiodes were also published in the February 14, 2008 issue of the journal Electronics Letters and the November 2007 issue of the journal IEEE Photonics Technology Letters.
The United States Department of Defense conducts global biosurveillance through several programs, including the Global Emerging Infections Surveillance and Response System.
Response to bioterrorism incident or threat.
Government agencies which would be called on to respond to a bioterrorism incident would include law enforcement, hazardous materials/decontamination units and emergency medical units. The US military has specialized units, which can respond to a bioterrorism event; among them are the United States Marine Corps' Chemical Biological Incident Response Force and the U.S. Army's 20th Support Command (CBRNE), which can detect, identify, and neutralize threats, and decontaminate victims exposed to bioterror agents.
There are four hospitals capable of caring for anyone with an exposure to a BSL3 or BSL4 pathogen, the special clinical studies unit at National Institutes of Health is one of them. National Institutes of Health built a facility in April 2010. This unit has state of the art isolation capabilities with a unique airflow system. This unit is also being trained to care for patients who are ill due to a highly infectious pathogen outbreak, such as ebola. The doctors work closely with USAMRIID, NBACC and IRF. Special trainings take place regularly in order to maintain a high level of confidence to care for these patients.

</doc>
<doc id="45585" url="https://en.wikipedia.org/wiki?curid=45585" title="Emperor Juntoku">
Emperor Juntoku

Genealogy.
Before his ascension to the Chrysanthemum Throne, his personal name (his "imina") was .
He was the third son of Emperor Go-Toba. His mother was Shigeko (重子), the daughter of Fujiwara Hanki (藤原範季)
Events of Juntoku's life.
Morinari-shinnō became Crown Prince in 1200. He was elevated to the throne after Emperor Go-Toba pressured Emperor Tsuchimikado into abdicating.
In actuality, Emperor Go-Toba wielded effective power as a cloistered emperor during the years of Juntoku's reign.
In 1221, he was forced to abdicate because of his participation in Go-Toba's unsuccessful attempt to displace the Kamakura bakufu with re-asserted Imperial power. This political and military struggle was called the Jōkyū War or the Jōkyū Incident ("Jōkyū-no ran").
After the "Jōkyū-no ran", Juntoku was sent into exile on Sado Island (佐渡島 or 佐渡ヶ島, both "Sadogashima"), where he remained until his death in 1242.
This emperor is known posthumously as Sado-no In (佐渡院) because his last years were spent at Sado. He was buried in a mausoleum, the Mano Goryo, on Sado's west coast. Juntoku's official Imperial tomb ("misasagi") is in Kyoto.
Juntoku was tutored in poetry by Fujiwara no Sadaie, who was also known as Teika. One of the emperor's poems was selected for inclusion in the what became a well-known anthology, the Ogura Hyakunin Isshu. This literary legacy in Teika's collection of poems has accorded Juntoku a continuing popular prominence beyond the scope of his other lifetime achievements. The poets and poems of the Hyakunin isshu form the basis for a card game ("uta karuta") which is still widely played today.
"Kugyō".
"Kugyō" (公卿) is a collective term for the very few most powerful men attached to the court of the Emperor of Japan in pre-Meiji eras.
In general, this elite group included only three to four men at a time. These were hereditary courtiers whose experience and background would have brought them to the pinnacle of a life's career. During juntoku's reign, this apex of the "Daijō-kan" included:
Eras of Juntoku's reign.
The years of Juntoku's reign are more specifically identified by more than one era name or "nengō".

</doc>
<doc id="45586" url="https://en.wikipedia.org/wiki?curid=45586" title="Indifference curve">
Indifference curve

In microeconomic theory, an indifference curve is a graph showing different bundles of goods between which a consumer is "indifferent." That is, at each point on the curve, the consumer has no preference for one bundle over another. One can equivalently refer to each point on the indifference curve as rendering the same level of utility (satisfaction) for the consumer. In other words an indifference curve is the locus of various points showing different combinations of two goods providing equal utility to the consumer. Utility is then a device to represent preferences rather than something from which preferences come. The main use of indifference curves is in the representation of potentially observable demand patterns for individual consumers over commodity bundles.
There are infinitely many indifference curves: one passes through each combination. A collection of (selected) indifference curves, illustrated graphically, is referred to as an indifference map.
History.
The theory of indifference curves was developed by Francis Ysidro Edgeworth, who explained in his 1881 book the mathematics needed for their drawing; later on, Vilfredo Pareto was the first author to actually draw these curves, in his 1906 book. The theory can be derived from William Stanley Jevons' ordinal utility theory, which posits that individuals can always rank any consumption bundles by order of preference.
Map and properties of indifference curves.
A graph of indifference curves for several utility levels of an individual consumer is called an indifference map. Points yielding different utility levels are each associated with distinct indifference curves and these indifference curves on the indifference map are like contour lines on a topographical map. Each point on the curve represents the same elevation. If you move "off" an indifference curve traveling in a northeast direction (assuming positive marginal utility for the goods) you are essentially climbing a mound of utility. The higher you go the greater the level of utility. The non-satiation requirement means that you will never reach the "top," or a "bliss point," a consumption bundle that is preferred to all others.
Indifference curves are typically represented to be:
Assumptions of consumer preference theory.
It also implies that the commodities are good rather than bad. Examples of bad commodities can be disease, pollution etc. because we always desire less of such things.
Application.
Consumer theory uses indifference curves and budget constraints to generate consumer demand curves. For a single consumer, this is a relatively simple process. First, let one good be an example market e.g., carrots, and let the other be a composite of all other goods. Budget constraints give a straight line on the indifference map showing all the possible distributions between the two goods; the point of maximum utility is then the point at which an indifference curve is tangent to the budget line (illustrated). This follows from common sense: if the market values a good more than the household, the household will sell it; if the market values a good less than the household, the household will buy it. The process then continues until the market's and household's marginal rates of substitution are equal. Now, if the price of carrots were to change, and the price of all other goods were to remain constant, the gradient of the budget line would also change, leading to a different point of tangency and a different quantity demanded. These price / quantity combinations can then be used to deduce a full demand curve. A line connecting all points of tangency between the indifference curve and the budget constraint is called the expansion path.
Examples of indifference curves.
In Figure 1, the consumer would rather be on "I3" than "I2", and would rather be on "I2" than "I1", but does not care where he/she is on a given indifference curve. The slope of an indifference curve (in absolute value), known by economists as the marginal rate of substitution, shows the rate at which consumers are willing to give up one good in exchange for more of the other good. For "most" goods the marginal rate of substitution is not constant so their indifference curves are curved. The curves are convex to the origin, describing the negative substitution effect. As price rises for a fixed money income, the consumer seeks less the expensive substitute at a lower indifference curve. The substitution effect is reinforced through the income effect of lower real income (Beattie-LaFrance). An example of a utility function that generates indifference curves of this kind is the Cobb-Douglas function formula_1. The negative slope of the indifference curve incorporates the willingness of the consumer to make trade offs.
If two goods are perfect substitutes then the indifference curves will have a constant slope since the consumer would be willing to switch between at a fixed ratio. The marginal rate of substitution between perfect substitutes is likewise constant. An example of a utility function that is associated with indifference curves like these would be formula_2.
If two goods are perfect complements then the indifference curves will be L-shaped. Examples of perfect complements include left shoes compared to right shoes: the consumer is no better off having several right shoes if she has only one left shoe - additional right shoes have zero marginal utility without more left shoes, so bundles of goods differing only in the number of right shoes they include - however many - are equally preferred. The marginal rate of substitution is either zero or infinite. An example of the type of utility function that has an indifference map like that above is the Leontief function: formula_3.
The different shapes of the curves imply different responses to a change in price as shown from demand analysis in consumer theory. The results will only be stated here. A price-budget-line change that kept a consumer in equilibrium on the same indifference curve:
Preference relations and utility.
Choice theory formally represents consumers by a preference relation, and use this representation to derive indifference curves showing combinations of equal preference to the consumer.
Preference relations.
Let
In the language of the example above, the set formula_4 is made of combinations of apples and bananas. The symbol formula_5 is one such combination, such as 1 apple and 4 bananas and formula_6 is another combination such as 2 apples and 2 bananas.
A preference relation, denoted formula_11, is a binary relation define on the set formula_4.
The statement
is described as 'formula_5 is weakly preferred to formula_6.' That is, formula_5 is at least as good as formula_6 (in preference satisfaction).
The statement
is described as 'formula_5 is weakly preferred to formula_6, and formula_6 is weakly preferred to formula_5.' That is, one is "indifferent" to the choice of formula_5 or formula_6, meaning not that they are unwanted but that they are equally good in satisfying preferences.
The statement
is described as 'formula_5 is weakly preferred to formula_6, but formula_6 is not weakly preferred to formula_5.' One says that 'formula_5 is strictly preferred to formula_6.'
The preference relation formula_11 is complete if all pairs formula_33 can be ranked. The relation is a transitive relation if whenever formula_13 and formula_35 then formula_36.
For any element formula_37, the corresponding indifference curve, formula_38 is made up of all elements of formula_4 which are indifferent to formula_40. Formally,
formula_41.
Formal link to utility theory.
In the example above, an element formula_5 of the set formula_4 is made of two numbers: The number of apples, call it formula_44 and the number of bananas, call it formula_45
In utility theory, the utility function of an agent is a function that ranks "all" pairs of consumption bundles by order of preference ("completeness") such that any set of three or more bundles forms a transitive relation. This means that for each bundle formula_46 there is a unique relation, formula_47, representing the utility (satisfaction) relation associated with formula_46. The relation formula_49 is called the utility function. The range of the function is a set of real numbers. The actual values of the function have no importance. Only the ranking of those values has content for the theory. More precisely, if formula_50, then the bundle formula_46 is described as at least as good as the bundle formula_52. If formula_53, the bundle formula_46 is described as strictly preferred to the bundle formula_52.
Consider a particular bundle formula_56 and take the total derivative of formula_47 about this point:
where formula_60 is the partial derivative of formula_47 with respect to its first argument, evaluated at formula_46. (Likewise for formula_63)
The indifference curve through formula_56 must deliver at each bundle on the curve the same utility level as bundle formula_56. That is, when preferences are represented by a utility function, the indifference curves are the level curves of the utility function. Therefore, if one is to change the quantity of formula_66 by formula_67, without moving off the indifference curve, one must also change the quantity of formula_68 by an amount formula_69 such that, in the end, there is no change in "U":
Thus, the ratio of marginal utilities gives the absolute value of the slope of the indifference curve at point formula_56. This ratio is called the marginal rate of substitution between formula_66 and formula_68.
Examples.
Linear utility.
If the utility function is of the form formula_75 then the marginal utility of formula_66 is formula_77 and the marginal utility of formula_68 is formula_79. The slope of the indifference curve is, therefore,
Observe that the slope does not depend on formula_66 or formula_68: the indifference curves are straight lines.
Cobb-Douglas utility.
If the utility function is of the form formula_83 the marginal utility of formula_66 is formula_85 and the marginal utility of formula_68 is formula_87.Where formula_88. The slope of the indifference curve, and therefore the negative of the marginal rate of substitution, is then
CES utility.
A general CES (Constant Elasticity of Substitution) form is
where formula_91 and formula_92. (The Cobb-Douglas is a special case of the CES utility, with formula_93.) The marginal utilities are given by
and
Therefore, along an indifference curve,
These examples might be useful for modelling individual or aggregate demand.
Biology.
As used in Biology, the indifference curve is a model for how animals 'decide' whether to perform a particular behavior, based on changes in two variables which can increase in intensity, one along the x-axis and the other along the y-axis. For example, the x-axis may measure the quantity of food available while the y-axis measures the risk involved in obtaining it. The indifference curve is drawn to predict the animal's behavior at various levels of risk and food availability.

</doc>
<doc id="45587" url="https://en.wikipedia.org/wiki?curid=45587" title="Chukyo">
Chukyo

Chukyo can refer to:
the city of Nagoya (中京 Chūkyō). Various things are named after the city:
仲恭 (Chūkyō)

</doc>
<doc id="45588" url="https://en.wikipedia.org/wiki?curid=45588" title="Emperor Go-Horikawa">
Emperor Go-Horikawa

This 13th-century sovereign was named after the 10th-century Emperor Horikawa and "go-" (後), translates literally as "later"; and thus, he is sometimes called the "Later Emperor Horikawa". The Japanese word "go" has also been translated to mean the "second one;" and in some older sources, this emperor may be identified as "Horikawa, the second," or as "Horikawa II."
Genealogy.
Before his ascension to the Chrysanthemum Throne, his personal name (his "imina") was , also known as Motsihito"-shinnō".
Events of Go-Horikawa's life.
In 1221, because of the Jōkyū Incident, an unsuccessful attempt by Emperor Go-Toba to seize real power, the Kamakura shogunate completely excluded those of the imperial family descended from Emperor Go-Toba from the Chrysanthemum throne, thus forcing Emperor Chūkyō to abdicate. After the Genpei War, he, as the grandson of the late Emperor Takakura, who was also a nephew of the then-exiled Retired Emperor Go-Toba, and Chūkyō's first cousin, was enthroned as Go-Horikawa. He ruled from July 29, 1221 to October 26 (?), 1232.
As Go-Horikawa was only ten-years-old at this time, his father Imperial Prince Morisada acted as cloistered emperor under the name Go-Takakura-in.
In 1232, he began his own cloistered rule, abdicating to his 1-year-old son, Emperor Shijō. However, he had a weak constitution, and his cloistered rule lasted just under two years before he died.
Emperor Go-Horikawa's Imperial tomb ("misasagi") is at Sennyū-ji in the .
"Kugyō".
"Kugyō" (公卿) is a collective term for the very few most powerful men attached to the court of the Emperor of Japan in pre-Meiji eras.
In general, this elite group included only three to four men at a time. These were hereditary courtiers whose experience and background would have brought them to the pinnacle of a life's career. During Go-Horikawa's reign, this apex of the "Daijō-kan" included:
Eras of Go-Horikawa's reign.
The years of Go-Horikawa's reign are more specifically identified by more than one era name or "nengō".

</doc>
<doc id="45589" url="https://en.wikipedia.org/wiki?curid=45589" title="Shijō">
Shijō


</doc>
<doc id="45590" url="https://en.wikipedia.org/wiki?curid=45590" title="Emperor Go-Saga">
Emperor Go-Saga

Emperor Go-Saga (後嵯峨天皇 "Go-Saga-tennō") (April 1, 1220 – March 17, 1272) was the 88th emperor of Japan, according to the traditional order of succession. This reign spanned the years 1242 through 1246.
This 13th-century sovereign was named after the 8th-century Emperor Saga and "go-" (後), translates literally as "later"; and thus, he is sometimes called the "Later Emperor Saga". The Japanese word "go" has also been translated to mean the "second one;" and in some older sources, this emperor may be identified as "Saga, the second," or as "Saga II."
Genealogy.
Before his ascension to the Chrysanthemum Throne, his personal name (his "imina") was .
He was the second son of Emperor Tsuchimikado, and second cousin of his predecessor Emperor Shijō.
Events of Go-Saga's life.
He ruled from February 21, 1242 to February 16, 1246.
When Emperor Tsuchimikado moved to Tosa Province (on Shikoku), he was raised by his mother's side of the family.
Because of the sudden death of Emperor Shijō at the age of 10, the question of succession arose. Because the expectations of the court nobility and the Bakufu conflicted, the issue was bitterly contested. Kujō Michiie and the court nobility supported Prince Tadanari (忠成王), a son of Retired Emperor Juntoku, but the "shikken" Hōjō Yasutoki was opposed to the sons of Juntoku because of his involvement in the Jōkyū War. Michiie instead supported Tsuchimikado's son Prince Kunihito as a neutral figure for Emperor. During these negotiations, there was a vacancy on the throne of 11 days.
In 1242, Prince Kunihito became emperor. In 1246 he abdicated to his son, Emperor Go-Fukakusa, beginning his reign as cloistered emperor. In 1259, he compelled Emperor Go-Fukakusa to abdicate to his younger brother, Emperor Kameyama. Imperial Prince Munetaka became shōgun instead of the Hōjō regents. Henceforth, the shōguns of the Kamakura Bakufu came from the imperial house. Still, the Hōjō regents increased their control of the shogunate, setting up the system of rule by regents.
The descendants of his two sons contested the throne between them, forming into two lines, the Jimyōin-tō (Go-Fukakusa's descendants) and the Daikakuji-tō (Kameyama's descendants).
In 1272, Go-Saga died.
Go-Saga's final resting place is designated as an Imperial mausoleum ("misasagi") at Saa no minami no "Misasagi" in Kyoto.
Kugyō.
In general, this elite group included only three to four men at a time. These were hereditary courtiers whose experience and background would have brought them to the pinnacle of a life's career. During Go-Saga's reign, this apex of the "Daijō-kan included:
Eras of Go-Saga's reign.
The years of Go-saga's reign are more specifically identified by more than one era name or "nengō".

</doc>
<doc id="45591" url="https://en.wikipedia.org/wiki?curid=45591" title="Emperor Go-Fukakusa">
Emperor Go-Fukakusa

This 13th-century sovereign was named after the 9th-century Emperor Nimmyō and "go-" (後), translates literally as "later;" and thus, he could be called the "Later Emperor Fukakusa". The Japanese word "go" has also been translated to mean the "second one;" and in some older sources, this emperor may be identified as "Fukakusa, the second," or as "Fukakusa II."
Name.
Before his ascension to the Chrysanthemum Throne, his personal name (his "imina") was .
Although the Roman-alphabet spelling of the name of this 13th-century emperor is the same as that of the personal name of a current member of the Imperial family, the kanji are different:
He was the second son of Emperor Go-Saga.
Events of Go-Fukakusa's life.
In 1259, at the insistence of Retired Emperor Go-Saga, he abdicated at the age of 15 to his younger brother, who would become Emperor Kameyama.
After Emperor Go-Uda's ascension in 1260, Saionji Sanekane negotiated with the Bakufu, and succeeded in getting Emperor Go-Fukakusa's son Hirohito named as Crown Prince. In 1287, with his ascension as Emperor Fushimi, Go-Fukakusa's cloistered rule began.
In 1290, he entered the priesthood, retiring from the position of cloistered Emperor. But, with his seventh son, Imperial Prince Hisaaki becoming the 8th Kamakura shōgun among other things, the position of his Jimyōin-tō became strengthened.
In 1304, he died. He is enshrined with other emperors at the imperial tomb called "Fukakusa no kita no misasagi" (深草北陵) in Fushimi-ku, Kyoto.
Kugyō.
In general, this elite group included only three to four men at a time. These were hereditary courtiers whose experience and background would have brought them to the pinnacle of a life's career. During Go-Fukakusa's reign, this apex of the "Daijō-kan included:
Eras of Go-Fukakusa's reign.
The years of Go-Fukakusa's reign are more specifically identified by more than one era name or "nengō".

</doc>
<doc id="45592" url="https://en.wikipedia.org/wiki?curid=45592" title="Bartolomeu Dias">
Bartolomeu Dias

Bartolomeu Dias (; Anglicized: Bartholomew Diaz; c. 1451 – 29 May 1500), a nobleman of the Portuguese royal household, was a Portuguese explorer. He sailed around the southernmost tip of Africa in 1488, reaching the Indian Ocean from the Atlantic, the first European known to have done so.
Purposes of the Dias expedition.
Bartolomeu Dias was a Knight of the royal court, superintendent of the royal warehouses, and sailing-master of the man-of-war, "São Cristóvão" (Saint Christopher). King John II of Portugal appointed him, on 10 October 1487, to head an expedition to sail around the southern tip of Africa in the hope of finding a trade route to India. Dias was also charged with searching for the lands ruled by Prester John, who was a fabled Christian priest and ruler.
The expedition.
Dias' ship "São Cristóvão" was piloted by Pêro de Alenquer. A second caravel, the "São Pantaleão", was commanded by João Infante and piloted by Álvaro Martins. Dias' brother Pêro Dias was the captain of the square-rigged support ship with João de Santiago as pilot.
The expedition sailed south along the west coast of Africa. Extra provisions were picked up on the way at the Portuguese fortress of São Jorge de Mina on the Gold Coast. After having sailed past Angola, Dias reached the Golfo da Conceicão (Walvis Bay) by December. Continuing south, he discovered first Angra dos Ilheus, being hit, then, by a violent storm. Thirteen days later, from the open ocean, he searched the coast again to the east, discovering and using the westerlies winds - the ocean gyre, but finding just ocean. Having rounded the Cape of Good Hope at a considerable distance to the west and southwest, he turned towards the east, and taking advantage of the winds of Antarctica that blow strongly in the South Atlantic, he sailed northeast. After 30 days without seeing land, he entered what he named Aguada de São Brás (Bay of Saint Blaise)—later renamed Mossel Bay—on 4 February 1488. Dias's expedition reached its furthest point on 12 March 1488 when they anchored at Kwaaihoek, near the mouth of the Bushman's River, where a padrão—the Padrão de São Gregório—was erected before turning back. Dias wanted to continue sailing to India, but he was forced to turn back when his crew refused to go further. It was only on the return voyage that he actually discovered the Cape of Good Hope, in May 1488. Dias returned to Lisbon in December of that year, after an absence of sixteen months.
The discovery of the passage around southern Africa was significant because, for the first time, Europeans realised they could trade directly with India and the other parts of Asia, bypassing the overland route through the Middle East, with its expensive middlemen. The official report of the expedition has been lost.
Bartolomeu Dias originally named the Cape of Good Hope the "Cape of Storms" ("Cabo das Tormentas"). It was later renamed (by King John II of Portugal) the Cape of Good Hope ("Cabo da Boa Esperança") because it represented the opening of a route to the east.
Follow-up voyages.
After these early attempts, the Portuguese took a decade-long break from Indian Ocean exploration. During that hiatus, it is likely that they received valuable information from a secret agent, Pêro da Covilhã, who had been sent overland to India and returned with reports useful to their navigators.
Using his experience with explorative travel, Dias helped in the construction of the "São Gabriel" and its sister ship, the "São Rafael" that were used by Vasco da Gama to circumnavigate the Cape of Good Hope and continue the route to India. Dias only participated in the first leg of Da Gama's voyage, until the Cape Verde Islands. He was then one of the captains of the second Indian expedition, headed by Pedro Álvares Cabral. This flotilla first reached the coast of Brazil, landing there in 1500, and then continued eastwards to India. Dias perished near the Cape of Good Hope that he presciently had named Cape of Storms. Four ships encountered a huge storm off the cape and were lost, including Dias', on 29 May 1500. A shipwreck found in 2008 by the Namdeb Diamond Corporation off Namibia was at first thought to be Dias' ship; however, recovered coins come from a later time.
Personal life.
Bartolomeu Dias was married and had two children:

</doc>
<doc id="45594" url="https://en.wikipedia.org/wiki?curid=45594" title="Kameyama">
Kameyama

Kameyama may refer to:

</doc>
<doc id="45595" url="https://en.wikipedia.org/wiki?curid=45595" title="Emperor Go-Uda">
Emperor Go-Uda

Emperor Go-Uda (後宇多天皇 "Go-Uda-tennō") (December 17, 1267 – July 16, 1324) was the 91st emperor of Japan, according to the traditional order of succession. His reign spanned the years from 1274 through 1287.
This 13th-century sovereign was named after the 9th-century Emperor Uda and "go-" (後), translates literally as "later"; and thus, he is sometimes called the "Later Emperor Uda". The Japanese word "go" has also been translated to mean the "second one;" and in some older sources, this emperor may be identified as "Uda, the second," or as "Uda II."
Genealogy.
Before his ascension to the Chrysanthemum Throne, his personal name ("imina") was Yohito-shinnō (世仁親王).
He was the second son of Emperor Kameyama. They were from the Daikakuji line.
Events of Go-Uda's life.
Yohito"-shinnō" became crown prince in 1268. According to the terms of the late emperor's will (Go-Saga died in 1272), in 1274, he would become emperor upon the death or abdication of Emperor Kameyama.
The retired Emperor Kameyama continued to exercise power as cloistered emperor.
During his reign, the unsuccessful Mongol invasions of Japan occurred, first in 1274 and again in 1281. Though they established a beachhead at Hakata, Kyushu, they were driven out within a short time.
In 1287, retired Emperor Go-Fukakusa, dissatisfied with the fact that his own lineage (the "Jimyōin-tō") did not control the throne, while that of his younger brother, the retired Emperor Kameyama (the "Daikakuji-tō") did, persuaded both the Bakufu and the imperial court to compel the Emperor to abdicate in favor of Go-Fukakusa's son (Emperor Fushimi).
After this time, the struggle between the Jimyōin-tō and the Daikakuji-tō over the imperial throne continued. After Go-Uda's abdication, his Daikakuji-tō controlled the throne from 1301 to 1308 (Emperor Go-Nijō) and again from 1318 until the era of northern and southern courts (begun 1332) when they became the southern court (ending in 1392).
Go-Uda was cloistered emperor during the reign of his own son, Go-Nijō, from 1301 until 1308, and again from 1318, when his second son Go-Daigo took the throne until 1321, when Go-Daigo began direct rule.
Emperor Go-Uda's Imperial mausoleum is the "Rengebuji no misasagi" (蓮華峯寺陵) in Ukyō-ku, Kyoto.
Kugyō.
"Kugyō" (公卿) is a collective term for the very few most powerful men attached to the court of the Emperor of Japan in pre-Meiji eras. Even during those years in which the court's actual influence outside the palace walls was minimal, the hierarchic organization persisted.
In general, this elite group included only three to four men at a time. These were hereditary courtiers whose experience and background would have brought them to the pinnacle of a life's career. During Go-Uda's reign, this apex of the "Daijō-kan included:
Eras of Go-Uda's reign.
The years of Go-Uda's reign are more specifically identified by more than one era name or "nengō".

</doc>
<doc id="45596" url="https://en.wikipedia.org/wiki?curid=45596" title="Thomas E. Dewey">
Thomas E. Dewey

Thomas Edmund Dewey (March 24, 1902 – March 16, 1971) was the 47th Governor of New York (1943–1954). In 1944 he was the Republican candidate for President, but lost to President Franklin D. Roosevelt in the closest of Roosevelt's four presidential elections. He was again the Republican candidate in 1948, but lost to the incumbent president Harry S. Truman in one of the greatest upsets in presidential election history.
As a New York City prosecutor early in his career, Dewey was relentless in his effort to curb the power of the American Mafia and of organized crime in general. Most famously, he successfully prosecuted Mafioso kingpin Charles "Lucky" Luciano on charges of compulsory prostitution in 1936. Luciano was given a thirty-year prison sentence. Dewey almost succeeded in apprehending Jewish mobster Dutch Schultz as well, but not before Schultz was murdered in 1935 in a hit ordered by The Commission itself.
Dewey led the moderate or progressive faction of the Republican Party, in which he fought conservative Ohio Senator Robert A. Taft. Dewey was an advocate for the professional and business community of the northeastern United States, which would later be called the "Eastern Establishment". This group consisted of internationalists who were in favor of the United Nations and the Cold War fight against communism and the Soviet Union, and it supported most of the New Deal social-welfare reforms enacted during the administration of Franklin D. Roosevelt. In addition he played a large part in the election of Dwight D. Eisenhower as President in 1952. Dewey's successor as leader of the progressive Republicans was Nelson Rockefeller, who became governor of New York in 1959. The New York State Thruway is named in Dewey's honor.
Early life and family.
Dewey was born and raised in Owosso, Michigan, where his father, George Martin Dewey, owned, edited, and published the local newspaper, the "Owosso Times." His mother, Annie Thomas (whom he called "Mater"), bequeathed her son "a healthy respect for common sense and the average man or woman who possessed it. She also left a headstrong assertiveness that many took for conceit, a set of small-town values never entirely erased by exposure to the sophisticated East, and a sense of proportion that moderated triumph and eased defeat." One journalist noted that "a boy he did show leadership and ambition above the average; by the time he was thirteen, he had a crew of nine other youngsters working for him" selling magazines in Owosso. He graduated from the University of Michigan in 1923, and from Columbia Law School in 1925. While at the University of Michigan, he joined Phi Mu Alpha Sinfonia, a national fraternity for men of music, and was a member of the Men's Glee Club. He was an excellent singer with a deep, baritone voice, and in 1923 he finished in third place in the National Singing Contest. He briefly considered a career as a professional singer, but decided against it after a temporary throat ailment convinced him that such a career would be risky. He then decided to pursue a career as a lawyer. He also wrote for "The Michigan Daily," the university's student newspaper.
On June 16, 1928 Dewey married Frances Eileen Hutt. A native of Sherman, Texas, she was a stage actress; after their marriage she dropped her acting career. They had two sons, Thomas E. Dewey Jr. and John Martin Dewey. Although Dewey served as a prosecutor and District Attorney in New York City for many years, his home from 1939 until his death was a large farm, called "Dapplemere," located near the town of Pawling some north of New York City. According to biographer Richard Norton Smith, Dewey "loved Dapplemere as did no other place", and Dewey was once quoted as saying that "I work like a horse five days and five nights a week for the privilege of getting to the country on the weekend." Dewey once told a reporter that "my farm is my roots...the heart of this nation is the rural small town." Dapplemere was part of a tight-knit rural community called Quaker Hill, which was known as a haven for the prominent and well-to-do. Among Dewey's neighbors on Quaker Hill were the famous reporter and radio broadcaster Lowell Thomas, the Reverend Norman Vincent Peale, and the legendary CBS News journalist Edward R. Murrow. Dewey was an active, lifelong member of the Episcopal Church.
Prosecutor.
Federal prosecutor.
Dewey first served as a federal prosecutor, then started a lucrative private practice on Wall Street; however, he left his practice for an appointment as special prosecutor to look into corruption in New York City—with the official title of Chief Assistant U.S. Attorney for the Southern District of New York. It was in this role that he first achieved headlines in the early 1930s, when he prosecuted bootlegger Waxey Gordon.
Dewey had used his excellent recall of details of crimes to trip up witnesses as a federal prosecutor; as a state prosecutor, he used telephone taps (which were perfectly legal at the time) to gather evidence, with the ultimate goal of bringing down entire criminal organizations. On that account, Dewey successfully lobbied for an overhaul in New York's criminal procedure law, which at that time required separate trials for each count of an indictment. Dewey's thoroughness and attention to detail became legendary; for one case he and his staff sifted "through 100,000 telephone slips to convict a Prohibition-era bootlegger".
Special prosecutor.
Dewey rocketed to fame in 1935, when he was appointed special prosecutor in New York County (Manhattan) by Governor Herbert H. Lehman. A "runaway grand jury" had publicly complained that William C. Dodge, the District Attorney, was not aggressively pursuing the mob and political corruption. Lehman, to avoid charges of partisanship, asked four prominent Republicans to serve as special prosecutor. All four refused and recommended Dewey.
Dewey moved ahead vigorously. He recruited a staff of over 60 assistants, investigators, process servers, stenographers, and clerks. New York Mayor Fiorello H. La Guardia assigned a picked squad of 63 police officers to Dewey's office. One writer stated that "Dewey...put on a very impressive show. All the paraphernalia, the hideouts and tapped telephones and so on, became famous. More than any other American of his generation except Lindbergh, Dewey became a creature of folklore and a national hero. What he appealed to most was the great American love of "results." People were much more interested in his ends than in his means. Another key to all this may be expressed in a single word: honesty. Dewey was honest."
Dewey's targets were "organized" racketeering: the large-scale criminal enterprises, especially extortion, the "numbers game" and prostitution. He pursued Tammany Hall political leaders known for their ties to gangsters, such as James Joseph Hines.
One of his biggest prizes was gangster Dutch Schultz, whom he had battled as both a federal and state prosecutor. Schultz's first trial ended in a deadlock; prior to his second trial, Schultz had the venue moved to Malone, New York, then moved there and garnered the sympathy of the townspeople through charitable acts so that when it came time for his trial, the jury found him innocent, liking him too much to convict him.
Dewey and La Guardia threatened Schultz with instant arrest and further charges. Schultz now proposed to murder Dewey. Dewey would be killed while he made his daily morning call to his office from a pay phone near his home. However, New York crime boss Lucky Luciano and the "Mafia Commission" decided that Dewey's murder would provoke an all-out crackdown. Instead they had Schultz killed. Schultz was shot to death in the restroom of a bar in Newark.
Dewey next turned his attention to Luciano. Dewey raided 80 houses of prostitution in the New York City area and arrested hundreds of prostitutes and "madams". Many of the prostitutes — some of whom told of being beaten and abused by Mafia thugs — were willing to testify to avoid prison time. Three implicated Luciano as controller of organized prostitution in the New York/New Jersey area — one of the largest prostitution rings in American history. In the greatest victory of his legal career, Dewey won the conviction of Luciano for the prostitution racket, with a sentence of 30 to 50 years.
However, Dewey did more than simply prosecute gangsters. In 1936 Dewey helped indict and convict Richard Whitney, the former president of the New York Stock Exchange, for embezzlement. Dewey also led efforts to protect dockworkers and poultry farmers and workers from racketeering in New York. In 1936 Dewey received The Hundred Year Association of New York's Gold Medal Award "in recognition of outstanding contributions to the City of New York". In 1939 Dewey prosecuted American Nazi leader Fritz Julius Kuhn for embezzlement, crippling Kuhn's organization and limiting its ability to support Nazi Germany in World War II.
Manhattan District Attorney.
In 1937 Dewey was elected District Attorney of New York County (Manhattan), defeating the Democratic nominee after Dodge decided not to run for re-election. Dewey was such a popular candidate for District Attorney that "election officials in Brooklyn posted large signs at polling places reading 'Dewey Isn't Running in This County'." By the late 1930s Dewey's successful efforts against organized crime—and especially his conviction of Lucky Luciano—had turned him into a national celebrity. His nickname, the "Gangbuster", was used for the popular 1930s "Gang Busters" radio series based on his fight against the mob. Hollywood film studios made several movies inspired by his exploits; "Marked Woman" starred Humphrey Bogart as a Dewey-like DA and Bette Davis as a "party girl" whose testimony helps convict the gang boss. A popular story from the time, possibly apocryphal, featured a young girl who told her father that she wanted to sue God to stop a prolonged spell of rain. When her father replied "you can't sue God and win", the girl said "I can if Dewey is my lawyer."
Governor of New York.
In 1938 Edwin Jaeckle, the New York Republican Party Chairman, selected Dewey to run, unsuccessfully, for Governor of New York against the popular Democratic incumbent Herbert H. Lehman. Dewey was only 36 years of age. He based his campaign on his record as a famous prosecutor of organized-crime figures in New York City. Although he was defeated, Dewey's surprisingly strong showing against Lehman (he lost by only 1.4%) brought him national political attention and made him a frontrunner for the 1940 Republican presidential nomination. Jaeckle was one of Dewey's top advisors and mentors for the remainder of his political career.
In 1942 Dewey ran for governor again, and won with a large plurality over Democrat John J. Bennett Jr. Bennett was not endorsed by the American Labor Party, whose candidate drew almost 10%. The ALP did endorse incumbent Lieutenant Governor Charles Poletti who lost narrowly to Dewey's running mate Thomas W. Wallace. In 1946 Dewey was re-elected by the greatest margin in state history to that point, almost 1.3 million votes. In 1950, he was elected to a third term by 572,000 votes.
Usually regarded as an honest and highly effective governor, Dewey doubled state aid to education; increased salaries for state employees; and still reduced the state's debt by over $100 million. He referred to his program as "pay-as-you-go liberalism...government can be progressive and solvent at the same time." Additionally he put through the first state law in the country that prohibited racial discrimination in employment. As governor, Dewey signed legislation that created the State University of New York. He played a leading role in securing support and funding for the New York State Thruway, which was eventually named in his honor. Dewey also streamlined and consolidated many state agencies to make them more efficient. During the Second World War construction in New York was limited, which allowed Dewey to create a $623 million budget surplus, which he placed into his "Postwar Reconstruction Fund." The fund would eventually create 14,000 new beds in the state's mental health system, provide public housing for 30,000 families, allow for the reforestation of 34 million trees, create a water pollution program, provide slum clearance, and pay for a "model veterans' program." Shortly after becoming governor in 1943, Dewey learned that some state workers and teachers were being paid only $900 a year, leading him to give "hefty raises, some as high as 150%" to state workers and teachers. His governorship was also "friendlier by far than his predecessors to the private sector", as Dewey created a state Department of Commerce to "lure new businesses and tourists to the Empire State, ease the shift from wartime boom, and steer small businessmen, in particular, through the maze of federal regulation and restriction." Between 1945 and 1948, 135,000 new businesses were started in New York.
One criticism that was made of Dewey as governor lay in his treatment of New York legislators and political opponents. Dewey "cracked the whip ruthlessly on (Republican) legislators who strayed from the party fold. Assemblymen have found themselves under investigation by the State Tax Department after opposing the Governor over an insurance regulation bill. Others discover job-rich construction projects, state buildings, even highways, directed to friendlier name="Smith, p. 38">(Smith, p. 38)</ref> Dewey "forced the legislature his own party dominates to reform its comfortable ways of payroll padding. Now legislative workers must verify in writing every two weeks what they have been doing to earn their salary; every state senator and assemblyman must verify that [they are telling the truth. All this has occasioned more than grumbling. Some Assemblymen have quit in protest. Others have been denied renomination by Dewey's formidable political organization. Reporters mutter among themselves about government by blackmail." However, Dewey did receive positive publicity for his reputation for honesty and integrity, as he "insisted on having every prospective holder of a job paying $2,500 or more rigorously probed by state police... accepted no anonymous campaign contributions, and had every large contributor not known personally to him investigated for motive", and, when he signed autographs, he would date them so that no one could imply a closer relationship than actually existed.
The journalists Neal Peirce and Jerry Hagstrom summarized Dewey's governorship by writing that "for sheer administrative talent, it is difficult to think of a twentieth-century governor who has excelled Thomas E. Dewey...hundreds of thousands of New York youngsters owe Dewey thanks for his leadership in creating a state university...a vigorous health-department program virtually eradicated tuberculosis in New York, highway building was pushed forward, and the state's mental hygiene program was thoroughly reorganized." With Jaeckle's help, Dewey also created a powerful political organization that allowed him to dominate New York state politics and influence national politics.
During his governorship, one writer observed that "A blunt fact about Mr. Dewey should be faced: it is that many people do not like him. He is, unfortunately, one of the least seductive personalities in public life. That he has made an excellent record as governor is indisputable. Even so, people resent what they call his vindictiveness, the "metallic" nature of his efficiency, his cockiness (which actually conceals a nature basically shy), and his suspiciousness. People say...that he is as devoid of charm as a rivet or a lump of stone."
He also strongly supported the death penalty. During his 12 years as Governor, over 90 people were electrocuted under New York authority. Among these were several of the mob-affiliated hitmen belonging to the murder-for-hire group Murder, Inc., which was headed up by major mob leaders Louis "Lepke" Buchalter and Albert Anastasia. Lepke himself went to the chair in 1944.
Presidential elections.
1940.
Dewey sought the 1940 Republican presidential nomination. He was considered the early favorite for the nomination, but his support ebbed in the late spring of 1940, as World War II suddenly became much more dangerous for America.
Some Republican leaders considered Dewey to be too young (he was only 38, just three years above the minimum age required by the US Constitution) and too inexperienced to lead the nation in wartime. Furthermore, Dewey's non-interventionist stance became problematic when Germany quickly conquered France, and seemed poised to invade Britain. As a result, many Republicans switched to Wendell Willkie, who was a decade older and supported aid to the Allies fighting Germany. Willkie lost to Franklin D. Roosevelt in the general election.
Dewey's foreign-policy position evolved during the 1940s; by 1944 he was considered an internationalist and a supporter of projects such as the United Nations. It was in 1940 that Dewey first clashed with Taft. Taft—who maintained his non-interventionist views and economic conservatism to his death—became Dewey's great rival for control of the Republican Party in the 1940s and early 1950s. Dewey became the leader of moderate-to-liberal Republicans, who were based in the northeastern, Pacific Coast and western states, while Taft became the leader of conservative Republicans who dominated most of the Midwest and parts of the South.
Dewey's biographer Richard Norton Smith wrote, "For fifteen years...these two combatants waged political warfare. Their dispute pitted East against Midwest, city against countryside, internationalist against isolationist, pragmatic liberals against principled conservatives. Each man thought himself the genuine spokesman of the future; each denounced the other as a political heretic." In a 1949 speech, Dewey criticized Taft and his followers by saying that "we have in our party some fine, high-minded patriotic people who honestly oppose farm price supports, unemployment insurance, old age benefits, slum clearance, and other social programs...these people believe in a laissez-faire society and look back wistfully to the miscalled 'good old days' of the nineteenth century...if such efforts to turn back the clock are actually pursued, you can bury the Republican Party as the deadest pigeon in the country." He added that people who opposed such social programs should "go out and try to get elected in a typical American community and see what happens to them. But they ought not to do it as Republicans."
However, in the speech Dewey added that the Republican Party believed in social progress "under a flourishing, competitive system of private enterprise where every human right is expanded...we are opposed to delivering the nation into the hands of any group who will have the power to tell the American people whether they may have food or fuel, shelter or jobs." Dewey believed in what he called "compassionate capitalism", and argued that "in the modern age, man's needs include as much economic security as is consistent with individual freedom." When Taft and his supporters criticized Dewey's policies as liberal "me-tooism", or "aping the New Deal in a vain attempt to outbid Roosevelt's heirs", Dewey responded that he was following in the tradition of Republicans such as Abraham Lincoln and Theodore Roosevelt, and that "it was conservative reforms like anti-trust laws and federal regulation of railroads...that retained the allegiance of the people for a capitalist system combining private incentive and public conscience."
1944.
Dewey was the frontrunner for the 1944 Republican nomination. In April 1944 he won the key Wisconsin primary, where he defeated Wendell Willkie, former Minnesota Governor Harold Stassen, and General Douglas MacArthur. Willkie's poor showing in Wisconsin forced him to quit the race. At the 1944 Republican Convention, Dewey's chief rivals—Stassen and Ohio Governor John W. Bricker—both withdrew and Dewey was nominated almost unanimously. Dewey then made Bricker (who was supported by Taft) his running mate. This made Dewey the first presidential candidate to be born in the 20th century. As of 2016, he was also the youngest Republican presidential nominee.
In the general election campaign, Dewey crusaded against the alleged inefficiencies, corruption and Communist influences in incumbent President Roosevelt's New Deal programs, but avoided military and foreign policy debates.
Dewey lost the election on November 7, 1944 to President Roosevelt. However, he polled 45.9% of the popular vote compared to Roosevelt's 53.4%, a stronger showing against FDR than any previous Republican opponent. However, Dewey received slightly fewer votes (22 million to FDR's 25 million) in total than Willkie did four years before. In the Electoral College, Roosevelt defeated Dewey by a margin of 432 to 99.
Dewey nearly included, in his campaign, claims that Roosevelt knew ahead of time about the attack on Pearl Harbor; Dewey added, "and instead of being re-elected he should be impeached." The U.S. military was extremely worried because that would let the Japanese know that the U.S. had broken the Purple code. Army General George C. Marshall made a persistent effort to persuade Dewey not to touch this topic; Dewey eventually yielded.
1948.
Dewey was the Republican candidate in the 1948 presidential election in which, in almost unanimous predictions by pollsters and the press, he was projected as the winner. His running mate was California governor Earl Warren. The "Chicago Daily Tribune" printed "DEWEY DEFEATS TRUMAN" as its post-election headline, issuing a few hundred copies before the returns showed that the winner was Harry S. Truman, the incumbent.
Indeed, given Truman's sinking popularity and the Democratic Party's three-way split (between Truman, Henry A. Wallace, and Strom Thurmond), Dewey had seemed unstoppable. Republicans figured that all they had to do to win was to avoid making any major mistakes, and as such Dewey did not take any risks. He spoke in platitudes, trying to transcend politics. Speech after speech was filled with empty statements of the obvious, such as the famous quote: "You know that your future is still ahead of you." An editorial in the "Louisville Courier-Journal" summed it up:
No presidential candidate in the future will be so inept that four of his major speeches can be boiled down to these historic four sentences: Agriculture is important. Our rivers are full of fish. You cannot have freedom without liberty. Our future lies ahead.
Part of the reason Dewey ran such a cautious, vague campaign came from his experience as a presidential candidate in 1944. In that election Dewey felt that he had allowed Roosevelt to draw him into a partisan, verbal "mudslinging" match, and he believed that this had cost him votes. As such, Dewey was convinced in 1948 to appear as non-partisan as possible, and to emphasize the positive aspects of his campaign while ignoring his opponent. This strategy proved to be a major mistake, as it allowed Truman to repeatedly criticize and ridicule Dewey, while Dewey never answered any of Truman's criticisms. Near the end of the campaign, Dewey considered adopting a more aggressive style and responding directly to Truman's criticisms, going so far as to tell his aides one evening that he wanted to "tear to shreds" a speech draft and make it more critical of the Democratic ticket. However, nearly all of his major advisors - including Edwin Jaeckle, Press Secretary James Hagerty, and aide Paul Lockwood - insisted that it would be a mistake to change tactics. Dewey's wife Frances strongly opposed her husband changing tactics, telling him, "If I have to stay up all night to see that you don't tear up that speech , I will." Dewey relented and continued to ignore Truman's attacks and to focus on positive generalities instead of issue specifics.
Dewey was not as conservative as the Republican-controlled 80th Congress, which also proved problematic for him. Truman tied Dewey to the "do-nothing" Congress. Indeed, Dewey had successfully battled Taft and his conservatives for the nomination at the Republican Convention. Taft had remained a non-interventionist even through the Second World War. Dewey, however, supported the Marshall Plan, the Truman Doctrine, recognition of Israel, and the Berlin airlift.
Dewey was repeatedly urged by the right wing of his party to engage in red-baiting, but he refused. In a debate before the Oregon primary with Harold Stassen, Dewey argued against outlawing the Communist Party of the United States of America, saying "you can't shoot an idea with a gun." He later told Styles Bridges, the Republican national campaign manager, that he was not "going around looking under beds". Dewey has been the only Republican to be nominated for President twice and lose both times.
1952.
Dewey did not run for President in 1952, but he played a major role in securing the Republican nomination for General Dwight Eisenhower. The 1952 campaign culminated in a climactic moment in the fierce rivalry between Dewey and Taft for control of the Republican Party.
Dewey played a key role in convincing Eisenhower to run against Taft. When Eisenhower became a candidate Dewey used his powerful political machine to win Eisenhower the support of delegates in New York and elsewhere.
Taft was an announced candidate and, given his age, he freely admitted 1952 would be his last chance to win the presidency. At the Republican Convention, pro-Taft delegates and speakers verbally attacked Dewey as the real power behind Eisenhower, but Dewey had the satisfaction of seeing Eisenhower win the nomination and end Taft's presidential hopes for the last time.
Dewey played a major role in helping California Senator Richard Nixon become Eisenhower's running mate. When Eisenhower won the presidency later that year, many of Dewey's closest aides and advisors became leading figures in the Eisenhower Administration. Among them were Herbert Brownell, who would become Eisenhower's Attorney General, James Hagerty, who would become White House Press Secretary, and John Foster Dulles, who would become Eisenhower's Secretary of State.
Later career.
Dewey's third term as governor of New York expired at the end of 1954, after which he retired from public service and returned to his law practice, Dewey Ballantine, although he remained a power broker behind the scenes in the Republican Party. In 1956, when Eisenhower mulled not running for a second term, he suggested Dewey as his choice as successor, but party leaders made it plain that they would not entrust the nomination to Dewey yet again, and ultimately Eisenhower decided to run for re-election. Dewey also played a major role that year in convincing Eisenhower to keep Nixon as his running mate; Eisenhower had considered dropping Nixon from the Republican ticket and picking someone he felt would be less partisan and controversial. However, Dewey argued that dropping Nixon from the ticket would only anger Republican voters while winning Eisenhower few votes from the Democrats. Dewey's arguments helped convince Eisenhower to keep Nixon on the ticket. In 1960 Dewey would strongly support Nixon's ultimately unsuccessful presidential campaign against Democrat John F. Kennedy.
By the 1960s, as the conservative wing assumed more and more power within the Republican Party, Dewey removed himself further and further from party matters. When the Republicans in 1964 gave Senator Barry Goldwater of Arizona, Taft's successor as the conservative leader, their presidential nomination, Dewey declined to even attend the GOP Convention in San Francisco; it was the first Republican Convention he had missed since 1936. President Lyndon Johnson offered Dewey a number of positions on several blue ribbon commissions, as well as a seat on the U.S. Supreme Court, but Dewey declined them all, for he preferred to remain in political retirement and concentrate on his highly profitable law firm. By the early 1960s Dewey's law practice had made him into a multimillionaire.
Although closely identified with the Republican Party for virtually his entire adult life, Dewey was a close friend of Democratic Senator Hubert H. Humphrey, and Dewey aided Humphrey in being named as the Democratic nominee for vice-president in 1964, advising Lyndon Johnson on ways to block efforts at the party convention by Kennedy loyalists to stampede Robert Kennedy onto the ticket as Johnson's running mate.
Death.
Dewey's wife Frances died in the summer of 1970, after battling breast cancer for six years. Later in 1970 Dewey began to date actress Kitty Carlisle, and there was talk of marriage between them. However, he died suddenly of a massive heart attack on March 16, 1971, while vacationing with friend Dwayne Andreas in Miami, Florida, following a round of golf with Boston Red Sox player Carl Yastrzemski. He was 68 years old, dying eight days before his 69th birthday. Following a public memorial service at Saint James' Episcopal Church in New York City, which was attended by President Richard Nixon, former Vice-President Humphrey, and other prominent politicians, Dewey was buried next to his wife Frances in the town cemetery of Pawling, New York. After his death his farm of Dapplemere was sold and renamed "Dewey Lane Farm" in his honor.
Public perception.
Dewey first came to nationwide attention as the "gangbuster", becoming a household name in the U.S. even before he entered presidential politics. At the age of 37, he was perceived as a rising star in the Republican Party and frontrunner for the presidential nomination in 1940. During that campaign with the war in Europe intensifying, he was widely considered too young and inexperienced for the presidency and lost the nomination to Wendell Willkie. His visibility propelled him to the governorship in 1942 and the 1944 Republican presidential nomination. Dewey was a forceful and inspiring speaker, traveling the whole country during his presidential campaigns and attracting uncommonly huge crowds.
During the 1944 election campaign, Dewey suffered an unexpected blow when a remark attributed to socialite Alice Roosevelt Longworth (daughter of Theodore Roosevelt) mocked Dewey as "the little man on the wedding cake" (alluding to his neat mustache and dapper dress). It was ridicule he could never shake. Several commentators and analysts in 1948 attributed the falloff in Dewey's popularity late in his presidential campaign, in part, to his distinctive mustache and resemblance to actor Clark Gable, which was said to raise doubts with voters as to the seriousness of Dewey as prospective leader of the Free World. Roger Masters, a professor of government at Dartmouth College, wrote: "The shaved face has become a reflection of the Protestant ethic. Politicians are supposed to control nature in some sense, so beards and mustaches, which imply a reluctance to control nature, are now reserved for artisans or academics."
Generally Dewey received varied reactions from the public, most praising his good intentions, honesty, administrative talents, and vague yet inspiring speeches, but most also criticizing his perceived stiffness, coldness, and aggressiveness in public. One of his biographers wrote that he had "a personality that attracted contempt and adulation in equal proportion." His friend and neighbor Lowell Thomas believed that Dewey was "an authentic colossus" whose "appetite for excellence to frighten less obsessive types", and his 1948 running mate, California Governor and future Chief Justice Earl Warren, "professed little personal affection for Dewey, but [believed him a born executive who would make a great president." On the other hand, President Franklin D. Roosevelt privately called Dewey "the little man" and a "son of a bitch", and to Robert Taft and other conservative Republicans Dewey "became synonymous with...New York newspapers, New York banks, New York arrogance - the very city Taft's America loves to hate." A Taft supporter once referred to Dewey as "that snooty little governor of New York."
Dewey alienated former Republican president Herbert Hoover, who confided to a friend "Dewey has no inner reservoir of knowledge on which to draw for his thinking," elaborating that "A man couldn't wear a mustache like that without having it affect his mind." However, the famed newspaper editor William Allen White praised Dewey as "an honest cop with the mind of an honest cop" and the pollster George Gallup once stated that Dewey was "the ablest public figure of his lifetime...the most misunderstood man in recent American history."
His presidential campaigns were hampered by Dewey's habit of making overly vague statements, defining his strategy as not being "prematurely specific" on controversial issues. In 1948, President Truman poked fun at Dewey's vague campaign by joking that the GOP (Republican Party) actually stood for "grand old platitudes." Dewey's frequent refusal to discuss specific issues and proposals in his campaigns was based partly on his belief in public opinion polls; one biographer claimed that he "had an almost religious belief in the revolutionary science of public-opinion sampling." He was the first presidential candidate to employ his own team of pollsters, and when a worried businessman told Dewey in the 1948 presidential campaign that he was losing ground to Truman and urged him to "talk specifics in his closing speeches", Dewey and his aide Paul Lockwood displayed polling data that showed Dewey still well ahead of Truman, and Dewey told the businessman "when you're leading, don't talk."
In 1940, Walter Lippman regarded him as an opportunist, who "changes his views from hour to hour… always more concerned with taking the popular position than he is in dealing with the real issues." The journalist John Gunther wrote that "There are plenty of vain and ambitious and uncharming politicians. This would not be enough to cause Dewey's lack of popularity. What counts more is that so many people think of him as opportunistic. Dewey seldom goes out on a limb by taking a personal position which may be unpopular...every step is carefully calculated and prepared." Adding to that, he had a tendency towards pomposity and was considered stiff and unapproachable in public, with his aide Ruth McCormick Simms once describing him as "cold, cold as a February iceberg". She however added that "he was brilliant and thoroughly honest." Leo O'Brien, a reporter for the United Press International (UPI), recalled Dewey in an interview by saying that "I hated his guts when he first came to Albany, and I loved him by the time he left. It was almost tragic – how he put on a pose that alienated people. Behind a pretty thin veneer he was a wonderful guy." John Gunther wrote in 1947 that some "people may not "like" Dewey, but (a) an inner core of advisers and friends, including some extremely distinguished people, have a loyalty to him little short of idolatrous, and (b) he is one of the greatest vote-getters in the history of the nation."
Journalist Irwin Ross summed up the contradictions in Dewey's personality by noting that "more than most politicians, he displayed an enormous gap between his private and his public manner. To friends and colleagues he was warm and gracious, considerate of others' views… He could tell a joke and was not dismayed by an off-color story. In public, however, he tended to freeze up, either out of diffidence or too stern a sense of the dignity of office. The smiles would seem forced… the glad-handing gesture awkward."
Legacy.
In 1964, the New York State legislature officially renamed the New York State Thruway in honor of Dewey. Signs on Interstate 95 between the end of the Bruckner Expressway (in the Bronx) and the Connecticut state line, as well as on the Thruway mainline (Interstate 87 between the Bronx-Westchester line and Albany, and Interstate 90 between Albany and the New York-Pennsylvania line) designate the name as "Governor Thomas E. Dewey Thruway," though this official designation is rarely used in reference to these roads.
Dewey's official papers from his years in politics and public life were given to the University of Rochester; they are housed in the university library and are available to historians and other writers.
In 2005, the New York City Bar Association named an award after Dewey. The Thomas E. Dewey Medal, sponsored by the law firm of Dewey & LeBoeuf LLP, is awarded annually to one outstanding Assistant District Attorney in each of New York City's five counties (New York, Kings, Queens, Bronx, and Richmond). The Medal was first awarded on November 29, 2005.
In May 2012, Dewey & LeBoeuf (the successor firm to Dewey Ballantine) filed for bankruptcy.

</doc>
<doc id="45597" url="https://en.wikipedia.org/wiki?curid=45597" title="Henry V of England">
Henry V of England

Henry V (9 August 1386 – 31 August 1422) was King of England from 1413 until his death at the age of 36 in 1422. He was the second English monarch who came from the House of Lancaster.
After military experience fighting the Welsh during the revolt of Owain Glyndŵr, and against the powerful aristocratic Percys of Northumberland at the Battle of Shrewsbury, Henry came into political conflict with his father, whose health was increasingly precarious from 1405 onward. After his father's death in 1413, Henry assumed control of the country and embarked on war with France in the ongoing Hundred Years' War (1337–1453) between the two nations. His military successes culminated in his famous victory at the Battle of Agincourt (1415) and saw him come close to conquering France. After months of negotiation with Charles VI of France, the Treaty of Troyes (1420) recognized Henry V as regent and heir apparent to the French throne, and he was subsequently married to Charles's daughter, Catherine of Valois (1401–37). Following Henry V's sudden and unexpected death in France two years later, he was succeeded by his infant son, who reigned as Henry VI (1422–61, 1470–71).
Early life.
Henry was born in the tower above the gatehouse of Monmouth Castle, Monmouth, Principality of Wales (and for that reason was sometimes called Henry of Monmouth). He was the son of 20-year-old Henry of Bolingbroke (later Henry IV of England), and 16-year-old Mary de Bohun. He was also the grandson of the influential John of Gaunt and great-grandson of Edward III of England. At the time of his birth, Richard II of England, his cousin once removed, was king. As he was not close to the line of succession to the throne, Henry's date of birth was not officially documented. His grandfather, John of Gaunt, was the guardian of the king at that time.
Upon the exile of Henry's father in 1398, Richard II of England took the boy into his own charge and treated him kindly. The young Henry accompanied King Richard to Ireland, and while in the royal service, he visited Trim Castle in County Meath, the ancient meeting place of the Irish Parliament. In 1399, Henry's grandfather died. The same year King Richard II was overthrown by the Lancastrian usurpation that brought Henry's father to the throne, and Henry was recalled from Ireland into prominence as heir apparent to the Kingdom of England. He was created Prince of Wales at his father's coronation, and Duke of Lancaster on 10 November 1399, the third person to hold the title that year. His other titles were Duke of Cornwall, Earl of Chester, and Duke of Aquitaine. A contemporary record notes that during that year Henry spent time at The Queen's College, Oxford, under the care of his uncle Henry Beaufort, the Chancellor of the university. From 1400 to 1404, he carried out the duties of High Sheriff of Cornwall.
Less than three years later, Henry was in command of part of the English forces—he led his own army into Wales against Owain Glyndŵr and joined forces with his father to fight Harry Hotspur at the Battle of Shrewsbury in 1403. It was there that the sixteen-year-old prince was almost killed by an arrow that became stuck in his face. An ordinary soldier might have died from such a wound, but Henry had the benefit of the best possible care. Over a period of several days, John Bradmore, the royal physician, treated the wound with honey to act as an antiseptic, crafted a tool to screw into the broken arrow shaft and thus extract the arrow without doing further damage, and then flushed the wound with alcohol. The operation was successful, but it left Henry with permanent scars, evidence of his experience in battle. For eighteen months, in 1410–11, Henry was in control of the country during his father's ill health, and he took full advantage of the opportunity to impose his own policies, but when the king recovered, he reversed most of these and dismissed the prince from his council.
Role in government and conflict with Henry IV.
The Welsh revolt of Owain Glyndŵr absorbed Henry's energies until 1408. Then, as a result of the king's ill health, Henry began to take a wider share in politics. From January 1410, helped by his uncles Henry Beaufort and Thomas Beaufort – legitimised sons of John of Gaunt – he had practical control of the government.
Both in foreign and domestic policy he differed from the king, who in November 1411 discharged the prince from the council. The quarrel of father and son was political only, though it is probable that the Beauforts had discussed the abdication of Henry IV, and their opponents certainly endeavoured to defame the prince.
Supposed riotous youth.
It may be that the tradition of Henry's riotous youth, immortalised by Shakespeare, is partly due to political enmity. Henry's record of involvement in war and politics, even in his youth, disproves this tradition. The most famous incident, his quarrel with the chief justice, has no contemporary authority and was first related by Sir Thomas Elyot in 1531.
The story of Falstaff originated in Henry's early friendship with Sir John Oldcastle, a supporter of the Lollards. Shakespeare's Falstaff was originally named "Oldcastle", following his main source, "The Famous Victories of Henry V". However, his descendants objected, and the name was changed (the character became a composite of several real persons, including Sir John Fastolf). That friendship, and the prince's political opposition to Thomas Arundel, Archbishop of Canterbury, perhaps encouraged Lollard hopes. If so, their disappointment may account for the statements of ecclesiastical writers like Thomas Walsingham that Henry, on becoming king, was suddenly changed into a new man.
Accession to the throne.
After Henry IV died on 20 March 1413, Henry V succeeded him and was crowned on 9 April 1413 at Westminster Abbey, London, Kingdom of England. The ceremony was marked by a terrible snowstorm, but the common people were undecided as to whether it was a good or bad omen. Henry was described as having been "very tall (6ft 3 in), slim, with dark hair cropped in a ring above the ears, and clean-shaven". His complexion was ruddy, the face lean with a prominent and pointed nose. Depending on his mood, his eyes "flashed from the mildness of a dove's to the brilliance of a lion's".
Domestic policy.
Henry tackled all of the domestic policies together and gradually built on them a wider policy. From the first, he made it clear that he would rule England as the head of a united nation. On the one hand, he let past differences be forgotten – the late Richard II was honourably re-interred; the young Mortimer was taken into favour; the heirs of those who had suffered in the last reign were restored gradually to their titles and estates. On the other hand, where Henry saw a grave domestic danger, he acted firmly and ruthlessly – such as the Lollard discontent in January 1414, including the execution by burning of Henry's old friend Sir John Oldcastle in 1417, so as to "nip the movement in the bud" and make his own position as ruler secure.
His reign was generally free from serious trouble at home. The exception was the Southampton Plot in favour of Mortimer, involving Henry Scrope, 3rd Baron Scrope of Masham and Richard, Earl of Cambridge (grandfather of the future King Edward IV of England), in July 1415.
Starting in August 1417, Henry V promoted the use of the English language in government, and his reign marks the appearance of Chancery Standard English as well as the adoption of English as the language of record within Government. He was the first king to use English in his personal correspondence since the Norman conquest, which had occurred 350 years earlier.
Foreign affairs.
Diplomacy.
Henry could now turn his attention to foreign affairs. A writer of the next generation was the first to allege that Henry was encouraged by ecclesiastical statesmen to enter into the French war as a means of diverting attention from home troubles. This story seems to have no foundation. Old commercial disputes and the support the French had lent to Owain Glyndŵr were used as an excuse for war, while the disordered state of France afforded no security for peace. The French king, Charles VI of France, was prone to mental illness; at times he thought he was made of glass, and his eldest son was an unpromising prospect. However, it was the old dynastic claim to the throne of France, first pursued by Edward III of England, that justified war with France in English opinion.
Following Agincourt, Sigismund, then King of Hungary and later Holy Roman Emperor, made a visit to Henry in hopes of making peace between England and France. His goal was to persuade Henry to modify his demands against the French. Henry lavishly entertained the emperor and even had him enrolled in the Order of the Garter. Sigismund, in turn, inducted Henry into the Order of the Dragon. Henry had intended to crusade for the order after uniting the English and French thrones, but he died before fulfilling his plans. Sigismund left England several months later, having signed the Treaty of Canterbury, acknowledging English claims to France.
Campaigns in France.
Henry may have regarded the assertion of his own claims as part of his royal duty, but in any case, a permanent settlement of the national debate was essential to the success of his foreign policy.
1415 campaign.
On 12 August 1415, Henry sailed for France, where his forces besieged the fortress at Harfleur, capturing it on 22 September. Afterwards, Henry decided to march with his army across the French countryside towards Calais, despite the warnings of his council. On 25 October 1415, on the plains near the village of Agincourt, a French army intercepted his route. Despite his men-at-arms being exhausted, outnumbered and malnourished, Henry led his men into battle, decisively defeating the French, who suffered severe losses. It is often argued that the French men-at-arms were bogged down in the muddy battlefield, soaked from the previous night of heavy rain, and that this hindered the French advance, allowing them to be sitting targets for the flanking English and Welsh archers. Most were simply hacked to death while completely stuck in the deep mud. Nevertheless, the victory is seen as Henry's greatest, ranking alongside the battle of Poitiers.
During the battle, Henry ordered that the French prisoners taken during the battle be put to death, including some of the most illustrious who could be used for ransom. Cambridge Historian Brett Tingley posits that Henry was concerned that the prisoners might turn on their captors when the English were busy repelling a third wave of enemy troops, thus jeopardising a hard-fought victory.
The victorious conclusion of Agincourt, from the English viewpoint, was only the first step in the campaign to recover the French possessions that he felt belonged to the English crown. Agincourt also held out the promise that Henry's pretensions to the French throne might be realised.
Diplomacy and command of the sea.
Command of the sea was secured by driving the Genoese allies of the French out of the English Channel. While Henry was occupied with peace negotiations in 1416, a French and Genoese fleet surrounded the harbour at the English-garrisoned Harfleur. A French land force also besieged the town. To relieve Harfleur, Henry sent his brother, John of Lancaster, the Duke of Bedford, who raised a fleet and set sail from Beachy Head on 14 August. The Franco-Genoese fleet was defeated the following day after a gruelling seven-hour battle, and Harfleur was relieved. Diplomacy successfully detached Emperor Sigismund, Holy Roman Emperor, from France, and the Treaty of Canterbury (1416) paved the way to end the Western Schism in the Church.
1417–20 campaign.
So, with those two potential enemies gone, and after two years of patient preparation following the Battle of Agincourt, Henry renewed the war on a larger scale in 1417. Lower Normandy was quickly conquered, and Rouen was cut off from Paris and besieged. This siege cast an even darker shadow on the reputation of the king than his order to slay the French prisoners at Agincourt. Rouen, starving and unable to support the women and children of the town, forced them out through the gates believing that Henry would allow them to pass through his army unmolested. However, Henry refused to allow this, and the expelled women and children died of starvation in the ditches surrounding the town. The French were paralysed by the disputes between Burgundians and Armagnacs. Henry skilfully played them off one against the other, without relaxing his warlike approach.
In January 1419, Rouen fell. Those Norman French who had resisted were severely punished: Alain Blanchard, who had hanged English prisoners from the walls of Rouen, was summarily executed; Robert de Livet, Canon of Rouen, who had excommunicated the English king, was packed off to England and imprisoned for five years.
By August, the English were outside the walls of Paris. The intrigues of the French parties culminated in the assassination of John the Fearless by the Dauphin's partisans at Montereau (10 September 1419). Philip the Good, the new Duke, and the French court threw themselves into Henry's arms. After six months of negotiation, the Treaty of Troyes recognised Henry as the heir and regent of France (see English Kings of France), and on 2 June 1420 at Troyes Cathedral, he married Catherine of Valois, the French king's daughter. (They had only one son; Henry was born on 6 December 1421 at Windsor Castle.) From June to July 1420, Henry's army besieged and took the castle at Montereau. He besieged and captured Melun in November, returning to England shortly thereafter.
1421 campaign and death.
While he was in England, Henry's brother Thomas of Lancaster, 1st Duke of Clarence, led the English forces in France. In March 1421, Thomas led the English to a disastrous defeat at the Battle of Baugé against an army of mainly Scottish allies of the Dauphin. The Duke himself was killed in the battle. On 10 June 1421, Henry sailed back to France to retrieve the situation. It would be his last military campaign. From July to August, Henry's forces besieged and captured Dreux, thus relieving allied forces at Chartres. That October, his forces lay siege to Meaux, capturing it on 2 May 1422.
Henry V died suddenly on 31 August 1422 at the Château de Vincennes, apparently from dysentery, which he had contracted during the siege of Meaux. He was only 35 years old and had reigned for nine years.
Shortly before his death, Henry V named his brother John of Lancaster, 1st Duke of Bedford, regent of France in the name of his son Henry VI of England, then only a few months old. Henry V did not live to be crowned King of France himself, as he might confidently have expected after the Treaty of Troyes, because the sickly Charles VI, to whom he had been named heir, survived him by two months. Henry's comrade-in-arms and Lord Steward John Sutton, 1st Baron Dudley, brought his body home to the Kingdom of England and bore the royal standard at his funeral. Henry V was buried in Westminster Abbey on 7 November 1422.
Arms.
As Prince of Wales, Henry's arms were those of the kingdom, differenced by a label argent of three points. Upon his accession, he inherited use of the arms of the kingdom undifferenced.
Marriage and ancestry.
He married Catherine of Valois in 1420, and their only child was Henry, who became Henry VI of England.

</doc>
<doc id="45598" url="https://en.wikipedia.org/wiki?curid=45598" title="Strepsiptera">
Strepsiptera

The Strepsiptera (translation: "twisted wing"', giving rise to the insects' common name, twisted-wing parasites) are an endopterygote order of insects with nine extant families making up about 600 species. The early-stage larvae and the short-lived adult males are not sessile, but most of their lives are spent as endoparasites in other insects, such as bees, wasps, leafhoppers, silverfish, and cockroaches.
Appearance and biology.
Males of the Strepsiptera have wings, legs, eyes, and antennae, and superficially look like flies, though their mouthparts cannot be used for feeding. Many of their mouthparts are modified into sensory structures. Adult males are very short-lived, usually surviving less than five hours, and do not feed. Females, in all families except the Mengenillidae, are not known to leave their hosts and are neotenic in form, lacking wings, legs, and eyes. Virgin females release a pheromone which the males use to locate them. In the Stylopidia, the female's anterior region protrudes out of the host body and the male mates by rupturing the female's brood canal opening, which lies between the head and prothorax. Sperm passes through the opening in a process termed hypodermic insemination. The offspring consume their mother from the inside in a process known as hemocelous viviparity. Each female thus produces many thousands of triungulin larvae that emerge from the brood opening on the head, which protrudes outside the host body. These larvae have legs (which lack a trochanter, the leg segment that forms the articulation between the basal coxa and the femur), and actively search out new hosts. Their hosts include members of the orders Zygentoma, Orthoptera, Blattodea, Mantodea, Heteroptera, Hymenoptera, and Diptera. In the strepsipteran family Myrmecolacidae, the males parasitize ants, while the females parasitize Orthoptera.
Strepsiptera eggs hatch inside the female, and the planidium larvae can move around freely within the female's haemocoel, which is unique to these animals. The female has a brood canal that communicates with the outside world, and the larvae escape through this. The larvae are very active, as they only have a limited amount of time to find a host before they exhaust their food reserves. These first-instar larvae have stemmata (simple, single-lens eyes), and once they latch onto a host, they enter it by secreting enzymes that soften the cuticle, 
usually in the abdominal region of the host. Some species have been reported to enter the eggs of hosts. Larvae of "Stichotrema dallatorreanurn" Hofeneder from Papua New Guinea were found to enter their orthopteran host's tarsus 
(foot). Once inside the host, they undergo hypermetamorphosis and become a less mobile, legless larval form. They induce the host to produce a bag-like structure inside which they feed and grow. This structure, made from host tissue, protects them from the immune defences of the host. Larvae go through four more 
instars, and in each moult separation of the older cuticle occurs, but no discarding ("apolysis without ecdysis"), leading to multiple layers being formed around the larvae.
Male larvae produce pupae after the last moult, but females directly become neotenous adults. The colour and shape of the host's abdomen may be changed and the host usually becomes sterile. The parasites then undergo pupation to become adults. Adult males emerge from the host bodies, while females stay inside. Females may occupy up to 90% of the abdominal volume of their hosts.
Adult male Strepsiptera have eyes unlike those of any other insect, resembling the schizochroal eyes found in the trilobite group known as the Phacopida. Instead of a compound eye consisting of hundreds to thousands of ommatidia, that each produce a pixel of the entire image - the strepsipteran eyes consist of only a few dozen "eyelets" that each produce a complete image. These eyelets are separated by cuticle and/or setae, giving the cluster eye as a whole a blackberry-like appearance.
Very rarely, multiple females may live within a single stylopized host; multiple males within a single host are somewhat more common. Adult males are rarely observed, however, although specimens may be lured using cages containing virgin females. Nocturnal specimens can also be collected at light traps.
Strepsiptera of the Myrmecolacidae family can cause their ant hosts to linger on the tips of grass leaves, increasing the chance of being found by the parasite's males (in case of females) and putting them in a good position for male emergence (in case of males).
Classification.
The order, named by William Kirby in 1813, is named for the hind wings, which are held at a twisted angle when at rest (Greek στρεψι-, combining form of , to twist; and , wing). The fore wings are reduced to halteres (and initially were thought to be dried and twisted).
Strepsiptera are an enigma to taxonomists. Originally, they were believed to be the sister group to the beetle families Meloidae and Ripiphoridae, which have similar parasitic development and forewing reduction; early molecular research suggested their inclusion as a sister group to the flies, in a clade called the halteria, which have one pair of the wings modified into halteres, and failed to support their relationship to the beetles. Further molecular studies, however, suggested they are outside the clade Mecopterida (containing the Diptera and Lepidoptera), but found no strong evidence for affinity with any other extant group. Study of their evolutionary position has been problematic due to difficulties in phylogenetic analysis arising from long branch attraction. The most basal strepsipteran is the fossil "Protoxenos janzeni" discovered in Baltic amber, while the most basal living strepsipteran is "Bahiaxenos relictus", the sole member of the family Bahiaxenidae. The earliest known strepsipteran fossil is that of "Cretostylops engeli", discovered in middle Cretaceous amber from Myanmar.
In 2012, a fresh molecular study revived the assertion that the Stepsiptera are the sister group of the Coleoptera (beetles).
Families.
The Strepsiptera have two major groups: the Stylopidia and Mengenillidia. The Mengenillidia include three extinct families (Cretostylopidae, Protoxenidae, and Mengeidae) plus two extant families (Bahiaxenidae and Mengenillidae; the latter is not monophyletic, however.) They are considered more primitive, and the known females (Mengenillidae only) are free-living, with rudimentary legs and antennae. The females have a single genital opening. The males have strong mandibles, a distinct labrum, and more than five antennal segments.
The other group, the Stylopidia, includes seven families: the Corioxenidae, Halictophagidae, Callipharixenidae, Bohartillidae, Elenchidae, Myrmecolacidae, and Stylopidae. All Stylopidia have endoparasitic females having multiple genital openings.
The Stylopidae have four-segmented tarsi and four- to six-segmented antennae, with the third segment having a lateral process. The family Stylopidae may be paraphyletic. The Elenchidae have two-segmented tarsi and four-segmented antennae, with the third segment having a lateral process. The Halictophagidae have three-segmented tarsi and seven-segmented antennae, with lateral processes from the third and fourth segments.
The Stylopidae mostly parasitize wasps and bees, the Elenchidae are known to parasitize Fulgoroidea, while the Halictophagidae are found on leafhoppers, treehoppers, and mole cricket hosts.
Strepsipteran insects in the genus "Xenos" parasitize "Polistes carnifex", a species of social wasps. These obligate parasites infect the developing wasp larvae in the nest and are present within the abdomens of female wasps when they hatch out. Here they remain until they thrust through the cuticle and pupate (males) or release infective first-instar larvae onto flowers (females). These larvae are transported back to their nests by foraging wasps.
Use in population control.
Many insects which have been considered as pests, such as leafhoppers and cockroaches, have species-specific Strepsipteran endoparasites. Inoculation of a pest population with the corresponding parasitoid has been found to aid in minimizing the impact of these pests. This method is useful in agriculture as a means of avoiding addition of chemicals to crops for pest control.

</doc>
