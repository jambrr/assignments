<doc id="42787" url="https://en.wikipedia.org/wiki?curid=42787" title="History of Estonia">
History of Estonia

The history of Estonia is a part of the history of Europe. Estonia was settled near the end of the last glacial era, beginning from around 8500 BC. Before the Germans invaded in the 13th century, proto-Estonians of ancient Estonia worshipped the spirits of nature. Starting with the Northern Crusades in the Middle Ages, Estonia became a battleground for centuries where Denmark, Germany, Russia, Sweden and Poland fought their many wars over controlling the important geographical position of the country as a gateway between East and West.
Being conquered by Danes and Germans in 1227, Estonia was ruled initially by Denmark in the north, by the Livonian Order, an autonomous part of the Monastic state of the Teutonic Knights and Baltic German ecclesiastical states of the Holy Roman Empire. From 1418–1562 the whole of Estonia was part of the Livonian Confederation. After the Livonian War, Estonia became part of Sweden from the 16th century to 1710/1721, when it was ceded to the Russian Empire as a result of the Great Northern War. Throughout this period the Baltic German nobility enjoyed autonomy, where the language of administration and education was German.
The Estophile Enlightenment Period (1750–1840) led to the Estonian national awakening in the middle of the 19th century. In the aftermath of World War I and the Russian revolutions, the Estonian Declaration of Independence was issued in February 1918. The Estonian War of Independence ensued on two fronts between the newly proclaimed state and Bolshevist Russia to the east and the Baltic German forces (the Baltische Landeswehr) to the south, resulting in the Tartu Peace Treaty recognising Estonian independence in perpetuity.
In 1940, Estonia was occupied and (according to e.g. the US, the EU, and the European Court of Human Rights) illegally annexed by the Soviet Union as a result of the Molotov–Ribbentrop Pact. During the war Estonia was occupied by Nazi Germany in 1941, then reoccupied by the Soviet Union in 1944. Estonia regained independence in 1991 after the collapse of the USSR and joined the European Union in 2004.
Ancient Estonia: pre-history.
Mesolithic Period.
The region has been populated since the end of the Late Pleistocene Ice Age, about 10,000 BC. The earliest traces of human settlement in Estonia are connected with the Kunda culture. The early mesolithic Pulli settlement is located by the Pärnu River. It has been dated to the beginning of the 9th millennium BC. The Kunda culture received its name from the "Lammasmäe" settlement site in northern Estonia, which dates from earlier than 8500 BC. Bone and stone artifacts similar to those found at Kunda have been discovered elsewhere in Estonia, as well as in Latvia, northern Lithuania and southern Finland. Among minerals, flint and quartz were used the most for making cutting tools.
Neolithic Period.
The beginning of the Neolithic Period is marked by the ceramics of the Narva culture, and appear in Estonia at the beginning of the 5th millennium. The oldest finds date from around 4900 BC. The first pottery was made of thick clay mixed with pebbles, shells or plants. The Narva-type ceramics are found throughout almost the entire Estonian coastal region and on the islands. The stone and bone tools of the era have a notable similarity with the artifacts of the Kunda culture.
Around the beginning of 4th millennium Comb Ceramic culture arrived in Estonia. Until the early 1980s the arrival of Finnic peoples, the ancestors of the Estonians, Finns, and Livonians, on the shores of the Baltic Sea was associated with the Comb Ceramic Culture. However, such a linking of archaeologically defined cultural entities with linguistic ones cannot be proven, and it has been suggested that the increase of settlement finds in the period is more likely to have been associated with an economic boom related to the warming of climate. Some researchers have even argued that a Uralic form of language may have been spoken in Estonia and Finland since the end of the last glaciation.
The burial customs of the comb pottery people included additions of figures of animals, birds, snakes and men carved from bone and amber. Antiquities from comb pottery culture are found from Northern Finland to Eastern Prussia.
The beginning of the Late Neolithic Period about 2200 BC is characterized by the appearance of the Corded Ware culture, pottery with corded decoration and well-polished stone axes (s.c. boat-shape axes). Evidence of agriculture is provided by charred grains of wheat on the wall of a corded-ware vessel found in Iru settlement. Osteological analysis show an attempt was made to domesticate the wild boar.
Specific burial customs were characterized by the dead being laid on their sides with their knees pressed against their breast, one hand under the head. Objects placed into the graves were made of the bones of domesticated animals.
Bronze Age.
The beginning of the Bronze Age in Estonia is dated to approximately 1800 BC. The development of the borders between the Finnic peoples and the Balts was under way. The first fortified settlements, Asva and Ridala on the island of Saaremaa and Iru in Northern Estonia, began to be built. The development of shipbuilding facilitated the spread of bronze. Changes took place in burial customs, a new type of burial ground spread from Germanic to Estonian areas, and stone cist graves and cremation burials became increasingly common, alongside a small number of boat-shaped stone graves.
About the 7th century BC, a large meteorite hit Saaremaa island and created the Kaali craters.
About 325 BC, the Greek explorer Pytheas possibly visited Estonia. The Thule island he described has been identified as Saaremaa by Lennart Meri, though this identification is not widely considered probable, as Saaremaa lies far south of the Arctic Circle.
Iron Age.
The Pre-Roman Iron Age began in Estonia about 500 BC and lasted until the middle of the 1st century AD. The oldest iron items were imported, although since the 1st century iron was smelted from local marsh and lake ore. Settlement sites were located mostly in places that offered natural protection. Fortresses were built, although used temporarily. The appearance of square Celtic fields surrounded by enclosures in Estonia date from the Pre-Roman Iron Age. The majority of stones with man-made indents, which presumably were connected with magic designed to increase crop fertility, date from this period. A new type of grave, quadrangular burial mounds, began to develop. Burial traditions show the clear beginning of social stratification.
The Roman Iron Age in Estonia is roughly dated to between 50 and 450 AD, the era that was affected by the influence of the Roman Empire. In material culture this is reflected by a few Roman coins, some jewellery and artefacts. The abundance of iron artefacts in Southern Estonia speaks of closer mainland ties with southern areas, while the islands of western and northern Estonia communicated with their neighbors mainly by sea. By the end of the period three clearly defined tribal dialectical areas—Northern Estonia, Southern Estonia, and Western Estonia including the islands—had emerged, the population of each having formed its own understanding of identity.
Early Middle Ages.
The name "Estonia" occurs first in a form of Aestii in the 1st century AD by Tacitus; however, it might have indicated Baltic tribes living in the area. In the Northern Sagas (9th century) the term started to be used to indicate the Estonians.
Ptolemy in his "Geography III" in the middle of the 2nd century CE mentions the Osilians among other dwellers on the Baltic shore.
According to the 5th-century Roman historian Cassiodorus, the people known to Tacitus as the Aestii were the Estonians. The extent of their territory in early medieval times is disputed, but the nature of their religion is not. They were known to the Scandinavians as experts in wind-magic, as were the Lapps (known at the time as Finns) in the North. Cassiodorus mentions Estonia in his book V. Letters 1–2 dating from the 6th century.
The Chudes, as mentioned by a monk Nestor in the earliest Russian chronicles, were the Ests or Esthonians.
In the 1st centuries AD political and administrative subdivisions began to emerge in Estonia. Two larger subdivisions appeared: the parish ("kihelkond") and the county ("maakond"). The parish consisted of several villages. Nearly all parishes had at least one fortress. The defense of the local area was directed by the highest official, the parish elder. The county was composed of several parishes, also headed by an elder. By the 13th century the following major counties had developed in Estonia: Saaremaa (Osilia), Läänemaa (Rotalia or Maritima), Harjumaa (Harria), Rävala (Revalia), Virumaa (Vironia), Järvamaa (Jervia), Sakala (Saccala), and Ugandi (Ugaunia).
Varbola Stronghold was one of the largest circular rampart fortresses and trading centers built in Estonia, Harju County () at the time.
In the 11th century the Scandinavians are frequently chronicled as combating the Vikings from the eastern shores of the Baltic Sea.
With the rise of Christianity, centralized authority in Scandinavia and Germany eventually led to the Baltic crusades.
The east Baltic world was transformed by military conquest: first the Livs, Letts and Estonians, then the Prussians and the Finns underwent defeat, baptism, military occupation and sometimes extermination by groups of Germans, Danes and Swedes.
Estonian Crusade: The Middle Ages.
Estonia was one of the last corners of medieval Europe to be Christianized. In 1193 Pope Celestine III called for a crusade against pagans in Northern Europe. The Northern Crusades from Northern Germany established the stronghold of Riga (in modern Latvia). With the help of the newly converted local tribes of Livs and Letts, the crusaders initiated raids into part of what is present-day Estonia in 1208. Estonian tribes fiercely resisted the attacks from Riga and occasionally themselves sacked territories controlled by the crusaders. In 1217 the German crusading order the Sword Brethren and their recently converted allies won a major battle in which the Estonian commander Lembitu was killed. The period of the several Northern Crusade battles in Estonia between 1208 and 1227 is also known as the period of the ancient Estonian fight for independence.
Danish Estonia.
Northern Estonia was conquered by Danish crusaders led by king Waldemar II, who arrived in 1219 on the site of the Estonian town of Lindanisse (now Tallinn) at (Latin) "Revelia" (Estonian) "Revala" or "Rävala", the adjacent ancient Estonian county. The Danish Army defeated the Estonians at the Battle of Lyndanisse.
The Estonians of Harria started a rebellion in 1343 (St. George's Night Uprising). The province was occupied by the Livonian Order as a result. In 1346, the Danish dominions in Estonia (Harria and Vironia) were sold for 10 000 marks to the Livonian Order.
Swedish coastal settlements.
The first written mention of the Estonian Swedes comes from 1294, in the laws of the town of Haapsalu. Estonian Swedes are one of the earliest known minorities in Estonia. They have also been called "Coastal Swedes" ("Rannarootslased" in Estonian), or according to their settlement area Ruhnu Swedes, Hiiu Swedes etc. They themselves used the expression "aibofolke" ("island people"), and called their homeland "Aiboland".
The ancient areas of Swedish settlement in Estonia were Ruhnu Island, Hiiumaa Island, the west coast and smaller islands (Vormsi, Noarootsi, Sutlepa, Riguldi, Osmussaar), the northwest coast of the Harju District (Nõva, Vihterpalu, Kurkse, the Pakri Peninsula and the Pakri Islands), and Naissaar Island near Tallinn. The towns with a significant percentage of Swedish population have been Haapsalu and Tallinn.
In earlier times Swedes also lived on the coasts of Saaremaa, the southern part of Läänemaa, the eastern part of Harjumaa and the western part of Virumaa.
Terra Mariana.
In 1227 the Sword Brethren conquered the last indigenous stronghold on the Estonian island of Saaremaa. After the conquest, all the remaining local pagans of Estonia were ostensibly Christianized. An ecclesiastical state Terra Mariana was established. The conquerors exercised control through a network of strategically located castles.
The territory was then divided between the Livonian branch of the Teutonic Order, the Bishopric of Dorpat (in Estonian: "Tartu piiskopkond") and the Bishopric of Ösel-Wiek (in Estonian: "Saare-Lääne piiskopkond"). The northern part of Estonia – more exactly Harjumaa and Virumaa districts (in German: Harrien und Wierland) – was a nominal possession of Denmark until 1346. Tallinn (Reval) was given the Lübeck Rights in 1248 and joined the Hanseatic League at the end of the 13th century. In 1343 the people of northern Estonia and Saaremaa (Oesel) Island started a rebellion (St. George's Night Uprising) against the rule of their German-speaking landlords. The uprising was put down, and four elected Estonian "kings" were killed in Paide during peace negotiations in 1343. Vesse, the rebel King of Saaremaa, was hanged in 1344.
Despite local rebellions and Muscovian invasions in 1481 and 1558, the local Low German-speaking upper class continued to rule Estonia. By the end of the Middle Ages, these Baltic Germans had established themselves as the governing elite in Estonia, both as traders and the urban middle-class in the cities, and as landowners in the countryside, through a network of manorial estates.
The Reformation.
The Protestant Reformation in Europe that began in 1517 with Martin Luther (1483–1546) spread to Estonia in the 1520s. The Reformation in Estonia was inspired and organized by local and Swedish secular and religious authorities – especially after the end of the Livonian War in 1582. Lutheranism spread literacy among the young, and it transformed religious art. However, the peasants were traditionalists and were more comfortable with Catholic traditions; they delayed the adoption of the new religion. After 1600, Swedish Lutheranism began to dominate the building, furnishing, and (modest) decoration of new churches. Church architecture was now designed to encourage congregational understanding of and involvement in the services. Pews and seats were installed for the common people to make listening to the sermon less of a burden, and altars often featured depictions of the Last Supper, but images and statues of the saints had disappeared. The Baltic German elite promoted Lutheranism, and language, education, religion and politics were greatly transformed. Church services were now given in the local vernacular, instead of Latin, and the first book was printed in Estonian.
Division of Estonia in the Livonian War.
During the Livonian War in 1561, northern Estonia submitted to Swedish control, while southern Estonia briefly came under the control of the Polish–Lithuanian Commonwealth in the 1580s. In 1625, mainland Estonia came entirely under Swedish rule. Estonia was administratively divided between the provinces of Estonia in the north and Livonia in southern Estonia and northern Latvia, a division which persisted until the early 20th century.
Ferdinand I, Holy Roman Emperor asked for help of Gustav I of Sweden, and the Kingdom of Poland also began direct negotiations with Gustavus, but nothing resulted because on 29 September 1560, Gustavus I Vasa died. The chances for success of Magnus von Lyffland and his supporters looked particularly good in 1560 and 1570. In the former case he had been recognised as their sovereign by the Bishopric of Ösel–Wiek and the Bishopric of Courland, and as their prospective ruler by the authorities of the Bishopric of Dorpat; the Bishopric of Reval with the Harrien-Wierland gentry were on his side; and the Livonian Order conditionally recognised his right of ownership of the principality of Estonia. Then, along with Archbishop Wilhelm von Brandenburg of the Archbishopric of Riga and his coadjutor Christoph von Mecklenburg, Kettler gave to Magnus the portions of the Kingdom of Livonia which he had taken possession of, but they refused to give him any more land. Once Eric XIV of Sweden became king, he took quick actions to get involved in the war. He negotiated a continued peace with Muscovy and spoke to the burghers of Reval city. He offered them goods to submit to him, as well as threatening them. By 6 June 1561, they submitted to him, contrary to the persuasions of Kettler to the burghers. The King's brother Johan married the Polish princess Catherine Jagiellon. Wanting to obtain his own land in Livonia, he loaned Poland money and then claimed the castles they had pawned as his own instead of using them to pressure Poland. After Johan returned to Finland, Erik XIV forbade him to deal with any foreign countries without his consent. Shortly after that Erik XIV started acting quickly and lost any allies he was about to obtain, either from Magnus or the Archbishop of Riga. Magnus was upset he had been tricked out of his inheritance of Holstein. After Sweden occupied Reval, Frederick II of Denmark made a treaty with Erik XIV of Sweden in August 1561. The brothers were in great disagreement, and Frederick II negotiated a treaty with Ivan IV on 7 August 1562, in order to help his brother obtain more land and stall further Swedish advance. Erik XIV did not like this and the Northern Seven Years' War between the Free City of Lübeck, Denmark, Poland, and Sweden broke out. While only losing land and trade, Frederick II and Magnus were not faring well. But in 1568, Erik XIV became insane, and his brother Johan III took his place. Johan III ascended to the throne of Sweden, and due to his friendship with Poland he began a policy against Muscovy. He would try to obtain more land in Livonia and exercise strength over Denmark. After all parties had been financially drained, Frederick II let his ally, King Sigismund II Augustus of the Polish–Lithuanian Commonwealth, know that he was ready for peace. On 13 December 1570, the Treaty of Stettin was concluded. It is, however, more difficult to estimate the scope and magnitude of the support Magnus received in Livonian cities. Compared to the Harrien-Wierland gentry, the Reval city council, and hence probably the majority of citizens, demonstrated a much more reserved attitude towards Denmark and King Magnus of Livonia. Nevertheless, there is no reason to speak about any strong pro-Swedish sentiments among the residents of Reval. The citizens who had fled to the Bishopric of Dorpat or had been deported to Muscovy hailed Magnus as their saviour until 1571. The analysis indicates that during the Livonian War a pro-independence wing emerged among the Livonian gentry and townspeople, forming the so-called "Peace Party". Dismissing hostilities, these forces perceived an agreement with Muscovy as a chance to escape the atrocities of war and avoid the division of Livonia. That is why Magnus, who represented Denmark and later struck a deal with Ivan the Terrible, proved a suitable figurehead for this faction.
The Peace Party, however, had its own armed forces – scattered bands of household troops ("Hofleute") under diverse command, which only united in action in 1565 (Battle of Pärnu and Siege of Reval, 1565), in 1570–1571 (Siege of Reval, 1570–1571; 30 weeks), and in 1574–1576 (first on Sweden's side, then came the sale of Wiek to the Danish Crown and the loss of the territory to the Muscovites). In 1575 after Muscovy attacked Danish claims in Livonia, Frederick II dropped out of the competition, as did the Holy Roman Emperor. After this Johan III held off on his pursuit for more land due to Muscovy obtaining lands that Sweden controlled. He used the next two years of truce to get in a better position. In 1578, he resumed the fight for not only Livonia, but also everywhere due to an understanding he made with Rzeczpospolita. In 1578 Magnus retired to Rzeczpospolita, and his brother all but gave up the land in Livonia.
Having rejected peace proposals from its enemies, Ivan the Terrible found himself in a difficult position by 1579, when the Crimean Khanate devastated Muscovian territories and burnt down Moscow (see Russo-Crimean Wars), the drought and epidemics had fatally affected the economy, the policy of oprichnina had thoroughly disrupted the government, while the Grand Principality of Lithuania had united with the Kingdom of Poland and acquired an energetic leader, Stefan Batory, supported by the Ottoman Empire (1576). Batory replied with a series of three offensives against Muscovy, trying to cut the Kingdom of Livonia from Muscovian territories. During his first offensive in 1579 with 22,000 men he retook Polotsk. During the second, in 1580, with a 29,000-strong army he took Velikie Luki, and in 1581 with a 100,000-strong army he started the Siege of Pskov. Frederick II had trouble continuing the fight against Muscovy unlike Sweden and Poland. He came to an agreement with John III of Sweden in 1580 giving him the titles in Livonia. That war would last from 1577 to 1582. Muscovy recognized Polish–Lithuanian control of Ducatus Ultradunensis only in 1582. After Magnus von Lyffland died in 1583, Poland invaded his territories in the Duchy of Courland and Frederick II decided to sell his rights of inheritance. Except for the island of Œsel, Denmark was out of the Baltic by 1585. In 1598 Polish Livonia was divided into:
Polish–Lithuanian Commonwealth.
During 1582–83 southern Estonia (Livonia) became part of the Polish–Lithuanian Commonwealth.
Estonia in the Swedish Empire.
The Duchy of Estonia placed itself under Swedish rule in 1561 to receive protection against Russia and Poland as the Livonian Order lost their foothold in the Baltic provinces. Territorially it represented the northern part of present-day Estonia.
Livonia was conquered from the Polish–Lithuanian Commonwealth by 1629 in the Polish–Swedish War. By the Treaty of Oliva between the Commonwealth and Sweden in 1660 following the Northern Wars the Polish–Lithuanian king renounced all claims to the Swedish throne and Livonia was formally ceded to Sweden. Swedish Livonia represents the southern part of present-day Estonia and the northern part of present-day Latvia (Vidzeme region).
In 1631, Gustavus II Adolphus of Sweden forced the nobility to grant the peasantry greater autonomy, and in 1632 established a printing press and university in the city of Tartu.
Estonia in the Russian Empire.
Sweden's defeat by Russia in the Great Northern War resulted in the capitulation of Estonia and Livonia in 1710, confirmed by the Treaty of Nystad in 1721, and Russian rule was then imposed on what later became modern Estonia. Nonetheless, the legal system, Lutheran church, local and town governments, and education remained mostly German until the late 19th century and partially until 1918.
The Russian era from the 1720s to the First World War was the golden age of the German elites. They owned most of the land and businesses, controlled the serfs, dominated all the cities, and got along quite well with the Russian imperial authorities. Unrest and rebellion was uncommon. The Germans were Lutherans, and so were the vast majority of the Estonian population, but the Germans had full control of the Lutheran churches. Moravian Protestant missionaries made an impact in the eighteenth century, and translated the complete Bible into Estonian. The Germans complained, so the imperial government banned the Moravians from 1743 to 1764. A theological faculty opened at the University of Dorpat (Tartu), with German professors. The local German gentry controlled the local churches and rarely hired Estonian graduates, but they made their mark as intellectuals and Estonian nationalists. In the 1840s, there was a movement of Lutheran peasants into the Russian Orthodox Church. The czar discouraged them when he realized they were challenging the local authorities. The German character of the Lutheran churches alienated many nationalists, who emphasized the secular in their subcultures. For example, choral societies offered a secular alternative to church music.
By 1819, the Baltic provinces were the first in the Russian empire in which serfdom was abolished, the largely autonomous nobility allowing the peasants to own their own land or move to the cities. These moves created the economic foundation for the coming to life of the local national identity and culture as Estonia was caught in a current of national awakening that began sweeping through Europe in the mid-19th century.
Tartu was a multicultural crossroads with strong representation of Russians, Germans and Estonians. Orthodox, Lutherans and Jews, scientists and humanists, all were quite active at the city's university. The students seemed uninterested in the Russification programs introduced in the 1890s.
The Estophile enlightenment period (1750–1840).
Educated German immigrants and local Baltic Germans in Estonia, educated at German universities, introduced Enlightenment ideas of rational thinking, ideas that propagated freedom of thinking and brotherhood and equality. The French Revolution provided a powerful motive for the enlightened local upper class to create literature for the peasantry. The freeing of the peasantry from serfdom on the nobles' estates in 1816 in Southern Estonia: Governorate of Livonia (Russian: Лифляндская губерния) and 1819 in Northern Estonia: Governorate of Estonia (Russian: Эстляндская губерния) by Alexander I of Russia gave rise to a debate as to the future fate of the former enslaved peoples. Although Baltic Germans by and large regarded the future of the Estonians as being a fusion with the Baltic Germans, the Estophile educated class admired the ancient culture of the Estonians and their era of freedom before the conquests by Danes and Germans in the 13th century. The Estophile Enlightenment Period formed the transition from religious Estonian literature to newspapers written in Estonian for the mass public.
National awakening.
A cultural movement sprang forth to adopt the use of Estonian as the language of instruction in schools, all-Estonian song festivals were held regularly after 1869, and a national literature in Estonian developed. "Kalevipoeg", Estonia's national epic, was published in 1861 in both Estonian and German.
1889 marked the beginning of the central government-sponsored policy of Russification. The impact of this was that many of the Baltic German legal institutions were either abolished or had to do their work in Russian – a good example of this is the University of Tartu.
As the Russian Revolution of 1905 swept through Estonia, the Estonians called for freedom of the press and assembly, for universal franchise, and for national autonomy. Estonian gains were minimal, but the tense stability that prevailed between 1905 and 1917 allowed Estonians to advance the aspiration of national statehood.
Road to the republic.
Estonia as a unified political entity first emerged after the Russian February Revolution of 1917. With the collapse of the Russian Empire in World War I, Russia's Provisional Government granted national autonomy to a unified Estonia in April. The Governorate of Estonia in the north (corresponding to the historic Danish Estonia) was united with the northern part of the Governorate of Livonia. Elections for a provisional parliament, "Maapäev", was organized, with the Menshevik and Bolshevik factions of the Russian Social Democratic Labour Party obtaining a part of the vote. On 5 November 1917, two days before the October Revolution in Saint Petersburg, Estonian Bolshevik leader Jaan Anvelt violently usurped power from the legally constituted Maapäev in a coup d'état, forcing the Maapäev underground.
In February, after the collapse of the peace talks between Soviet Russia and the German Empire, mainland Estonia was occupied by the Germans. Bolshevik forces retreated to Russia. Between the Russian Red Army's retreat and the arrival of advancing German troops, the Salvation Committee of the Estonian National Council Maapäev issued the Estonian Declaration of Independence in Pärnu on 23 February 1918.
War of Independence.
After the collapse of the short-lived puppet government of the United Baltic Duchy and the withdrawal of German troops in November 1918, an Estonian Provisional Government retook office. A military invasion by the Red Army followed a few days later, however, marking the beginning of the Estonian War of Independence (1918–1920). The Estonian army cleared the entire territory of Estonia of the Red Army by February 1919. On 5–7 April 1919 the Estonian Constituent Assembly was elected.
Victory.
On 2 February 1920, the Treaty of Tartu was signed by the Republic of Estonia and the Russian SFSR. The terms of the treaty stated that Russia renounced in perpetuity all rights to the territory of Estonia. The first Constitution of Estonia was adopted on 15 June 1920. The Republic of Estonia obtained international recognition and became a member of the League of Nations in 1921.
In nearby Finland similar circumstances resulted in a bloody civil war. Despite repeated threats from fascist movements, Finland became and remained a free democracy under the rule of law. By contrast Estonia, without a civil war, started as a democracy and was turned into a dictatorship in 1934.
Interwar period.
The first period of independence lasted 22 years, beginning in 1918. Estonia underwent a number of economic, social, and political reforms necessary to come to terms with its new status as a sovereign state. Economically and socially, land reform in 1919 was the most important step. Large estate holdings belonging to the Baltic nobility were redistributed among the peasants and especially among volunteers in the Estonian War of Independence. Estonia's principal markets became Scandinavia, the United Kingdom, and western Europe, with some exports to the United States and to the Soviet Union.
The first constitution of the Republic of Estonia, adopted in 1920, established a parliamentary form of government. The parliament ("Riigikogu") consisted of 100 members elected for three-year terms. Between 1920 and 1934, Estonia had 21 governments.
A mass anticommunist and antiparliamentary Vaps Movement emerged in the 1930s. In October 1933 a referendum on constitutional reform initiated by the Vaps Movement was approved by 72.7 percent. The league spearheaded replacement of the parliamentary system with a presidential form of government and laid the groundwork for an April 1934 presidential election, which it expected to win. However, the Vaps Movement was thwarted by a pre-emptive coup d'état on 12 March 1934, by Head of State Konstantin Päts, who then established his own authoritarian rule until a new constitution came to force. During the Era of Silence, political parties were banned, and the parliament was not in session between 1934 and 1938 as the country was ruled by decree by Päts. The Vaps Movement was officially banned and finally disbanded in December 1935. On 6 May 1936, 150 members of the league went on trial and 143 of them were convicted to long-term prison sentences. They were granted an amnesty and freed in 1938, by which time the league had lost most of its popular support.
The interwar period was one of great cultural advancement. Estonian language schools were established, and artistic life of all kinds flourished. One of the more notable cultural acts of the independence period, unique in western Europe at the time of its passage in 1925, was a guarantee of cultural autonomy to minority groups comprising at least 3,000 persons, including Jews (see history of the Jews in Estonia). Historians see the lack of any bloodshed after a nearly "700-year German rule" as indication that it must have been mild by comparison.
Estonia had pursued a policy of neutrality, but it was of no consequence after the Soviet Union and Nazi Germany signed the Molotov–Ribbentrop Pact on 23 August 1939. In the agreement, the two great powers agreed to divide up the countries situated between them (Poland, Lithuania, Latvia, Estonia, and Finland), with Estonia falling in the Soviet "sphere of influence". After the invasion of Poland, the Orzeł incident took place when Polish submarine ORP "Orzeł" looked for shelter in Tallinn but escaped after the Soviet Union attacked Poland on 17 September. Estonia's lack of will and/or inability to disarm and intern the crew caused the Soviet Union to accuse Estonia of "helping them escape" and claim that Estonia was not neutral. On 24 September 1939, the Soviet Union threatened Estonia with war unless provided with military bases in the country—an ultimatum with which the Estonian government complied.
World War II.
Following the conclusion of the Molotov-Ribbentrop Pact and the Soviet invasion of Poland, warships of the Red Navy appeared off Estonian ports on 24 September 1939, and Soviet bombers began a threatening patrol over Tallinn and the nearby countryside. Moscow demanded Estonia assent to an agreement which allowed the USSR to establish military bases and station 25,000 troops on Estonian soil for the duration of the European war. The government of Estonia accepted the ultimatum, signing the corresponding agreement on 28 September 1939.
Soviet occupation (1940).
The Republic of Estonia was occupied by the Soviet Union in June 1940.
On 12 June 1940, the order for a total military blockade of Estonia by the Soviet Baltic Fleet was given.
On 14 June 1940, while the world's attention was focused on the fall of Paris to Nazi Germany a day earlier, the Soviet military blockade of Estonia went into effect, and two Soviet bombers downed Finnish passenger airplane "Kaleva" flying from Tallinn to Helsinki carrying three diplomatic pouches from the U.S. legations in Tallinn, Riga and Helsinki. US Foreign Service employee Henry W. Antheil, Jr., was killed in the crash.
On 16 June 1940, the Soviet Union invaded Estonia. Molotov accused the Baltic states of conspiracy against the Soviet Union and delivered an ultimatum to Estonia for the establishment of a government approved of by the Soviets.
The Estonian government decided, given the overwhelming Soviet force both on the borders and inside the country, not to resist, to avoid bloodshed and open war. Estonia accepted the ultimatum, and the statehood of Estonia de facto ceased to exist as the Red Army exited from their military bases in Estonia on 17 June. The following day, some 90,000 additional troops entered the country. The military occupation of the Republic of Estonia was rendered "official" by a communist coup d'état supported by the Soviet troops, followed by "parliamentary elections" where all but pro-Communist candidates were outlawed. The "parliament" so elected proclaimed Estonia a Socialist Republic on 21 July 1940 and unanimously requested Estonia to be "accepted" into the Soviet Union. Those who had fallen short of the "political duty" of voting Estonia into the USSR, who had failed to have their passports stamped for so voting, were allowed to be shot in the back of the head by Soviet tribunals. Estonia was formally annexed into the Soviet Union on 6 August and renamed the Estonian Soviet Socialist Republic. In 1979, the European Parliament would condemn "the fact that the occupation of these formerly independent and neutral States by the Soviet Union occurred in 1940 following the Molotov/Ribbentrop pact, and continues," and sought to help restore Estonian, Latvian and Lithuanian independence through political means.
The Soviet authorities, having gained control over Estonia, immediately imposed a regime of terror. During the first year of Soviet occupation (1940–1941) over 8,000 people, including most of the country's leading politicians and military officers, were arrested. About 2,200 of the arrested were executed in Estonia, while most of the others were moved to Gulag prison camps in Russia, from where very few were later able to return alive. On 14 June 1941, when mass deportations took place simultaneously in all three Baltic countries, about 10,000 Estonian civilians were deported to Siberia and other remote areas of the Soviet Union, where nearly half of them later perished. Of the 32,100 Estonian men who were forcibly relocated to Russia under the pretext of mobilisation into the Soviet army after the German invasion of the Soviet Union in 1941, nearly 40 percent died within the next year in the so-called "labour battalions" of hunger, cold and overworking. During the first Soviet occupation of 1940–41 about 500 Jews were deported to Siberia.
Estonian graveyards and monuments were destroyed. Among others, the Tallinn Military Cemetery had the majority of gravestones from 1918–1944 destroyed by the Soviet authorities, and this graveyard became reused by the Red Army. Other cemeteries destroyed by the authorities during the Soviet era in Estonia include Baltic German cemeteries established in 1774 (Kopli cemetery, Mõigu cemetery) and the oldest cemetery in Tallinn, from the 16th century, Kalamaja cemetery.
Many countries including the United States did not recognize the seizure of Estonia by the USSR. Such countries recognized Estonian diplomats and consuls who still functioned in many countries in the name of their former governments. These aging diplomats persisted in this anomalous situation until the ultimate restoration of Baltic independence.
Ernst Jaakson, the longest-serving foreign diplomatic representative to the United States, served as vice-consul from 1934, and as consul general in charge of the Estonian legation in the United States from 1965 until reestablishment of Estonia's independence. On 25 November 1991, he presented credentials as Estonian ambassador to the United States.
German occupation (1941–1944).
After Nazi Germany invaded the Soviet Union on 22 June 1941, and the Wehrmacht reached Estonia in July 1941, most Estonians greeted the Germans with relatively open arms and hoped to restore independence. But it soon became clear that sovereignty was out of the question. Estonia became a part of the German-occupied "Ostland". A "Sicherheitspolizei" was established for internal security under the leadership of Ain-Ervin Mere. The initial enthusiasm that accompanied the liberation from Soviet occupation quickly waned as a result, and the Germans had limited success in recruiting volunteers. The draft was introduced in 1942, resulting in some 3,400 men fleeing to Finland to fight in the Finnish Army rather than join the Germans. Finnish Infantry Regiment 200 (Estonian: "soomepoisid") was formed out of Estonian volunteers in Finland. With the Allied victory over Germany becoming certain in 1944, the only option to save Estonia's independence was to stave off a new Soviet invasion of Estonia until Germany's capitulation.
By January 1944, the front was pushed back by the Soviet Army almost all the way to the former Estonian border. Narva was evacuated. Jüri Uluots, the last legitimate prime minister of the Republic of Estonia (according to the Constitution of the Republic of Estonia) prior to its fall to the Soviet Union in 1940, delivered a radio address that implored all able-bodied men born from 1904 through 1923 to report for military service. (Before this, Uluots had opposed Estonian mobilization.) The call drew support from all across the country: 38,000 volunteers jammed registration centers. Several thousand Estonians who had joined the Finnish army came back across the Gulf of Finland to join the newly formed Territorial Defense Force, assigned to defend Estonia against the Soviet advance. It was hoped that by engaging in such a war Estonia would be able to attract Western support for the cause of Estonia's independence from the USSR and thus ultimately succeed in achieving independence.
The initial formation of the volunteer SS Estonian legion created in 1942 was eventually expanded to become a full-sized conscript division of the Waffen-SS in 1944, the 20th Waffen Grenadier Division of the SS. The Estonian units saw action defending the Narva line throughout 1944.
As the Germans started to retreat on 18 September 1944, Jüri Uluots, the last Prime Minister of the Estonian Republic prior to Soviet occupation, assumed the responsibilities of president (as dictated in the Constitution) and appointed a new government while seeking recognition from the Allies. On 22 September 1944, as the last German units pulled out of Tallinn, the city was re-occupied by the Soviet Red Army. The new Estonian government fled to Stockholm, Sweden, and operated in exile until 1992, when Heinrich Mark, the prime minister of the Estonian government in exile acting as president, presented his credentials to incoming president Lennart Meri.
The Holocaust in Estonia.
The process of Jewish settlement in Estonia began in the 19th century, when in 1865 Russian Tsar Alexander II granted them the right to enter the region. The creation of the Republic of Estonia in 1918 marked the beginning of a new era for the Jews. Approximately 200 Jews fought in combat for the creation of the Republic of Estonia, and 70 of these men were volunteers. From the very first days of her existence as a state, Estonia showed her tolerance towards all the peoples inhabiting her territories. On 12 February 1925, the Estonian government passed a law pertaining to the cultural autonomy of minority peoples. The Jewish community quickly prepared its application for cultural autonomy. Statistics on Jewish citizens were compiled. They totaled 3,045, fulfilling the minimum requirement of 3,000. In June 1926 the Jewish Cultural Council was elected and Jewish cultural autonomy was declared. Jewish cultural autonomy was of great interest to the global Jewish community. The Jewish National Endowment presented the Government of the Republic of Estonia with a certificate of gratitude for this achievement.
There were, at the time of Soviet occupation in 1940, approximately 2,000 Estonian Jews. Many Jewish people were deported to Siberia along with other Estonians by the Soviets. It is estimated that 500 Jews suffered this fate. With the invasion of the Baltics, it was the intention of the Nazi government to use the Baltic countries as their main area of mass genocide. Consequently, Jews from countries outside the Baltics were shipped there to be exterminated. Out of the approximately 4,300 Jews in Estonia prior to the war, between 1,500 and 2,000 were entrapped by the Nazis, and an estimated 10,000 Jews were killed in Estonia after having been deported to camps there from elsewhere in Eastern Europe.
There have been seven ethnic Estonians – Ralf Gerrets, Ain-Ervin Mere, Jaan Viik, Juhan Jüriste, Karl Linnas, Aleksander Laak and Ervin Viks – who have faced trials for crimes against humanity since the reestablishment of Estonian independence and the formation of the Estonian International Commission for Investigation of Crimes Against Humanity. Markers were put in place for the 60th anniversary of the mass executions that were carried out at the Lagedi, Vaivara and Klooga (Kalevi-Liiva) camps in September 1944.
Fate of other minorities during and after World War II.
The Baltic Germans had voluntarily evacuated to Germany (in accordance with Hitler's order) following the Molotov–Ribbentrop Pact of August 1939.
Almost all the remaining Estonian Swedes fled Aiboland in August 1944, often in their small boats to the Swedish island of Gotland.
The Russian minority grew significantly in numbers during the postwar era.
Soviet reoccupation (1944–1991).
Stalinism.
In World War II Estonia had suffered huge losses. Ports had been destroyed, and 45% of industry and 40% of the railways had become damaged. Estonia's population had decreased by one-fifth, about 200,000 people. Some 10% of the population (over 80,000 people) had fled to the West between 1940 and 1944, first to countries such as Sweden and Finland and then to other western countries, often by refugee ships such as the SS Walnut. More than 30,000 soldiers had been killed in action. In 1944 Russian air raids had destroyed Narva and one-third of the residential area in Tallinn. By the late autumn of 1944, Soviet forces had ushered in a second phase of Soviet rule on the heels of the German troops withdrawing from Estonia, and followed it up by a new wave of arrests and executions of people considered disloyal to the Soviets.
An anti-Soviet guerrilla movement known as the "Metsavennad" ("Forest Brothers") developed in the countryside, reaching its zenith in 1946–48. It is hard to tell how many people were in the ranks of the "Metsavennad"; however, it is estimated that at different times there could have been about 30,000–35,000 people. Probably the last Forest Brother was caught in September 1978, and killed himself during his apprehension.
In March 1949, 20,722 people (2.5% of the population) were deported to Siberia. By the beginning of the 1950s, the occupying regime had suppressed the resistance movement.
After the war the Communist Party of the Estonian Soviet Socialist Republic (ECP) became the pre-eminent organization in the republic. The ethnic Estonian share in the total ECP membership decreased from 90% in 1941 to 48% in 1952.
Khrushchev era.
After Stalin's death, Communist Party membership vastly expanded its social base to include more ethnic Estonians. By the mid-1960s, the percentage of ethnic Estonian membership stabilized near 50%. On the eve of perestroika the ECP claimed about 100,000 members; less than half were ethnic Estonians and they totalled less than 7% of the country's population.
One positive aspect of the post-Stalin era in Estonia was the regranting of permission in the late 1950s for citizens to make contact with foreign countries. Ties were reactivated with Finland, and in the 1960s, a ferry connection was opened from Tallinn to Helsinki and Estonians began watching Finnish television. This electronic "window on the West" afforded Estonians more information on current affairs and more access to Western culture and thought than any other group in the Soviet Union. This heightened media environment was important in preparing Estonians for their vanguard role in extending perestroika during the Gorbachev era.
Capital investments.
In 1955 the TV Centre was built in Tallinn; it began TV broadcasts on 29 June of that year. The Tallinn Song Festival Grounds, the venue for the song festivals, were built in 1960.
Health care.
Only after the Khrushchev Thaw period of 1956 did healthcare networks start to stabilise. Due to natural development, science and technology advanced and popular welfare increased. All demographic indicators improved; birth rates increased, mortality decreased. Healthcare became freely available to everybody during the Soviet era.
Brezhnev era.
In the late 1970s, Estonian society grew increasingly concerned about the threat of cultural Russification to the Estonian language and national identity. By 1981, Russian was taught in the first grade of Estonian-language schools and was also introduced into Estonian pre-school teaching.
Moscow Olympic Games of 1980.
Tallinn was selected as the host of the sailing events which led to controversy since many governments had not "de jure" recognized ESSR as part of the USSR. During the preparations to the Olympics, sports buildings were built in Tallinn, along with other general infrastructure and broadcasting facilities. This wave of investment included Tallinn Airport, Hotell Olümpia, Tallinn TV Tower, Pirita Yachting Centre and Linnahall.
Andropov and Chernenko era.
On 10 November 1982, Leonid Brezhnev died and was succeeded by Yuri Andropov, the former head of the KGB. Andropov introduced limited economic reforms and established an anti-corruption program. On 9 February 1984, Andropov died and was succeeded by Konstantin Chernenko who in turn died on 10 March 1985.
Alcoholism.
Alcoholism became a growing health issue. Until 1985 and the beginning of glasnost, it was illegal to publish statistical data on alcohol sales. It is estimated that alcoholism peaked in 1982–1984, when consumption reached 11.2 litres of absolute alcohol per person per annum. (In comparison, in Finland during the same period, consumption was only 6–7 litres per person per annum.)
Gorbachev era.
By the beginning of the Gorbachev era, concern over the cultural survival of the Estonian people had reached a critical point. The ECP remained stable in the early perestroika years but waned in the late 1980s. Other political movements, groupings and parties moved to fill the power vacuum. The first and most important was the Estonian Popular Front, established in April 1988 with its own platform, leadership and broad constituency. The Greens and the dissident-led Estonian National Independence Party soon followed.
Restoration of "de facto" independence.
The Estonian Sovereignty Declaration was issued on 16 November 1988. By 1989 the political spectrum had widened, and new parties were formed and re-formed almost daily. The republic's Supreme Soviet transformed into an authentic regional lawmaking body. This relatively conservative legislature passed an early declaration of sovereignty (16 November 1988); a law on economic independence (May 1989) confirmed by the Supreme Soviet of the Soviet Union that November; a language law making Estonian the official language (January 1989); and local and republic election laws stipulating residency requirements for voting and candidacy (August, November 1989).
Despite the emergence of the Popular Front and the Supreme Soviet as a new lawmaking body, since 1989 the different segments of the indigenous Estonian population had been politically mobilized by different and competing actors. The Popular Front's proposal to declare the independence of Estonia as a new, so-called "third republic" whose citizens would be all those living there at the moment, found less and less support over time.
A grassroots Estonian Citizens' Committees Movement launched in 1989 with the objective of registering all pre-war citizens of the Republic of Estonia and their descendants in order to convene a Congress of Estonia. Their emphasis was on the illegal nature of the Soviet system and that hundreds of thousands of inhabitants of Estonia had not ceased to be citizens of the Estonian Republic which still existed "de jure", recognized by the majority of Western nations. Despite the hostility of the mainstream official press and intimidation by Soviet Estonian authorities, dozens of local citizens' committees were elected by popular initiative all over the country. These quickly organized into a nationwide structure, and by the beginning of 1990 over 900,000 people had registered themselves as citizens of the Republic of Estonia.
The spring of 1990 saw two free elections and two alternative legislatures developed in Estonia. On 24 February 1990, the 464-member Congress of Estonia (including 35 delegates of refugee communities abroad) was elected by the registered citizens of the republic. The Congress of Estonia convened for the first time in Tallinn 11–12 March 1990, passing 14 declarations and resolutions. A 70-member standing committee (Eesti Komitee) was elected with Tunne Kelam as its chairman.
In March 1991 a referendum was held on the issue of independence. This was somewhat controversial, as holding a referendum could be taken as signalling that Estonian independence would be established rather than "re"-established. There was some discussion about whether it was appropriate to allow the Russian immigrant minority to vote, or if this decision should be reserved exclusively for citizens of Estonia. In the end all major political parties backed the referendum, considering it most important to send a strong signal to the world. To further legitimise the vote, all residents of Estonia were allowed to participate. The result vindicated these decisions, as the referendum produced a strong endorsement for independence. Turnout was 82%, and 64% of all possible voters in the country backed independence, with only 17% against.
Although the majority of Estonia's large Russian-speaking diaspora of Soviet-era immigrants did not support full independence, they were divided in their goals for the republic. In March 1990 some 18% of Russian speakers supported the idea of a fully independent Estonia, up from 7% the previous autumn, and by early 1990 only a small minority of ethnic Estonians were opposed to full independence.
In the 18 March 1990, elections for the 105-member Supreme Soviet, all residents of Estonia were eligible to participate, including all Soviet-era immigrants from the U.S.S.R. and approximately 50,000 Soviet troops stationed there. The Popular Front coalition, composed of left and centrist parties and led by former Central Planning Committee official Edgar Savisaar, gained a parliamentary majority.
On 8 May 1990, the Supreme Council of the Republic of Estonia (renamed the previous day) changed the name to the Republic of Estonia. Through a strict, non-confrontational policy in pursuing independence, Estonia managed to avoid the violence which Latvia and Lithuania incurred in the bloody January 1991 crackdowns and in the border customs-post guard murders that summer. During the attempted August coup in the U.S.S.R., Estonia was able to maintain constant operation and control of its telecommunications facilities, thereby offering the West a clear view into the latest developments and serving as a conduit for swift Western support and recognition of Estonia's "confirmation" of independence on 20 August 1991. 20 August remains a national holiday in Estonia because of this. Following Europe's lead, the United States formally reestablished diplomatic relations with Estonia on 2 September, and the Supreme Soviet of Russia offered recognition on 6 September.
Since the debates about whether the future independent Estonia would be established as a new republic or a continuation of the first republic were not yet complete by the time of the August coup, while the members of the Supreme Soviet generally agreed that independence should be declared rapidly, a compromise was hatched between the two main sides: instead of "declaring" independence, which would imply a new start, or explicitly asserting continuity, the declaration would "confirm" Estonia as a state independent of the Soviet Union, and willing to reestablish diplomatic relations of its own accord. The text of the statement was in Estonian and only a few paragraphs in length.
After more than three years of negotiations, on 31 August 1994, the armed forces of Russia withdrew from Estonia. Since fully regaining independence Estonia has had twelve governments with nine prime ministers: Mart Laar, Andres Tarand, Tiit Vähi, Mart Siimann, Siim Kallas, Juhan Parts, Andrus Ansip and Taavi Rõivas. The PMs of the interim government (1990–1992) were Edgar Savisaar and Tiit Vähi.
Since the last Russian troops left in 1994, Estonia has been free to promote economic and political ties with Western Europe. Estonia opened accession negotiations with the European Union in 1998 and joined in 2004, shortly after becoming a member of NATO.
Contemporary Estonian government.
On 28 June 1992, Estonian voters approved the constitutional assembly's draft constitution and implementation act, which established a parliamentary government with a president as chief of state and with a government headed by a prime minister. The Riigikogu, a unicameral legislative body, is the highest organ of state authority. It initiates and approves legislation sponsored by the prime minister. The prime minister has full responsibility and control over his cabinet.
Meri presidency and Laar premiership (1992–2001).
Parliamentary and presidential elections were held on 20 September 1992. Approximately 68% of the country's 637,000 registered voters cast ballots. Lennart Meri, an outstanding writer and former Minister of Foreign Affairs, won this election and became president. He chose 32-year-old historian and Christian Democratic Party founder Mart Laar as prime minister.
In February 1992, and with amendments in January 1995, the Riigikogu renewed Estonia's 1938 citizenship law, which also provides equal civil protection to resident aliens. Elected on an ambitious programme of reform, Mart Laar's cabinet took several decisive measures (Shock therapy). Fast privatization was pursued and the role of the state in the economy as well as in the social affairs was reduced dramatically. After an initial steep decline in GDP, the Estonian economy started to grow again in 1995. Changes came with a social price: the average life expectancy in Estonia in 1994 was lower than in Belarus, Ukraine and even Moldova. Among the vulnerable sectors of society, the radical reforms sparked an outrage. In January 1993, a pensioners' demonstration took place in Tallinn, as pensioners felt it was impossible to live with a pension as low as the one in effect at the time (260 EEK (around 20 EUR) a month). The meeting was aggressive and demonstrators attacked the minister of social affairs Marju Lauristin.
The opposition won the 1995 election, but to a large extent continued with the previous governments' policies.
In 1996, Estonia ratified a border agreement with Latvia and completed work with Russia on a technical border agreement. President Meri was re-elected in free and fair indirect elections in August and September in 1996. During parliamentary elections in 1999, the seats in the Riigikogu were divided as follows: the Estonian Centre Party received 28, the Pro Patria Union 18, the Estonian Reform Party 18, the People's Party Moderates (election cartel between Moderates and People's Party) 17, Coalition Party 7, Country People's Party (now People's Union of Estonia) 7, and the United People's Party's electoral cartel 6 seats. Pro Patria Union, the Reform Party, and the Moderates formed a government with Mart Laar as prime minister, whereas the Centre Party with the Coalition Party, People's Union, United People's Party, and members of parliament who were not members of factions formed the opposition in the Riigikogu.
The 1999 Parliamentary election, with a 5% threshold and no electoral cartel allowed, resulted in a disaster for the Coalition Party, which achieved only seven seats together with two of its smaller allies. Estonian Ruralfolk Party, which participated the election on its own list, obtained seven seats as well.
The programme of Mart Laar's government was signed by Pro Patria Union, the Reform Party, the Moderates, and the People's Party. The latter two merged soon after, so Mart Laar's second government is widely known as "Kolmikliit", or the Tripartite coalition. Notwithstanding the different political orientation of the ruling parties, the coalition stayed united until Laar resigned in December 2001, after the Reform Party had broken up the same coalition in Tallinn municipality, making opposition leader Edgar Savisaar the new mayor of Tallinn. After the resignation of Laar, the Reform Party and Estonian Centre Party formed a coalition that lasted until the next parliamentary election, in 2003.
The Moderates joined with the People's Party on 27 November 1999, forming the People's Party Moderates.
Rüütel presidency and Siim Kallas government (2001–2002).
In fall 2001 Arnold Rüütel became the President of the Republic of Estonia, and in January 2002 Prime Minister Laar stepped down. On 28 January 2002, the new government was formed from a coalition of the centre-right Estonian Reform Party and the more left wing Centre Party, with Siim Kallas from the Reform Party of Estonia as Prime Minister.
In 2003, Estonia joined the NATO defense alliance.
Juhan Parts government (2003).
Following parliamentary elections in 2003, the seats were allocated as follows (the United People's Party failed to meet the 5% threshold):
Voter turnout was higher than expected at 58%. The results saw the Centre Party win the most votes, but they were only 0.8% ahead of the new Res Publica party. As a result, both parties won 28 seats, which was a disappointment for the Centre Party who had expected to win the most seats. Altogether the right of centre parties won 60 seats, compared to only 41 for the left wing, and so were expected to form the next government. Both the Centre and Res Publica parties said that they should get the chance to try and form the next government, while ruling out any deal between themselves. President Rüütel had to decide who he should nominate as Prime Minister and therefore be given the first chance at forming a government. On 2 April he invited the leader of the Res Publica party, Juhan Parts, to form a government, and after negotiations a coalition government composed of Res Publica, the Reform Party and the People's Union of Estonia was formed on 10 April.
On 14 September 2003, following negotiations that began in 1998, the citizens of Estonia were asked in a referendum whether or not they wished to join the European Union. With 64% of the electorate turning out, the referendum passed with a 66.83% margin in favor, 33.17% against. Accession to the EU took place on 1 May of the following year.
In February 2004 the People's Party Moderates renamed themselves the Social Democratic Party of Estonia.
On 8 May 2004, a defection of several Centre Party members to form a new party, the Social Liberal Party, over a row concerning the Centrists' "no" stance to joining the European Union changed the allocation of the seats in the Riigikogu. Social-liberals had eight seats, but a hope to form a new party disappeared by 10 May 2005, because most members in the social-liberal group joined other parties.
Andrus Ansip government (2004).
On 24 March Prime Minister Juhan Parts announced his resignation following a vote of no confidence in the Riigikogu against Minister of Justice Ken-Marti Vaher, which was held on 21 March. The result was 54 pro (Social Democrats, Social Liberals, People's Union, Pro Patria Union and Reform Party) with no against or neutral MPs. 32 MPs (Res Publica and Centre Party) did not take part.
On 4 April 2005, President Rüütel nominated Reform party leader Andrus Ansip as Prime Minister designate and asked him to form a new government, the eighth in twelve years. Ansip formed a government out of a coalition of his Reform Party with the People's Union and the Centre Party. Approval by the Riigikogu, which by law must decide within 14 days of his nomination, came on 12 April 2005. Ansip was backed by 53 out of 101 members of the Estonian parliament. Forty deputies voted against his candidature. The general consensus in the Estonian media seems to be that the new cabinet, on the level of competence, is not necessarily an improvement over the old one.
On 18 May 2005, Estonia signed a border treaty with the Russian Federation in Moscow. The treaty was ratified by the Riigikogu on 20 June 2005. However, in the end of June the Russian Ministry of Foreign Affairs informed that it did not intend to become a party to the border treaty and did not consider itself bound by the circumstances concerning the object and the purposes of the treaty because the Riigikogu had attached a preambule to the ratification act that referenced earlier documents that mentioned the Soviet occupation and the uninterrupted legal continuity of the Republic of Estonia during the Soviet period. The issue remains unsolved and is the focus of European-level discussions.
On 4 April 2006, Fatherland Union and Res Publica decided to form a united right-conservative party. The two parties joining was approved on 4 June by both parties in Pärnu. The joined party name is Isamaa ja Res Publica Liit (Union of Pro Patria and Res Publica).
2007 elections.
The 2007 parliamentary elections have shown an improvement in the scores of the Reform Party, gaining 12 seats and reaching 31 MPs; the Centre Party held, while the unified right-conservative Union of Pro Patria and Res Publica lost 16. Socialdemocrats gained 4 seats, while the Greens entered the Parliaments with 7 seats, at the expense of the agrarian People's Union which lost 6. The new configuration of the Estonian Parliament shows a prevalence of centre-left parties. The Centre Party, led by the mayor of Tallinn Edgar Savisaar, has been increasingly excluded from collaboration, since his open collaboration with Putin's United Russia party, real estate scandals in Tallinn, and the Bronze Soldier controversy, considered as a deliberate attempt of splitting the Estonian society by provoking the Russian minority. The lack of a concrete possibility for government alternance in Estonia has been quoted as a concern.
Estonia and the European Union.
On 14 September 2003, following negotiations that began in 1998, the citizens of Estonia were asked in a referendum whether or not they wished to join the European Union. With 64% of the electorate turning out the referendum passed with a 66.83% margin in favor, 33.17% against. Accession to the EU took place on 1 May of the following year.
In its first European Parliament elections in 2004, Estonia elected three MEPs for the Social Democratic Party (PES), while the governing Res Publica Party and People's Union polled poorly, not being able to gain any of the other three MEP posts. The voter turnout in Estonia was one of the lowest of all member countries, at only 26.8%. A similar trend was visible in most of the new member states that joined the EU in 2004.
The European Parliament election of 2009 in Estonia scored a 43.9% turnout – about 17.1% higher than during the previous election, and slightly above the European average of 42.94%. Six seats were up for taking in this election: two of them were won by the Estonian Centre Party. Estonian Reform Party, Union of Pro Patria and Res Publica, Social Democratic Party and an independent candidate Indrek Tarand (who gathered the support of 102,460 voters, only 1,046 votes less than the winner of the election) all won one seat each. The success of independent candidates has been attributed both to general disillusionment with major parties and the use of closed lists which rendered voters incapable of casting a vote for specific candidates in party lists.
On 1 January 2011 Estonia adopted the euro. The enlargement of the eurozone was hailed as a good sign in a period of global financial crisis. However, the government cut down public service salaries; the only opposition, in the absence of organised unions, came from Estonian teachers, whose salary cuts were therefore limited.
Estonian euro coins entered circulation on 1 January 2011. Estonia was the fifth of ten states that joined the EU in 2004, and the first ex-Soviet republic to join the eurozone. Of the ten new member states, Estonia was the first to unveil its design. It originally planned to adopt the euro on 1 January 2007; however, it did not formally apply when Slovenia did, and officially changed its target date to 1 January 2008, and later, to 1 January 2011. On 12 May 2010 the European Commission announced that Estonia had met all criteria to join the eurozone. On 8 June 2010, the EU finance ministers agreed that Estonia would be able to join the euro on 1 January 2011. On 13 July 2010, Estonia received the final approval from the ECOFIN to adopt the euro as from 1 January 2011. On the same date the exchange rate at which the kroon would be exchanged for the euro (€1 = 15.6466 krooni) was also announced. On 20 July 2010, mass production of Estonian euro coins began in the Mint of Finland.
Being a member of the eurozone, NATO and the European Union, Estonia is the most integrated in Western European organizations of all Nordic states.
Estonia–Russia relations in the late 2000s.
Estonia–Russia relations remain tense. According to the Estonian Internal Security Service, Russian influence operations in Estonia form a complex system of financial, political, economic and espionage activities in the Republic of Estonia for the purposes of influencing Estonia's political and economic decisions in ways considered favourable to the Russian Federation and conducted under the sphere-of-influence doctrine known as "near abroad". According to the Centre for Geopolitical Studies, the Russian information campaign, which the centre characterises as a "real mud-throwing" exercise, had provoked a split in Estonian society amongst Russian speakers, inciting some to riot over the relocation of the Bronze Soldier of Tallinn, a cenotaph commemorating the soldiers killed in World War II. Estonia regarded the 2007 cyberattacks on Estonia as an information operation intended to influence the decisions and actions of the Estonian government. While Russia denied any direct involvement in the attacks, hostile rhetoric in the media from the political elite influenced people to attack. Following the 2007 cyber-attacks, the NATO Cooperative Cyber Defence Centre of Excellence (CCDCOE) was established in Tallinn.

</doc>
<doc id="42788" url="https://en.wikipedia.org/wiki?curid=42788" title="380">
380

__NOTOC__
Year 380 (CCCLXXX) was a leap year starting on Wednesday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Augustus and Augustus (or, less frequently, year 1133 "Ab urbe condita"). The denomination 380 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="42789" url="https://en.wikipedia.org/wiki?curid=42789" title="377">
377

__NOTOC__
Year 377 (CCCLXXVII) was a common year starting on Sunday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Augustus and Merobaudes (or, less frequently, year 1130 "Ab urbe condita"). The denomination 377 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Arts and sciences.
</onlyinclude>

</doc>
<doc id="42790" url="https://en.wikipedia.org/wiki?curid=42790" title="375">
375

__NOTOC__
Year 375 (CCCLXXV) was a common year starting on Thursday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year after the Consulship of Augustus and Equitius (or, less frequently, year 1128 "Ab urbe condita"). The denomination 375 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="42794" url="https://en.wikipedia.org/wiki?curid=42794" title="History of Burundi">
History of Burundi

Burundi is one of the few countries in Africa, along with its closely linked neighbour Rwanda among others (such as Botswana, Lesotho, and Swaziland), to be a direct territorial continuation of a pre-colonial era African state.
Kingdom of Burundi (1680–1966).
The origins of Burundi are known from a mix of oral history and archaeology. There are two main founding legends for Burundi. Both suggest that the nation was founded by a man named Cambarantama. The legend most promoted today states that he was Rwandan. The other version, more common in pre-colonial Burundi says that Cambarantama came from the southern state of Buha.
The first evidence of the Burundian state is from 16th century where it emerged on the eastern foothills. Over the following centuries it expanded, annexing smaller neighbours and competing with Rwanda. Its greatest growth occurred under Ntare IV Rutaganzwa Rugamba, who ruled the country from about 1796 to 1850 and saw the kingdom double in size.
The Kingdom of Burundi was characterized by a hierarchical political authority and tributary economic exchange. The king, known as the "mwami" headed a princely aristocracy ("ganwa") which owned most of the land and required a tribute, or tax, from local farmers and herders. In the mid-18th century, this Tutsi royalty consolidated authority over land, production, and distribution with the development of the "ubugabire"—a patron-client relationship in which the populace received royal protection in exchange for tribute and land tenure.
European contact (1856).
European explorers and missionaries made brief visits to the area as early as 1856, and they compared the organization of the kingdom of Burundi with that of the old Greek empire. It was not until 1899 that Burundi became a part of German East Africa. Unlike the Rwandan monarchy, which decided to accept the German advances, the Burundian king Mwezi IV Gisabo opposed all European influence, refusing to wear European clothing and resisting the advance of European missionaries or administrators.
German East Africa (1899-1916).
The Germans used armed force and succeeded in doing great damage, but did not destroy the king’s power. Eventually they backed one of the king's sons-in-law Maconco in a revolt against Gisabo. Gisabo was eventually forced to concede and agreed to German suzerainty. The Germans then helped him suppress Maconco's revolt. The smaller kingdoms along the western shore of Lake Victoria were also attached to Burundi.
Even after this the foreign presence was minimal and the kings continued to rule much as before. The Europeans did, however, bring devastating diseases affecting both people and animals. Affecting the entire region, Burundi was especially hard hit. A great famine hit in 1905, with others striking the entire Great Lakes region in 1914, 1923 and 1944. Between 1905 and 1914 half the population of the western plains region died.
Belgian and United Nations governance (1916-1962).
In 1916 Belgian troops conquered the area during the First World War. In 1923, the League of Nations mandated to Belgium the territory of Ruanda-Urundi, encompassing modern-day Rwanda and Burundi, but stripping the western kingdoms and giving them to British administered Tanganyika. The Belgians administered the territory through indirect rule, building on the Tutsi-dominated aristocratic hierarchy.
Following World War II, Ruanda-Urundi became a United Nations Trust Territory under Belgian administrative authority. It wasn't until 10 November 1959 that Belgium committed itself to political reform and legalised the emergence of competing political parties. Two political parties emerged: the Union for National Progress (UPRONA), a multi-ethnic party led by Tutsi Prince Louis Rwagasore and the Christian Democratic Party (PDC) supported by Belgium. On 13 October 1961, Prime Minister Prince Rwagasore was assassinated following an UPRONA victory in Burundi's United Nations supervised legislative elections of 8 September 1961
Independence (1962).
Full independence was achieved on July 1, 1962. In the context of weak democratic institutions at independence, Tutsi King Mwambutsa IV Bangiriceng established a constitutional monarchy comprising equal numbers of Hutus and Tutsis. The 15 January 1965 assassination of the Hutu prime minister Pierre Ngendandumwe set in motion a series of destabilizing Hutu revolts and subsequent governmental repression.
These were in part in reaction to Rwanda's "Social Revolution" of 1959-1961, where Rwandan Tutsi were subject to mass murder by the new government of Hutu Grégoire Kayibanda. In Burundi the Tutsi became committed to ensuring they would not meet the same fate and much of the country's military and police forces became controlled by Tutsis. Unlike Rwanda, which allied itself with the United States in the Cold War, Burundi after independence became affiliated with China.
The monarchy refused to recognize gains by Hutu candidates in the first legislative elections held by Burundi as an independent country on 10 May 1965. In response, a group of Hutu carried out a failed coup attempt against the monarchy on 18 October 1965, which in turn prompted the killing of scores of Hutu politicians and intellectuals. On 8 July 1966, King Mwambutsa IV was deposed by his son, Prince Ntare V, who himself was deposed by his prime minister Capt. Michel Micombero on 28 November 1966. Micombero abolished the monarchy and declared a republic. A de facto military regime emerged and civil unrest continued throughout the late 1960s and early 1970s. Micombero headed a clique of ruling Hima, the Tutsi subgroup located in southern Burundi. Similar to 1965, rumors of an impending Hutu coup in 1969 prompted the arrest and execution of scores of prominent political and military figures.
In June 1971, a group of Banyaruguru, the socially "higher up" subgroup of Tutsi located in the north of the country, were accused of conspiracy by the ruling Hima clique. On 14 January 1972, a military tribunal sentenced four Banyaruguru officers and five civilians to death, and seven to life imprisonment. To the Hima concerns about a Hutu uprising or Banyaruguru-led coup was added the return of Ntare V from exile, a potential rallying point for the Hutu majority.
1972 genocide.
On April 29, there was an outbreak of violence in the south of the country, also the base of the Hima, where bands of roving Hutu committed innumerable atrocities against Tutsi civilians. All civilian and military authorities in the city of Bururi were killed and the insurgents then seized the armories in the towns of Rumonge and Nyanza-Lac. They then attempted to kill every Tutsi they could, as well as some Hutu who refused to participate in the rebellion, before retreating to Vyanda, near Bururi, and proclaiming a "Republic of Martyazo."
A week after the insurgent proclamation of a republic, government troops moved in. Meanwhile, President Micombero declared martial law on May 30 and asked Zairean President Mobutu Sese Seko for assistance. Congolese paratroopers were deployed to secure the airport while the Burundi army moved into the countryside. Africanist René Lemarchand notes, "What followed was not so much a repression as a hideous slaughter of Hutu civilians. The carnage went on unabated through the month of August. By then virtually every educated Hutu element, down to secondary school students, was either dead or in flight."
Because the perpetrators, composed of government troops and the Jeunesses Révolutionnaires Rwagasore (JRR), the youth wing of the Union for National Progress ruling party, targeted primarily civil servants, educated males and university students, solely because of their "Hutuness" and irrespective of whether they posed a threat, Lemarchand terms the eradication a "partial genocide." One of the first to be killed was deposed monarch Ntare V, in Gitega.
As president, Micombero became an advocate of African socialism and received support from the People's Republic of China. He imposed a staunch regime of law and order, sharply repressing Hutu militarism.
From late April to September 1972, an estimated 200,000 to 300,000 Hutu were killed. About 300,000 people became refugees, with most fleeing to Tanzania. In an effort to attract sympathy from the United States, the Tutsi-dominated government accused the Hutu rebels of having Communist leanings, although there is no credible evidence that this was actually the case. Lemarchand notes that, while crushing the rebellion was the first priority, the genocide was successful in a number of other objectives: ensuring the long-term stability of the Tutsi state by eliminating Hutu elites and potential elites; turning the army, police and gendarmerie into a Tutsi monopoly; denying the potential return of monarchy through the murder of Ntare V; and creating a new legitimacy for the Hima-dominated state as protector of the country, especially for the previously fractious Tutsi-Banyaruguru.
Post-1972 genocide developments.
In 1976, Colonel Jean-Baptiste Bagaza took power in a bloodless coup. Although Bagaza led a Tutsi-dominated military regime, he encouraged land reform, electoral reform, and national reconciliation. In 1981, a new constitution was promulgated. In 1984, Bagaza was elected head of state, as the sole candidate. After his election, Bagaza's human rights record deteriorated as he suppressed religious activities and detained political opposition members.
In 1987, Major Pierre Buyoya overthrew Col. Bagaza in a military coup d'état. He dissolved opposition parties, suspended the 1981 constitution, and instituted his ruling Military Committee for National Salvation (CSMN). During 1988, increasing tensions between the ruling Tutsis and the majority Hutus resulted in violent confrontations between the army, the Hutu opposition, and Tutsi hardliners. During this period, an estimated 150,000 people were killed, with tens of thousands of refugees fleeing to neighboring countries. Buyoya formed a commission to investigate the causes of the 1988 unrest and to develop a charter for democratic reform.
In 1991, Buyoya approved a constitution that provided for a president, non-ethnic government, and a parliament. Burundi's first Hutu president, Melchior Ndadaye, of the Hutu-dominated Front for Democracy in Burundi (FRODEBU) Party, was elected in 1993.
1993 Genocide and Civil War (1993-2005).
Ndadaye was assassinated three months later, in October 1993, by Tutsi army extremists. The country’s situation rapidly declined as Hutu peasants began to rise up and massacre Tutsi. In acts of brutal retribution, the Tutsi army proceeded to round up thousands of Hutu and kill them. The Rwandan Genocide in 1994, sparked by the killing of Ndadaye’s successor Cyprien Ntaryamira, further aggravated the conflict in Burundi by sparking additional massacres of Tutsis.
A decade of civil war followed, as the Hutu formed militias in the refugee camps of northern Tanzania. An estimated 300,000 people were killed in clashes and reprisals against the local population, with 550,000 citizens (nine percent of the population) being displaced. After the assassination of Ntaryamira, the Hutu presidency and Tutsi military operated under a power-sharing political system until July 1996, when Tutsi Pierre Buyoya seized power in a military coup. Under international pressure, the warring factions negotiated a peace agreement in Arusha in 2000, which called for ethnically balanced military and government and democratic elections.
Two powerful Hutu rebel groups (the CNDD-FDD and the FNL) refused to sign the peace agreement and fighting continued in the countryside. Finally, the CNDD-FDD agreed to sign a peace deal in November 2003 and joined the transitional government. The last remaining rebel group, the FNL, continued to reject the peace process and committed sporadic acts of violence in 2003 and 2004, finally signing a cease fire agreement in 2006.
Post-war (2005-).
Former President Domitien Ndayizeye and his political supporters were arrested in 2006 and accused of plotting a coup, but later he was acquitted by the Supreme Court. International human rights groups claimed that the current government was framing Domitien Ndayizeye by torturing him into false confessions of a coup plot. Along with these accusations, in December 2006 the International Crisis Group labeled Burundi’s government with a “deteriorating” status in its treatment of human rights. The organization reported that the government had arrested critics, muzzled the press, committed human rights abuses, and tightened its control over the economy, and that "unless it this authoritarian course, it risk[ed triggering violent unrest and losing the gains of peace process."
In February 2007, the U.N. officially shut down its peacekeeping operations in Burundi and turned its attention to rebuilding the nation’s economy, which relies heavily on tea and coffee, but which had suffered severely during 12 years of civil war. The U.N. had deployed 5,600 peacekeepers since 2004, and several hundred troops remained to work with the African Union in monitoring the ceasefire. The U.N. donated $35 million to Burundi to work on infrastructure, to promote democratic practices, to rebuild the military, and to defend human rights.
SOS Children, an NGO, uses HIV testing and prevention strategies, counseling, de-stigmatization, antiretroviral drugs and condoms to combat AIDS. Sample testing had shown that those who were HIV positive were 20 percent of the urban population and 6% of the rural population. Nevertheless, the death toll due to the syndrome has been devastating: the UN estimated 25,000 deaths in 2001 and Oxfam estimated 45,000 deaths in 2003.
Reaching a stable compromise on post-transition power-sharing was difficult. Although a post-transition constitution was approved in September 2004, it was approved over a boycott by the Tutsi parties. In addition, the Arusha Peace Agreement mandated that local and national elections be held before the ending of the transitional period on 31 October 2004, but transitional institutions were extended. On 28 February 2005, however, Burundians popularly approved a post-transitional constitution by national referendum, with elections set to take place throughout the summer of 2005. After local, parliamentary, and other elections in June and July, on 19 August 2005, the good governance minister, Pierre Nkurunziza, became the first post-transitional president.
He was re-elected in 2010 with more than 91% of the votes amidst an opposition boycott and sworn in for his second term on 26 August 2010.
In April, 2015, Nkurunziza announced that he would seek a third term in office. The opposition said that Nkurunziza's bid to extend his term was in defiance of the constitution, as it bars the president from running for a third term. However, Nkurunziza's allies said his first term did not count as he was appointed by parliament and not directly by the people. On April 26 police clashed with demonstrators protesting Nkurunziza’s announcement that he would seek a third term in office. At least six people were killed in the first two days of ongoing protests. The government shut down multiple radio stations and arrested a prominent civil society leader, Pierre-Claver Mbonimpa. UN General Secretary Ban Ki-moon said, in a statement, that he had despatched his special envoy for the region, Said Djinnit, to Burundi for talks with Nkurunziza. African Union commission head Nkosazana Dlamini-Zuma said she welcomed a decision by Burundi's Senate to ask the Constitutional Court to rule whether Nkurunziza could stand for re-election. More than 24,000 people fled Burundi in April, as tensions mounted ahead of presidential elections in June, the UN refugee agency said.
On May 13, 2015, Burundi army General Godefroid Niyombareh, former head of Burundian intelligence, declared a coup via radio while Nkurunziza was abroad attending a summit in Tanzania with other African leaders. Niyombareh had been fired by the President in February. Despite reports that gunshots had been heard and people were celebrating in the streets of the capital, government officials dismissed the threat and claimed to remain in control.
Opposition groups announced on 26 June that they would boycott the election. 
Speaking to a Kenyan television station on 6 July, one of the coup leaders, General Leonard Ngendakumana, called for armed rebellion against Nkurunziza. He said that his group was responsible for the grenade attacks and said that "our intent is to intensify". Fighting was reported in northern Burundi on 10–11 July. The military said on 13 July that 31 rebels had been killed and 170 had been captured in those battles; it said that six of its own soldiers had also been wounded. The Burundian government stated that the rebels had crossed into northern Burundi through the Nyungwe Forest from Rwanda but the Rwandan government denied this. Ngendakumana said that the rebels were from his group.
Shortly after the election was held on 21 July, without the participation of the opposition, main opposition leader Agathon Rwasa proposed the formation of a national unity government, while warning of the potential for more violence and armed rebellion against Nkurunziza. As conditions for participating in such a government, Rwasa said that Nkurunziza's third term would need to be greatly truncated to no more than a year and new elections would have to be held, although he admitted that he doubted Nkurunziza would accept those conditions. He also urged those hoping to oust Nkurunziza through violence to instead focus on dialogue. The government welcomed the idea of forming a national unity government, but rejected the notion of truncating Nkurunziza's new term.
The presidential election results were announced on 24 July 2015. Nkurunziza won the election with 69.41% of the vote. Agathon Rwasa was placed second and credited with 18.99% despite calling for a boycott.
See also.
General:

</doc>
<doc id="42795" url="https://en.wikipedia.org/wiki?curid=42795" title="Anorexia (symptom)">
Anorexia (symptom)

Anorexia is the decreased sensation of appetite. While the term in non-scientific publications is often used interchangeably with anorexia nervosa, many possible causes exist for a decreased appetite, some of which may be harmless, while others indicate a serious clinical condition or pose a significant risk.
For example, anorexia of infection is part of the acute phase response (APR) to infection. The APR can be triggered by lipopolysaccharides and peptidoglycans from bacterial cell walls, bacterial DNA, double-stranded viral RNA, and viral glycoproteins, which can trigger production of a variety of proinflammatory cytokines. These can have an indirect effect on appetite by a number of means, including peripheral afferents from their sites of production in the body, by enhancing production of leptin from fat stores. Inflammatory cytokines can also signal to the central nervous system more directly by specialized transport mechanisms through the blood–brain barrier, via circumventricular organs (which are outside the barrier), or by triggering production of eicosanoids in the endothelial cells of the brain vasculature. Ultimately the control of appetite by this mechanism is thought to be mediated by the same factors normally controlling appetite, such as neurotransmitters (serotonin, dopamine, histamine, norepinephrine, corticotropin releasing factor, neuropeptide Y, and α-melanocyte-stimulating hormone).
Complications.
Sudden cardiac death.
Anorexia is a relatively common condition that can lead patients to have dangerous electrolyte imbalances, leading to acquired long QT syndrome which can result in sudden cardiac death. This can develop over a prolonged period of time, and the risk is further heightened when feeding resumes after a period of abstaining from consumption. Care must be taken under such circumstances to avoid potentially fatal complications of refeeding syndrome.

</doc>
<doc id="42798" url="https://en.wikipedia.org/wiki?curid=42798" title="Sophie Marceau">
Sophie Marceau

Sophie Marceau (; born Sophie Danièle Sylvie Maupu; 17 November 1966) is a French actress, director, screenwriter, and author. As a teenager, Marceau achieved popularity with her debut films "La Boum" (1980) and "La Boum 2" (1982), receiving a César Award for Most Promising Actress. She became a film star in Europe with a string of successful films, including "L'Étudiante" (1988), "Pacific Palisades" (1990), "Fanfan" (1993), and "Revenge of the Musketeers" (1994). Marceau became an international film star with her performances in "Braveheart" (1995), "Firelight" (1997), and the 19th James Bond film "The World Is Not Enough" (1999).
Early life.
Marceau was born 17 November 1966 in Paris, the second child of Simone (née Morisset), a shop assistant, and Benoît Maupu, a truck driver. Her parents divorced when she was nine years old.
Film career.
In February 1980, Marceau and her mother came across a model agency looking for teenagers. Marceau had photos taken at the agency, but did not think anything would come of it. At the same time, Françoise Menidrey, the casting director for Claude Pinoteau's "La Boum" (1980), asked modeling agencies to recommend a new teenager for the project. After viewing the rushes, Alain Poiré, the director of the Gaumont Film Company, signed Marceau to a long-term contract. "La Boum" was a hit movie, not only in France, where 4,378,500 tickets were sold, but also in several other European countries. In 1981, Marceau made her singing debut with French singer François Valéry on record "Dream in Blue", written by Pierre Delanoë.
In 1982, at age 15, Marceau bought back her contract with Gaumont for one million French francs. She borrowed most of the money. After starring in the sequel film "La Boum 2" (1982), Marceau focused on more dramatic roles, including the historical drama "Fort Saganne" in 1984 with Gérard Depardieu and Catherine Deneuve, "Joyeuses Pâques" ("Happy Easter") in 1984, "L'amour braque" and "Police" in 1985, and "Descente aux enfers" ("Descent Into Hell") in 1986. In 1988, she starred in "L'Étudiante" ("The Student") and the historical adventure film "Chouans!". That year, Marceau was named Best Romantic Actress at the International Festival of Romantic Movies for her role in "Chouans!"
In 1989, Marceau starred in "My Nights Are More Beautiful Than Your Days", which was directed by her long-time boyfriend Andrzej Zulawski. In 1990, she starred in "Pacific Palisades" and "La note bleue", her third film directed by her companion. In 1991, she ventured into the theater in "Eurydice", which earned Marceau the Moliere Award for Best Female Newcomer. Throughout the 1990s, Marceau began making less-dramatic films, such as the comedy "Fanfan" in 1993 and "Revenge of the Musketeers" ("La fille de d'Artagnan") in 1994—both popular in Europe and abroad. That year, she returned to the theatre as Eliza Doolittle in "Pygmalion".
Marceau achieved international recognition in 1995 playing the role of Princess Isabelle in Mel Gibson's "Braveheart". That year, she was part of an ensemble of international actors in the French film directed by Michelangelo Antonioni and Wim Wenders, "Beyond the Clouds". In 1997, she continued her string of successful films with William Nicholson's "Firelight", filmed in England, Véra Belmont's "Marquise", filmed in France, and Bernard Rose's "Anna Karenina", filmed in Russia. In 1999, she played Hippolyta in "A Midsummer Night's Dream", and the villainess Bond girl Elektra King in "The World Is Not Enough". In 2000, Marceau teamed up again with her then-boyfriend Andrzej Zulawski to film "Fidelity", playing the role of a talented photographer who takes a job at a scandal-mongering tabloid and becomes romantically involved with an eccentric children's book publisher.
In recent years, Marceau has continued to appear in a wide variety of roles, mainly in French films, playing a widowed nurse in "Nelly" ("À ce soir") in 2004, an undercover police agent in "Anthony Zimmer" in 2005, and the troubled daughter of a murdered film star in "Trivial" in 2007. In 2008, Marceau played a member of the French Resistance movement in "Female Agents", and a struggling single mother in "LOL (Laughing Out Loud)". In 2009, she teamed up with Monica Bellucci in "Don't Look Back" about the mysterious connection between two women who have never met. In 2010, Marceau played a successful business executive forced to confront her unhappy childhood in "With Love... from the Age of Reason" ("L'âge de raison").
In 2012, Marceau played a 40-something career woman who falls in love with a young jazz musician in "Happiness Never Comes Alone". In 2013, she appeared in "Arrêtez-moi" ("Arrest Me") as a woman who shows up at a police station and confesses to the murder of her abusive husband several years earlier.
She was selected to be on the jury for the main competition section of the 2015 Cannes Film Festival.
Author and director.
In 1995, Marceau wrote a semi-autobiographical novel, "Menteuse" (the English translation, "Telling Lies", was published in 2001). Marceau's work was described as "an exploration of female identity".
In 2002, Marceau made her directorial debut in the feature film "Speak to Me of Love", for which she was named Best Director at the Montreal World Film Festival. The film starred Judith Godrèche. It was her second directorial effort, following her nine-minute short film "L'aube à l'envers" in 1995, which also starred Godrèche. In 2007, she directed "Trivial", her second feature film.
Personal life.
From 1985 to 2001, Marceau had a relationship with director Andrzej Żuławski. Their son Vincent was born in July 1995. In 2001, Marceau separated from Żuławski and began a six-year relationship with producer Jim Lemley. They have a daughter, Juliette (born June 2002 in London).
From 2007 to 2014, she had a relationship with Christopher Lambert, with whom she appeared in the films "Trivial" and "Cartagena". On July 11, 2014 the couple announced their amicable separation.

</doc>
<doc id="42799" url="https://en.wikipedia.org/wiki?curid=42799" title="Speech synthesis">
Speech synthesis

Speech synthesis is the artificial production of human speech. A computer system used for this purpose is called a speech computer or speech synthesizer, and can be implemented in software or hardware products. A text-to-speech (TTS) system converts normal language text into speech; other systems render symbolic linguistic representations like phonetic transcriptions into speech.
Synthesized speech can be created by concatenating pieces of recorded speech that are stored in a database. Systems differ in the size of the stored speech units; a system that stores phones or diphones provides the largest output range, but may lack clarity. For specific usage domains, the storage of entire words or sentences allows for high-quality output. Alternatively, a synthesizer can incorporate a model of the vocal tract and other human voice characteristics to create a completely "synthetic" voice output.
The quality of a speech synthesizer is judged by its similarity to the human voice and by its ability to be understood clearly. An intelligible text-to-speech program allows people with visual impairments or reading disabilities to listen to written works on a home computer. Many computer operating systems have included speech synthesizers since the early 1990s.
A text-to-speech system (or "engine") is composed of two parts: a front-end and a back-end. The front-end has two major tasks. First, it converts raw text containing symbols like numbers and abbreviations into the equivalent of written-out words. This process is often called "text normalization", "pre-processing", or "tokenization". The front-end then assigns phonetic transcriptions to each word, and divides and marks the text into prosodic units, like phrases, clauses, and sentences. The process of assigning phonetic transcriptions to words is called "text-to-phoneme" or "grapheme-to-phoneme" conversion. Phonetic transcriptions and prosody information together make up the symbolic linguistic representation that is output by the front-end. The back-end—often referred to as the "synthesizer"—then converts the symbolic linguistic representation into sound. In certain systems, this part includes the computation of the "target prosody" (pitch contour, phoneme durations), which is then imposed on the output speech.
History.
Long before the invention of electronic signal processing, some people tried to build machines to emulate human speech. Some early legends of the existence of "Brazen Heads" involved Pope Silvester II (d. 1003 AD), Albertus Magnus (1198–1280), and Roger Bacon (1214–1294).
In 1779 the Danish scientist Christian Kratzenstein, working at the Russian Imperial Academy of Sciences and Arts, built models of the human vocal tract that could produce the five long vowel sounds (in notation: , , , and ). There followed the bellows-operated "acoustic-mechanical speech machine" of Wolfgang von Kempelen of Pressburg, Hungary, described in a 1791 paper. This machine added models of the tongue and lips, enabling it to produce consonants as well as vowels. In 1837, Charles Wheatstone produced a "speaking machine" based on von Kempelen's design, and in 1846, Joseph Faber exhibited the "Euphonia". In 1923 Paget resurrected Wheatstone's design.
In the 1930s Bell Labs developed the vocoder, which automatically analyzed speech into its fundamental tones and resonances. From his work on the vocoder, Homer Dudley developed a keyboard-operated voice-synthesizer called The Voder (Voice Demonstrator), which he exhibited at the 1939 New York World's Fair.
Dr. Franklin S. Cooper and his colleagues at Haskins Laboratories built the Pattern playback in the late 1940s and completed it in 1950. There were several different versions of this hardware device; only one currently survives. The machine converts pictures of the acoustic patterns of speech in the form of a spectrogram back into sound. Using this device, Alvin Liberman and colleagues discovered acoustic cues for the perception of phonetic segments (consonants and vowels).
Dominant systems in the 1980s and 1990s were the MITalk system, based largely on the work of Dennis Klatt at MIT, and the Bell Labs system; the latter was one of the first multilingual language-independent systems, making extensive use of natural language processing methods.
Early electronic speech-synthesizers sounded robotic and were often barely intelligible. The quality of synthesized speech has steadily improved, but output from contemporary speech synthesis systems remains clearly distinguishable from actual human speech.
Kurzweil predicted in 2005 that as the cost-performance ratio caused speech synthesizers to become cheaper and more accessible, more people would benefit from the use of text-to-speech programs.
Electronic devices.
The first computer-based speech-synthesis systems originated in the late 1950s. Noriko Umeda "et al." developed the first general English text-to-speech system in 1968 at the Electrotechnical Laboratory, Japan. In 1961 physicist John Larry Kelly, Jr and his colleague Louis Gerstman used an IBM 704 computer to synthesize speech, an event among the most prominent in the history of Bell Labs. Kelly's voice recorder synthesizer (vocoder) recreated the song "Daisy Bell", with musical accompaniment from Max Mathews. Coincidentally, Arthur C. Clarke was visiting his friend and colleague John Pierce at the Bell Labs Murray Hill facility. Clarke was so impressed by the demonstration that he used it in the climactic scene of his screenplay for his novel "", where the HAL 9000 computer sings the same song as astronaut Dave Bowman puts it to sleep. Despite the success of purely electronic speech synthesis, research into mechanical speech-synthesizers continues.
Handheld electronics featuring speech synthesis began emerging in the 1970s. One of the first was the Telesensory Systems Inc. (TSI) "Speech+" portable calculator for the blind in 1976. Other devices had primarily educational purposes, such as the Speak & Spell toy produced by Texas Instruments in 1978. Fidelity released a speaking version of its electronic chess computer in 1979. The first video game to feature speech synthesis was the 1980 shoot 'em up arcade game, "Stratovox", from Sun Electronics.
Another early example, the arcade version of "Berzerk", also dates from 1980. The Milton Bradley Company produced the first multi-player electronic game using voice synthesis, "Milton", in the same year.
Synthesizer technologies.
The most important qualities of a speech synthesis system are "naturalness" and "intelligibility". Naturalness describes how closely the output sounds like human speech, while intelligibility is the ease with which the output is understood. The ideal speech synthesizer is both natural and intelligible. Speech synthesis systems usually try to maximize both characteristics.
The two primary technologies generating synthetic speech waveforms are "concatenative synthesis" and "formant synthesis". Each technology has strengths and weaknesses, and the intended uses of a synthesis system will typically determine which approach is used.
Concatenation synthesis.
Concatenative synthesis is based on the concatenation (or stringing together) of segments of recorded speech. Generally, concatenative synthesis produces the most natural-sounding synthesized speech. However, differences between natural variations in speech and the nature of the automated techniques for segmenting the waveforms sometimes result in audible glitches in the output. There are three main sub-types of concatenative synthesis.
Unit selection synthesis.
Unit selection synthesis uses large databases of recorded speech. During database creation, each recorded utterance is segmented into some or all of the following: individual phones, diphones, half-phones, syllables, morphemes, words, phrases, and sentences. Typically, the division into segments is done using a specially modified speech recognizer set to a "forced alignment" mode with some manual correction afterward, using visual representations such as the waveform and spectrogram. An index of the units in the speech database is then created based on the segmentation and acoustic parameters like the fundamental frequency (pitch), duration, position in the syllable, and neighboring phones. At run time, the desired target utterance is created by determining the best chain of candidate units from the database (unit selection). This process is typically achieved using a specially weighted decision tree.
Unit selection provides the greatest naturalness, because it applies only a small amount of digital signal processing (DSP) to the recorded speech. DSP often makes recorded speech sound less natural, although some systems use a small amount of signal processing at the point of concatenation to smooth the waveform. The output from the best unit-selection systems is often indistinguishable from real human voices, especially in contexts for which the TTS system has been tuned. However, maximum naturalness typically require unit-selection speech databases to be very large, in some systems ranging into the gigabytes of recorded data, representing dozens of hours of speech. Also, unit selection algorithms have been known to select segments from a place that results in less than ideal synthesis (e.g. minor words become unclear) even when a better choice exists in the database. Recently, researchers have proposed various automated methods to detect unnatural segments in unit-selection speech synthesis systems.
Diphone synthesis.
Diphone synthesis uses a minimal speech database containing all the diphones (sound-to-sound transitions) occurring in a language. The number of diphones depends on the phonotactics of the language: for example, Spanish has about 800 diphones, and German about 2500. In diphone synthesis, only one example of each diphone is contained in the speech database. At runtime, the target prosody of a sentence is superimposed on these minimal units by means of digital signal processing techniques such as linear predictive coding, PSOLA or MBROLA. or more recent techniques such as pitch modification in the source domain using discrete cosine transform Diphone synthesis suffers from the sonic glitches of concatenative synthesis and the robotic-sounding nature of formant synthesis, and has few of the advantages of either approach other than small size. As such, its use in commercial applications is declining, although it continues to be used in research because there are a number of freely available software implementations.
Domain-specific synthesis.
Domain-specific synthesis concatenates prerecorded words and phrases to create complete utterances. It is used in applications where the variety of texts the system will output is limited to a particular domain, like transit schedule announcements or weather reports. The technology is very simple to implement, and has been in commercial use for a long time, in devices like talking clocks and calculators. The level of naturalness of these systems can be very high because the variety of sentence types is limited, and they closely match the prosody and intonation of the original recordings.
Because these systems are limited by the words and phrases in their databases, they are not general-purpose and can only synthesize the combinations of words and phrases with which they have been preprogrammed. The blending of words within naturally spoken language however can still cause problems unless the many variations are taken into account. For example, in non-rhotic dialects of English the ""r"" in words like ""clear"" is usually only pronounced when the following word has a vowel as its first letter (e.g. ""clear out"" is realized as ). Likewise in French, many final consonants become no longer silent if followed by a word that begins with a vowel, an effect called liaison. This alternation cannot be reproduced by a simple word-concatenation system, which would require additional complexity to be context-sensitive.
Formant synthesis.
Formant synthesis does not use human speech samples at runtime. Instead, the synthesized speech output is created using additive synthesis and an acoustic model (physical modelling synthesis). Parameters such as fundamental frequency, voicing, and noise levels are varied over time to create a waveform of artificial speech. This method is sometimes called "rules-based synthesis"; however, many concatenative systems also have rules-based components.
Many systems based on formant synthesis technology generate artificial, robotic-sounding speech that would never be mistaken for human speech. However, maximum naturalness is not always the goal of a speech synthesis system, and formant synthesis systems have advantages over concatenative systems. Formant-synthesized speech can be reliably intelligible, even at very high speeds, avoiding the acoustic glitches that commonly plague concatenative systems. High-speed synthesized speech is used by the visually impaired to quickly navigate computers using a screen reader. Formant synthesizers are usually smaller programs than concatenative systems because they do not have a database of speech samples. They can therefore be used in embedded systems, where memory and microprocessor power are especially limited. Because formant-based systems have complete control of all aspects of the output speech, a wide variety of prosodies and intonations can be output, conveying not just questions and statements, but a variety of emotions and tones of voice.
Examples of non-real-time but highly accurate intonation control in formant synthesis include the work done in the late 1970s for the Texas Instruments toy Speak & Spell, and in the early 1980s Sega arcade machines and in many Atari, Inc. arcade games using the TMS5220 LPC Chips. Creating proper intonation for these projects was painstaking, and the results have yet to be matched by real-time text-to-speech interfaces.
Articulatory synthesis.
Articulatory synthesis refers to computational techniques for synthesizing speech based on models of the human vocal tract and the articulation processes occurring there. The first articulatory synthesizer regularly used for laboratory experiments was developed at Haskins Laboratories in the mid-1970s by Philip Rubin, Tom Baer, and Paul Mermelstein. This synthesizer, known as ASY, was based on vocal tract models developed at Bell Laboratories in the 1960s and 1970s by Paul Mermelstein, Cecil Coker, and colleagues.
Until recently, articulatory synthesis models have not been incorporated into commercial speech synthesis systems. A notable exception is the NeXT-based system originally developed and marketed by Trillium Sound Research, a spin-off company of the University of Calgary, where much of the original research was conducted. Following the demise of the various incarnations of NeXT (started by Steve Jobs in the late 1980s and merged with Apple Computer in 1997), the Trillium software was published under the GNU General Public License, with work continuing as gnuspeech. The system, first marketed in 1994, provides full articulatory-based text-to-speech conversion using a waveguide or transmission-line analog of the human oral and nasal tracts controlled by Carré's "distinctive region model".
More recent synthesizers, developed by Jean Schoentgen, Jorge C. Lucero and colleagues, incorporate models of vocal fold biomechanics, glottal aerodynamics and acoustic wave propagation in the bronqui, traquea, nasal and oral cavities, and thus constitute full systems of physics-based speech simulation.
HMM-based synthesis.
HMM-based synthesis is a synthesis method based on hidden Markov models, also called Statistical Parametric Synthesis. In this system, the frequency spectrum (vocal tract), fundamental frequency (voice source), and duration (prosody) of speech are modeled simultaneously by HMMs. Speech waveforms are generated from HMMs themselves based on the maximum likelihood criterion.
Sinewave synthesis.
Sinewave synthesis is a technique for synthesizing speech by replacing the formants (main bands of energy) with pure tone whistles.
Challenges.
Text normalization challenges.
The process of normalizing text is rarely straightforward. Texts are full of heteronyms, numbers, and abbreviations that all require expansion into a phonetic representation. There are many spellings in English which are pronounced differently based on context. For example, "My latest project is to learn how to better project my voice" contains two pronunciations of "project".
Most text-to-speech (TTS) systems do not generate semantic representations of their input texts, as processes for doing so are unreliable, poorly understood, and computationally ineffective. As a result, various heuristic techniques are used to guess the proper way to disambiguate homographs, like examining neighboring words and using statistics about frequency of occurrence.
Recently TTS systems have begun to use HMMs (discussed above) to generate "parts of speech" to aid in disambiguating homographs. This technique is quite successful for many cases such as whether "read" should be pronounced as "red" implying past tense, or as "reed" implying present tense. Typical error rates when using HMMs in this fashion are usually below five percent. These techniques also work well for most European languages, although access to required training corpora is frequently difficult in these languages.
Deciding how to convert numbers is another problem that TTS systems have to address. It is a simple programming challenge to convert a number into words (at least in English), like "1325" becoming "one thousand three hundred twenty-five." However, numbers occur in many different contexts; "1325" may also be read as "one three two five", "thirteen twenty-five" or "thirteen hundred and twenty five". A TTS system can often infer how to expand a number based on surrounding words, numbers, and punctuation, and sometimes the system provides a way to specify the context if it is ambiguous. Roman numerals can also be read differently depending on context. For example, "Henry VIII" reads as "Henry the Eighth", while "Chapter VIII" reads as "Chapter Eight".
Similarly, abbreviations can be ambiguous. For example, the abbreviation "in" for "inches" must be differentiated from the word "in", and the address "12 St John St." uses the same abbreviation for both "Saint" and "Street". TTS systems with intelligent front ends can make educated guesses about ambiguous abbreviations, while others provide the same result in all cases, resulting in nonsensical (and sometimes comical) outputs, such as "co-operation" being rendered as "company operation".
Text-to-phoneme challenges.
Speech synthesis systems use two basic approaches to determine the pronunciation of a word based on its spelling, a process which is often called text-to-phoneme or grapheme-to-phoneme conversion (phoneme is the term used by linguists to describe distinctive sounds in a language). The simplest approach to text-to-phoneme conversion is the dictionary-based approach, where a large dictionary containing all the words of a language and their correct pronunciations is stored by the program. Determining the correct pronunciation of each word is a matter of looking up each word in the dictionary and replacing the spelling with the pronunciation specified in the dictionary. The other approach is rule-based, in which pronunciation rules are applied to words to determine their pronunciations based on their spellings. This is similar to the "sounding out", or synthetic phonics, approach to learning reading.
Each approach has advantages and drawbacks. The dictionary-based approach is quick and accurate, but completely fails if it is given a word which is not in its dictionary. As dictionary size grows, so too does the memory space requirements of the synthesis system. On the other hand, the rule-based approach works on any input, but the complexity of the rules grows substantially as the system takes into account irregular spellings or pronunciations. (Consider that the word "of" is very common in English, yet is the only word in which the letter "f" is pronounced .) As a result, nearly all speech synthesis systems use a combination of these approaches.
Languages with a phonemic orthography have a very regular writing system, and the prediction of the pronunciation of words based on their spellings is quite successful. Speech synthesis systems for such languages often use the rule-based method extensively, resorting to dictionaries only for those few words, like foreign names and borrowings, whose pronunciations are not obvious from their spellings. On the other hand, speech synthesis systems for languages like English, which have extremely irregular spelling systems, are more likely to rely on dictionaries, and to use rule-based methods only for unusual words, or words that aren't in their dictionaries.
Evaluation challenges.
The consistent evaluation of speech synthesis systems may be difficult because of a lack of universally agreed objective evaluation criteria. Different organizations often use different speech data. The quality of speech synthesis systems also depends to a large degree on the quality of the production technique (which may involve analogue or digital recording) and on the facilities used to replay the speech. Evaluating speech synthesis systems has therefore often been compromised by differences between production techniques and replay facilities.
Since 2005, however, some researchers have started to evaluate speech synthesis systems using a common speech dataset.
Prosodics and emotional content.
A study in the journal "Speech Communication" by Amy Drahota and colleagues at the University of Portsmouth, UK, reported that listeners to voice recordings could determine, at better than chance levels, whether or not the speaker was smiling. It was suggested that identification of the vocal features that signal emotional content may be used to help make synthesized speech sound more natural. One of the related issues is modification of the pitch contour of the sentence, depending upon whether it is an affirmative, interrogative or exclamatory sentence. One of the techniques for pitch modification uses discrete cosine transform in the source domain (linear prediction residual). Such pitch synchronous pitch modification techniques need a priori pitch marking of the synthesis speech database using techniques such as epoch extraction using dynamic plosion index applied on the integrated linear prediction residual of the voiced regions of speech.
Dedicated hardware.
Early Technology (not available anymore)
Current (as of 2013)
Hardware and software systems.
Popular systems offering speech synthesis as a built-in capability.
Mattel.
The Mattel Intellivision game console offered the Intellivoice Voice Synthesis module in 1982. It included the SP0256 Narrator speech synthesizer chip on a removable cartridge. The Narrator had 2kB of Read-Only Memory (ROM), and this was utilized to store a database of generic words that could be combined to make phrases in Intellivision games. Since the Orator chip could also accept speech data from external memory, any additional words or phrases needed could be stored inside the cartridge itself. The data consisted of strings of analog-filter coefficients to modify the behavior of the chip's synthetic vocal-tract model, rather than simple digitized samples.
SAM.
Also released in 1982, Software Automatic Mouth was the first commercial all-software voice synthesis program. It was later used as the basis for Macintalk. The program was available for non-Macintosh Apple computers (including the Apple II, and the Lisa), various Atari models and the Commodore 64. The Apple version preferred additional hardware that contained DACs, although it could instead use the computer's one-bit audio output (with the addition of much distortion) if the card was not present. The Atari made use of the embedded POKEY audio chip. Speech playback on the Atari normally disabled interrupt requests and shut down the ANTIC chip during vocal output. The audible output is extremely distorted speech when the screen is on. The Commodore 64 made use of the 64's embedded SID audio chip.
Atari.
Arguably, the first speech system integrated into an operating system was the 1400XL/1450XL personal computers designed by Atari, Inc. using the Votrax SC01 chip in 1983. The 1400XL/1450XL computers used a Finite State Machine to enable World English Spelling text-to-speech synthesis. Unfortunately, the 1400XL/1450XL personal computers never shipped in quantity.
The Atari ST computers were sold with "stspeech.tos" on floppy disk.
Apple.
The first speech system integrated into an operating system that shipped in quantity was Apple Computer's MacInTalk. The software was licensed from 3rd party developers Joseph Katz and Mark Barton (later, SoftVoice, Inc.) and was featured during the 1984 introduction of the Macintosh computer. This January demo required 512 kilobytes of RAM memory. As a result, it could not run in the 128 kilobytes of RAM the first Mac actually shipped with. So, the demo was accomplished with a prototype 512k Mac, although those in attendance were not told of this and the synthesis demo created considerable excitement for the Macintosh. In the early 1990s Apple expanded its capabilities offering system wide text-to-speech support. With the introduction of faster PowerPC-based computers they included higher quality voice sampling. Apple also introduced speech recognition into its systems which provided a fluid command set. More recently, Apple has added sample-based voices. Starting as a curiosity, the speech system of Apple Macintosh has evolved into a fully supported program, PlainTalk, for people with vision problems. VoiceOver was for the first time featured in Mac OS X Tiger (10.4). During 10.4 (Tiger) & first releases of 10.5 (Leopard) there was only one standard voice shipping with Mac OS X. Starting with 10.6 (Snow Leopard), the user can choose out of a wide range list of multiple voices. VoiceOver voices feature the taking of realistic-sounding breaths between sentences, as well as improved clarity at high read rates over PlainTalk. Mac OS X also includes say, a command-line based application that converts text to audible speech. The AppleScript Standard Additions includes a say verb that allows a script to use any of the installed voices and to control the pitch, speaking rate and modulation of the spoken text.
The Apple iOS operating system used on the iPhone, iPad and iPod Touch uses VoiceOver speech synthesis for accessibility. Some third party applications also provide speech synthesis to facilitate navigating, reading web pages or translating text.
AmigaOS.
The second operating system to feature advanced speech synthesis capabilities was AmigaOS, introduced in 1985. The voice synthesis was licensed by Commodore International from SoftVoice, Inc., who also developed the original MacinTalk text-to-speech system. It featured a complete system of voice emulation for American English, with both male and female voices and "stress" indicator markers, made possible through the Amiga's audio chipset. The synthesis system was divided into a translator library which converted unrestricted English text into a standard set of phonetic codes and a narrator device which implemented a formant model of speech generation.. AmigaOS also featured a high-level "Speak Handler", which allowed command-line users to redirect text output to speech. Speech synthesis was occasionally used in third-party programs, particularly word processors and educational software. The synthesis software remained largely unchanged from the first AmigaOS release and Commodore eventually removed speech synthesis support from AmigaOS 2.1 onward.
Despite the American English phoneme limitation, an unofficial version with multilingual speech synthesis was developed. This made use of an enhanced version of the translator library which could translate a number of languages, given a set of rules for each language.
Microsoft Windows.
Modern Windows desktop systems can use SAPI 4 and SAPI 5 components to support speech synthesis and speech recognition. SAPI 4.0 was available as an optional add-on for Windows 95 and Windows 98. Windows 2000 added Narrator, a text–to–speech utility for people who have visual impairment. Third-party programs such as CoolSpeech, Textaloud and Ultra Hal can perform various text-to-speech tasks such as reading text aloud from a specified website, email account, text document, the Windows clipboard, the user's keyboard typing, etc. Not all programs can use speech synthesis directly. Some programs can use plug-ins, extensions or add-ons to read text aloud. Third-party programs are available that can read text from the system clipboard.
Microsoft Speech Server is a server-based package for voice synthesis and recognition. It is designed for network use with web applications and call centers.
Text-to-speech systems.
Text-to-Speech (TTS) refers to the ability of computers to read text aloud. A TTS Engine converts written text to a phonemic representation, then converts the phonemic representation to waveforms that can be output as sound. TTS engines with different languages, dialects and specialized vocabularies are available through third-party publishers.
Android.
Version 1.6 of Android added support for speech synthesis (TTS).
Internet.
Currently, there are a number of applications, plugins and gadgets that can read messages directly from an e-mail client and web pages from a web browser or Google Toolbar such as Text-to-voice which is an add-on to Firefox. Some specialized software can narrate RSS-feeds. On one hand, online RSS-narrators simplify information delivery by allowing users to listen to their favourite news sources and to convert them to podcasts. On the other hand, on-line RSS-readers are available on almost any PC connected to the Internet. Users can download generated audio files to portable devices, e.g. with a help of podcast receiver, and listen to them while walking, jogging or commuting to work.
A growing field in Internet based TTS is web-based assistive technology, e.g. 'Browsealoud' from a UK company and Readspeaker. It can deliver TTS functionality to anyone (for reasons of accessibility, convenience, entertainment or information) with access to a web browser. The non-profit project was created in 2006 to provide a similar web-based TTS interface to the Wikipedia.
Other work is being done in the context of the W3C through the W3C Audio Incubator Group with the involvement of The BBC and Google Inc.
Open source.
Systems that operate on free and open source software systems including Linux are various, and include open-source programs such as the Festival Speech Synthesis System which uses diphone-based synthesis, as well as more modern and better-sounding techniques, eSpeak, which supports a broad range of languages, and gnuspeech which uses articulatory synthesis from the Free Software Foundation.
Speech synthesis markup languages.
A number of markup languages have been established for the rendition of text as speech in an XML-compliant format. The most recent is Speech Synthesis Markup Language (SSML), which became a W3C recommendation in 2004. Older speech synthesis markup languages include Java Speech Markup Language (JSML) and SABLE. Although each of these was proposed as a standard, none of them have been widely adopted.
Speech synthesis markup languages are distinguished from dialogue markup languages. VoiceXML, for example, includes tags related to speech recognition, dialogue management and touchtone dialing, in addition to text-to-speech markup.
Applications.
Speech synthesis has long been a vital assistive technology tool and its application in this area is significant and widespread. It allows environmental barriers to be removed for people with a wide range of disabilities. The longest application has been in the use of screen readers for people with visual impairment, but text-to-speech systems are now commonly used by people with dyslexia and other reading difficulties as well as by pre-literate children. They are also frequently employed to aid those with severe speech impairment usually through a dedicated voice output communication aid.
Speech synthesis techniques are also used in entertainment productions such as games and animations. In 2007, Animo Limited announced the development of a software application package based on its speech synthesis software FineSpeech, explicitly geared towards customers in the entertainment industries, able to generate narration and lines of dialogue according to user specifications. The application reached maturity in 2008, when NEC Biglobe announced a web service that allows users to create phrases from the voices of characters.
In recent years, Text to Speech for disability and handicapped communication aids have become widely deployed in Mass Transit. Text to Speech is also finding new applications outside the disability market. For example, speech synthesis, combined with speech recognition, allows for interaction with mobile devices via natural language processing interfaces.
Text-to speech is also used in second language acquisition. Voki, for instance, is an educational tool created by Oddcast that allows users to create their own talking avatar, using different accents. They can be emailed, embedded on websites or shared on social media.
In addition, speech synthesis is a valuable computational aid for the analysis and assessment of speech disorders. A voice quality synthesizer, developed by Jorge C. Lucero et al. at University of Brasilia, simulates the physics of phonation and includes models of vocal frequency jitter and tremor, airflow noise and laryngeal asymmetries. The synthesizer has been used to mimic the timbre of dysphonic speakers with controlled levels of roughness, breathiness and strain.
APIs.
Multiple companies offer TTS APIs to their customers to accelerate development of new applications utilizing TTS technology. Companies offering TTS APIs include AT&T, CereProc, DIOTEK, IVONA, Neospeech, Readspeaker, SYNVO and YAKiToMe!. For mobile app development, Android operating system has been offering text to speech API for a long time. Most recently, with iOS7, Apple started offering an API for text to speech.

</doc>
<doc id="42800" url="https://en.wikipedia.org/wiki?curid=42800" title="374">
374

__NOTOC__
Year 374 (CCCLXXIV) was a common year starting on Wednesday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Augustus and Equitius (or, less frequently, year 1127 "Ab urbe condita"). The denomination 374 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="42801" url="https://en.wikipedia.org/wiki?curid=42801" title="Nutella">
Nutella

Nutella (; ) is the brand name of a sweetened hazelnut cocoa spread. Manufactured by the Italian company Ferrero, it was introduced to the market in 1964.
History.
Pietro Ferrero, who owned a bakery in Alba, Piedmont, an area known for the production of hazelnuts, sold an initial batch of of ""Pasta Gianduja"" in 1946. At the time, there was very little chocolate because cocoa was in short supply due to World War II rationing. So Ferrero used hazelnuts, which are plentiful in the Piedmont region of Italy (northwest), to extend the chocolate supply. This ""Pasta Gianduja"" was originally a solid block, but Ferrero started to sell a creamy version in 1951 as ""Supercrema"".
In 1963, Ferrero's son Michele Ferrero revamped "Supercrema" with the intention of marketing it throughout Europe. Its composition was modified and it was renamed "Nutella". The first jar of Nutella left the Ferrero factory in Alba on 20 April 1964. The product was an instant success and remains widely popular.
In 2012, French senator Yves Daudigny proposed a tax increase on palm oil from €100 to €400 per metric tonne. At 20 percent, palm oil is one of Nutella's main ingredients and the tax was dubbed "the Nutella tax" in the media.
World Nutella Day is February 5.
On 14 May 2014, Poste italiane issued a 50th anniversary Nutella commemorative stamp. The 70 Euro cent stamp was designed by Istituto Poligrafico e Zecca dello Stato and features a jar of Nutella on a golden background. Ferrero held a Nutella Day on 17 and 18 May to celebrate the anniversary.
Ingredients.
The main ingredients of Nutella are sugar, Palm oil and hazelnut followed by cocoa solids and skimmed milk. In the United States, Nutella contains soy products. Nutella is marketed as "hazelnut cream" in many countries. Under Italian law, it cannot be labeled as a "chocolate cream", as it does not meet minimum cocoa solids concentration criteria. Ferrero consumes 25 percent of the global supply of hazelnuts.
The traditional Piedmont recipe, Gianduja, was a mixture containing approximately 71.5% hazelnut paste and 19.5% chocolate. It was developed in Piedmont, Italy after taxes on cocoa beans hindered the manufacture and distribution of conventional chocolate.
Production.
Nutella is produced in various facilities. In the North American market, it is produced at a plant in Brantford, Ontario in Canada and most recently in San José Iturbide, Guanajuato, Mexico.
For Australia and New Zealand, Nutella has been manufactured in Lithgow, New South Wales since the late 1970s.
Two of the four Ferrero plants in Italy produce Nutella, in Alba, Piedmont, and in Sant'Angelo dei Lombardi in Campania. In France, a production facility is located in Villers-Écalles. For Eastern Europe (including Southeast Europe, Poland, Turkey, Czech Republic and Slovakia) and South Africa, it is produced in Warsaw and Manisa. For Germany and northern Europe, Nutella is produced at the Ferrero plant in Stadtallendorf, which has been in existence since 1956.
Ferrero also has a plant in Brazil, which supplies the Brazilian market, with part of the production being exported overseas.
Processing.
Nutella is a form of a chocolate spread. Therefore, the production process for this food item is very similar to a generic production of chocolate spread. Nutella is made from sugar, modified palm oil, hazelnuts, cocoa, skimmed milk powder, whey powder, lecithin, and vanillin.
The process of making chocolate spread begins with the extraction of cocoa powder from the cocoa bean. These cocoa beans are harvested from cocoa trees and are left to dry for about ten days before being shipped for processing. Typically cocoa beans contain approximately 50 percent of cocoa butter; therefore, they must be roasted to reduce the cocoa bean into a liquid form. This step is not sufficient for turning cocoa bean into a chocolate paste because it solidifies at room temperature, and would not be spreadable. After the initial roast, the liquid paste is sent to presses, which are used to squeeze the butter out of the cocoa bean. The final products are round discs of chocolate made of pure compressed cocoa. The cocoa butter is transferred elsewhere so it can be used in other products.
The second process involves the hazelnuts. Once the hazelnuts have arrived at the processing plant, a quality control is issued to inspect the nuts so they are suitable for processing. A guillotine is used to chop the nuts to inspect the interior. After this process, the hazelnuts are cleaned and roasted. A second quality control is issued by a computer-controlled blast of air, which removes the bad nuts from the batch. This ensures that each jar of Nutella is uniform in its look and taste. Approximately 50 hazelnuts can be found in each jar of Nutella, as claimed by the company.
The cocoa powder is then mixed with the hazelnuts along with sugar, vanillin and skim milk in a large tank until it becomes a paste-like spread. Modified palm oil is then added to help retain the solid phase of the Nutella at room temperature, which substitutes for the butter found in the cocoa bean. In addition, whey powder is added to the mix because it acts as a binder for the paste. Whey powder is an additive commonly used in spreads to prevent the coagulation of the product because it stabilizes the fat emulsions. Similarly to whey powder lecithin, which is a form of a fatty substance found in animal and plant tissues, is used to emulsify as it promotes homogenized mixing of the different ingredients allowing the paste to become spreadable. It also aids the lipophilic properties of the cocoa powder which, again, keeps the product from separating. Vanillin is added to enhance the sweetness of the chocolate. The finished product is then packaged.
Nutrition.
Nutella contains 10.5 percent of saturated fat and 58% of processed sugar by weight. A two-tablespoon (37 gram) serving of Nutella contains 200 calories including 99 calories from 11 grams of fat (3.5g of which are saturated) and 80 calories from 21 grams of sugar. The spread also contains 15 mg of sodium and 2g of protein per serving (for reference a Canadian serving size is 1 tablespoon or 19 grams).
Storage.
Despite refrigeration slowing the growth of bacteria in these kinds of products, it is stated on the label that nutella does not need to be refrigerated. This is because the large quantity of sugar in the product acts as a preservative to prevent the growth of microorganisms. More specifically the sugar acts as a preservative by binding the water in the product, which prevents the microorganisms from growing. In fact, refrigeration causes nutella to harden as it contains fats from the hazelnuts. When nut fats are placed in cold temperatures they become too hard to spread. Hazelnuts contain almost 91 percent monounsaturated fat, which are known to be liquid at room temperature and solidify at refrigerator temperatures. Room temperature allow the product to have a smooth and spreadable consistency because the monounsaturated oils from the hazelnut are liquid at this state. In addition, the palm oil used in nutella does not require refrigeration because it contains high amounts of saturated fat and resists becoming rancid. Therefore, nutella can be stored at room temperature in the cabinet without going rancid before the best before date. The remaining ingredients in nutella such as cocoa, skimmed milk powder, soy lecithin, and vanillin also do not require refrigeration. Hence, nutella does not need to be put in the refrigerator after opening.
Class action lawsuit.
In the United States, Ferrero was sued in a class action for false advertising leading to consumer inferences that Nutella has nutritional and health benefits from advertising claims that Nutella is 'part of a nutritious breakfast'.
In April 2012, Ferrero agreed to pay a $3 million settlement (up to $4 per jar for up to five jars in returns by customers). The settlement also required Ferrero to make changes to Nutella's labeling and marketing, including television commercials and their website.

</doc>
<doc id="42802" url="https://en.wikipedia.org/wiki?curid=42802" title="Sophie B. Hawkins">
Sophie B. Hawkins

Sophie Ballantine Hawkins (born November 1, 1964) is an American singer, songwriter, musician and painter. Her highest-charting singles are "Damn I Wish I Was Your Lover," "Right Beside You," and "As I Lay Me Down."
Career.
Hawkins's debut album, "Tongues and Tails", was released in 1992. It achieved both worldwide commercial success and critical acclaim, earning her a Grammy nomination for Best New Artist in 1993. The single "Damn I Wish I Was Your Lover" went to #5 on the "Billboard" Hot 100 singles chart in the USA and was also a Top 20 hit in the UK. Hawkins was asked to perform Bob Dylan's "I Want You," which she covered on "Tongues and Tails", for the 1992 Madison Square Garden concert honoring Dylan's 30th anniversary as a musician (later released as "The 30th Anniversary Concert Celebration").
"Whaler", her second album, was released in 1994. Produced by Stephen Lipson, it also contained a US top 10 hit, "As I Lay Me Down", and was certified gold. Three singles from the album made the UK Top 40, including "Right Beside You" which peaked at #13 (and reached number 2 on the Dutch Top 40 singles chart.).
A 1998 documentary by Gigi Gaston, titled "The Cream Will Rise", followed Hawkins during one of her tours and captured her struggle to deal with past troubles with her family, including her mother and brother. Music and riffs by Hawkins were included throughout the film.
Also in 1998, Hawkins's record company at the time, Sony Music, delayed the release of her third album. Its executives were unhappy with the finished product and wanted Hawkins to rework some of the material. In particular, they insisted that Hawkins remove a banjo track from one of the songs. Hawkins refused to accommodate them, citing artistic integrity as her main reason. After a lengthy battle between Hawkins and the company, the album, "Timbre", was eventually released in 1999, though Sony declined to promote it. Hawkins subsequently left the label and founded her own label, Trumpet Swan Productions. In 2001, "Timbre" was re-released on Hawkins's label, now as a two-disc set that contained new songs, demos, remixes, and videos. Her first independently recorded and released album, "Wilderness", was released in 2004.
In 2012, Hawkins starred as Janis Joplin in the play, "Room 105" which was written and directed by her longtime girlfriend and manager, Gigi Gaston. After another long hiatus she released her fifth album of all new material in 2012, titled "The Crossing".
On April 4, 2013, Sophie appeared on the TV series "Community" as herself, performing "Damn I Wish I Was Your Lover" and "As I Lay Me Down" during the community college's "Sophie Hawkins Dance" (so named because students confused the singer with Sadie Hawkins).
Personal life.
In an interview with "Rock Cellar Magazine" in 2012, Hawkins said she identifies as omnisexual. Although there were rumors she had dated Martina Navratilova and Jodie Foster, she denied those, saying "I've never met any of the women I'm supposed to have had affairs with." 
In 1994, Hawkins posed nude for "Interview Magazine." As she explained it to Ed Rampell when he interviewed her for "Q Magazine," she met the photographer, Bruce Weber, and was asked if she could do a photo shoot with him. She had her own clothes when she showed up to the photo session, but he had a dress he wanted her to try on. She did not think it looked very good on her. It got to the point where she was only wearing a coat, and Weber suggested she remove that too. By this stage, she was not even thinking about how she looked as she felt quite comfortable with him. Weber later told her that giving her the unflattering dress was part of his plan to get her naked.
On November 18, 2008, she gave birth to a son, Dashiell Gaston Hawkins. He was named in part for Hawkins's longtime partner and manager, Gigi Gaston, who had directed the documentary "The Cream Will Rise."
On July 7, 2015, at age 50, Hawkins gave birth to her second child, Esther Ballantine Hawkins. She had conceived the child 20 years prior as Hawkins had 15 embryos frozen.
Politics.
In August 2007, Hawkins headlined the first Los Angeles Women's Music Festival in support of its dual agenda of supporting animal rescue groups and promoting and supporting female musicians. Hawkins is a vegan and a long-time supporter of animal rights.
In February 2008, Hawkins re-recorded her hit "Damn I Wish I Was Your Lover" as "Damn, We Wish You Were President" in support of presidential candidate Hillary Clinton. Hawkins also wrote in her blog, "Hillary Clinton's achievements come from her heart. She has initiated so much positive change for families, children, victims of crime and the environment in her struggle for the forward movement of America and the working people of this nation."
In May 2010, Hawkins began supporting Waterkeeper Alliance, an organization of on-the-water advocates who patrol and protect more than 100,000 miles of rivers, streams and coastlines in North and South America, Europe, Australia, Asia and Africa. She donated 100% of the proceeds of her single "The Land, the Sea, and the Sky" to the organization.
In February 2011, Hawkins performed at the Big Gay Party event staged by GOProud, an organization of gay conservatives, as part of the year's Conservative Political Action Conference festivities. In an after-show interview in the reason.tv documentary "Liberal in Bed, Conservative in the Head: Sophie B. Hawkins", Hawkins gave her views on issues such as gun ownership, the free market, limited government and identity politics.

</doc>
<doc id="42803" url="https://en.wikipedia.org/wiki?curid=42803" title="Video CD">
Video CD

Video CD (abbreviated as VCD, and also known as Compact Disc digital video) is a home video format and the first format for distributing films on standard optical discs. The format was widely adopted in Southeast Asia instead of VHS and Betamax systems.
The format is a standard digital format for storing video on a compact disc. VCDs are playable in dedicated VCD players, most DVD and Blu-ray Disc players, personal computers, and some video game consoles.
The Video CD standard was created in 1993
by Sony, Philips, Matsushita, and JVC and is referred to as the White Book standard.
Although they have been superseded by other mediums, VCDs continue to be retailed as a low-cost video format.
Brief history.
In 1979, Philips introduced the optical LaserDisc, which was about in diameter. This disc could hold an hour of analog video along with digital audio on each side. The Laserdisc provided picture quality nearly double that of VHS tape and audio quality far superior to VHS.
Philips later teamed up with Sony to develop a new type of disc, the compact disc or CD. Introduced in 1982 in Japan (1983 in the U.S.), the CD is about in diameter, and is single-sided. The format was initially designed to store digitized sound and proved to be a success in the music industry.
A few years later, Philips decided to give CDs the ability to produce video, just like its Laserdisc counterpart. This led to the creation of CD Video (CD-V) in 1987. However, the disc's small size significantly impeded the ability to store analog video; thus only 5 minutes of picture information could fit on the disc's surface (despite the fact that the audio was digital). Therefore, CD-V distribution was limited to featuring music videos.
By the early 1990s engineers were able to digitize and compress video signals, greatly improving storage efficiency. Because this new format could hold 83 minutes of audio and video, releasing movies on compact discs finally became a reality. Extra capacity was obtained by sacrificing the error correction (it was believed that minor errors in the datastream would go unnoticed by the viewer). This format was named Video CD or VCD.
VCD enjoyed a brief period of success, with a few major feature films being released in the format (usually as a 2 disc set). However the introduction of the CD-R disc and associated recorders stopped the release of feature films in their tracks because the VCD format had no means of preventing unauthorized (and perfect) copies from being made. However, VCDs are still being released in several countries in Asia, but they recently had means of copy-protection.
The development of more sophisticated, higher capacity optical disc formats yielded the DVD format, released only a few years later with a copy protection mechanism. DVD players use lasers that are of shorter wavelength than those used on CDs, allowing the recorded pits to be smaller, so that more information can be stored. The DVD was so successful that it eventually pushed VHS out of the video market once suitable recorders became widely available. Nevertheless, VCDs made considerable inroads into developing nations, where they are still in use today.
Technical specifications.
Structure.
Video CDs comply with the CD-i Bridge format, and are authored using tracks in CD-ROM XA mode. The first track of a VCD is in CD-ROM XA Mode 2 Form 1, and stores metadata and menu information inside an ISO 9660 filesystem. This track may also contain other non-essential files, and is shown by operating systems when loading the disc. This track can be absent from a VCD, which would still work but would not allow it to be properly displayed in computers.
The rest of the tracks are usually in CD-ROM XA Mode 2 Form 2 and contain video and audio multiplexed in an MPEG program stream (MPEG-PS) container, but CD audio tracks are also allowed. Using Mode 2 Form 2 allows roughly 800 megabytes of VCD data to be stored on one 80 minute CD (versus 700 megabytes when using CD-ROM Mode 1). This is achieved by sacrificing the error correction redundancy present in Mode 1. It was considered that small errors in the video and audio stream pass largely unnoticed. This, combined with the net bitrate of VCD video and audio, means that almost exactly 80 minutes of VCD content can be stored on an 80-minute CD, 74 minutes of VCD content on a 74-minute CD, and so on. This was done in part to ensure compatibility with existing CD drive technology, specifically the earliest "1x" speed CD drives.
Video.
Video specifications
Although many DVD video players support playback of VCDs, VCD video is only compatible with the DVD-Video standard if encoded at 29.97 frames per second or 25 frames per second. 
The 352x240 and 352x288 (or SIF) resolutions were chosen because it is half the horizontal and vertical resolution of NTSC video, and half the horizontal resolution of PAL (the vertical resolution of PAL already being half of the 576 active lines). This is approximately half the resolution of an analog VHS tape which is ~330 horizontal and 480 vertical (NTSC) or 330x576 (PAL).
Audio.
Audio specifications
As with most CD-based formats, VCD audio is incompatible with the DVD-Video standard due to a difference in sampling frequency; DVDs require 48 kHz, whereas VCDs use 44.1 kHz.
Advantages of compression.
By compressing both the video and audio streams, a VCD is able to hold 74 minutes of picture and sound information, nearly the same duration as a standard 74 minute audio CD. The MPEG-1 compression used records mostly the differences between successive video frames, rather than write out each frame individually. Similarly, the audio frequency range is limited to those sounds most clearly heard by the human ear.
Other features.
The VCD standard also features the option of DVD-quality still images/slide shows with audio, at resolutions of 704x480 (NTSC) or 704x576 (PAL/SECAM). Version 2.0 also adds the playback control (PBC), featuring a simple menu like DVD-Video.
Internal Control.
An example of the software control chart (taken from "Flower And Snake" disc 1 of 3) including menu commands found in the configuration volume as "CDI_VCD.CFG"
Similar formats.
CD-i Digital Video.
Shortly before the advent of White Book VCD, Philips started releasing movies in the Green Book CD-i format, calling the subformat CD-i Digital Video (CD-i DV). While these used a similar format (MPEG-1), due to minor differences between the standards these discs are not compatible with VCD players. Philips' CD-i players with the Full Motion Video MPEG-1 decoder cartridge would play both formats. Only a few CD-i DV titles were released before the company switched to the current VCD format for publishing movies.
XVCD.
XVCD (eXtended Video CD) is the name generally given to any format that stores MPEG-1 video on a compact disc in CD-ROM XA Mode 2 Form 2, but does not strictly follow the VCD standard in terms of the encoding of the video or audio.
A normal VCD is encoded to MPEG-1 at a constant bit rate (CBR), so all scenes are required to use exactly the same data rate, regardless of complexity. However, video on an XVCD is typically encoded at a variable bit rate (VBR), so complex scenes can use a much higher data rate for a short time, while simpler scenes will use lower data rates. Some XVCDs use lower bitrates in order to fit longer videos onto the disc, while others use higher bitrates to improve quality. MPEG-2 may be used instead of MPEG-1.
To further reduce the data rate without significantly reducing quality, the size of the GOP can be increased, a different MPEG-1 quantization matrix can be used, the maximum data rate can be exceeded, and the bit rate of the MP2 audio can be reduced (or even the use of MP3 audio instead of MP2 audio). These changes can be advantageous for those who want to either maximize video quality, or use fewer discs.
KVCD.
KVCD (K Video Compression Dynamics) is an XVCD variant that requires the use of a proprietary quantization matrix, available for non-commercial use. KVCD is notable because the specification recommends a non-standard resolution of 528x480 or 528x576. KVCDs encoded at this resolution are only playable by computers with CD-ROM drives, and a small number of DVD players.
DVCD.
DVCD or Double VCD is a method to accommodate longer videos on a CD. A non-standard CD is overburned to include up to 100 minutes of video. However, some CD-ROM drives and players have problems reading these CDs, mostly because the groove spacing is outside specifications and the player's laser servo is unable to track it.
DVI.
DVI (Digital Video Interactive) is a compression technique that stored 72 minutes of video on a CD-ROM. In 1998, Intel acquired the technology from RCA's Sarnoff Research Labs. DVI never caught on.
SVCD.
Super Video CD is a format intended to be the successor of VCD, offering better quality of image and sound.
Adoption.
In North America.
The advent of recordable CDs, inexpensive recorders, and compatible DVD players spurred VCD acceptance in the US in the late 1990s and early 2000s. However, DVD burners and DVD-Video recorders were available by that time, and equipment and media costs for making DVD-Video fell rapidly. DVD-Video, with its longer run time and much higher quality, quickly overshadowed VCD in areas that could afford it. In addition many early DVD players could not read recordable (CD-R) media, and this limited the compatibility of home-made VCDs. Almost every modern stand-alone DVD-Video player can play VCDs burned on recordable media.
In Asia.
The VCD format was very popular throughout Asia
(except Japan and South Korea) in the late 1990s through the 2000s, with 8 million VCD players sold in China in 1997 alone,
and more than half of all Chinese households owning at least one VCD player by 2005. However, popularity has declined over the years, as the number of Hong Kong factories that produced VCDs dropped from 98 in 1999 to 26 in 2012.
This popularity is, in part, because most households did not already own VHS players when VCDs were introduced, the low price of the players, their tolerance of high humidity (a notable problem for VCRs), easy storage and maintenance, and the lower-cost media. Western sources have cited unauthorized content as a principal incentive for VCD player ownership.
VCDs are often produced and sold in Asian countries and regions, such as Mainland China, Taiwan, Hong Kong, Singapore, Malaysia, Thailand, Burma, Indonesia, Philippines, Vietnam, India, Pakistan and Bangladesh. In many Asian countries, major Hollywood studios (and Asian home video distributors) have licensed companies to officially produce and distribute the VCDs, such as MCA Home Video in Pakistan, Intercontinental Video Ltd. of Hong Kong, Sunny Video in Malaysia, Vision in Indonesia, CVD International and Pacific Marketing and Entertainment Group in Thailand, Excel Home Video in India, Berjaya-HVN and InnoForm Media in both Malaysia and Singapore, Scorpio East Entertainment in Singapore, as well as VIVA Video, Magnavision Home Video, and C-Interactive Digital Entertainment in the Philippines. Legal Video CDs can often be found in established video stores and major book outlets in most Asian countries. They are typically packaged in jewel cases like commercial CDs, though higher-profile films may be released in keep cases. The consumer should always check for the VCD or DVD logo so as to avoid purchasing the wrong format.
In Asia, the use of VCDs as carriers for karaoke music is very common. One channel would feature a mono track with music and singing, another channel a pure instrumental version for karaoke singing. Prior to this, karaoke music was carried on laserdiscs.
Worldwide trends.
VCD's growth has slowed in areas that can afford DVD-Video, which offers most of the same advantages, as well as better picture quality
(higher resolution with fewer digital compression artifacts) due to its larger storage capacity. However, VCD has simultaneously seen significant new growth in emerging economies like India, Indonesia, South America and Africa as a low-cost alternative to DVD. As of 2004, the worldwide popularity of VCD was increasing.
Compared with VHS.
Overall picture quality is intended to be comparable to VHS video.
Poorly compressed VCD video can sometimes be of lower quality than VHS video, for example exhibiting VCD block artifacts (rather than the analog noise seen in VHS sources), but does not deteriorate further with each use. While both formats need fast-forwarding to find certain scenes, rewinding to the beginning upon reaching the end is not required in VCD. The resolution is just slightly below that of common VHS resolution.
Though technically superior to tape-based mediums, VCDs have a few minor flaws. Videos in the format do not come with closed caption (on-screen text to aid viewers with hearing problems). When watching a film that exceeds 74 minutes, which is the maximum video capacity of one disc, a viewer would have to change the disc upon reaching half-way (unless the discs are played on a VCD changer that could hold multiple discs as well as playing them automatically in succession), whereas a single VHS can hold 3½ hours of continuous video (as of 2014, 10 hour VHS tapes are available).
Compared with DVD.
When playing a DVD, the viewer is brought to a main menu which gives them options (watch the feature film, view "deleted scenes", play some special applications, etc.). VCDs are usually straightforward, playing them often goes directly to the video with extras (mostly trailers and commercials) taking place before or after it.
Subtitles are found on many Asian VCDs but cannot be removed, unlike DVDs. The subtitles are embedded on the video during the encoding process. It's not uncommon to find a VCD with subtitles for two languages.
Though the VCD technology can support it, most films carried on VCDs do not contain chapters, requiring the viewer to fast-forward to resume the program after playback has been stopped. This is mostly because VCD technology is able to start playback at a chapter point but there is nothing to signal the player that the chapter has changed during a program. This can be confusing for the user as the player will indicate that it is still playing chapter 1 when it has played through to chapter 2 or later. Pressing the Next button would cause playback from the beginning of chapter 2. However, preview material is sometimes stored in a separate chapter, followed by a single chapter for the film.
VCDs are often bilingual. Because they feature stereo audio, disc players have an option to play only the left or right audio channel. For example, ERA of Hong Kong's release of the animated film "The Iron Giant" features English on the left audio channel and Cantonese on the right; more commonly Hong Kong VCDs will feature Mandarin on one channel and Cantonese on the other. This is similar to selecting a language track on a DVD, except it's limited to 2 languages, due to there being only two audio channels (left and right). The audio track effectively becomes monaural.
VCD's most noticeable disadvantage compared to DVD is image quality, due both to the more aggressive compression necessary to fit video into such a small capacity as well as the compression method used. Additionally, VCDs are available only in stereo, while DVDs are capable of six channels of discrete surround sound. The audio compression of VCDs also suffers from not being able to pull off the Haas effect for matrixed surround sound.
Hardware and software support.
Early devices supporting Video CD playback include the Philips CD-i systems and the Amiga CD-32 (albeit via an optional decoder card).
Video CDs are not popular in the US, Canada and Europe, so its support is limited among mainstream software. Windows Media Player prior to version 9 and QuickTime Player do not support playing VCD directly, though they can play the .DAT files (stored under \MPEGAV for video and audio data) reliably, and plugins were available. Windows Vista added native support of VCD along with DVD-Video and can launch the preferred application upon insertion. The disc format is also supported using Windows Media Player Classic variations and VLC Media Player both support VCDs natively.
Direct access playback support is available within Windows XP MCE, Windows Vista, Windows 7, BSD, Mac OS, Linux, and Darwin, among others, either directly or with updates and compatible software.
Disc playback is also available both natively and as an option on some CD- and DVD-based video game consoles, including PC-FX, Sega Saturn (pictured), Sega Dreamcast, and Sony PlayStation (only on the SCPH-5903 model).
Most DVD players are compatible with VCDs, and VCD-only players are available throughout Asia, and online through many shopping sites. Older Blu-ray and HD-DVD players also retained support, as do CBHD players as well. However, most current Blu-ray players and the Sony PlayStation 3 cannot play VCDs; this is because while they have backwards playback compatibility with the DVD standard, these player can not read VCD data because the player software does not have support for MPEG-1 video and audio.

</doc>
<doc id="42806" url="https://en.wikipedia.org/wiki?curid=42806" title="Cyclone">
Cyclone

In meteorology, a cyclone is a large scale air mass that rotates around strong centers of low pressure. This is usually characterized by inward spiraling winds that rotate counterclockwise in the Northern Hemisphere and clockwise in the Southern Hemisphere of the Earth. Most large-scale cyclonic circulations are centered on areas of low atmospheric pressure. The largest low-pressure systems are cold-core polar cyclones and extratropical cyclones of the largest scale (synoptic scale). According to the National Hurricane Center glossary, warm-core cyclones such as tropical cyclones and subtropical cyclones also lie within the synoptic scale.
Mesocyclones, tornadoes and dust devils lie within the smaller mesoscale. Upper level cyclones can exist without the presence of a surface low, and can pinch off from the base of the Tropical Upper Tropospheric Trough during the summer months in the Northern Hemisphere. Cyclones have also been seen on extraterrestrial planets, such as Mars and Neptune.
Cyclogenesis describes the process of cyclone formation and intensification. Extratropical cyclones form as waves in large regions of enhanced mid-latitude temperature contrasts called baroclinic zones. These zones contract to form weather fronts as the cyclonic circulation closes and intensifies. Later in their life cycle, cyclones occlude as cold core systems. A cyclone's track is guided over the course of its 2 to 6 day life cycle by the steering flow of the cancer or subtropical jet stream.
Weather fronts separate two masses of air of different densities and are associated with the most prominent meteorological phenomena. Air masses separated by a front may differ in temperature or humidity. Strong cold fronts typically feature narrow bands of thunderstorms and severe weather, and may on occasion be preceded by squall lines or dry lines. They form west of the circulation center and generally move from west to east. Warm fronts form east of the cyclone center and are usually preceded by stratiform precipitation and fog. They move poleward ahead of the cyclone path. Occluded fronts form late in the cyclone life cycle near the center of the cyclone and often wrap around the storm center.
Tropical cyclogenesis describes the process of development of tropical cyclones. Tropical cyclones form due to latent heat driven by significant thunderstorm activity, and are warm core. Cyclones can transition between extratropical, subtropical, and tropical phases under the right conditions. Mesocyclones form as warm core cyclones over land, and can lead to tornado formation. Waterspouts can also form from mesocyclones, but more often develop from environments of high instability and low vertical wind shear. In the Atlantic and the northeastern Pacific oceans, a tropical cyclone is generally referred to as a hurricane (from the name of the ancient Central American deity of wind, Huracan), in the Indian and south Pacific oceans it is called a cyclone, and in the northwestern Pacific it is called a typhoon.
Structure.
There are a number of structural characteristics common to all cyclones. A cyclone is a low-pressure area. A cyclone's center (often known in a mature tropical cyclone as the eye), is the area of lowest atmospheric pressure in the region. Near the center, the pressure gradient force (from the pressure in the center of the cyclone compared to the pressure outside the cyclone) and the force from the Coriolis effect must be in an approximate balance, or the cyclone would collapse on itself as a result of the difference in pressure.
Because of the Coriolis effect, the wind flow around a large cyclone is counterclockwise in the Northern Hemisphere and clockwise in the Southern Hemisphere. Cyclonic circulation is sometimes referred to as contra solem. In the Northern Hemisphere, the fastest winds relative to the surface of the Earth therefore occur on the eastern side of a northward-moving cyclone and on the northern side of a westward-moving one; the opposite occurs in the Southern Hemisphere. (The wind flow around an anticyclone, on the other hand, is clockwise in the northern hemisphere, and counterclockwise in the southern hemisphere.)
Formation.
Cyclogenesis is the development or strengthening of cyclonic circulation in the atmosphere (a low-pressure area). Arctic Climatology and Meteorology. Cyclogenesis is an umbrella term for several different processes, all of which result in the development of some sort of cyclone. It can occur at various scales, from the microscale to the synoptic scale.
Extratropical cyclones form as waves along weather fronts before occluding later in their life cycle as cold core cyclones.
Tropical cyclones form due to latent heat driven by significant thunderstorm activity, and are warm core. They can be extremely dangerous.
The surface low has a variety of ways of forming. Topography can force a surface low when dense low-level high-pressure system ridges in east of a north-south mountain barrier. Mesoscale convective systems can spawn surface lows which are initially warm core. The disturbance can grow into a wave-like formation along the front and the low will be positioned at the crest. Around the low, flow will become cyclonic, by definition. This rotational flow will push polar air equatorward west of the low via its trailing cold front, and warmer air with push poleward low via the warm front. Usually the cold front will move at a quicker pace than the warm front and "catch up" with it due to the slow erosion of higher density airmass located out ahead of the cyclone and the higher density airmass sweeping in behind the cyclone, usually resulting in a narrowing warm sector. At this point an occluded front forms where the warm air mass is pushed upwards into a trough of warm air aloft, which is also known as a trowal.
Tropical cyclogenesis is the technical term describing the development and strengthening of a tropical cyclone in the atmosphere. The mechanisms through which tropical cyclogenesis occurs are distinctly different from those through which mid-latitude cyclogenesis occurs. Tropical cyclogenesis involves the development of a warm-core cyclone, due to significant convection in a favorable atmospheric environment. There are six main requirements for tropical cyclogenesis: sufficiently warm sea surface temperatures, atmospheric instability, high humidity in the lower to middle levels of the troposphere, enough Coriolis force to develop a low-pressure center, a preexisting low-level focus or disturbance, and low vertical wind shear. An average of 86 tropical cyclones of tropical storm intensity form annually worldwide, with 47 reaching hurricane/typhoon strength, and 20 becoming intense tropical cyclones (at least Category 3 intensity on the Saffir–Simpson Hurricane Scale).
Synoptic scale.
The following types of cyclones are identifiable in synoptic charts.
Surface-based types.
There are three main types surface-based cyclones: Extratropical cyclones, Subtropical cyclones and Tropical cyclones
Extratropical cyclone.
An extratropical cyclone is a synoptic scale low-pressure weather system that does not have tropical characteristics, being connected with fronts and horizontal gradients in temperature and dew point otherwise known as "baroclinic zones".
The descriptor "extratropical" refers to the fact that this type of cyclone generally occurs outside of the tropics, in the middle latitudes of the planet. These systems may also be described as "mid-latitude cyclones" due to their area of formation, or "post-tropical cyclones" where extratropical transition has occurred, and are often described as "depressions" or "lows" by weather forecasters and the general public. These are the everyday phenomena which along with anti-cyclones, drive the weather over much of the Earth.
Although extratropical cyclones are almost always classified as baroclinic since they form along zones of temperature and dewpoint gradient within the westerlies, they can sometimes become barotropic late in their life cycle when the temperature distribution around the cyclone becomes fairly uniform with radius. An extratropical cyclone can transform into a subtropical storm, and from there into a tropical cyclone, if it dwells over warm waters and develops central convection, which warms its core. One intense type of extratropical cyclone that strikes during wintertime is a "nor'easter".
Polar low.
A polar low is a small-scale, short-lived atmospheric low-pressure system (depression) that is found over the ocean areas poleward of the main polar front in both the Northern and Southern Hemispheres. Polar lows are cold-core so they can be considered as a subset of extratropical cyclones. Polar lows were first identified on the meteorological satellite imagery that became available in the 1960s, which revealed many small-scale cloud vortices at high latitudes. The most active polar lows are found over certain ice-free maritime areas in or near the Arctic during the winter, such as the Norwegian Sea, Barents Sea, Labrador Sea and Gulf of Alaska. Polar lows dissipate rapidly when they make landfall. Antarctic systems tend to be weaker than their northern counterparts since the air-sea temperature differences around the continent are generally smaller . However, vigorous polar lows can be found over the Southern Ocean. During winter, when cold-core lows with temperatures in the mid-levels of the troposphere reach move over open waters, deep convection forms which allows polar low development to become possible. The systems usually have a horizontal length scale of less than and exist for no more than a couple of days. They are part of the larger class of mesoscale weather systems. Polar lows can be difficult to detect using conventional weather reports and are a hazard to high-latitude operations, such as shipping and gas and oil platforms. Polar lows have been referred to by many other terms, such as polar mesoscale vortex, Arctic hurricane, Arctic low, and cold air depression. Today the term is usually reserved for the more vigorous systems that have near-surface winds of at least 17 m/s.
Subtropical.
A subtropical cyclone is a weather system that has some characteristics of a tropical cyclone and some characteristics of an extratropical cyclone. They can form between the equator and the 50th parallel. As early as the 1950s, meteorologists were unclear whether they should be characterized as tropical cyclones or extratropical cyclones, and used terms such as quasi-tropical and semi-tropical to describe the cyclone hybrids. By 1972, the National Hurricane Center officially recognized this cyclone category. Subtropical cyclones began to receive names off the official tropical cyclone list in the Atlantic Basin in 2002. They have broad wind patterns with maximum sustained winds located farther from the center than typical tropical cyclones, and exist in areas of weak to moderate temperature gradient.
Since they form from initially extratropical cyclones which have colder temperatures aloft than normally found in the tropics, the sea surface temperatures required for their formation are lower than the tropical cyclone threshold by three degrees Celsius, or five degrees Fahrenheit, lying around 23 degrees Celsius. This means that subtropical cyclones are more likely to form outside the traditional bounds of the hurricane season. Although subtropical storms rarely have hurricane-force winds, they may become tropical in nature as their cores warm.
Tropical.
A tropical cyclone is a storm system characterized by a low-pressure center and numerous thunderstorms that produce strong winds and flooding rain. A tropical cyclone feeds on heat released when moist air rises, resulting in condensation of water vapour contained in the moist air. They are fueled by a different heat mechanism than other cyclonic windstorms such as nor'easters, European windstorms, and polar lows, leading to their classification as "warm core" storm systems.
The term "tropical" refers to both the geographic origin of these systems, which form almost exclusively in tropical regions of the globe, and their formation in Maritime Tropical air masses. The term "cyclone" refers to such storms' cyclonic nature, with counterclockwise rotation in the Northern Hemisphere and clockwise rotation in the Southern Hemisphere. Depending on their location and strength, tropical cyclones are referred to by other names, such as hurricane, typhoon, tropical storm, cyclonic storm, tropical depression, or simply as a cyclone.
While tropical cyclones can produce extremely powerful winds and torrential rain, they are also able to produce high waves and damaging storm surge. They develop over large bodies of warm water, and lose their strength if they move over land. This is the reason coastal regions can receive significant damage from a tropical cyclone, while inland regions are relatively safe from receiving strong winds. Heavy rains, however, can produce significant flooding inland, and storm surges can produce extensive coastal flooding up to from the coastline. Although their effects on human populations can be devastating, tropical cyclones can also relieve drought conditions. They also carry heat and energy away from the tropics and transport it toward temperate latitudes, which makes them an important part of the global atmospheric circulation mechanism. As a result, tropical cyclones help to maintain equilibrium in the Earth's troposphere.
Many tropical cyclones develop when the atmospheric conditions around a weak disturbance in the atmosphere are favorable. Others form when other types of cyclones acquire tropical characteristics. Tropical systems are then moved by steering winds in the troposphere; if the conditions remain favorable, the tropical disturbance intensifies, and can even develop an eye. On the other end of the spectrum, if the conditions around the system deteriorate or the tropical cyclone makes landfall, the system weakens and eventually dissipates. A tropical cyclone can become extratropical as it moves toward higher latitudes if its energy source changes from heat released by condensation to differences in temperature between air masses; From an operational standpoint, a tropical cyclone is usually not considered to become subtropical during its extratropical transition.
Upper level types.
Polar cyclone.
A polar, sub-polar, or Arctic cyclone (also known as a polar vortex) is a vast area of low pressure which strengthens in the winter and weakens in the summer. A polar cyclone is a low-pressure weather system, usually spanning to , in which the air circulates in a counterclockwise direction in the northern hemisphere, and a clockwise direction in the southern hemisphere. In the Northern Hemisphere, the polar cyclone has two centers on average. One center lies near Baffin Island and the other over northeast Siberia. In the southern hemisphere, it tends to be located near the edge of the Ross ice shelf near 160 west longitude. When the polar vortex is strong, westerly flow descends to the Earth's surface. When the polar cyclone is weak, significant cold outbreaks occur.
TUTT cell.
Under specific circumstances, upper cold lows can break off from the base of the Tropical Upper Tropospheric Trough (TUTT), which is located mid-ocean in the Northern Hemisphere during the summer months. These upper tropospheric cyclonic vortices, also known as TUTT cells or TUTT lows, usually move slowly from east-northeast to west-southwest, and generally do not extend below 20,000 feet in altitude. A weak inverted surface trough within the trade wind is generally found underneath them, and they may also be associated with broad areas of high-level clouds. Downward development results in an increase of cumulus clouds and the appearance of a surface vortex. In rare cases, they become warm-core, resulting in the vortex becoming a tropical cyclone. Upper cyclones and upper troughs which trail tropical cyclones can cause additional outflow channels and aid in their intensification process. Developing tropical disturbances can help create or deepen upper troughs or upper lows in their wake due to the outflow jet emanating from the developing tropical disturbance/cyclone.
Mesoscale.
The following types of cyclones are not identifiable in synoptic charts.
Mesocyclone.
A mesocyclone is a vortex of air, to in diameter (the mesoscale of meteorology), within a convective storm.
Air rises and rotates around a vertical axis, usually in the same direction as low-pressure systems in both northern and southern hemisphere. They are most often cyclonic, that is, associated with a localized low-pressure region within a supercell. Such storms can feature strong surface winds and severe hail. Mesocyclones often occur together with updrafts in supercells, where tornadoes may form. About 1700 mesocyclones form annually across the United States, but only half produce tornadoes.
Tornado.
A tornado is a violently rotating column of air that is in contact with both the surface of the earth and a cumulonimbus cloud or, in rare cases, the base of a cumulus cloud. Also referred to as twisters, a collequial term in America, or cyclones, although the word cyclone is used in meteorology, in a wider sense, to name any closed low-pressure circulation.
Dust devil.
A dust devil is a strong, well-formed, and relatively long-lived whirlwind, ranging from small (half a metre wide and a few metres tall) to large (more than 10 metres wide and more than 1000 metres tall). The primary vertical motion is upward. Dust devils are usually harmless, but can on rare occasions grow large enough to pose a threat to both people and property.
Waterspout.
A waterspout is a columnar vortex forming over water that is, in its most common form, a non-supercell tornado over water that is connected to a cumuliform cloud. While it is often weaker than most of its land counterparts, stronger versions spawned by mesocyclones do occur.
Steam devil.
A gentle vortex over calm water or wet land made visible by rising water vapour.
Fire whirl.
A fire whirl – also colloquially known as a fire devil, fire tornado, firenado, or fire twister – is a whirlwind induced by a fire and often made up of flame or ash.
Other planets.
Cyclones are not unique to Earth. Cyclonic storms are common on Jovian planets, such as the Small Dark Spot on Neptune. It is about one third the diameter of the Great Dark Spot and received the nickname "Wizard's Eye" because it looks like an eye. This appearance is caused by a white cloud in the middle of the Wizard's Eye. Mars has also exhibited cyclonic storms. Jovian storms like the Great Red Spot are usually mistakenly named as giant hurricanes or cyclonic storms. However, this is inaccurate, as the Great Red Spot is, in fact, the inverse phenomenon, an anticyclone.

</doc>
<doc id="42808" url="https://en.wikipedia.org/wiki?curid=42808" title="Lignite">
Lignite

Lignite, often referred to as brown coal, is a soft brown combustible sedimentary rock formed from naturally compressed peat. It is considered the lowest rank of coal due to its relatively low heat content. It has a carbon content around 60–70%. It is mined all around the world and is used almost exclusively as a fuel for steam-electric power generation, but is also mined for its germanium content in China. As of 2014, about 12% of Germany's energy and, specifically, 27% of Germany's electricity comes from lignite power plants, while in Greece, lignite provides about 50% of its power needs.
Characteristics.
Lignite is brownish-black in color and has a carbon content around 60–70%, a high inherent moisture content sometimes as high as 75%, and an ash content ranging from 6% to 19% compared with 6% to 12% for bituminous coal.
The energy content of lignite ranges from 10 to 20 MJ/kg (9–17 million BTU per short ton) on a moist, mineral-matter-free basis. The energy content of lignite consumed in the United States averages (13 million BTU/ton), on the as-received basis (i.e., containing both inherent moisture and mineral matter). The energy content of lignite consumed in Victoria, Australia, averages (6.5 million BTU/ton).
Lignite has a high content of volatile matter which makes it easier to convert into gas and liquid petroleum products than higher-ranking coals. Unfortunately, its high moisture content and susceptibility to spontaneous combustion can cause problems in transportation and storage. It is now known that efficient processes that remove latent moisture locked within the structure of brown coal will relegate the risk of spontaneous combustion to the same level as black coal, will transform the calorific value of brown coal to a black coal equivalent fuel, and significantly reduce the emissions profile of 'densified' brown coal to a level similar to or better than most black coals. However, removing the moisture increases the cost of the final lignite fuel.
Uses.
Because of its low energy density and typically high moisture content, brown coal is inefficient to transport and is not traded extensively on the world market compared with higher coal grades. It is often burned in power stations near the mines, such as in Australia's Latrobe Valley and Luminant's Monticello plant in Texas. Primarily because of latent high moisture content and low energy density of brown coal, carbon dioxide emissions from traditional brown-coal-fired plants are generally much higher per megawatt generated than for comparable black-coal plants, with the world's highest-emitting plant being Hazelwood Power Station, Victoria. The operation of traditional brown-coal plants, particularly in combination with strip mining, can be politically contentious due to environmental concerns. An environmentally beneficial use of lignite can be found in its use in cultivation and distribution of biological control microbes that suppress plant disease causing microbes. The carbon enriches the organic matter in the soil while the biological control microbes provide an alternative to chemical pesticides 
Reaction with quaternary amine forms a product called amine-treated lignite (ATL), which is used in drilling mud to reduce fluid loss during drilling.
Geology.
Lignite begins as an accumulation of partially decayed plant material, or peat. Burial by other sediments results in increasing temperature, depending on the local geothermal gradient and tectonic setting, and increasing pressure. This causes compaction of the material and loss of some of the water and volatile matter (primarily methane and carbon dioxide). This process, called coalification, concentrates the carbon content, and thus the heat content, of the material. Deeper burial and the passage of time result in further expulsion of moisture and volatile matter, eventually transforming the material into higher rank coals such as bituminous and anthracite coal.
Lignite deposits are typically younger than higher-ranked coals, with the majority of them having formed during the Tertiary period.
Resources.
The Latrobe Valley in the state of Victoria, Australia, contains estimated reserves of some 65 billion tonnes of brown coal. The deposit is equivalent to 25% of known world reserves. The coal seams are up to 100 metres thick, with multiple coal seams often giving virtually continuous brown coal thickness of up to 230 metres. Seams are covered by very little overburden (10 to 20 metres).
Types.
Lignite can be separated into two types. The first is xyloid lignite or fossil wood and the second form is the compact lignite or perfect lignite.
Although xyloid lignite may sometimes have the tenacity and the appearance of ordinary wood, it can be seen that the combustible woody tissue has experienced a great modification. It is reducible to a fine powder by trituration, and if submitted to the action of a weak solution of potash, it yields a considerable quantity of humic acid. Leonardite is an oxidized form of lignite, which is also contains high levels of humic acid.
Jet is a hardened, gem-like form of lignite used in various types of jewelry.

</doc>
<doc id="42809" url="https://en.wikipedia.org/wiki?curid=42809" title="423">
423

__NOTOC__
Year 423 (CDXXIII) was a common year starting on Monday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Marinianus and Asclepiodotus (or, less frequently, year 1176 "Ab urbe condita"). The denomination 423 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="42810" url="https://en.wikipedia.org/wiki?curid=42810" title="424">
424

__NOTOC__
Year 424 (CDXXIV) was a leap year starting on Tuesday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Castinus and Victor (or, less frequently, year 1177 "Ab urbe condita"). The denomination 424 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
China.
</onlyinclude>

</doc>
<doc id="42811" url="https://en.wikipedia.org/wiki?curid=42811" title="425">
425

__NOTOC__
Year 425 (CDXXV) was a common year starting on Thursday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Theodosius and Valentinianus (or, less frequently, year 1178 "Ab urbe condita"). The denomination 425 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="42812" url="https://en.wikipedia.org/wiki?curid=42812" title="428">
428

__NOTOC__
Year 428 (CDXXVIII) was a leap year starting on Sunday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Felix and Taurus (or, less frequently, year 1181 "Ab urbe condita"). The denomination 428 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="42813" url="https://en.wikipedia.org/wiki?curid=42813" title="202">
202

__NOTOC__
Year 202 (CCII) was a common year starting on Friday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Severus and Antoninus (or, less frequently, year 955 "Ab urbe condita"). The denomination 202 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="42814" url="https://en.wikipedia.org/wiki?curid=42814" title="203">
203

__NOTOC__
Year 203 (CCIII) was a common year starting on Saturday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Plautianus and Geta (or, less frequently, year 956 "Ab urbe condita"). The denomination 203 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="42821" url="https://en.wikipedia.org/wiki?curid=42821" title="La Paz">
La Paz

Nuestra Señora de La Paz (; ), commonly known as La Paz (; ), named Chuqi Yapu (Chuquiago) in Aymara, is Bolivia's third-most populous city (after Santa Cruz and El Alto), the seat of the country's government, and the capital of La Paz Department. It is located on the western side of Bolivia at an elevation of roughly above sea level.
It is, "de facto", the world's highest administrative capital. While the official capital of Bolivia (and its seat of justice) is Sucre, La Paz has more government departments.
In May 2015, La Paz was officially recognized as one of the New7Wonders Cities together with Vigan, Doha, Durban, Havana, Beirut, and Kuala Lumpur.
The city sits in a bowl surrounded by the high mountains of the altiplano. As it grew, the city of La Paz climbed the hills, resulting in varying elevations from . Overlooking the city is towering triple-peaked Illimani, which is always snow-covered and can be seen from many parts of the city, including from the neighboring city of El Alto. As of the 2008 census, the city had a population of 877,363.
La Paz Metropolitan area, formed by the cities of La Paz, El Alto, and Viacha, make the most populous urban area of Bolivia, with a population of 2.3 million inhabitants and surpassing the metropolitan area of Santa Cruz de la Sierra.
History.
Although the Spanish conquistadors entered the area in 1535, La Paz was not founded until 1548. Originally it was to be at the site of the Native American settlement, Laja, with the full name of the city being "Nuestra Señora de La Paz" (meaning "Our Lady of Peace"). The name commemorated the restoration of peace following the insurrection of Gonzalo Pizarro and fellow conquistadors four years earlier against Blasco Núñez Vela, the first viceroy of Peru. The town site was moved a few days later to its present location in the valley of Chuquiago, which is more clement.
Control over the former Inca lands had been entrusted to Pedro de la Gasca by the Spanish king (and Holy Roman Emperor) Emperor Charles V. Gasca commanded Alonso de Mendoza to found a new city commemorating the end of the civil wars in Peru; the city of La Paz was founded on October 20, 1548 by Alonzo de Mendoza with Juan de Vargas as its first mayor.
In 1549, Juan Gutierrez Paniagua was commanded to design an urban plan that would designate sites for public areas, plazas, official buildings, and a cathedral. La Plaza de los Españoles, which is known today as the Plaza Murillo, was chosen as the location for government buildings as well as the Metropolitan Cathedral.
Spain controlled La Paz with a firm grip and the Spanish king had the last word in all matters political. In 1781, for a total of six months, a group of Aymara people laid siege to La Paz. Under the leadership of Tupac Katari, they destroyed churches and government property. Thirty years later Indians laid a two-month siege on La Paz – where and when the legend of the Ekeko is set. In 1809 the struggle for independence from the Spanish rule brought uprisings against the royalist forces. It was on July 16, 1809 that Pedro Domingo Murillo famously said that the Bolivian revolution was igniting a lamp that nobody would be able to turn-off. This formally marked the beginning of the Liberation of South America from Spain. In La Paz, simultaneously with the city of Sucre, was made the first revolution against the Spanish Crown the 16 July 1809. This event is known as the Primer Grito Libertario de América.
Pedro Domingo Murillo was hanged at the Plaza de los Españoles that night, but his name would be eternally remembered in the name of the plaza, and he would be remembered as the voice of revolution across South America.
In 1825, after the decisive victory of the republicans at Ayacucho over the Spanish army in the course of the Spanish American wars of independence, the city's full name was changed to "La Paz de Ayacucho" (meaning "The Peace of Ayacucho").
In 1898, La Paz was made the "de facto" seat of the national government, with Sucre remaining the nominal historical as well as judiciary capital. This change reflected the shift of the Bolivian economy away from the largely exhausted silver mines of Potosí to the exploitation of tin near Oruro, and resulting shifts in the distribution of economic and political power among various national elites.
Geography.
Located at (−16.5, −68.1333), La Paz is built in a canyon created by the Choqueyapu River (now mostly built over), which runs northwest to southeast. The city's main thoroughfare, which roughly follows the river, changes names over its length, but the central tree-lined section running through the downtown core is called the Prado.
The geography of La Paz (in particular the altitude) reflects society: the lower areas of the city are more affluent. While many middle-class residents live in high-rise condos near the center, the houses of the truly affluent are located in the lower neighborhoods southwest of the Prado. And looking up from the center, the surrounding hills are plastered with makeshift brick houses of those less economically fortunate.
The satellite city of El Alto, in which the airport is located, is spread over a broad area to the west of the canyon, on the Altiplano. La Paz is renowned for its unique markets, very unusual topography, and traditional culture.
La Paz is located in the valleys of the Andes, and is closer to the Eastern split of the Altiplano region. Therefore, it is closer to the famous mountains such as the Illimani (guardian of La Paz), Huayna Potosi, Mururata, and Illampu. On the Western side of the Altiplano divide, about an hour to the West of the La Paz, is the site of the tallest mountain in Bolivia and 9th tallest mountain in the Andes, the Sajama Volcano. In July 1994, an earthquake rated at 8.2 struck just north of La Paz.
Climate.
At more than above sea level, higher parts of La Paz have an unusual subtropical highland climate ("Cwc", according to the Köppen climate classification), with subpolar oceanic characteristics (the average temperature of the warmest month is lower than 10 °C). The whole city has rainy summers and dry winters. Night-time temperatures range from cold to very cold. Snow flurries can occur in winter, especially at dawn and it usually melts before noon. At these high altitudes despite being located only 16 degrees from the equator, the city's average temperature is similar to that of cities such as Bergen, Norway or Tórshavn, Faroe Islands, located as far as 60 and 62 degrees from the equator respectively.
The temperatures in the central La Paz, at , and in the "Zona Sur" (Southern Zone), at above sea level, are warmer (subtropical highland climate "Cwb", according to the Köppen classification).
Owing to the altitude of the city, temperatures are consistently cool to mild throughout the year, though the diurnal temperature variation is typically large. The city has a relatively dry climate, with rainfall occurring mainly in the slightly warmer months of November to March.
At 4,012 metres, February and March, the two cloudiest months of the year, both in late summer, receive a low daily average of around 5 hours of sunshine. Conversely, June and July, the two sunniest months of the year, both in winter, receive an abundant daily average of around 8 hours of sunshine.
The seasonally uneven distribution of the year's annual precipitation often results in destructive mudslides experienced in summer, due to the excessive amount of precipitation typically observed throughout the season. At 3,250 metres, the wettest month is January with a monthly average of and the driest is July with .
The warmest temperature recorded was and the coldest was .
Colonial architecture.
The city of La Paz has a consistently decreasing volume of colonial buildings, mostly centered around the vicinity of the Plaza Murillo. Due to a lack of funds and the inability of property owners to pay for restorations to colonial buildings, many have been torn down, or are in a dilapidated state. As historic buildings are more expensive to keep, land owners find it less of a burden to construct more modern buildings as opposed to keeping the old ones. Although there has been an increasing number of projects and propositions to restore some of the city's colonial buildings, the future of these historic edifices remains uncertain.
Economy.
The economy of La Paz has improved greatly in recent years, mainly as a result of improved political stability. Due to the long period of high inflation and economic struggle faced by Bolivians in the 1980s and early 1990s, a large informal economy developed. Evidence of this is provided by the markets found all around the city. While there are stable markets, almost every street in the downtown area and surrounding neighborhoods has at least one vendor on it. La Paz remains the principal center of manufacturing enterprises that produce finished-product goods for the country, with about two-thirds of Bolivia's manufacturing located nearby. Historically, industry in Bolivia has been dominated by mineral processing and the preparation of agricultural products. However, in the urban centre of La Paz, small plants carry out a large portion of the industry. Food, tobacco products, clothing, various consumer goods, building materials, and agricultural tools are produced. "The tin quotations from London are watched in La Paz with close interest as an index of the country's prosperity; a third of the national revenue and more than half of the total customs in 1925 were derived from tin; in short, that humble but indispensable metal is the hub around which Bolivia's economic life revolves. The tin deposits of Bolivia, second largest in the world, ... invite development."
Sports.
La Paz is the home of some of the biggest football teams in Bolivia.
The city is host to several other teams that play in the first and second divisions such as:
With the exception of Deportivo Municipal and Unión Maestranza, all the other teams play the majority of their games in the city stadium, the Estadio Hernando Siles, which also hosts the national football team and international games. Always Ready frequently play at the Estadio Rafael Mendoza which belongs to The Strongest, who rarely use the stadium due to its relatively small capacity.
Education.
The city hosts some of the most important universities of the country:
Tourism.
La Paz is an important cultural center of Bolivia. The city hosts several cathedrals belonging to the colonial times, such as the San Francisco Cathedral and the Metropolitan Cathedral, this last one located on Murillo Square, which is also home of the political and administrative power of the country. Hundreds of different museums can be found across the city, the most notable ones on Jaén Street, which street design has been preserved from the Spanish days and is home of 10 different museums.
The home of the Bolivian government is located on Murillo Square and is known as "Palacio Quemado" ("Burnt Palace") as it has been on fire several times. The palace has been restored many times since, but the name has remained untouched.
Transportation.
Automobiles and public transportation are still the main means to get into the city. In March 2012, more than 1.5 million vehicles were registered. Heavy traffic is common in the city center and traffic jams occur on peak hours.
Highways.
The La Paz-El Alto Highway is a toll road that connects the city of La Paz with the neighboring city of El Alto. It is the city's main highway. It allows easy access to El Alto International Airport. The highway runs 11,7 km and crosses the city of El Alto:
The Southern District, one of La Paz's most affluent and commercial neighborhoods, is relatively separated from the rest of the city, including the CBD. The Avenida Costanera and Avenida Kantutani (Costanera and Kantutani Avenues) connect the southern district with the rest of the metropolitan area.
Air.
El Alto International Airport (IATA code: LPB) is La Paz's national and international airport and a principal hub for Línea Aérea Amaszonas and Transporte Aéreo Militar. It also serves as a focus city for Boliviana de Aviación, Bolivia's flag-carrier and largest airline. The airport is located in the city of El Alto and is () south-west of La Paz's city center. At an elevation of , it is the highest international airport and fifth highest commercial airport in the world. The runway has a length of . It is one of Bolivia's three main international gateways, along with Jorge Wilstermann International Airport and Viru Viru International Airport.
International carriers serving El Alto International Airport include American Airlines, Avianca, Avianca Ecuador, LAN Airlines, LAN Perú, Peruvian Airlines and Sky Airline, which offer direct flights from La Paz to cities such as Miami, Bogotá, Lima, Iquique, Santiago and Cusco. Though, most international traffic, including flights to Europe operates out of Viru Viru International Airport in Santa Cruz de la Sierra which is at a much lower altitude and is capable of handling larger aircraft.
Airport facilities include ATMs, cafés and restaurants, car rentals, duty-free shops, and free Wi-Fi internet. Additionally, the airport supplies travelers with oxygen for those who suffer from altitude sickness.
Bus.
La Paz Bus Station, originally a bus and train station, was built by the French architect Gustave Eiffel. It is the main gateway for inter-city buses with several daily departures to all the main Bolivian cities, and routes to Chile and Peru. The city is connected by road with the city of Oruro from where there are routes to Sucre, Potosí and the south of the country. Another highway branches off before Oruro to reach Cochabamba and Santa Cruz. Roads to the west go to Copacabana and Tiwanaku, near Lake Titicaca, and continue to Cuzco, Peru via the border town of Desaguadero. There are also roads north to get to Yungas crossing the Andes Mountains.
Departures to smaller cities and towns within the department use informal stations located in Villa Fátima (departures to Los Yungas, Beni and Pando), Upper San Pedro (for Apolo) and near the General Cemetery (for Copacabana, Lake Titicaca, or via Tiwanaku to Desaguadero on the Peruvian border).
Cable car system.
A system of urban transit aerial cable cars called Mi Teleférico ("My Cable Car") was opened in 2014. Currently three lines are in operation, and six more lines are in the planning stage. The initial three lines were built by the Austrian company Doppelmayr. The first two lines (Red and Yellow) connect La Paz with El Alto.
Water supply.
The water supply of La Paz is threatened by the impact of climate change through the melting of glaciers. The city receives its drinking water from three water systems: El Alto, Achachiucala and Pampahasi. La Paz shares the first and largest of these systems with its sister city El Alto. All three systems are fed by glaciers and rivers in the Cordillera mountain range. 20-28 % of its water is fed by glaciers, the remainder coming from rainfall and snowmelt. The glaciers recede as a result of climate change, initially increasing water availability during the dry season, but ultimately threatening a substantial decrease in dry season run-off when they completely disappear. A small glacier, the Chacaltaya near El Alto, already disappeared in 2008. The El Alto system receives its water from the Tuni Dam and two water channels. These channels divert water that flows from the Zongo Glacier on the slopes of Huayna Potosi and from Condoriri North of El Alto. The 2.9 km long Zongo glacier retreats at a rate of about 18 meters per year. The Tuni and Condoriri glaciers have lost 39% of their area between 1983 and 2006. According to a study by the Stockholm Environment Institute (SEI), the El Alto system is the least resilient against the impact of climate change among the three systems. The study says that reducing water distribution losses is the most effective short-term strategy to deal with water scarcity. New water sources further to the North in the Cordillera include the Khara Kota and Taypicacha, but they are expensive to develop and their water supply is also affected by glacier melt.
International relations.
Twin towns and sister cities.
La Paz is part of the Union of Ibero-American Capital Cities from October 12, 1982 establishing brotherly relations with the following cities:
Additionally, agreement was reached by Twin Cities with:
La Paz is also a member of Merco Ciudades, a group of 180 cities within Mercosur, since 1999.

</doc>
<doc id="42822" url="https://en.wikipedia.org/wiki?curid=42822" title="244">
244

__NOTOC__
Year 244 (CCXLIV) was a leap year starting on Monday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Armenius and Aemilianus (or, less frequently, year 997 "Ab urbe condita"). The denomination 244 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years. 
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="42825" url="https://en.wikipedia.org/wiki?curid=42825" title="246">
246

__NOTOC__
Year 246 (CCXLVI) was a common year starting on Thursday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Praesens and Albinus (or, less frequently, year 999 "Ab urbe condita"). The denomination 246 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Asia.
</onlyinclude>

</doc>
<doc id="42826" url="https://en.wikipedia.org/wiki?curid=42826" title="247">
247

__NOTOC__
Year 247 (CCXLVII) was a common year starting on Friday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Philippus and Severus (or, less frequently, year 1000 "Ab urbe condita"). The denomination 247 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Asia.
</onlyinclude>

</doc>
<doc id="42827" url="https://en.wikipedia.org/wiki?curid=42827" title="248">
248

__NOTOC__
Year 248 (CCXLVIII) was a leap year starting on Saturday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Philippus and Severus (or, less frequently, year 1001 "Ab urbe condita"). The denomination 248 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="42828" url="https://en.wikipedia.org/wiki?curid=42828" title="249">
249

__NOTOC__
Year 249 (CCXLIX) was a common year starting on Monday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Gavius and Aquilinus (or, less frequently, year 1002 "Ab urbe condita"). The denomination 249 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="42829" url="https://en.wikipedia.org/wiki?curid=42829" title="243">
243

__NOTOC__
Year 243 (CCXLIII) was a common year starting on Sunday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Arrianus and Papus (or, less frequently, year 996 "Ab urbe condita"). The denomination 243 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Roman Empire.
</onlyinclude>

</doc>
<doc id="42831" url="https://en.wikipedia.org/wiki?curid=42831" title="242">
242

__NOTOC__
Year 242 (CCXLII) was a common year starting on Saturday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Gratus and Lepidus (or, less frequently, year 995 "Ab urbe condita"). The denomination 242 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="42832" url="https://en.wikipedia.org/wiki?curid=42832" title="241">
241

__NOTOC__
Year 241 (CCXLI) was a common year starting on Friday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Gordianus and Pompeianus (or, less frequently, year 994 "Ab urbe condita"). The denomination 241 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Europe.
</onlyinclude>

</doc>
<doc id="42833" url="https://en.wikipedia.org/wiki?curid=42833" title="240">
240

__NOTOC__
Year 240 (CCXL) was a leap year starting on Wednesday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Sabinus and Venustus (or, less frequently, year 993 "Ab urbe condita"). The denomination 240 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="42834" url="https://en.wikipedia.org/wiki?curid=42834" title="Inessive case">
Inessive case

Inessive case (abbreviated ; from Latin "inesse" "to be in or at") is a locative grammatical case. This case carries the basic meaning of "in": for example, "in the house" is "talo·ssa" in Finnish, "maja·s" in Estonian, "куд·са" (kud·sa) in Moksha, "etxea·n" in Basque, "nam·e" in Lithuanian, "sāt·ā" in Latgalian and "ház·ban" in Hungarian.
In Finnish the inessive case is typically formed by adding "ssa/ssä". Estonian adds "s" to the genitive stem. In Moksha, "са" (sa) is added. In Hungarian, the suffix "ban/ben" is most commonly used for inessive case, although many others, such as "on/en/ön" and others are also used, especially with cities.
In the Finnish language, the inessive case is considered the first (in Estonian the second) of the six locative cases, which correspond to locational prepositions in English. The remaining five cases are:

</doc>
<doc id="42835" url="https://en.wikipedia.org/wiki?curid=42835" title="Elative case">
Elative case

Elative (abbreviated ; from Latin "efferre" "to bring or carry out") is a locative case with the basic meaning "out of".
Usage.
Uralic languages.
In Finnish elative is typically formed by adding "sta/stä", in Estonian by adding "-st" to the genitive stem. In Hungarian the suffix "-ból/-ből" is used for elative.
"talosta" - "out of the house, from house" (Finnish "talo" = "house")<br>
"majast" - "out of the house, from house" (Estonian "maja" = "house")<br>
"házból" - "out of house" (Hungarian "ház" = "house")
In some dialects of colloquial Finnish it is common to drop the last vowel and thus the usage of elative resembles that of Estonian, for example ""talost"".
See also.
Other locative cases are:

</doc>
<doc id="42836" url="https://en.wikipedia.org/wiki?curid=42836" title="239">
239

__NOTOC__
Year 239 (CCXXXIX) was a common year starting on Tuesday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Gordianus and Aviola (or, less frequently, year 992 "Ab urbe condita"). The denomination 239 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="42837" url="https://en.wikipedia.org/wiki?curid=42837" title="Illative case">
Illative case

Illative (abbreviated ; from Latin "illatus" "brought in") is, in the Finnish language, the Estonian language, the Lithuanian language, and the Hungarian language, the third of the locative cases with the basic meaning of "into (the inside of)". An example from Hungarian is "a házba" (into the house, with "a ház" meaning "the house"). An example from Estonian is "majasse" and "majja" (into the house), formed from "maja" (a house). An example from Finnish is "taloon" (into the house), formed from "talo" (a house).
In Finnish.
The case is formed by adding "-hVn", where 'V' represents the last vowel, and then removing the 'h' if a simple long vowel would result. For example, "talo + hVn" becomes "talohon", where the 'h' elides and produces "taloon" with a simple long 'oo'; cf. "maa + hVn" becomes "maahan", without the elision of 'h'. This unusually complex way of adding a suffix can be explained by its reconstructed origin: a voiced palatal fricative. (Modern Finnish has lost palatalization and fricatives other than 'h' or 's'.) In the dialect of Pohjanmaa, the 'h' is not removed; one says, "talohon".
The other locative cases in Finnish, Estonian and Hungarian are:
In Lithuanian.
The illative case, denoting direction of movement, is now rare in the standard language but was common in the spoken language, especially in certain dialects. Its singular form, heard more often than the plural, appears in books, newspapers, etc. Most Lithuanian nouns can take the illative ending, indicating that from the descriptive point of view the illative still can be treated as a case in Lithuanian. Since the beginning of the 20th century it isn't included in the lists of standard Lithuanian cases in most grammar books and textbooks, and the prepositional construction į+accusative is more frequently used today to denote direction. The illative case was used extensively in older Lithuanian; the first Lithuanian grammar book, by Daniel Klein, mentions both illative and į+accusative but calls the usage of the illative "more elegant". Later, it has often appeared in the written texts of the authors who grew up in Dzukija or Eastern Aukštaitija, such as Vincas Krėvė-Mickevičius.
The illative case in Lithuanian has its own endings, which are different for each declension paradigm, although quite regular, compared with some other Lithuanian cases. An ending of the illative always ends with "n" in the singular, and "sna" is the final part of an ending of the illative in the plural.
Certain fixed phrases in the standard language are illatives, such as "patraukti atsakomybėn" ("to arraign"), "dešinėn!" ("turn right").
Examples:

</doc>
<doc id="42838" url="https://en.wikipedia.org/wiki?curid=42838" title="Adessive case">
Adessive case

In Uralic languages, such as Finnish, Estonian and Hungarian, the adessive case (abbreviated ; from Latin "adesse" "to be present") is the fourth of the locative cases with the basic meaning of "on". For example, Estonian "laud" (table) and "laual" (on the table), Hungarian "asztal" and "asztalnál" (at the table). It is also used as an instrumental case in Finnish.
In Finnish, the suffix is "-lla/-llä", e.g. "pöytä" (table) and "pöydällä" (on the table). In addition, it can specify "being around the place", as in "koululla" (at the school including the schoolyard), as contrasted with the inessive "koulussa" (in the school, inside the building). 
In Estonian, the ending "-l" is added to the genitive case, e.g. "laud" (table) - "laual" (on the table). Besides the meaning "on", this case is also used to indicate ownership. For example, "mehe"l" on auto" means "the man owns a car".
As the Uralic languages don't possess the verb "to have", it is the subject in the adessive case + "on" (for example, "minulla on", "I have", literally "at me is").
The other locative cases in Finnish, Estonian and Hungarian are:
Finnish.
The Finnish adessive has the word ending -lla or -llä (according to the rules of vowel harmony). It is usually added to nouns and associated adjectives. 
It is used in the following ways.

</doc>
<doc id="42839" url="https://en.wikipedia.org/wiki?curid=42839" title="Allative case">
Allative case

Allative case (abbreviated ; from Latin "allāt-", "afferre" "to bring to") is a type of the locative cases used in several languages. The term allative is generally used for the lative case in the majority of languages which do not make finer distinctions.
Finnish.
In the Finnish language, the allative is the fifth of the locative cases, with the basic meaning of "onto". Its ending is "-lle", for example "pöytä" (table) and "pöydälle" (onto the top of the table). In addition, it is the logical complement of the adessive case for referring to "being around the place". For example, "koululle" means "to the vicinity of the school". With time, the use is the same: "ruokatunti" (lunch break) and "... lähti ruokatunnille" ("... left to the lunch break"). Some actions require the case, e.g. "kävely" - "mennä kävelylle" "a walk - go for a walk".
The other locative cases in Finnish and Estonian are these:
Baltic languages.
In the Lithuanian and Latvian languages the allative had been used dialectally as an innovation since the Proto-Indo-European, but it is almost out of use in modern times. Its ending in Lithuanian is "-op" which was shortened from "-opi", whereas its ending in Latvian is "-up". In the modern languages the remains of the allative can be found in certain fixed expressions that have become adverbs, such as Lit. "išėjo Dievop" ("gone to God", i.e. died), "velniop!" ("to hell!"), "nuteisti myriop" ("sentence to death"), "rudeniop" ("towards autumn"), "vakarop" ("towards the evening"), Lat. "mājup" ("towards home"), "kalnup" ("uphill"), "lejup" ("downhill").
Greek.
In Mycenaean Greek, a "-de" ending is used to denote an allative, when it is not being used as an enclitic, e.g. "te-qa-de", *"Tʰēgʷasde", "to Thebes" (Linear B: ). This ending survives into Ancient Greek in words such as "Athḗnaze", from accusative "Athḗnās" + "-de".
Accusative.
The Latin accusative of towns and small islands is used for motion towards, like the allative case.

</doc>
<doc id="42841" url="https://en.wikipedia.org/wiki?curid=42841" title="238">
238

__NOTOC__
Year 238 (CCXXXVIII) was a common year starting on Monday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Pius and Pontianus (or, less frequently, year 991 "Ab urbe condita"). The denomination 238 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Commerce.
</onlyinclude>

</doc>
<doc id="42842" url="https://en.wikipedia.org/wiki?curid=42842" title="Essive case">
Essive case

The essive or similaris case (abbreviated ) is one example of a grammatical case, an inflectional morphological process by which a form is altered or marked in order to indicate its grammatical function. Marking of the essive case on a noun can express it as a definite period of time during which something happens or during which a continuous action was completed. The essive case can also denote a form as a temporary location, state of being, or character in which the subject was at a given time. The latter of these meanings is often referred to as the equivalent of the English phrase "as a ___".
In the Finnish language, this case is marked by adding "-na/-nä" to the stem of the noun.
Use of the essive case for specifying times, days, and dates when something happens is also apparent in Finnish.
In Finnish, the essive case is technically categorized as an old locative case, or a case which in some way indicates spatial location. However, in the present day language, the case has lost the majority of its spatial meaning. The case instead typically denotes a state that is temporary or inclined to change.
Some fixed expressions do retain the essive in its ancient locative meaning however, e.g. "at home" is "kotona". 
When marking something that in fact cannot literally change states, the essive case can implicate the presence of alternative states or even two individual, differing "worlds". This can been seen in the following example:
The example above illustrates the process by which marking of the essive case can be seen as creating two differing ‘worlds’: one real and one illusionary. The "temporary" component of the meaning encoded by marking of the essive case on the Finnish word for "genuine" (aito) makes a distinction between the perceived state of the subject as genuine at the time of purchase and the actual state of subject as not genuine as it is perceived at present or at the time of the moment of speech.
If the inessive were used e.g. "kodissani" this would distinguish the activity from reading the papers, say, in the garage or in the garden (of the home).
In the Estonian language, this case is marked by adding "-na" to the genitive stem. Marking of this case in Estonian denotes the capacity in which the subject acts. The essive case is used for indicating "states of being", but not of "becoming", which is instead marked by either the translative case, the elative case, or the nominative case.
In the Spanish language, the essive case does serve as a locative case which encodes spatial meaning. The essive case is marked by use of the adposition "en", which translates to English as "on". The Spanish essive case and its relation to two other locative cases, the allative case (encoded by Spanish adposition "a" meaning "to") and the ablative case (encoded by Spanish adposition "de" meaning "from"), is discussed by Dein Creissels in "Space in Languages: Linguistic Systems and Cognitive Categories". Creissels asserts that Spanish is just one example of a European languages in which these three cases are distinct, as opposed to other European languages which exhibit some conflation between marking of the essive case and of the allative case. Below is an example of the adposition encoding the essive case in Spanish:

</doc>
<doc id="42843" url="https://en.wikipedia.org/wiki?curid=42843" title="237">
237

__NOTOC__
Year 237 (CCXXXVII) was a common year starting on Sunday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Perpetuus and Felix (or, less frequently, year 990 "Ab urbe condita"). The denomination 237 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="42844" url="https://en.wikipedia.org/wiki?curid=42844" title="Translative case">
Translative case

The translative case (abbreviated ) is a grammatical case that indicates a change in state of a noun, with the general sense of "becoming "X"" or "change to "X"".
In the Finnish language, this is the counterpart of the essive case, with the basic meaning of a change of state. It is also used for expressing "in (a language)", "considering it's a (status)" and "by (a time)". Its ending is "-ksi". Examples:
Examples in Estonian:
Examples in Hungarian. The ending is -vá / -vé after a vowel; assimilating to the final consonsant otherwise:

</doc>
<doc id="42845" url="https://en.wikipedia.org/wiki?curid=42845" title="Instructive case">
Instructive case

In the Finnish language, the instructive case has the basic meaning of "by means of". It is a comparatively rarely used case, though it is found in some commonly used expressions, such as "omin silmin" → "with one's own eyes".
In modern Finnish, many of its instrumental uses are being superseded by the adessive case, as in "minä matkustin junalla" → "I travelled "by train"."
It is also used with Finnish verbal second infinitives to mean "by ...ing", e.g. "lentäen" → "by flying", "by air" ("lentää" = "to fly").
In Estonian, the instructive case (Estonian: viisiütlev) also exists, but only in some words. (f.e: "jalgsi" - "on foot", from "jalg" - foot)
In Turkish, the suffix "-le" is used for this purpose. Ex: Trenle geldim "I came via train".

</doc>
<doc id="42846" url="https://en.wikipedia.org/wiki?curid=42846" title="Abessive case">
Abessive case

In linguistics, abessive (abbreviated or ), caritive and privative (abbreviated ) is the grammatical case expressing the lack or absence of the marked noun. In English, the corresponding function is expressed by the preposition "without" or by the suffix "-less."
The name "abessive" is derived from Latin "abesse" "to be away/absent", and is especially used in reference to Uralic languages. The name "caritive" is derived from Latin "carere" "to lack", and is especially used in reference to Caucasian languages. The name "privative" is derived from Latin "privare" "to deprive".
In Afro-Asiatic Languages.
Somali.
In the Somali language, the abessive case is marked by "-laa" or "-la" and dropping all but the first syllable on certain words. For example:
In Australian languages.
Martuthunira.
In Martuthunira, the privative case is formed with two suffixes, "-wirriwa" and "-wirraa". What determines which suffix is used in a given situation is unclear.
In Uralic languages.
Finnish.
In the Finnish language, the abessive case is marked by "-tta" for back vowels and "-ttä" for front vowels according to vowel harmony. For example:
An equivalent construction exists using the word "ilman" and the partitive:
or, more uncommonly:
The abessive case of nouns is rarely used in writing and even less in speech, although some abessive forms are more common than their equivalent "ilman" forms:
The abessive is, however, commonly used in nominal forms of verbs (formed with the affix "-ma-" / "-mä-"), such as "puhu-ma-tta" "without speaking", "osta-ma-tta" "without buying," "välittä-mä-ttä" "without caring:"
This form can often be replaced by using the negative form of the verb:
It is possible to occasionally hear what is considered wrong usage of the abessive in Finnish, where the abessive and "ilman" forms are combined:
There is debate as to whether this is interference from Estonian.
Estonian.
Estonian also uses the abessive, which is marked by "-ta" in both the singular and the plural:
Unlike in Finnish, the abessive is commonly used in both written and spoken Estonian.
The nominal forms of verbs are marked with the affix "-ma-" and the abessive marker "-ta":
Tallinn has a pair of bars that play on the use of the comitative and abessive, the Nimeta baar (the nameless bar) and the Nimega baar (the bar with a name).
Skolt Sami.
The abessive marker for nouns in Skolt Sámi is "-tää" in both the singular and the plural:
The abessive-like non-finite verb form (converb) is "-ǩâni" or "-kani":
Unlike in Finnish, the abessive is still commonly used in Skolt Sámi.
Inari Sami.
The abessive marker for nouns in Inari Sámi is "-táá." The corresponding non-finite verb form is "-hánnáá," "-hinnáá" or "-hennáá."
Other Sami languages.
The abessive is not used productively in the Western Sámi languages, although it may occur as a cranberry morpheme.
Hungarian.
In Hungarian, the abessive case is marked by "-talan" for back vowels and "-telen" for front vowels according to vowel harmony. Sometimes, with certain roots, the suffix becomes "-tlan" or "-tlen". For example:
There is also the postposition "nélkül," which also means without, but is not meant for physical locations.
In Turkic Languages.
Bashkir.
In Bashkir the suffix is "-һыҙ"/"-һеҙ" ("-hïð"/"-hĭð").
Turkish.
The suffix "-siz" (variations: "-sız", "-suz", "-süz") is used in Turkish.
Ex: "evsiz" ("ev" = house, houseless/homeless), "barksız", "görgüsüz" ("görgü" = good manners, ill-bred), "yurtsuz".
Azerbaijani.
The same suffix is used in the Azerbaijani language.
Chuvash.
In Chuvash the suffix is "-сĂр".
Kyrgyz.
In Kyrgyz the suffix is "-сIз".

</doc>
<doc id="42847" url="https://en.wikipedia.org/wiki?curid=42847" title="Comitative case">
Comitative case

The comitative case (abbreviated ) is a grammatical case that denotes accompaniment. In English, the preposition "with", in the sense of "in company with" or "together with", plays a substantially similar role (other uses of "with", e.g. with the meaning of "using" or "by means of" (I cut bread with a knife), correspond to the instrumental case or related cases).
Core meaning.
Comitative case encodes a relationship of "accompaniment" between two participants in an event, called the "accompanee" and the "companion." In addition, there is a "relator" (which can be of multiple lexical categories, but is most commonly an affix or adposition). Use of Comitative case gives prominence to the accompanee. For example:
In this case, "il professore" is the accompanee, "i suoi studenti" is the companion, and "con" is the relator. As the accompanee, "il professore" is the most prominent.
Animacy also plays a major role in most languages that have a Comitative case. One group of languages requires both the accompanee and the companion to be either human or animate. Another group requires both to be in the same category—that is, both human or both animate. A third group requires an animate accompanee and an inanimate companion. The remaining languages have no restrictions based on animacy.
Comparison to similar cases.
The definition of Comitative case is often conflated or confused with other similar cases, especially Instrumental case and Associative case.
The chief difference between Comitative and Instrumental is this: while Comitative relates an accompanee and a companion, Instrumental relates an agent, an object, and a patient. Enrique Palancar defines the role of Instrumental case as ‘the role played by the object the Agent manipulates to achieve a change of state of the Patient.’ Even though the difference is straightforward, Instrumental and Comitative are expressed the same way in many languages, including English, so it is often difficult to separate them.
Russian is one of many languages which differentiates morphologically between Instrumental and Comitative, so an example from Russian will help illustrate the difference.
In Russian, Comitative is marked by adding a preposition “s” and declining the companion in the Instrumental case. In the Instrumental case, the object is declined but there is no preposition added.
Comitative case is also often confused with Associative case. Before the term Comitative was applied to the accompanee-companion relationship, the relationship was often called Associative case, and some linguists still use the latter term It is important to distinguish between Comitative and Associative, though, because Associative also refers to a specific variety of Comitative used in Hungarian.
Expressions of the Comitative semantic relation.
Grammatical case is a category of inflectional morphology, thus the Comitative case is an expression of the Comitative semantic relation through inflectional affixation—that is, through prefixes, suffixes and circumfixes. Although all three major types of affixes are used in at least a few languages, suffixes are the most common expression. Languages which use affixation to express the Comitative semantic relation include Hungarian, which uses suffixes; Totonac, which uses prefixes; and Chukchi, which uses circumfixes.
Comitative relations are also commonly expressed by using adpositions—that is, prepositions, postpositions and circumpositions. Examples of languages which use adpositional constructions to express Comitative relations are French, which uses prepositions; Wayãpi, which uses postpositions; and Bambara, which uses circumpositions.
Adverbial constructions can also mark Comitative relations, although they act very similarly to adpositions. One language which uses adverbs to mark Comitative case is Latvian.
The final way in which Comitative relations can be expressed is by serial-verb constructions. In these languages, the Comitative marker is usually a verb whose basic meaning is “to follow.” A language which marks Comitative relations with serial-verb constructions is Chinese.
Examples.
Indo-European languages.
French.
French uses prepositions to express the Comitative semantic relation.
In this case, the preposition “avec” is used to express the Comitative semantic relation. The preposition “avec” is the standard Comitative marker in French; however, French has a special case called Ornative, a variety of Comitative which is used for bodily property or clothes. The French Ornative marker is “à”.
Latvian.
In Latvian, both Instrumental and Comitative are expressed with the preposition “ar” However, “ar” is only used when the companion is in accusative and singular, or when it is in dative and plural. Otherwise the coordinating conjunction “un” is used.
In the example above, “ar” is used because Rudolf, the companion, is in accusative and singular. Below, “ar” is used in the other location where it is allowable, with a dative plural companion.
Uralic languages.
Estonian.
In Estonian, the Comitative ("kaasaütlev") marker is the suffix “-ga”.
Finnish.
In Finnish, the comitative case ("komitatiivi") has the suffix "-ne" with adjectives and "-ne-" + a mandatory possessive suffix with the main noun. There is no singular-plural distinction; only the plural of the comitative is used in both singular and plural senses, thus it appears always as "-ine-". For instance, "with their big ships" is "suuri·ne laivo·i·ne·en" (big-COM ship(oblique)-PL-COM-POS 3PL), while "with his/her big ships" is "suuri·ne laivo·i·ne·nsa" ((big-COM ship(oblique)-PL-COM-POS 3SG)). It is rarely used and is mainly a feature of the formal literary language, appearing very rarely in everyday speech.
The regular "with" is expressed with the postposition "kanssa", thus this form is used in most cases, e.g. "suurien laivojensa kanssa" "with their big ships". The two forms may contrast, however, since the comitative always comes with the possessive suffix, and thus can be only used when the agent has possession of some sort over the main noun. For instance, "Ulkoministeri jatkaa kollegoineen neuvotteluja sissien kanssa", "The foreign minister, with from his colleagues, continues the negotiations with the guerrillas", has "kollegoineen" "with his colleagues" contrasted with "sissien kanssa" "with the guerrillas", the former "possessed", the latter not.
Sami languages.
As there are many Sami languages there are variations between them. In the largest Sami language, Northern Sami, the comitative case means either communion, fellowship, connection - or instrument, tool. It can be used either as an object or as an adverbial.
It is expressed through the suffix "-in" in Northern Sami, and is the same in both singular and plural.
An example of the object use in Northern Sami is "Dat láve álo riidalit isidi"in"", meaning "She always argues "with" her husband". An example of the adverbial use is "Mun čálán bleahka"in"", meaning "I write "with" ink".
Hungarian.
In Hungarian, Comitative case is marked by the suffix “-stul/-stül,” as shown in the example below.
However, the Comitative case marker cannot be used if the companion has a plural marker. So when the Comitative marker is added to a noun, it obscures whether that noun is singular or plural.
Chukchi.
Chukchi uses a circumfix to express Comitative case.
In the example, the circumfix га-ма is attached to the root мэлгар “gun” to express Comitative.
Drehu.
In Drehu, there are two prepositions which can be used to mark Comitative. Which of the prepositions is used is determined by the classes of the accompanee and companion.
Hausa.
The Comitative marker in Hausa is the preposition “dà.” In Hausa, a prepositional phrase marked for Comitative can be moved to the front of the sentence for emphasis, as shown in the examples below.
In Hausa it is ungrammatical to do the same with coordinating conjunctions. For example, if the companions were “dog and cat,” it would be ungrammatical to move either “dog” or “cat” to the front of the sentence for emphasis, while it is grammatical to do so when there is a Comitative marker rather than a conjunction.

</doc>
<doc id="42848" url="https://en.wikipedia.org/wiki?curid=42848" title="234">
234

__NOTOC__
Year 234 (CCXXXIV) was a common year starting on Wednesday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Pupienus and Sulla (or, less frequently, year 987 "Ab urbe condita"). The denomination 234 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
China.
</onlyinclude>

</doc>
<doc id="42849" url="https://en.wikipedia.org/wiki?curid=42849" title="Prolative case">
Prolative case

The prolative case (abbreviated ), also called the vialis case (abbreviated ), prosecutive case (abbreviated ), traversal case, mediative case, or translative case, is a grammatical case of a noun or pronoun that has the basic meaning of "by way of".
In the Finnish language, the so-called prolative has a restricted, by some almost fossilized meaning "by (medium of transaction)". That means, it can still be used for other words, but then does not sound 'native' / 'modern'. Some native examples are for example, "postitse" ("by post"), "puhelimitse" ("by phone"), "meritse" ("by sea"), "netitse" ("over the Internet"). The prolative forms are considered by some Finnish grammarians to be adverbs because they do not show agreement on adjectives like the other Finnish cases (also called the "concord test"). This claim is not true, however, because an adjective will agree with the prolative: "Hän hoiti asian pitkitse kirjeitse."
The prolative exists in a similar state in the Estonian language.
The vialis case in Eskimo–Aleut languages has a similar interpretation, used to express movement using a surface or way. For example, "by way of or through the house".
Basque grammars frequently list the "nortzat / nortako" case (suffix "-tzat" or "-tako") as "prolative" ("prolatiboa"). However, the meaning of this case is unrelated to the one just described above for other languages and alternatively has been called "essive / translative", It is found under this name in Tundra Nenets, in Old Basque and, with spatial nouns, in Mongolian. 

</doc>
<doc id="42850" url="https://en.wikipedia.org/wiki?curid=42850" title="233">
233

__NOTOC__
Year 233 (CCXXXIII) was a common year starting on Tuesday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Claudius and Paternus (or, less frequently, year 986 "Ab urbe condita"). The denomination 233 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Roman Empire.
</onlyinclude>

</doc>
<doc id="42851" url="https://en.wikipedia.org/wiki?curid=42851" title="232">
232

__NOTOC__
Year 232 (CCXXXII) was a leap year starting on Sunday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Lupus and Maximus (or, less frequently, year 985 "Ab urbe condita"). The denomination 232 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="42852" url="https://en.wikipedia.org/wiki?curid=42852" title="Radio frequency">
Radio frequency

Radio frequency (RF) is any of the electromagnetic wave frequencies that lie in the range extending from around to , which include those frequencies used for communications or radar signals. RF usually refers to
electrical rather than mechanical oscillations. However, mechanical RF systems do exist (see mechanical filter and RF MEMS).
Although radio "frequency" is a rate of oscillation, the term "radio frequency" or its abbreviation "RF" are used as a synonym for radio – i.e., to describe the use of wireless communication, as opposed to communication via electric wires. Examples include:
Special properties of RF current.
Electric currents that oscillate at radio frequencies have special properties not shared by direct current or alternating current of lower frequencies. 
Radio communication.
To receive radio signals an antenna must be used. However, since the antenna will pick up thousands of radio signals at a time, a radio tuner is necessary to "tune into" a particular frequency (or frequency range). This is typically done via a resonator – in its simplest form, a circuit with a capacitor and an inductor form a tuned circuit. The resonator amplifies oscillations within a particular frequency band, while reducing oscillations at other frequencies outside the band. Another method to isolate a particular radio frequency is by oversampling (which gets a wide range of frequencies) and picking out the frequencies of interest, as done in software defined radio.
The distance over which radio communications is useful depends significantly on things other than wavelength, such as transmitter power, receiver quality, type, size, and height of antenna, mode of transmission, noise, and interfering signals. Ground waves, tropospheric scatter and skywaves can all achieve greater ranges than line-of-sight propagation. The study of radio propagation allows estimates of useful range to be made.
In medicine.
Radio frequency (RF) energy, in the form of radiating waves or electrical currents, has been used in medical treatments for over 75 years, generally for minimally invasive surgeries, using radiofrequency ablation and cryoablation, including the treatment of sleep apnea. Magnetic resonance imaging (MRI) uses radio frequency waves to generate images of the human body.
Radio frequencies at non-ablation energy levels are sometimes used as a form of cosmetic treatment that can tighten skin, reduce fat (lipolysis), or promote healing.
RF diathermy is a medical treatment that uses RF induced heat as a form of physical or occupational therapy and in surgical procedures. It is commonly used for muscle relaxation. It is also a method of heating tissue electromagnetically for therapeutic purposes in medicine. Diathermy is used in physical therapy and occupational therapy to deliver moderate heat directly to pathologic lesions in the deeper tissues of the body. Surgically, the extreme heat that can be produced by diathermy may be used to destroy neoplasms, warts, and infected tissues, and to cauterize blood vessels to prevent excessive bleeding. The technique is particularly valuable in neurosurgery and surgery of the eye. Diathermy equipment typically operates in the short-wave radio frequency (range 1–100 MHz) or microwave energy (range 434–915 MHz).
Pulsed electromagnetic field therapy (PEMF) is a medical treatment that purportedly helps to heal bone tissue reported in a recent NASA study. This method usually employs electromagnetic radiation of different frequencies - ranging from static magnetic fields, through extremely low frequencies (ELF) to higher radio frequencies (RF) administered in pulses.
Effects on the human body.
Extremely low frequency RF.
High-power extremely low frequency RF with electric field levels in the low kV/m range are known to induce perceivable currents within the human body that create an annoying tingling sensation. These currents will typically flow to ground through a body contact surface such as the feet, or arc to ground where the body is well insulated.
Microwaves.
Microwave exposure at low-power levels below the Specific absorption rate set by government regulatory bodies are considered harmless non-ionizing radiation and have no effect on the human body. However, levels above the Specific absorption rate set by the U.S. Federal Communications Commission are considered potentially harmful (see Mobile phone radiation and health).
Long-term human exposure to high-levels of microwaves is recognized to cause cataracts according to experimental animal studies and epidemiological studies. The mechanism is unclear but may include changes in heat sensitive enzymes that normally protect cell proteins in the lens. Another mechanism that has been advanced is direct damage to the lens from pressure waves induced in the aqueous humor.
High-power exposure to microwave RF is known to create a range of effects from lower to higher power levels, ranging from unpleasant burning sensation on the skin and microwave auditory effect, to extreme pain at the mid-range, to physical burning and blistering of skin and internals at high power levels (see microwave burn).
General RF exposure.
The 1999 revision of Canadian Safety Code 6 recommended electric field limits of 100 kV/m for pulsed EMF to prevent air breakdown and spark discharges, mentioning rationale related to auditory effect and energy-induced unconsciousness in rats. The pulsed EMF limit was removed in later revisions, however.
For health effects see electromagnetic radiation and health.
For high-power RF exposure see radiation burn.
For low-power RF exposure see radiation-induced cancer.
As a weapon.
A heat ray is an RF harassment device that makes use of microwave radio frequencies to create an unpleasant heating effect in the upper layer of the skin. A publicly known heat ray weapon called the Active Denial System was developed by the US military as an experimental weapon to deny the enemy access to an area. A death ray is a weapon that delivers heat ray electromagnetic energy at levels that injure human tissue. The inventor of the death ray, Harry Grindell Matthews, claims to have lost sight in his left eye while developing his death ray weapon based on a primitive microwave magnetron from the 1920s (note that a typical microwave oven induces a tissue damaging cooking effect inside the oven at about 2 kV/m.)
Measurement.
Since radio frequency radiation has both an electric and a magnetic component, it is often convenient to express intensity of radiation field in terms of units specific to each component. The unit "volts per meter" (V/m) is used for the electric component, and the unit "amperes per meter" (A/m) is used for the magnetic component. One can speak of an electromagnetic field, and these units are used to provide information about the levels of electric and magnetic field strength at a measurement location.
Another commonly used unit for characterizing an RF electromagnetic field is "power density". Power density is most accurately used when the point of measurement is far enough away from the RF emitter to be located in what is referred to as the far field zone of the radiation pattern. In closer proximity to the transmitter, i.e., in the "near field" zone, the physical relationships between the electric and magnetic components of the field can be complex, and it is best to use the field strength units discussed above. Power density is measured in terms of power per unit area, for example, milliwatts per square centimeter (mW/cm²). When speaking of frequencies in the microwave range and higher, power density is usually used to express intensity since exposures that might occur would likely be in the far field zone.

</doc>
<doc id="42853" url="https://en.wikipedia.org/wiki?curid=42853" title="Partitive case">
Partitive case

The partitive case (abbreviated or more ambiguously ) is a grammatical case which denotes "partialness", "without result", or "without specific identity". It is also used in contexts where a subgroup is selected from a larger group, or with numbers.
Finnish.
In the Finnic languages, such as Finnish and Estonian, this case is often used to express unknown identities and irresultative actions. For example, it is found in the following circumstances, with the characteristic ending of "-a" or "-ta":
Where not mentioned, the accusative case would be ungrammatical. For example, the partitive must always be used after singular numerals.
As an example of the irresultative meaning of the partitive, "ammuin karhun" (accusative) means "I shot the bear (dead)", whereas "ammuin karhua" (partitive) means "I shot (at) the bear" without specifying if it was even hit. Notice that Finnish has no native future tense, so that the partitive provides an important reference to the present as opposed to the future. Thus "luen kirjaa" means "I am reading a/the book" whereas "luen kirjan" means I will read a/the book". Thus "luen" can mean "I am reading" or "I will read" depending on the case form of the word that follows. The partitive form "kirjaa" indicates incompleted action and hence the meaning of the verb form is present tense. The accusative form "kirjan" indicates completed action when used with the past tense verb but indicates planned future action when used with a verb in the present tense. Hence "luen kirjan" means "I will read the book".
The case with an unspecified identity is "onko teillä kirjoja", which uses the partitive, because it refers to unspecified books, as contrasted to nominative "onko teillä (ne) kirjat?", which means "do you have (those) books?"
The partitive case comes from the older ablative case. This meaning is preserved e.g. in "kotoa" (from home), "takaa" (from behind), where it means "from".
A Western Finnish dialectal phenomenon seen in some dialects is the assimilation of the final "-a" into a preceding vowel, thus making the chroneme the partitive marker. For example, "suurii" → "suuria" "some big --".
Sámi.
Of the Sámi languages, Inari and Skolt Sámi still have a partitive, although it is slowly disappearing and its function is being taken over by other cases.
Skolt Sámi.
The partitive is used only in the singular and can always be replaced by the genitive. The partitive marker is "-d".
1. It appears after numbers larger than 6:
This can be replaced with "kää´uc čâustõõǥǥ".
2. It is also used with certain postpositions:
This can be replaced with "kuä´đ vuâstta".
3. It can be used with the comparative to express that which is being compared:
This would nowadays more than likely be replaced by "pue´rab ko kå´ll"

</doc>
<doc id="42854" url="https://en.wikipedia.org/wiki?curid=42854" title="Absolutive case">
Absolutive case

The absolutive case (abbreviated ) is the unmarked grammatical case of a core argument of a verb (generally other than the nominative) that is used as the citation form of a noun.
In ergative–absolutive languages.
In ergative–absolutive languages, the absolutive is the case used to mark both the subject of an intransitive verb and the object of a transitive verb in addition to being used for the citation form of a noun. It contrasts with the marked ergative case, which marks the subject of a transitive verb.
For example, in Basque the noun "mutil" ("boy") takes the bare singular article "-a" both as the subject of the intransitive clause "mutila etorri da" ("the boy came") and as the object of the transitive clause "Irakasleak mutila ikusi du" ("the teacher has seen the boy") in which the subject bears the ergative ending "-a-k".
In a very few cases, a marked absolutive has been reported, including in Nias and Sochiapam Chinantec.
In nominative–absolutive languages.
In nominative–absolutive languages, also called "marked-nominative" languages, the nominative has a case inflection, and it is the accusative and citation form that are unmarked. The unmarked accusative/citation form may be called absolutive to clarify that the citation form is used for the accusative case role rather than for the nominative, as it is in most nominative–accusative languages.
In tripartite languages.
In tripartite languages, both the agent and object of a transitive clause have case forms, ergative and accusative, and the agent of an intransitive clause is the unmarked citation form. It is occasionally called the intransitive case, but "absolutive" is also used and is perhaps more accurate since it is not limited to core agents of intransitive verbs.
In nominative–accusative languages.
In nominative–accusative languages, both core cases may be marked, but often, it is only the accusative that is marked. In such situations, the term 'absolutive' could aptly describe the nominative, but the term is seldom used that way.

</doc>
<doc id="42856" url="https://en.wikipedia.org/wiki?curid=42856" title="The Bridge on the River Kwai">
The Bridge on the River Kwai

The Bridge on the River Kwai is a British-American 1957 World War II epic film directed by David Lean and starring William Holden, Jack Hawkins, Alec Guinness, and Sessue Hayakawa. Based on the novel "Le Pont de la Rivière Kwai" (1952) by Pierre Boulle, the film is a work of fiction, but borrows the construction of the Burma Railway in 1942–43 for its historical setting. The movie was filmed in Ceylon (now known as Sri Lanka). The bridge in the film was near Kitulgala.
Carl Foreman was the initial screenwriter, but Lean replaced him with Michael Wilson. Both writers had to work in secret, as they were on the Hollywood blacklist. As a result, Boulle (who did not speak English) was credited and received the Academy Award for Best Adapted Screenplay; many years later, Foreman and Wilson posthumously received the Academy Award.
The film was widely praised, winning seven Academy Awards (including Best Picture) at the 30th Academy Awards. In 1997 the film was deemed "culturally, historically, or aesthetically significant" and selected for preservation in the National Film Registry by the United States Library of Congress. It is widely considered to be one of the greatest films in history.
Plot.
In early 1943, World War II British prisoners arrive by train at a Japanese prison camp in Burma. The commandant, Colonel Saito (Sessue Hayakawa), informs them that all prisoners, regardless of rank, are to work on the construction of a railway bridge over the River Kwai that will connect Bangkok and Rangoon. The senior British officer, Lieutenant Colonel Nicholson (Alec Guinness), reminds Saito that the Geneva Conventions exempt officers from manual labour.
At the following morning's assembly, Nicholson orders his officers to remain behind when the enlisted men are sent off to work. Saito slaps him across the face with his copy of the conventions and threatens to have them shot, but Nicholson refuses to back down. When Major Clipton (James Donald), the British medical officer, intervenes, Saito leaves the officers standing all day in the intense tropical heat. That evening, the officers are placed in a punishment hut, while Nicholson is locked in an iron box.
Meanwhile, three prisoners attempt to escape. Two are shot dead, but United States Navy Commander Shears (William Holden), gets away, although badly wounded. He stumbles into a village and the villagers help him leave by boat.
Nicholson refuses to compromise. Meanwhile, the prisoners are working as little as possible and sabotaging whatever they can. Should Saito fail to meet his deadline, he would be obliged to commit ritual suicide. Desperate, Saito uses the anniversary of Japan's victory in the Russo-Japanese War as an excuse to save face and announces a general amnesty, releasing Nicholson and his officers from manual labor (although that war ended in the late summer of 1905 and it's the winter of 1943).
Nicholson conducts an inspection and is shocked by the poor job being done by his men. Over the protests of some of his officers, he orders Captain Reeves (Peter Williams) and Major Hughes (John Boxer) to design and build a proper bridge, despite its military value to the Japanese, for the sake of maintaining his men's morale. The Japanese engineers had chosen a poor site, so the original construction is abandoned and a new bridge is begun downstream.
Shears is enjoying his hospital stay in Ceylon with a beautiful nurse (Ann Sears), when British Major Warden (Jack Hawkins) informs him that the U.S. Navy has transferred him over to the British to join a commando mission to destroy the bridge before it's completed. Shears is appalled at the idea and confesses that he is not an officer, but merely had appropriated an officer's uniform prior to his capture, expecting that this revelation will invalidate the transfer order. However, Warden responds he already knew the truth and tells Shears that the American Navy's desire to avoid having to deal with his actions is the very reason they agreed to his transfer. Assured that he will be allowed to retain the privileges of being an officer and accepting that he actually has no choice, Shears relents and "volunteers" for the mission.
Meanwhile, Nicholson drives his men hard to complete the bridge on time. For him, its completion will exemplify the ingenuity and hard work of the British Army for generations. When he asks that their Japanese counterparts join in as well, a resigned Saito replies that he has already given the order.
The commandos parachute in, with one man killed on landing. Later, Warden is wounded in an encounter with a Japanese patrol and has to be carried on a litter. He, Shears, and Canadian Lieutenant Joyce (Geoffrey Horne) reach the river in time with the assistance of Siamese women bearers and their village chief, Khun Yai. Under cover of darkness, Shears and Joyce plant explosives on the bridge towers below the water line.
A train carrying soldiers and important dignitaries is scheduled to be the first to cross the bridge the following day, so Warden waits to destroy both. However, at daybreak the commandos are horrified to see that the water level has dropped, exposing the wire connecting the explosives to the detonator. Making a final inspection, Nicholson spots the wire and brings it to Saito's attention. As the train is heard approaching, they hurry down to the riverbank to investigate. The commandos are shocked that their own man is about to uncover the plot.
Joyce, manning the detonator, breaks cover and stabs Saito to death. Aghast, Nicholson yells for help, while attempting to stop Joyce from reaching the detonator. As he wrestles with Nicholson, Joyce tells Nicholson that he is a British officer under orders to destroy the bridge. When Joyce is shot dead by Japanese fire, Shears swims across the river, but is fatally wounded as he reaches Nicholson. Recognizing the dying Shears, Nicholson exclaims, "What have I done?" Warden fires his mortar, mortally wounding Nicholson. The dazed colonel stumbles towards the detonator and collapses on the plunger, just in time to blow up the bridge and send the train hurtling into the river below. Witnessing the carnage, Clipton shakes his head muttering, "Madness! ... Madness!"
Cast.
Geoffrey Horne is the last surviving primary cast member.
Historical parallels.
The largely fictional film plot is loosely based on the building in 1943 of one of the railway bridges over the Mae Klong—renamed Khwae Yai in the 1960s—at a place called Tha Ma Kham, five kilometres from the Thai town of Kanchanaburi.
According to the Commonwealth War Graves Commission:
"The notorious Burma-Siam railway, built by Commonwealth, Dutch and American prisoners of war, was a Japanese project driven by the need for improved communications to support the large Japanese army in Burma. During its construction, approximately 13,000 prisoners of war died and were buried along the railway. An estimated 80,000 to 100,000 civilians also died in the course of the project, chiefly forced labour brought from Malaya and the Dutch East Indies, or conscripted in Siam (Thailand) and Burma. Two labour forces, one based in Siam and the other in Burma, worked from opposite ends of the line towards the centre."
The incidents portrayed in the film are mostly fictional, and though it depicts bad conditions and suffering caused by the building of the Burma Railway and its bridges, historically the conditions were much worse than depicted. The real senior Allied officer at the bridge was British Lieutenant Colonel Philip Toosey. Some consider the film to be an insulting parody of Toosey. On a BBC "Timewatch" programme, a former prisoner at the camp states that it is unlikely that a man like the fictional Nicholson could have risen to the rank of lieutenant colonel, and, if he had, due to his collaboration he would have been "quietly eliminated" by the other prisoners. Julie Summers, in her book "The Colonel of Tamarkan", writes that Pierre Boulle, who had been a prisoner of war in Thailand, created the fictional Nicholson character as an amalgam of his memories of collaborating French officers. He strongly denied the claim that the book was anti British, although many involved in the film itself (including Alec Guinness) felt otherwise.
Toosey was very different from Nicholson and was certainly not a collaborator who felt obliged to work with the Japanese. Toosey in fact did as much as possible to delay the building of the bridge. While Nicholson disapproves of acts of sabotage and other deliberate attempts to delay progress, Toosey encouraged this: termites were collected in large numbers to eat the wooden structures, and the concrete was badly mixed.
In an interview that forms part of the 1969 BBC2 documentary "Return to the River Kwai" made by former POW John Coast, Boulle outlined the reasoning that led him to conceive the character of Nicholson. A transcript of the interview and the documentary as a whole can be found in the new edition of John Coast's book "Railroad of Death". Coast's documentary sought to highlight the real history behind the film (partly through getting ex-POWs to question its factual basis, for example Dr Hugh de Wardener and Lt-Col Alfred Knights), which angered many former POWs. The documentary itself was described by one newspaper reviewer when it was shown on Boxing Day 1974 ("The Bridge on the River Kwai" had been shown on BBC1 on Christmas Day 1974) as "Following the movie, this is a rerun of the antidote."
Some of the characters in the film use the names of real people who were involved in the Burma Railway. Their roles and characters, however, are fictionalised. For example, a Sergeant-Major Risaburo Saito was in real life second in command at the camp. In the film, a Colonel Saito is camp commandant. In reality, Risaburo Saito was respected by his prisoners for being comparatively merciful and fair towards them. Toosey later defended him in his war crimes trial after the war, and the two became friends.
The destruction of the bridge as depicted in the film is entirely fictional. In fact, two bridges were built: a temporary wooden bridge and a permanent steel/concrete bridge a few months later. Both bridges were used for two years, until they were destroyed by Allied bombing. The steel bridge was repaired and is still in use today.
Ernest Gordon (1916-2002), a survivor of the POW camps and railway construction described in the movie had this to say about it in "Through the valley of the Kwai", by Ernest Gordon, Harper & Row, 1962; also published c. 1980 as "Miracle on the River Kwai" and 2002 as "To End All Wars", also the loose basis of the 2002 movie "To End All Wars": 
"In Pierre Boulle's book "The Bridge over the River Kwai" and the film which was based on it, the impression was given that British officers not only took part in building the bridge willingly, but finished in record time to demonstrate to the enemy their superior efficiency. This was an entertaining story. But I am writing a factual account, and in justice to these men—living and dead—who worked on that bridge, I must make it clear that we never did so willingly. We worked at bayonet point and under bamboo lash, taking any risk to sabotage the operation whenever the opportunity arose."
Japanese views of the book and movie.
The Japanese resented the conclusion in the movie that their engineers were less capable than British engineers. In fact, Japanese engineers had been surveying the route of the railway since 1937 and were highly organized. They also disliked the "glorification of the superiority of Western civilization" represented in the movie, for example, the British being able to build a bridge that the Japanese could not.
Production.
Screenplay.
The screenwriters, Carl Foreman and Michael Wilson, were on the Hollywood blacklist and could only work on the film in secret. The two did not collaborate on the script; Wilson took over after Lean was dissatisfied with Foreman's work. The official credit was given to Pierre Boulle (who did not speak English), and the resulting Oscar for Best Screenplay (Adaptation) was awarded to him. Only in 1984 did the Academy rectify the situation by retroactively awarding the Oscar to Foreman and Wilson, posthumously in both cases. Subsequent releases of the film finally gave them proper screen credit. David Lean himself also claimed that producer Sam Spiegel cheated him out of his rightful part in the credits since he had had a major hand in the script.
The film was relatively faithful to the novel, with two major exceptions. Shears, who is a British commando officer like Warden in the novel, became an American sailor who escapes from the POW camp. Also, in the novel, the bridge is not destroyed: the train plummets into the river from a secondary charge placed by Warden, but Nicholson (never realising "what have I done?") does not fall onto the plunger, and the bridge suffers only minor damage. Boulle nonetheless enjoyed the film version though he disagreed with its climax.
Filming.
Many directors were considered for the project, among them: John Ford, William Wyler, Howard Hawks, Fred Zinnemann, and Orson Welles (who was also offered a starring role).
The film was an international co-production between companies in Britain and the United States. It is set in Thailand, but was filmed mostly near Kitulgala, Ceylon (now Sri Lanka), with a few scenes shot in England.
Director David Lean clashed with his cast members on multiple occasions, particularly Alec Guinness and James Donald, who thought the novel was anti-British. Lean had a lengthy row with Guinness over how to play the role of Nicholson; Guinness wanted to play the part with a sense of humour and sympathy, while Lean thought Nicholson should be "a bore." On another occasion, Lean and Guinness argued over the scene where Nicholson reflects on his career in the army. Lean filmed the scene from behind Guinness, and exploded in anger when Guinness asked him why he was doing this. After Guinness was done with the scene, Lean said "Now you can all fuck off and go home, you English actors. Thank God that I'm starting work tomorrow with an American actor (William Holden)."
Alec Guinness later said that he subconsciously based his walk while emerging from "the Oven" on that of his eleven-year-old son Matthew, who was recovering from polio at the time, a disease that left him temporarily paralyzed from the waist down. Guinness later reflected on the scene, calling it the "finest piece of work" he had ever done.
Lean nearly drowned when he was swept away by the river current during a break from filming.
The filming of the bridge explosion was to be done on 10 March 1957, in the presence of S.W.R.D. Bandaranaike, then Prime Minister of Ceylon, and a team of government dignitaries. However, cameraman Freddy Ford was unable to get out of the way of the explosion in time, and Lean had to stop filming. The train crashed into a generator on the other side of the bridge and was wrecked. It was repaired in time to be blown up the next morning, with Bandaranaike and his entourage present.
According to the supplemental material in the Blu-ray digipak, a thousand "tons" of explosives were used to blow up the bridge.
The producers nearly suffered a catastrophe following the filming of the bridge explosion. To ensure they captured the one-time event, multiple cameras from several angles were used. Ordinarily, the film would have been taken by boat to London, but due to the Suez crisis this was impossible; therefore the film was taken by air freight. When the shipment failed to arrive in London, a worldwide search was undertaken. To the producers' horror the film containers were found a week later on an airport tarmac in Cairo, sitting in the hot sun. Although it was not exposed to sunlight, the heat-sensitive colour film stock should have been hopelessly ruined; however, when processed the shots were perfect and appeared in the film.
Music and soundtrack.
A memorable feature of the film is the tune that is whistled by the POWs—the first strain of the march "Colonel Bogey"—when they enter the camp. The march was written in 1914 by Kenneth J. Alford, a pseudonym of British Bandmaster Frederick J. Ricketts. The Colonel Bogey strain was accompanied by a counter-melody using the same chord progressions, then continued with film composer Malcolm Arnold's own composition, "The River Kwai March," played by the off-screen orchestra taking over from the whistlers, though Arnold's march was not heard in completion on the soundtrack. Mitch Miller had a hit with a recording of both marches.
The soundtrack of the film is largely diegetic; background music is not widely used. In many tense, dramatic scenes, only the sounds of nature are used. An example of this is when commandos Warden and Joyce hunt a fleeing Japanese soldier through the jungle, desperate to prevent him from alerting other troops. Arnold won an Academy Award for the film's score.
Box office performance.
"Variety" reported that this film was the #1 moneymaker of 1958, with a US take of $18,000,000. The second highest moneymaker of 1958 was "Peyton Place" at $12,000,000; in third place was "Sayonara" at $10,500,000.
The movie was re-released in 1964 and earned an estimated $2.6 million in North American rentals.
Accolades.
Recognition.
The film has been selected for preservation in the United States National Film Registry.
Channel 4 held a poll in 2005 to find the 100 Greatest War Movies: "The Bridge on the River Kwai" came in at #10, behind "Black Hawk Down" and in front of "The Dam Busters".
The British Film Institute placed "The Bridge on the River Kwai" as the eleventh greatest British film.
First TV broadcast.
The 167-minute film was first telecast, uncut, by ABC-TV in color on the evening of 25 September 1966, as a three hours-plus ABC Movie Special. The telecast of the film lasted more than three hours because of the commercial breaks. It was still highly unusual at that time for a television network to show such a long film in one evening; most films of that length were still generally split into two parts and shown over two evenings. But the unusual move paid off for ABC—the telecast drew huge ratings. On the evenings of 28 and 29 January 1973, ABC broadcast another David Lean colour spectacular, "Lawrence of Arabia", but that broadcast was split into two parts over two evenings, due to the film's nearly four-hour length.
Restorations.
The film was restored in 1985 by Columbia Pictures. The separate dialogue, music and effects were located and remixed with newly recorded "atmospheric" sound effects. The image was restored by OCS, Freeze Frame, and Pixel Magic with George Hively editing.
On 2 November 2010 Columbia Pictures released a newly restored "The Bridge on the River Kwai" for the first time on Blu-ray. According to Columbia Pictures, they followed an all-new 4K digital restoration from the original negative with newly restored 5.1 audio. The original negative for the feature was scanned at 4k (roughly four times the resolution in High Definition), and the colour correction and digital restoration were also completed at 4k. The negative itself manifested many of the kinds of issues one would expect from a film of this vintage: torn frames, imbedded emulsion dirt, scratches through every reel, colour fading. Unique to this film, in some ways, were other issues related to poorly made optical dissolves, the original camera lens and a malfunctioning camera. These problems resulted in a number of anomalies that were very difficult to correct, like a ghosting effect in many scenes that resembles color mis-registration, and a tick-like effect with the image jumping or jerking side-to-side. These issues, running throughout the film, were addressed to a lesser extent on various previous DVD releases of the film and might not have been so obvious in standard definition.

</doc>
<doc id="42858" url="https://en.wikipedia.org/wiki?curid=42858" title="Sudetenland">
Sudetenland

The Sudetenland (Czech and , ) is the German name (used in English in the first half of the 20th century) to refer to those northern, southwest, and western areas of Czechoslovakia which were inhabited primarily by German speakers, specifically the border districts of Bohemia, Moravia, and those parts of Silesia located within Czechoslovakia.
The name is derived from that of the Sudetes mountains, which run along the northern Czech border as far as Silesia and contemporary Poland, although it encompassed areas well beyond those mountains.
The word "Sudetenland" did not come into existence until the early 20th century and did not come to prominence after the First World War, when the German-dominated Austria-Hungary was dismembered and the Sudeten Germans found themselves living in the new country of Czechoslovakia. The "Sudeten crisis" of 1938 was provoked by the demands of Nazi Germany that the Sudetenland be annexed to Germany, which in fact took place after the later infamous Munich Agreement. When Czechoslovakia was reconstituted after the Second World War, the Sudeten Germans were largely expelled, and the region today is inhabited primarily by Czech speakers.
Parts of the current Czech regions of Karlovy Vary, Liberec, Olomouc, Moravia-Silesia, and Ústí nad Labem are situated within the former Sudetenland.
History.
The areas later known as Sudetenland never formed a single historical region, which makes it difficult to distinguish the history of the Sudetenland apart from that of Bohemia, until the advent of nationalism in the 19th century.
Early origins.
The Celtic and Boii tribes settled there and the region was first mentioned on the map of Ptolemaios in the 2nd century AD. The Germanic tribe of the Marcomanni dominated the entire core of the region in later centuries. Those tribes already built cities like Brno, but moved west during the Migration Period. In the 7th century AD Slavic people moved in and were united under Samo's realm. Later in the High Middle Ages Germans settled into the less populated border region.
In the Middle Ages the regions situated on the mountainous border of the Duchy and the Kingdom of Bohemia had since the Migration Period been settled mainly by western Slavic Czechs. Along the Bohemian Forest in the west, the Czech lands bordered on the German Slavic tribes (German Sorbs) stem duchies of Bavaria and Franconia; marches of the medieval German kingdom had also been established in the adjacent Austrian lands south of the Bohemian-Moravian Highlands and the northern Meissen region beyond the Ore Mountains. In the course of the "Ostsiedlung" (settlement of the east) German settlement from the 13th century onwards continued to move into the Upper Lusatia region and the duchies of Silesia north of the Sudetes mountain range.
From as early as the second half of the 13th century onwards these Bohemian border regions were settled by ethnic Germans, who were invited by the Přemyslid Bohemian kings—especially by Ottokar II (1253–1278) and Wenceslaus II (1278–1305). After the extinction of the Přemyslid dynasty in 1306, the Bohemian nobility backed John of Luxembourg as king against his rival Duke Henry of Carinthia. In 1322 King John of Bohemia acquired (for the third time) the formerly Imperial Egerland region in the west and was able to vassalize most of the Piast Silesian duchies, acknowledged by King Casimir III of Poland by the 1335 Treaty of Trentschin. His son, Bohemian King Charles IV, was elected King of the Romans in 1346 and crowned Holy Roman Emperor in 1355. He added the Lusatias to the Lands of the Bohemian Crown, which then comprised large territories with a significant German population.
In the hilly border regions German settlers established major manufactures of forest glass. The situation of the German population was aggravated by the Hussite Wars (1419–1434), though there were also some Germans among the Hussite insurgents.
By then Germans largely settled the hilly Bohemian border regions as well as the cities of the lowlands; mainly people of Bavarian descent in the South Bohemian and South Moravian Region, in Brno, Jihlava, České Budějovice and the West Bohemian Plzeň Region; Franconian people in Žatec; Upper Saxons in adjacent North Bohemia, where the border with the Saxon Electorate was fixed by the 1459 Peace of Eger; Germanic Silesians in the adjacent Sudetes region with the County of Kladsko, in the Moravian–Silesian Region, in Svitavy and Olomouc. The city of Prague had a German-speaking majority from the last third of the 17th century until 1860, but after 1910 had the proportion of German speakers decreased to 6.7% of the population.
From the Luxembourgs, the rule over Bohemia passed through George of Podiebrad to the Jagiellon dynasty and finally to the House of Habsburg in 1526. Both Czech and German Bohemians suffered heavily in the Thirty Years War. Bohemia lost 70% of its population. From the defeat of the Bohemian Revolt that collapsed at the 1620 Battle of White Mountain, the Habsburgs gradually integrated the Kingdom of Bohemia into their monarchy. During the subsequent Counter-Reformation, less populated areas were resettled with Catholic Germans from the Austrian lands. From 1627 the Habsburgs enforced the so-called "Verneuerte Landesordnung" ("Renewed Land's Constitution") and one of its consequences was that German according to mother tongue gradually became the primary and official language while Czech declined to a secondary role in the Empire. Also in 1749 Austrian Empire enforced German as the official language again. Emperor Joseph II in 1780 renounced the coronation ceremony as Bohemian king and unsuccessfully tried to push German through as sole official language in all Habsburg lands (including Hungary). Nevertheless, German cultural influence grew stronger during the Age of Enlightenment and Weimar Classicism.
On the other hand, in the course of the Romanticism movement national tensions arose, both in the form of the Austroslavism ideology developed by Czech politicians like František Palacký and Pan-Germanist activist raising the German question. Conflicts between Czech and German nationalists emerged in the 19th century, for instance in the Revolutions of 1848: while the German-speaking population of Bohemia and Moravia wanted to participate in the building of a German nation state, the Czech-speaking population insisted on keeping Bohemia out of such plans. The Bohemian Kingdom remained a part of the Austrian Empire and Austria-Hungary until its dismemberment after the First World War.
Emergence of the term.
In the wake of growing nationalism, the name ""Sudetendeutsche"" (Sudeten Germans) emerged by the early 20th century. It originally constituted part of a larger classification of three groupings of Germans within the Austro-Hungarian Empire, which also included ""Alpine Deutschen"" () in what later became the Republic of Austria and ""Balkandeutsche"" () in Hungary and the regions east of it. Of these three terms, only the term ""Sudetendeutsche"" survived, because of the ethnic and cultural conflicts within Bohemia.
World War I and its aftermath.
During World War I, what would later be known as the Sudetenland experienced a rate of war deaths higher than most other German-speaking areas of Austria-Hungary and exceeded only by German South Moravia and Carinthia. Thirty-four of each 1,000 inhabitants were killed.
Austria-Hungary broke apart at the end of World War I. Late in October 1918, an independent Czechoslovak state, consisting of the lands of the Bohemian kingdom and areas belonging to the Kingdom of Hungary, was proclaimed. The German deputies of Bohemia, Moravia, and Silesia in the Imperial Council ("Reichsrat") referred to the Fourteen Points of U.S. President Woodrow Wilson and the right proposed therein to self-determination, and attempted to negotiate the union of the German-speaking territories with the new Republic of German Austria, which itself aimed at joining Weimar Germany.
The German-speaking parts of the former Lands of the Bohemian Crown remained in a newly created Czechoslovakia, a multi-ethnic state of several nations: Czechs, Germans, Slovaks, Hungarians, Poles and Ruthenians. On 20 September 1918, the Prague government asked the United States's opinion for the Sudetenland. President Woodrow Wilson sent Ambassador Archibald Coolidge into Czechoslovakia. After Coolidge became witness of German Bohemian demonstrations, Coolidge suggested the possibility of ceding certain German-speaking parts of Bohemia to Germany (Cheb) and Austria (South Moravia and South Bohemia). He also insisted that the German-inhabited regions of West and North Bohemia remain within Czechoslovakia. The American delegation at the Paris talks, with Allen Dulles as the American's chief diplomat in the Czechoslovak Commission who emphasized preserving the unity of the Czech lands, decided not to follow Coolidge's proposal.
Four regional governmental units were established:
The U.S. commission to the Paris Peace Conference issued a declaration which gave unanimous support for "unity of Czech lands". In particular the declaration stated:
Several German minorities according to their mother tongue in Moravia—including German-speaking populations in Brno, Jihlava, and Olomouc—also attempted to proclaim their union with German Austria, but failed. The Czechs thus rejected the aspirations of the German Bohemians and demanded the inclusion of the lands inhabited by ethnic Germans in their state, despite the presence of more than 90% (as of 1921) ethnic Germans (which led to the presence of 23.4% of Germans in all of Czechoslovakia), on the grounds they had always been part of lands of the Bohemian Crown. The Treaty of Saint-Germain in 1919 affirmed the inclusion of the German-speaking territories within Czechoslovakia. Over the next two decades, some Germans in the Sudetenland continued to strive for a separation of the German-inhabited regions from Czechoslovakia.
Within the Czechoslovak Republic (1918–1938).
According to the February 1921 census, 3,123,000 native German speakers lived in Czechoslovakia—23.4% of the total population. The controversies between the Czechs and the German-speaking minority lingered on throughout the 1920s, and intensified in the 1930s.
During the Great Depression the mostly mountainous regions populated by the German minority, together with other peripheral regions of Czechoslovakia, were hurt by the economic depression more than the interior of the country. Unlike the less developed regions (Ruthenia, Moravian Wallachia), the Sudetenland had a high concentration of vulnerable export-dependent industries (such as glass works, textile industry, paper-making, and toy-making industry). Sixty percent of the bijouterie and glass-making industry were located in the Sudetenland, 69% of employees in this sector were Germans speaking according to mother tongue, and 95% of bijouterie and 78% of other glassware was produced for export. The glass-making sector was affected by decreased spending power and also by protective measures in other countries and many German workers lost their work.
The high unemployment made people more open to populist and extremist movements such as fascism, communism, and German irredentism. In these years, the parties of German nationalists and later the Nazi Sudeten German National Socialist Party (SdP) with its radical demands gained immense popularity among Germans in Czechoslovakia. After 1933 Czechoslovakia remained the only democracy in central and eastern Europe.
Sudeten Crisis.
The increasing aggressiveness of Hitler prompted the Czechoslovak military to build extensive Czechoslovak border fortifications starting in 1936 to defend the troubled border region.
Immediately after the "Anschluss" of Austria into the Third Reich in March 1938, Hitler made himself the advocate of ethnic Germans living in Czechoslovakia, triggering the "Sudeten Crisis". The following month, Sudeten Nazis, led by Konrad Henlein, agitated for autonomy. On 24 April 1938 the SdP proclaimed the "", which demanded in eight points the complete equality between the Sudeten Germans and the Czech people. The government accepted these claims on 30 June 1938.
In August, British Prime Minister, Neville Chamberlain, sent Lord Runciman on a Mission to Czechoslovakia in order to see if he could obtain a settlement between the Czechoslovak government and the Germans in the Sudetenland. Lord Runciman's first day included meetings with President Beneš and Prime Minister Milan Hodža as well as a direct meeting with the Sudeten Germans from Henlein's SdP. On the next day he met with Dr and Mme Beneš and later met non-Nazi Germans in his hotel.
A full account of his report—including summaries of the conclusions of his meetings with the various parties—which he made in person to the Cabinet on his return to Britain is found in the Document CC 39(38). Lord Runciman expressed sadness that he could not bring about agreement with the various parties, but he agreed with Lord Halifax that the time gained was important. He reported on the situation of the Sudeten Germans, and he gave details of four plans which had been proposed to deal with the crisis, each of which had points which, he reported, made it unacceptable to the other parties to the negotiations.
The four were: Transfer of the Sudetenland to the Reich; hold a plebiscite on the transfer of the Sudetenland to the Reich, organize a Four Power Conference on the matter, create a federal Czechoslovakia. At the meeting, he said that he was very reluctant to offer his own solution; he had not seen this as his task. The most that he said was that the great centres of opposition were in Eger and Asch, in the northwestern corner of Bohemia, which contained about 800,000 Germans and very few others.
He did say that the transfer of these areas to Germany would almost certainly be a good thing; he added that the Czechoslovak army would certainly oppose this very strongly, and that Beneš had said that they would fight rather than accept it.
British Prime Minister Neville Chamberlain met with Adolf Hitler in "Berchtesgaden" on 15 September and agreed to the cession of the Sudetenland; three days later, French Prime Minister Édouard Daladier did the same. No Czechoslovak representative was invited to these discussions. Germany was now able to walk into the Sudetenland without firing a shot.
Chamberlain met Hitler in Godesberg on 22 September to confirm the agreements. Hitler, aiming to use the crisis as a pretext for war, now demanded not only the annexation of the Sudetenland but the immediate military occupation of the territories, giving the Czechoslovak army no time to adapt their defence measures to the new borders. To achieve a solution, Italian dictator Benito Mussolini suggested a conference of the major powers in Munich and on 29 September, Hitler, Daladier and Chamberlain met and agreed to Mussolini's proposal (actually prepared by Hermann Göring) and signed the Munich Agreement, accepting the immediate occupation of the Sudetenland. The Czechoslovak government, though not party to the talks, submitted to compulsion and promised to abide by the agreement on 30 September.
The Sudetenland was relegated to Germany between 1 October and 10 October 1938. The Czech part of Czechoslovakia was subsequently invaded by Germany in March 1939, with a portion being annexed and the remainder turned into the Protectorate of Bohemia and Moravia. The Slovak part declared its independence from Czechoslovakia, becoming the Slovak Republic (Slovak State), a satellite state and ally of Nazi Germany. (The Ruthenian part — Subcarpathian Rus — made also an attempt to declare its sovereignty as Carpatho-Ukraine but only with ephemeral success. This area was annexed by Hungary.)
Part of the borderland was also invaded and annexed by Poland.
Sudetenland as part of Nazi Germany.
The Sudetenland was initially put under military administration, with General Wilhelm Keitel as military governor. On 21 October 1938, the annexed territories were divided, with the southern parts being incorporated into the neighbouring Reichsgaue "Niederdonau", "Oberdonau" and "Bayerische Ostmark".
The northern and western parts were reorganized as the Reichsgau "Sudetenland", with the city of Reichenberg (present-day Liberec) established as its capital. Konrad Henlein (now openly a NSDAP member) administered the district first as "Reichskommissar" (until 1 May 1939) and then as "Reichsstatthalter" (1 May 1939 – 4 May 1945). Sudetenland consisted of three political districts: Eger (with Karlsbad as capital), Aussig (Aussig) and Troppau (Troppau).
Shortly after the annexation, the Jews living in the Sudetenland were widely persecuted. Only a few weeks afterwards, the Kristallnacht occurred. As elsewhere in Germany, many synagogues were set on fire and numerous leading Jews were sent to concentration camps. In later years, the Nazis transported up to 300,000 Czech and Slovak Jews to concentration camps, where many of them were killed or died. Jews and Czechs were not the only afflicted peoples; German socialists, communists and pacifists were widely persecuted as well. Some of the German socialists fled the Sudetenland via Prague and London to other countries. The Gleichschaltung would permanently alter the community in the Sudetenland.
Despite this, on 4 December 1938 there were elections in Reichsgau Sudetenland, in which 97.32% of the adult population voted for NSDAP. About a half million Sudeten Germans joined the Nazi Party which was 17.34% of the total German population in Sudetenland (the average NSDAP membership participation in Nazi Germany was merely 7.85% in 1944). This means the Sudetenland was one of the most pro-Nazi regions of the Third Reich. Because of their knowledge of the Czech language, many Sudeten Germans were employed in the administration of the ethnic Czech Protectorate of Bohemia and Moravia as well as in Nazi organizations (Gestapo, etc.). The most notable was Karl Hermann Frank: the SS and Police general and Secretary of State in the Protectorate.
Expulsions and resettlement after World War II.
Shortly after the liberation of Czechoslovakia in May 1945, the use of the term "Sudety" (Sudetenland) in official communications was banned and replaced by the term "pohraniční území" (border territory).
After World War II in summer 1945 the Potsdam Conference decided that Sudeten Germans would have to leave Czechoslovakia (see Expulsion of Germans after World War II). As a consequence of the immense hostility against all Germans that had grown within Czechoslovakia due to Nazi behavior, the overwhelming majority of Germans were expelled (while the relevant Czechoslovak legislation provided for the remaining Germans who were able to prove their anti-Nazi affiliation).
The number of expelled Germans in the early phase (spring–summer 1945) is estimated to be around 500,000 people. Following the Beneš decrees and starting in 1946, the majority of the Germans were expelled and in 1950 only 159,938 (from 3,149,820 in 1930) still lived in the Czech Republic. The remaining Germans, proven anti-fascists and forced laborers, were allowed to stay in Czechoslovakia, but were later forcefully dispersed within the country Some German refugees from Czechoslovakia are represented by the Sudetendeutsche Landsmannschaft.
Many of the Germans who stayed in Czechoslovakia later emigrated to West Germany (more than 100,000). As the German population was transferred out of the country, the former Sudetenland was resettled, mostly by Czechs but also by other nationalities of Czechoslovakia: Slovaks, Greeks (arriving in the wake of the Greek Civil War 1946–49), Volhynian Czechs, Gypsies, Jews and Hungarians (though the Hungarians were forced into this and later returned home—see Hungarians in Slovakia: Population exchanges).
Some areas—such as part of Czech Silesian-Moravian borderland, southwestern Bohemia (Šumava National Park), western and northern parts of Bohemia—remained depopulated for several strategic reasons (extensive mining and military interests) or are now protected national parks and landscapes. Moreover, before the establishment of the Iron Curtain in 1952–55, the so-called "forbidden zone" was established (by means of engineer equipment) up to 2 km (1.2 mi) from the border in which no civilians could reside. A wider region, or "border zone" existed, up to 12 km from the border, in which no "disloyal" or "suspect" civilians could reside or work. Thus, the entire Aš-Bulge fell within the border zone; this status remained until the Velvet Revolution in 1989.
There remained areas with noticeable German minorities in the westernmost borderland around Cheb, where skilled forced labour of remaining ethnic German men continued in mining and industry until 1955, sanctioned under the Yalta Conference protocols ; in the Egerland, German minority organizations continue to exist. Also, the small town of Kravaře () in the multiethnic Hlučín Region of Czech Silesia has an ethnic German majority (2006), including an ethnic German mayor.
In the 2001 census, approximately 40,000 people in the Czech Republic claimed German ethnicity.

</doc>
<doc id="42863" url="https://en.wikipedia.org/wiki?curid=42863" title="A Streetcar Named Desire">
A Streetcar Named Desire

A Streetcar Named Desire is a 1947 play written by American playwright Tennessee Williams which received the Pulitzer Prize for Drama in 1948. The play opened on Broadway on December 3, 1947, and closed on December 17, 1949, in the Ethel Barrymore Theatre. The Broadway production was directed by Elia Kazan and starred Jessica Tandy, Karl Malden, Marlon Brando, and Kim Hunter. The London production opened in 1949 with Bonar Colleano, Vivien Leigh, and Renee Asherson and was directed by Laurence Olivier. The drama "A Streetcar Named Desire" is often regarded as among the finest plays of the 20th century, and is generally considered to be Williams' greatest.
Plot.
After the loss of her family home, Belle Reve, to creditors, Blanche DuBois travels from the small town of Laurel, Mississippi, to the New Orleans French Quarter to live with her younger, married sister, Stella, and brother-in-law, Stanley Kowalski. Blanche is in her thirties, and with no money, she has nowhere else to go.
Blanche tells Stella that she has taken a leave of absence from her English teaching position because of her nerves. Blanche laments the shabbiness of her sister’s two-room flat. She finds Stanley loud and rough, eventually referring to him as "common". Stanley, in return, does not care for Blanche's manners and dislikes her presence.
Stanley later questions Blanche about her earlier marriage. Blanche had married when she was very young, but her husband died, leaving her widowed and alone. The memory of her dead husband causes Blanche some obvious distress. Stanley, worried that he has been cheated out of an inheritance, demands to know what happened to Belle Reve, once a large plantation and the DuBois family home. Blanche hands over all the documents pertaining to Belle Reve. While looking at the papers, Stanley notices a bundle of letters that Blanche emotionally proclaims are personal love letters from her dead husband. For a moment, Stanley seems caught off guard over her proclaimed feelings. Afterwards, he informs Blanche that Stella is going to have a baby.
The night after Blanche’s arrival, during one of Stanley’s poker parties, Blanche meets Mitch, one of Stanley’s poker player buddies. His courteous manner sets him apart from the other men. Their chat becomes flirtatious and friendly, and Blanche easily charms him; they like each other. Suddenly becoming upset over multiple interruptions, Stanley explodes in a drunken rage and strikes Stella. Blanche and Stella take refuge with the upstairs neighbor, Eunice. When Stanley recovers, he cries out from the courtyard below for Stella to come back by repeatedly calling her name until she comes down and allows herself to be carried off to bed. After Stella returns to Stanley, Blanche and Mitch sit at the bottom of the steps in the courtyard, where Mitch apologizes for Stanley's coarse behavior.
Blanche is bewildered that Stella would go back with him after such violence. The next morning, Blanche rushes to Stella and describes Stanley as a subhuman animal, though Stella assures Blanche that she and Stanley are fine. Stanley overhears the conversation but keeps silent. When Stanley comes in, Stella hugs and kisses him, letting Blanche know that her low opinion of Stanley does not matter.
As the weeks pass, Blanche and Stanley continue to not get along. Blanche has hope in Mitch, and tells Stella that she wants to go away with him and not be anyone’s problem. During a meeting between the two, Blanche confesses to Mitch that once she was married to a young man, Allan Grey, whom she later discovered in a sexual encounter with an older man. Grey later committed suicide when Blanche told him she was disgusted with him. The story touches Mitch, who tells Blanche that they need each other. It seems certain that they will get married.
Later on, Stanley repeats gossip to Stella that he has gathered on Blanche, telling her that Blanche was fired from her teaching job for having sex with a student and that she lived at a hotel known for prostitutes. Stella erupts in anger over Stanley’s cruelty after he states that he has also told Mitch about the rumors, but the fight is cut short as she goes into labor and is sent to the hospital.
As Blanche waits at home alone, Mitch arrives and confronts Blanche with the stories that Stanley has told him. At first she denies everything, but eventually confesses that the stories are true. She pleads for forgiveness, but an angry and humiliated Mitch rejects her. He then advances toward her as though to rape her; in response, Blanche screams "fire", and he runs away in fright.
When Stella has the baby, Stanley and Blanche are left alone in the apartment. In their final confrontation, it is strongly implied that Stanley rapes Blanche, imminently resulting in her psychotic crisis.
Weeks later, at another poker game at the Kowalski apartment, Stella and her neighbor, Eunice, are packing Blanche's belongings. Blanche has suffered a complete mental breakdown and is to be committed to a mental hospital. Although Blanche has told Stella about Stanley's assault, Stella cannot bring herself to believe her sister's story. When a doctor and a nurse arrive to take Blanche to the hospital, she initially resists them and collapses on the floor in confusion. Mitch, present at the poker game, breaks down in tears. When the doctor helps Blanche up, she goes willingly, with him, saying, "Whoever you are, I have always depended upon the kindness of strangers." The play ends with Stanley continuing to comfort Stella while the poker game continues uninterrupted.
Stage productions.
Original Broadway production.
The original Broadway production was produced by Irene Mayer Selznick. It opened at the Shubert in New Haven shortly before moving to the Ethel Barrymore Theatre on December 3, 1947. Selznick originally wanted to cast Margaret Sullavan and John Garfield, but settled on Jessica Tandy and Marlon Brando, who were virtual unknowns at the time. The opening night cast also included Kim Hunter as Stella and Karl Malden as Mitch. Tandy was cast after Williams saw her performance in a West Coast production of his one-act play "Portrait of a Madonna". Williams believed that by casting Brando, who was young for the part as it was originally conceived, would evolve Kowalski from being a vicious older man to someone whose unintentional cruelty can be attributed to youthful ignorance. Despite its shocking scenes and gritty dialogue, the audience applauded for half an hour after the debut performance ended.
Brooks Atkinson, reviewing the opening in "The New York Times" described Tandy's "superb performance" as "almost incredibly true," concluding that Williams "has spun a poignant and luminous story."
Later in the run, Uta Hagen replaced Tandy, Carmelita Pope replaced Hunter, and Anthony Quinn replaced Brando. Hagen and Quinn took the show on a national tour and then returned to Broadway for additional performances. Early on, when Brando broke his nose, Jack Palance took over his role. Ralph Meeker also took on the part of Stanley both in the Broadway and touring companies. Tandy received a Tony Award for Best Actress in a Play in 1948, sharing the honor with Judith Anderson's portrayal of Medea and with Katharine Cornell.
Uta Hagen's Blanche on the national tour was directed not by Elia Kazan, who had directed the Broadway production, but by Harold Clurman, and it has been reported, both in interviews by Hagen and observations by contemporary critics, that the Clurman-directed interpretation shifted the focus of audience sympathy back to Blanche and away from Stanley (where the Kazan version had located it). This was the original conception of the play, and has been reflected in subsequent revivals.
The original Broadway production closed after 855 performances in 1949.
Original cast.
Original London production.
The London production, directed by Laurence Olivier, opened on October 12, 1949, and starred Bonar Colleano, Vivien Leigh, and Renee Asherson.
"Belle Reprieve".
Bette Bourne and Paul Shaw of the British gay theater company, Bloolips, and Peggy Shaw and Lois Weaver of the American lesbian theater company, Split Britches, collaborated and performed a gender-bent production of "Belle Reprieve", a twisted adaption of "Streetcar". “This theatrical piece creates a Brechtian,” “epic drama” that relies on the reflective rather than emotional involvement of the audience--a “commentary on the sexual roles and games in Williams’s text.” Blanche was played by Bette Bourne as “man in a dress," Stanley was played by Peggy Shaw as a “butch lesbian,” Mitch was played by Paul Shaw as a “fairy disguised as a man,” and Stella was played by Lois Weaver as a “woman disguised as a woman.”
Revivals.
The first all-black production of "Streetcar" was probably the one performed by the Summer Theatre Company at Lincoln University in Jefferson City, Missouri, in August 1953 and directed by one of Williams's former classmates at Iowa, Thomas D. Pawley, as noted in the "Streetcar" edition of the "Plays in Production" series published by Cambridge University Press. The black and cross-gendered productions of "Streetcar" since the mid-1950s are much too numerous to list here.
Tallulah Bankhead, for whom Williams had originally written the role of Blanche, starred in a 1956 New York City Center Company production directed by Herbert Machiz.
The first Broadway revival of the play was in 1973. It was produced by the Lincoln Center, at the Vivian Beaumont Theater, and starred Rosemary Harris as Blanche, James Farentino as Stanley and Patricia Conolly as Stella.
Famously, The Simpsons also did an episode, "A Streetcar Named Marge", in which the play was featured. Ned Flanders and Marge took the leading roles as Stanley and Blanche, respectively.
The Spring 1988 revival at the Circle in the Square Theatre starred Aidan Quinn opposite Blythe Danner as Blanche and Frances McDormand as Stella.
A highly publicized revival in 1992 starred Alec Baldwin as Stanley and Jessica Lange as Blanche. It was staged at the Ethel Barrymore Theatre, the same theatre that the original production was staged in. This production proved so successful that it was filmed for television. It featured Timothy Carhart as Mitch and Amy Madigan as Stella, as well as future "Sopranos" stars James Gandolfini and Aida Turturro. Gandolfini was Carhart's understudy.
In 1997, Le Petit Theatre du Vieux Carre in New Orleans mounted a 50th Anniversary production, with music by the Marsalis family, starring Michael Arata and Shelly Poncy. In 2009, the Walnut Street Theatre in Philadelphia, where the original pre-Broadway tryout occurred, began a production of the play for its 200th anniversary season.
The 2005 Broadway revival was directed by Edward Hall and produced by The Roundabout Theater Company. It starred John C. Reilly as Stanley, Amy Ryan as Stella, and Natasha Richardson as Blanche. The production would mark Natasha Richardson's final appearance on Broadway owing to her death in 2009 in a skiing accident.
The Sydney Theatre Company production of "A Streetcar Named Desire" premiered on September 5 and ran until October 17, 2009. This production, directed by Liv Ullmann, starred Cate Blanchett as Blanche, Joel Edgerton as Stanley, Robin McLeavy as Stella and Tim Richards as Mitch.
From July 2009 until October 2009, Rachel Weisz and Ruth Wilson starred in a highly acclaimed revival of the play in London's West End at the Donmar Warehouse directed by Rob Ashford.
In November 2010, an Oxford University student production was staged at the Oxford Playhouse which sold out and was critically acclaimed.
In April 2012, Blair Underwood, Nicole Ari Parker, Daphne Rubin-Vega and Wood Harris starred in a multiracial adaptation at the Broadhurst Theatre. Theatre review aggregator "Curtain Critic" gave the production a score of 61 out of 100 based on the opinions of 17 critics.
A production at the Young Vic, London, opened on July 23, 2014, and closed on September 19, 2014. Directed by Benedict Andrews and starring Gillian Anderson, Ben Foster and Vanessa Kirby, this production garnered critical acclaim and is the fastest selling show ever produced by the Young Vic. On September 16, 2014, the performance was relayed live to over one thousand cinemas in the UK as part of the National Theatre Live project to broadcast the best of British theatre live from the London stage to cinemas across the UK and around the world. Thus far, the production has been screened in over 2000 venues. From April 23, 2016 till June 4, 2016, the same production is being reprised at the new St. Ann's Warehouse in Brooklyn, NYC.
Adaptations.
Film.
In 1951, a film adaptation of the play, directed by Elia Kazan, with Malden, Brando, and Hunter reprising their Broadway roles, joined by Vivien Leigh from the London production for the part of Blanche. The movie won four Academy Awards, including three acting awards (Leigh for Best Actress, Malden for Best Supporting Actor and Hunter for Best Supporting Actress), the first time a film won three out of four acting awards (Brando was nominated for Best Actor but lost). Jessica Tandy was the only lead actor from the original Broadway production not to appear in the 1951 film. References to Allan Grey's sexual orientation are essentially removed, due to Motion Picture Production Code restrictions. Instead, the reason for his suicide is changed to a general "weakness". The ending itself was also slightly altered. Stella does not remain with Stanley, as she does in the play.
Pedro Almodóvar's 1999 Academy Award-winning film, "All About My Mother", features a Spanish-language version of the play being performed by some of the supporting characters and the play itself plays an important role in the film. However, some of the film's dialogue is taken from the 1951 film version, not the original stage version.
The 1973 Woody Allen film "Sleeper" includes a late scene in which Miles (Woody) and Luna (Diane Keaton) briefly take on the roles of Stanley (Luna) and Blanche (Woody).
It was noted by many critics that the 2013 Academy Award-winning Woody Allen film "Blue Jasmine" had much in common with "Streetcar" and is most likely a loose adaptation. It shares a very similar plot and characters, although it has been suitably updated for modern film audiences.
Opera.
In 1995, an opera was adapted and composed by André Previn with a libretto by Philip Littell. It had its premiere at the San Francisco Opera during the 1998–99 season, and featured Renée Fleming as Blanche.
Ballet.
A 1952 ballet production with choreography by Valerie Bettis, which Mia Slavenska and Frederic Franklin's Slavenska-Franklin Ballet debuted at Her Majesty's Theatre in Montreal , featured the music of Alex North, who had composed the music for the 1951 film.
Another ballet production was staged by John Neumeier in Frankfurt in 1983. Music included "Visions fugitives" by Prokofiev and Alfred Schnittke's First Symphony.
In 2012, Scottish Ballet collaborated with theatre and film director Nancy Meckler and international choreographer Annabelle Lopez Ochoa to create a new staging of "A Streetcar Named Desire".
Television.
In 1955, the television program "Omnibus" featured Jessica Tandy reviving her original Broadway performance as Blanche, with her husband, Hume Cronyn, as Mitch. It aired only portions of the play that featured the Blanche and Mitch characters.
The multi-Emmy Award-winning 1984 television version featured Ann-Margret as Blanche, Treat Williams as Stanley, Beverly D'Angelo as Stella and Randy Quaid as Mitch. It was directed by John Erman and the teleplay was adapted by Oscar Saul. The music score by composed by Marvin Hamlisch. Ann-Margret, D'Angelo and Quaid were all nominated for Emmy Awards, but none won. However, it did win four Emmys, including one for cinematographer Bill Butler. Ann-Margret won a Golden Globe award for her performance and Treat Williams was nominated for Best Actor in a Miniseries or TV Movie.
A 1995 television version was based on the highly successful Broadway revival that starred Alec Baldwin and Jessica Lange. However, only Baldwin and Lange were from the stage production. The TV version added John Goodman as Mitch and Diane Lane as Stella. This production was directed by Glenn Jordan. Baldwin, Lange and Goodman all received Emmy Award nominations. Lange won a Golden Globe award (for Best Actress in a Miniseries or TV Movie), while Baldwin was nominated for Best Actor, but did not win.
In 1998, PBS aired a taped version of the opera adaptation that featured the original San Francisco Opera cast. The program received an Emmy Award nomination for Outstanding Classical Music/Dance Program.
Inspirations.
The Desire Line ran from 1920 to 1948, at the height of streetcar use in New Orleans. The route ran down Bourbon, through the Quarter, to Desire Street in the Bywater district, and back up to Canal. Blanche's route in the play—"They told me to take a streetcar named Desire, transfer to one called Cemeteries and ride six blocks and get off at—Elysian Fields!"—is allegorical, taking advantage of New Orleans's colorful street names.
The character of Blanche is thought to be based on Williams' sister, Rose Williams, who struggled with mental health issues and became incapacitated after a lobotomy.
Theatre critic and former actress Blanche Marvin, a friend of Williams, says the playwright used her name for the character Blanche DuBois, named the character's sister Stella after Marvin's former surname "Zohar" (which means "Star"), and took the play's line "I've always depended on the kindness of strangers" from something she said to him.
"A Streetcar Named Success".
"A Streetcar Named Success" is an essay by Tennessee Williams about art and the artist's role in society. It is often included in paper editions of "A Streetcar Named Desire". A version of this essay first appeared in "The New York Times" on November 30, 1947, four days before the opening of "A Streetcar Named Desire". Another version of this essay, entitled "The Catastrophe of Success", is sometimes used as an introduction to "The Glass Menagerie".
Auction record.
On October 1, 2009, Swann Galleries auctioned an unusually fine copy of "A Streetcar Named Desire", New York, 1947, signed by Williams and dated 1976 for $9,000, a record price for a signed copy of the play.

</doc>
<doc id="42866" url="https://en.wikipedia.org/wiki?curid=42866" title="Java Message Service">
Java Message Service

The Java Message Service (JMS) API is a Java Message Oriented Middleware (MOM) API for sending messages between two or more clients. JMS is a part of the Java Platform, Enterprise Edition, and is defined by a specification developed under the Java Community Process as JSR 914. It is a messaging standard that allows application components based on the Java Enterprise Edition (Java EE) to create, send, receive, and read messages. It allows the communication between different components of a "distributed application" to be loosely coupled, reliable, and asynchronous.
General idea of messaging.
Messaging is a form of "loosely coupled" distributed communication, where in this context the term 'communication' can be understood as an exchange of messages between software components. Message-oriented technologies attempt to relax "tightly coupled" communication (such as TCP network sockets, CORBA or RMI) by the introduction of an intermediary component. This approach allows software components to communicate 'indirectly' with each other. Benefits of this include message senders not needing to have precise knowledge of their receivers.
The advantages of messaging include the ability to integrate heterogeneous platforms, reduce system bottlenecks, increase scalability, and respond more quickly to change.
Version history.
JMS 2.0 is maintained under the Java Community Process as JSR 343.
Elements.
The following are JMS elements:
Models.
The JMS API supports two models:
Point-to-point model.
In point-to-point messaging system, messages are routed to an individual consumer which maintains a queue of "incoming" messages. This messaging type is built on the concept of message queues, senders, and receivers. Each message is addressed to a specific queue, and the receiving clients extract messages from the queues established to hold their messages. While any number of producers can send messages to the queue, each message is guaranteed to be delivered, and consumed by one consumer. Queues retain all messages sent to them until the messages are consumed or until the messages expire. If no consumers are registered to consume the messages, the queue holds them until a consumer registers to consume them.
Publish/subscribe model.
The publish/subscribe model supports publishing messages to a particular message topic. "Subscribers" may register interest in receiving messages on a particular message topic. In this model, neither the "publisher" nor the subscriber knows about each other. A good analogy for this is an anonymous bulletin board
JMS provides a way of separating the application from the transport layer of providing data. The same Java classes can be used to communicate with different JMS providers by using the Java Naming and Directory Interface (JNDI) information for the desired provider. The classes first use a "connection factory" to connect to the queue or topic, and then use populate and send or publish the messages. On the receiving side, the clients then receive or subscribe to the messages.
URI scheme.
RFC 6167 defines a jms: URI scheme for the Java Message Service.
Provider implementations.
To use JMS, one must have a JMS provider that can manage the sessions, queues and topics. Starting from Java EE version 1.4, JMS provider has to be contained in "all" Java EE application servers. This can be implemented using the message inflow management of the Java EE Connector Architecture, which was first made available in that version.
The following is a list of JMS providers:
A historical comparison matrix of JMS providers from 2005 is available at http://www.theserverside.com/reviews/matrix.tss

</doc>
<doc id="42869" url="https://en.wikipedia.org/wiki?curid=42869" title="Java Platform, Enterprise Edition">
Java Platform, Enterprise Edition

Java Platform, Enterprise Edition or Java EE is a widely used enterprise computing platform developed under the Java Community Process. The platform provides an API and runtime environment for developing and running enterprise software, including network and web services, and other large-scale, multi-tiered, scalable, reliable, and secure network applications. Java EE extends the Java Platform, Standard Edition (Java SE), providing an API for object-relational mapping, distributed and multi-tier architectures, and web services. The platform incorporates a design based largely on modular components running on an application server. Software for Java EE is primarily developed in the Java programming language. The platform emphasizes convention over configuration and annotations for configuration. Optionally XML can be used to override annotations or to deviate from the platform defaults.
Version history.
The platform was known as "Java 2 Platform, Enterprise Edition" or "J2EE" until the name was changed to "Java Platform, Enterprise Edition" or "Java EE" in version 5. The current version is called "Java EE 7".
Standards and specifications.
Java EE is defined by its specification. As with other Java Community Process specifications, providers must meet certain conformance requirements in order to declare their products as "Java EE compliant".
Java EE includes several API specifications, such as RMI, e-mail, JMS, web services, XML, etc., and defines how to coordinate them. Java EE also features some specifications unique to Java EE for components. These include Enterprise JavaBeans, connectors, servlets, JavaServer Pages and several web service technologies. This allows developers to create enterprise applications that are portable and scalable, and that integrate with legacy technologies. A Java EE application server can handle transactions, security, scalability, concurrency and management of the components it is deploying, in order to enable developers to concentrate more on the business logic of the components rather than on infrastructure and integration tasks.
General APIs.
The Java EE APIs includes several technologies that extend the functionality of the base Java SE APIs.
The servlet specification defines a set of APIs to service mainly HTTP requests. It includes the JavaServer Pages (JSP) specification.
The Java API for WebSocket specification defines a set of APIs to service WebSocket connections.
This package defines the root of the JavaServer Faces ("JSF") API. JSF is a technology for constructing user interfaces out of components.
This package defines the component part of the JavaServer Faces API. Since JSF is primarily component oriented, this is one of the core packages. The package overview contains a UML diagram of the component hierarchy.
This package defines the classes and interfaces for Java EE's Expression Language. The Expression Language ("EL") is a simple language originally designed to satisfy the specific needs of web application developers. It is used specifically in JSF to bind components to (backing) beans and in CDI to name beans, but can be used throughout the entire platform.
These packages define the injection annotations for the Contexts and Dependency Injection (CDI) APIs.
These packages define the context annotations and interfaces for the Contexts and Dependency Injection (CDI) API.
The Enterprise JavaBean ("EJB") specification defines a set of lightweight APIs that an object container (the EJB container) will support in order to provide transactions (using JTA), remote procedure calls (using RMI or RMI-IIOP), concurrency control, dependency injection and access control for business objects. This package contains the Enterprise JavaBeans classes and interfaces that define the contracts between the enterprise bean and its clients and between the enterprise bean and the ejb container.
This package contains the annotations and interfaces for the declarative validation support offered by the Bean Validation API. Bean Validation provides a unified way to provide constraints on beans (e.g. JPA model classes) that can be enforced cross-layer. In Java EE, JPA honors bean validation constraints in the persistence layer, while JSF does so in the view layer.
This package contains the contracts between a persistence provider and the managed classes and the clients of the Java Persistence API (JPA).
This package provides the Java Transaction API ("JTA") that contains the interfaces and annotations to interact with the transaction support offered by Java EE. Even though this API abstracts from the really low-level details, the interfaces are also considered somewhat low-level and the average application developer in Java EE is either assumed to be relying on transparent handling of transactions by the higher level EJB abstractions, or using the annotations provided by this API in combination with CDI managed beans.
This package provides the core of the Java Authentication SPI ("JASPIC") that contains the interfaces and classes to build authentication modules for secure Java EE applications. Authentication modules are responsible for the interaction dialog with a user (e.g. redirecting to a Form or to an OpenID provider), verifying the user's input (e.g. by doing an LDAP lookup, database query or contacting the OpenID provider with a token) and retrieving a set of groups/roles that the authenticated user is in or has (e.g. by again doing an LDAP lookup or database query).
This package provides the interfaces for interacting directly with Java EE's platform default managed thread pool. A higher-level executor service working on this same thread pool can be used optionally. The same interfaces can be used for user-defined managed thread pools, but this relies on vendor specific configuration and is not covered by the Java EE specification.
This package defines the Java Message Service ("JMS") API. The JMS API provides a common way for Java programs to create, send, receive and read an enterprise messaging system's messages.
This package defines the entry AP for Java EE Batch Applications. The Batch Applications API provides the means to run long running background tasks that possibly involve a large volume of data and which may need to be periodically executed.
This package defines the Java EE Connector Architecture ("JCA") API. Java EE Connector Architecture (JCA) is a Java-based technology solution for connecting application servers and enterprise information systems ("EIS") as part of enterprise application integration ("EAI") solutions. This is a low-level API aimed at vendors that the average application developer typically does not come in contact with.
Web profile.
In an attempt to limit the footprint of web containers, both in physical and in conceptual terms, the web profile was created, a subset of the Java EE specifications.
The Java EE web profile comprises the following:
Certified application servers.
Differences between implementations.
Although by definition all Java EE implementations provide the same base level of technologies (namely, the Java EE spec and the associated APIs), they can differ considerably with respect to extra features (like connectors, clustering, fault tolerance, high availability, security, etc.), installed size, memory footprint, startup time, etc.
Code sample.
The code sample shown below demonstrates how various technologies in Java EE 7 are used together to build a web form for editing a user.
In Java EE a (web) UI can be built using Servlet, JavaServer Pages ("JSP"), or JavaServer Faces ("JSF") with Facelets. The example below uses JSF and Facelets. Not explicitly shown is that the input components use the Java EE Bean Validation API under the covers to validate constraints.
Example Backing Bean class.
To assist the view, Java EE uses a concept called a "Backing Bean". The example below uses contexts and Dependency Injection (CDI) and Enterprise JavaBean ("EJB").
Example DAO class.
To implement business logic, Enterprise JavaBean ("EJB") is the dedicated technology in Java EE. For the actual persistence, JDBC or Java Persistence API ("JPA") can be used. The example below uses EJB and JPA. Not explicitly shown is that JTA is used under the covers by EJB to control transactional behavior.
Example Entity class.
For defining entity/model classes Java EE provides the Java Persistence API ("JPA"), and for expressing constraints on those entities it provides the Bean Validation API. The example below uses both these technologies.

</doc>
<doc id="42870" url="https://en.wikipedia.org/wiki?curid=42870" title="Java Platform, Micro Edition">
Java Platform, Micro Edition

Java Platform, Micro Edition, or Java ME, is a Java platform designed for embedded systems (such as mobile devices, for example). Target devices range from industrial controls to mobile phones (especially feature phones) and set-top boxes. Java ME was formerly known as Java 2 Platform, Micro Edition (J2ME).
Java ME was designed by Sun Microsystems, acquired by Oracle Corporation in 2010; the platform replaced a similar technology, PersonalJava. Originally developed under the Java Community Process as JSR 68, the different flavors of Java ME have evolved in separate JSRs. Sun provides a reference implementation of the specification, but has tended not to provide free binary implementations of its Java ME runtime environment for mobile devices, rather relying on third parties to provide their own.
As of 22 December 2006, the Java ME source code is licensed under the GNU General Public License, and is released under the project name phoneME.
As of 2008, all Java ME platforms are currently restricted to JRE 1.3 features and use that version of the class file format (internally known as version 47.0). Should Oracle ever declare a new round of Java ME configuration versions that support the later class file formats and language features, such as those corresponding to JRE 1.5 or 1.6 (notably, generics), it will entail extra work on the part of all platform vendors to update their JREs.
Java ME devices implement a "profile". The most common of these are the Mobile Information Device Profile aimed at mobile devices, such as cell phones, and the Personal Profile aimed at consumer products and embedded devices like set-top boxes and PDAs. Profiles are subsets of "configurations", of which there are currently two: the Connected Limited Device Configuration (CLDC) and the Connected Device Configuration (CDC).
There are more than 2.1 billion Java ME enabled mobile phones and PDAs. It is popular in sub $200 devices such as Nokia's Series 40. It was also used on the Bada operating system and on Symbian OS along with native software. Users of Windows CE, Windows Mobile, Maemo, MeeGo and Android can download Java ME for their respective environments.
Connected Limited Device Configuration.
The Connected Limited Device Configuration (CLDC) contains a strict subset of the Java-class libraries, and is the minimum amount needed for a Java virtual machine to operate. CLDC is basically used for classifying myriad devices into a fixed configuration.
A configuration provides the most basic set of libraries and virtual-machine features that must be present in each implementation of a J2ME environment. When coupled with one or more profiles, the Connected Limited Device Configuration gives developers a solid Java platform for creating applications for consumer and embedded devices.
The configuration is designed for devices with 160KB to 512KB total memory, which has a minimum of 160KB of ROM and 32KB of RAM available for the Java platform.
Mobile Information Device Profile.
Designed for mobile phones, the Mobile Information Device Profile includes a GUI, and a data storage API, and MIDP 2.0 includes a basic 2D gaming API. Applications written for this profile are called MIDlets. Almost all new cell phones come with a MIDP implementation, and it is now the de facto standard for downloadable cell phone games. However, many cellphones can run only those MIDlets that have been approved by the carrier, especially in North America.
"JSR 271: Mobile Information Device Profile 3" (Final release on 09 Dec, 2009) specified the 3rd generation Mobile Information Device Profile (MIDP3), expanding upon the functionality in all areas as well as improving interoperability across devices. A key design goal of MIDP3 is backward compatibility with MIDP2 content.
Information Module Profile.
The Information Module Profile (IMP) is a profile for embedded, "headless" devices such as vending machines, industrial embedded applications, security systems, and similar devices with either simple or no display and with some limited network connectivity.
Originally introduced by Siemens Mobile and Nokia as JSR-195, IMP 1.0 is a strict subset of MIDP 1.0 except that it doesn't include user interface APIs — in other words, it doesn't include support for the Java package codice_1. JSR-228, also known as IMP-NG, is IMP's next generation that is based on MIDP 2.0, leveraging MIDP 2.0's new security and networking types and APIs, and other APIs such as codice_2 and codice_3, but again it doesn't include UI APIs, nor the game API.
Connected Device Configuration.
The Connected Device Configuration is a subset of Java SE, containing almost all the libraries that are not GUI related. It is richer than CLDC.
Foundation Profile.
The Foundation Profile is a Java ME Connected Device Configuration (CDC) profile. This profile is intended to be used by devices requiring a complete implementation of the Java virtual machine up to and including the entire Java Platform, Standard Edition API. Typical implementations will use some subset of that API set depending on the additional profiles supported. This specification was developed under the Java Community Process.
Personal Basis Profile.
The Personal Basis Profile extends the Foundation Profile to include lightweight GUI support in the form of an AWT subset. This is the platform that BD-J is built upon.
Implementations.
Sun provides a reference implementation of these configurations and profiles for MIDP and CDC. Starting with the JavaME 3.0 SDK, a NetBeans-based IDE will support them in a single IDE.
In contrast to the numerous binary implementations of the Java Platform built by Sun for servers and workstations, Sun does not provide any binaries for the platforms of Java ME targets with the exception of an MIDP 1.0 JRE (JVM) for Palm OS. Sun provides no J2ME JRE for the Microsoft Windows Mobile (Pocket PC) based devices, despite an open-letter campaign to Sun to release a rumored internal implementation of PersonalJava known by the code name "Captain America". Third party implementations like JBlend and JBed are widely used by Windows Mobile vendors like HTC and Samsung.
Operating systems targeting Java ME have been implemented by DoCoMo in the form of DoJa, and by SavaJe as SavaJe OS. The latter company was purchased by Sun in April 2007 and now forms the basis of Sun's JavaFX Mobile. The company IS2T provides a Java ME virtual machine (MicroJvm) for any RTOS and even with no RTOS (then qualified as baremetal). When baremetal, the virtual machine is the OS/RTOS: the device boots in Java.
MicroEmu provides an open source (LGPL) implementation of an MIDP emulator. This is a Java Applet-based emulator and can be embedded in web pages.
The open-source Mika VM aims to implement JavaME CDC/FP, but is not certified as such (certified implementations are required to charge royalties, which is impractical for an open-source project). Consequently, devices which use this implementation are not allowed to claim JavaME CDC compatibility.
The linux-based Android operating system uses a proprietary version of Java that is similar, but not identical to Java Me.
ESR.
The ESR consortium is devoted to Standards for embedded Java. Especially cost effective Standards.
Typical applications domains are industrial control, machine-to-machine, medical, e-metering, home automation, consumer, human-to-machine-interface, ...

</doc>
<doc id="42871" url="https://en.wikipedia.org/wiki?curid=42871" title="Java Platform, Standard Edition">
Java Platform, Standard Edition

Java Platform, Standard Edition or Java SE is a widely used platform for development and deployment of portable code for desktop and server environments. Java SE uses the object-oriented Java programming language. It is part of the Java software platform family. Java SE defines a wide range of general purpose APIs – such as Java APIs for the Java Class Library – and also includes the Java Language Specification and the Java Virtual Machine Specification. One of the most well-known implementations of Java SE is Oracle Corporation's Java Development Kit (JDK).
Nomenclature, standards and specifications.
Java SE was known as Java 2 Platform, Standard Edition or J2SE from version 1.2 until version 1.5. The "SE" is used to distinguish the base platform from the Enterprise Edition (Java EE) and Micro Edition (Java ME) platforms. The "2" was originally intended to emphasize the major changes introduced in version 1.2, but was removed in version 1.6. The naming convention has been changed several times over the Java version history. Starting with J2SE 1.4 (Merlin), Java SE has been developed under the Java Community Process, which produces descriptions of proposed and final specifications for the Java platform called Java Specification Requests (JSR). JSR 59 was the umbrella specification for J2SE 1.4 and JSR 176 specified J2SE 5.0 (Tiger). Java SE 6 (Mustang) was released under JSR 270.
Java Platform, Enterprise Edition (Java EE) is a related specification that includes all the classes in Java SE, plus a number that are more useful to programs that run on servers as opposed to workstations.
Java Platform, Micro Edition (Java ME) is a related specification intended to provide a certified collection of Java APIs for the development of software for small, resource-constrained devices such as cell phones, PDAs and set-top boxes.
The Java Runtime Environment (JRE) and Java Development Kit (JDK) are the actual files downloaded and installed on a computer to run or develop Java programs, respectively.
General purpose packages.
java.lang.
The Java package contains fundamental classes and interfaces closely tied to the language and runtime system. This includes the root classes that form the class hierarchy, types tied to the language definition, basic exceptions, math functions, threading, security functions, as well as some information on the underlying native system. This package contains 22 of 32 codice_1 classes provided in JDK 6.
The main classes and interfaces in codice_2 are:
Classes in codice_2 are automatically imported into every source file.
java.lang.ref.
The package provides more flexible types of references than are otherwise available, permitting limited interaction between the application and the Java Virtual Machine (JVM) garbage collector. It is an important package, central enough to the language for the language designers to give it a name that starts with "java.lang", but it is somewhat special-purpose and not used by a lot of developers. This package was added in J2SE 1.2.
Java has an expressive system of references and allows for special behavior for garbage collection. A normal reference in Java is known as a "strong reference." The codice_7 package defines three other types of references — soft, weak, and phantom references. Each type of reference is designed for a specific use.
Each of these reference types extends the class, which provides the method to return a strong reference to the referent object (or codice_8 if the reference has been cleared or if the reference type is phantom), and the method to clear the reference.
The codice_7 also defines the class , which can be used in each of the applications discussed above to keep track of objects that have changed reference type. When a codice_10 is created it is optionally registered with a reference queue. The application polls the reference queue to get references that have changed reachability state.
java.lang.reflect.
Reflection is a constituent of the Java API that lets Java code examine and "reflect" on Java components at runtime and use the reflected members. Classes in the package, along with codice_11 and accommodate applications such as debuggers, interpreters, object inspectors, class browsers, and services such as object serialization and JavaBeans that need access to either the public members of a target object (based on its runtime class) or the members declared by a given class. This package was added in JDK 1.1.
Reflection is used to instantiate classes and invoke methods using their names, a concept that allows for dynamic programming. Classes, interfaces, methods, fields, and constructors can all be discovered and used at runtime. Reflection is supported by metadata that the JVM has about the program.
Techniques.
There are basic techniques involved in reflection:
Discovery.
Discovery typically starts with an object and calling the method to get the object's codice_12. The codice_12 object has several methods for discovering the contents of the class, for example:
Use by name.
The codice_12 object can be obtained either through discovery, by using the "class literal" (e.g. codice_19) or by using the name of the class (e.g. ). With a codice_12 object, member codice_21, codice_22, or codice_23 objects can be obtained using the symbolic name of the member. For example:
codice_21, codice_22, and codice_23 objects can be used to dynamically access the represented member of the class. For example:
Arrays and proxies.
The codice_46 package also provides an class that contains static methods for creating and manipulating array objects, and since J2SE 1.3, a class that supports dynamic creation of proxy classes that implement specified interfaces.
The implementation of a codice_47 class is provided by a supplied object that implements the interface. The codice_48's method is called for each method invoked on the proxy object—the first parameter is the proxy object, the second parameter is the codice_21 object representing the method from the interface implemented by the proxy, and the third parameter is the array of parameters passed to the interface method. The codice_39 method returns an codice_32 result that contains the result returned to the code that called the proxy interface method.
java.io.
The package contains classes that support input and output. The classes in the package are primarily stream-oriented; however, a class for random access files is also provided. The central classes in the package are and , which are abstract base classes for reading from and writing to byte streams, respectively. The related classes and are abstract base classes for reading from and writing to character streams, respectively. The package also has a few miscellaneous classes to support interactions with the host file system.
Streams.
The stream classes follow the decorator pattern by extending the base subclass to add features to the stream classes. Subclasses of the base stream classes are typically named for one of the following attributes:
The stream subclasses are named using the naming pattern codice_52 where codice_53 is the name describing the feature and codice_54 is one of codice_55, codice_56, codice_57, or codice_58.
The following table shows the sources/destinations supported directly by the codice_59 package:
Other standard library packages provide stream implementations for other destinations, such as the codice_55 returned by the method or the Java EE class.
Data type handling and processing or filtering of stream data is accomplished through stream filters. The filter classes all accept another compatible stream object as a parameter to the constructor and "decorate" the enclosed stream with additional features. Filters are created by extending one of the base filter classes , , , or .
The codice_57 and codice_58 classes are really just byte streams with additional processing performed on the data stream to convert the bytes to characters. They use the default character encoding for the platform, which as of J2SE 5.0 is represented by the returned by the static method. The class converts an codice_55 to a codice_57 and the class converts an codice_56 to a codice_58. Both these classes have constructors that support specifying the character encoding to use. If no encoding is specified, the program uses the default encoding for the platform.
The following table shows the other processes and filters that the codice_59 package directly supports. All these classes extend the corresponding codice_68 class.
Random access.
The class supports "random access" reading and writing of files. The class uses a "file pointer" that represents a byte-offset within the file for the next read or write operation. The file pointer is moved implicitly by reading or writing and explicitly by calling the or methods. The current position of the file pointer is returned by the method.
File system.
The class represents a file or directory path in a file system. codice_69 objects support the creation, deletion and renaming of files and directories and the manipulation of file attributes such as "read-only" and "last modified timestamp". codice_69 objects that represent directories can be used to get a list of all the contained files and directories.
The class is a file descriptor that represents a source or sink (destination) of bytes. Typically this is a file, but can also be a console or network socket. codice_71 objects are used to create codice_69 streams. They are obtained from codice_69 streams and codice_74 sockets and datagram sockets.
java.nio.
In J2SE 1.4, the package (NIO or New I/O) was added to support memory-mapped I/O, facilitating I/O operations closer to the underlying hardware with sometimes dramatically better performance. The codice_75 package provides support for a number of buffer types. The subpackage provides support for different character encodings for character data. The subpackage provides support for "channels," which represent connections to entities that are capable of performing I/O operations, such as files and sockets. The codice_76 package also provides support for fine-grained locking of files.
java.math.
The package supports multiprecision arithmetic (including modular arithmetic operations) and provides multiprecision prime number generators used for cryptographic key generation. The main classes of the package are:
java.net.
The package provides special IO routines for networks, allowing HTTP requests, as well as other common transactions.
java.text.
The package implements parsing routines for strings and supports various human-readable languages and locale-specific parsing.
java.util.
Data structures that aggregate objects are the focus of the package. Included in the package is the Collections API, an organized data structure hierarchy influenced heavily by the design patterns considerations.
Special purpose packages.
java.applet.
Created to support Java applet creation, the package lets applications be downloaded over a network and run within a guarded sandbox. Security restrictions are easily imposed on the sandbox. A developer, for example, may apply a digital signature to an applet, thereby labeling it as safe. Doing so allows the user to grant the applet permission to perform restricted operations (such as accessing the local hard drive), and removes some or all the sandbox restrictions. Digital certificates are issued by certificate authorities.
java.beans.
Included in the package are various classes for developing and manipulating beans, reusable components defined by the JavaBeans architecture. The architecture provides mechanisms for manipulating properties of components and firing events when those properties change.
The APIs in codice_80 are intended for use by a bean editing tool, in which beans can be combined, customized, and manipulated. One type of bean editor is a GUI designer in an integrated development environment.
java.awt.
The , or Abstract Window Toolkit, provides access to a basic set of GUI widgets based on the underlying native platform's widget set, the core of the GUI event subsystem, and the interface between the native windowing system and the Java application. It also provides several basic layout managers, a datatransfer package for use with the Clipboard and Drag and Drop, the interface to input devices such as mice and keyboards, as well as access to the system tray on supporting systems. This package, along with codice_81 contains the largest number of enums (7 in all) in JDK 6.
java.rmi.
The package provides Java remote method invocation to support remote procedure calls between two java applications running in different JVMs.
java.security.
Support for security, including the message digest algorithm, is included in the package.
java.sql.
An implementation of the JDBC API (used to access SQL databases) is grouped into the package.
javax.rmi.
The package provides the support for the remote communication between applications, using the RMI over IIOP protocol. This protocol combines RMI and CORBA features.
Java SE Core Technologies - CORBA / RMI-IIOP
javax.swing.
Swing is a collection of routines that build on codice_82 to provide a platform independent widget toolkit. uses the 2D drawing routines to render the user interface components instead of relying on the underlying native operating system GUI support.
This package contains the largest number of classes (133 in all) in JDK 6. This package, along with codice_82 also contains the largest number of enums (7 in all) in JDK 6. It supports pluggable looks and feels (PLAFs) so that widgets in the GUI can imitate those from the underlying native system. Design patterns permeate the system, especially a modification of the model-view-controller pattern, which loosens the coupling between function and appearance. One inconsistency is that (as of J2SE 1.3) fonts are drawn by the underlying native system, and not by Java, limiting text portability. Workarounds, such as using bitmap fonts, do exist. In general, "layouts" are used and keep elements within an aesthetically consistent GUI across platforms.
javax.swing.text.html.parser.
The package provides the error tolerant HTML parser that is used for writing various web browsers and web bots.
javax.xml.bind.annotation.
The package contains the largest number of Annotation Types (30 in all) in JDK 6. It defines annotations for customizing Java program elements to XML Schema mapping.
OMG packages.
org.omg.CORBA.
The package provides the support for the remote communication between applications using the General Inter-ORB Protocol and supports other features of the common object request broker architecture. Same as RMI and RMI-IIOP, this package is for calling remote methods of objects on other virtual machines (usually via network).
This package contains the largest number of codice_84 classes (45 in all) in JDK 6. From all communication possibilities CORBA is portable between various languages; however, with this comes more complexity.
org.omg.PortableInterceptor.
The package contains the largest number of interfaces (39 in all) in JDK 6. It provides a mechanism to register ORB hooks through which ORB services intercept the normal flow of execution of the ORB.
Critical security issues with the Java SE plugin.
Several critical security vulnerabilities have been reported, the most recent in January 2013. Security alerts from Oracle announce critical security-related patches to Java SE.

</doc>
<doc id="42874" url="https://en.wikipedia.org/wiki?curid=42874" title="Elia Kazan">
Elia Kazan

Elia Kazan (born Elias Kazantzoglou, September 7, 1909 – September 28, 2003) was an American director, producer, writer and actor, described by "The New York Times" as "one of the most honored and influential directors in Broadway and Hollywood history".
He was born in Istanbul, to Cappadocian Greek parents. After studying acting at Yale, he acted professionally for eight years, later joining the Group Theater in 1932, and co-founded the Actors Studio in 1947. With Robert Lewis and Cheryl Crawford, his actors' studio introduced "Method Acting" under the direction of Strasberg. Kazan acted in a few films, including "City for Conquest" (1940).
Noted for drawing out the best dramatic performances from his actors, he directed 21 actors to Oscar nominations, resulting in nine wins. He directed a string of successful films, including "A Streetcar Named Desire" (1951), "On the Waterfront" (1954), and "East of Eden" (1955). During his career, he won two Oscars as Best Director and received an Honorary Oscar, won three Tony Awards, and four Golden Globes.
His films were concerned with personal or social issues of special concern to him. Kazan writes, "I don't move unless I have some empathy with the basic theme." His first such "issue" film was "Gentleman's Agreement" (1947), with Gregory Peck, which dealt with anti-Semitism in America. It received 8 Oscar nominations and 3 wins, including Kazan's first for Best Director. It was followed by "Pinky", one of the first films in mainstream Hollywood to address racial prejudice against black people. In 1954, he directed "On the Waterfront", a film about union corruption on the New York harbor waterfront. "A Streetcar Named Desire" (1951), an adaptation of the stage play which he had also directed, received 12 Oscar nominations, winning 4, and was Marlon Brando's breakthrough role. In 1955, he directed John Steinbeck's "East of Eden", which introduced James Dean to movie audiences.
A turning point in Kazan's career came with his testimony as a witness before the House Committee on Un-American Activities in 1952 at the time of the Hollywood blacklist, which brought him strong negative reactions from many liberal friends and colleagues. His testimony helped end the careers of former acting colleagues Morris Carnovsky and Art Smith, along with ending the work of playwright Clifford Odets. Kazan later justified his act by saying he took "only the more tolerable of two alternatives that were either way painful and wrong." Nearly a half-century later, his anti-Communist testimony continued to cause controversy. When Kazan was awarded an honorary Oscar in 1999, dozens of actors chose not to applaud as 250 demonstrators picketed the event.
Kazan influenced the films of the 1950s and '60s with his provocative, issue-driven subjects. Director Stanley Kubrick called him, "without question, the best director we have in America, capable of performing miracles with the actors he uses." Film author Ian Freer concludes that "if his achievements are tainted by political controversy, the debt Hollywood—and actors everywhere—owes him is enormous." In 2010, Martin Scorsese co-directed the documentary film "A Letter to Elia" as a personal tribute to Kazan.
Early life.
Elia Kazan was born in the Fener district of Istanbul, to Cappadocian Greek parents originally from Kayseri in Anatolia. His parents, George and Athena Kazantzoglou ("née" Shishmanoglou), emigrated to the United States when he was four years old. He was named after his paternal grandfather, Elia Kazantzoglou. His maternal grandfather was Isaak Shishmanoglou. Elia's brother, Avraam, was born in Berlin and later became a psychiatrist.
As a young boy, he was remembered as being shy, and his college classmates described him as more of a loner. Much of his early life was portrayed in his autobiographical book, "America America", which he made into a film in 1963. In it, he describes his family as "alienated" from both their parents' Greek Orthodox values and from those of mainstream America. His mother's family were cotton merchants who imported cotton from England, and sold it wholesale. His father became a rug merchant after emigrating to the United States, and expected that his son would go into the family business.
After attending public schools in New York, he enrolled at Williams College in Massachusetts, where he helped pay his way by waiting tables and washing dishes, although he still graduated cum laude. He also worked as a bartender at various fraternities, but never joined one. While a student at Williams, he earned the nickname "Gadg," for Gadget, because, he said, "I was small, compact, and handy to have around." The nickname was eventually taken up by his stage and film stars.
In "America America" he tells how, and why, his family left Turkey and moved to America. Kazan notes that much of it came from stories that he heard as a young boy. He says during an interview that "it's all true: the wealth of the family was put on the back of a donkey, and my uncle, really still a boy, went to Istanbul ... to gradually bring the family there to escape the oppressive circumstances... It's also true that he lost the money on the way, and when he got there he swept rugs in a little store."
Kazan notes some of the controversial aspects of what he put in the film. He writes, "I used to say to myself when I was making the film that America was a dream of total freedom in all areas." To make his point, the character who portrays Kazan's uncle Avraam kisses the ground when he gets through customs, while the Statue of Liberty and the American flag are in the background. Kazan had considered whether that kind of scene might be too much for American audiences:
Before undertaking the film, Kazan wanted to confirm many of the details about his family's background. At one point, he sat his parents down and recorded their answers to his questions. He remembers eventually asking his father a "deeper question: 'Why America? What were you hoping for?'" His mother gave him the answer, however: "A.E. brought us here." Kazan states that "A.E. was my uncle Avraam Elia, the one who left the Anatolian village with the donkey. At twenty-eight, somehow—this was the wonder—he made his way to New York. He sent home money and in time brought my father over. Father sent for my mother and my baby brother and me when I was four."
Kazan writes of the movie, "It's my favorite of all the films I've made; the first film that was entirely mine."
Stage career.
Group Theater.
In 1932, after spending two years at the Yale University School of Drama, he moved to New York City to become a professional stage actor. His first opportunity came with a small group of actors engaged in presenting plays containing "social commentary". They were called the Group Theater, which showcased many lesser known plays with deep social or political messages. After struggling to be accepted by them, he discovered his first strong sense of self in America within the "family of the Group Theater, and more loosely in the radical social and cultural movements of the time," writes film author Joanna E. Rapf.
In Kazan's autobiography, Kazan writes of the "lasting impact on him of the Group," noting in particular, Lee Strasberg and Harold Clurman as "father figures", along with his close friendship with playwright Clifford Odets. Kazan, during an interview with Michel Ciment, describes the Group:
Kazan, in his autobiography, also describes Strasberg as a vital leader of the group:
Kazan's first national success came as New York theatrical director. Although initially he worked as an actor on stage, and told early in his acting career that he had no acting ability, he surprised many critics by becoming one of the Group’s most capable actors. In 1935 he played the role of a strike-leading taxi driver in a drama by Clifford Odets, "Waiting for Lefty", and his performance was called "dynamic," leading some to describe him as the "proletarian thunderbolt."
Among the themes that would run through all of his work were "personal alienation and an outrage over social injustice", writes film critic William Baer. Other critics have likewise noted his "strong commitment to the social and social psychological—rather than the purely political—implications of drama".
By the mid-1930s, when he was 26, he began directing a number of the Group Theater's plays, including Robert Ardrey's well-known play Thunder Rock. In 1942 he achieved his first notable success by directing a play by Thornton Wilder, "The Skin of Our Teeth", starring Tallulah Bankhead and Fredric March. The play, though controversial, was a critical and commercial success and won Wilder a Pulitzer Prize. Kazan won the New York Drama Critics Award for Best Director and Bankhead for best actress. Kazan then went on to direct "Death of a Salesman" by Arthur Miller, and then directed "A Streetcar Named Desire" by Tennessee Williams, both of which were also successful. Kazan's wife, Molly Thacher, the reader for the Group, discovered Williams and awarded him a "prize that launched his career."
The Group Theater's summer rehearsal headquarters was at Pine Brook Country Club, located in the countryside of Nichols, Connecticut, during the 1930s and early 1940s. Along with Kazan were numerous other artists: Harry Morgan, John Garfield, Luise Rainer, Frances Farmer, Will Geer, Howard Da Silva, Clifford Odets, Lee J. Cobb and Irwin Shaw.
Actors Studio.
In 1947, he founded the Actors Studio, a non-profit workshop, with actors Robert Lewis and Cheryl Crawford. In 1951, Lee Strasberg became its director, and it remained a non-profit enterprise. Strasberg introduced the "Method" to the Actors Studio, an umbrella term for a constellation of systemizations of Konstantin Stanislavsky's teachings. The "Method" school of acting became the predominant system of post-WWII Hollywood, though it has waned in influence somewhat since.
Among the students of Strasberg were Marlon Brando, Montgomery Clift, Mildred Dunnock, Julie Harris, Karl Malden, Patricia Neal, Maureen Stapleton, Eli Wallach, and James Whitmore. 
Kazan directed two of the Studio's protegee's, Karl Malden and Marlon Brando, in the Tennessee Williams play "A Streetcar Named Desire".
Film career.
Though at the height of his stage success, Kazan turned to Hollywood as a director of motion pictures. He first directed two short films, but his first feature film was "A Tree Grows in Brooklyn" (1945), one of his first attempts to film dramas focused on contemporary concerns, which later became his forte. Two years later he directed "Gentleman's Agreement", where he tackled a seldom-discussed topic in America, antisemitism, for which he won his first Oscar as Best Director. In 1949 he again dealt with controversial subject when he directed "Pinky", which dealt with issues of racism in America, and was nominated for 3 Academy Awards.
In 1947, he directed the courtroom drama "Boomerang!", and in 1950 he directed "Panic in the Streets", starring Richard Widmark, in a thriller shot on the streets of New Orleans. In that film, Kazan experimented with a documentary style of cinematography, which succeeded in "energizing" the action scenes. He won the Venice Film Festival, International Award as director, and the film also won two Academy Awards. Kazan had requested that Zero Mostel also act in the film, despite Mostel being "blacklisted" as a result of HUAC testimony a few years earlier. Kazan writes of his decision:
In 1951, after introducing and directing Marlon Brando and Karl Malden in the stage version, he went on to cast both in the film version of the play, "A Streetcar Named Desire", which won 4 Oscars, being nominated for 12.
Despite these plaudits, the film was considered a step back cinematically with the feel of filmed theater, though Kazan did at first use a more open setting but then felt compelled to revert to the stage atmosphere to remain true to the script. He explains:
Kazan's next film was "Viva Zapata!" (1952) which also starred Marlon Brando. This time the film added real atmosphere with the use of location shots and strong character accents. Kazan called this his "first real film" because of those factors.
In 1954 he again used Brando as a star in "On the Waterfront". As a continuation of the socially relevant themes that he developed in New York, the film exposed corruption within New York's longshoremen's union. It too was nominated for 12 Academy Awards, and won 8, including Best Picture, Best Director and Best Actor, for Marlon Brando.
"On the Waterfront" was also the screen debut for Eva Marie Saint, who won the Oscar for Best Supporting Actress for her role. Saint recalls that Kazan selected her for the role after he had her do an improvisational skit with Brando playing the other character. She had no idea that he was looking to fill any particular film part, however, but remembers that Kazan set up the scenario with Brando which brought out surprising emotions:
"Life" magazine described "On the Waterfront" as the "most brutal movie of the year" but with "the year's tenderest love scenes," and stating that Saint was a "new discovery" in films. In its cover story about Saint, it speculated that it will probably be as Edie in "On the Waterfront" that she "starts her real trip to fame."
The film made use of extensive on-location street scenes and waterfront shots, and included a notable score by noted composer Leonard Bernstein.
After the success of "On the Waterfront" he went on to direct the screen adaptation of John Steinbeck's novel, "East of Eden" in 1955. As director, Kazan again used another unknown actor, James Dean. Kazan had seen Dean on stage in New York and after an audition gave him the starring role along with an exclusive contract with Warner Bros. Dean flew back to Los Angeles with Kazan in 1954, the first time he had ever flown in a plane, bringing his clothes in a brown paper bag. The film's success introduced James Dean to the world and established him as a popular actor. He went on to star in "Rebel Without a Cause" (1955), directed by Kazan's friend, Nicholas Ray, and then "Giant", (dir. George Stevens, 1956)
Author Douglas Rathgeb describes the difficulties Kazan had in turning Dean into a new star, noting how Dean was a controversial figure at Warner Bros. from the time he arrived. There were rumors that he "kept a loaded gun in his studio trailer; that he drove his motorcycle dangerously down studio streets or sound stages; that he had bizarre and unsavory friends." As a result, Kazan was forced to "baby-sit the young actor in side-by-side trailers," so he wouldn't run away during production. Co-star Julie Harris worked overtime to quell Dean's panic attacks. In general, Dean was oblivious to Hollywood's methods, and Rathgeb notes that "his radical style did not mesh with Hollywood's corporate gears."
Dean himself was amazed at his own performance on screen when he later viewed a rough cut of the film. Kazan had invited director Nicholas Ray to a private showing, with Dean, as Ray was looking for someone to play the lead in "Rebel Without a Cause". Ray watched Dean's powerful acting on the screen; but it didn't seem possible that it was the same person in the room. Ray felt Dean was shy and totally withdrawn as he sat there hunched over. "Dean himself did not seem to believe it," notes Rathgeb. "He watched himself with an odd, almost adolescent fascination, as if he were admiring someone else."
The film also made good use of on-location and outdoor scenes, along with an effective use of early widescreen format, making the film one of Kazan's most accomplished works. James Dean died the following year, at the age of 24, in an accident with his sports car outside of Los Angeles. He had only made three films, and the only completed film he ever saw was "East of Eden".
In 1961, he introduced Warren Beatty in his first screen appearance with a starring role in "Splendor in the Grass" (1961), with Natalie Wood; the film was nominated for two Oscars and won one. Author Peter Biskind points out that Kazan "was the first in a string of major directors Beatty sought out, mentors or father figures from whom he wanted to learn." Biskind notes also that they "were wildly dissimilar—mentor vs. protege, director vs. actor, immigrant outsider vs. native son. Kazan was armed with the confidence born of age and success, while Beatty was virtually aflame with the arrogance of youth." Kazan recalls his impressions of Beatty:
Biskind describes an episode during the first week of shooting, where Beatty was angered at something Kazan said: "The star lashed out at the spot where he knew Kazan was most vulnerable, the director's friendly testimony before the HUAC. He snapped, 'Lemme ask you something—why did you name all those names?'"
Beatty himself recalled the episode: "In some patricidal attempt to stand up to the great Kazan, I arrogantly and stupidly challenged him on it." Biskind describes how "Kazan grabbed his arm, asking, 'What did you say?' and dragged him off to a tiny dressing room ... whereupon the director proceeded to justify himself for two hours." Beatty, years later, during a Kennedy Center tribute to Kazan, stated to the audience that Kazan "had given him the most important break in his career."
Beatty's costar, Natalie Wood, was in a transition period in her career, having mostly been cast in roles as a child or teenager, and she was now hoping to be cast in adult roles. Biographer Suzanne Finstad notes that a "turning point" in her life as an actress was upon seeing the film "A Streetcar Named Desire": "She was transformed, in awe of Kazan and of Vivien Leigh's performance... became a role model for Natalie." In 1961, after a "series of bad films, her career was already in decline," notes Rathgeb. Kazan himself writes that the "sages" of the film community declared her as "washed up" as an actress, although he still wanted to interview her for his next film:
Kazan cast her as the female lead in "Splendor in the Grass", and her career rebounded. Finstad feels that despite Wood never receiving training in Method acting techniques, "working with Kazan brought her to the greatest emotional heights of her career. The experience was exhilarating but wrenching for Natalie, who faced her demons on "Splendor."" She adds that a scene in the film, as a result of "Kazan's wizardry ... produced a hysteria in Natalie that may be her most powerful moment as an actress."
Actor Gary Lockwood, who also acted in the film, felt that "Kazan and Natalie were a terrific marriage, because you had this beautiful girl, and you had somebody that could get things out of her." Kazan's favorite scene in the movie was the last one, when Wood goes back to see her lost first love, Bud (Beatty). "It's terribly touching to me. I still like it when I see it," writes Kazan. "And I certainly didn't need to tell her how to play it. She understood it perfectly."
Screenwriters.
Kazan was noted for his close collaboration with screenwriters. On Broadway, he worked with Arthur Miller, Tennessee Williams, and William Inge; in film, he worked again with Willams ("A Streetcar Named Desire" and "Baby Doll"), Inge ("Splendor in the Grass"), Budd Schulberg ("On the Waterfront" and "A Face in the Crowd"), John Steinbeck ("Viva Zapata!"), and Harold Pinter ("The Last Tycoon"). As an instrumental figure in the careers of many of the best writers of his time, "he always treated them and their work with the utmost respect." In 2009, a previously unproduced screenplay by Williams, "The Loss of a Teardrop Diamond", was released as a film. Williams wrote the screenplay specifically for Kazan to direct during the 1950s.
Williams became one of Kazan's closest and most loyal friends, and Kazan often pulled Williams out of "creative slumps" by redirecting his focus with new ideas. In 1959, in a letter to Kazan, he writes, "Some day you will know how much I value the great things you did with my work, how you lifted it above its measure by your great gift."
Among Kazan's other films were "Panic in the Streets" (1950), "East of Eden" (1955), "Baby Doll" (1956), "Wild River" (1960), and "The Last Tycoon" (1976).
Literary career.
In between his directing work he wrote four best-selling novels, including "America, America", and "The Arrangement", both of which tell the story of Kazan's Greek immigrant ancestors. Both novels were later made into films.
Directing style.
Preference for unknown actors.
Kazan strove for "cinematic realism," a quality he often achieved by discovering and working with unknown actors, many of whom treated him as their mentor, which gave him the flexibility to depict "social reality with both accuracy and vivid intensity." He also felt that casting the right actors accounted for 90% of a movie's ultimate success or failure. As a result of his efforts, he also gave actors such as Lee Remick, Jo Van Fleet, Warren Beatty, Andy Griffith, James Dean, and Jack Palance, their first major movie roles. He explained to director and producer George Stevens, Jr. that he felt that "big stars are barely trained or not very well trained. They also have bad habits... they're not pliable anymore." Kazan also describes how and why he gets to know his actors on a personal level:
Kazan goes on to describe how he got to understand James Dean, as an example:
Topics of personal and social realism.
Kazan chooses his subjects to express personal and social events that he is familiar with. He describes his thought process before taking on a project:
Film historian Joanna E. Rapf notes that among the methods Kazan used in his work with actors, was his initial focus on "reality", although his style was not defined as "naturalistic." She adds: "He respects his script, but casts and directs with a particular eye for expressive action and the use of emblematic objects." Kazan himself states that "unless the character is somewhere in the actor himself, you shouldn't cast him."
In his later years he changed his mind about some of the philosophy behind the Group Theater, in that he no longer felt that the theater was a
"collective art," as he once believed:
Film author Peter Biskind described Kazan's career as "fully committed to art and politics, with the politics feeding the work." Kazan, however, has downplayed that impression:
Nonetheless, there have been clear messages in some of his films that involved politics in various ways. In 1954, he directed "On the Waterfront", written by screenwriter Budd Schulberg, which was a film about union corruption in New York. Some critics consider it "one of the greatest films in the history of international cinema." Another political film was "A Face in the Crowd" (1957). His protagonist, played by Andy Griffith (in his film debut) is not a politician, yet his career suddenly becomes deeply involved in politics. According to film author Harry Keyishian, Kazan and screenwriter Budd Schulberg were using the film to warn audiences about the dangerous potential of the new medium of television. Kazan explains that he and Schulberg were trying to warn "of the power TV would have in the political life of the nation." Kazan states, "Listen to what the candidate says; don't be taken in by his charm or his trust-inspiring personality. Don't buy the advertisement; buy what's in the package."
Use of "Method" acting.
As a product of the Group Theater and Actors Studio, he was most noted for his use of "Method" actors, especially Brando and Dean. During an interview in 1988, Kazan said, "I did whatever was necessary to get a good performance "including" so-called Method acting. I made them run around the set, I scolded them, I inspired jealousy in their girlfriends... The director is a "desperate beast!" ... You don't deal with actors as dolls. You deal with them as people who are poets to a certain degree." Actor Robert De Niro called him a "master of a new kind of psychological and behavioral faith in acting."
Kazan was aware of the limited range of his directing abilities:
He explained that he tried to inspire his actors to offer ideas:
Kazan, however, held strong ideas about the scenes, and would try to merge an actor's suggestions and inner feelings with his own. Despite the strong eroticism created in "Baby Doll", for example, he set limits. Before shooting a seduction scene between Eli Wallach and Carroll Baker, he privately asked Wallach, "Do you think you actually go through with seducing that girl?" Wallach writes, "I hadn't thought about that question before, but I answered ... 'No.'" Kazan replies, "Good idea, play it that way." Kazan, many years later, explained his rationale for scenes in that film:
Being an "actor's director".
Joanna Rapf adds that Kazan was most admired for his close work with actors, noting that director Nicholas Ray considered him "the best actor's director the United States has ever produced." Film historian Foster Hirsch explains that "he created virtually a new acting style, which was the style of the Method... allowed for the actors to create great depth of psychological realism."
Among the actors who describe Kazan as an important influence in their career were Patricia Neal, who co-starred with Andy Griffith in "A Face in the Crowd" (1957): "He was very good. He was an actor and he knew how we acted. He would come and talk to you privately. I liked him a lot." Anthony Franciosa, a supporting actor in the film, explains how Kazan encouraged his actors:
However, in order to get quality acting from Andy Griffith, in his first screen appearance, and achieve what Schickel calls "an astonishing movie debut," Kazan would often take surprising measures. In one important and highly emotional scene, for example, Kazan had to give Griffith fair warning: "I may have to use extraordinary means to make you do this. I may have to get out of line. I don't know any other way of getting an extraordinary performance out of an actor."
Actress Terry Moore calls Kazan her "best friend," and notes that "he made you feel better than you thought you could be. I never had another director that ever touched him. I was spoiled for life." "He would find out if your life was like the character," says Carroll Baker, star of "Baby Doll", "he was the best director with actors."
Kazan's need to remain close to his actors continued up to his last film, "The Last Tycoon" (1976). He remembers that Robert De Niro, the star of the film, "would do almost anything to succeed," and even cut his weight down from 170 to 128 pounds for the role. Kazan adds that De Niro "is one of a select number of actors I've directed who work hard at their trade, and the only one who asked to rehearse on Sundays. Most of the others play tennis. Bobby and I would go over the scenes to be shot."
The powerful dramatic roles Kazan brought out from many of his actors was due, partly, to his ability to recognize their personal character traits. Although he didn't know De Niro before this film, for example, Kazan later writes, "Bobby is more meticulous ... he's very imaginative. He's very precise. He figures everything out both inside and outside. He has good emotion. He's a character actor: everything he does he calculates. In a good way, but he calculates." Kazan developed and used those personality traits for his character in the film. Although the film did poorly at the box office, some reviewers praised De Niro's acting. Film critic Marie Brenner writes that "for De Niro, it is a role that surpasses even his brilliant and daring portrayal of Vito Corleone in "The Godfather, part II", ... performance deserves to be compared with the very finest."
Marlon Brando, in his autobiography, goes into detail about the influence Kazan had on his acting:
HUAC testimony.
Until his death, Kazan remained controversial in some circles for testimony he gave before the House Committee on Un-American Activities (HUAC) in 1952, a period that many, such as journalist Michael Mills, feel was "the most controversial period in Hollywood history." When he was in his mid-20s, during the Depression years 1934 to 1936, he had been a member of the American Communist Party in New York, for a year and a half.
In April 1952, the Committee called on Kazan, under oath, to identify Communists from that period 16 years earlier. Kazan initially refused to provide names, but eventually named eight former Group Theater members who he said had been Communists: Clifford Odets, J. Edward Bromberg, Lewis Leverett, Morris Carnovsky, Phoebe Brand, Tony Kraber, Ted Wellman, and Paula Miller, who later married Lee Strasberg. He testified that Odets quit the party at the same time that he did. All the persons named were already known to HUAC, however. The move cost Kazan many friends within the film industry, including playwright Arthur Miller.
Kazan would later write in his autobiography of the "warrior pleasure at withstanding his 'enemies.'"
When Kazan received an Honorary Academy Award in 1999, the audience was noticeably divided in their reaction, with some including Nick Nolte, Ed Harris, Ian McKellen and Amy Madigan refusing to applaud, and many others, such as actors Kathy Bates, Meryl Streep and Warren Beatty and producer George Stevens, Jr. standing and applauding. Stevens speculates on why he, Beatty, and many others in the audience chose to stand and applaud:
"Los Angeles Times" film critic Kenneth Turan, agreed, writing "The only criterion for an award like this is the work". Kazan was already "denied accolades" from the American Film Institute, and other film critics associations. According to Mills, "It's time for the Academy to recognize this genius," adding that "We applauded when the great Chaplin finally had his hour." In response, former vice president of the Los Angeles Film Critics Association, Joseph McBride, claimed that an honorary award recognizes "the totality of what he represents, and Kazan's career, post 1952, was built on the ruin of other people's careers."
In later interviews, Kazan explained some of the early events that made him decide to become a friendly witness, most notably in relation to the Group Theater, which he called his first "family," and the "best thing professionally" that ever happened to him:
Mills notes that prior to becoming a "friendly witness," Kazan discussed the issues with Miller:
Miller put his arm around Kazan and retorted, "don’t worry about what I’ll think. Whatever you do is okay with me, because I know that your heart is in the right place."
In his memoirs, Kazan writes that his testimony meant that "the big shot had become the outsider." He also notes that it strengthened his friendship with another outsider, Tennessee Williams, with whom he collaborated on numerous plays and films. He called Williams "the most loyal and understanding friend I had through those black months."
Personal life.
Elia Kazan was married three times. His first wife was playwright Molly Day Thacher. They were married from 1932 until her death in 1963; this marriage produced two daughters and two sons, including screenwriter Nicholas Kazan. His second marriage, to the actress Barbara Loden, lasted from 1969 until her death in 1980, and produced one son. His marriage, in 1982, to Frances Rudge continued until his death, in 2003, aged 94.
In 1978, the U.S. government paid for Kazan and his family to travel to Kazan's birthplace where many of his films were to be shown. During a speech in Athens, he discussed his films and his personal and business life in the U.S., along with the messages he tried to convey:
He also offered his opinions about the role of the U.S. as a world model for democracy:
Elia Kazan died from natural causes in his Manhattan apartment, September 28, 2003 aged 94.
Legacy.
Kazan became known as an "actor's director" because he was able to elicit some of the best performances in the careers of many of his stars. Under his direction, his actors received 21 Academy Award nominations and won nine Oscars.
He won as Best Director for "Gentleman's Agreement" (1947) and for "On the Waterfront" (1954). Both "A Streetcar Named Desire" (1951) and "On the Waterfront" were nominated for twelve Academy Awards, respectively winning four and eight.
Kazan never lost his identification with the oppressed people he remembered from the depths of the Great Depression. With his many years with the Group Theater and Actors Studio in New York City and later triumphs on Broadway, he became famous "for the power and intensity of his actors' performances." He was the pivotal figure in launching the film careers of Marlon Brando, James Dean, Julie Harris, Eva Marie Saint, Warren Beatty, Lee Remick, Karl Malden, and many others. Seven of Kazan's films won a total of 20 Academy Awards. Dustin Hoffman commented that he "doubted whether he, Robert De Niro, or Al Pacino, would have become actors without Mr. Kazan's influence."
Upon his death, at the age of 94, the "New York Times" described him as "one of the most honored and influential directors in Broadway and Hollywood history." The "Death of a Salesman" and "A Streetcar Named Desire", two plays he directed, are considered to be some of the greatest of the 20th century. Although he became a respected director on Broadway, he made an equally impressive transition into one of the major film directors of his time. Critic William Baer notes that throughout his career "he constantly rose to the challenge of his own aspirations", adding that "he was a pioneer and visionary who greatly affected the history of both stage and cinema". Certain of his film-related material and personal papers are contained in the Wesleyan University Cinema Archives to which scholars and media experts from around the world may have full access.
His controversial stand during his testimony in front of the House Committee on Un-American Activities (HUAC) in 1952, became the low point in his career, although he remained convinced that he made the right decision to give the names of Communist Party members. He stated in an interview in 1976:
During his career, Kazan won both Tony and Oscar Awards for directing on stage and screen. In 1982, President Ronald Reagan presented him with the Kennedy Center honors award, a national tribute for lifetime achievement in the arts. At the ceremony, screenwriter Budd Schulberg, who wrote "On the Waterfront", thanks his lifelong friend saying, "Elia Kazan has touched us all with his capacity to honor not only the heroic man, but the hero in every man."
In 1999, when he was 90 years old, Kazan received an honorary Oscar for lifetime achievement. During the ceremony, he was accompanied by Martin Scorsese and Robert De Niro. The propriety of such an honor for Kazan who "named names" at the HUAC hearings remains contentious. Many in Hollywood felt that enough time had passed since the hearings that it was appropriate to recognize Kazan's accomplishments in this manner, although others in attendance of the ceremony did not and would not applaud. Kazan appreciated the award:
Martin Scorsese has directed a film documentary, "A Letter to Elia" (2010), considered to be an "intensely personal and deeply moving tribute" to Kazan. Scorsese was "captivated" by Kazan's films as a young man, and the documentary mirrors his own life story while he also credits Kazan as the inspiration for his becoming a filmmaker. It won a Peabody Award in 2010.
Awards and honors.
In addition to these awards, Kazan has a star on the Hollywood Walk of Fame, which is located on 6800 Hollywood Boulevard. He is also a member of the American Theater Hall of Fame.

</doc>
<doc id="42877" url="https://en.wikipedia.org/wiki?curid=42877" title="Ninety-ninety rule">
Ninety-ninety rule

In computer programming and software engineering, the ninety-ninety rule is a humorous aphorism that states:
This adds up to 180% in a wry allusion to the notoriety of software development projects significantly over-running their schedules (see software development effort estimation). It expresses both the rough allocation of time to easy and hard portions of a programming project and the cause of the lateness of many projects as failure to anticipate the hard parts. In other words, it takes both more time and more coding than expected to make a project work.
The rule is attributed to Tom Cargill of Bell Labs and was made popular by Jon Bentley's September 1985 "Programming Pearls" column in "Communications of the ACM", in which it was titled the "Rule of Credibility".
In some agile software projects, this rule also surfaces when a task is portrayed as "relatively done". This indicates a common scenario where planned work is completed but cannot be signed off, pending a final activity which may not occur for a substantial amount of time.

</doc>
<doc id="42878" url="https://en.wikipedia.org/wiki?curid=42878" title="Nevil Maskelyne">
Nevil Maskelyne

The Reverend Dr Nevil Maskelyne FRS (6 October 1732 – 9 February 1811) was the fifth British Astronomer Royal. He held the office from 1765 to 1811.
Biography.
Maskelyne was born in London, the third son of Edmund Maskelyne of Purton, Wiltshire. Maskelyne's father died when he was 12, leaving the family in reduced circumstances. Maskelyne attended Westminster School and was still a pupil there when his mother died in 1748. His interest in astronomy had begun while at Westminster School, shortly after the eclipse of 14 July 1748.
Maskelyne entered St Catharine's College, Cambridge in 1749, graduating as seventh wrangler in 1754. Ordained as a minister in 1755, he became a fellow of Trinity College, Cambridge in 1756 and a Fellow of the Royal Society in 1758.
About 1785 Maskelyne married Sophia Rose of Cotterstock, Northamptonshire. Their only child, Margaret (1786–1858), was the mother of Mervyn Herbert Nevil Story-Maskelyne (1823–1911) professor of mineralogy at Oxford (1856–95). Maskelyne's sister, Margaret (1735-1817), married Robert Clive.
Nevil Maskelyne is buried in the churchyard of St Mary the Virgin, the parish church of the village of Purton, Wiltshire, England.
Career.
Measurement of longitude.
In 1760 the Royal Society appointed Maskelyne as an astronomer on one of their expeditions to observe the 1761 transit of Venus. He and Robert Waddington were sent to the island of St. Helena. This was an important observation since accurate measurements would allow the accurate calculation of Earth's distance from the Sun, which would in turn allow the actual rather than the relative scale of the solar system to be calculated. This would allow, it was argued, the production of more accurate astronomical tables, in particular those predicting the motion of the Moon.
Bad weather prevented observation of the transit; however, Maskelyne used his journey to trial a method of determining longitude using the position of the moon, which became known as the lunar distance method. He returned to England, resuming his position as curate at Chipping Barnet in 1761, and began work on a book, publishing the lunar-distance method of longitude calculation and providing tables to facilitate its use in 1763 in "The British Mariner's Guide", which included the suggestion that to facilitate the finding of longitude at sea, lunar distances should be calculated beforehand for each year and published in a form accessible to navigators.
In 1763 the Board of Longitude sent Maskelyne to Barbados in order to carry out an official trial of three contenders for a Longitude reward. He was to carry out observations on board ship and to calculate the longitude of the capital, Bridgetown by observation of Jupiter's satellites. The three methods on trial were John Harrison's sea watch (now known as H4), Tobias Mayer's lunar tables and a marine chair made by Christopher Irwin, intended to help observations of Jupiter's satellites on board ship. Both Harrison's watch and lunar-distance observations based on Mayer's lunar tables produced results within the terms of the Longitude Act, although the former appeared to be more accurate. Harrison's watch had produced Bridgetown's longitude with an error of less than ten miles, while the lunar-distance observations were accurate to within 30 nautical miles.
Maskelyne reported the results of the trial to the Board of Longitude on 9 February 1765. On 26 February 1765 he had been appointed Astronomer Royal following the unexpected death of Nathaniel Bliss in 1764; making him "ex officio" a Commissioner of Longitude. The Commissioners understood that the timekeeping and astronomical methods of finding longitude were complementary. The lunar-distance method could more quickly be rolled out, with Maskelyne's proposal that tables like those in his "The British Mariner's Guide" be published for each year. This proposal led to the establishment of The Nautical Almanac, the production of which, as Astronomer Royal, Maskelyne oversaw. Taking even occasional astronomical observations was also the only way to check that a timekeeper was keeping good time over the course of a long voyage. The Commissioners also needed to know that more than one sea watch could be made, and that Harrison's methods could be communicated to other watchmakers.
The Board of Longitude therefore decided that rewards should be given Harrison (£10,000), Mayer (£3000, posthumously) and others involved in helping to develop the lunar-distance method. Harrison was told that a further reward of £10,000 would be forthcoming if he could demonstrate the replicability of his watch. It is worth noting that although Harrison and his son later accused Maskelyne of bias against the timekeeping method, charges repeated by authors such as Dava Sobel and Rupert Gould, Maskelyne never submitted a method or an idea of his own for consideration by the Board of Longitude. He was to play a significant role in having marine timekeepers, as well as the lunar-distance method, developed, tested and used on board voyages of exploration.
Since the observations that fed into the Nautical Almanac were made at the Royal Observatory, Greenwich, the Greenwich meridian became the reference for measurements of longitude in the Royal Navy, and on British Admiralty charts. It was recommended for adoption as the international Prime Meridian in 1884.
Measurement of latitude.
Maskelyne took a great interest in various geodetical operations, notably the measurement of the length of a degree of latitude in Maryland and Pennsylvania, executed by Mason and Dixon in 1766 – 1768, and later the determination of the relative longitude of Greenwich and Paris. On the French side the work was conducted by Count Cassini, Legendre, and Méchain; on the English side by General Roy. This triangulation was the beginning of the great trigonometrical survey which was subsequently extended all over Britain. His observations appeared in four large folio volumes from 1776–1811, some of them being reprinted in Samuel Vince's "Elements of Astronomy".
Schiehallion experiment.
In 1772 Maskelyne proposed to the Royal Society what was to become known as the "Schiehallion experiment" (named after the mountain on which it was performed), for the determination of the Earth’s density using a plumb line. He was not the first to suggest this, Pierre Bouguer and Charles-Marie de la Condamine having attempted the same experiment in 1738.
Maskelyne performed his experiment in 1774 on Schiehallion in Perthshire, Scotland, the mountain being chosen due to its regular conical shape which permitted a reasonably accurate determination of its volume. The apparent difference of latitude between two stations on opposite sides of the mountain were compared with the real difference of latitude obtained by triangulation.
From Maskelyne’s observations Charles Hutton deduced a density for the earth 4.5 times that of water (the modern value is 5.515).
Other work.
Maskelyne’s first contribution to astronomical literature was "A Proposal for Discovering the Annual Parallax of Sirius", published in 1760. Subsequent contributions to the "Transactions" contained his observations of the transits of Venus (1761 and 1769), on the tides at Saint Helena (1762), and on various astronomical phenomena at Saint Helena (1764) and at Barbados (1764).
Maskelyne also introduced several practical improvements, such as the measurement of time to tenths of a second and prevailed upon the government to replace Bird’s mural quadrant by a repeating circle 6 feet (1.8 m) in diameter. The new instrument was constructed by Edward Troughton but Maskelyne did not live to see it completed.

</doc>
<doc id="42880" url="https://en.wikipedia.org/wiki?curid=42880" title="John Flamsteed">
John Flamsteed

John Flamsteed FRS (19 August 1646 – 31 December 1719) was an English astronomer and the first Astronomer Royal. He catalogued over 3000 stars.
Life.
Flamsteed was born in Denby, Derbyshire, England, the only son of Stephen Flamsteed and his first wife, Mary Spadman. He was educated at the free school of Derby, and was educated at Derby School, in St Peter's Churchyard, Derby, near where his father carried on a malting business. At that time, most masters of the school were Puritans. Flamsteed had a solid knowledge of Latin, essential for reading the literature of the day, and a love of history, leaving the school in May, 1662.
His progress to Jesus College, Cambridge, recommended by the Master of Derby School, was delayed by some years of chronic ill health. During those years, Flamsteed gave his father some help in his business, and from his father learnt arithmetic and the use of fractions, developing a keen interest in mathematics and astronomy. In July 1662, he was fascinated by the thirteenth century work of Johannes de Sacrobosco, "De sphaera mundi", and on 12 September 1662 observed his first partial solar eclipse. Early in 1663, he read Thomas Fale's "The Art of Dialling", which set off an interest in sundials. In the summer of 1663, he read Wingate's "Canon", William Oughtred's "Canon", and Thomas Stirrup's "Art of Dialling". At about the same time, he acquired Thomas Street's "Astronomia Carolina, or A New Theory of the Celestial Motions" ("Caroline Tables"). He associated himself with local gentlemen interested in astronomy, including William Litchford, whose library included the work of the astrologer John Gadbury which included astronomical tables by Jeremiah Horrocks, who had died in 1641 at the age of twenty-two. Flamsteed was greatly impressed (as Isaac Newton had been) by the work of Horrocks.
In August 1665, at the age of nineteen and as a gift for his friend Litchford, Flamsteed wrote his first paper on astronomy, entitled "Mathematical Essays", concerning the design, use and construction of an astronomer's quadrant, including tables for the latitude of Derby.
In September 1670, Flamsteed visited Cambridge and entered his name as an undergraduate at Jesus College. While it seems he never took up full residence, he was there for two months in 1674, and had the opportunity to hear Isaac Newton's "Lucasian Lectures".
Ordained a deacon, he was preparing to take up a living in Derbyshire when he was invited to London by his patron Jonas Moore, Surveyor-General of the Ordnance. Moore had recently made an offer to the Royal Society to pay for the establishment of an observatory. These plans were, however, preempted when Charles II was persuaded by his mistress, Louise de Kérouaille, Duchess of Portsmouth, to hear about a proposal to find longitude by the position of the Moon from an individual known as Le Sieur de St. Pierre. Charles appointed a Royal Commission to examine the proposal in December 1674, consisting of Lord Brouncker, Seth Ward, Samuel Moreland, Christopher Wren, Silius Titus, John Pell and Robert Hooke.
Having arrived in London on 2 February 1675, and staying with Jonas Moore at the Tower of London, Flamsteed had the opportunity to be taken by Titus to meet the King. He was subsequently admitted as an official Assistant to the Royal Commission and supplied observations in order to test St Pierre's proposal and to offer his own comments. The Commission's conclusions were that, although St Pierre's proposal was not worth further consideration, the King should consider establishing an observatory and appointing an observer in order to better map the stars and the motions of the Moon in order to underpin the successful development of the lunar-distance method of finding longitude.
On 4 March 1675 Flamsteed was appointed by royal warrant "The King's Astronomical Observator" — the first English Astronomer Royal, with an allowance of £100 a year. The warrant stated his task as "rectifieing the Tables of the motions of the Heavens, and the places of the fixed stars, so as to find out the so much desired Longitude of places for Perfecteing the Art of Navigation". In June 1675, another royal warrant provided for the founding of the Royal Greenwich Observatory, and Flamsteed laid the foundation stone on 10 August.
In February 1676, he was admitted a Fellow of the Royal Society, and in July, he moved into the Observatory where he lived until 1684, when he was finally appointed priest to the parish of Burstow, Surrey. He held that office, as well as that of Astronomer Royal, until his death. He is buried at Burstow, and the east window in the church was dedicated to him as a memorial. 
After his death, his papers and scientific instruments were taken by his widow. The papers were returned many years later, but the instruments disappeared.
Scientific work.
Flamsteed accurately calculated the solar eclipses of 1666 and 1668. He was responsible for several of the earliest recorded sightings of the planet Uranus, which he mistook for a star and catalogued as '34 Tauri'. The first of these was in December 1690, which remains the earliest known sighting of Uranus by an astronomer.
On 16 August 1680 Flamsteed catalogued a star, 3 Cassiopeiae, that later astronomers were unable to corroborate. Three hundred years later, the American astronomical historian William Ashworth suggested that what Flamsteed may have seen was the most recent supernova in the galaxy's history, an event which would leave as its remnant the strongest radio source outside of the solar system, known in the third Cambridge (3C) catalogue as 3C 461 and commonly called Cassiopeia A by astronomers. Because the position of "3 Cassiopeiae" does not precisely match that of Cassiopeia A, and because the expansion wave associated with the explosion has been worked backward to the year 1667 and not 1680, some historians feel that all Flamsteed may have done was incorrectly note the position of a star already known.
In 1681 Flamsteed proposed that the two great comets observed in November and December 1680 were not separate bodies, but rather a single comet travelling first towards the Sun and then away from it. Although Isaac Newton first disagreed with Flamsteed, he later came to agree with him and theorized that comets, like planets, moved around the sun in large, closed elliptical orbits. An angry Flamsteed later learned that Newton had gained access to Flamsteed's observations and data, with the aid of Edmund Halley.
As Astronomer Royal, Flamsteed spent some forty years observing and making meticulous records for his star catalogue, which would eventually triple the number of entries in Tycho Brahe's sky atlas. Unwilling to risk his reputation by releasing unverified data, he kept the incomplete records under seal at Greenwich. In 1712, Isaac Newton, then President of the Royal Society, and Edmund Halley again obtained Flamsteed's data and published a pirated star catalogue. Flamsteed managed to gather three hundred of the four hundred printings and burned them. "If Sir I.N. would be sensible of it, I have done both him and Dr. Halley a great kindness," he wrote to his assistant Abraham Sharp.
In 1725 Flamsteed's own version of "Historia Coelestis Britannica" was published posthumously, edited by his wife, Margaret. This contained Flamsteed's observations, and included a catalogue of 2,935 stars to much greater accuracy than any prior work. It was considered the first significant contribution of the Greenwich Observatory, and the numerical Flamsteed designations for stars that were added subsequently to a French edition are still in use. In 1729 his wife published his "Atlas Coelestis", assisted by Joseph Crosthwait and Abraham Sharp, who were responsible for the technical side.

</doc>
<doc id="42882" url="https://en.wikipedia.org/wiki?curid=42882" title="Cosmogony">
Cosmogony

Cosmogony (or cosmogeny) is any model concerning the coming-into-existence (i.e. origin) of either the cosmos (i.e. universe), or the so-called reality of sentient beings. Developing a complete theoretical model has implications in both the philosophy of science and epistemology.
Etymology.
The word comes from the Koine Greek κοσμογονία (from κόσμος "cosmos, the world") and the root of γί(γ)νομαι / γέγονα ("come into a new state of being"). In astronomy, cosmogony refers to the study of the origin of particular astrophysical objects or systems, and is most commonly used in reference to the origin of the universe, the solar system, or the earth-moon system.
Overview.
The Big Bang theory is the prevailing cosmological model of the early development of the universe. The most commonly held view is that the universe was once a gravitational singularity, which expanded extremely rapidly from its hot and dense state. However, while this expansion is well-modeled by the Big Bang theory, the origins of the singularity remain as one of the unsolved problems in physics.
Cosmologist and science communicator Sean M. Carroll explains two competing types of explanations for the origins of the singularity which is the main disagreement between the scientists who study cosmogony and centers on the question of whether time existed "before" the emergence of our universe or not. One cosmogonical view sees time as fundamental and even eternal: The universe could have contained the singularity because the universe evolved or changed from a prior state (the prior state was "empty space", or maybe a state that could not be called "space" at all). The other view, held by proponents like Stephen Hawking, says that there was no change through time because "time" itself emerged along with this universe (in other words, there can be no "prior" to the universe). Thus, it remains unclear what combination of "stuff", space, or time emerged with the singularity and this universe.
One problem in cosmogony is that there is currently no theoretical model that explains the earliest moments of the universe's existence (during the Planck time) because of a lack of a testable theory of quantum gravity. Researchers in string theory and its extensions (for example, M theory), and of loop quantum cosmology, have nevertheless proposed solutions of the type just discussed.
Another issue facing the field of particle physics is a need for more expensive and technologically advanced particle accelerators to test proposed theories (for example, that the universe was caused by colliding membranes).
Compared with cosmology.
Cosmogony can be distinguished from cosmology, which studies the universe at large and throughout its existence, and which technically does not inquire directly into the source of its origins. There is some ambiguity between the two terms. For example, the cosmological argument from theology regarding the existence of God is technically an appeal to cosmogonical rather than cosmological ideas. In practice, there is a scientific distinction between cosmological and cosmogonical ideas. Physical cosmology is the science that attempts to explain all observations relevant to the development and characteristics of the universe as a whole. Questions regarding why the universe behaves in such a way have been described by physicists and cosmologists as being extra-scientific (i.e., metaphysical), though speculations are made from a variety of perspectives that include extrapolation of scientific theories to untested regimes (i.e., at Planck scales), and philosophical or religious ideas.
Theoretical scenarios.
Cosmogonists have only tentative theories for the early stages of the universe and its beginning. , no accelerator experiments probe energies of sufficient magnitude to provide any experimental insight into the behavior of matter at the energy levels that prevailed shortly after the Big Bang. Furthermore, since astronomical observations imply a singularity at the origin of the universe, experiments at any given high energy level will always be dwarfed by the infinite energy level predicted by Big Bang Theory. Therefore, significant technological and conceptual advances would be needed to propose a scientific test for cosmogonical theories.
Proposed theoretical scenarios differ radically, and include string theory and M-theory, the Hartle–Hawking initial state, string landscape, brane inflation, the Big Bang, and the ekpyrotic universe. Some of these models are mutually compatible, whereas others are not.

</doc>
<doc id="42886" url="https://en.wikipedia.org/wiki?curid=42886" title="Gattaca">
Gattaca

Gattaca is a 1997 American science fiction film written and directed by Andrew Niccol. It stars Ethan Hawke and Uma Thurman, with Jude Law, Loren Dean, Ernest Borgnine, Gore Vidal, and Alan Arkin appearing in supporting roles. The film presents a biopunk vision of a future society driven by eugenics where potential children are conceived through genetic manipulation to ensure they possess the best hereditary traits of their parents. The film centers on Vincent Freeman, played by Hawke, who was conceived outside the eugenics program and struggles to overcome genetic discrimination to realize his dream of traveling into space.
The movie draws on concerns over reproductive technologies which facilitate eugenics, and the possible consequences of such technological developments for society. It also explores the idea of destiny and the ways in which it can and does govern lives. Characters in "Gattaca" continually battle both with society and with themselves to find their place in the world and who they are destined to be according to their genes.
The film's title is based on the first letters of guanine, adenine, thymine, and cytosine, the four nucleobases of DNA. It was a 1997 nominee for the Academy Award for Best Art Direction and the Golden Globe Award for Best Original Score.
The film flopped at the box office, but it received generally positive reviews and has since gained a cult following.
Plot.
In "the not-too-distant future", eugenics is common. A genetic registry database uses biometrics to classify those so created as "valids" while those conceived by traditional means and more susceptible to genetic disorders are known as "in-valids". Genetic discrimination is illegal, but in practice genotype profiling is used to identify valids to qualify for professional employment while in-valids are relegated to menial jobs.
Vincent Freeman is conceived without the aid of genetic selection; his genetics indicate a high probability of several disorders and an estimated life span of 30.2 years. His parents, regretting their decision, use genetic selection to give birth to their next child, Anton. Growing up, the two brothers often play a game of "chicken" by swimming out to sea with the first one returning to shore considered the loser; Vincent always loses. Vincent dreams of a career in space travel but is reminded of his genetic inferiority. One day Vincent challenges Anton to a game of chicken and bests him before Anton starts to drown. Vincent saves Anton and then leaves home.
Vincent works as an in-valid, cleaning office spaces including that of Gattaca Aerospace Corporation, a space-flight conglomerate. He gets a chance to pose as a valid by using hair, skin, blood and urine samples from a donor, Jerome Eugene Morrow, who is a former swimming star paralyzed due to a car accident. With Jerome's genetic makeup, Vincent gains employment at Gattaca, and is assigned to be navigator for an upcoming trip to Saturn's moon Titan. To keep his identity hidden, Vincent must meticulously groom and scrub down daily to remove his own genetic material, and pass daily DNA scanning and urine tests using Jerome's samples.
Gattaca becomes embroiled in controversy when one of its administrators is murdered a week before the flight. The police find a fallen eyelash of Vincent's at the scene. An investigation is launched to find the murderer, Vincent being the top suspect. Through this, Vincent becomes close to a co-worker, Irene Cassini, and falls in love with her. Though a valid, Irene has a higher risk of heart failure that will prevent her from joining any deep space Gattaca mission. Vincent also learns that Jerome's paralysis is by his own hand; after coming in second place in a swim meet, Jerome threw himself in front of a car. Jerome maintains that he was designed to be the best, yet wasn't, and that is the source of his suffering.
Vincent repeatedly evades scrutiny from the investigation, and it is revealed that Gattaca's mission director was the killer, as the administrator was threatening to cancel the mission. Vincent learns the identity of the detective who closed the case, his brother Anton, who has become aware of Vincent's presence. The brothers meet, and Anton warns Vincent that what he is doing is illegal, but Vincent asserts that he has gotten to this position on his own merits. Anton challenges Vincent to one more game of chicken. As the two swim out in the dead of night, Anton is surprised at Vincent's stamina, and Vincent reveals that his trick to winning was not saving energy for the swim back. Anton turns back and begins to drown, but Vincent rescues him and swims them both back to shore using celestial navigation.
On the day of the launch, Jerome reveals that he has stored enough DNA samples for Vincent to last two lifetimes upon his return, and gives him an envelope to open once in flight. After saying goodbye to Irene, Vincent prepares to board but discovers there is a final genetic test, and he currently lacks any of Jerome's samples. He is surprised when Dr. Lamar, the person in charge of background checks, reveals that he knows Vincent has been posing as a valid. Lamar admits that his son looks up to Vincent and wonders whether his son, genetically selected but "not all that they promised", could break the limits just as Vincent has. He passes Vincent as a valid. As the rocket launches, Jerome dons his swimming medal and immolates himself in his home's incinerator; Vincent opens the note from Jerome to find only a lock of Jerome's hair attached to it. Vincent muses on this, stating "For someone who was never meant for this world, I must confess, I’m suddenly having a hard time leaving it. Of course, they say every atom in our bodies was once a part of a star. Maybe I'm not leaving; maybe I'm going home."
Production.
The exteriors (including the roof scene), and some of the interior shots, of the Gattaca complex were filmed at Frank Lloyd Wright's 1960 Marin County Civic Center in San Rafael, California. The parking lot scenes were shot at the Otis College of Art and Design, distinguished by its punch card-like windows, located near LAX in Los Angeles. The exterior of Vincent Freeman's house was shot at the CLA Building on the campus of California State Polytechnic University, Pomona (Cal Poly Pomona). Other exterior shots were filmed at the bottom of the spillway of the Sepulveda Dam and outside the The Forum in Inglewood. The solar power plant mirrors sequence was filmed at the Kramer Junction Solar Electric Generating Station.
Design.
The movie uses a swimming treadmill in the opening minutes to punctuate the swimming and futuristic themes. The futuristic turbine cars are based on 1960s car models like Rover P6, Citroën DS19 and Studebaker Avanti, and futuristic buildings represent modern architecture of the 1950s.
Title.
The film was shot under the working title "The Eighth Day", a reference to the seven days of creation in the Christian Bible. However by the time its release was scheduled for the fall of 1997, the Belgian film "Le huitième jour" had already been released in the US under the title "The Eighth Day". As a result, the film was retitled "Gattaca".
Release.
"Gattaca" was released in theaters on October 24, 1997, and opened at number 5 at the box office; trailing "I Know What You Did Last Summer", "The Devil's Advocate", "Kiss the Girls", and "Seven Years in Tibet". Over the first weekend the film brought in $4.3 million. It ended its theatrical run with a domestic total of $12.5 million against a reported production budget of $36 million.
Home media.
"Gattaca" was released on DVD on July 1, 1998, and was also released on Superbit DVD. Special Edition DVD and Blu-ray versions were released on March 11, 2008. Both editions contain a deleted scene featuring historical figures like Einstein, Lincoln, etc., who according to the texts are supposedly genetically deficient.
Critical reception.
"Gattaca" received positive reviews from critics; the film received an 82% "fresh" rating from Rotten Tomatoes, based on 55 reviews, with a rating average of 7.1/10. The critical consensus states that "Intelligent and scientifically provocative, Gattaca is an absorbing sci fi drama that poses important interesting ethical questions about the nature of science." On Metacritic, the film received "generally favorable reviews" with a score of 64 out of 100. Roger Ebert stated, "This is one of the smartest and most provocative of science fiction films, a thriller with ideas." James Berardinelli praised it for "energy and tautness" and its "thought-provoking script and thematic richness."
Despite critical acclaim, "Gattaca" was not a box office success, but it is said to have crystallized the debate over the controversial topic of human genetic engineering. The film's dystopian depiction of "genoism" has been cited by many bioethicists and laymen in support of their hesitancy about, or opposition to, eugenics and the societal acceptance of the genetic-determinist ideology that may frame it. In a 1997 review of the film for the journal "Nature Genetics", molecular biologist Lee M. Silver stated that ""Gattaca" is a film that all geneticists should see if for no other reason than to understand the perception of our trade held by so many of the public-at-large".
In 2004, bioethicist James Hughes criticized the premise and influence of the film "Gattaca", arguing that:
Soundtrack.
The score for "Gattaca" was composed by Michael Nyman, and the original soundtrack was released on October 21, 1997.
Legacy.
Television series.
On October 30, 2009, "Variety" reported that Sony Pictures was developing a television adaptation of the feature film as a one-hour police procedural set in the future. The show was to be written by Gil Grant, who has written for "24" and "NCIS".
Political references.
U.S. Senator Rand Paul (R-KY) used near-verbatim portions of the plot summary from the Wikipedia entry on "Gattaca" in a speech at Liberty University on October 28, 2013 supporting Virginia Attorney General Ken Cuccinelli's campaign for Governor of Virginia. Paul accused pro-choice politicians of advocating eugenics in a manner similar to the events in "Gattaca".
Further reading.
Jon Frauley. 2010. "Biopolitics and the Governance of Genetic Capital in "Gattaca"." "Criminology, Deviance and the Silver Screen: The Fictional Reality and the Criminological Imagination". New York: Palgrave Macmillan.

</doc>
<doc id="42888" url="https://en.wikipedia.org/wiki?curid=42888" title="Human genome">
Human genome

<section begin=lead />The human genome is the complete set of nucleic acid sequence for humans ("Homo sapiens"), encoded as DNA within the 23 chromosome pairs in cell nuclei and in a small DNA molecule found within individual mitochondria. Human genomes include both protein-coding DNA genes and noncoding DNA. Haploid human genomes, which are contained in germ cells (the egg and sperm gamete cells created in the meiosis phase of sexual reproduction before fertilization creates a zygote) consist of three billion DNA base pairs, while diploid genomes (found in somatic cells) have twice the DNA content. While there are significant differences among the genomes of human individuals (on the order of 0.1%), these are considerably smaller than the differences between humans and their closest living relatives, the chimpanzees (approximately 4%) and bonobos.<section end=lead />
The Human Genome Project produced the first complete sequences of individual human genomes, with the first draft sequence and initial analysis being published on February 12, 2001. The human genome was the first of all vertebrates to be completely sequenced. As of 2012, thousands of human genomes have been completely sequenced, and many more have been mapped at lower levels of resolution. The resulting data are used worldwide in biomedical science, anthropology, forensics and other branches of science. There is a widely held expectation that genomic studies will lead to advances in the diagnosis and treatment of diseases, and to new insights in many fields of biology, including human evolution.
Although the sequence of the human genome has been (almost) completely determined by DNA sequencing, it is not yet fully understood. Most (though probably not all) genes have been identified by a combination of high throughput experimental and bioinformatics approaches, yet much work still needs to be done to further elucidate the biological functions of their protein and RNA products. Recent results suggest that most of the vast quantities of noncoding DNA within the genome have associated biochemical activities, including regulation of gene expression, organization of chromosome architecture, and signals controlling epigenetic inheritance.
There are an estimated 20,000-25,000 human protein-coding genes. The estimate of the number of human genes has been repeatedly revised down from initial predictions of 100,000 or more as genome sequence quality and gene finding methods have improved, and could continue to drop further. Protein-coding sequences account for only a very small fraction of the genome (approximately 1.5%), and the rest is associated with non-coding RNA molecules, regulatory DNA sequences, LINEs, SINEs, introns, and sequences for which as yet no function has been determined.
Molecular organization and gene content.
The total length of the human genome is over 3 billion base pairs. The genome is organized into 22 paired chromosomes, plus the X chromosome (one in males, two in females) and, in males only, one Y chromosome. These are all large linear DNA molecules contained within the cell nucleus. The genome also includes the mitochondrial DNA, a comparatively small circular molecule present in each mitochondrion. Basic information about these molecules and their gene content, based on a reference genome that does not represent the sequence of any specific individual, are provided in the following table. (Data source: Ensembl genome browser release 68, July 2012)
Table 1 (above) summarizes the physical organization and gene content of the human reference genome, with links to the original analysis, as published in the Ensembl database at the European Bioinformatics Institute (EBI) and Wellcome Trust Sanger Institute. Chromosome lengths were estimated by multiplying the number of base pairs by 0.34 nanometers, the distance between base pairs in the DNA double helix. The number of proteins is based on the number of initial precursor mRNA transcripts, and does not include products of alternative pre-mRNA splicing, or modifications to protein structure that occur after translation.
The number of variations is a summary of unique DNA sequence changes that have been identified within the sequences analyzed by Ensembl as of July, 2012; that number is expected to increase as further personal genomes are sequenced and examined. In addition to the gene content shown in this table, a large number of non-expressed functional sequences have been identified throughout the human genome (see below). Links open windows to the reference chromosome sequence in the EBI genome browser. The table also describes prevalence of genes encoding structural RNAs in the genome.
MiRNA, or MicroRNA, functions as a post-transcriptional regulator of gene expression. Ribosomal RNA, or rRNA, makes up the RNA portion of the ribosome and is critical in the synthesis of proteins. Small nuclear RNA, or snRNA, is found in the nucleus of the cell. Its primary function is in the processing of pre-mRNA molecules and also in the regulation of transcription factors. SnoRNA, or Small nucleolar RNA, primarily functions in guiding chemical modifications to other RNA molecules.
Completeness of the human genome sequence.
Although the human genome has been completely sequenced for all practical purposes, there are still hundreds of gaps in the sequence. A recent study noted more than 160 euchromatic gaps of which 50 gaps were closed. However, there are still numerous gaps in the heterochromatic parts of the genome which is much harder to sequence due to numerous repeats and other intractable sequence features.
Coding vs. noncoding DNA.
The content of the human genome is commonly divided into coding and noncoding DNA sequences. Coding DNA is defined as those sequences that can be transcribed into mRNA and translated into proteins during the human life cycle; these sequences occupy only a small fraction of the genome (<2%). Noncoding DNA is made up of all of those sequences (ca. 98% of the genome) that are not used to encode proteins.
Some noncoding DNA contains genes for RNA molecules with important biological functions (noncoding RNA, for example ribosomal RNA and transfer RNA). The exploration of the function and evolutionary origin of noncoding DNA is an important goal of contemporary genome research, including the ENCODE (Encyclopedia of DNA Elements) project, which aims to survey the entire human genome, using a variety of experimental tools whose results are indicative of molecular activity.
Because non-coding DNA greatly outnumbers coding DNA, the concept of the sequenced genome has become a more focused analytical concept than the classical concept of the DNA-coding gene.
Mutation Rate of Human Genome.
Mutation rate of human genome is a very important factor in calculating evolutionary time points. Researchers calculated the number of genetic variations between human and apes. Dividing that number by age of fossil of most recent common ancestor of humans and ape, researchers calculated the mutation rate. Recent studies using next generation sequencing technologies concluded a slow mutation rate which doesn't add up with human migration pattern time points and suggesting a new evolutionary time scale. 100,000 year old human fossil found in Israel threw more questions on human migration time points.
Coding sequences (protein-coding genes).
Protein-coding sequences represent the most widely studied and best understood component of the human genome. These sequences ultimately lead to the production of all human proteins, although several biological processes (e.g. DNA rearrangements and alternative pre-mRNA splicing) can lead to the production of many more unique proteins than the number of protein-coding genes.
The complete modular protein-coding capacity of the genome is contained within the exome, and consists of DNA sequences encoded by exons that can be translated into proteins. Because of its biological importance, and the fact that it constitutes less than 2% of the genome, sequencing of the exome was the first major milepost of the Human Genome Project.
Number of protein-coding genes. About 20,000 human proteins have been annotated in databases such as Uniprot. Historically, estimates for the number of protein genes have varied widely, ranging up to 2,000,000 in the late 1960s, but several researchers pointed out in the early 1970s that the estimated mutational load from deleterious mutations placed an upper limit of approximately 40,000 for the total number of functional loci (this includes protein-coding and functional non-coding genes).
The number of human protein-coding genes is not significantly larger than that of many less complex organisms, such as the roundworm and the fruit fly. This difference may result from the extensive use of alternative pre-mRNA splicing in humans, which provides the ability to build a very large number of modular proteins through the selective incorporation of exons.
Protein-coding capacity per chromosome. Protein-coding genes are distributed unevenly across the chromosomes, ranging from a few dozen to more than 2000, with an especially high gene density within chromosomes 19, 11, and 1 (Table 1). Each chromosome contains various gene-rich and gene-poor regions, which may be correlated with chromosome bands and GC-content . The significance of these nonrandom patterns of gene density is not well understood.
Size of protein-coding genes. The size of protein-coding genes within the human genome shows enormous variability (Table 2). For example, the gene for histone H1a (HIST1HIA) is relatively small and simple, lacking introns and encoding mRNA sequences of 781 nt and a 215 amino acid protein (648 nt open reading frame). Dystrophin (DMD) is the largest protein-coding gene in the human reference genome, spanning a total of 2.2 MB, while Titin (TTN) has the longest coding sequence (114,414 bp), the largest number of exons (363), and the longest single exon (17,106 bp). Over the whole genome, the median size of an exon is 122 bp (mean = 145 bp), the median number of exons is 7 (mean = 8.8), and the median coding sequence encodes 367 amino acids (mean = 447 amino acids; Table 21 in ).
Table 2. Examples of human protein-coding genes. Chrom, chromosome. Alt splicing, alternative pre-mRNA splicing. (Data source: Ensembl genome browser release 68, July 2012)
Noncoding DNA (ncDNA).
Noncoding DNA is defined as all of the DNA sequences within a genome that are not found within protein-coding exons, and so are never represented within the amino acid sequence of expressed proteins. By this definition, more than 98% of the human genomes is composed of ncDNA.
Numerous classes of noncoding DNA have been identified, including genes for noncoding RNA (e.g. tRNA and rRNA), pseudogenes, introns, untranslated regions of mRNA, regulatory DNA sequences, repetitive DNA sequences, and sequences related to mobile genetic elements.
Numerous sequences that are included within genes are also defined as noncoding DNA. These include genes for noncoding RNA (e.g. tRNA, rRNA), and untranslated components of protein-coding genes (e.g. introns, and 5' and 3' untranslated regions of mRNA).
Protein-coding sequences (specifically, coding exons) constitute less than 1.5% of the human genome. In addition, about 26% of the human genome is introns. Aside from genes (exons and introns) and known regulatory sequences (8–20%), the human genome contains regions of noncoding DNA. The exact amount of noncoding DNA that plays a role in cell physiology has been hotly debated. Recent analysis by the ENCODE project indicates that 80% of the entire human genome is either transcribed, binds to regulatory proteins, or is associated with some other biochemical activity.
It however remains controversial whether all of this biochemical activity contributes to cell physiology, or whether a substantial portion of this is the result transcriptional and biochemical noise, which must be actively filtered out by the organism. Excluding protein-coding sequences, introns, and regulatory regions, much of the non-coding DNA is composed of:
Many DNA sequences that do not play a role in gene expression have important biological functions. Comparative genomics studies indicate that about 5% of the genome contains sequences of noncoding DNA that are highly conserved, sometimes on time-scales representing hundreds of millions of years, implying that these noncoding regions are under strong evolutionary pressure and positive selection.
Many of these sequences regulate the structure of chromosomes by limiting the regions of heterochromatin formation and regulating structural features of the chromosomes, such as the telomeres and centromeres. Other noncoding regions serve as origins of DNA replication. Finally several regions are transcribed into functional noncoding RNA that regulate the expression of protein-coding genes (for example ), mRNA translation and stability (see miRNA), chromatin structure (including histone modifications, for example ), DNA methylation (for example ), DNA recombination (for example ), and cross-regulate other noncoding RNAs (for example ). It is also likely that many transcribed noncoding regions do not serve any role and that this transcription is the product of non-specific RNA Polymerase activity.
Pseudogenes.
Pseudogenes are inactive copies of protein-coding genes, often generated by gene duplication, that have become nonfunctional through the accumulation of inactivating mutations. Table 1 shows that the number of pseudogenes in the human genome is on the order of 13,000, and in some chromosomes is nearly the same as the number of functional protein-coding genes. Gene duplication is a major mechanism through which new genetic material is generated during molecular evolution.
For example, the olfactory receptor gene family is one of the best-documented examples of pseudogenes in the human genome. More than 60 percent of the genes in this family are non-functional pseudogenes in humans. By comparison, only 20 percent of genes in the mouse olfactory receptor gene family are pseudogenes. Research suggests that this is a species-specific characteristic, as the most closely related primates all have proportionally fewer pseudogenes. This genetic discovery helps to explain the less acute sense of smell in humans relative to other mammals.
Genes for noncoding RNA (ncRNA).
Noncoding RNA molecules play many essential roles in cells, especially in the many reactions of protein synthesis and RNA processing. ncRNAs include tRNA, ribosomal RNA, microRNA, snRNA and other non-coding RNA genes including about 60,000 long non coding RNAs (lncRNAs). It should be noted that while the number of reported lncRNA genes continues to rise and the exact number in the human genome is yet to be defined, many of them are argued to be non-functional.
Many ncRNAs are critical elements in gene regulation and expression. Noncoding RNA also contributes to epigenetics, transcription, RNA splicing, and the translational machinery. The role of RNA in genetic regulation and disease offers a new potential level of unexplored genomic complexity.
Introns and untranslated regions of mRNA.
In addition to the ncRNA molecules that are encoded by discrete genes, the initial transcripts of protein coding genes usually contain extensive noncoding sequences, in the form of introns, 5'-untranslated regions (5'-UTR), and 3'-untranslated regions (3'-UTR). Within most protein-coding genes of the human genome, the length of intron sequences is 10- to 100-times the length of exon sequences (Table 2).
Regulatory DNA sequences.
The human genome has many different regulatory sequences which are crucial to controlling gene expression. Conservative estimates indicate that these sequences make up 8% of the genome, however extrapolations from the ENCODE project give that 20-40% of the genome is gene regulatory sequence. Some types of non-coding DNA are genetic "switches" that do not encode proteins, but do regulate when and where genes are expressed (called enhancers).
Regulatory sequences have been known since the late 1960s. The first identification of regulatory sequences in the human genome relied on recombinant DNA technology. Later with the advent of genomic sequencing, the identification of these sequences could be inferred by evolutionary conservation. The evolutionary branch between the primates and mouse, for example, occurred 70–90 million years ago. So computer comparisons of gene sequences that identify conserved non-coding sequences will be an indication of their importance in duties such as gene regulation.
Other genomes have been sequenced with the same intention of aiding conservation-guided methods, for exampled the pufferfish genome. However, regulatory sequences disappear and re-evolve during evolution at a high rate.
As of 2012, the efforts have shifted toward finding interactions between DNA and regulatory proteins by the technique ChIP-Seq, or gaps where the DNA is not packaged by histones (DNase hypersensitive sites), both of which tell where there are active regulatory sequences in the investigated cell type.
Repetitive DNA sequences.
Repetitive DNA sequences comprise approximately 50% of the human genome.
About 8% of the human genome consists of tandem DNA arrays or tandem repeats, low complexity repeat sequences that have multiple adjacent copies (e.g. "CAGCAGCAG..."). The tandem sequences may be of variable lengths, from two nucleotides to tens of nucleotides. These sequences are highly variable, even among closely related individuals, and so are used for genealogical DNA testing and forensic DNA analysis.
Repeated sequences of fewer than ten nucleotides (e.g. the dinucleotide repeat (AC)n) are termed microsatellite sequences. Among the microsatellite sequences, trinucleotide repeats are of particular importance, as sometimes occur within coding regions of genes for proteins and may lead to genetic disorders. For example, Huntington's disease results from an expansion of the trinucleotide repeat (CAG)n within the "Huntingtin" gene on human chromosome 4. Telomeres (the ends of linear chromosomes) end with a microsatellite hexanucleotide repeat of the sequence (TTAGGG)n.
Tandem repeats of longer sequences (arrays of repeated sequences 10–60 nucleotides long) are termed minisatellites.
Mobile genetic elements (transposons) and their relics.
Transposable genetic elements, DNA sequences that can replicate and insert copies of themselves at other locations within a host genome, are an abundant component in the human genome. The most abundant transposon lineage, "Alu", has about 50,000 active copies, and can be inserted into intragenic and intergenic regions. One other lineage, LINE-1, has about 100 active copies per genome (the number varies between people). Together with non-functional relics of old transposons, they account for over half of total human DNA. Sometimes called "jumping genes", transposons have played a major role in sculpting the human genome. Some of these sequences represent endogenous retroviruses, DNA copies of viral sequences that have become permanently integrated into the genome and are now passed on to succeeding generations.
Mobile elements within the human genome can be classified into LTR retrotransposons (8.3% of total genome), SINEs (13.1% of total genome) including Alu elements, LINEs (20.4% of total genome), SVAs and Class II DNA transposons (2.9% of total genome).
Genomic variation in humans.
Human Reference Genome.
With the exception of identical twins, all humans show significant variation in genomic DNA sequences. The Human Reference Genome (HRG) is used as a standard sequence reference.
There are several important points concerning the Human Reference Genome--
Measuring human genetic variation.
Most studies of human genetic variation have focused on single-nucleotide polymorphisms (SNPs), which are substitutions in individual bases along a chromosome. Most analyses estimate that SNPs occur 1 in 1000 base pairs, on average, in the euchromatic human genome, although they do not occur at a uniform density. Thus follows the popular statement that "we are all, regardless of race, genetically 99.9% the same", although this would be somewhat qualified by most geneticists. For example, a much larger fraction of the genome is now thought to be involved in copy number variation. A large-scale collaborative effort to catalog SNP variations in the human genome is being undertaken by the International HapMap Project.
The genomic loci and length of certain types of small repetitive sequences are highly variable from person to person, which is the basis of DNA fingerprinting and DNA paternity testing technologies. The heterochromatic portions of the human genome, which total several hundred million base pairs, are also thought to be quite variable within the human population (they are so repetitive and so long that they cannot be accurately sequenced with current technology). These regions contain few genes, and it is unclear whether any significant phenotypic effect results from typical variation in repeats or heterochromatin.
Most gross genomic mutations in gamete germ cells probably result in inviable embryos; however, a number of human diseases are related to large-scale genomic abnormalities. Down syndrome, Turner Syndrome, and a number of other diseases result from nondisjunction of entire chromosomes. Cancer cells frequently have aneuploidy of chromosomes and chromosome arms, although a cause and effect relationship between aneuploidy and cancer has not been established.
Mapping human genomic variation.
Whereas a genome sequence lists the order of every DNA base in a genome, a genome map identifies the landmarks. A genome map is less detailed than a genome sequence and aids in navigating around the genome.
An example of a variation map is the HapMap being developed by the International HapMap Project. The HapMap is a haplotype map of the human genome, "which will describe the common patterns of human DNA sequence variation." It catalogs the patterns of small-scale variations in the genome that involve single DNA letters, or bases.
Researchers published the first sequence-based map of large-scale structural variation across the human genome in the journal "Nature" in May 2008. Large-scale structural variations are differences in the genome among people that range from a few thousand to a few million DNA bases; some are gains or losses of stretches of genome sequence and others appear as re-arrangements of stretches of sequence. These variations include differences in the number of copies individuals have of a particular gene, deletions, translocations and inversions.
SNP Frequency across the Human Genome.
Single-nucleotide polymorphisms (SNPs) do not occur homogeneously across the human genome. In fact, there is enormous diversity in SNP frequency between genes, reflecting different selective pressures on each gene as well as different mutation and recombination rates across the genome. However, studies on SNPs are biased towards coding regions, the data generated from them are unlikely to reflect the overall distribution of SNPs throughout the genome. Therefore, the SNP Consortium protocol was designed to identify SNPs with no bias towards coding regions and the Consortium's 100,000 SNPs generally reflect sequence diversity across the human chromosomes. in non-coding sequence and synonymous changes in coding sequence are generally more common than non-synonymous changes, reflecting greater selective pressure reducing diversity at positions dictating amino acid identity. Transitional changes are more common than transversions, with CpG dinucleotides showing the highest mutation rate, presumably due to deamination.
Personal genomes.
A personal genome sequence is a (nearly) complete sequence of the chemical base pairs that make up the DNA of a single person. Because medical treatments have different effects on different people due to genetic variations such as single-nucleotide polymorphisms (SNPs), the analysis of personal genomes may lead to personalized medical treatment based on individual genotypes.
The first personal genome sequence to be determined was that of Craig Venter in 2007. Personal genomes had not been sequenced in the public Human Genome Project to protect the identity of volunteers who provided DNA samples. That sequence was derived from the DNA of several volunteers from a diverse population. However, early in the Venter-led Celera Genomics genome sequencing effort the decision was made to switch from sequencing a composite sample to using DNA from a single individual, later revealed to have been Venter himself. Thus the Celera human genome sequence released in 2000 was largely that of one man. Subsequent replacement of the early composite-derived data and determination of the diploid sequence, representing both sets of chromosomes, rather than a haploid sequence originally reported, allowed the release of the first personal genome. In April 2008, that of James Watson was also completed. Since then hundreds of personal genome sequences have been released, including those of Desmond Tutu, and of a Paleo-Eskimo. In November 2013, a Spanish family made their personal genomics data obtained by direct-to-consumer genetic testing with 23andMe publicly available under a Creative Commons public domain license. This is believed to be the first such public genomics dataset for a whole family.
The sequencing of individual genomes further unveiled levels of genetic complexity that had not been appreciated before. Personal genomics helped reveal the significant level of diversity in the human genome attributed not only to SNPs but structural variations as well. However, the application of such knowledge to the treatment of disease and in the medical field is only in its very beginnings. Exome sequencing has become increasingly popular as a tool to aid in diagnosis of genetic disease because the exome contributes only 1% of the genomic sequence but accounts for roughly 85% of mutations that contribute significantly to disease.
Human genetic disorders.
Most aspects of human biology involve both genetic (inherited) and non-genetic (environmental) factors. Some inherited variation influences aspects of our biology that are not medical in nature (height, eye color, ability to taste or smell certain compounds, etc.). Moreover, some genetic disorders only cause disease in combination with the appropriate environmental factors (such as diet). With these caveats, genetic disorders may be described as clinically defined diseases caused by genomic DNA sequence variation. In the most straightforward cases, the disorder can be associated with variation in a single gene. For example, cystic fibrosis is caused by mutations in the CFTR gene, and is the most common recessive disorder in caucasian populations with over 1,300 different mutations known.
Disease-causing mutations in specific genes are usually severe in terms of gene function, and are fortunately rare, thus genetic disorders are similarly individually rare. However, since there are many genes that can vary to cause genetic disorders, in aggregate they constitute a significant component of known medical conditions, especially in pediatric medicine. Molecularly characterized genetic disorders are those for which the underlying causal gene has been identified, currently there are approximately 2,200 such disorders annotated in the OMIM database.
Studies of genetic disorders are often performed by means of family-based studies. In some instances population based approaches are employed, particularly in the case of so-called founder populations such as those in Finland, French-Canada, Utah, Sardinia, etc. Diagnosis and treatment of genetic disorders are usually performed by a geneticist-physician trained in clinical/medical genetics. The results of the Human Genome Project are likely to provide increased availability of genetic testing for gene-related disorders, and eventually improved treatment. Parents can be screened for hereditary conditions and counselled on the consequences, the probability it will be inherited, and how to avoid or ameliorate it in their offspring.
As noted above, there are many different kinds of DNA sequence variation, ranging from complete extra or missing chromosomes down to single nucleotide changes. It is generally presumed that much naturally occurring genetic variation in human populations is phenotypically neutral, i.e. has little or no detectable effect on the physiology of the individual (although there may be fractional differences in fitness defined over evolutionary time frames). Genetic disorders can be caused by any or all known types of sequence variation. To molecularly characterize a new genetic disorder, it is necessary to establish a causal link between a particular genomic sequence variant and the clinical disease under investigation. Such studies constitute the realm of human molecular genetics.
With the advent of the Human Genome and International HapMap Project, it has become feasible to explore subtle genetic influences on many common disease conditions such as diabetes, asthma, migraine, schizophrenia, etc. Although some causal links have been made between genomic sequence variants in particular genes and some of these diseases, often with much publicity in the general media, these are usually not considered to be genetic disorders "per se" as their causes are complex, involving many different genetic and environmental factors. Thus there may be disagreement in particular cases whether a specific medical condition should be termed a genetic disorder. The categorized table below provides the prevalence as well as the genes or chromosomes associated with some human genetic disorders.
Evolution.
Comparative genomics studies of mammalian genomes suggest that approximately 5% of the human genome has been conserved by evolution since the divergence of extant lineages approximately 200 million years ago, containing the vast majority of genes. The published chimpanzee genome differs from that of the human genome by 1.23% in direct sequence comparisons. Around 20% of this figure is accounted for by variation within each species, leaving only ~1.06% consistent sequence divergence between humans and chimps at shared genes. This nucleotide by nucleotide difference is dwarfed, however, by the portion of each genome that is not shared, including around 6% of functional genes that are unique to either humans or chimps.
In other words, the considerable observable differences between humans and chimps may be due as much or more to genome level variation in the number, function and expression of genes rather than DNA sequence changes in shared genes. Indeed, even within humans, there has been found to be a previously unappreciated amount of copy number variation (CNV) which can make up as much as 5 – 15% of the human genome. In other words, between humans, there could be +/- 500,000,000 base pairs of DNA, some being active genes, others inactivated, or active at different levels. The full significance of this finding remains to be seen. On average, a typical human protein-coding gene differs from its chimpanzee ortholog by only two amino acid substitutions; nearly one third of human genes have exactly the same protein translation as their chimpanzee orthologs. A major difference between the two genomes is human chromosome 2, which is equivalent to a fusion product of chimpanzee chromosomes 12 and 13</ref> (later renamed to chromosomes 2A and 2B, respectively).
Humans have undergone an extraordinary loss of olfactory receptor genes during our recent evolution, which explains our relatively crude sense of smell compared to most other mammals. Evolutionary evidence suggests that the emergence of color vision in humans and several other primate species has diminished the need for the sense of smell.
Mitochondrial DNA.
The human mitochondrial DNA is of tremendous interest to geneticists, since it undoubtedly plays a role in mitochondrial disease. It also sheds light on human evolution; for example, analysis of variation in the human mitochondrial genome has led to the postulation of a recent common ancestor for all humans on the maternal line of descent (see Mitochondrial Eve).
Due to the lack of a system for checking for copying errors, mitochondrial DNA (mtDNA) has a more rapid rate of variation than nuclear DNA. This 20-fold higher mutation rate allows mtDNA to be used for more accurate tracing of maternal ancestry. Studies of mtDNA in populations have allowed ancient migration paths to be traced, such as the migration of Native Americans from Siberia or Polynesians from southeastern Asia. It has also been used to show that there is no trace of Neanderthal DNA in the European gene mixture inherited through purely maternal lineage. Due to the restrictive all or none manner of mtDNA inheritance, this result (no trace of Neanderthal mtDNA) would be likely unless there were a large percentage of Neanderthal ancestry, or there was strong positive selection for that mtDNA (for example, going back 5 generations, only 1 of your 32 ancestors contributed to your mtDNA, so if one of these 32 was pure Neanderthal you would expect that ~3% of your autosomal DNA would be of Neanderthal origin, yet you would have a ~97% chance to have no trace of Neanderthal mtDNA).
Epigenome.
Epigenetics describes a variety of features of the human genome that transcend its primary DNA sequence, such as chromatin packaging, histone modifications and DNA methylation, and which are important in regulating gene expression, genome replication and other cellular processes. Epigenetic markers strengthen and weaken transcription of certain genes but do not affect the actual sequence of DNA nucleotides. DNA methylation is a major form of epigenetic control over gene expression and one of the most highly studied topics in epigenetics. During development, the human DNA methylation profile experiences dramatic changes. In early germ line cells, the genome has very low methylation levels. These low levels generally describe active genes. As development progresses, parental imprinting tags lead to increased methylation activity.
Epigenetic patterns can be identified between tissues within an individual as well as between individuals themselves. Identical genes that have differences only in their epigenetic state are called epialleles. Epialleles can be placed into three categories: those directly determined by an individual’s genotype, those influenced by genotype, and those entirely independent of genotype. The epigenome is also influenced significantly by environmental factors. Diet, toxins, and hormones impact the epigenetic state. Studies in dietary manipulation have demonstrated that methyl-deficient diets are associated with hypomethylation of the epigenome. Such studies establish epigenetics as an important interface between the environment and the genome.

</doc>
<doc id="42889" url="https://en.wikipedia.org/wiki?curid=42889" title="Fusor">
Fusor

A fusor is a device that uses an electric field to heat ions to conditions suitable for nuclear fusion. The machine has a voltage between two metal cages inside a vacuum. Positive ions fall down this voltage drop, building up speed. If they collide in the center, they can fuse. This is a type of inertial electrostatic confinement device.
A Farnsworth–Hirsch fusor is the most common type of fusor. This design came from work by Philo T. Farnsworth (in 1964) and Robert L. Hirsch in 1967. A variant of fusor had been proposed previously by William Elmore, James L. Tuck, and Ken Watson at the Los Alamos National Laboratory though they never built the machine.
Fusors have been built by various institutions. These include academic institutions such as the University of Wisconsin–Madison, the Massachusetts Institute of Technology and government entities, such as the Atomic Energy Organization of Iran and the Turkish Atomic Energy Authority. Fusors have also been developed commercially, as sources for neutrons by DaimlerChrysler Aerospace and as a method for generating medical isotopes. Fusors have also become very popular for hobbyists and amateurs. A growing number of amateurs have performed nuclear fusion using simple fusor machines.
Mechanism.
For every volt that an ion of ±1 charge is accelerated across, it gains 11,604 kelvins in temperature. For example, a typical magnetic confinement fusion plasma is 15 keV, which is a temperature increase of approximately 174 megakelvins for a singly-charged ion. Because most of the ions fall into the wires of the cage, fusors suffer from high conduction losses. At bench top, these losses can be at least five orders of magnitude higher than fusion power made, even when the fusor is in star mode. Hence, no fusor has ever come close to break-even energy output.
History.
The fusor was originally conceived by Philo T. Farnsworth, better known for his pioneering work in television. In the early 1930s, he investigated a number of vacuum tube designs for use in television, and found one that led to an interesting effect. In this design, which he called the "multipactor", electrons moving from one electrode to another were stopped in mid-flight with the proper application of a high-frequency magnetic field. The charge would then accumulate in the center of the tube, leading to high amplification. Unfortunately it also led to high erosion on the electrodes when the electrons eventually hit them, and today the multipactor effect is generally considered a problem to be avoided.
What particularly interested Farnsworth about the device was its ability to focus electrons at a particular point. One of the biggest problems in fusion research is to keep the hot fuel from hitting the walls of the container. If this is allowed to happen, the fuel cannot be kept hot enough for the fusion reaction to occur. Farnsworth reasoned that he could build an electrostatic plasma confinement system in which the "wall" fields of the reactor were electrons or ions being held in place by the "multipactor". Fuel could then be injected through the wall, and once inside it would be unable to escape. He called this concept a virtual electrode, and the system as a whole the "fusor".
Design.
Farnsworth's original fusor designs were based on cylindrical arrangements of electrodes, like the original multipactors. Fuel was ionized and then fired from small accelerators through holes in the outer (physical) electrodes. Once through the hole they were accelerated towards the inner reaction area at high velocity. Electrostatic pressure from the positively charged electrodes would keep the fuel as a whole off the walls of the chamber, and impacts from new ions would keep the hottest plasma in the center. He referred to this as inertial electrostatic confinement, a term that continues to be used to this day.
Work at Farnsworth Television labs.
All of this work had taken place at the Farnsworth Television labs, which had been purchased in 1949 by ITT Corporation, as part of its plan to become the next RCA. However, a fusion research project was not regarded as immediately profitable. In 1965, the board of directors started asking Geneen to sell off the Farnsworth division, but he had his 1966 budget approved with funding until the middle of 1967. Further funding was refused, and that ended ITT's experiments with fusion.
Things changed dramatically with the arrival of Robert Hirsch, and the introduction of the modified Hirsch-Meeks fusor patent. New fusors based on Hirsch's design were first constructed between 1964 and 1967. Hirsch published his design in a paper in 1967. His design included ion beams to shoot ions into the vacuum chamber.
The team then turned to the AEC, then in charge of fusion research funding, and provided them with a demonstration device mounted on a serving cart that produced more fusion than any existing "classical" device. The observers were startled, but the timing was bad; Hirsch himself had recently revealed the great progress being made by the Soviets using the tokamak. In response to this surprising development, the AEC decided to concentrate funding on large tokamak projects, and reduce backing for alternative concepts.
Recent developments.
In the early 1980s, disappointed by the slow progress on "big machines", a number of physicists took a fresh look at alternative designs. George H. Miley at the University of Illinois picked up on the fusor and re-introduced it into the field. A low but steady interest in the fusor has persisted since. An important development was the successful commercial introduction of a fusor-based neutron generator. From 2006 until his death in 2007, Robert W. Bussard gave talks on a reactor similar in design to the fusor, now called the polywell, that he stated would be capable of useful power generation. Most recently, the fusor has gained popularity among amateurs, who choose them as home projects due to their relatively low space, money, and power requirements. An online community of "fusioneers", The Open Source Fusor Research Consortium, or Fusor.net, is dedicated to reporting developments in the world of fusors and aiding other amateurs in their projects. The site includes forums, articles and papers done on the fusor, including Farnsworth's original patent, as well as Hirsch's patent of his version of the invention.
Fusion in fusors.
Basic fusion.
Nuclear fusion refers to reactions in which lighter nuclei are combined to become heavier nuclei. This process changes mass into energy which in may be captured to provide fusion power. Many types of atoms can be fused. The easiest to fuse are deuterium and tritium. This happens when the ions have to have a temperature of at least 4 keV (kiloelectronvolts) or about 45 million kelvins. The second easiest reaction is fusing deuterium with itself. Because this gas is cheaper, it is the fuel commonly used by amateurs. The ease of doing a fusion reaction is measured by its cross section.
Net power.
At such conditions, the atoms are ionized and make a plasma. The energy generated by fusion, inside a hot plasma cloud can be found with the following equation.
where:
This equation shows that energy varies with the temperature, density, speed of collision, and fuel used. To reach net power, fusion reactions have to occur fast enough to make up for energy losses. Any power plant using fusion will hold in this hot cloud. Plasma clouds lose energy through conduction and radiation. Conduction is when ions, electrons or neutrals touch a surface and leak out. Energy is lost with the particle. Radiation is when energy leaves the cloud as light. Radiation increases as the temperature rises. To get net power from fusion, you must overcome these losses. This leads to an equation for power output.
where:
John Lawson used this equation to estimate some conditions for net power based on a Maxwellian cloud. This is the Lawson criterion. Fusors typically suffer from conduction losses due to the wire cage being in the path of the recirculating plasma.
In fusors.
In the original fusor design, several small particle accelerators, essentially TV tubes with the ends removed, inject ions at a relatively low voltage into a vacuum chamber. In the Hirsch version of the fusor, the ions are produced by ionizing a dilute gas in the chamber. In either version there are two concentric spherical electrodes, the inner one being charged negatively with respect to the outer one (to about 80 kV). Once the ions enter the region between the electrodes, they are accelerated towards the center.
In the fusor, the ions are accelerated to several keV by the electrodes, so heating as such is not necessary (as long as the ions fuse before losing their energy by any process). Whereas 45 megakelvins is a very high temperature by any standard, the corresponding voltage is only 4 kV, a level commonly found in such devices as neon lights and televisions. To the extent that the ions remain at their initial energy, the energy can be tuned to take advantage of the peak of the reaction cross section or to avoid disadvantageous (for example neutron-producing) reactions that might occur at higher energies.
Various attempts have been made at increasing deuterium ionization rate, including heaters within "ion-guns", (similar to the "electron gun" which forms the basis for old-style television display tubes), as well as magnetron type devices, (which are the power sources for microwave ovens), which can enhance ion formation using high-voltage electro-magnetic fields. Any method which increases ion density (within limits which preserve ion mean-free path), or ion energy, can be expected to enhance the fusion yield, typically measured in the number of neutrons produced per second.
The ease with which the ion energy can be increased appears to be particularly useful when "high temperature" fusion reactions are considered, such as proton-boron fusion, which has plentiful fuel, requires no radioactive tritium, and produces no neutrons in the primary reaction.
Common considerations.
Modes of operation.
Fusors have at least two modes of operation (possibly more): Star Mode and Halo Mode. Halo mode is characterized by a broad symmetric glow, with one or two electron beams exiting the structure. There is little fusion. The halo mode occurs in higher pressure tanks, and as the vacuum improves, the device transitions to star mode. Star mode appears as bright beams of light emanating from the device center.
Power density.
Because the electric field made by the cages is negative, it cannot simultaneously trap both positively charged ions and negative electrons. Hence, there must be some regions of charge accumulation, which will result in an upper limit on the achievable density. This could place an upper limit on the machine's power density, which may keep it too low for power production.
Thermalization of the ion velocities.
When they first fall into the center of the fusor, the ions will all have the same energy, but the velocity distribution will rapidly approach a Maxwell–Boltzmann distribution. This would occur through simple Coulomb collisions in a matter of milliseconds, but beam-beam instabilities will occur orders of magnitude faster still. In comparison, any given ion will require a few minutes before undergoing a fusion reaction, so that the monoenergetic picture of the fusor, at least for power production, is not appropriate. One consequence of the thermalization is that some of the ions will gain enough energy to leave the potential well, taking their energy with them, without having undergone a fusion reaction.
Electrodes.
There are a number of unsolved challenges with the electrodes in a fusor power system. To begin with, the electrodes cannot influence the potential within themselves, so it would seem at first glance that the fusion plasma would be in more or less direct contact with the inner electrode, resulting in contamination of the plasma and destruction of the electrode. However, the majority of the fusion tends to occur in microchannels formed in areas of minimum electric potential, seen as visible "rays" penetrating the core. These form because the forces within the region correspond to roughly stable "orbits". Approximately 40% of the high energy ions in a typical grid operating in star mode may be within these microchannels. Nonetheless, grid collisions remain the primary energy loss mechanism for Farnsworth-Hirsch fusors. Complicating issues is the challenge in cooling the central electrode; any fusor producing enough power to run a power plant seems destined to also destroy its inner electrode. As one fundamental limitation, any method which produces a neutron flux that is captured to heat a working fluid will also bombard its electrodes with that flux, heating them as well.
Attempts to resolve these problems include Bussard's Polywell system, D. C. Barnes' modified Penning trap approach, and the University of Illinois's fusor which retains grids but attempts to more tightly focus the ions into microchannels to attempt to avoid losses. While all three are Inertial electrostatic confinement (IEC) devices, only the last is actually a "fusor".
Radiation.
Nonrelativistic particles will radiate energy as light when they change speed. This loss rate can be estimated using the Larmor formula. Inside a fusor there is a cloud of ions and electrons. These particles will accelerate or decelerate as they move about. These changes in speed make the cloud lose energy as light. The radiation from a fusor can (at least) be in the visible, ultraviolet and X-ray spectrum, depending on the type of fusor used. These changes in speed can be due to electrostatic interactions between particles (ion to ion, ion to electron, electron to electron). This is referred to bremsstrahlung radiation, and is common in fusors. Changes in speed can also be due to interactions between the particle and the electric field. Since there are no magnetic fields, fusors emit no Cyclotron radiation at slow speeds, or synchrotron radiation at high speeds.
In "Fundamental limitations on plasma fusion systems not in thermodynamic equilibrium", Todd Rider argues that a quasineutral isotropic plasma will lose energy due to Bremsstrahlung at a rate prohibitive for any fuel other than D-T (or possibly D-D or D-He3). This paper is not applicable to IEC fusion, as a quasineutral plasma cannot be contained by an electric field, which is a fundamental part of IEC fusion. However, in an earlier paper, "A general critique of inertial-electrostatic confinement fusion systems", Rider addresses the common IEC devices directly, including the fusor. In the case of the fusor the electrons are generally separated from the mass of the fuel isolated near the electrodes, which limits the loss rate. However, Rider demonstrates that practical fusors operate in a range of modes that either lead to significant electron mixing and losses, or alternately lower power densities. This appears to be a sort of catch-22 that limits the output of any fusor-like system.
Commercial Applications.
Neutron Source.
The fusor has been demonstrated as a viable neutron source. Typical fusors cannot reach fluxes as high as nuclear reactor or particle accelerator sources, but are sufficient for many uses. Importantly, the neutron generator easily sits on a benchtop, and can be turned off at the flick of a switch. A commercial fusor was developed as a non-core business within DaimlerChrysler Aerospace - Space Infrastructure, Bremen between 1996 and early 2001. After the project was effectively ended, the former project manager established a company which is called NSD-Fusion. To date, the highest neutron flux achieved by a fusor-like device has been 3 × 1011 neutrons per second with the deuterium-deuterium fusion reaction.
Medical isotopes.
Commercial startups have used the neutron fluxes generated by fusors to generate Mo-99, an isotope used for medical care.
Fusor examples.
Professional.
Fusors have been theoretically studied at multiple institutions, including: Kyoto University, and Kyushu University. Researchers meet annually at the US-Japan Workshop on Inertial Electrostatic Confinement Fusion. Listed here, are actual machines built.
Amateur.
A number of amateurs have built working fusors and detected neutrons. Many fusor enthusiasts connect on forums and message boards online. Below are some examples of working fusors. 

</doc>
<doc id="42890" url="https://en.wikipedia.org/wiki?curid=42890" title="Philo Farnsworth">
Philo Farnsworth

Philo Taylor Farnsworth (August 19, 1906 – March 11, 1971) was an American inventor and television pioneer. He made many contributions that were crucial to the early development of all-electronic television. He is perhaps best known for his 1927 invention of the first fully functional all-electronic image pickup device (video camera tube), the "image dissector", as well as the first fully functional and complete all-electronic television system. He was also the first person to demonstrate such a system to the public. Farnsworth developed a television system complete with receiver and camera, which he produced commercially in the form of the Farnsworth Television and Radio Corporation, from 1938 to 1951, in Fort Wayne, Indiana.
In later life, Farnsworth invented a small nuclear fusion device, the Farnsworth–Hirsch fusor, or simply "fusor", employing inertial electrostatic confinement (IEC). Although not a practical device for generating nuclear energy, the fusor serves as a viable source of neutrons. The design of this device has been the acknowledged inspiration for other fusion approaches including the Polywell reactor concept in terms of a general approach to fusion design. Farnsworth held 300 patents, mostly in radio and television.
Early life.
Philo T. Farnsworth was born August 19, 1906, the eldest of five children of Lewis Edwin Farnsworth and Serena Amanda Bastian, an LDS couple then living in a small log cabin built by Lewis's father in a place called Indian Creek near Beaver, Utah. In 1918, the family moved to a relative's 240-acre ranch near Rigby, Idaho, where Lewis supplemented his farming income by hauling freight with his horse-drawn wagon. Philo was excited to find his new home was wired for electricity, with a Delco generator providing power for lighting and farm machinery. He was a quick student in mechanical and electrical technology, repairing the troublesome generator, and upon finding a burned out electric motor among some items discarded by the previous tenants, proceeding to rewind the armature and convert his mother's hand-powered washing machine into an electric-powered one. Philo developed an early interest in electronics after his first telephone conversation with an out-of-state relative and the discovery of a large cache of technology magazines in the attic of the family’s new home, and won a $25 first prize in a pulp-magazine contest for inventing a magnetized car lock.
Farnsworth excelled in chemistry and physics at Rigby High School. He asked his high school science teacher, Justin Tolman, for advice about an electronic television system he was contemplating. He provided the teacher with sketches and diagrams covering several blackboards to show how it might be accomplished electronically. He asked his teacher if he should go ahead with his ideas, and he was encouraged to do so.
One of the drawings he did on a blackboard for his chemistry teacher was recalled and reproduced for a patent interference case between Farnsworth and Radio Corporation of America (RCA).
In 1923, the Farnsworths moved to Provo, Utah, and Farnsworth attended Brigham Young High School beginning that fall. His father died of pneumonia in January 1924, at age 58, and Farnsworth, as eldest son, assumed responsibility for sustaining the family while still attending high school and graduating in June 1924. He went on to attend Brigham Young University that year, and to earn Junior Radio-Trician certification from the National Radio Institute, adding a full certification in 1925. While attending college, he met Provo High School student Elma “Pem” Gardner, (February 25, 1908 – April 27, 2006), whom he would later marry.
Later in 1924, Farnsworth applied to the United States Naval Academy in Annapolis, Maryland, where he was recruited after he earned the nation's second highest score on academy tests. However, he was already thinking ahead to his television projects and, upon learning the government would own his patents if he stayed in the military, he sought and received an honorable discharge within months, under a provision in which the eldest child in a fatherless family could be excused from military service in order to provide for his family. He returned to Provo and enrolled again at Brigham Young University, where he was allowed to take advanced science classes.
Philo worked while his sister Agnes, the elder of the two sisters, took charge of the family home and the second-floor boarding house (with the help of a cousin then living with the family). The Farnsworths later moved into half of a duplex, with family friends the Gardners moving into the other side when it became vacant. Philo developed a close friendship with Pem Gardner's brother, Cliff Gardner, who shared Farnsworth's interest in electronics. The two moved to Salt Lake City to start a radio repair business.
The business failed and Gardner returned to Provo. Farnsworth remained in Salt Lake City, and through enrollment in a University of Utah job-placement service became acquainted with Leslie Gorrell and George Everson, a pair of San Francisco philanthropists who were then conducting a Salt Lake City Community Chest fundraising campaign.
They agreed to fund Farnsworth's early television research with an initial $6,000 in backing, and set up a laboratory in Los Angeles for Farnsworth to carry out his experiments. Before relocating to California, Farnsworth married Pem Gardner Farnsworth (February 25, 1908 – April 27, 2006), on May 27, 1926, and the two traveled to the West Coast in a Pullman coach.
Career.
A few months after arriving in California, Farnsworth was prepared to show his models and drawings to a patent attorney who was nationally recognized as an authority on electrophysics. Everson and Gorrell agreed that Farnsworth should apply for patents for his designs, a decision which proved crucial in later disputes with RCA. Most television systems in use at the time used image scanning devices ("rasterizers") employing rotating "Nipkow disks" comprising lenses arranged in spiral patterns such that they swept across an image in a succession of short arcs while focusing the light they captured on photosensitive elements, thus producing a varying electrical signal corresponding to the variations in light intensity. Farnsworth recognized the limitations of the mechanical systems, and that an all-electronic scanning system could produce a superior image for transmission to a receiving device.
On September 7, 1927, Farnsworth's image dissector camera tube transmitted its first image, a simple straight line, to a receiver in another room of his laboratory at 202 Green Street in San Francisco. Pem Farnsworth recalled in 1985 that her husband broke the stunned silence of his lab assistants by saying, "There you are — electronic television!" The source of the image was a glass slide, backlit by an arc lamp. An extremely bright source was required because of the low light sensitivity of the design. By 1928, Farnsworth had developed the system sufficiently to hold a demonstration for the press. His backers had demanded to know when they would see dollars from the invention, so the first image shown was, appropriately, a dollar sign. In 1929, the design was further improved by elimination of a motor-generator, so the television system now had no mechanical parts. That year, Farnsworth transmitted the first live human images using his television system, including a three and a half-inch image of his wife Pem.
Many inventors had built electromechanical television systems before Farnsworth's seminal contribution, but Farnsworth designed and built the world's first working all-electronic television system, employing electronic scanning in both the pickup and display devices. He first demonstrated his system to the press on September 3, 1928, and to the public at the Franklin Institute in Philadelphia on August 25, 1934.
In 1930, Vladimir Zworykin, who had been developing his own all-electronic television system at Westinghouse in Pittsburgh since 1923, but which he had never been able to make work or satisfactorily demonstrate to his superiors, was recruited by RCA to lead its television development department. Before leaving his old employer, Zworykin visited Farnsworth's laboratory and was sufficiently impressed with the performance of the Image Dissector that he reportedly had his team at Westinghouse make several copies of the device for experimentation. But Zworykin later abandoned research on the Image Dissector, which at the time required extremely bright illumination of its subjects to be effective, and turned his attention to what would become the Iconoscope. In a 1970s series of videotaped interviews, Zworykin recalled that, "Farnsworth was closer to this thing you're using now a video camera than anybody, because he used the cathode-ray tube for transmission. But, Farnsworth didn't have the mosaic discrete light elements, he didn't have storage. Therefore, definition was very low... But he was very proud, and he stuck to his method." Contrary to Zworykin's statement, Farnsworth's patent #2,087,683 for the Image Dissector (filed April 26, 1933) features a "low velocity" method of electron scanning, and describes "discrete particles" whose "potential" is manipulated and "saturated" to varying degrees depending on their velocity. Farnsworth's patent numbers 2,140,695 and 2,233,888 are for a "charge storage dissector" and "charge storage amplifier," respectively.
In 1931, David Sarnoff of RCA offered to buy Farnsworth's patents for US$100,000, with the stipulation that he become an employee of RCA, but Farnsworth refused. In June of that year, Farnsworth joined the Philco company and moved to Philadelphia along with his wife and two children. RCA would later file an interference suit against Farnsworth, claiming Zworykin's 1923 patent had priority over Farnsworth's design, despite the fact it could present no evidence that Zworykin had actually produced a functioning transmitter tube before 1931. Farnsworth had lost two interference claims to Zworykin in 1928, but this time he prevailed and the U.S. Patent Office rendered a decision in 1934 awarding priority of the invention of the image dissector to Farnsworth. RCA lost a subsequent appeal, but litigation over a variety of issues continued for several years with Sarnoff finally agreeing to pay Farnsworth royalties. Zworykin received a patent in 1928 for a color transmission version of his 1923 patent application; he also divided his original application in 1931, receiving a patent in 1935, while a second one was eventually issued in 1938 by the Court of Appeals on a non-Farnsworth-related interference case, and over the objection of the Patent Office.
In 1932, while in England to raise money for his legal battles with RCA, Farnsworth met with John Logie Baird, a Scottish inventor who had given the world's first public demonstration of a working television system in London in 1926, using an electro-mechanical imaging system, and who was seeking to develop electronic television receivers. Baird demonstrated his mechanical system for Farnsworth. Baird's company directors pursued a merger with Farnsworth, paying $50,000 to supply electronic television equipment and provide access to Farnsworth patents. Baird and Farnsworth competed with EMI for the U.K. standard television system, but EMI merged with the Marconi Company in 1934, gaining access to the RCA Iconoscope patents. After trials of both systems, the BBC committee chose the Marconi-EMI system, which was by then virtually identical to RCA's system. The image dissector scanned well but had poor light sensitivity compared to the Marconi-EMI Iconoscopes, dubbed "Emitrons".
In March 1932, Philco denied Farnsworth time to travel to Utah to bury his young son Kenny, placing a strain on Farnsworth's marriage, and possibly marking the beginning of his struggle with depression. In May 1933, Philco severed its relationship with Farnsworth because, said Everson, "it become apparent that Philo's aim at establishing a broad patent structure through research [was not identical with the production program of Philco." In Everson's view the decision was mutual and amicable. Philco set up shop at 127 East Mermaid Lane in Philadelphia, and In 1934 held the first public exhibition of his device at the Franklin Institute in that city.
After sailing to Europe in 1934, Farnsworth secured an agreement with Goerz-Bosch-Fernseh in Germany. Some image dissector cameras were used to broadcast the 1936 Olympic Games in Berlin.
Farnsworth returned to his laboratory, and by 1936 his company was regularly transmitting entertainment programs on an experimental basis. That same year, while working with University of Pennsylvania biologists, Farnsworth developed a process to sterilize milk using radio waves. He also invented a fog-penetrating beam for ships and airplanes.
In 1936 he attracted the attention of "Collier's Weekly", which described his work in glowing terms. "One of those amazing facts of modern life that just don't seem possible – namely, electrically scanned television that seems destined to reach your home next year, was largely given to the world by a nineteen-year-old boy from Utah ... Today, barely thirty years old he is setting the specialized world of science on its ears."
In 1938, Farnsworth established the Farnsworth Television and Radio Corporation in Fort Wayne, Indiana, with E. A. Nicholas as president and himself as director of research. In September 1939, after a more than decade-long legal battle, RCA finally conceded to a multi-year licensing agreement concerning Farnsworth's 1927 patent for television totaling $1 million. RCA was then free, after showcasing electronic television at New York World's Fair on April 20, 1939, to sell electronic television cameras to the public.
Farnsworth Television and Radio Corporation was purchased by International Telephone and Telegraph (ITT) in 1951. During his time at ITT, Farnsworth worked in a basement laboratory known as "the cave" on Pontiac Street in Fort Wayne. From there he introduced a number of breakthrough concepts, including a defense early warning signal, submarine detection devices, radar calibration equipment and an infrared telescope. "Philo was a very deep person – tough to engage in conversation, because he was always thinking about what he could do next," said Art Resler, an ITT photographer who documented Farnsworth’s work in pictures. One of Farnsworth's most significant contributions at ITT was the PPI Projector, an enhancement on the iconic "circular sweep" radar display, which allowed safe air traffic control from the ground. This system developed in the 1950s was the forerunner of today’s air traffic control systems.
In addition to his electronics research, ITT management agreed to nominally fund Farnsworth's nuclear fusion research. He and staff members invented and refined a series of fusion reaction tubes called "fusors". For scientific reasons unknown to Farnsworth and his staff, the necessary reactions lasted no longer than thirty seconds. In December 1965, ITT came under pressure from its board of directors to terminate the expensive project and sell the Farnsworth subsidiary. It was only due to the urging of president Harold Geneen that the 1966 budget was accepted, extending ITT's fusion research for an additional year. The stress associated with this managerial ultimatum, however, caused Farnsworth to suffer a relapse. A year later he was terminated and eventually allowed medical retirement.
In the spring of 1967, Farnsworth and his family moved back to Utah to continue his fusion research at Brigham Young University, which presented him with an honorary doctorate. The university also offered him office space and an underground concrete bunker for the project. Realizing the fusion lab was to be dismantled at ITT, Farnsworth invited staff members to accompany him to Salt Lake City, as team members in Philo T. Farnsworth Associates (PTFA). By late 1968, the associates began holding regular business meetings and PTFA was underway. Although a contract with the National Aeronautics and Space Administration (NASA) was promptly secured, and more possibilities were within reach, financing stalled for the $24,000 in monthly expenses required to cover salaries and equipment rental.
By Christmas 1970, PTFA had failed to secure the necessary financing, and the Farnsworths had sold all their own ITT stock and cashed in Philo's life insurance policy to maintain organizational stability. The underwriter had failed to provide the financial backing that was to have supported the organization during its critical first year. The banks called in all outstanding loans, repossession notices were placed on anything not previously sold, and the Internal Revenue Service put a lock on the laboratory door until delinquent taxes were paid. In January 1971, PTFA disbanded. Farnsworth had begun abusing alcohol in his later years, and as a consequence he became seriously ill with pneumonia, and died on March 11, 1971.
Farnsworth's wife Elma Gardner "Pem" Farnsworth fought for decades after his death to assure his place in history. Farnsworth always gave her equal credit for creating television, saying, "my wife and I started this TV." She died on April 27, 2006, at age 98. The inventor and wife were survived by two sons, Russell (then living in New York), and Kent (then living in Fort Wayne, Indiana).
In 1999, "Time" magazine included Farnsworth in the "".
Inventions.
Electronic television.
Farnsworth worked out the principle of the image dissector in the summer of 1921, not long before his fifteenth birthday, and demonstrated the first working version on September 7, 1927, having turned 21 the previous August. A farm boy, his inspiration for scanning an image as series of lines came from the back-and-forth motion used to plow a field. In the course of a patent interference suit brought by RCA in 1934 and decided in February 1935, his high school chemistry teacher, Justin Tolman, produced a sketch he had made of a blackboard drawing Farnsworth had shown him in spring 1922. Farnsworth won the suit; RCA appealed the decision in 1936 and lost. Although Farnsworth was paid royalties by RCA, he never became wealthy. The video camera tube that evolved from the combined work of Farnsworth, Zworykin and many others was used in all television cameras until the late 20th century, when alternate technologies such as charge-coupled devices started to appear.
Farnsworth also developed the "image oscillite", a cathode ray tube that displayed the images captured by the image dissector.
Farnsworth called his device an image dissector because it converted individual elements of the image into electricity one at a time. He replaced the spinning disks with caesium, an element that emits electrons when exposed to light.
Fusor.
The Farnsworth–Hirsch fusor is an apparatus designed by Farnsworth to create nuclear fusion. Unlike most controlled fusion systems, which slowly heat a magnetically confined plasma, the fusor injects high-temperature ions directly into a reaction chamber, thereby avoiding a considerable amount of complexity.
When the Farnsworth-Hirsch fusor was first introduced to the fusion research world in the late 1960s, the fusor was the first device that could clearly demonstrate it was producing fusion reactions at all. Hopes at the time were high that it could be quickly developed into a practical power source. However, as with other fusion experiments, development into a power source has proven difficult. Nevertheless, the fusor has since become a practical neutron source and is produced commercially for this role.
Other inventions.
At the time he died, Farnsworth held 300 U.S. and foreign patents. His inventions contributed to the development of radar, infra-red night vision devices, the electron microscope, the baby incubator, the gastroscope, and the astronomical telescope.
TV appearance.
Although he was the man responsible for its technology, Farnsworth appeared only once on a television program. On July 3, 1957, he was a mystery guest ("Doctor X") on the CBS quiz show "I've Got A Secret". He fielded questions from the panel as they unsuccessfully tried to guess his secret ("I invented electronic television."). For stumping the panel, he received $80 and a carton of Winston cigarettes. Host Garry Moore then spent a few minutes discussing with Farnsworth his research on such projects as high-definition television, flat-screen receivers, and fusion power.
Farnsworth said, "There had been attempts to devise a television system using mechanical disks and rotating mirrors and vibrating mirrors — all mechanical. My contribution was to take out the moving parts and make the thing entirely electronic, and that was the concept that I had when I was just a freshman in high school in the Spring of 1921 at age 14." When Moore asked about others' contributions, Farnsworth agreed, "There are literally thousands of inventions important to television. I hold something in excess of 165 American patents." The host then asked about his current research, and the inventor replied, "In television, we're attempting first to make better utilization of the bandwidth, because we think we can eventually get in excess of 2000 lines instead of 525 ... and do it on an even narrower channel ... which will make for a much sharper picture. We believe in the picture-frame type of a picture, where the visual display will be just a screen. And we hope for a memory, so that the picture will be just as though it's pasted on there."
A letter to the editor of the Idaho Falls "Post Register" disputed that Farnsworth had made only one television appearance. Roy Southwick claimed "... I interviewed Mr. Farnsworth back in 1953 - the first day KID-TV went on the air." KID-TV, which later became KIDK-TV, was then located near the Rigby area where Farnsworth grew up.
Memorials and legacy.
In a 1996 videotaped interview by the Academy of Television Arts & Sciences, Elma Farnsworth recounts Philo's change of heart about the value of television, after seeing how it showed man walking on the moon, in real time, to millions of viewers:
In fiction, Farnsworth appeared in the "Futurama" episode "All The Presidents' Heads" as an ancestor of Professor Farnsworth and Philip J. Fry, and was referred to as having invented the television.
Farnsworth and the introduction of television are significant characters in "Carter Beats the Devil", a novel by Glen David Gold published in 2001 by Hyperion.
A fictionalized representation of Farnsworth appears in Canadian writer Wayne Johnston's 1994 novel, "Human Amusements". The main character in the novel appears as the protagonist in a television show that features Farnsworth as the main character. In the show, an adolescent Farnsworth invents many different devices (television among them) while being challenged at every turn by a rival inventor.
Misquote.
Farnsworth is sometimes quoted as telling his son Kent, with regard to television: "There's nothing on it worthwhile, and we're not going to watch it in this household, and I don't want it in your intellectual diet." Yet, his family's website makes it clear that this is "Kent's" summation of his father's view, rather than a direct quote.
Fort Wayne factory razing, residence history.
In 2010, the former Farnsworth factory in Fort Wayne, Indiana, was razed, eliminating the "cave," where many of Farnsworth's inventions were first created, and where its radio and television receivers and transmitters, television tubes, and radio-phonographs were mass-produced under the Farnsworth, Capehart, and Panamuse trade names. The facility was located at 3702 E. Pontiac St.
Also that year, additional Farnsworth factory artifacts were added to the Fort Wayne History Center's collection, including a radio-phonograph and three table-top radios from the 1940s, as well as advertising and product materials from the 1930s to the 1950s.
Farnsworth's Fort Wayne residence from 1948-1967, then the former Philo T. Farnsworth Television Museum, stands on the northwest corner of E. State and E. St Joseph Boulevards. The residence is recognized by an Indiana state historical marker and was listed on the National Register of Historic Places in 2013.
Marion, Indiana factory.
In addition to Fort Wayne, Farnsworth operated a factory in Marion, Indiana, that made shortwave radios used by American combat soldiers in World War II. Acquired by 
RCA after the war, the facility was located at 3301 S. Adams St.

</doc>
<doc id="42891" url="https://en.wikipedia.org/wiki?curid=42891" title="Adjustable spanner">
Adjustable spanner

An adjustable wrench (US) or adjustable spanner (UK) is a wrench with a "jaw" of adjustable width, allowing it to be used with different sizes of fastener head (nut, bolt, etc.) rather than just one fastener, as with a conventional fixed spanner.
Forms and names.
In many European as well as Middle Eastern countries (e.g. France, Germany, Portugal, Spain, Italy, Syria, Lebanon, Turkey, etc.) the adjustable wrench is called an "English key" as it was first invented in 1842 by the English engineer Richard Clyburn. Another English engineer, Edwin Beard Budding, is also credited with the invention. Improvements followed: on 22 September 1885 Enoch Harris received US patent 326868 for his spanner that permitted both the jaw width and the angle of the handles to be adjusted and locked. Other countries, like Denmark, Poland and Israel, refer to it as a "Swedish key" as its invention has been attributed to the Swedish inventor Johan Petter Johansson, who in 1892 received a patent for an improved design of the adjustable spanner that is still used today. Johansson's spanner was a further development of Clyburn's original "screw spanner". In some countries (e.g. Czech Republic, Egypt, Greece, Hungary, Serbia, Iran, Slovakia, Slovenia, Poland, Romania, Bulgaria) it is called "French key" (in Poland, "Swedish" or "French" key depending on type). In the USA, the tool is known as a Crescent wrench or an adjustable wrench.
There are many forms of adjustable spanners, from the taper locking spanners which needed a hammer to set the movable jaw to the size of the nut, to the modern screw adjusted spanner. Some adjustable spanners automatically adjust to the size of the nut. Simpler models use a serrated edge to lock the movable jaw to size, while more sophisticated versions are digital types that use sheets or feelers to set the size.
The fixed jaw can withstand bending stress far better than can the movable jaw, because the latter is supported only by the flat surfaces on either side of the guide slot, not the full thickness of the tool. The tool is therefore usually angled so that the movable jaw's area of contact is closer to the body of the tool, which means less bending stress.
Monkey wrenches are another type of adjustable spanner with a long history; the origin of the name is unclear.
The type of straight adjustable spanner with jaws at right angles to the handle shown here as an "English Key" is mainly called a "King Dick" spanner in the United Kingdom because of a popular British brand of small, handy and reliable adjustable spanner used throughout the 1900s and used in great numbers during World War 2.
Proper use.
The movable jaw should be snugly adjusted to the nut or bolt head in order to prevent damage to the fastener's head, or "rounding". This type of spanner should not be used on a rounded off nut, as this can overload the movable jaw. Nor should such a wrench be used "end on" in cramped quarters (except perhaps when the nut is barely more than finger-tight), where a socket wrench is more appropriate.
Some cheaper brands' jaws move when twisting on tight nuts.
Famous brands.
In the United States and Canada, the adjustable spanner (adjustable wrench) is colloquially referred to as a "crescent wrench" due to the widespread Crescent brand of adjustable wrenches. The Crescent brand of hand tools is owned and marketed by Apex Tool Group, LLC. In some parts of Europe, adjustable spanners are often called a Bahco. This term refers to the company of the Swedish inventor Johan Petter Johansson, which was originally called B.A. (Bernt August) Hjort & Company.
The Swedes themselves call the key "skiftnyckel" which is translated into adjustable key (shifting key).

</doc>
