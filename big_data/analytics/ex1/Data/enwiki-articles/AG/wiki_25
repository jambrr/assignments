<doc id="41524" url="https://en.wikipedia.org/wiki?curid=41524" title="Renaissance architecture">
Renaissance architecture

Renaissance architecture is the architecture of the period between the early 14th and early 17th centuries in different regions of Europe, demonstrating a conscious revival and development of certain elements of ancient Greek and Roman thought and material culture. Stylistically, Renaissance architecture followed Gothic architecture and was succeeded by Baroque architecture. Developed first in Florence, with Filippo Brunelleschi as one of its innovators, the Renaissance style quickly spread to other Italian cities. The style was carried to France, Germany, England, Russia and other parts of Europe at different dates and with varying degrees of impact.
Renaissance style places emphasis on symmetry, proportion, geometry and the regularity of parts as they are demonstrated in the architecture of classical antiquity and in particular ancient Roman architecture, of which many examples remained. Orderly arrangements of columns, pilasters and lintels, as well as the use of semicircular arches, hemispherical domes, niches and aedicules replaced the more complex proportional systems and irregular profiles of medieval buildings.
Historiography.
The word "Renaissance" derived from the term ""la rinascita"", which means rebirth, first appeared in Giorgio Vasari's "Vite de' più eccellenti architetti, pittori, et scultori Italiani" The Lives of the Artists, 1550–60.
Although the term Renaissance was used first by the French historian Jules Michelet, it was given its more lasting definition from the Swiss historian Jacob Burckhardt, whose book, "Die Kultur der Renaissance in Italien" 1860,The Civilization of the Renaissance in Italy, 1860, English translation, by SGC Middlemore, in 2 vols., London, 1878) was influential in the development of the modern interpretation of the Italian Renaissance. The folio of measured drawings "Édifices de Rome moderne; ou, Recueil des palais, maisons, églises, couvents et autres monuments" (The Buildings of Modern Rome), first published in 1840 by Paul Letarouilly, also played an important part in the revival of interest in this period.Erwin Panofsky, "Renaissance and Renascences in Western Art", (New York: Harper and Row, 1960) The Renaissance style was recognized by contemporaries in the term ""all'antica"", or "in the ancient manner" (of the Romans).
Development in Italy – influences.
Italy of the 15th century, and the city of Florence in particular, was home to the Renaissance. It is in Florence that the new architectural style had its beginning, not slowly evolving in the way that Gothic grew out of Romanesque, but consciously brought to being by particular architects who sought to revive the order of a past "Golden Age". The scholarly approach to the architecture of the ancient coincided with the general revival of learning. A number of factors were influential in bringing this about.
Architectural.
Italian architects had always preferred forms that were clearly defined and structural members that expressed their purpose. Many Tuscan Romanesque buildings demonstrate these characteristics, as seen in the Florence Baptistery and Pisa Cathedral.
Italy had never fully adopted the Gothic style of architecture. Apart from the Cathedral of Milan, (influenced by French Rayonnant Gothic), few Italian churches show the emphasis on vertical, the clustered shafts, ornate tracery and complex ribbed vaulting that characterise Gothic in other parts of Europe.
The presence, particularly in Rome, of ancient architectural remains showing the ordered Classical style provided an inspiration to artists at a time when philosophy was also turning towards the Classical.
Political.
In the 15th century, Florence, Venice and Naples extended their power through much of the area that surrounded them, making the movement of artists possible. This enabled Florence to have significant artistic influence in Milan, and through Milan, France.
In 1377, the return of the Pope from the Avignon Papacy and the re-establishment of the Papal court in Rome, brought wealth and importance to that city, as well as a renewal in the importance of the Pope in Italy, which was further strengthened by the Council of Constance in 1417. Successive Popes, especially Julius II, 1503–13, sought to extend the Pope’s temporal power throughout Italy.
Commercial.
In the early Renaissance, Venice controlled sea trade over goods from the East. The large towns of Northern Italy were prosperous through trade with the rest of Europe, Genoa providing a seaport for the goods of France and Spain; Milan and Turin being centers of overland trade, and maintaining substantial metalworking industries.
Trade brought wool from England to Florence, ideally located on the river for the production of fine cloth, the industry on which its wealth was founded. By dominating Pisa, Florence gained a seaport, and also maintained dominance of Genoa.
In this commercial climate, one family in particular turned their attention from trade to the lucrative business of money-lending. The Medici became the chief bankers to the princes of Europe, becoming virtually princes themselves as they did so, by reason of both wealth and influence.
Along the trade routes, and thus offered some protection by commercial interest, moved not only goods but also artists, scientists and philosophers.
Religious.
The return of the Pope Gregory XI from Avignon in September 1377 and the resultant new emphasis on Rome as the center of Christian spirituality, brought about a boom in the building of churches in Rome such as had not taken place for nearly a thousand years. This commenced in the mid 15th century and gained momentum in the 16th century, reaching its peak in the Baroque period. The construction of the Sistine Chapel with its uniquely important decorations and the entire rebuilding of St Peter's, one of Christendom's most significant churches, were part of this process.
In wealthy republican Florence, the impetus for church-building was more civic than spiritual. The unfinished state of the enormous cathedral dedicated to the Blessed Virgin Mary did no honour to the city under her patronage. However, as the technology and finance were found to complete it, the rising dome did credit not only to the Blessed Virgin, its architect and the Church but also the Signoria, the Guilds and the sectors of the city from which the manpower to construct it was drawn. The dome inspired further religious works in Florence.
Philosophic.
The development of printed books, the rediscovery of ancient writings, the expanding of political and trade contacts and the exploration of the world all increased knowledge and the desire for education.
The reading of philosophies that were not based on Christian theology led to the development of Humanism through which it was clear that while God had established and maintained order in the Universe, it was the role of Man to establish and maintain order in Society.
Civil.
Through Humanism, civic pride and the promotion of civil peace and order were seen as the marks of citizenship. This led to the building of structures such as Brunelleschi's Hospital of the Innocents with its elegant colonnade forming a link between the charitable building and the public square, and the Laurentian Library where the collection of books established by the Medici family could be consulted by scholars.
Some major ecclesiastical building works were also commissioned, not by the church, but by guilds representing the wealth and power of the city. Brunelleschi’s dome at Florence Cathedral, more than any other building, belonged to the populace because the construction of each of the eight segments was achieved by a different sector of the city.
Patronage.
As in the Platonic academy of Athens, it was seen by those of Humanist understanding that those people who had the benefit of wealth and education ought to promote the pursuit of learning and the creation of that which was beautiful. To this end, wealthy families—the Medici of Florence, the Gonzaga of Mantua, the Farnese in Rome, the Sforzas in Milan—gathered around them people of learning and ability, promoting the skills and creating employment for the most talented artists and architects of their day.
Architectural theory.
During the Renaissance, architecture became not only a question of practice, but also a matter for theoretical discussion. Printing played a large role in the dissemination of ideas.
Principal phases.
Historians often divide the Renaissance in Italy into three phases. Whereas art historians might talk of an "Early Renaissance" period, in which they include developments in 14th-century painting and sculpture, this is usually not the case in architectural history. The bleak economic conditions of the late 14th century did not produce buildings that are considered to be part of the Renaissance. As a result, the word "Renaissance" among architectural historians usually applies to the period 1400 to ca. 1525, or later in the case of non-Italian Renaissances.
Historians often use the following designations:
Quattrocento.
In the "Quattrocento", concepts of architectural order were explored and rules were formulated. (See- Characteristics of Renaissance Architecture, below.) The study of classical antiquity led in particular to the adoption of Classical detail and ornamentation.
Space, as an element of architecture, was utilised differently from the way it had been in the Middle Ages. Space was organised by proportional logic, its form and rhythm subject to geometry, rather than being created by intuition as in Medieval buildings. The prime example of this is the Basilica di San Lorenzo in Florence by Filippo Brunelleschi (1377–1446).
High Renaissance.
During the "High Renaissance", concepts derived from classical antiquity were developed and used with greater surety. The most representative architect is Bramante (1444–1514) who expanded the applicability of classical architecture to contemporary buildings. His San Pietro in Montorio (1503) was directly inspired by circular Roman temples. He was, however, hardly a slave to the classical forms and it was his style that was to dominate Italian architecture in the 16th century.
Mannerism.
During the Mannerist period, architects experimented with using architectural forms to emphasize solid and spatial relationships. The Renaissance ideal of harmony gave way to freer and more imaginative rhythms. The best known architect associated with the Mannerist style was Michelangelo (1475–1564), who is credited with inventing the giant order, a large pilaster that stretches from the bottom to the top of a façade. He used this in his design for the Campidoglio in Rome.
Prior to the 20th century, the term "Mannerism" had negative connotations, but it is now used to describe the historical period in more general non-judgemental terms.
From Renaissance to Baroque.
As the new style of architecture spread out from Italy, most other European countries developed a sort of proto-Renaissance style, before the construction of fully formulated Renaissance buildings. Each country in turn then grafted its own architectural traditions to the new style, so that Renaissance buildings across Europe are diversified by region.
Within Italy the evolution of Renaissance architecture into Mannerism, with widely diverging tendencies in the work of Michelangelo and Giulio Romano and Andrea Palladio, led to the Baroque style in which the same architectural vocabulary was used for very different rhetoric.
Outside Italy, Baroque architecture was more widespread and fully developed than the Renaissance style, with significant buildings as far afield as Mexico and the Philippines.
Characteristics.
The obvious distinguishing features of Classical Roman architecture were adopted by Renaissance architects. However, the forms and purposes of buildings had changed over time, as had the structure of cities. Among the earliest buildings of the reborn Classicism were churches of a type that the Romans had never constructed. Neither were there models for the type of large city dwellings required by wealthy merchants of the 15th century. Conversely, there was no call for enormous sporting fixtures and public bath houses such as the Romans had built. The ancient orders were analysed and reconstructed to serve new purposes.
Plan.
The plans of Renaissance buildings have a square, symmetrical appearance in which proportions are usually based on a module. Within a church, the module is often the width of an aisle. The need to integrate the design of the plan with the façade was introduced as an issue in the work of Filippo Brunelleschi, but he was never able to carry this aspect of his work into fruition. The first building to demonstrate this was St. Andrea in Mantua by Alberti. The development of the plan in secular architecture was to take place in the 16th century and culminated with the work of Palladio.
Façade.
Façades are symmetrical around their vertical axis. Church façades are generally surmounted by a pediment and organised by a system of pilasters, arches and entablatures. The columns and windows show a progression towards the centre. One of the first true Renaissance façades was the Cathedral of Pienza (1459–62), which has been attributed to the Florentine architect Bernardo Gambarelli (known as Rossellino) with Alberti perhaps having some responsibility in its design as well.
Domestic buildings are often surmounted by a cornice. There is a regular repetition of openings on each floor, and the centrally placed door is marked by a feature such as a balcony, or rusticated surround. An early and much copied prototype was the façade for the Palazzo Rucellai (1446 and 1451) in Florence with its three registers of pilasters
Columns and pilasters.
The Roman orders of columns are used:- Tuscan, Doric, Ionic, Corinthian and Composite. The orders can either be structural, supporting an arcade or architrave, or purely decorative, set against a wall in the form of pilasters. During the Renaissance, architects aimed to use columns, pilasters, and entablatures as an integrated system. One of the first buildings to use pilasters as an integrated system was in the Old Sacristy (1421–1440) by Brunelleschi.
Arches.
Arches are semi-circular or (in the Mannerist style) segmental. Arches are often used in arcades, supported on piers or columns with capitals. There may be a section of entablature between the capital and the springing of the arch. Alberti was one of the first to use the arch on a monumental scale at the St. Andrea in Mantua.
Vaults.
Vaults do not have ribs. They are semi-circular or segmental and on a square plan, unlike the Gothic vault which is frequently rectangular. The barrel vault is returned to architectural vocabulary as at the St. Andrea in Mantua.
Domes.
The dome is used frequently, both as a very large structural feature that is visible from the exterior, and also as a means of roofing smaller spaces where they are only visible internally. After the success of the dome in Brunelleschi’s design for the Basilica di Santa Maria del Fiore and its use in Bramante’s plan for St. Peter's Basilica (1506) in Rome, the dome became an indispensable element in church architecture and later even for secular architecture, such as Palladio's Villa Rotonda.
Ceilings.
Roofs are fitted with flat or coffered ceilings. They are not left open as in Medieval architecture. They are frequently painted or decorated.
Doors.
Doors usually have square lintels. They may be set within an arch or surmounted by a triangular or segmental pediment.
Openings that do not have doors are usually arched and frequently have a large or decorative keystone.
Windows.
Windows may be paired and set within a semi-circular arch. They may have square lintels and triangular or segmental pediments, which are often used alternately. Emblematic in this respect is the Palazzo Farnese in Rome, begun in 1517.
In the Mannerist period the “Palladian” arch was employed, using a motif of a high semi-circular topped opening flanked with two lower square-topped openings. Windows are used to bring light into the building and in domestic architecture, to give views. Stained glass, although sometimes present, is not a feature.
Walls.
External walls are generally constructed of brick, rendered, or faced with stone in highly finished ashlar masonry, laid in straight courses. The corners of buildings are often emphasised by rusticated quoins. Basements and ground floors were often rusticated, as at the Palazzo Medici Riccardi (1444–1460) in Florence. Internal walls are smoothly plastered and surfaced with lime wash. For more formal spaces, internal surfaces are decorated with frescoes.
Details.
Courses, mouldings and all decorative details are carved with great precision. Studying and mastering the details of the ancient Romans was one of the important aspects of Renaissance theory. The different orders each required different sets of details. Some architects were stricter in their use of classical details than others, but there was also a good deal of innovation in solving problems, especially at corners. Mouldings stand out around doors and windows rather than being recessed, as in Gothic Architecture. Sculptured figures may be set in niches or placed on plinths. They are not integral to the building as in Medieval architecture.
Development in Italy – Early Renaissance.
The leading architects of the Early Renaissance or Quattrocento were Brunelleschi, Michelozzo and Alberti.
Brunelleschi.
The person generally credited with bringing about the Renaissance view of architecture is Filippo Brunelleschi, (1377–1446). The underlying feature of the work of Brunelleschi was "order".
In the early 15th century, Brunelleschi began to look at the world to see what the rules were that governed one's way of seeing. He observed that the way one sees regular structures such as the Baptistery of Florence and the tiled pavement surrounding it follows a mathematical order—linear perspective.
The buildings remaining among the ruins of ancient Rome appeared to respect a simple mathematical order in the way that Gothic buildings did not. One incontrovertible rule governed all Ancient Roman architecture—a semi-circular arch is exactly twice as wide as it is high. A fixed proportion with implications of such magnitude occurred nowhere in Gothic architecture. A Gothic pointed arch could be extended upwards or flattened to any proportion that suited the location. Arches of differing angles frequently occurred within the same structure. No set rules of proportion applied.
From the observation of the architecture of Rome came a desire for symmetry and careful proportion in which the form and composition of the building as a whole and all its subsidiary details have fixed relationships, each section in proportion to the next, and the architectural features serving to define exactly what those rules of proportion are. Brunelleschi gained the support of a number of wealthy Florentine patrons, including the Silk Guild and Cosimo de' Medici.
Florence Cathedral.
Brunelleschi's first major architectural commission was for the enormous brick dome which covers the central space of Florence's cathedral, designed by Arnolfo di Cambio in the 14th century but left unroofed. While often described as the first building of the Renaissance, Brunelleschi's daring design utilizes the pointed Gothic arch and Gothic ribs that were apparently planned by Arnolfio. It seems certain, however, that while stylistically Gothic, in keeping with the building it surmounts, the dome is in fact structurally influenced by the great dome of Ancient Rome, which Brunelleschi could hardly have ignored in seeking a solution. This is the dome of the Pantheon, a circular temple, now a church.
Inside the Pantheon's single-shell concrete dome is coffering which greatly decreases the weight. The vertical partitions of the coffering effectively serve as ribs, although this feature does not dominate visually. At the apex of the Pantheon's dome is an opening, 8 meters across. Brunelleschi was aware that a dome of enormous proportion could in fact be engineered without a keystone. The dome in Florence is supported by the eight large ribs and sixteen more internal ones holding a brick shell, with the bricks arranged in a herringbone manner. Although the techniques employed are different, in practice both domes comprise a thick network of ribs supporting very much lighter and thinner infilling. And both have a large opening at the top.
San Lorenzo.
The new architectural philosophy of the Renaissance is best demonstrated in the churches of San Lorenzo, and Santo Spirito in Florence. Designed by Brunelleschi in about 1425 and 1428 respectively, both have the shape of the Latin cross. Each has a modular plan, each portion being a multiple of the square bay of the aisle. This same formula controlled also the vertical dimensions. In the case of Santo Spirito, which is entirely regular in plan, transepts and chancel are identical, while the nave is an extended version of these. In 1434 Brunelleschi designed the first Renaissance centrally planned building, Santa Maria degli Angeli of Florence. It is composed of a central octagon surrounded by a circuit of eight smaller chapels. From this date onwards numerous churches were built in variations of these designs.
Michelozzo.
Michelozzo Michelozzi (1396–1472), was another architect under patronage of the Medici family, his most famous work being the Palazzo Medici Riccardi, which he was commissioned to design for Cosimo de' Medici in 1444. A decade later he built the Villa Medici at Fiesole. Among his other works for Cosimo are the library at the Convent of San Marco, Florence. He went into exile in Venice for a time with his patron. He was one of the first architects to work in the Renaissance style outside Italy, building a palace at Dubrovnik.
The Palazzo Medici Riccardi is Classical in the details of its pedimented windows and recessed doors, but, unlike the works of Brunelleschi and Alberti, there are no "orders" of columns in evidence. Instead, Michelozzo has respected the Florentine liking for rusticated stone. He has seemingly created three orders out of the three defined rusticated levels, the whole being surmounted by an enormous Roman-style cornice which juts out over the street by 2.5 meters.
Alberti.
Leon Battista Alberti, born in Genoa (1402–1472), was an important Humanist theoretician and designer whose book on architecture "De re Aedificatoria" was to have lasting effect. An aspect of Humanism was an emphasis of the anatomy of nature, in particular the human form, a science first studied by the Ancient Greeks. Humanism made man the measure of things. Alberti perceived the architect as a person with great social responsibilities.
He designed a number of buildings, but unlike Brunelleschi, he did not see himself as a builder in a practical sense and so left the supervision of the work to others. Miraculously, one of his greatest designs, that of the Church of Sant'Andrea in Mantua, was brought to completion with its character essentially intact. Not so the church of San Francesco in Rimini, a rebuilding of a Gothic structure, which, like Sant'Andrea, was to have a façade reminiscent of a Roman triumphal arch. This was left sadly incomplete.
Sant'Andrea is an extremely dynamic building both without and within. Its triumphal façade is marked by extreme contrasts. The projection of the order of pilasters that define the architectural elements, but are essentially non-functional, is very shallow. This contrasts with the gaping deeply recessed arch which makes a huge portico before the main door. The size of this arch is in direct contrast to the two low square-topped openings that frame it. The light and shade play dramatically over the surface of the building because of the shallowness of its mouldings and the depth of its porch. In the interior Alberti has dispensed with the traditional nave and aisles. Instead there is a slow and majestic progression of alternating tall arches and low square doorways, repeating the "triumphal arch" motif of the façade.
Two of Alberti’s best known buildings are in Florence, the Palazzo Rucellai and at Santa Maria Novella. For the palace, Alberti applied the classical orders of columns to the façade on the three levels, 1446–51. At Santa Maria Novella he was commissioned to finish the decoration of the façade. He completed the design in 1456 but the work was not finished until 1470.
The lower section of the building had Gothic niches and typical polychrome marble decoration. There was a large ocular window in the end of the nave which had to be taken into account. Alberti simply respected what was already in place, and the Florentine tradition for polychrome that was well established at the Baptistery of San Giovanni, the most revered building in the city. The decoration, being mainly polychrome marble, is mostly very flat in nature, but a sort of order is established by the regular compartments and the circular motifs which repeat the shape of the round window. For the first time, Alberti linked the lower roofs of the aisles to nave using two large scrolls. These were to become a standard Renaissance device for solving the problem of different roof heights and bridge the space between horizontal and vertical surfaces.
Spread of the Renaissance in Italy.
In the 15th century the courts of certain other Italian states became centres for spreading of Renaissance philosophy, art and architecture.
In Mantua at the court of the Gonzaga, Alberti designed two churches, the Basilica of Sant'Andrea and San Sebastiano.
Urbino was an important centre with the ancient ducal palace being extended for Federico da Montefeltro in the mid 15th century. The duke employed Luciano Laurana from Dalmatia, renowned for his expertise at fortification. The design incorporates much of the earlier medieval building and includes an unusual turreted three-storeyed façade. Laurana was assisted by Francesco di Giorgio Martini. Later parts of the building are clearly Florentine in style, particularly the inner courtyard, but it is not known who the designer was.
Ferrara, under the Este, was expanded in the late fifteenth century, with several new palaces being built such as the Palazzo dei Diamanti and Palazzo Schifanoia for Borso d'Este. In Milan, under the Visconti, the Certosa di Pavia was completed, and then later under the Sforza, the Castello Sforzesco was built.
In Venice, San Zaccaria received its Renaissance façade at the hands of Antonio Gambello and Mauro Codussi, begun in the 1480s. Giovanni Maria Falconetto, the Veronese architect-sculptor, introduced Renaissance architecture to Padua with the Loggia Cornaro in the garden of Alvise Cornaro.
In southern Italy, Renaissance masters were called to Naples by Alfonso V of Aragon after his conquest of the Kingdom of Naples. The most notable examples of Renaissance architecture in that city are the Cappella Caracciolo, attributed to Bramante, and the Palazzo Orsini di Gravina, built by Gabriele d'Angelo between 1513 and 1549.
High Renaissance.
In the late 15th century and early 16th century, architects such as Bramante, Antonio da Sangallo the Younger and others showed a mastery of the revived style and ability to apply it to buildings such as churches and city palazzo which were quite different from the structures of ancient times. The style became more decorated and ornamental, statuary, domes and cupolas becoming very evident.
The architectural period is known as the "High Renaissance" and coincides with the age of Leonardo, Michelangelo and Raphael.
Bramante.
Donato Bramante, (1444–1514), was born in Urbino and turned from painting to architecture, finding his first important patronage under Ludovico Sforza, Duke of Milan, for whom he produced a number of buildings over 20 years. After the fall of Milan to the French in 1499, Bramante travelled to Rome where he achieved great success under papal patronage.
Bramante’s finest architectural achievement in Milan is his addition of crossing and choir to the abbey church of Santa Maria delle Grazie (Milan). This is a brick structure, the form of which owes much to the Northern Italian tradition of square domed baptisteries. The new building is almost centrally planned, except that, because of the site, the chancel extends further than the transept arms. The hemispherical dome, of approximately 20 metres across, rises up hidden inside an octagonal drum pierced at the upper level with arched classical openings. The whole exterior has delineated details decorated with the local terracotta ornamentation.
In Rome Bramante created what has been described as "a perfect architectural gem", the Tempietto in the Cloister of San Pietro in Montorio. This small circular temple marks the spot where St Peter was martyred and is thus the most sacred site in Rome. The building adapts the style apparent in the remains of the Temple of Vesta, the most sacred site of Ancient Rome. It is enclosed by and in spatial contrast with the cloister which surrounds it. As approached from the cloister, as in the picture above, it is seen framed by an arch and columns, the shape of which are echoed in its free-standing form.
Bramante went on to work at the Vatican where he designed the impressive Cortili of St. Damaso and of the Belvedere. In 1506 Bramante’s design for Pope Julius II’s rebuilding of St. Peter’s Basilica was selected, and the foundation stone laid. After Bramante’s death and many changes of plan, Michelangelo, as chief architect, reverted to something closer to Bramante’s original proposal. See below- Michelangelo.
Sangallo.
Antonio da Sangallo the Younger, (1485–1546), was one of a family of military engineers. His uncle, Giuliano da Sangallo was one of those who submitted a plan for the rebuilding of St Peter’s and was briefly a co-director of the project, with Raphael.
Antonio da Sangallo also submitted a plan for St Peter’s and became the chief architect after the death of Raphael, to be succeeded himself by Michelangelo.
His fame does not rest upon his association with St Peter’s but in his building of the Farnese Palace, “the grandest palace of this period”, started in 1530. The impression of grandness lies in part in its sheer size, (56 m long by 29.5 meters high) and in its lofty location overlooking a broad piazza. It is also a building of beautiful proportion, unusual for such a large and luxurious house of the date in having been built principally of stuccoed brick, rather than of stone. Against the smooth pink-washed walls the stone quoins of the corners, the massive rusticated portal and the stately repetition of finely detailed windows give a powerful effect, setting a new standard of elegance in palace-building. The upper of the three equally sized floors was added by Michelangelo. It is probably just as well that this impressive building is of brick; the travertine for its architectural details came not from a quarry, but from the Colosseum.
Raphael.
Raphael, (1483–1520), Urbino, trained under Perugino in Perugia before moving to Florence, was for a time the chief architect for St. Peter’s, working in conjunction with Antonio Sangallo. He also designed a number of buildings, most of which were finished by others. His single most influential work is the Palazzo Pandolfini in Florence with its two stories of strongly articulated windows of a "tabernacle" type, each set around with ordered pilasters, cornice and alternate arched and triangular pediments.
Mannerism.
Mannerism in architecture was marked by widely diverging tendencies in the work of Michelangelo, Giulio Romano, Baldassare Peruzzi and Andrea Palladio, that led to the Baroque style in which the same architectural vocabulary was used for very different rhetoric.
Peruzzi.
Baldassare Peruzzi, (1481–1536), was an architect born in Siena, but working in Rome, whose work bridges the High Renaissance and the Mannerist.
His Villa Farnesina of 1509 is a very regular monumental cube of two equal stories, the bays being strongly articulated by orders of pilasters. The building is unusual for its frescoed walls.
Peruzzi’s most famous work is the Palazzo Massimo alle Colonne in Rome. The unusual features of this building are that its façade curves gently around a curving street. It has in its ground floor a dark central portico running parallel to the street, but as a semi enclosed space, rather than an open loggia. Above this rise three undifferentiated floors, the upper two with identical small horizontal windows in thin flat frames which contrast strangely with the deep porch, which has served, from the time of its construction, as a refuge to the city’s poor.
Giulio Romano.
Giulio Romano (1499–1546), was a pupil of Raphael, assisting him on various works for the Vatican. Romano was also a highly inventive designer, working for Federico II Gonzaga at Mantua on the Palazzo Te, (1524–1534), a project which combined his skills as architect, sculptor and painter. In this work, incorporating garden grottoes and extensive frescoes, he uses illusionistic effects, surprising combinations of architectural form and texture, and the frequent use of features that seem somewhat disproportionate or out of alignment. The total effect is eerie and disturbing. Ilan Rachum cites Romano as "“one of the first promoters of Mannerism”".
Michelangelo.
Michelangelo Buonarroti (1475–1564) was one of the creative giants whose achievements mark the High Renaissance. He excelled in each of the fields of painting, sculpture and architecture and his achievements brought about significant changes in each area. His architectural fame lies chiefly in two buildings: the interiors of the Laurentian Library and its lobby at the monastery of San Lorenzo in Florence, and St Peter's Basilica in Rome.
St Peter's was ""the greatest creation of the Renaissance"", and a great number of architects contributed their skills to it. But at its completion, there was more of Michelangelo’s design than of any other architect, before or after him.
St Peter's.
The plan that was accepted at the laying of the foundation stone in 1506 was that by Bramante. Various changes in plan occurred in the series of architects that succeeded him, but Michelangelo, when he took over the project in 1546, reverted to Bramante’s Greek-cross plan and redesigned the piers, the walls and the dome, giving the lower weight-bearing members massive proportions and eliminating the encircling aisles from the chancel and identical transept arms. Helen Gardner says: "Michelangelo, with a few strokes of the pen, converted its snowflake complexity into a massive, cohesive unity."
Michelangelo’s dome was a masterpiece of design using two masonry shells, one within the other and crowned by a massive lantern supported, as at Florence, on ribs. For the exterior of the building he designed a giant order which defines every external bay, the whole lot being held together by a wide cornice which runs unbroken like a rippling ribbon around the entire building.
There is a wooden model of the dome, showing its outer shell as hemispherical. When Michelangelo died in 1564, the building had reached the height of the drum. The architect who succeeded Michelangelo was Giacomo della Porta. The dome, as built, has a much steeper projection than the dome of the model. It is generally presumed that it was della Porta who made this change to the design, to lessen the outward thrust. But, in fact it is unknown who it was that made this change, and it equally possible, and a stylistic likelihood that the person who decided upon the more dynamic outline was Michelangelo himself, at some time during the years that he supervised the project.
Laurentian Library.
Michelangelo was at his most Mannerist in the design of the vestibule of the Laurentian Library, also built by him to house the Medici collection of books at the convent of San Lorenzo in Florence, the same San Lorenzo’s at which Brunelleschi had recast church architecture into a Classical mold and established clear formula for the use of Classical orders and their various components.
Michelangelo takes all Brunelleschi’s components and bends them to his will. The Library is upstairs. It is a long low building with an ornate wooden ceiling, a matching floor and crowded with corrals finished by his successors to Michelangelo’s design. But it is a light room, the natural lighting streaming through a long row of windows that appear positively crammed between the order of pilasters that march along the wall. The vestibule, on the other hand, is tall, taller than it is wide and is crowded by a large staircase that pours out of the library in what Pevsner refers to as a “flow of lava”, and bursts in three directions when it meets the balustrade of the landing. It is an intimidating staircase, made all the more so because the rise of the stairs at the center is steeper than at the two sides, fitting only eight steps into the space of nine.
The space is crowded and it is to be expected that the wall spaces would be divided by pilasters of low projection. But Michelangelo has chosen to use paired columns, which, instead of standing out boldly from the wall, he has sunk deep into recesses within the wall itself. In San Lorenzo's church nearby, Brunelleschi used little scrolling console brackets to break the strongly horizontal line of the course above the arcade. Michelangelo has borrowed Brunelleschi’s motifs and stood each pair of sunken columns on a pair of twin console brackets. Pevsner says the "“Laurenziana... reveals Mannerism in its most sublime architectural form”".
Giacomo della Porta.
Giacomo della Porta, (c.1533–1602), was famous as the architect who made the dome of St Peter’s Basilica a reality. The change in outline between the dome as it appears in the model and the dome as it was built, has brought about speculation as to whether the changes originated with della Porta or with Michelangelo himself.
Della Porta spent nearly all his working life in Rome, designing villas, palazzi and churches in the Mannerist style. One of his most famous works is the façade of the Church of the Gesù, a project that he inherited from his teacher Jacopo Barozzi da Vignola. Most characteristics of the original design are maintained, subtly transformed to give more weight to the central section, where della Porta uses, among other motifs, a low triangular pediment overlaid on a segmental one above the main door. The upper storey and its pediment give the impression of compressing the lower one. The center section, like that of Sant'Andrea at Mantua, is based on the Triumphal Arch, but has two clear horizontal divisions like Santa Maria Novella. See Alberti above. The problem of linking the aisles to the nave is solved using Alberti’s scrolls, in contrast to Vignola’s solution which provided much smaller brackets and four statues to stand above the paired pilasters, visually weighing down the corners of the building. The influence of the design may be seen in Baroque churches throughout Europe.
Andrea Palladio.
Andrea Palladio, (1508–80), "the most influential architect of the whole Renaissance"', was, as a stonemason, introduced to Humanism by the poet Giangiorgio Trissino. His first major architectural commission was the rebuilding of the Basilica Palladiana at Vicenza, in the Veneto where he was to work most of his life.
Palladio was to transform the architectural style of both palaces and churches by taking a different perspective on the notion of Classicism. While the architects of Florence and Rome looked to structures like the Colosseum and the Arch of Constantine to provide formulae, Palladio looked to classical temples with their simple peristyle form. When he used the “triumphal arch” motif of a large arched opening with lower square-topped opening on either side, he invariably applied it on a small scale, such as windows, rather than on a large scale as Alberti used it at Sant’Andrea’s. This Ancient Roman motif is often referred to as the "Palladian Arch".
The best known of Palladio’s domestic buildings is Villa Capra, otherwise known as "la Rotonda", a centrally planned house with a domed central hall and four identical façades, each with a temple-like portico like that of the Pantheon in Rome. At the Villa Cornaro, the projecting portico of the north façade and recessed loggia of the garden façade are of two ordered stories, the upper forming a balcony.
Like Alberti, della Porta and others, in the designing of a church façade, Palladio was confronted by the problem of visually linking the aisles to the nave while maintaining and defining the structure of the building. Palladio’s solution was entirely different from that employed by della Porta. At the church of San Giorgio Maggiore in Venice he overlays a tall temple, its columns raised on high plinths, over another low wide temple façade, its columns rising from the basements and its narrow lintel and pilasters appearing behind the giant order of the central nave.
Progression from Early Renaissance through to Baroque.
In Italy, there appears to be a seamless progression from Early Renaissance architecture through the High Renaissance and Mannerist to the Baroque style. Pevsner comments about the vestibule of the Laurentian Library that it "has often been said that the motifs of the walls show Michelangelo as the father of the Baroque".
While continuity may be the case in Italy, it was not necessarily the case elsewhere. The adoption of the Renaissance style of architecture was slower in some areas than in others, as may be seen in England, for example. Indeed, as Pope Julius II was having the ancient Basilica of St. Peter’s demolished to make way for the new, Henry VII of England was adding a glorious new chapel in the Perpendicular Gothic style to Westminster Abbey.
Likewise, the style that was to become known as Baroque evolved in Italy in the early 17th century, at about the time that the first fully Renaissance buildings were constructed at Greenwich and Whitehall in England, after a prolonged period of experimentation with Classical motifs applied to local architectural forms, or conversely, the adoption of Renaissance structural forms in the broadest sense with an absence of the formulae that governed their use. While the English were just discovering what the rules of Classicism were, the Italians were experimenting with methods of breaking them. In England, following the Restoration of the Monarchy in 1660, the architectural climate changed, and taste moved in the direction of the Baroque. Rather than evolving, as it did in Italy, it arrived fully fledged.
In a similar way, in many parts of Europe that had few purely classical and ordered buildings like Brunelleschi’s Santo Spirito and Michelozzo’s Medici Riccardi Palace, Baroque architecture appeared almost unheralded, on the heels of a sort of Proto-Renaissance local style. The spread of the Baroque and its replacement of traditional and more conservative Renaissance architecture was particularly apparent in the building of churches as part of the Counter Reformation.
Spread in Europe.
The 16th century saw the economic and political ascendancy of France and Spain, and then later of Holland, England, Germany and Russia. The result was that these places began to import the Renaissance style as indicators of their new cultural position. This also meant that it was not until about 1500 and later that signs of Renaissance architectural style began to appear outside Italy.
Though Italian architects were highly sought after, such as Sebastiano Serlio in France, Aristotile Fioravanti in Russia, and Francesco Fiorentino in Poland, soon, non-Italians were studying Italian architecture and translating it into their own idiom. These included Philibert de l'Orme (1510–1570) in France, Juan Bautista de Toledo (died: 1567) in Spain, Inigo Jones (1573–1652) in England and Elias Holl (1573–1646) in Germany.
Books or ornament prints with engraved illustrations demonstrating plans and ornament were very important in spreading Renaissance styles in Northern Europe, with among the most important authors being Androuet du Cerceau in France, and Hans Vredeman de Vries in the Netherlands, and Wendel Dietterlin, author of "Architectura" (1593–94) in Germany.
Croatia.
In the 15th century, Croatia was divided into three states – the northern and central part of Croatia and Slavonia were in union with the Kingdom of Hungary, while Dalmatia, with the exception of independent Dubrovnik, was under the rule of the Venetian Republic. The Cathedral of St.James in Šibenik, was begun in 1441 in the Gothic style by Giorgio da Sebenico "(Juraj Dalmatinac)". Its unusual construction does not use mortar, the stone blocks, pilasters and ribs being bonded with joints and slots in the way that was usual in wooden constructions. In 1477 the work was unfinished, and continued under Niccolò di Giovanni Fiorentino who respected the mode of construction and the plan of the former architect, but continued the work which includes the upper windows, the vaults and the dome, in the Renaissance style. The combination of a high barrel vault with lower half-barrel vaults over the aisles the gives the façade its distinctive trefoil shape, the first of this type in the region. The cathedral was listed as a UNESCO World Heritage List in 2001.
Kingdom of Hungary.
One of the earliest places to be influenced by the Renaissance style of architecture was the Kingdom of Hungary. The style appeared following the marriage of King Matthias Corvinus and Beatrice of Naples in 1476. Many Italian artists, craftsmen and masons arrived at Buda with the new queen. Important remains of the Early Renaissance summer palace of King Matthias can be found in Visegrád. The Ottoman conquest of Hungary after 1526 cut short the development of Renaissance architecture in the country and destroyed its most famous examples. Today, the only completely preserved work of Hungarian Renaissance architecture is the Bakócz Chapel (commissioned by the Hungarian cardinal Tamás Bakócz), now part of the Esztergom Basilica.
Russia.
Prince Ivan III introduced Renaissance architecture to Russia by inviting a number of architects from Italy, who brought new construction techniques and some Renaissance style elements with them, while in general following the traditional designs of the Russian architecture. In 1475 the Bolognese architect Aristotele Fioravanti came to rebuild the Cathedral of the Dormition in the Moscow Kremlin, damaged in an earthquake. Fioravanti was given the 12th-century Vladimir Cathedral as a model, and produced a design combining traditional Russian style with a Renaissance sense of spaciousness, proportion and symmetry.
In 1485 Ivan III commissioned the building of a royal Terem Palace within the Kremlin, with Aloisio da Milano being the architect of the first three floors. Aloisio da Milano, as well as the other Italian architects, also greatly contributed to the construction of the Kremlin walls and towers. The small banqueting hall of the Russian Tsars, called the Palace of Facets because of its facetted upper story, is the work of two Italians, Marco Ruffo and Pietro Solario, and shows a more Italian style. In 1505, an Italian known in Russia as Aleviz Novyi built 12 churches for Ivan III, including the Cathedral of the Archangel, a building remarkable for the successful blending of Russian tradition, Orthodox requirements and Renaissance style.
Poland.
Polish Renaissance architecture is divided into three periods:
The First period (1500–50), is the so-called "Italian". Most of Renaissance buildings were building of this time were by Italian architects, mainly from Florence including Francesco Fiorentino and Bartolomeo Berrecci (Wawel Courtyard, Sigismund's Chapel).
In the Second period (1550–1600), Renaissance architecture became more common, with the beginnings of Mannerist and under the influence of the Netherlands, particularly in Pomerania. Buildings include the New Cloth Hall in Kraków and city halls in Tarnów, Sandomierz, Chełm (demolished) and most famously in Poznań.
In the Third period (1600–50), the rising power of Jesuits and Counter Reformation gave impetus to the development of Mannerist architecture and Baroque.Wilfried Koch, "Style w architekturze", Warsaw 1996Tadeusz Broniewski, "Historia architektury dla wszystkich" Wydawnictwo Ossolineum, 1990Mieczysław Gębarowicz, "Studia nad dziejami kultury artystycznej późnego renesansu w Polsce", Toruń 1962</ref>
Crown of Bohemia.
The Renaissance style first appeared in the Crown of Bohemia in the 1490s. Bohemia together with its incorporated lands, especially Moravia, thus ranked among the areas of the Holy Roman Empire with the earliest known examples of the Renaissance architecture.
The lands of the Bohemian Crown were never part of the ancient Roman Empire, thus they missed their own ancient classical heritage and had to be dependent on the primarily Italian models. As well as in other Central European countries the Gothic style kept its position especially in the church architecture. The traditional Gothic architecture was considered timeless and therefore able to express the sacredness. The Renaissance architecture coexisted with the Gothic style in Bohemia and Moravia until the late 16th century (e. g. the residential part of a palace was built in the modern Renaissance style but its chapel was designed with Gothic elements). The façades of Czech Renaissance buildings were often decorated with sgraffito (figural or ornamental).
During the reign of Holy Roman Emperor and Bohemian King Rudolph II, the city of Prague became one of the most important European centers of the late Renaissance art (so-called Mannerism). Nevertheless, not many architecturally significant buildings have been preserved from that time.
France.
During the early years of the 16th century the French were involved in wars in northern Italy, bringing back to France not just the Renaissance art treasures as their war booty, but also stylistic ideas. In the Loire Valley a wave of building was carried and many Renaissance châteaux appeared at this time, the earliest example being the Château d'Amboise (c. 1495) in which Leonardo da Vinci spent his last years. The style became dominant under Francis I (See Châteaux of the Loire Valley).
Netherlands/Flanders.
As in painting, Renaissance architecture took some time to reach the Netherlands and did not entirely supplant the Gothic elements. An architect directly influenced by the Italian masters was Cornelis Floris de Vriendt, who designed the city hall of Antwerpen, finished in 1564. The style sometimes known as "Antwerp Mannerism", keeping a similar overall structure to late-Gothic buildings, but with larger windows and much florid decoration and detailing in Renaissance styles, was widely influential across Northern Europe, for example in Elizabethan architecture, and is part of the wider movement of Northern Mannerism.
In the early 17th century Dutch Republic, Hendrick de Keyser played an important role in developing the Amsterdam Renaissance style, which has local characteristics including the prevalence of tall narrow town-houses, the "trapgevel" or Dutch gable and the employment of decorative triangular pediments over doors and windows in which the apex rises much more steeply than in most other Renaissance architecture, but in keeping with the profile of the gable. Carved stone details are often of low profile, in strapwork resembling leatherwork, a stylistic feature originating in the School of Fontainebleau. This feature was exported to England.
Germany.
The Renaissance in Germany was inspired first by German philosophers and artists such as Albrecht Dürer and Johannes Reuchlin who visited Italy. Important early examples of this period are especially the Landshut Residence, the Castle in Heidelberg, Johannisburg Palace in Aschaffenburg, the City Hall and Fugger Houses in Augsburg and St. Michael in Munich. A particular form of Renaissance architecture in Germany is the Weser Renaissance, with prominent examples such as the City Hall of Bremen and the Juleum in Helmstedt.
In July 1567 the city council of Cologne approved a design in the Renaissance style by Wilhelm Vernukken for a two storied loggia for Cologne City Hall. St Michael in Munich is the largest Renaissance church north of the Alps. It was built by Duke William V of Bavaria between 1583 and 1597 as a spiritual center for the Counter Reformation and was inspired by the Church of il Gesù in Rome. The architect is unknown. Many examples of Brick Renaissance buildings can be found in Hanseatic old towns, such as Stralsund, Wismar, Lübeck, Lüneburg, Friedrichstadt and Stade. Notable German Renaissance architects include Friedrich Sustris, Benedikt Rejt, Abraham van den Blocke, Elias Holl and Hans Krumpper.
England.
Renaissance architecture arrived in England during the reign of Elizabeth I, having first spread through the Low countries where among other features it acquired versions of the Dutch gable, and Flemish strapwork in geometric designs adorning the walls. The new style tended to manifest itself in large square tall houses such as Longleat House.
The first great exponent of Italian Renaissance architecture in England was Inigo Jones (1573–1652), who had studied architecture in Italy where the influence of Palladio was very strong. Jones returned to England full of enthusiasm for the new movement and immediately began to design such buildings as the Queen's House at Greenwich in 1616 and the Banqueting House at Whitehall three years later. These works, with their clean lines, and symmetry were revolutionary in a country still enamoured with mullion windows, crenellations and turrets.
Spain.
In Spain, Renaissance began to be grafted to Gothic forms in the last decades of the 15th century. The new style is called Plateresque, because of the extremely decorated façade, that brought to the mind the decorative motifs of the intricately detailed work of silversmiths, the "Plateros". Classical orders and candelabra motifs ("a candelieri") combined freely into symmetrical wholes.
From the mid-sixteenth century, under such architects as Pedro Machuca, Juan Bautista de Toledo and Juan de Herrera there was a closer adherence to the art of ancient Rome, sometimes anticipating Mannerism, examples of which include the palace of Charles V in Granada and the Escorial.
Portugal.
As in Spain, the adoption of the Renaissance style in Portugal was gradual. The so-called Manueline style (c. 1490–1535) married Renaissance elements to Gothic structures with the superficial application of exuberant ornament similar to the Isabelline Gothic of Spain. Examples of Manueline include the Belém Tower, a defensive building of Gothic form decorated with Renaissance-style loggias, and the Jerónimos Monastery, with Renaissance ornaments decorating portals, columns and cloisters.
The first "pure" Renaissance structures appear under King John III, like the Chapel of Nossa Senhora da Conceição in Tomar (1532–40), the "Porta Especiosa" of Coimbra Cathedral and the Graça Church at Évora (c. 1530–1540), as well as the cloisters of the Cathedral of Viseu (c. 1528–1534) and Convent of Christ in Tomar (John III Cloisters, 1557–1591). The Lisbon buildings of São Roque Church (1565–87) and the Mannerist Monastery of São Vicente de Fora (1582–1629), strongly influenced religious architecture in both Portugal and its colonies in the next centuries.
Scandinavia.
The Renaissance architecture that found its way to Scandinavia was influenced by the Flemish architecture, and included high gables and a castle air as demonstrated in the architecture of Frederiksborg Palace. Consequently, much of the Neo-Renaissance to be found in the Scandinavian countries is derived from this source.
In Denmark, Renaissance architecture thrived during the reigns of Frederick II and especially Christian IV. Inspired by the French castles of the times, Flemish architects designed masterpieces such as Kronborg Castle in Helsingør and Frederiksborg Palace in Hillerod. Frederiksborg Palace (1602–1620) in Hillerod is the largest Renaissance palace in Scandinavia.
Elsewhere, in Sweden, with Gustav Vasa's seizure of power and the onset of the Protestant reformation, church construction and aristocratic building projects came to a near standstill. During this time period, several magnificent so-called Vasa castles appeared. They were erected at strategic locations to control the country as well as to accommodate the travelling royal court. Gripsholm Castle, Kalmar Castle and Vadstena Castle are known for their fusion of medieval elements with Renaissance architecture.
The architecture of Norway was influenced partly by the occurrence of the plague during the Renaissance era. After the Black Death, monumental construction in Norway came to a standstill. There are few examples of Renaissance architecture in Norway, the most prominent being renovations to the medieval Rosenkrantz Tower in Bergen, Barony Rosendal in Hardanger, and the contemporary Austrat manor near Trondheim, and parts of Akershus Fortress.
There is little evidence of Renaissance influence in Finnish architecture.
Baltic States.
The Renaissance arrived late in what is today Estonia, Latvia and Lithuania, the so-called Baltic States, and did not make a great imprint architecturally. It was a politically tumultuous time, marked by the decline of the State of the Teutonic Order and the Livonian War.
In Estonia, artistic influences came from Dutch, Swedish and Polish sources. The building of the Brotherhood of the Blackheads in Tallinn with a façade designed by Arent Passer, is the only truly Renaissance building in the country that has survived more or less intact. Significantly for these troubled times, the only other examples are purely military buildings, such as the "Fat Margaret" cannon tower, also in Tallinn.
Latvian Renaissance architecture was influenced by Polish-Lithuanian and Dutch style, with Mannerism following from Gothic without intermediaries. St. John's Church in the Latvian capital of Riga is example of an earlier Gothic church which was reconstructed in 1587–89 by the Dutch architect Gert Freze (Joris Phraeze). The prime example of Renaissance architecture in Latvia is the heavily decorated House of the Blackheads, rebuilt from an earlier Medieval structure into its present Mannerist forms as late as 1619–25 by the architects A. and L. Jansen. It was destroyed during World War II and rebuilt during the 1990s.
Lithuania meanwhile formed one half of the large Polish-Lithuanian commonwealth. Renaissance influences grew stronger during the reign of the Grand Dukes of Lithuania Sigismund I the Old and Sigismund II Augustus. The Palace of the Grand Dukes of Lithuania (destroyed in 1801, a copy built in 2002–2009) show Italian influences. Several architects of Italian origin were active in the country, including Bernardino Zanobi de Gianotis, Giovanni Cini and Giovanni Maria Mosca.
Legacy.
During the 19th century there was a conscious revival of the style in Renaissance Revival architecture, that paralleled the Gothic Revival. Whereas the Gothic style was perceived by architectural theorists as being the most appropriate style for Church building, the Renaissance palazzo was a good model for urban secular buildings requiring an appearance of dignity and reliability such as banks, gentlemen's clubs and apartment blocks. Buildings that sought to impress, such as the Paris Opera, were often of a more Mannerist or Baroque style. Architects of factories, office blocks and department stores continued to use the Renaissance palazzo form into the 20th century, in Mediterranean Revival Style architecture with an Italian Renaissance emphasis.
Many of the concepts and forms of Renaissance architecture can be traced through subsequent architectural movements—from Renaissance to High-Renaissance, to Mannerism, to Baroque (or Rococo), to Neo-Classicism, and to Eclecticism. While Renaissance style and motifs were largely purged from Modernism, they have been reasserted in some Postmodern architecture. The influence of Renaissance architecture can still be seen in many of the modern styles and rules of architecture today.

</doc>
<doc id="41525" url="https://en.wikipedia.org/wiki?curid=41525" title="Amadeus">
Amadeus

Amadeus is a play by Peter Shaffer, which gives a highly fictionalized account of the lives of the composers Wolfgang Amadeus Mozart and Antonio Salieri. First performed in 1979, "Amadeus" was inspired by a short 1830 play by Alexander Pushkin called "Mozart and Salieri" (which was also used as the libretto for an opera of the same name by Nikolai Rimsky-Korsakov in 1897).
In the play, significant use is made of the music of Mozart, Salieri and other composers of the period. The premieres of Mozart's operas "The Abduction from the Seraglio", "The Marriage of Figaro," "Don Giovanni", and "The Magic Flute" are each the setting for key scenes of the play.
"Amadeus" won the 1981 Tony Award for Best Play. It was adapted by Shaffer for the 1984 Academy Award-winning film of the same name.
Plot.
Since the original run, Shaffer has extensively revised his play, including changes to plot details; the following is common to all revisions.
At the opening of the tale, Salieri is an old man, having long outlived his fame. Speaking directly to the audience, he claims to have used poison to assassinate Mozart, and promises to explain himself. The action then flashes back to the eighteenth century, at a time when Salieri has not met Mozart in person, but has heard of him and his music. He adores Mozart's compositions, and is thrilled at the chance to meet Mozart in person, during a salon at which some of Mozart's compositions will be played. When he finally does catch sight of Mozart, however, he is deeply disappointed to find that Mozart himself lacks the grace and charm of his compositions: When Salieri first meets him, Mozart is crawling around on his hands and knees, engaging in profane talk with his future bride Constanze Weber.
Salieri cannot reconcile Mozart's boorish behaviour with the genius that God has inexplicably bestowed upon him. Indeed, Salieri, who has been a devout Catholic all his life, cannot believe that God would choose Mozart over him for such a gift. Salieri renounces God and vows to do everything in his power to destroy Mozart as a way of getting back at his Creator.
Throughout much of the rest of the play, Salieri masquerades as Mozart's ally to his face while doing his utmost to destroy his reputation and any success his compositions may have. On more than one occasion it is only the direct intervention of the Emperor himself that allows Mozart to continue (interventions which Salieri opposes, and then is all too happy to take credit for when Mozart assumes it was he who intervened). Salieri also humiliates Mozart's wife when she comes to Salieri for aid, and smears Mozart's character with the Emperor and the court. A major theme in "Amadeus" is Mozart's repeated attempts to win over the aristocratic "public" with increasingly brilliant compositions, which are always frustrated either by Salieri or by the aristocracy's own inability to appreciate Mozart's genius.
The play ends with Salieri attempting suicide in a last attempt to be remembered, leaving a confession of having murdered Mozart with arsenic. He survives, however, and his confession is met with disbelief, leaving him to wallow once again in mediocrity.
Background and production.
Historical accuracy.
Shaffer used artistic license in his portrayals of both Mozart and Salieri. Documentary evidence suggests that there was some antipathy between the two men, but the idea that Salieri was the instigator of Mozart's demise is not taken seriously by scholars of the men's lives and careers. While historically there may have been actual rivalry and tension between Mozart and Salieri, there is also evidence that they enjoyed a relationship marked by mutual respect. As an example, Salieri later tutored Mozart's son Franz in music. He also conducted some of Mozart's works, both in Mozart's lifetime and afterwards.
Writer David Cairns called "Amadeus" "myth-mongering" and argued against Shaffer's portrait of Mozart as "two contradictory beings, sublime artist and fool", positing instead that Mozart was "fundamentally well-integrated". Cairns also rejects the "romantic legend" that Mozart always wrote out perfect manuscripts of works already completely composed in his head, citing major and prolonged revisions to several manuscripts (see: Mozart's compositional method).
Notable productions.
"Amadeus" was first presented at the Royal National Theatre, London in 1979, directed by Sir Peter Hall and starring Paul Scofield as Salieri, Simon Callow as Mozart, and Felicity Kendal as Constanze. (Callow later appeared in the film version in a different role.) It was later transferred in modified form to the West End, starring Frank Finlay as Salieri. The cast also included Andrew Cruickshank (Rosenberg), Basil Henson (von Strack), Philip Locke (Greybig), John Normington (Joseph II) and Nicholas Selby (van Swieten).
The play premiered on Broadway in 1980 with Ian McKellen as Salieri, Tim Curry as Mozart, and Jane Seymour as Constanze. It ran for 1,181 performances and was nominated for seven Tony Awards (best actor for both McKellen and Curry, best director for Peter Hall, best play, best costume design, lighting, and set design for John Bury), of which it won five (including a best actor Tony for McKellen). During the run of the play McKellen was replaced by John Wood, Frank Langella, David Dukes, David Birney, John Horton, and Daniel Davis. Curry was replaced by Peter Firth, Peter Crook, Dennis Boutsikaris, John Pankow, Mark Hamill, and John Thomas Waite. Also playing Constanze were Amy Irving, Suzanne Lederer, Michele Farr, Caris Corfman and Maureen Moore.
Adam Redfield and Terry Finn appeared as Mozart and Constanze, respectively, in the 1984 Virginia Stage Company production. Performed at the Wells Theatre in Norfolk, the drama was directed by Charles Towers.
The play was revived in 1999 at the Music Box Theatre, New York City, directed again by Peter Hall and ran for 173 performances (December 15, 1999 until May 14, 2000), receiving Tony Award nominations for Best Revival and Best Actor in a Play (David Suchet, who played Salieri). Also in the cast were Michael Sheen as Mozart, Cindy Katz as Constanze and David McCallum as Joseph II.
In July 2006, the Los Angeles Philharmonic presented a production of portions from the latest revision of the play at the Hollywood Bowl. Neil Patrick Harris starred as Mozart, Kimberly Williams-Paisley as Constanze Mozart, and Michael York as Salieri. Leonard Slatkin conducted the Philharmonic Orchestra.
Rupert Everett played Salieri in a production at the newly refurbished Chichester Festival Theatre from July 12, 2014 until August 2, 2014. The cast also featured Joshua McGuire as Mozart, Jessie Buckley as Constanze and John Standing as Count Orsini-Rosenberg. Simon Jones played Joseph II. Peter Shaffer attended the play himself at the closing performance.
Film and other adaptations.
The 1984 film adaptation won an Academy Award for Best Picture. In total, the film won eight Academy Awards. It starred F. Murray Abraham as Salieri (winning the Oscar for Best Actor for this role), Tom Hulce as Mozart, and Elizabeth Berridge as Constanze. The play was thoroughly reworked by Shaffer and the film's director, Miloš Forman with scenes and characters not found in the play. While the focus of the play is primarily on Salieri, the film goes further into developing the characters of both composers.
In 1983, BBC Radio 3 broadcast the play directed by Sir Peter Hall and starring the original cast of his National Theatre production. The cast included:
This radio production was re-broadcast on 2 January 2011 as part of Radio 3's "Genius of Mozart" season.
To celebrate Mozart's 250th birthday in 2006, BBC Radio 2 broadcast an adaptation by Neville Teller of Shaffer's play in eight fifteen-minute episodes directed by Peter Leslie Wilde and narrated by F. Murray Abraham as Salieri (re-broadcast 24 May – 2 June 2010 on BBC Radio 7).

</doc>
<doc id="41527" url="https://en.wikipedia.org/wiki?curid=41527" title="Contrapposto">
Contrapposto

Contrapposto is an Italian term that means counterpose. It is used in the visual arts to describe a human figure standing with most of its weight on one foot so that its shoulders and arms twist off-axis from the hips and legs. This gives the figure a more dynamic, or alternatively relaxed appearance. It can also be used to refer to multiple figures which are in counter-pose (or opposite pose) to one another. It can further encompass the tension as a figure changes from resting on a given leg to walking or running upon it (so-called "ponderation"). The leg that carries the weight of the body is known as the "engaged" leg, the relaxed leg is known as the "free" leg. Contrapposto is less emphasized than the more sinuous S Curve, and creates the illusion of past and future movement.
Contrapposto was an extremely important sculptural development, for its appearance marks the first time in Western art that the human body is used to express a psychological disposition. The balanced, harmonious pose of the Kritios Boy suggests a calm and relaxed state of mind, an evenness of temperament that is part of the ideal of man represented. 
From this point onwards Greek sculptors went on to explore how the body could convey the whole range of human experience, culminating in the desperate anguish and pathos of Laocoön and his Sons (1st century CE) in the Hellenistic period.
History.
Classical.
The first known statue to use contrapposto is Kritios Boy, c. 480 BC, so called because it was once attributed to the sculptor Kritios. It is possible, even likely, that earlier bronze statues had used the technique, but if they did, they have not survived and Kenneth Clark called the statue "the first beautiful nude in art". The statue is a Greek marble original and not a Roman copy.
Prior to the introduction of contrapposto, the statues that dominated ancient Greece were the archaic kouros (male) and the kore (female). Contrapposto has been used since the dawn of classical western sculpture. According to the "canon" of the Classical Greek Sculptor Polykleitos in the 4th century BC, it is one of the most important characteristics of his figurative works and those of his successors, Lysippos, Skopas, etc. The Polykletian statues – for example, Discophoros ("discus-bearer") and Doryphoros ("spear-bearer") – are idealized athletic young men with the divine sense, and captured in contrapposto. In these works, the pelvis is no longer axial with the vertical statue as in the archaic style of earlier Greek sculpture before "Kritios Boy".
Contrapposto can be clearly seen in the Roman copies of the statues of Hermes and Heracles. A famous example is the marble statue of Hermes with the infant Dionysus in Olympia by Praxiteles. It can also be seen in the Roman copies of Polyclitus' amazon.
Greek Art emphasized humanism along with the human mind and the human body’s beauty. Greek youths trained and competed in athletic contests in the nude. A great contribution to the contrapposto pose was the concept of a canon of proportions, in which mathematical properties are used to create proportions.
Renaissance.
Classical contrapposto was revived in the Renaissance by the Italian artists Donatello and Leonardo da Vinci, followed by Michelangelo, Raphael and other artists of the High Renaissance. One of the major achievements of the Italian Renaissance was the re-discovery of contrapposto, although in Mannerism it became greatly over-used.
Modern times.
The technique continues to be widely employed in sculpture.

</doc>
<doc id="41528" url="https://en.wikipedia.org/wiki?curid=41528" title="Forrest Gump">
Forrest Gump

Forrest Gump is a 1994 American epic romantic-comedy-drama film based on the 1986 novel of the same name by Winston Groom. The film was directed by Robert Zemeckis and stars Tom Hanks, Robin Wright, Gary Sinise, Mykelti Williamson, and Sally Field. The story depicts several decades in the life of Forrest Gump, a slow-witted but kind-hearted, good-natured and athletically prodigious man from Alabama who witnesses, and in some cases influences, some of the defining events of the latter half of the 20th century in the United States; more specifically, the period between Forrest's birth in 1944 and 1982. The film differs substantially from Winston Groom's novel, including Gump's personality and several events that were depicted.
Principal photography took place in late 1993, mainly in Georgia, North Carolina, and South Carolina. Extensive visual effects were used to incorporate the protagonist into archived footage and to develop other scenes. A comprehensive soundtrack was featured in the film, using music intended to pinpoint specific time periods portrayed on screen. Its commercial release made it a top-selling soundtrack, selling over twelve million copies worldwide.
Released in the United States on July 6, 1994, "Forrest Gump" became a commercial success as the top grossing film in North America released in that year, being the first major success for Paramount Pictures since the studio's sale to Viacom, earning over worldwide during its theatrical run. In 1995 it won the Academy Awards for Best Picture, Best Director for Robert Zemeckis, Best Actor for Tom Hanks, Best Adapted Screenplay for Eric Roth, Best Visual Effects, and Best Film Editing. It also garnered multiple other awards and nominations, including Golden Globes, People's Choice Awards, and Young Artist Awards, among others. Since the film's release varying interpretations have been made of the film's protagonist and its political symbolism. In 1996, a themed restaurant, Bubba Gump Shrimp Company, opened based on the film and has since expanded to multiple locations worldwide. The scene of Gump running across the country is often referred to when real-life people attempt the feat. In 2011, the Library of Congress selected "Forrest Gump" for preservation in the United States National Film Registry as being "culturally, historically, or aesthetically significant".
Plot.
In 1981, Forrest Gump watches a feather fall from the sky at a bus stop in Savannah, Georgia. He recounts his life story to strangers who sit next to him on the bench, recounting his childhood in the town of Greenbow, Alabama.
On his first day of school, Forrest meets a girl named Jenny Curran, whose life is followed in parallel to Gump's at times. Despite his below average intelligence quotient, his ability to run at lightning speed (despite being a victim of polio as a young child) gets him into college on a football scholarship. After his college graduation, he enlists in the army, where he makes friends with Bubba, who convinces Gump to go into the shrimping business with him when the war is over. They are sent to Vietnam, and during an ambush, Bubba is killed in action. Gump ends up saving much of his platoon, including his Lieutenant, Dan Taylor, who loses both his legs as a result of injuries. Gump is awarded the Medal of Honor for his heroism.
While Gump is in recovery for a shot to his buttocks, he discovers his uncanny ability for ping-pong, eventually gaining popularity and rising to celebrity status, later playing ping-pong competitively against Chinese teams in ping-pong diplomacy. At an anti-war rally in Washington, D.C., Gump reunites with Jenny, who has been living a counterculture lifestyle.
Returning home, Gump endorses a company that makes ping-pong paddles, earning himself , which he uses to buy a shrimping boat, fulfilling his promise to Bubba. Lieutenant Dan joins Gump, and although they initially have little success, after finding their boat the only surviving one in the area after Hurricane Carmen, they begin to pull in huge amounts of shrimp. They use their income to buy an entire fleet of shrimp boats. Lieutenant Dan invests the money in Apple and Gump is financially secure for the rest of his life. He returns home to see his mother's last days.
One day, Jenny returns to visit Gump and he proposes marriage to her. She declines, though feels obliged to prove her love to him by having sex with him. She leaves early the next morning. On a whim, Gump elects to go for a run. Seemingly capriciously, he decides to keep running across the country several times, over three and a half years, becoming famous in the process.
In present-day, Gump reveals that he is waiting at the bus stop because he received a letter from Jenny who, having seen him run on television, asks him to visit her. Once he is reunited with Jenny, she introduces him to his son, also named Forrest. Jenny tells Gump she is suffering from an unknown virus (possibly HIV, though this is never specified). Together the three move back to Greenbow, Alabama. Jenny and Forrest finally marry but she dies soon afterward.
Father and son are waiting for the school bus on little Forrest's first day of school. Opening the book his son is taking to school, the white feather from the beginning of the film is caught on a breeze and drifts skyward.
Production.
Script.
The film is based on the 1986 novel by Winston Groom. Both center on the character of Forrest Gump. However, the film primarily focuses on the first eleven chapters of the novel, before skipping ahead to the end of the novel with the founding of Bubba Gump Shrimp Co. and the meeting with Forrest, Jr. In addition to skipping some parts of the novel, the film adds several aspects to Gump's life that do not occur in the novel, such as his needing leg braces as a child and his run across the United States.
Gump's core character and personality are also changed from the novel; among other things his film character is less of an autistic savant—in the novel, while playing football at the university, he fails craft and gym, but receives a perfect score in an advanced physics class he is enrolled in by his coach to satisfy his college requirements. The novel also features Gump as an astronaut, a professional wrestler, and a chess player.
Two directors were offered the opportunity to direct the film before Robert Zemeckis was selected. Terry Gilliam turned down the offer. Barry Sonnenfeld was attached to the film, but left to direct "Addams Family Values".
Filming.
Filming began in August 1993 and ended in December of that year. Although most of the film is set in Alabama, filming took place mainly in and around Beaufort, South Carolina, as well as parts of coastal Virginia and North Carolina, including a running shot on the Blue Ridge Parkway. Downtown portions of the fictional town of Greenbow were filmed in Varnville, South Carolina. The scene of Forrest running through Vietnam while under fire was filmed on Fripp Island, South Carolina. Additional filming took place on the Biltmore Estate in Asheville, North Carolina, and along the Blue Ridge Parkway near Boone, North Carolina. The most notable place was Grandfather Mountain where a part of the road is named "Forrest Gump Curve". The Gump family home set was built along the Combahee River near Yemassee, South Carolina, and the nearby land was used to film Curran's home as well as some of the Vietnam scenes. Over 20 palmetto trees were planted to improve the Vietnam scenes. Forrest Gump narrated his life's story in Chippewa Square in Savannah, Georgia as he sat at a bus stop bench. There were other scenes filmed in and around the Savannah area as well, including a running shot on the Richard V. Woods Memorial Bridge in Beaufort while he was being interviewed by the press, and on West Bay Street in Savannah. Most of the college campus scenes were filmed in Los Angeles at the University of Southern California. The lighthouse that Forrest runs across to reach the Atlantic Ocean the first time is the Marshall Point Lighthouse in Port Clyde, Maine.
Visual effects.
Ken Ralston and his team at Industrial Light & Magic were responsible for the film's visual effects. Using CGI techniques, it was possible to depict Gump meeting deceased personages and shaking their hands. Hanks was first shot against a blue screen along with reference markers so that he could line up with the archive footage. To record the voices of the historical figures, voice actors were filmed and special effects were used to alter lip-syncing for the new dialogue. Archival footage was used and with the help of such techniques as chroma key, image warping, morphing, and rotoscoping, Hanks was integrated into it.
In one Vietnam War scene, Gump carries Bubba away from an incoming napalm attack. To create the effect, stunt actors were initially used for compositing purposes. Then, Hanks and Williamson were filmed, with Williamson supported by a cable wire as Hanks ran with him. The explosion was then filmed, and the actors were digitally added to appear just in front of the explosions. The jet fighters and napalm canisters were also added by CGI.
The CGI removal of actor Gary Sinise's legs, after his character had them amputated, was achieved by wrapping his legs with a blue fabric, which later facilitated the work of the "roto-paint" team to paint out his legs from every single frame. At one point, while hoisting himself into his wheelchair, his legs are used for support.
The scene where Forrest spots Jenny at a peace rally at the Lincoln Memorial and Reflecting Pool in Washington, D.C., required visual effects to create the large crowd of people. Over two days of filming, approximately 1,500 extras were used. At each successive take, the extras were rearranged and moved into a different quadrant away from the camera. With the help of computers, the extras were multiplied to create a crowd of several hundred thousand people.
Release.
Critical reception.
The film received generally positive reviews. The review aggregator website Rotten Tomatoes reported that 72% of critics gave the film a positive review based on a sample of 83 reviews. At the website Metacritic, the film earned a rating of 82/100 based on 19 reviews by mainstream critics. CinemaScore reported that audiences gave the film a rare "A+" grade.
The story was commended by several critics. Roger Ebert of the "Chicago Sun-Times" wrote, "I've never met anyone like Forrest Gump in a movie before, and for that matter I've never seen a movie quite like 'Forrest Gump.' Any attempt to describe him will risk making the movie seem more conventional than it is, but let me try. It's a comedy, I guess. Or maybe a drama. Or a dream. The screenplay by Eric Roth has the complexity of modern fiction...The performance is a breathtaking balancing act between comedy and sadness, in a story rich in big laughs and quiet truths...What a magical movie." Todd McCarthy of "Variety" wrote that the film "has been very well worked out on all levels, and manages the difficult feat of being an intimate, even delicate tale played with an appealingly light touch against an epic backdrop." The film did receive notable pans from several major reviewers. Anthony Lane of "The New Yorker" called the film "Warm, wise, and wearisome as hell." Owen Gleiberman of "Entertainment Weekly" said that the film "reduces the tumult of the last few decades to a virtual-reality theme park: a baby-boomer version of Disney's America."
Critics have compared Gump with various characters and people including Huckleberry Finn, Bill Clinton, and Ronald Reagan. Peter Chomo writes that Gump acts as a "social mediator and as an agent of redemption in divided times". Peter Travers of "Rolling Stone" called Gump "everything we admire in the American character – honest, brave, and loyal with a heart of gold." "The New York Times" reviewer Janet Maslin called Gump a "hollow man" who is "self-congratulatory in his blissful ignorance, warmly embraced as the embodiment of absolutely nothing." Marc Vincenti of "Palo Alto Weekly" called the character "a pitiful stooge taking the pie of life in the face, thoughtfully licking his fingers." Bruce Kawin and Gerald Mast's textbook on film history notes that Forrest Gump's dimness was a metaphor for glamorized nostalgia in that he represented a blank slate by which the Baby Boomer generation projected their memories of those events.
The film is commonly seen as a polarizing one for audiences, with "Entertainment Weekly" writing in 2004, "Nearly a decade after it earned gazillions and swept the Oscars, Robert Zemeckis's ode to 20th-century America still represents one of cinema's most clearly drawn lines in the sand. One half of folks see it as an artificial piece of pop melodrama, while everyone else raves that it's sweet as a box of chocolates."
Box office performance.
Produced on a budget of $55 million, "Forrest Gump" opened in 1,595 theaters in its first weekend of domestic release, earning $24,450,602. Motion picture business consultant and screenwriter Jeffrey Hilton suggested to producer Wendy Finerman to double the P&A (film marketing budget) based on his viewing of an early print of the film. The budget was immediately increased, per his advice. The film placed first in the weekend's box office, narrowly beating "The Lion King", which was in its fourth week of release. For the first ten weeks of its release, the film held the number one position at the box office. The film remained in theaters for 42 weeks, earning $329.7 million in the United States and Canada, making it the fourth-highest grossing film at that time (behind only "E.T. the Extra-Terrestrial", "Star Wars IV: A New Hope", and "Jurassic Park").
The film took 66 days to surpass $250 million and was the fastest grossing Paramount film to pass $100 million, $200 million, and $300 million in box office receipts (at the time of its release). The film had gross receipts of $329,694,499 in the U.S. and Canada and $347,693,217 in international markets for a total of $677,387,716 worldwide. Even with such revenue, the film was known as a "successful failure"—due to distributors' and exhibitors' high fees, Paramount's "losses" clocked in at $62 million, leaving executives realizing the necessity of better deals. This has, however, also been associated with Hollywood accounting, where expenses are inflated in order to minimize profit sharing. It is Robert Zemeckis' highest-grossing film to date.
Home media.
"Forrest Gump" was first released on VHS tape on April 27, 1995, as a two-disc Laserdisc set on April 28, 1995, (including the "Through the Eyes of Forrest" special feature), before being released in a two-disc DVD set on August 28, 2001. Special features included director and producer commentaries, production featurettes, and screen tests. The film was released on Blu-ray disc in November 2009.
Accolades.
The film won the 67th Academy Awards for the Best Picture, Best Actor in a Leading Role, Best Director, Best Visual Effects, Best Adapted Screenplay, and Best Film Editing. The film was nominated for seven Golden Globe Awards, winning three of them: Best Actor – Motion Picture Drama, Best Director – Motion Picture, and Best Motion Picture – Drama. The film was also nominated for six Saturn Awards and won two for Best Fantasy Film and Best Supporting Actor (Film).
In addition to the film's multiple awards and nominations, it has also been recognized by the American Film Institute on several of its lists. The film ranks 37th on "100 Years...100 Cheers", 71st on "100 Years...100 Movies", and 76th on "100 Years...100 Movies (10th Anniversary Edition)". In addition, the quote "Mama always said life was like a box of chocolates. You never know what you're gonna get," was ranked 40th on "100 Years...100 Movie Quotes". The film also ranked at number 240 on "Empire"s list of the 500 Greatest Movies of All Time.
In December 2011, "Forrest Gump" was selected for preservation in the Library of Congress' National Film Registry. The Registry said that the film was "honored for its technological innovations (the digital insertion of Gump seamlessly into vintage archival footage), its resonance within the culture that has elevated Gump (and what he represents in terms of American innocence) to the status of folk hero, and its attempt to engage both playfully and seriously with contentious aspects of the era's traumatic history."
American Film Institute Lists
Author controversy.
Winston Groom was paid $350,000 for the screenplay rights to his novel "Forrest Gump" and was contracted for a 3 percent share of the film's net profits. However, Paramount and the film's producers did not pay him, using Hollywood accounting to posit that the blockbuster film lost money. Tom Hanks, by contrast, contracted for the film's gross receipts instead of a salary, and he and director Zemeckis each received $40 million. Additionally, Groom was not mentioned once in any of the film's six Oscar-winner speeches.
Groom's dispute with Paramount was later effectively resolved after Groom declared he was satisfied with Paramount's explanation of their accounting, this coinciding with Groom receiving a seven-figure contract with Paramount for film rights to another of his books, "Gump & Co."
Symbolism.
Feather.
Various interpretations have been suggested for the feather present at the opening and conclusion of the film. Sarah Lyall of "The New York Times" noted several suggestions made about the feather: "Does the white feather symbolize the unbearable lightness of being? Forrest Gump's impaired intellect? The randomness of experience?" Hanks interpreted the feather as: "Our destiny is only defined by how we deal with the chance elements to our life and that's kind of the embodiment of the feather as it comes in. Here is this thing that can land anywhere and that it lands at your feet. It has theological implications that are really huge." Sally Field compared the feather to fate, saying: "It blows in the wind and just touches down here or there. Was it planned or was it just perchance?" Visual effects supervisor Ken Ralston compared the feather to an abstract painting: "It can mean so many things to so many different people."
Political interpretations.
In Tom Hanks' words, "The film is non-political and thus non-judgmental." Nevertheless, in 1994, CNN's "Crossfire" debated whether the film promoted conservative values or was an indictment of the counterculture movement of the 1960s. Thomas Byers, in a "Modern Fiction Studies" article, called the film "an aggressively conservative film".
It has been noted that while Gump follows a very conservative lifestyle, Jenny's life is full of countercultural embrace, complete with drug usage, promiscuity, and antiwar rallies, and that their eventual marriage might be a kind of reconciliation. Jennifer Hyland Wang argued in a "Cinema Journal" article that Jenny's death to an unnamed virus "...symbolizes the death of liberal America and the death of the protests that defined a decade She also notes the film's screenwriter, Eric Roth, when developing the screenplay from the novel, had "...transferred all of Gump's flaws and most of the excesses committed by Americans in the 1960s and 1970s to her [Jenny."
Other commentators believe the film forecast the 1994 Republican Revolution and used the image of Forrest Gump to promote movement leader Newt Gingrich's traditional, conservative values. Jennifer Hyland Wang observes the film idealizes the 1950s, as made evident by the lack of "whites only" signs in Gump's southern childhood, and "revisions" the 1960s as a period of social conflict and confusion. She argues this sharp contrast between the decades criticizes the counterculture values and reaffirms conservatism. As viewed by political scientist Joe Paskett, this film is "one of the best films of all time". Wang argued the film was used by Republican politicians to illustrate a "traditional version of recent history" to gear voters towards their ideology for the congressional elections. In addition, presidential candidate Bob Dole cited the film's message in influencing his campaign due to its "...message that has made film one of Hollywood's all-time greatest box office hits: no matter how great the adversity, the American Dream is within everybody's reach."
In 1995, "National Review" included "Forrest Gump" in its list of the "Best 100 Conservative Movies" of all time. Then, in 2009, the magazine ranked the film number four on its 25 Best Conservative Movies of the Last 25 Years list. "Tom Hanks plays the title character, an amiable dunce who is far too smart to embrace the lethal values of the 1960s. The love of his life, wonderfully played by Robin Wright Penn, chooses a different path; she becomes a drug-addled hippie, with disastrous results."
James Burton, a communication arts professor at Salisbury University, argued that conservatives claimed "Forrest Gump" as their own due less to the content of the film and more to the historical and cultural context of 1994. Burton claimed the film's content and advertising campaign were affected by the cultural climate of the 1990s, which emphasized family values and "American values"—values epitomized in the successful book "Hollywood vs. America". He claimed this climate influenced the apolitical nature of the film, which allowed for many different political interpretations.
Burton points out that many conservative critics and magazines (John Simon, James Bowman, the "World Report") initially either criticized the film or praised it only for its non-political elements. Only after the popularity of the film was well-established did conservatives embrace the film as an affirmation of traditional values. Burton implies the liberal-left could have prevented the conservatives from claiming rights to the film, had it chosen to vocalize elements of the film such as its criticism of military values. Instead, the liberal-left focused on what the film omitted, such as the feminist and civil rights movements.
Some commentators see the conservative readings of "Forrest Gump" as indicants of the death of irony in American culture. Vivian Sobchack notes that the film's humor and irony relies on the assumption of the audience's historical (self-) consciousness.
Soundtrack.
The 32-song soundtrack from the film was released on July 6, 1994. With the exception of a lengthy suite from Alan Silvestri's score, all the songs are previously released; the soundtrack includes songs from Elvis Presley, Fleetwood Mac, Creedence Clearwater Revival, Aretha Franklin, Lynyrd Skynyrd, Three Dog Night, The Byrds, The Doors, The Mamas & the Papas, The Doobie Brothers, Simon & Garfunkel, Bob Seger, and Buffalo Springfield among others. Music producer Joel Sill reflected on compiling the soundtrack: "We wanted to have very recognizable material that would pinpoint time periods, yet we didn't want to interfere with what was happening cinematically." The two-disc album has a variety of music from the 1950s–1980s performed by American artists. According to Sills, this was due to Zemeckis' request, "All the material in there is American. Bob (Zemeckis) felt strongly about it. He felt that Forrest wouldn't buy anything but American."
The soundtrack reached a peak of number 2 on the "Billboard" album chart. The soundtrack went on to sell twelve million copies, and is one of the top selling albums in the United States. The score for the film was composed and conducted by Alan Silvestri and released on August 2, 1994.
Proposed sequel.
The screenplay for the sequel was written by Eric Roth in 2001. It is based on the original novel's sequel, "Gump and Co." written by Winston Groom in 1995. Roth's script begins with Forrest sitting on a bench waiting for his son to return from school. After the September 11 attacks, Roth, Zemeckis, and Hanks decided the story was no longer "relevant." In March 2007, however, it was reported Paramount producers took another look at the screenplay.
On the very first page of the sequel novel, Forrest Gump tells readers "Don't never let nobody make a movie of your life's story," though "Whether they get it right or wrong, it doesn't matter." The first chapter of the book suggests the real-life events surrounding the film have been incorporated into Forrest's storyline, and that Forrest got a lot of media attention as a result of the film. During the course of the sequel novel, Gump runs into Tom Hanks and at the end of the novel in the film's release, including Gump going on "The David Letterman Show" and attending the Academy Awards.

</doc>
<doc id="41531" url="https://en.wikipedia.org/wiki?curid=41531" title="Stanislaw Ulam">
Stanislaw Ulam

Stanisław Marcin Ulam (pronounced ; 13 April 1909 – 13 May 1984) was a Polish-American mathematician. He participated in America's Manhattan Project, originated the Teller–Ulam design of thermonuclear weapons, invented the Monte Carlo method of computation, and suggested nuclear pulse propulsion. In pure and applied mathematics, he proved some theorems and proposed several conjectures.
Born into a wealthy Polish Jewish family, Ulam studied mathematics at the Lwów Polytechnic Institute, where he earned his PhD in 1933 under the supervision of Kazimierz Kuratowski. In 1935, John von Neumann, whom Ulam had met in Warsaw, invited him to come to the Institute for Advanced Study in Princeton, New Jersey, for a few months. From 1936 to 1939, he spent summers in Poland and academic years at Harvard University in Cambridge, Massachusetts, where he worked to establish important results regarding ergodic theory. On 20 August 1939, he sailed for America for the last time with his 17-year-old brother Adam Ulam. He became an assistant professor at the University of Wisconsin–Madison in 1940, and a United States citizen in 1941.
In October 1943, he received an invitation from Hans Bethe to join the Manhattan Project at the secret Los Alamos Laboratory in New Mexico. There, he worked on the hydrodynamic calculations to predict the behavior of the explosive lenses that were needed by an implosion-type weapon. He was assigned to Edward Teller's group, where he worked on Teller's "Super" bomb for Teller and Enrico Fermi. After the war he left to become an associate professor at the University of Southern California, but returned to Los Alamos in 1946 to work on thermonuclear weapons. With the aid of a cadre of female "computers", including his wife Françoise Aron Ulam, he found that Teller's "Super" design was unworkable. In January 1951, Ulam and Teller came up with the Teller–Ulam design, which is the basis for all thermonuclear weapons.
Ulam considered the problem of nuclear propulsion of rockets, which was pursued by Project Rover, and proposed, as an alternative to Rover's nuclear thermal rocket, to harness small nuclear explosions for propulsion, which became Project Orion. With Fermi and John Pasta, Ulam studied the Fermi–Pasta–Ulam problem, which became the inspiration for the field of non-linear science. He is probably best known for realising that electronic computers made it practical to apply statistical methods to functions without known solutions, and as computers have developed, the Monte Carlo method has become a common and standard approach to many problems.
Poland.
Ulam was born in Lemberg, Galicia, on 13 April 1909. At this time, Galicia was in the Kingdom of Galicia and Lodomeria of the Austro-Hungarian Empire, known to Poles as the Austrian partition. In 1918, it became part of the newly restored Poland, the Second Polish Republic, and the city took its Polish name again, Lwów.
The Ulams were a wealthy Polish Jewish family of bankers, industrialists, and other professionals. Ulam's immediate family was "well-to-do but hardly rich". His father, Józef Ulam, was born in Lwów and was a lawyer, and his mother, Anna (née Auerbach), was born in Stryj. His uncle, Michał Ulam, was an architect, building contractor, and lumber industrialist. From 1916 until 1918, Józef's family lived temporarily in Vienna. After they returned, Lwów became the epicenter of the Polish–Ukrainian War, during which the city experienced a Ukrainian siege.
In 1919, Ulam entered Lwów Gymnasium Nr. VII, from which he graduated in 1927. He then studied mathematics at the Lwów Polytechnic Institute. Under the supervision of Kazimierz Kuratowski, he received his Master of Arts degree in 1932, and became a Doctor of Science in 1933. At the age of 20, in 1929, he published his first paper "Concerning Function of Sets" in the journal "Fundamenta Mathematicae". From 1931 until 1935, he traveled to and studied in Wilno (Vilnius), Vienna, Zurich, Paris, and Cambridge, England, where he met G. H. Hardy and Subrahmanyan Chandrasekhar.
Along with Stanisław Mazur, Mark Kac, Włodzimierz Stożek, Kuratowski, and others, Ulam was a member of the Lwów School of Mathematics. Its founders were Hugo Steinhaus and Stefan Banach, who were professors at the University of Lwów. Mathematicians of this "school" met for long hours at the Scottish Café, where the problems they discussed were collected in the Scottish Book, a thick notebook provided by Banach's wife. Ulam was a major contributor to the book. Of the 193 problems recorded between 1935 and 1941, he contributed 40 problems as a single author, another 11 with Banach and Mazur, and an additional 15 with others. In 1957, he received from Steinhaus a copy of the book, which had survived the war, and translated it into English. In 1981, Ulam's friend R. Daniel Maudlin published an expanded and annotated version.
Coming to America.
In 1935, John von Neumann, whom Ulam had met in Warsaw, invited him to come to the Institute for Advanced Study in Princeton, New Jersey, for a few months. In December of that year, Ulam sailed to America. At Princeton, he went to lectures and seminars, where he heard Oswald Veblen, James Alexander, and Albert Einstein. During a tea party at von Neumann's house, he encountered G. D. Birkhoff, who suggested that he apply for a position with the Harvard Society of Fellows. Following up on Birkhoff's suggestion, Ulam spent summers in Poland and academic years at Harvard University in Cambridge, Massachusetts from 1936 to 1939, where he worked with John C. Oxtoby to establish results regarding ergodic theory. These appeared in Annals of Mathematics in 1941.
On 20 August 1939, in Gdynia, Józef Ulam, along with his brother Szymon, put his two sons, Stanislaw and 17 year old Adam, on a ship headed for America. Two weeks later, the . Within two months, the Germans completed their occupation of western Poland, and the Soviets and occupied eastern Poland. Within two years, Józef Ulam and the rest of his family were victims of the Holocaust, Hugo Steinhaus was in hiding, Kazimierz Kuratowski was lecturing at the underground university in Warsaw, Włodzimierz Stożek and his two sons had been killed in the massacre of Lwów professors, and the last problem had been recorded in the Scottish Book. Stefan Banach survived the Nazi occupation by feeding lice at Rudolf Weigl's typhus research institute. In 1963, Adam Ulam, who had become an eminent kremlinologist at Harvard, received a letter from George Volsky, who hid in Józef Ulam's house after deserting from the Polish army. This reminiscence gave a chilling account of Lwów's chaotic scenes in late 1939. In later life Ulam described himself as "an agnostic. Sometimes I muse deeply on the forces that are for me invisible. When I am almost close to the idea of God, I feel immediately estranged by the horrors of this world, which he seems to tolerate".
In 1940, after being recommended by Birkhoff, Ulam became an assistant professor at the University of Wisconsin–Madison. Here, he became a United States citizen in 1941. That year, he married Françoise Aron. She had been a French exchange student at Mount Holyoke College, whom he met in Cambridge. They had one daughter, Claire. In Madison, Ulam met his friend and colleague C. J. Everett, with whom he would collaborate on a number of papers.
Manhattan Project.
In early 1943, Ulam asked von Neumann to find him a war job. In October, he received an invitation to join an unidentified project near Santa Fe, New Mexico. The letter was signed by Hans Bethe, who had been appointed as leader of the theoretical division of Los Alamos National Laboratory by Robert Oppenheimer, its scientific director. Knowing nothing of the area, he borrowed a New Mexico guide book. On the checkout card, he found the names of his Wisconsin colleagues, Joan Hinton, David Frisch, and Joseph McKibben, all of whom had mysteriously disappeared. This was Ulam's introduction to the Manhattan Project, which was America's wartime effort to create the atomic bomb.
Hydrodynamical calculations of implosion.
A few weeks after Ulam reached Los Alamos in February 1944, the project experienced a crisis. In April, Emilio Segrè discovered that plutonium made in reactors would not work in a gun-type plutonium weapon like the "Thin Man", which was being developed in parallel with a uranium weapon, the "Little Boy" that was dropped on Hiroshima. This problem threatened to waste an enormous investment in new reactors at the Hanford site and to make slow separation of uranium isotopes the only way to prepare fissile material suitable for use in bombs. To respond, Oppenheimer implemented, in August, a sweeping reorganization of the laboratory to focus on development of an implosion-type weapon and appointed George Kistiakowsky head of the implosion department. He was a professor at Harvard and an expert on precise use of explosives.
The basic concept of implosion is to use chemical explosives to crush a chunk of fissile material into a critical mass, where neutron multiplication leads to a nuclear chain reaction, releasing a large amount of energy. Cylindrical implosive configurations had been studied by Seth Neddermeyer, but von Neumann, who had experience with shaped charges used in armor-piercing ammunition, was a vocal advocate of spherical implosion driven by explosive lenses. He realized that the symmetry and speed with which implosion compressed the plutonium were critical issues, and enlisted Ulam to help design lens configurations that would provide nearly spherical implosion. Within an implosion, because of enormous pressures and high temperatures, solid materials behave much like fluids. This meant that hydrodynamical calculations were needed to predict and minimize asymmetries that would spoil a nuclear detonation. Of these calculations, Ulam said:
Nevertheless, with the primitive facilities available at the time, Ulam and von Neumann did carry out numerical computations that led to a satisfactory design. This motivated their advocacy of a powerful computational capability at Los Alamos, which began during the war years, continued through the cold war, and still exists. Otto Frisch remembered Ulam as "a brilliant Polish topologist with a charming French wife. At once he told me that he was a pure mathematician who had sunk so low that his latest paper actually contained numbers with decimal points!"
Statistics of branching and multiplicative processes.
Even the inherent statistical fluctuations of neutron multiplication within a chain reaction have implications with regard to implosion speed and symmetry. In November 1944, David Hawkins and Ulam addressed this problem in a report entitled "Theory of Multiplicative Processes". This report, which invokes probability-generating functions, is also an early entry in the extensive literature on statistics of branching and multiplicative processes. In 1948, its scope was extended by Ulam and Everett.
Early in the Manhattan project, Enrico Fermi's attention was focused on the use of reactors to produce plutonium. In September 1944, he arrived at Los Alamos, shortly after breathing life into the first Hanford reactor, which had been poisoned by a xenon isotope. Soon after Fermi's arrival, Teller's "Super" bomb group, of which Ulam was a part, was transferred to a new division headed by Fermi. Fermi and Ulam formed a relationship that became very fruitful after the war.
Post war Los Alamos.
In September 1945, Ulam left Los Alamos to become an associate professor at the University of Southern California in Los Angeles. In January 1946, he suffered an acute attack of encephalitis, which put his life in danger, but which was alleviated by emergency brain surgery. During his recuperation, many friends visited, including Nicholas Metropolis from Los Alamos and the famous mathematician Paul Erdős, who remarked: "Stan, you are just like before." This was encouraging, because Ulam was concerned about the state of his mental faculties, for he had lost the ability to speak during the crisis. Another friend, Gian-Carlo Rota, asserted in a 1987 article that the attack changed Ulam's personality; afterwards, he turned from rigorous pure mathematics to more speculative conjectures concerning the application of mathematics to physics and biology. This assertion was not accepted by Françoise Aron Ulam.
By late April 1946, Ulam had recovered enough to attend a secret conference at Los Alamos to discuss thermonuclear weapons. Those in attendance included Ulam, von Neumann, Metropolis, Teller, Stan Frankel, and others. Throughout his participation in the Manhattan Project, Teller's efforts had been directed toward the development of a "super" weapon based on nuclear fusion, rather than toward development of a practical fission bomb. After extensive discussion, the participants reached a consensus that his ideas were worthy of further exploration. A few weeks later, Ulam received an offer of a position at Los Alamos from Metropolis and Robert D. Richtmyer, the new head of its theoretical division, at a higher salary, and the Ulams returned to Los Alamos.
Monte Carlo method.
Late in the war, under the sponsorship of von Neumann, Frankel and Metropolis began to carry out calculations on the first general-purpose electronic computer, the ENIAC at the Aberdeen Proving Ground in Maryland. Shortly after returning to Los Alamos, Ulam participated in a review of results from these calculations. Earlier, while playing solitaire during his recovery from surgery, Ulam had thought about playing hundreds of games to estimate statistically the probability of a successful outcome. With ENIAC in mind, he realized that the availability of computers made such statistical methods very practical. John von Neumann immediately saw the significance of this insight. In March 1947 he proposed a statistical approach to the problem of neutron diffusion in fissionable material. Because Ulam had often mentioned his uncle, Michał Ulam, "who just had to go to Monte Carlo" to gamble, Metropolis dubbed the statistical approach "The Monte Carlo method". Metropolis and Ulam published the first unclassified paper on the Monte Carlo method in 1949.
Fermi, learning of Ulam's breakthrough, devised an analog computer known as the Monte Carlo trolley, later dubbed the FERMIAC. The device performed a mechanical simulation of random diffusion of neutrons. As computers improved in speed and programmability, these methods became more useful. In particular, many Monte Carlo calculations carried out on modern massively parallel supercomputers are embarrassingly parallel applications, whose results can be very accurate.
Teller–Ulam design.
On 29 August 1949, the Soviet Union tested its first fission bomb, the RDS-1. Created under the supervision of Lavrentiy Beria, who sought to duplicate the American effort, this weapon was nearly identical to Fat Man, for its design was based on information provided by spies Klaus Fuchs, Theodore Hall, and David Greenglass. In response, on 31 January 1950, President Harry S. Truman announced a crash program to develop a fusion bomb.
To advocate an aggressive development program, Ernest Lawrence and Luis Alvarez came to Los Alamos, where they conferred with Norris Bradbury, the laboratory director, and with George Gamow, Edward Teller, and Ulam. Soon, these three became members of a short-lived committee appointed by Bradbury to study the problem, with Teller as chairman. At this time, research on the use of a fission weapon to create a fusion reaction had been ongoing since 1942, but the design was still essentially the one originally proposed by Teller. His concept was to put tritium and/or deuterium in close proximity to a fission bomb, with the hope that the heat and intense flux of neutrons released when the bomb exploded, would ignite a self-sustaining fusion reaction. Reactions of these isotopes of hydrogen are of interest because the energy per unit mass of fuel released by their fusion is much larger than that from fission of heavy nuclei.
Because the results of calculations based on Teller's concept were discouraging, many scientists believed it could not lead to a successful weapon, while others had moral and economic grounds for not proceeding. Consequently, several senior people of the Manhattan Project opposed development, including Bethe and Oppenheimer. To clarify the situation, Ulam and von Neumann resolved to do new calculations to determine whether Teller's approach was feasible. To carry out these studies, von Neumann decided to use electronic computers: ENIAC at Aberdeen, a new computer, MANIAC, at Princeton, and its twin, which was under construction at Los Alamos. Ulam enlisted Everett to follow a completely different approach, one guided by physical intuition. Françoise Ulam was one of a cadre of women "computers" who carried out laborious and extensive computations of thermonuclear scenarios on mechanical calculators, supplemented and confirmed by Everett's slide rule. Ulam and Fermi collaborated on further analysis of these scenarios. The results showed that, in workable configurations, a thermonuclear reaction would not ignite, and if ignited, it would not be self-sustaining. Ulam had used his expertise in Combinatorics to analyze the chain reaction in deuterium, which was much more complicated than the ones in uranium and plutonium, and he concluded that no self-sustaining chain reaction would take place at the (low) densities that Teller was considering. In late 1950, these conclusions were confirmed by von Neumann's results.
In January 1951, Ulam had another idea: to channel the mechanical shock of a nuclear explosion so as to compress the fusion fuel. On the recommendation of his wife, Ulam discussed this idea with Bradbury and Mark before he told Teller about it. Almost immediately, Teller saw its merit, but noted that soft X-rays from the fission bomb would compress the thermonuclear fuel more strongly than mechanical shock and suggested ways to enhance this effect. On 9 March 1951, Teller and Ulam submitted a joint report describing these innovations. A few weeks later, Teller suggested placing a fissile rod or cylinder at the center of the fusion fuel. The detonation of this "spark plug" would help to initiate and enhance the fusion reaction. The design based on these ideas, called staged radiation implosion, has become the standard way to build thermonuclear weapons. It is often described as the "Teller–Ulam design".
In September 1951, after a series of differences with Bradbury and other scientists, Teller resigned from Los Alamos, and returned to the University of Chicago. At about the same time, Ulam went on leave as a visiting professor at Harvard for a semester. Although Teller and Ulam submitted a joint report on their design and jointly applied for a patent on it, they soon became involved in a dispute over who deserved credit. After the war, Bethe returned to Cornell University, but he was deeply involved in the development of thermonuclear weapons as a consultant. In 1954, he wrote an article on the history of the H-bomb, which presents his opinion that both men contributed very significantly to the breakthrough. This balanced view is shared by others who were involved, including Mark and Fermi, but Teller persistently attempted to downplay Ulam's role. "After the H-bomb was made," Bethe recalled, "reporters started to call Teller the father of the H-bomb. For the sake of history, I think it is more precise to say that Ulam is the father, because he provided the seed, and Teller is the mother, because he remained with the child. As for me, I guess I am the midwife."
With the basic fusion reactions confirmed, and with a feasible design in hand, there was nothing to prevent Los Alamos from testing a thermonuclear device. On 1 November 1952, the first thermonuclear explosion occurred when Ivy Mike was detonated on Enewetak Atoll, within the US Pacific Proving Grounds. This device, which used liquid deuterium as its fusion fuel, was immense and utterly unusable as a weapon. Nevertheless, its success validated the Teller–Ulam design, and stimulated intensive development of practical weapons.
Fermi–Pasta–Ulam problem.
When Ulam returned to Los Alamos, his attention turned away from weapon design and toward the use of computers to investigate problems in physics and mathematics. With John Pasta, who helped Metropolis to bring MANIAC on line in March 1952, he explored these ideas in a report "Heuristic Studies in Problems of Mathematical Physics on High Speed Computing Machines", which was submitted on 9 June 1953. It treated several problems that cannot be addressed within the framework of traditional analytic methods: billowing of fluids, rotational motion in gravitating systems, magnetic lines of force, and hydrodynamic instabilities.
Soon, Pasta and Ulam became experienced with electronic computation on MANIAC, and by this time, Enrico Fermi had settled into a routine of spending academic years at the University of Chicago and summers at Los Alamos. During these summer visits, Pasta and Ulam joined him to study a variation of the classic problem of a string of masses held together by springs that exert forces linearly proportional to their displacement from equilibrium. Fermi proposed to add to this force a nonlinear component, which could be chosen to be proportional to either the square or cube of the displacement, or to a more complicated "broken linear" function. This addition is the key element of the Fermi–Pasta–Ulam problem, which is often designated by the abbreviation FPU.
A classical spring system can be described in terms of vibrational modes, which are analogous to the harmonics that occur on a stretched violin string. If the system starts in a particular mode, vibrations in other modes do not develop. With the nonlinear component, Fermi expected energy in one mode to transfer gradually to other modes, and eventually, to be distributed equally among all modes. This is roughly what began to happen shortly after the system was initialized with all its energy in the lowest mode, but much later, essentially all the energy periodically reappeared in the lowest mode. This behavior is very different from the expected equipartition of energy. It remained mysterious until 1965, when Kruskal and Zabusky showed that, after appropriate mathematical transformations, the system can be described by the Korteweg–de Vries equation, which is the prototype of nonlinear partial differential equations that have soliton solutions. This means that FPU behavior can be understood in terms of solitons.
Nuclear propulsion.
Starting in 1955, Ulam and Frederick Reines considered nuclear propulsion of aircraft and rockets. This is an attractive possibility, because the nuclear energy per unit mass of fuel is a million times greater than that available from chemicals. From 1955 to 1972, their ideas were pursued during Project Rover, which explored the use of nuclear reactors to power rockets. In response to a question by Senator John O. Pastore at a congressional committee hearing on "Outer Space Propulsion by Nuclear Energy", on January 22, 1958, Ulam replied that "the future as a whole of mankind is to some extent involved inexorably now with going outside the globe."
Ulam and C. J. Everett also proposed, in contrast to Rover's continuous heating of rocket exhaust, to harness small nuclear explosions for propulsion. Project Orion was a study of this idea. It began in 1958 and ended in 1965, after the Partial Nuclear Test Ban Treaty of 1963 banned nuclear weapons tests in the atmosphere and in space. Work on this project was spearheaded by physicist Freeman Dyson, who commented on the decision to end Orion in his article, "Death of a Project".
Bradbury appointed Ulam and John H. Manley as research advisors to the laboratory director in 1957. These newly created positions were on the same administrative level as division leaders, and Ulam held his until he retired from Los Alamos. In this capacity, he was able to influence and guide programs in many divisions: theoretical, physics, chemistry, metallurgy, weapons, health, Rover, and others.
In addition to these activities, Ulam continued to publish technical reports and research papers. One of these introduced the Fermi–Ulam model, an extension of Fermi's theory of the acceleration of cosmic rays. Another, with Paul Stein and Mary Tsingou, titled "Quadratic Transformations", was an early investigation of chaos theory and is considered the first published use of the phrase "chaotic behavior".
Return to academia.
During his years at Los Alamos, Ulam was a visiting professor at Harvard from 1951 to 1952, MIT from 1956 to 1957, the University of California, San Diego, in 1963, and the University of Colorado at Boulder from 1961 to 1962 and 1965 to 1967. In 1967, the last of these positions became permanent, when Ulam was appointed as professor and Chairman of the Department of Mathematics at Boulder, Colorado. He kept a residence in Santa Fe, New Mexico, which made it convenient to spend summers at Los Alamos as a consultant.
In Colorado, where he rejoined his friends Gamow, Richtmyer, and Hawkins, Ulam's research interests turned toward biology. In 1968, recognizing this emphasis, the University of Colorado School of Medicine appointed Ulam as Professor of Biomathematics, and he held this position until his death. With his Los Alamos colleague Robert Schrandt he published a report, "Some Elementary Attempts at Numerical Modeling of Problems Concerning Rates of Evolutionary Processes", which applied his earlier ideas on branching processes to biological inheritance. Another, report, with William Beyer, Temple F. Smith, and M. L. Stein, titled "Metrics in Biology", introduced new ideas about biometric distances.
When he retired from Colorado in 1975, Ulam had begun to spend winter semesters at the University of Florida, where he was a graduate research professor. Except for sabbaticals at the University of California, Davis from 1982 to 1983, and at Rockefeller University from 1980 to 1984, this pattern of spending summers in Colorado and Los Alamos and winters in Florida continued until Ulam died of an apparent heart attack in Santa Fe on 13 May 1984.
Paul Erdős noted that "he died suddenly of heart failure, without fear or pain, while he could still prove and conjecture." In 1987, Françoise Ulam deposited his papers with the American Philosophical Society Library in Philadelphia. She continued to live in Santa Fe until she died on 30 April 2011, at the age of 93. Both Françoise and her husband are buried with her French family in Montparnasse Cemetery in Paris.
Impact and legacy.
From the publication of his first paper as a student in 1929 until his death, Ulam was constantly writing on mathematics. The list of Ulam's publications includes more than 150 papers. Topics represented by a significant number of papers are: set theory (including measurable cardinals and abstract measures), topology, transformation theory, ergodic theory, group theory, projective algebra, number theory, combinatorics, and graph theory. In March 2009, the Mathematical Reviews database contained 697 papers with the name "Ulam".
Notable results of this work are:
With his pivotal role in the development of thermonuclear weapons, Stanislaw Ulam changed the world. According to Françoise Ulam: "Stan would reassure me that, barring accidents, the H-bomb rendered nuclear war impossible." In 1980, Ulam and his wife appeared in the television documentary "The Day After Trinity".
The Monte Carlo method has become a ubiquitous and standard approach to computation, and the method has been applied to a vast number of scientific problems. In addition to problems in physics and mathematics, the method has been applied to finance, social science, environmental risk assessment, linguistics, radiation therapy, and sports.
The Fermi–Pasta–Ulam problem is credited not only as "the birth of experimental mathematics", but also as inspiration for the vast field of Nonlinear Science. In his Lilienfeld Prize lecture, David K. Campbell noted this relationship and described how FPU gave rise to ideas in chaos, solitons, and dynamical systems. In 1980, Donald Kerr, laboratory director at Los Alamos, with the strong support of Ulam and Mark Kac, founded the Center for Nonlinear Studies (CNLS). In 1985, CNLS initiated the "Stanislaw M. Ulam Distinguished Scholar" program, which provides an annual award that enables a noted scientist to spend a year carrying out research at Los Alamos.
The fiftieth anniversary of the original FPU paper was the subject of the March 2005 issue of the journal Chaos, and the topic of the 25th Annual International Conference of CNLS. The University of Southern Mississippi and the University of Florida supported the "Ulam Quarterly", which was active from 1992 to 1996, and which was one of the first online mathematical journals. Florida's Department of Mathematics has sponsored, since 1998, the annual "Ulam Colloquium Lecture", and in March 2009, the "Ulam Centennial Conference".
Ulam's work on non-Euclidean distance metrics in the context of molecular biology made a significant contribution to sequence analysis and his contributions in theoretical biology are considered watersheds in the development of cellular automata theory, population biology, pattern recognition, and biometrics generally. Colleagues noted that some of his greatest contributions were in clearly identifying problems to be solved and general techniques for solving them.
In 1987, Los Alamos issued a special issue of its "Science" publication, which summarized his accomplishments, and which appeared, in 1989, as the book "From Cardinals to Chaos". Similarly, in 1990, the University of California Press issued a compilation of mathematical reports by Ulam and his Los Alamos collaborators: "Analogies Between Analogies". During his career, Ulam was awarded honorary degrees by the Universities of New Mexico, Wisconsin, and Pittsburgh.

</doc>
<doc id="41533" url="https://en.wikipedia.org/wiki?curid=41533" title="György Dalos">
György Dalos

György Dalos (born September 23, 1943) is a Hungarian Jewish writer and historian. He is best known for his novel "1985", and "The Guest from the Future: Anna Akhmatova and Isaiah Berlin".
Life.
Dalos was born in Budapest and spent his childhood with his grandparents, as his father had died in 1945 in a work camp, where he had been sent to as a Jew during World War II. From 1962 to 1967, he studied history at the Lomonossov University in Moscow. He then returned to his native town Budapest to work as a museologist. In 1968, Dalos was accused of "Maoist activities" and was handed seven months prison on probation and a Berufsverbot (professional disqualification) and a publication ban; due to that, he worked as a translator. In 1977, he was among the founders of the opposition movement against the Communist regime of Hungary. In 1988/89 he was co-editor of the East German underground opposition paper "Ostkreuz". From 1995 to 1999, Dalos was head of the Institute for Hungarian Culture in Berlin. Since 2009 he is member of the International Council of Austrian Service Abroad.
Dalos lived in Vienna from 1987 to 1995. Since 1995, he has lived in Berlin as a freelance publisher and editor.
Work.
Articles

</doc>
<doc id="41534" url="https://en.wikipedia.org/wiki?curid=41534" title="Eldred v. Ashcroft">
Eldred v. Ashcroft

Eldred v. Ashcroft, 537 U.S. 186 (2003) was decision by the Supreme Court of the United States upholding the constitutionality of the 1998 Sonny Bono Copyright Term Extension Act (CTEA).
Background.
The Sonny Bono Copyright Term Extension Act (or CTEA) extended existing copyright terms by an additional 20 years from the terms set by the Copyright Act of 1976. The law affected both new and existing works (making it both a "prospective" extension as well as a "retroactive" one). Specifically, for works published before January 1, 1978 and still in copyright on October 27, 1998, the term was extended to 95 years. For works authored by "individuals" on or after January 1, 1978 (including new works), the copyright term was extended to equal the life of the author plus 70 years. For works authored by joint authors, the copyright term was extended to the life of the last surviving author plus 70 years. In the case of works-for-hire, anonymous or pseudonymous works, the term was set at 95 years from the date of first publication, or 120 years from creation.
The practical result of this was to prevent a number of works from entering the public domain in 1998 and following years, as would have occurred under the Copyright Law of 1976. Materials which the plaintiffs had worked with and were ready to republish were now unavailable due to copyright restrictions.
The lead petitioner, Eric Eldred, is an Internet publisher. Eldred was joined by a group of commercial and non-commercial interests who relied on the public domain for their work. These included Dover Publications, a commercial publisher of paperback books; Luck's Music Library, Inc., and Edwin F. Kalmus & Co., Inc., publishers of orchestral sheet music; and a large number of "amici" including the Free Software Foundation, the American Association of Law Libraries, the Bureau of National Affairs, and the College Art Association.
Supporting the law were the U.S. government, represented by the Attorney General in an "ex officio" capacity (originally Janet Reno, later replaced by John Ashcroft), along with a set of "amici" including the Motion Picture Association of America, the Recording Industry Association of America, ASCAP and Broadcast Music Incorporated.
District court.
The original complaint was filed in the United States District Court for the District of Columbia on January 11, 1999. The plaintiffs' argument was threefold:
In response, the government argued that Congress does indeed have the latitude to retroactively extend terms, so long as the individual extensions are also for "limited times," as required by the Constitution. As an argument for this position, they referred to the Copyright Act of 1790, the first Federal copyright legislation, which applied Federal protection to existing works. Furthermore, they argued, neither the First Amendment nor the doctrine of public trust is applicable to copyright cases.
On October 28, 1999, Judge June Green issued a brief opinion rejecting all three of the petitioners' arguments. On the first count, she wrote that Congress had the power to extend terms as it wished, as long as the terms themselves were of limited duration. On the second count, she rejected the notion of First Amendment scrutiny in copyright cases, based on her interpretation of "Harper and Row Publishers, Inc., v. Nation Enterprises", an earlier Supreme Court decision. On the third count, she rejected the notion that public trust doctrine was applicable to copyright law.
Court of Appeals.
The plaintiffs appealed the decision of the district court to the United States Court of Appeals for the District of Columbia Circuit, filing their initial brief on May 22, 2000, and arguing the case on October 5 of the same year in front of a three-judge panel. Arguments were similar to those made in the district court, except for those regarding the public trust doctrine, which were not included in the appeal.
Instead, the plaintiffs extended their argument on the copyright clause to note that the clause requires Congress to "promote the Progress of Science and useful Arts," and argued that retroactive extensions do not directly serve this purpose in the standard "quid pro quo" previously required by the courts.
The case was decided on February 16, 2001. The appeals court upheld the decision of the district court in a 2-1 opinion. In his dissent, Judge David Sentelle agreed with the plaintiffs that CTEA was indeed unconstitutional based on the "limited Times" requirement. Supreme Court precedent, he argued, held that one must be able to discern an "outer limit" to a limited power; in the case of retrospective copyright extensions, Congress could continue to extend copyright terms indefinitely through a set of limited extensions, thus rendering the "limited times" requirement meaningless.
Following this ruling, plaintiffs petitioned for a rehearing "en banc" (in front of the full panel of nine judges). This petition was rejected, 7–2, with Judges Sentelle and David Tatel dissenting.
Supreme Court.
On October 11, 2001, the plaintiffs filed a petition for certiorari to the Supreme Court of the United States. On February 19, 2002, the Court granted Certiorari, agreeing to hear the case.
Oral arguments were presented on October 9, 2002. Lead counsel for the plaintiff was Lawrence Lessig; the government's case was argued by Solicitor General Theodore Olson.
Lessig focused the Plaintiffs' brief to emphasize the Copyright clause restriction, as well as the First Amendment argument from the Appeals case. The decision to emphasize the Copyright clause argument was based on both the minority opinion of Judge Sentelle in the appeals court, and on several recent Supreme Court decisions authored by Chief Justice William Rehnquist: "United States v. Lopez" and "United States v. Morrison".
In both of those decisions, Rehnquist, along with four of the Court's more conservative justices, held Congressional legislation unconstitutional, because that legislation exceeded the limits of the Constitution's Commerce clause. This profound reversal of precedent, Lessig argued, could not be limited to only one of the enumerated powers. If the court felt that it had the power to review legislation under the Commerce clause, Lessig argued, then the Copyright clause deserved similar treatment, or at very least a "principled reason" must be stated for according such treatment to only one of the enumerated powers.
On January 15, 2003, the Court held the CTEA constitutional by a 7–2 decision. The majority opinion, written by Justice Ginsburg, relied heavily on the Copyright Acts of 1790, 1831, 1909, and 1976 as precedent for retroactive extensions. One of the arguments supporting the act was the life expectancy has significantly increased among the human population since the 18th century, and therefore copyright law needed extending as well. However, the major argument for the act that carried over into the case was that the Constitution specified that Congress only needed to set time limits for copyright, the length of which was left to their discretion. Thus, as long as the limit is not "forever," any limit set by Congress can be deemed constitutional.
A key factor in the CTEA’s passage was a 1993 European Union (EU) directive instructing EU members to establish a baseline copyright term of life plus 70 years and to deny this longer term to the works of any non-EU country whose laws did not secure the same extended term. By extending the baseline United States copyright term, Congress sought to ensure that American authors would receive the same copyright protection in Europe as their European counterparts.
The Supreme Court declined to address Lessig's contention that "Lopez" and "Morrison" offered precedent for enforcing the Copyright clause, and instead reiterated the lower court's reasoning that a retroactive term extension can satisfy the "limited times" provision in the copyright clause, as long as the extension itself is limited instead of perpetual. Furthermore, the Court refused to apply the proportionality standards of the Fourteenth Amendment or the free-speech standards in the First Amendment to limit Congress's ability to confer copyrights for limited terms.
Justice Breyer dissented, arguing that the CTEA amounted to a grant of perpetual copyright that undermined public interests. While the constitution grants Congress power to extend copyright terms in order to "promote the progress of science and useful arts," CTEA granted precedent to continually renew copyright terms making them virtually perpetual. Justice Breyer argued in his dissent that it is highly unlikely any artist will be more inclined to produce work knowing their great-grandchildren will receive royalties. With regard to retroactive copyright extension, he viewed it foolish to apply the government's argument that income received from royalties allows artists to produce more work saying, "How will extension help today’s Noah Webster create new works 50 years after his death?".
In a separate dissenting opinion, Justice Stevens also challenged the virtue of an individual reward, analyzing it from the perspective of patent law. He argued that the focus on compensation results only in “frustrating the legitimate members of the public who want to make use of it (a completed invention) in a free market.” Further, the compelling need to encourage creation is proportionally diminished once a work is already created. Yet while a formula pairing commercial viability to duration of protection may be said to produce more economically efficient results in respect of high technology inventions with shorter shelf-lives, the same perhaps cannot be said for certain forms of copyrighted works, for which the present value of expenditures relating to creation depend less on scientific equipment and research and development programs and more on unquantifiable creativity.
Lessig expressed surprise that no decision was authored by Chief Justice Rehnquist or by any of the other four justices who supported the "Lopez" or "Morrison" decisions. Lessig later expressed regret that he based his argument on precedent rather than attempting to demonstrate that the weakening of the public domain would cause harm to the economic health of the country.

</doc>
<doc id="41535" url="https://en.wikipedia.org/wiki?curid=41535" title="Bix Beiderbecke">
Bix Beiderbecke

Leon Bismark "Bix" Beiderbecke (March 10, 1903 – August 6, 1931) was an American jazz cornetist, jazz pianist, and composer.
With Louis Armstrong and Muggsy Spanier, Beiderbecke was one of the most influential jazz soloists of the 1920s. His turns on "Singin' the Blues" and "I'm Coming, Virginia" (both 1927), in particular, demonstrated an unusual purity of tone and a gift for improvisation. With these two recordings, especially, he helped to invent the jazz ballad style and hinted at what, in the 1950s, would become cool jazz. "In a Mist" (1927), one of a handful of his piano compositions and one of only two he recorded, mixed classical (Impressionist) influences with jazz syncopation.
A native of Davenport, Iowa, Beiderbecke taught himself to play cornet largely by ear, leading him to adopt a non-standard fingering some critics have connected to his original sound. He first recorded with Midwestern jazz ensembles, The Wolverines and The Bucktown Five in 1924, after which he played briefly for the Detroit-based Jean Goldkette Orchestra before joining Frankie "Tram" Trumbauer for an extended gig at the Arcadia Ballroom in St. Louis. Beiderbecke and Trumbauer joined Goldkette in 1926. The band toured widely and famously played a set opposite Fletcher Henderson at the Roseland Ballroom in New York City in . He made his greatest recordings in 1927 (see above). In 1928, Trumbauer and Beiderbecke left Detroit to join the best-known and most prestigious dance orchestra in the country: the New-York-based Paul Whiteman Orchestra.
Beiderbecke's most influential recordings date from his time with Goldkette and Whiteman, although they were generally recorded under his own name or Trumbauer's. The Whiteman period also marked a precipitous decline in Beiderbecke's health, brought on by the demand of the bandleader's relentless touring and recording schedule in combination with Beiderbecke's persistent alcoholism. A few stints in rehabilitation centers, as well as the support of Whiteman and the Beiderbecke family in Davenport, did not check Beiderbecke's decline in health. He left the Whiteman band in 1930 and the following summer died in his Queens apartment at the age of 28.
His death, in turn, gave rise to one of the original legends of jazz. In magazine articles, musicians' memoirs, novels, and Hollywood films, Beiderbecke has been reincarnated as a Romantic hero, the "Young Man with a Horn". His life has been portrayed as a battle against such common obstacles to art as family and commerce, while his death has been seen as a martyrdom for the sake of art. The musician-critic Benny Green sarcastically called Beiderbecke "jazz's Number One Saint," while Ralph Berton compared him to Jesus. Beiderbecke remains the subject of scholarly controversy regarding his true name, the cause of his death, and the importance of his contributions to jazz.
Early life.
Beiderbecke was born on March 10, 1903, in Davenport, Iowa, the son of Bismark Herman and Agatha Jane (Hilton) Beiderbecke. There is disagreement over whether Beiderbecke was christened Leon Bismark (and nicknamed "Bix") or Leon Bix. His father was nicknamed "Bix", as, for a time, was his older brother, Charles Burnette "Burnie" Beiderbecke. Burnie Beiderbecke claimed that the boy was named Leon Bix and subsequent biographers have reproduced birth certificates to that effect. However, more recent research—which takes into account church and school records in addition to the will of a relative—has suggested that he was originally named Leon Bismark. Regardless, his parents called him Bix, which seems to have been his preference. In a letter to his mother when he was nine years old, Beiderbecke signed off, "frome your Leon Bix Beiderbecke not Bismark Remeber ".
Beiderbecke's father, the son of German immigrants, was a well-to-do coal and lumber merchant, named after the Iron Chancellor of his native Germany. Beiderbecke's mother was the daughter of a Mississippi riverboat captain. She played the organ at Davenport's First Presbyterian Church, and encouraged young Bix's interest in the piano. Beiderbecke was the youngest of three children. His brother, Burnie, was born in 1895, and his sister, Mary Louise, in 1898. He began playing piano at age two or three. His sister recalls that he stood on the floor and played it with his hands over his head. Five years later, he was the subject of an admiring article in the "Davenport Daily Democrat" that proclaimed: "Seven-year-old boy musical wonder! Little Bickie Beiderbecke plays any selection he hears."
At age ten, his older brother Burnie recalled that he stopped coming home for supper, instead hurrying down to the riverfront and slipping aboard one or another of the excursion boats to play the Calliope. A friend remembered that the plots of the silent matinees Bix and his friends watched on Saturdays didn't interest him much, but as soon as the lights came on he would rush home to see if he could duplicate the melodies the accompanist had played during the action.
When his brother Burnie returned to Davenport at the end of 1918 after serving stateside during World War I, he brought with him a Victrola phonograph and several records, including "Tiger Rag" and "Skeleton Jangle" by the Original Dixieland Jazz Band. From these records, Bix first learned to love hot jazz; he taught himself to play cornet by listening to Nick LaRocca's horn lines. Beiderbecke also listened to jazz music off the riverboats that docked in downtown Davenport. Louis Armstrong and the drummer Baby Dodds claimed to have met Beiderbecke when their New-Orleans-based excursion boat stopped in Davenport. Historians disagree over whether that is true.
Beiderbecke attended Davenport High School from 1919 to 1921. During this time, he sat in and played professionally with various bands, including those of Wilbur Hatch, Floyd Bean and Carlisle Evans. In the spring of 1920 he performed for the school's Vaudeville Night, singing in a vocal quintet called the Black Jazz Babies and playing his horn. He also performed, at the invitation of his friend Fritz Putzier, in Neal Buckley's Novelty Orchestra. The group was hired for a gig in December 1920, but a complaint was lodged with the American Federation of Musicians, Local 67, that the boys did not have union cards. In an audition before a union executive, Beiderbecke was forced to sight read and failed. He did not earn his card.
On April 22, 1921, a month after he turned 18, Beiderbecke was arrested by two Davenport police officers on a charge brought by the father of a young girl. According to biographer Jean Pierre Lion, "Bix was accused of having taken this man's five-year-old daughter into a garage and committing on her an act qualified by the police report as 'lewd and lascivious.'" Although Beiderbecke was briefly taken into custody and held on a $1,500 bond, the charge was dropped after the girl was not made available to testify. According to an affidavit submitted by her father, this was because "of the child's age and the harm that would result to her in going over this case." It is not clear from the father's affidavit if the girl had identified Beiderbecke. Until recently, biographers have largely ignored this incident in Beiderbecke's life, and Lion was the first, in 2005, to print the police blotter and affidavit associated with the arrest. He dismissed the seriousness of the charge, but speculated that the arrest nevertheless might have led Beiderbecke to "feel abandoned and ashamed: he saw himself as suspect of perversion." Beiderbecke fans and scholars continue to argue over this incident's relevance and importance.
Beiderbecke's parents enrolled him in the exclusive Lake Forest Academy, north of Chicago in Lake Forest, Illinois. While historians have traditionally suggested that his parents sent him to Lake Forest to discourage his interest in jazz, others have begun to doubt this version of events, believing that he may have been sent away in response to his arrest. Regardless, Mr. and Mrs. Beiderbecke apparently felt that a boarding school would provide their son with both the necessary faculty attention and discipline to improve his academic performance. His interests, however, remained limited to music and sports. In pursuit of the former, Beiderbecke took the train into Chicago to catch the hot jazz bands at clubs and speakeasies, including the infamous Friar's Inn, where he listened to and sometimes sat in with the New Orleans Rhythm Kings. He also traveled to the predominantly African-American South Side to listen to what he called "real" jazz musicians. "Don't think I'm getting hard, Burnie," he wrote to his brother, "but I'd go to hell to hear a good band." On campus, he helped organize the Cy-Bix Orchestra with drummer Walter "Cy" Welge and almost immediately got into trouble with the Lake Forest headmaster for performing indecorously at a school dance.
Beiderbecke often failed to return to his dormitory before curfew, and sometimes stayed off-campus the next day. In the early morning hours of May 20, he was caught on the fire escape to his dormitory, attempting to climb back into his room. The faculty voted to expel him the next day, due both to his academic failings and his extracurricular activities, which included drinking. The headmaster informed Beiderbecke's parents by letter that following his expulsion school officials confirmed that Beiderbecke "was drinking himself and was responsible, in part at least, in having liquor brought into the School." Soon after, Beiderbecke began pursuing a career in music.
He returned to Davenport briefly in the summer of 1922, then moved to Chicago to join the Cascades Band, working that summer on Lake Michigan excursion boats. He gigged around Chicago until the fall of 1923, at times returning to Davenport to work for his father.
Career.
Wolverines.
Beiderbecke joined the Wolverine Orchestra late in 1923, and the seven-man group first played a speakeasy called the Stockton Club near Hamilton, Ohio. Specializing in hot jazz and recoiling from so-called sweet music, the band took its name from one of its most frequent numbers, Jelly Roll Morton's "Wolverine Blues." During this time, Beiderbecke also took piano lessons from a young woman who introduced him to the works of Eastwood Lane. Lane's piano suites and orchestral arrangements were both self-consciously American and influenced by the French Impressionists, and it is said to have greatly influenced Beiderbecke's style, especially on "In a Mist." A subsequent gig at Doyle's Dance Academy in Cincinnati became the occasion for a series of band and individual photographs that resulted in the most famous image of Beiderbecke—sitting fresh-faced, his hair perfectly combed, his horn resting on his right knee.
On February 18, 1924, the Wolverines first recorded at Gennett Records in Richmond, Indiana. Their two sides that day included "Fidgety Feet", written by Nick LaRocca and Larry Shields from the Original Dixieland Jazz Band, and "Jazz Me Blues." Beiderbecke's solo on the latter suggested something new and significant in jazz, according to biographers Richard M. Sudhalter and Philip R. Evans:
Both qualities—complementary or "correlated" phrasing and cultivation of the vocal, "singing" middle-range of the cornet—are on display in Bix's "Jazz Me Blues" solo, along with an already discernible inclination for unusual accidentals and inner chordal voices. It is a pioneer record, introducing a musician of great originality with a pace-setting band. And it astonished even the Wolverines themselves.
The Wolverines recorded 15 sides for Gennett Records between February and October 1924. The titles revealed a tough and well-formed cornet talent. His lip had toughened from earlier, more tentative years; on nine of the Wolverines' recorded titles he proceeds commandingly from lead to opening solo without any need for a respite from playing.
Beiderbecke made his first recordings 21 months before Armstrong recorded as a leader with the Hot Five. Beiderbecke's style was very different from that of Louis Armstrong according to "The Oxford Companion to Jazz":
Where Armstrong emphasized showmanship and virtuosity, Beiderbecke emphasized melody, even when improvising, and—different from Armstrong and contrary to how the Bix Beiderbecke of legend would be portrayed—he rarely strayed into the upper reaches of the register. Paul Mares of the New Orleans Rhythm Kings insisted that Beiderbecke's chief influence was the New Orleans cornetist Emmett Hardy, who died in 1925 at the age of 23. Indeed, Beiderbecke had met Hardy and the clarinetist Leon Roppolo in Davenport in 1921 when the two joined a local band and played in town for three months. Beiderbecke apparently spent time with them, but the degree to which Hardy's style influenced Beiderbecke's is difficult to know because Hardy never recorded. In some respects, Beiderbecke's playing was "sui generis", but he nevertheless listened to and studied the music around him: from Armstrong and Joe "King" Oliver to the Original Dixieland Jazz Band and the New Orleans Rhythm Kings to Claude Debussy and Maurice Ravel.
Soon, he was listening to Hoagy Carmichael, too. A law student and aspiring pianist and songwriter, Carmichael invited the Wolverines to Bloomington, Indiana, late in April 1924. Beiderbecke had met Carmichael a couple of times before and the two became friends. On May 6, 1924, the Wolverines recorded a tune Carmichael had written especially for Beiderbecke and his colleagues: "Riverboat Shuffle".
Beiderbecke left the Wolverines in October 1924 for a spot with Jean Goldkette in Detroit, but the job didn't last long. Goldkette recorded for the Victor Talking Machine Company, whose musical director, Eddie King, objected to Beiderbecke's hot-jazz style of soloing; it wasn't copacetic with the commercial obligations that came with the band's recording contract. King also was frustrated by the cornetist's inability to deftly sight read. After a few weeks, Beiderbecke was bounced from the Goldkette band, but soon arranged a recording session back in Richmond with some of its members. On January 26, 1925, Bix and His Rhythm Jugglers set two tunes to wax: "Toddlin' Blues", another number by LaRocca and Shields, and Beiderbecke's own composition, "Davenport Blues". Beiderbecke biographer Lion has complained that the second number was marred by the alcohol consumed by the musicians. In subsequent years, "Davenport Blues" has been recorded by musicians from Bunny Berigan to Ry Cooder to Geoff Muldaur.
The following month, Beiderbecke enrolled at the University of Iowa in Iowa City, Iowa. His stint in academia was even briefer than his time in Detroit, however. When he attempted to pack his course schedule with music, his guidance counselor forced him instead to take religion, ethics, physical education, and military training. It was an institutional blunder that Benny Green described as being, in retrospect, "comical," "fatuous," and "a parody." Beiderbecke promptly began to skip classes, and after he participated in a drunken bar fight, he was expelled. That summer he played with his friends Don Murray and Howdy Quicksell at a lake resort in Michigan. The band was run by Goldkette, and it put Beiderbecke in touch with another musician he had met before: the C-melody saxophone player Frankie Trumbauer. The two hit it off, both personally and musically, despite Trumbauer having been warned by other musicians: "Look out, he's trouble. He drinks and you'll have a hard time handling him." They were inseparable for much of the rest of Beiderbecke's career, with Trumbauer acting as a father figure to Beiderbecke. When Trumbauer organized a band for an extended run at the Arcadia Ballroom in St. Louis, Beiderbecke joined him. There he also played alongside the clarinetist Pee Wee Russell, who praised Beiderbecke's ability to drive the band. "He more or less made you play whether you wanted to or not," Russell said. "If you had any talent at all he made you play better."
Goldkette.
In the spring of 1926, Trumbauer closed up shop in St. Louis and, with Beiderbecke, moved to Detroit, this time to play with Goldkette's headline ensemble. They played the summer at Hudson Lake, a resort in northern Indiana, and split the next year between touring, recording, and performing at Detroit's Graystone Ballroom. In October 1926, Goldkette's "Famous Fourteen", as they came to be called, opened at the Roseland Ballroom in New York City opposite the Fletcher Henderson Orchestra, one of the East Coast's outstanding African American big bands. The Roseland promoted a "Battle of the Bands" in the local press and, on October 12, after a night of furious playing, Goldkette's men were declared the winners. "We […] were amazed, angry, morose, and bewildered," Rex Stewart, Fletcher's lead trumpeter, said of listening to Beiderbecke and his colleagues play. He called the experience "most humiliating".
Although the band recorded numerous sides for Victor during this period, none of them showcases Beiderbecke's most famous solos. Much of Goldkette's money was made through these records, but they were subject—as Eddie King had well understood—to the forces of the commercial market. As a result, their sound was often "sweeter" than what many of the hot jazz musicians would have preferred. In addition to their sessions with Goldkette, Beiderbecke and his friends recorded under their own names for the Okeh label. For instance, on February 4, 1927, Frank Trumbauer and His Orchestra recorded "Trumbology", "Clarinet Marmalade", and "Singin' the Blues", all three of which featured some of Beiderbecke's best work. Again with Trumbauer, Beiderbecke re-recorded Carmichael's "Riverboat Shuffle" in May and delivered two of his best known solos a few days later on "I'm Coming, Virginia" and "Way Down Yonder in New Orleans". Beiderbecke earned co-writing credit with Trumbauer on "For No Reason at All in C", recorded under the name Tram, Bix and Eddie (in their Three Piece Band). Beiderbecke switched between cornet and piano on that number, and then in September played only piano for his recording of "In A Mist". This was perhaps the most fruitful year of his short career.
Under financial pressure, Goldkette folded his premier band in September in New York. Paul Whiteman hoped to snatch up Goldkette's best musicians for his traveling orchestra, but Beiderbecke, Trumbauer, Murray, Bill Rank, Eddie Lang, Joe Venuti, Chauncey Morehouse, and Frank Signorelli instead joined the bass saxophone player Adrian Rollini at the Club New Yorker. When that job ended sooner than expected, in October 1927, Beiderbecke and Trumbauer signed on with Whiteman. They joined his orchestra in Indianapolis on October 27.
Whiteman.
The Paul Whiteman Orchestra was the most popular and highest paid band of the day. In spite of Whiteman's nickname, "The King of Jazz", his was not a jazz ensemble, but a popular music outfit that played bits of jazz and classical music according to the demands of its record-buying and concert-going audience. Whiteman was perhaps best known for having premiered George Gershwin's "Rhapsody in Blue" in New York in 1924, and the orchestrator of that piece, Ferde Grofé, continued to be an important part of the band in 1928. At three hundred pounds, Whiteman was huge both physically and culturally—"a man flabby, virile, quick, coarse, untidy and sleek, with a hard core of shrewdness in an envelope of sentimentalism," according to a 1926 "New Yorker" profile. And many Beiderbecke partisans have turned Whiteman into a villain in the years since.
Benny Green, in particular, derided Whiteman for being a mere "mediocre vaudeville act," and suggesting that "today we only tolerate the horrors of Whiteman's recordings at all in the hope that here and there a Bixian fragment will redeem the mess." Richard Sudhalter has responded by suggesting that Beiderbecke saw Whiteman as an opportunity to pursue musical ambitions that did not stop at jazz:
Colleagues have testified that, far from feeling bound or stifled by the Whiteman orchestra, as Green and others have suggested, Bix often felt a sense of exhilaration. It was like attending a music school, learning and broadening: formal music, especially the synthesis of the American vernacular idiom with a more classical orientation, so much sought-after in the 1920s, were calling out to him.
The education that Beiderbecke did not receive from the University of Iowa, in other words, he sought through Whiteman. In the meantime, Beiderbecke played on four number-one records in 1928, all under the Whiteman name: "Together", "Ramona", "My Angel", and "Ol' Man River", which featured Bing Crosby on vocals. This accomplishment says less about the jazz excellence of these records than it does about the tastes of the largely white, record-buying public to which Whiteman (and Goldkette before him) catered.
For Beiderbecke, the downside of being with Whiteman was the relentless touring and recording schedule, exacerbated by Beiderbecke's alcoholism. On November 30, 1928, in Cleveland, Beiderbecke suffered what Lion terms "a severe nervous crisis" and Sudhalter and Evans suggest "was in all probability an acute attack of delirium tremens," presumably triggered by Beiderbecke's attempt to curb his alcohol intake. "He cracked up, that's all," trombonist Bill Rank said. "Just went to pieces; broke up a roomful of furniture in the hotel."
In February 1929, Beiderbecke returned home to Davenport to convalesce and was hailed by the local press as "the world's hottest cornetist." He then spent the summer with Whiteman's band in Hollywood in preparation for the shooting of a new talking picture, "The King of Jazz". Production delays prevented any real work from being done on the film, leaving Beiderbecke and his pals plenty of time to drink heavily. By September, he was back in Davenport, where his parents helped him to seek treatment. He spent a month, from October 14 until November 18, at the Keeley Institute in Dwight, Illinois.
While he was away, Whiteman famously kept a chair empty in Beiderbecke's honor. But when he returned to New York at the end of January 1930, the renowned soloist did not rejoin Whiteman and performed only sparingly. On his last recording session, in New York, on September 15, 1930, Beiderbecke played on the original recording of Hoagy Carmichael's new song, "Georgia on My Mind", with Carmichael doing the vocal, Eddie Lang on guitar, Joe Venuti on violin, Jimmy Dorsey on clarinet and alto saxophone, Jack Teagarden on trombone, and Bud Freeman on tenor saxophone. The song would go on to become a jazz and popular music standard. In 2014, the 1930 recording of "Georgia on My Mind" was inducted into the Grammy Hall of Fame.
Two years earlier, Beiderbecke had influenced another Carmichael standard, "Star Dust". A Beiderbecke riff caught in Carmichael's head and became the tune's chorus. Bing Crosby, who sang with Whiteman, also cited Beiderbecke as an important influence. "Bix and all the rest would play and exchange ideas on the piano," he said.
With all the noise a New York pub going on, I don't know how they heard themselves, but they did. I didn't contribute anything, but I listened and learned […] I was now being influenced by these musicians, particularly horn men. I could hum and sing all of the jazz choruses from the recordings made by Bix, Phil Napoleon, and the rest.
Following the Wall Street Crash of 1929, the once-booming music industry contracted and work became more difficult to find. For a while, Beiderbecke's only income came from a radio show booked by Whiteman, "The Camel Pleasure Hour". However, during a live broadcast on October 8, 1930, Beiderbecke's seemingly limitless gift for improvisation finally failed him: "He stood up to take his solo, but his mind went blank and nothing happened," recalled a fellow musician, Frankie Cush. Whiteman finally let Beiderbecke go. The cornetist spent the rest of the year at home in Davenport and then, in February 1931, he returned to New York one last time.
Death.
He pulled me in and pointed to the bed. His whole body was trembling violently. He was screaming there were two Mexicans hiding under his bed with long daggers. To humor him, I looked under the bed and when I rose to assure him there was no one hiding there, he staggered and fell, a dead weight, in my arms. I ran across the hall and called in a woman doctor, Dr. Haberski, to examine him. She pronounced him dead.
Historians have disagreed over the identity of the doctor who pronounced Beiderbecke dead. The official cause of death, meanwhile, was lobar pneumonia, with scholars continuing to debate the extent to which his alcoholism was also a factor. Beiderbecke's mother and brother took the train to New York and brought his body home to Davenport. He was buried there on August 11 in the family plot at Oakdale Cemetery.
Legend and legacy.
At the time of his death Beiderbecke was little known except among fellow musicians, and for several years critics paid little attention to his music. As Jean Pierre Lion has pointed out, "The only serious and analytical obituary to have been published in the months" after his death was by a Frenchman, Hugues Panassié. The notice appeared in October 1931 and began with a bit of hyperbole and an incorrect fact, two hallmarks of much of the subsequent writing about Beiderbecke: "The announcement of Bix Beiderbecke's death plunged all jazz musicians into despair. We first believed it was a false alarm, as we had heard so often before about Bix. Unfortunately, precise information has been forthcoming, and we even know the day—August 7—when he passed away."
The "New Republic" critic Otis Ferguson wrote two short articles for the magazine, "Young Man with a Horn" (July 29, 1936) and "Young Man with a Horn Again" (November 18, 1940), that worked to revive interest not only in Beiderbecke's music but also in his biography. Beiderbecke "lived very briefly […] in what might be called the servants' entrance to art," Ferguson wrote. "His story is a good story, quite humble and right." The romantic notion of the short-lived, doomed jazz genius can be traced back at least as far as Beiderbecke, and lived on in Glenn Miller, Charlie Parker, Billie Holiday, Jaco Pastorius and many more.
Ferguson's sense of what was "right" became the basis for the Beiderbecke Romantic legend, which has traditionally emphasized the musician's Iowa roots, his often careless dress, his difficulty sight reading, the purity of his tone, his drinking, and his early death. These themes were repeated by Beiderbecke's friends in various memoirs, including "The Stardust Road" (1946) and "Sometimes I Wonder" (1965) by Hoagy Carmichael, "Really the Blues" (1946) by Mezz Mezzrow, and "We Called It Music" (1947) by Eddie Condon. Beiderbecke was portrayed as a tragic genius along the lines of Ludwig van Beethoven. "For his talent there were no conservatories to get stuffy in, no high-trumpet didoes to be learned doggedly, note-perfect as written," Ferguson wrote, "because in his chosen form the only writing of any account was traced in the close shouting air of Royal Gardens, Grand Pavilions, honkeytonks, etc." He was "this big overgrown kid, who looked like he'd been snatched out of a cradle in the cornfields," Mezzrow wrote. "The guy didn't have an enemy in the world," recalled Beiderbecke's friend Russ Morgan, "ut he was "out of this world" most of the time." According to Ralph Berton, he was "as usual gazing off into his private astronomy," but his cornet, Condon famously quipped, sounded "like a girl saying yes."
In 1938, Dorothy Baker borrowed the titles of her friend Otis Ferguson's two articles and published the novel "Young Man with a Horn". Her story of the doomed trumpet player Rick Martin was inspired, she wrote, by "the music, but not the life" of Beiderbecke, but the image of Martin quickly became the image of Beiderbecke: His story is about "the gap between the man's musical ability and his ability to fit it to his own life." In 1950, Michael Curtiz directed the film "Young Man with a Horn", starring Kirk Douglas, Lauren Bacall, and Doris Day. In this version, in which Hoagy Carmichael also plays a role, the Rick Martin character lives.
In "Blackboard Jungle", a 1955 film starring Glenn Ford and Sidney Poitier, Beiderbecke's music is briefly featured, but as a symbol of cultural conservatism in a nation on the cusp of the rock and roll revolution.
In 1971, on the 40th anniversary of Beiderbecke's death, the Bix Beiderbecke Memorial Jazz Festival was founded in Davenport, Iowa, to honor the musician. In 1974, Sudhalter and Evans published their biography, "Bix: Man and Legend", which was nominated for a National Book Award. In 1977, the Beiderbecke childhood home at 1934 Grand Avenue in Davenport was added to the National Register of Historic Places.
Beiderbecke's music was featured in three British comedy drama television series, all written by Alan Plater: "The Beiderbecke Affair" (1984), "The Beiderbecke Tapes" (1987), and "The Beiderbecke Connection" (1988). In 1991, the Italian director Pupi Avati released "Bix: An Interpretation of a Legend". Filmed partially in the Beiderbecke home, which Avati had purchased and renovated, "Bix" was screened at the Cannes Film Festival.
At the beginning of the 21st century, Beiderbecke's music continues to reside mostly out of the mainstream and some of the facts of his life are still debated, but scholars largely agree—due in part to the influence of Sudhalter and Evans—that he was an important innovator in early jazz; jazz cornetists, including Sudhalter (before his death in 2008), and Tom Pletcher, closely emulate his style. In 2003, to mark the hundredth anniversary of his birth, the Greater Astoria Historical Society and other community organizations, spearheaded by Paul Maringelli and The Bix Beiderbecke Sunnyside Memorial Committee, erected a plaque in Beiderbecke's honor at the apartment building in which he died in Queens. That same year, Frederick Turner published his novel "1929", which followed the facts of Beiderbecke's life fairly closely, focusing on his summer in Hollywood and featuring appearances by Al Capone and Clara Bow. The critic and musician Digby Fairweather sums up Beiderbecke's musical legacy, arguing that "with Louis Armstrong, Bix Beiderbecke was the most striking of jazz's cornet (and of course, trumpet) fathers; a player who first captivated his 1920s generation and after his premature death, founded a dynasty of distinguished followers beginning with Jimmy McPartland and moving on down from there."
Music.
Style and influence.
Bix Beiderbecke and Louis Armstrong were among jazz's first soloists. In New Orleans, jazz had been ensemble playing, with the various instruments weaving their parts into a single and coherent aural tapestry. There had been soloists, to be sure, with the clarinetist Sidney Bechet the best known among them, but these players "lacked the technical resources and, even more, the creative depth to make the solo the compelling centerpiece of jazz music." That changed in 1924 when Beiderbecke and Armstrong began to make their most important records. According to the critic Terry Teachout, they are "the two most influential figures in the early history of jazz" and "the twin lines of descent from which most of today's jazz can be traced."
Beiderbecke's cornet style is often described by contrasting it with Armstrong's markedly different approach. Armstrong was a virtuoso on his instrument, and his solos often took advantage of that fact. Beiderbecke was largely, although not completely, self-taught, and the constraints imposed by that fact were evident in his music. While Armstrong often soared into the upper register, Beiderbecke stayed in the middle range, more interested in exploring the melody and harmonies than in dazzling the audience. Armstrong often emphasized the performance aspect of his playing, while Beiderbecke tended to stare at his feet while playing, uninterested in personally engaging his listeners. Armstrong was deeply influenced by the blues, while Beiderbecke was influenced as much by modernist composers such as Debussy and Ravel as by his fellow jazzmen.
Beiderbecke's most famous solo was on "Singin' the Blues", recorded February 4, 1927. It has been hailed as an important example of the "jazz ballad style"—"a slow or medium-tempo piece played gently and sweetly, but not cloyingly, with no loss of muscle." The tune's laid-back emotions hinted at what would become, in the 1950s, the cool jazz style, personified by Chet Baker and Bill Evans. More than that, though, "Singin' the Blues" has been noted for the way its improvisations feel less improvised than composed, with each phrase building on the last in a logical fashion. Benny Green describes the solo's effect on practiced ears:
When a musician hears Bix's solo on 'Singing the Blues', he becomes aware after two bars that the soloist knows exactly what he is doing and that he has an exquisite sense of discord and resolution. He knows also that this player is endowed with the rarest jazz gift of all, a sense of form which lends to an improvised performance a coherence which no amount of teaching can produce. The listening musician, whatever his generation or his style, recognizes Bix as a modern, modernism being not a style but an attitude.
Like Green, who made particular mention of Beiderbecke's "amount of teaching," the jazz historian Ted Gioia also has emphasized Beiderbecke's lack of formal instruction, suggesting that it caused him to adopt "an unusual, dry embouchure" and "unconventional fingerings," which he retained for the rest of his life. Gioia points to "a characteristic streak of obstinacy" in Beiderbecke that provokes "this chronic disregard of the tried-and-true." He argues that this stubbornness was behind Beiderbecke's decision not to switch from cornet to trumpet when many other musicians, including Armstrong, did so. In addition, Gioia highlights Beiderbecke's precise timing, relaxed delivery, and pure tone, which contrasted with "the dirty, rough-edged sound" of King Oliver and his protégé Armstrong, whose playing was often more energetic and whose style held more sway early in the 1920s than Beiderbecke's. Gioia further wonders whether the many hyperbolic and quasi-poetic descriptions of Beiderbecke's style—most notably Condon's "like a girl saying yes"—may indicate that Beiderbecke's sound was muddled on recordings.
Eddie Condon, Hoagy Carmichael, and Mezz Mezzrow, all of whom hyperbolically raved about his playing, also saw Beiderbecke play live or performed alongside him. Condon, for instance, wrote of being amazed by Beiderbecke's piano playing: "All my life I had been listening to music […] But I had never heard anything remotely like what Beiderbecke played. For the first time I realized music isn't all the same, it had become an entirely new set of sounds" "I tried to explain Bix to the gang," Carmichael wrote, but "was no good, like the telling of a vivid, personal dream [… the emotion couldn't be transmitted."
Mezzrow described Beiderbecke's tone as being "pickled in alcohol […] I have never heard a tone like he got before or since. He played mostly open horn, every note full, big, rich and round, standing out like a pearl, loud but never irritating or jangling, with a powerful drive that few white musicians had in those days."
Some critics have highlighted "Jazz Me Blues", recorded with the Wolverines on February 18, 1924, as being particularly important to understanding Beiderbecke's style. Although it was one of his earliest recordings, the hallmarks of his playing were evident. "The overall impression we get from this solo, as in all of Bix at his best," writes the trumpeter Randy Sandke, "is that every note is spontaneous yet inevitable." Richard Hadlock describes Beiderbecke's contribution to "Jazz Me Blues" as "an ordered solo that seems more inspired by clarinetists Larry Shields of the ODJB and Leon Roppolo of the NORK than by other trumpet players." He goes on to suggest that clarinetists, by virtue of their not being tied to the melody as much as cornetists and trumpet players, could explore harmonies.
"Jazz Me Blues" was also important because it introduced what has been called the "correlated chorus", a method of improvising that Beiderbecke's Davenport friend Esten Spurrier attributed to both Beiderbecke and Armstrong. "Louis departed greatly from all cornet players in his ability to compose a close-knit individual 32 measures with all phrases compatible with each other", Spurrier told the biographers Sudhalter and Evans, "so Bix and I always credited Louis as being the father of the correlated chorus: play two measures, then two related, making four measures, on which you played another four measures related to the first four, and so on ad infinitum to the end of the chorus. So the secret was simple—a series of related phrases."
Beiderbecke plays piano on his recordings "Big Boy" (October 8, 1924), "For No Reason at All in C" (May 13, 1927), "Wringin' and Twistin'" (September 17, 1927)—all with ensembles—and his only solo recorded work, "In a Mist" (September 8, 1927). Critic Frank Murphy argues that many of the same characteristics that mark Beiderbecke on the cornet mark him on the keyboard: the uncharacteristic fingering, the emphasis on inventive harmonies, and the correlated choruses. Those inventive harmonies, on both cornet and piano, eventually helped point the way to bebop, which abandoned melody almost entirely.
Compositions.
Bix Beiderbecke wrote or co-wrote six instrumental compositions during his career:
"Candlelights", "Flashes", and "In the Dark" are piano compositions transcribed with the help of Bill Challis but never recorded by Beiderbecke. Two additional compositions were attributed to him by two other jazz composers: "Betcha I Getcha", attributed to Beiderbecke as a co-composer by Joe Venuti, the composer of the song, and "Cloudy", attributed to Beiderbecke by composer Charlie Davis as a composition from circa 1924.
Grammy Hall of Fame.
Bix Beiderbecke was posthumously inducted into the Grammy Hall of Fame, which is a special Grammy award established in 1973 to honor recordings that are at least 25 years old and that have "qualitative or historical significance."

</doc>
<doc id="41536" url="https://en.wikipedia.org/wiki?curid=41536" title="Duke Ellington">
Duke Ellington

Edward Kennedy "Duke" Ellington (April 29, 1899 – May 24, 1974) was an American composer, pianist, and bandleader of a jazz orchestra, which he led from 1923 until his death in a career spanning over fifty years.
Born in Washington, D.C., Ellington was based in New York City from the mid-1920s onward, and gained a national profile through his orchestra's appearances at the Cotton Club in Harlem. In the 1930s, his orchestra toured in Europe. Though widely considered to have been a pivotal figure in the history of jazz, Ellington embraced the phrase "beyond category" as a liberating principle, and referred to his music as part of the more general category of American Music, rather than to a musical genre such as jazz.
Some of the musicians who were members of Ellington's orchestra, such as saxophonist Johnny Hodges, are considered to be among the best players in jazz. Ellington melded them into the best-known orchestral unit in the history of jazz. Some members stayed with the orchestra for several decades. A master at writing miniatures for the three-minute 78 rpm recording format, Ellington often composed specifically to feature the style and skills of his individual musicians.
Often collaborating with others, Ellington wrote more than one thousand compositions; his extensive body of work is the largest recorded personal jazz legacy, with many of his works having become standards. Ellington also recorded songs written by his bandsmen, for example Juan Tizol's "Caravan", and "Perdido", which brought a Spanish tinge to big band jazz. After 1941, Ellington collaborated with composer-arranger-pianist Billy Strayhorn, whom he called his writing and arranging companion. With Strayhorn, he composed many extended compositions, or suites, as well as additional short pieces. Following an appearance at the Newport Jazz Festival, in July 1956, Ellington and his orchestra enjoyed a major career revival and embarked on world tours. Ellington recorded for most American record companies of his era, performed in several films, scoring several, and composed stage musicals.
Due to his inventive use of the orchestra, or big band, and thanks to his eloquence and charisma, Ellington is generally considered to have elevated the perception of jazz to an art form on a par with other more traditional musical genres. His reputation continued to rise after he died, and he was awarded a special Pulitzer Prize for music in 1999.
Early life.
Edward Kennedy Ellington was born on April 29, 1899, to James Edward Ellington and Daisy (Kennedy) Ellington in Washington, D.C. Both his parents were pianists. Daisy primarily played parlor songs and James preferred operatic arias. They lived with his maternal grandparents at 2129 Ida Place (now Ward Place), NW in the West End neighborhood of Washington, D.C. Duke's father was born in Lincolnton, North Carolina, on April 15, 1879, and moved to Washington, D.C. in 1886 with his parents. Daisy Kennedy was born in Washington, D.C., on January 4, 1879, the daughter of a former American slave. James Ellington made blueprints for the United States Navy. When Ellington was a child, his family showed racial pride and support in their home, as did many other families. African Americans in D.C. worked to protect their children from the era's Jim Crow laws.
At the age of seven, Ellington began taking piano lessons from Marietta Clinkscales. Daisy surrounded her son with dignified women to reinforce his manners and teach him to live elegantly. Ellington's childhood friends noticed that his casual, offhand manner, his easy grace, and his dapper dress gave him the bearing of a young nobleman, and began calling him "Duke." Ellington credited his chum Edgar McEntree for the nickname. "I think he felt that in order for me to be eligible for his constant companionship, I should have a title. So he called me Duke."
Though Ellington took piano lessons, he was more interested in baseball. "President Roosevelt (Teddy) would come by on his horse sometimes, and stop and watch us play", he recalled. Ellington went to Armstrong Technical High School in Washington, D.C. He gained his first job selling peanuts at Washington Senators baseball games.
In the summer of 1914, while working as a soda jerk at the Poodle Dog Café, Ellington wrote his first composition, "Soda Fountain Rag" (also known as the "Poodle Dog Rag"). He created the piece by ear, as he had not yet learned to read and write music. "I would play the 'Soda Fountain Rag' as a one-step, two-step, waltz, tango, and fox trot", Ellington recalled. "Listeners never knew it was the same piece. I was established as having my own repertoire." In his autobiography, "Music is my Mistress" (1973), Ellington wrote that he missed more lessons than he attended, feeling at the time that playing the piano was not his talent.
Ellington started sneaking into Frank Holiday's Poolroom at the age of fourteen. Hearing the poolroom pianists play ignited Ellington's love for the instrument, and he began to take his piano studies seriously. Among the many piano players he listened to were Doc Perry, Lester Dishman, Louis Brown, Turner Layton, Gertie Wells, Clarence Bowser, Sticky Mack, Blind Johnny, Cliff Jackson, Claude Hopkins, Phil Wurd, Caroline Thornton, Luckey Roberts, Eubie Blake, Joe Rochester, and Harvey Brooks.
Ellington began listening to, watching, and imitating ragtime pianists, not only in Washington, D.C., but in Philadelphia and Atlantic City, where he vacationed with his mother during the summer months. Dunbar High School music teacher Henry Lee Grant gave him private lessons in harmony. With the additional guidance of Washington pianist and band leader Oliver "Doc" Perry, Ellington learned to read sheet music, project a professional style, and improve his technique. Ellington was also inspired by his first encounters with stride pianists James P. Johnson and Luckey Roberts. Later in New York he took advice from Will Marion Cook, Fats Waller, and Sidney Bechet. Ellington started to play gigs in cafés and clubs in and around Washington, D.C. His attachment to music was so strong that in 1916 he turned down an art scholarship to the Pratt Institute in Brooklyn. Three months before graduating he dropped out of Armstrong Manual Training School, where he was studying commercial art.
Working as a freelance sign-painter from 1917, Ellington began assembling groups to play for dances. In 1919 he met drummer Sonny Greer from New Jersey, who encouraged Ellington's ambition to become a professional musician. Ellington built his music business through his day job: when a customer asked him to make a sign for a dance or party, he would ask if they had musical entertainment; if not, Ellington would offer to play for the occasion. He also had a messenger job with the U.S. Navy and State departments, where he made a wide range of contacts. Ellington moved out of his parents' home and bought his own as he became a successful pianist. At first, he played in other ensembles, and in late 1917 formed his first group, "The Duke's Serenaders" ("Colored Syncopators", his telephone directory advertising proclaimed). He was also the group's booking agent. His first play date was at the True Reformer's Hall, where he took home 75 cents.
Ellington played throughout the Washington, D.C. area and into Virginia for private society balls and embassy parties. The band included childhood friend Otto Hardwick, who began playing the string bass, then moved to C-melody sax and finally settled on alto saxophone; Arthur Whetsol on trumpet; Elmer Snowden on banjo; and Sonny Greer on drums. The band thrived, performing for both African-American and white audiences, a rarity in the segregated society of the day.
Music career.
Early career.
When his drummer Sonny Greer was invited to join the Wilber Sweatman Orchestra in New York City, Ellington made the fateful decision to leave behind his successful career in Washington, D.C., and move to Harlem, ultimately becoming part of the Harlem Renaissance. New dance crazes such as the Charleston emerged in Harlem, as well as African-American musical theater, including Eubie Blake's "Shuffle Along". After the young musicians left the Sweatman Orchestra to strike out on their own, they found an emerging jazz scene that was highly competitive and hard to crack. They hustled pool by day and played whatever gigs they could find. The young band met stride pianist Willie "The Lion" Smith, who introduced them to the scene and gave them some money. They played at rent-house parties for income. After a few months, the young musicians returned to Washington, D.C., feeling discouraged.
In June 1923, a gig in Atlantic City, New Jersey, led to a play date at the prestigious Exclusive Club in Harlem. This was followed in September 1923 by a move to the Hollywood Club – 49th and Broadway – and a four-year engagement, which gave Ellington a solid artistic base. He was known to play the bugle at the end of each performance. The group was initially called Elmer Snowden and his Black Sox Orchestra and had seven members, including trumpeter James "Bubber" Miley. They renamed themselves The Washingtonians. Snowden left the group in early 1924 and Ellington took over as bandleader. After a fire, the club was re-opened as the Club Kentucky (often referred to as the Kentucky Club).
Ellington made eight records in 1924, receiving composing credit on three including "Choo Choo". In 1925, Ellington contributed four songs to "Chocolate Kiddies" starring Lottie Gee and Adelaide Hall, an all-African-American revue which introduced European audiences to African-American styles and performers. Duke Ellington and his Kentucky Club Orchestra grew to a group of ten players; they developed their own sound by displaying the non-traditional expression of Ellington's arrangements, the street rhythms of Harlem, and the exotic-sounding trombone growls and wah-wahs, high-squealing trumpets, and sultry saxophone blues licks of the band members. For a short time soprano saxophonist Sidney Bechet played with them, imparting his propulsive swing and superior musicianship to the young band members.
Cotton Club engagement.
In October 1926, Ellington made an agreement with agent-publisher Irving Mills, giving Mills a 45% interest in Ellington's future. Mills had an eye for new talent and published compositions by Hoagy Carmichael, Dorothy Fields, and Harold Arlen early in their careers. After recording a handful of acoustic titles during 1924–26, Ellington's signing with Mills allowed him to record prolifically, although sometimes he recorded different versions of the same tune. Mills often took a co-composer credit. From the beginning of their relationship, Mills arranged recording sessions on nearly every label including Brunswick, Victor, Columbia, OKeh, Pathê (and its Perfect label), the ARC/Plaza group of labels (Oriole, Domino, Jewel, Banner) and their dime-store labels (Cameo, Lincoln, Romeo), Hit of the Week, and Columbia's cheaper labels (Harmony, Diva, Velvet Tone, Clarion) labels which gave Ellington popular recognition. On OKeh, his records were usually issued as The Harlem Footwarmers, while the Brunswick's were usually issued as The Jungle Band. Whoopee Makers and the Ten Black Berries were other pseudonyms.
In September 1927, King Oliver turned down a regular booking for his group as the house band at Harlem's Cotton Club; the offer passed to Ellington after Jimmy McHugh suggested him and Mills arranged an audition. Ellington had to increase from a six to eleven-piece group to meet the requirements of the Cotton Club's management for the audition, and the engagement finally began on December 4. With a weekly radio broadcast, the Cotton Club's exclusively white and wealthy clientele poured in nightly to see them. At the Cotton Club, Ellington's group performed all the music for the revues, which mixed comedy, dance numbers, vaudeville, burlesque, music, and illegal alcohol. The musical numbers were composed by Jimmy McHugh and the lyrics by Dorothy Fields (later Harold Arlen and Ted Koehler), with some Ellington originals mixed in. (Here he moved in with a dancer, his second wife Mildred Dixon.) Weekly radio broadcasts from the club gave Ellington national exposure, while Ellington also recorded Fields-JMcHugh and Fats Waller–Andy Razaf songs.
Although trumpeter Bubber Miley was a member of the orchestra for only a short period, he had a major influence on Ellington's sound. As an early exponent of growl trumpet, Miley changed the sweet dance band sound of the group to one that was hotter, which contemporaries termed Jungle Style. In October 1927, Ellington and his Orchestra recorded several compositions with Adelaide Hall. One side in particular, "Creole Love Call", became a worldwide sensation and gave both Ellington and Hall their first hit record. Miley had composed most of "Creole Love Call" and "Black and Tan Fantasy". An alcoholic, Miley had to leave the band before they gained wider fame. He died in 1932 at the age of 29, but he was an important influence on Cootie Williams, who replaced him.
In 1929, the Cotton Club Orchestra appeared on stage for several months in Florenz Ziegfeld's Show Girl, along with vaudeville stars Jimmy Durante, Eddie Foy, Jr., Ruby Keeler, and with music and lyrics by George Gershwin and Gus Kahn. Will Vodery, Ziegfeld's musical supervisor, recommended Ellington for the show, and, according to John Hasse's "Beyond Category: The Life and Genius of Duke Ellington", "Perhaps during the run of Show Girl, Ellington received what he later termed ' valuable lessons in orchestration from Will Vodery.' In his 1946 biography, "Duke Ellington", Barry Ulanov wrote:
Ellington's film work began with "Black and Tan" (1929), a nineteen-minute all-African-American RKO short in which he played the hero "Duke". He also appeared in the Amos 'n' Andy film "Check and Double Check" released in 1930. That year, Ellington and his Orchestra connected with a whole different audience in a concert with Maurice Chevalier and they also performed at the Roseland Ballroom, "America's foremost ballroom". Australian-born composer Percy Grainger was an early admirer and supporter. He wrote "The three greatest composers who ever lived are Bach, Delius and Duke Ellington. Unfortunately Bach is dead, Delius is very ill but we are happy to have with us today The Duke". Ellington's first period at the Cotton Club concluded in 1931.
The early 1930s.
Ellington led the orchestra by conducting from the keyboard using piano cues and visual gestures; very rarely did he conduct using a baton. As a bandleader, Ellington was not a strict disciplinarian; he maintained control of his orchestra with a combination of charm, humor, flattery, and astute psychology. A complex, private person, he revealed his feelings to only his closest intimates and effectively used his public persona to deflect attention away from himself .
Ellington signed exclusively to Brunswick in 1932 and stayed with them through late 1936 (albeit with a short-lived 1933–34 switch to Victor when Irving Mills temporarily moved him and his other acts from Brunswick).
As the Depression worsened, the recording industry was in crisis, dropping over 90% of its artists by 1933. Ivie Anderson was hired as their featured vocalist in 1931. She is the vocalist on "It Don't Mean a Thing (If It Ain't Got That Swing)" (1932) among other recordings. Sonny Greer had been providing occasional vocals and continued to do in a cross-talk feature with Anderson. Radio exposure helped maintain Ellington's public profile as his orchestra began to tour. The other records of this era include: "Mood Indigo" (1930), "Sophisticated Lady" (1933), "Solitude" (1934), and "In a Sentimental Mood" (1935)
While the band's United States audience remained mainly African-American in this period, the Ellington orchestra had a significant following overseas, exemplified by the success of their trip to England and Scotland in 1933 and their 1934 visit to the European mainland. The British visit saw Ellington win praise from members of the serious music community, including composer Constant Lambert, which gave a boost to Ellington's interest in composing longer works.
Those longer pieces had already begun to appear. He had composed and recorded Creole Rhapsody as early as 1931 (issued as both sides of a 12" record for Victor and both sides of a 10" record for Brunswick), and a tribute to his mother, "Reminiscing in Tempo", took four 10" record sides to record in 1935 after her death in that year. "Symphony in Black" (also 1935), a short film, featured his extended piece 'A Rhapsody of Negro Life'. It introduced Billie Holiday, and won an Academy Award as the best musical short subject. Ellington and his Orchestra also appeared in the features "Murder at the Vanities" and "Belle of the Nineties" (both 1934).
For agent Mills the attention was a publicity triumph, as Ellington was now internationally known. On the band's tour through the segregated South in 1934, they avoided some of the traveling difficulties of African-Americans by touring in private railcars. These provided easy accommodations, dining, and storage for equipment while avoiding the indignities of segregated facilities.
Competition was intensifying though, as swing bands like Benny Goodman's, began to receive popular attention. Swing dancing became a youth phenomenon, particularly with white college audiences, and danceability drove record sales and bookings. Jukeboxes proliferated nationwide, spreading the gospel of swing. Ellington's band could certainly swing, but their strengths were mood, nuance, and richness of composition, hence his statement "jazz is music, swing is business".
The later 1930s.
From 1936, Ellington began to make recordings with smaller groups (sextets, octets, and nonets) drawn from his then-15-man orchestra and he composed pieces intended to feature a specific instrumentalist, as with "Jeep's Blues" for Johnny Hodges, "Yearning for Love" for Lawrence Brown, "Trumpet in Spades" for Rex Stewart, "Echoes of Harlem" for Cootie Williams and "Clarinet Lament" for Barney Bigard. In 1937, Ellington returned to the Cotton Club which had relocated to the mid-town Theater District. In the summer of that year, his father died, and due to many expenses, Ellington's finances were tight, although his situation improved the following year.
After leaving agent Irving Mills, he signed on with the William Morris Agency. Mills though continued to record Ellington. After only a year, his Master and Variety labels, the small groups had recorded for the latter, collapsed in late 1937, Mills placed Ellington back on Brunswick and those small group units on Vocalion through to 1940. Well known sides continued to be recorded, "Caravan" in 1937, and "I Let a Song Go Out of My Heart" the following year.
Billy Strayhorn, originally hired as a lyricist, began his association with Ellington in 1939. Nicknamed "Swee' Pea" for his mild manner, Strayhorn soon became a vital member of the Ellington organization. Ellington showed great fondness for Strayhorn and never failed to speak glowingly of the man and their collaborative working relationship, "my right arm, my left arm, all the eyes in the back of my head, my brain waves in his head, and his in mine". Strayhorn, with his training in classical music, not only contributed his original lyrics and music, but also arranged and polished many of Ellington's works, becoming a second Ellington or "Duke's doppelganger". It was not uncommon for Strayhorn to fill in for Duke, whether in conducting or rehearsing the band, playing the piano, on stage, and in the recording studio. The 1930s ended with a very successful European tour just as World War II loomed in Europe.
Ellington in the early to mid-1940s.
Some of the musicians who joined Ellington at this time created a sensation in their own right. The short-lived Jimmy Blanton transformed the use of double bass in jazz, allowing it to function as a solo/melodic instrument rather than a rhythm instrument alone. Terminal illness forced him to leave by late 1941 after only about two years. Ben Webster, the Orchestra's first regular tenor saxophonist, whose main tenure with Ellington spanned 1939 to 1943, started a rivalry with Johnny Hodges as the Orchestra's foremost voice in the sax section.
Trumpeter Ray Nance joined, replacing Cootie Williams who had defected to Benny Goodman. Additionally, Nance added violin to the instrumental colors Ellington had at his disposal. Recordings exist of Nance's first concert date on November 7, 1940, at Fargo, North Dakota. Privately made by Jack Towers and Dick Burris, these recordings were first legitimately issued in 1978 as "Duke Ellington at Fargo, 1940 Live"; they are among the earliest of innumerable live performances which survive. Nance was also an occasional vocalist, although Herb Jeffries was the main male vocalist in this era (until 1943) while Al Hibbler (who replaced Jeffries in 1943) continued until 1951. Ivie Anderson left in 1942 after eleven years: the longest term of any of Ellington's vocalists.
Once again recording for Victor (from 1940), with the small groups recording for their Bluebird label, three-minute masterpieces on 78 rpm record sides continued to flow from Ellington, Billy Strayhorn, Ellington's son Mercer Ellington, and members of the Orchestra. "Cotton Tail", "Main Stem", "Harlem Airshaft", "Jack the Bear", and dozens of others date from this period. Strayhorn's "Take the "A" Train" a hit in 1941, became the band's theme, replacing "East St. Louis Toodle-Oo". Ellington and his associates wrote for an orchestra of distinctive voices who displayed tremendous creativity. Mary Lou Williams, working as a staff arranger, would briefly join Ellington a few years later.
Ellington's long-term aim though was to extend the jazz form from that three-minute limit, of which he was an acknowledged master. While he had composed and recorded some extended pieces before, such works now became a regular feature of Ellington's output. In this, he was helped by Strayhorn, who had enjoyed a more thorough training in the forms associated with classical music than Ellington. The first of these, "Black, Brown, and Beige" (1943), was dedicated to telling the story of African-Americans, and the place of slavery and the church in their history. Ellington debuted "Black, Brown and Beige" in Carnegie Hall on January 23, 1943, beginning an annual series of concerts there over the next four years. While some jazz musicians had played at Carnegie Hall before, none had performed anything as elaborate as Ellington's work. Unfortunately, starting a regular pattern, Ellington's longer works were generally not well received.
A partial exception was "Jump for Joy", a full-length musical based on themes of African-American identity, debuted on July 10, 1941 at the Mayan Theater in Los Angeles. Hollywood luminaries like actors John Garfield and Mickey Rooney invested in the production, and Charlie Chaplin and Orson Welles offered to direct. At one performance though, Garfield insisted Herb Jeffries, who was light skinned, should wear make-up. Ellington objected in the interval, and compared Jeffries to Al Jolson. The change was reverted, and the singer later commented that the audience must have thought he was an entirely different character in the second half of the show.
Although it had sold-out performances, and received positive reviews, it ran for only 122 performances until September 29, 1941, with a brief revival in November of that year. Its subject matter did not make it appealing to Broadway; Ellington had unfulfilled plans to take it there. Despite this disappointment, a Broadway production of Ellington's "Beggar's Holiday", his sole book musical, premiered on December 23, 1946 under the direction of Nicholas Ray.
The settlement of the first recording ban of 1942–43, leading to an increase in royalties paid to musicians, had a serious effect on the financial viability of the big bands, including Ellington's Orchestra. His income as a songwriter ultimately subsidized it. Although he always spent lavishly and drew a respectable income from the Orchestra's operations, the band's income often just covered expenses.
Early post-war years.
World War II brought about a swift end to the big band era as musicians went off to serve in the military and travel restrictions made touring difficult. When the war ended, the focus of popular music shifted towards crooners such as Frank Sinatra and Jo Stafford, so Ellington's wordless vocal feature "Transblucency" (1946) with Kay Davis was not going to have a similar reach. With inflation setting in after 1945, the cost of hiring big bands went up and club owners preferred smaller jazz groups who played in new styles such as bebop. Dancing in clubs also subjected club owners to a new wartime tax which continued for many years after, which made small bands more cost-effective for club owners.
Ellington continued on his own course through these tectonic shifts. While Count Basie was forced to disband his whole ensemble and work as an octet for a time, Ellington was able to tour most of Western Europe between April 6 and June 30, 1950, with the orchestra playing 74 dates over 77 days. During the tour, according to Sonny Greer, the newer works were not performed, though Ellington's extended composition, "Harlem" (1950) was in the process of being completed at this time. Ellington later presented its score to music-loving President Harry Truman. Also during his time in Europe, Ellington would compose the music for a stage production by Orson Welles. Titled "Time Runs" in Paris and "An Evening With Orson Welles" in Frankfurt, the variety show also featured a newly discovered Eartha Kitt, who performed Ellington's original song "Hungry Little Trouble" as Helen of Troy.
In 1951, Ellington suffered a significant loss of personnel: Sonny Greer, Lawrence Brown, and most importantly Johnny Hodges left to pursue other ventures, although only Greer was a permanent departee. Drummer Louie Bellson replaced Greer, and his "Skin Deep" was a hit for Ellington. Tenor player Paul Gonsalves had joined in December 1950 after periods with Count Basie and Dizzy Gillespie and stayed for the rest of his life, while Clark Terry joined in November 1951.
During the early 1950s, Ellington's career was at a low point with his style being generally seen as outmoded, but his reputation did not suffer as badly as some artists. André Previn said in 1952: "You know, Stan Kenton can stand in front of a thousand fiddles and a thousand brass and make a dramatic gesture and every studio arranger can nod his head and say, "Oh, yes, that's done like this." But Duke merely lifts his finger, three horns make a sound, and I don’t know what it is!" However, by 1955, after three years of recording for Capitol, Ellington lacked a regular recording affiliation.
Career revival.
Ellington's appearance at the Newport Jazz Festival on July 7, 1956 returned him to wider prominence and introduced him to a new generation of fans. The feature "Diminuendo and Crescendo in Blue" comprised two tunes that had been in the band's book since 1937 but largely forgotten until Ellington, who had abruptly ended the band's scheduled set because of the late arrival of four key players, called the two tunes as the time was approaching midnight. Announcing that the two pieces would be separated by an interlude played by tenor saxophonist Paul Gonsalves, Ellington proceeded to lead the band through the two pieces, with Gonsalves' 27-chorus marathon solo whipping the crowd into a frenzy, leading the Maestro to play way beyond the curfew time despite urgent pleas from festival organizer George Wein to bring the program to an end.
The concert made international headlines, led to one of only five "Time" magazine cover stories dedicated to a jazz musician, and resulted in an album produced by George Avakian that would become the best-selling LP of Ellington's career. Much of the music on the vinyl LP was, in effect, simulated, with only about 40% actually from the concert itself. According to Avakian, Ellington was dissatisfied with aspects of the performance and felt the musicians had been under rehearsed. The band assembled the next day to re-record several of the numbers with the addition of artificial crowd noise, none of which was disclosed to purchasers of the album. Not until 1999 was the concert recording properly released for the first time. The revived attention brought about by the Newport appearance should not have surprised anyone, Johnny Hodges had returned the previous year, and Ellington's collaboration with Strayhorn had been renewed around the same time, under terms more amenable to the younger man.
The original "Ellington at Newport" album was the first release in a new recording contract with Columbia Records which yielded several years of recording stability, mainly under producer Irving Townsend, who coaxed both commercial and artistic productions from Ellington.
In 1957, CBS (Columbia Records' parent corporation) aired a live television production of "A Drum Is a Woman", an allegorical suite which received mixed reviews. His hope that television would provide a significant new outlet for his type of jazz was not fulfilled. Tastes and trends had moved on without him. Festival appearances at the new Monterey Jazz Festival and elsewhere provided venues for live exposure, and a European tour in 1958 was well received. "Such Sweet Thunder" (1957), based on Shakespeare's plays and characters, and "The Queen's Suite" (1958), dedicated to Britain's Queen Elizabeth II, were products of the renewed impetus which the Newport appearance helped to create, although the latter work was not commercially issued at the time. The late 1950s also saw Ella Fitzgerald record her "Duke Ellington Songbook" (Verve) with Ellington and his orchestra—a recognition that Ellington's songs had now become part of the cultural canon known as the 'Great American Songbook'.
Ellington at this time (with Strayhorn) began to work directly on scoring for film soundtracks, in particular "Anatomy of a Murder" (1959), with James Stewart, in which Ellington appeared fronting a roadhouse combo, and "Paris Blues" (1961), which featured Paul Newman and Sidney Poitier as jazz musicians. "Detroit Free Press" music critic Mark Stryker concludes that the work of Billy Strayhorn and Ellington in "Anatomy of a Murder", a trial court drama film directed by Otto Preminger, is "indispensable, . . . too sketchy to rank in the top echelon among Ellington-Strayhorn masterpiece suites like "Such Sweet Thunder" and "The Far East Suite", but its most inspired moments are their equal."
Film historians have recognized the soundtrack "as a landmark – the first significant Hollywood film music by African Americans comprising non-diegetic music, that is, music whose source is not visible or implied by action in the film, like an on-screen band." The score avoided the cultural stereotypes which previously characterized jazz scores and rejected a strict adherence to visuals in ways that presaged the New Wave cinema of the ’60s". Ellington and Strayhorn, always looking for new musical territory, produced suites for John Steinbeck's novel "Sweet Thursday", Tchaikovsky's "Nutcracker Suite" and Edvard Grieg's "Peer Gynt".
In the early 1960s, Ellington embraced recording with artists who had been friendly rivals in the past, or were younger musicians who focused on later styles. The Ellington and Count Basie orchestras recorded together. During a period when he was between recording contracts, he made records with Louis Armstrong (Roulette), Coleman Hawkins, John Coltrane (both for Impulse) and participated in a session with Charles Mingus and Max Roach which produced the "Money Jungle" (United Artists) album. He signed to Frank Sinatra's new Reprise label, but the association with the label was short-lived.
Musicians who had previously worked with Ellington returned to the Orchestra as members: Lawrence Brown in 1960 and Cootie Williams in 1962."The writing and playing of music is a matter of intent... You can't just throw a paint brush against the wall and call whatever happens art. My music fits the tonal personality of the player. I think too strongly in terms of altering my music to fit the performer to be impressed by accidental music. You can't take doodling seriously." He was now performing all over the world; a significant part of each year was spent on overseas tours. As a consequence, he formed new working relationships with artists from around the world, including the Swedish vocalist Alice Babs, and the South African musicians Dollar Brand and Sathima Bea Benjamin ("A Morning in Paris", 1963/1997).
Ellington wrote an original score for director Michael Langham's production of Shakespeare's "Timon of Athens" at the Stratford Festival in Ontario, Canada which opened on July 29, 1963. Langham has used it for several subsequent productions, including a much later adaptation by Stanley Silverman which expands the score with some of Ellington's best-known works.
Last years.
Ellington was a Pulitzer Prize for Music nominee in 1965 but another nominee was selected. Then 66 years old, he said: "Fate is being kind to me. Fate doesn't want me to be famous too young." In 1999 he was posthumously awarded a special Pulitzer Prize (not the Music prize), "commemorating the centennial year of his birth, in recognition of his musical genius, which evoked aesthetically the principles of democracy through the medium of jazz and thus made an indelible contribution to art and culture."
In September 1965, he premiered the first of his Sacred Concerts. He created a jazz Christian liturgy. Although the work received mixed reviews, Ellington was proud of the composition and performed it dozens of times. This concert was followed by two others of the same type in 1968 and 1973, known as the Second and Third Sacred Concerts. These generated controversy in what was already a tumultuous time in the United States. Many saw the Sacred Music suites as an attempt to reinforce commercial support for organized religion, though Ellington simply said it was "the most important thing I've done". The Steinway piano upon which the Sacred Concerts were composed is part of the collection of the Smithsonian's National Museum of American History. Like Haydn and Mozart, Ellington conducted his orchestra from the piano – he always played the keyboard parts when the Sacred Concerts were performed.
Despite his advancing age (he turned 65 in the spring of 1964), Ellington showed no sign of slowing down as he continued to make vital and innovative recordings, including "The Far East Suite" (1966), "New Orleans Suite" (1970), "Latin American Suite" (1972) and "The Afro-Eurasian Eclipse" (1971), much of it inspired by his world tours. It was during this time that he recorded his only album with Frank Sinatra, entitled "Francis A. & Edward K." (1967).
Although he made two more stage appearances before his death, Ellington performed what is considered his final full concert in a ballroom at Northern Illinois University on March 20, 1974.
The last three shows Ellington and his orchestra performed were one on March 21, 1973 at Purdue University's Hall of Music and two on March 22, 1973 at the Sturges-Young Auditorium in Sturgis, Michigan.
Ellington died on May 24, 1974 of complications from lung cancer and pneumonia, a few weeks after his 75th birthday. At his funeral, attended by over 12,000 people at the Cathedral of St. John the Divine, Ella Fitzgerald summed up the occasion, "It's a very sad day. A genius has passed." He was interred in the Woodlawn Cemetery, The Bronx, New York City.
Following Duke's death, his son Mercer took over leadership of the orchestra, continuing until his own death in 1996. Like the Count Basie Orchestra, this group continued to release albums long after Duke Ellington's death. "Digital Duke", credited to The Duke Ellington Orchestra, won the 1988 Grammy Award for Best Large Jazz Ensemble Album. Mercer Ellington had been handling all administrative aspects of his father's business for several decades. Mercer's children continue a connection with their grandfather's work.
Personal life.
Ellington married his high school sweetheart, Edna Thompson (d. 1967), on July 2, 1918, when he was 19. The next spring, on March 11, 1919, Edna gave birth to their only son, Mercer Kennedy Ellington.
Ellington was joined in New York City by his wife and son in the late twenties, but the couple soon permanently separated. According to her obituary in "Jet" magazine, she was "homesick for Washington" and returned. In 1928, Ellington became the companion of Mildred Dixon, who traveled with him, managed Tempo Music, inspired songs at the peak of his career, and reared his son Mercer.
In 1938 he left his family (his son was then 19) and moved in with Beatrice "Evie" Ellis, a Cotton Club employee. Their relationship, though stormy, continued after Ellington met and formed a relationship with Fernanda de Castro Monte in the early 1960s. Ellington supported both women for the rest of his life.
Ellington's sister Ruth (1915–2004) later ran Tempo Music, his music publishing company. Ruth's second husband was the bass-baritone McHenry Boatwright, whom she met when he sang at her brother's funeral.
As an adult, son Mercer Ellington (d. 1996) played trumpet and piano, and led his own band. He also worked as his father's business manager, eventually taking full control of the band after Duke's death. He was an important archivist of his father's musical life.
Legacy.
Memorials.
Numerous memorials have been dedicated to Duke Ellington, in cities from New York and Washington, D.C. to Los Angeles. Ellington is buried in Woodlawn Cemetery in The Bronx, New York City.
In Ellington's birthplace, Washington, D.C., the Duke Ellington School of the Arts educates talented students, who are considering careers in the arts, by providing intensive arts instruction and strong academic programs that prepare students for post-secondary education and professional careers. Originally built in 1935, the Calvert Street Bridge was renamed the Duke Ellington Bridge in 1974.
In 1989, a bronze plaque was attached to the newly named Duke Ellington Building at 2121 Ward Place, NW. In 2012, the new owner of the building commissioned a mural by Aniekan Udofia that appears above the lettering "Duke Ellington".
In 2010 the triangular park, across the street from Duke Ellington's birth site, at the intersection of New Hampshire and M Streets, NW was named the Duke Ellington Park. 
Ellington's residence at 2728 Sherman Avenue, NW, during the years 1919–1922, is marked by a bronze plaque.
On February 24, 2009, the United States Mint launched a new coin featuring Duke Ellington, making him the first African American to appear by himself on a circulating U.S. coin. Ellington appears on the reverse (tails) side of the District of Columbia quarter. The coin is part of the U.S. Mint's program honoring the District and the U.S. territories and celebrates Ellington's birthplace in the District of Columbia. Ellington is depicted on the quarter seated at a piano, sheet music in hand, along with the inscription "Justice for All", which is the District's motto.
Ellington lived for years in a townhouse on the corner of Manhattan's Riverside Drive and West 106th Street. After his death, West 106th Street was officially renamed Duke Ellington Boulevard. A large memorial to Ellington, created by sculptor Robert Graham, was dedicated in 1997 in New York's Central Park, near Fifth Avenue and 110th Street, an intersection named Duke Ellington Circle.
A statue of Ellington at a piano is featured at the entrance to UCLA's Schoenberg Hall. According to UCLA Magazine:
The Essentially Ellington High School Jazz Band Competition and Festival is a nationally renowned annual competition for prestigious high school bands. Started in 1996 at Jazz at Lincoln Center, the festival is named after Ellington because of the large focus that the festival places on his works.
Tributes.
Gunther Schuller wrote in 1989Ellington composed incessantly to the very last days of his life. Music was indeed his mistress; it was his total life and his commitment to it was incomparable and unalterable. In jazz he was a giant among giants. And in twentieth century music, he may yet one day be recognized as one of the half-dozen greatest masters of our time.
Martin Williams said: "Duke Ellington lived long enough to hear himself named among our best composers. And since his death in 1974, it has become not at all uncommon to see him named, along with Charles Ives, as the greatest composer we have produced, regardless of category."
In the opinion of Bob Blumenthal of "The Boston Globe" in 1999: "n the century since his birth, there has been no greater composer, American or otherwise, than Edward Kennedy Ellington."
In 2002, scholar Molefi Kete Asante listed Duke Ellington on his list of 100 Greatest African Americans.
While his compositions are now the staple of the repertoire of music conservatories, they have been revisited by artists and musicians around the world both as a source of inspiration and a bedrock of their own performing careers.
There are hundreds of albums dedicated to the music of Duke Ellington and Billy Strayhorn by artists famous and obscure. "Sophisticated Ladies", an award-winning 1981 musical revue, incorporated many tunes from Ellington's repertoire. A second Broadway musical interpolating Ellington's music, "Play On!", debuted in 1997.
Awards and honors.
Grammy Awards.
Ellington earned 12 Grammy awards from 1959 to 2000, three of which were posthumous.
Grammy Hall of Fame.
Recordings of Duke Ellington were inducted into the Grammy Hall of Fame, which is a special Grammy award established in 1973 to honor recordings that are at least twenty-five years old, and that have qualitative or historical significance.

</doc>
<doc id="41538" url="https://en.wikipedia.org/wiki?curid=41538" title="Rahsaan Roland Kirk">
Rahsaan Roland Kirk

Rahsaan Roland Kirk (August 7, 1935 – December 5, 1977) was an American jazz multi-instrumentalist who played tenor saxophone, flute and many other instruments. He was renowned for his onstage vitality, during which virtuoso improvisation was accompanied by comic banter, political ranting, and the ability to play several instruments simultaneously.
Biography.
Kirk was born Ronald Theodore Kirk in Columbus, Ohio, where he lived in a neighborhood known as Flytown. He felt compelled by a dream to transpose two letters in his first name to make "Roland." He became blind at an early age as a result of poor medical treatment. In 1970, Kirk added "Rahsaan" to his name after hearing it in a dream.
Preferring to lead his own bands, Kirk rarely performed as a sideman, although he did record with arranger Quincy Jones and drummer Roy Haynes and had notable stints with bassist Charles Mingus. One of his best-known recorded performances is the lead flute and solo on Jones' "Soul Bossa Nova", a 1964 hit song repopularized in the "Austin Powers" films (Jones 1964; McLeod et al. 1997).
His playing was generally rooted in soul jazz or hard bop, but Kirk's knowledge of jazz history allowed him to draw from many elements of the music's past, from ragtime to swing and free jazz. Kirk also absorbed classical influences, and his artistry reflected elements of pop music by composers such as Smokey Robinson and Burt Bacharach, as well as Duke Ellington, John Coltrane and other jazz musicians. The live album "Bright Moments" (1973) is an example of one of his shows. His main instrument was the tenor saxophone, supplemented by other saxes, and contrasted with the lighter sound of the flute. At times he would play a number of these horns at once, harmonizing with himself, or sustain a note for lengthy durations by using circular breathing, or play the rare, seldom heard nose flute. A number of his instruments were exotic or homemade, but even while playing two or three saxophones at once, the music was intricate, powerful jazz with a strong feel for the blues.
Kirk was politically outspoken. During his concerts, between songs he often talked about topical issues, including black history and the civil rights movement. His monologues were often laced with satire and absurdist humor. According to comedian Jay Leno, when Leno toured with Kirk as Kirk's opening act, Kirk would introduce him by saying, "I want to introduce a young brother who knows the black experience and knows all about the white devils ... Please welcome Jay Leno!"
In 1975, Kirk suffered a major stroke which led to partial paralysis of one side of his body. However, he continued to perform and record, modifying his instruments to enable him to play with one arm. At a live performance at Ronnie Scott's Jazz Club in London he even managed to play two instruments, and carried on to tour internationally and to appear on television.
He died from a second stroke in 1977 after performing in the Frangipani Room of the Indiana University Student Union in Bloomington, Indiana.
Instruments and technique.
Kirk played and collected a number of musical instruments, mainly various saxophones, clarinets and flutes. His main instruments were tenor saxophone and two obscure saxophones: the stritch (a straight alto sax lacking the instrument's characteristic upturned bell) and a manzello (a modified saxello soprano sax, with a larger, upturned bell). Kirk modified these instruments himself to accommodate his simultaneous playing technique.
He typically appeared on stage with all three horns hanging around his neck, as well as a variety of other instruments, including flutes and whistles, and often kept a gong within reach. Kirk also played clarinet, harmonica, English horn, and recorders, and was a competent trumpeter. He often had unique approaches, using a saxophone mouthpiece on a trumpet or playing nose flute. He additionally used many non-musical devices, such as alarm clocks, sirens, or a section of common garden hose (dubbed "the black mystery pipes"). His studio recordings also used tape-manipulated "musique concrète" and primitive electronic sounds (before such things became commonplace).
Kirk was also an influential flautist, employing several techniques that he developed himself. One technique was to sing or hum into the flute at the same time as playing. Another was to play the standard transverse flute at the same time as a nose flute.
He used the multiple horns to play true chords, essentially functioning as a one-man saxophone section. Kirk insisted that he was only trying to emulate the sounds he heard in his head.
Kirk was a major exponent of circular breathing. Using this technique, he was not only able to sustain a single note for an extended period; he could also play sixteenth-note runs of almost unlimited length, and at high speeds. His circular breathing ability enabled him to record "Concerto For Saxophone" on the "Prepare Thyself to Deal With a Miracle" LP in one continuous take of about 20 minutes' playing with no discernible "break" for inhaling. His long-time producer at Atlantic Jazz, Joel Dorn, believed he should have received credit in "The Guinness Book of World Records" for such feats (he was capable of playing continuously "without taking a breath" for far longer than exhibited on that LP), but this never happened.
"The Case of the 3 Sided Dream in Audio Color" was a unique album in jazz and popular music recorded annals. It was a two-LP set, with Side 4 apparently "blank", the label not indicating any content. However, once word of "the secret message" got around among Rahsaan's fans, one would find that about 12 minutes into Side 4 appeared the first of two telephone answering machine messages recorded by Kirk, the second following soon thereafter (but separated by more blank grooves). The surprise impact of these segments appearing on "blank" Side 4 was lost on the CD reissue of this album.
He gleaned information on what was happening in the world via audio media like radio and the sounds coming from TV sets. His later recordings often incorporated his spoken commentaries on current events, including Richard Nixon's involvement in the Watergate scandal. The "3-Sided Dream" album was a "concept album" which incorporated of "found" or environmental sounds and tape loops, tapes being played backwards, etc. Snippets of Billie Holiday singing are also heard briefly. The album even confronts the rise of influence of computers in society, as Rahsaan threatens to pull the plug on the machine trying to tell him what to do.
In the album "Other Folks' Music" the spoken words of Paul Robeson, another outspoken black artist, can be briefly heard.
Discography.
As sideman.
With Jaki Byard
With Tubby Hayes
With Roy Haynes
With Quincy Jones
With Les McCann
With Charles Mingus
With Tommy Peltier

</doc>
<doc id="41544" url="https://en.wikipedia.org/wiki?curid=41544" title="John McLoughlin">
John McLoughlin

Dr. John McLoughlin, baptized Jean-Baptiste McLoughlin, (October 19, 1784 – September 3, 1857) was a Chief Factor and Superintendent of the Columbia District of the Hudson's Bay Company at Fort Vancouver from 1824 to 1845. He was later known as the "Father of Oregon" for his role in assisting the American cause in the Oregon Country in the Pacific Northwest. In the late 1840s his general store in Oregon City was famous as the last stop on the Oregon Trail.
Childhood and early career.
McLoughlin was born in Rivière-du-Loup, Quebec, of Irish (his grandfather came from Sharagower in the Inishowen peninsular of County Donegal), Scottish, and French Canadian descent. He lived with his great uncle, Colonel William Fraser, for a while as a child. Though baptized Roman Catholic, he was raised Anglican and in his later life he returned to the Roman Catholic faith. In 1798, he began to study medicine with Sir James Fisher of Quebec.
After studying for 4½ years he was granted a license to practice medicine on April 30, 1803. He was hired as a physician at Fort William, a fur-gathering post of the North West Company on Lake Superior; there he became a trader and mastered several Indian languages.
In 1814 he became a partner in the company. In 1816 McLoughlin was arrested for the murder of Robert Semple, the governor of the Red River Colony, after the Battle of Seven Oaks, though it is often claimed he stood in proxy for some Indians who were blamed. He was tried on October 30, 1818, and the charges were dismissed. McLoughlin was instrumental in the negotiations leading to the North West Company's 1821 merger with the Hudson's Bay Company. He was promoted to head the Lac la Pluie district temporarily shortly after the merger.
The Columbia District.
In 1824 the Hudson's Bay Company appointed McLoughlin, already a Chief Factor, Superintendent of the Columbia District (roughly parallel to what Americans know as the Oregon Country), with Peter Skene Ogden appointed to assist him. At the time, the region was under joint occupation of both the United States and Britain pursuant to the Treaty of 1818. Upon his arrival, he determined that the headquarters of the company at Fort Astoria (now Astoria, Oregon) at the mouth of the Columbia River was unfit. The York Factory Express trade route evolved from an earlier express brigade used by the North West Company between Fort George (originally Fort Astoria founded in 1811 by John Jacob Astor's American Fur Company), at the mouth of the Columbia River, to Fort William on Lake Superior.
In the 1821 merger with the North West Company, the Hudson's Bay Company gained control of NWC trading posts west of the Rocky Mountains, with headquarters at Fort George (formerly Astoria). George Simpson, Governor of Hudson's Bay Company, visited the Columbia District in 1824-25, journeying from York Factory. He investigated a quicker route than previously used, following the Saskatchewan River and crossing the mountains at Athabasca Pass. This route was thereafter followed by the York Factory Express brigades.
McLoughlin built Fort Vancouver (now Vancouver, Washington) as a replacement for Fort George, on the north side of the Columbia across from the mouth of the Willamette River, at a site chosen by Sir George Simpson. The post was opened for business on March 19, 1825. From his Columbia Department headquarters in Fort Vancouver he supervised trade and kept peace with the Indians, inaugurated salmon and timber trade with Mexican controlled California and Hawaii, and supplied Russian America with produce.
By 1825 there were usually two brigades, each setting out from opposite ends of the route, Fort Vancouver in the Columbia District on the lower Columbia River and the other from York Factory on Hudson Bay, in spring and passing each other in the middle of the continent. Each brigade consisted of about forty to seventy five men and two to five specially made boats and travelled at breakneck speed (for the time). Indians along the way were often paid in trade goods to help them portage around falls and unnavigable rapids. An 1839 report cites the travel time as three months and ten days—almost 26 miles (40 km) per day on average. These men carried supplies in and furs out by boat, horseback and as back packs for the forts and trading posts along the route. They also carried status reports for supplies needed, furs traded etc. From Dr. John McLoughlin head of the Oregon Country HBC operations, and the other fort managers along the route. 
Fort Vancouver became the center of activity in the Pacific Northwest. Every year ships would come from London to drop off supplies and trade goods in exchange for the furs. It was the nexus for the fur trade on the Pacific Coast; its influence reached from the Rocky Mountains to the Hawaiian Islands, and from Russian Alaska into Mexican-controlled California. From Fort Vancouver, at its pinnacle, McLoughlin watched over 34 outposts, 24 ports, six ships, and 600 employees. Under McLoughlin's management, the Columbia Department remained highly profitable, in part due to the ongoing high demand for beaver hats in Europe.
In 1828, McLoughlin was in charge at Fort Vancouver when American explorer Jedediah Smith and three other survivors arrived following the massacre of fifteen members of his exploring party by Umpqua people, who lived to the south in Oregon.
McLoughlin's appearance, 6 foot 4 inches (193 cm) tall with long, prematurely white hair, brought him respect; but he was also generally known for his fair treatment of the people with whom he dealt, whether they were British subjects, U.S. citizens, or of indigenous origin. At the time, the wives of many Hudson's Bay field employees were indigenous, including McLoughlin's wife Marguerite; who was metis, the daughter of an aboriginal woman and one of the original partners of the North West Company, Jean-Etienne Wadin. She was the widow of Alexander McKay, a trader killed in the "Tonquin" incident. Her son Thomas became McLoughlin's stepson.
When three Japanese fishermen, among them Otokichi, were shipwrecked on the Olympic Peninsula in 1834, McLoughlin, envisioning an opportunity to use them to open trade with Japan, sent the trio to London on the "Eagle" to try to convince the Crown of his plan. They reached London in 1835, probably the first Japanese to do so since the 16th century Christopher and Cosmas. The British Government finally did not show interest, and the castaways were sent to Macau so that they could be returned to Japan, but that was not possible as Japan was closed to most outsiders at the time.
Relations with American settlers.
In 1821 the British Parliament imposed the laws of Upper Canada on British subjects in Columbia District, and gave the authority to enforce those laws to the Hudson's Bay Company. John McLoughlin, as Superintendent of Fort Vancouver, applied the law to British subjects, kept peace with the natives and sought to maintain law and order over American settlers as well.
In the early 1840s, with the arrival of the first wagon trains via the Oregon Trail, McLoughlin disobeyed company orders and extended substantial aid to the American settlers. Relations between Britain and the United States had become very strained, and many expected war to break out any time. McLoughlin's aid probably prevented an armed attack on his outpost by the numerous American settlers. The settlers understood that his motives were not purely altruistic, and some resented the assistance, working against him for the rest of his life.
The Hudson Bay Company officially discouraged settlement because it interfered with the lucrative fur trade. The company belatedly realized that the increasing numbers of American settlers in the area would result in Columbia District becoming part of U.S. territory. In 1841, Hudson Bay Company Governor George Simpson ordered Alexander Ross to organize a party of Red River settlers to emigrate and occupy the land for Britain. James Sinclair took over leadership from Ross, and when the expedition of almost 200 men women and children reached Fort Vancouver later that year most were settled on Hudson's Bay farms at Nisqually and Cowlitz and some settled south of the Columbia River In the Willamette Valley
As tensions mounted in the Oregon boundary dispute; Simpson, realizing that border might ultimately be as far north as the 49th parallel, ordered McLoughlin to relocate their regional headquarters to Vancouver Island. McLoughlin, in turn, directed James Douglas to construct Fort Camosun (now Victoria, British Columbia, Canada) in 1843. But McLoughlin, whose life was increasingly connected to the Willamette River Valley, refused to move there.
McLoughlin was involved with the debate over the future of the Oregon Country. He advocated an independent nation that would be free of the United States during debates at the Oregon Lyceum in 1842 through his lawyer. This view won support at first and a resolution adopted, but was later moved away from in favor of a resolution by George Abernethy of the Methodist Mission to wait on forming an independent country.
John McLoughlin lost one son to a violent death. John McLoughlin, Jr. had been appointed the second Clerk in Charge at Fort Stikine, only to die in April 1842 at the hands of one of the fort employees, Urbain Heroux, who was charged with his murder but acquitted for lack of evidence, which added to the grievances John Sr. held against the Company. In 2015, Goose Lane Editions published "The Bastard of Fort Stikine", a historical look at McLoughlin, Jr.'s violent death, by forensic anthropologist and author Debra Komar.
In 1843 American settlers established their own government, called the Provisional Government of Oregon. A legislative committee drafted a code of laws known as the Organic Law. It included the creation of an executive committee of three, a judiciary, militia, land laws, and four counties. There was vagueness and confusion over the nature of the 1843 Organic Law, in particular whether it was a constitutional or statutory. In 1844 a new legislative committee decided to consider it statutory. The 1845 Organic Law made additional changes, including allowing the participation of British subjects in the government. Although the Oregon Treaty of 1846 settled the boundaries of US jurisdiction upon all lands south of the 49th parallel, the Provisional Government continued to function until 1849, when the first governor of Oregon Territory arrived.
Later life in the Oregon Territory.
After resigning from the Hudson's Bay Company in 1846, McLoughlin moved his family back south to Oregon City in the Willamette Valley. The Oregon Treaty had been ratified by that time, and the region, now known as the Oregon Territory, was part of the United States. The valley was the destination of choice for settlers streaming in over the Oregon Trail. At his Oregon City store he sold food and farming tools to settlers.
In 1847, McLoughlin was given the Knighthood of St. Gregory, bestowed on him by Pope Gregory XVI. He became a U.S. citizen in 1849. McLoughlin's opponents succeeded in inserting a clause forfeiting his land claim in the Donation Land Claim Act of 1850 by Samuel R. Thurston. Although it was never enforced, it embittered the elderly McLoughlin. He served as mayor of Oregon City in 1851, winning 44 of 66 votes. He died of natural causes in 1857. His grave is now located beside his home overlooking downtown Oregon City.
Legacy.
In 1953, the state of Oregon donated to the National Statuary Hall Collection a bronze statue of McLoughlin, which is currently displayed at the Capitol Visitor Center. The title "Father of Oregon" was officially bestowed on him by the Oregon Legislative Assembly in 1957, on the centennial of his death. Many public works in Oregon are named after him, including:
McLoughlin's former residence in Oregon City, now known as the McLoughlin House, is today a museum; it is part of the Fort Vancouver National Historic Site.

</doc>
<doc id="41545" url="https://en.wikipedia.org/wiki?curid=41545" title="Avogadro constant">
Avogadro constant

In chemistry and physics, the Avogadro constant is the number of constituent particles, usually atoms or molecules, that are contained in the amount of substance given by one mole. Thus, it is the proportionality factor that relates the molar mass of a compound to the mass of a sample. Avogadro's constant, often designated with the symbol "N"A or "L", has the value in the International System of Units (SI).
Previous definitions of chemical quantity involved Avogadro's number, a historical term closely related to the Avogadro constant, but defined differently: Avogadro's number was initially defined by Jean Baptiste Perrin as the number of atoms in one gram-molecule of atomic hydrogen, meaning one gram of hydrogen. This number is also known as Loschmidt constant in German literature. The constant was later redefined as the number of atoms in 12 grams of the isotope carbon-12 (12C), and still later generalized to relate amounts of a substance to their molecular weight. For instance, to a first approximation, 1 gram of hydrogen element (H), having the atomic (mass) number 1, has hydrogen atoms. Similarly, 12 grams of 12C, with the mass number 12 (atomic number 6), has the same number of carbon atoms, . Avogadro's number is a dimensionless quantity, and has the same numerical value of the Avogadro constant given in base units. In contrast, the Avogadro constant has the dimension of reciprocal amount of substance.
Revisions in the base set of SI units necessitated redefinitions of the concepts of chemical quantity. Avogadro's number, and its definition, was deprecated in favor of the Avogadro constant and its definition. Changes in the SI units are proposed to fix the value of the constant to exactly when it is expressed in the unit mol−1, in which an "X" at the end of a number means one or more final digits yet to be agreed upon.
History.
The Avogadro constant is named after the early 19th-century Italian scientist Amedeo Avogadro, who, in 1811, first proposed that the volume of a gas (at a given pressure and temperature) is proportional to the number of atoms or molecules regardless of the nature of the gas. The French physicist Jean Perrin in 1909 proposed naming the constant in honor of Avogadro. Perrin won the 1926 Nobel Prize in Physics, largely for his work in determining the Avogadro constant by several different methods.
The value of the Avogadro constant was first indicated by Johann Josef Loschmidt, who in 1865 estimated the average diameter of the molecules in air by a method that is equivalent to calculating the number of particles in a given volume of gas. This latter value, the number density formula_1 of particles in an ideal gas, is now called the Loschmidt constant in his honor, and is related to the Avogadro constant, "N"A, by
where "p"0 is the pressure, "R" is the gas constant and "T"0 is the absolute temperature. The connection with Loschmidt is the root of the symbol "L" sometimes used for the Avogadro constant, and German-language literature may refer to both constants by the same name, distinguished only by the units of measurement.
Accurate determinations of Avogadro's number require the measurement of a single quantity on both the atomic and macroscopic scales using the same unit of measurement. This became possible for the first time when American physicist Robert Millikan measured the charge on an electron in 1910. The electric charge per mole of electrons is a constant called the Faraday constant and had been known since 1834 when Michael Faraday published his works on electrolysis. By dividing the charge on a mole of electrons by the charge on a single electron the value of Avogadro's number is obtained. Since 1910, newer calculations have more accurately determined the values for the Faraday constant and the elementary charge. ("See below")
Perrin originally proposed the name Avogadro's number ("N") to refer to the number of molecules in one gram-molecule of oxygen (exactly of oxygen, according to the definitions of the period), and this term is still widely used, especially in introductory works. The change in name to "Avogadro constant" ("N"A) came with the introduction of the mole as a base unit in the International System of Units (SI) in 1971, which recognized amount of substance as an independent dimension of measurement. With this recognition, the Avogadro constant was no longer a pure number, but had a unit of measurement, the reciprocal mole (mol−1).
While it is rare to use units of amount of substance other than the mole, the Avogadro constant can also be expressed in units such as the pound mole (lb-mol) and the ounce mole (oz-mol).
General role in science.
Avogadro's constant is a scaling factor between macroscopic and microscopic (atomic scale) observations of nature. As such, it provides the relation between other physical constants and properties. For example, based on 2014 CODATA values, it establishes the following relationship between the gas constant "R" and the Boltzmann constant "k"B,
and the Faraday constant "F" and the elementary charge "e",
The Avogadro constant also enters into the definition of the unified atomic mass unit, u,
where "M"u is the molar mass constant.
Measurement.
Coulometry.
The earliest accurate method to measure the value of the Avogadro constant was based on coulometry. The principle is to measure the Faraday constant, "F", which is the electric charge carried by one mole of electrons, and to divide by the elementary charge, "e", to obtain the Avogadro constant.
The classic experiment is that of Bower and Davis at NIST, and relies on dissolving silver metal away from the anode of an electrolysis cell, while passing a constant electric current "I" for a known time "t". If "m" is the mass of silver lost from the anode and "A" the atomic weight of silver, then the Faraday constant is given by:
The NIST scientists devised a method to compensate for silver lost from the anode by mechanical causes, and conducted an isotope analysis of the silver used to determine its atomic weight. Their value for the conventional Faraday constant is "F" = , which corresponds to a value for the Avogadro constant of : both values have a relative standard uncertainty of .
Electron mass measurement.
The Committee on Data for Science and Technology (CODATA) publishes values for physical constants for international use. It determines the Avogadro constant from the ratio of the molar mass of the electron "A"("e")"M" to the rest mass of the electron "m":
The relative atomic mass of the electron, "A"("e"), is a directly-measured quantity, and the molar mass constant, "M", is a defined constant in the SI. The electron rest mass, however, is calculated from other measured constants:
As may be observed in the table below, the main limiting factor in the precision of the Avogadro constant is the uncertainty in the value of the Planck constant, as all the other constants that contribute to the calculation are known more precisely.
X-ray crystal density (XRCD) methods.
A modern method to determine the Avogadro constant is the use of X-ray crystallography. Silicon single crystals may be produced today in commercial facilities with extremely high purity and with few lattice defects. This method defines the Avogadro constant as the ratio of the molar volume, "V", to the atomic volume "V"atom:
The unit cell of silicon has a cubic packing arrangement of 8 atoms, and the unit cell volume may be measured by determining a single unit cell parameter, the length of one of the sides of the cube, "a".
In practice, measurements are carried out on a distance known as "d"(Si), which is the distance between the planes denoted by the Miller indices {220}, and is equal to "a"/√8. The 2006 CODATA value for "d"(Si) is , a relative uncertainty of , corresponding to a unit cell volume of .
The isotope proportional composition of the sample used must be measured and taken into account. Silicon occurs in three stable isotopes (28Si, 29Si, 30Si), and the natural variation in their proportions is greater than other uncertainties in the measurements. The atomic weight "A" for the sample crystal can be calculated, as the relative atomic masses of the three nuclides are known with great accuracy. This, together with the measured density "ρ" of the sample, allows the molar volume "V" to be determined:
where "M" is the molar mass constant. The 2006 CODATA value for the molar volume of silicon is 12.058 8349(11) cm3mol−1, with a relative standard uncertainty of .
As of the 2006 CODATA recommended values, the relative uncertainty in determinations of the Avogadro constant by the X-ray crystal density method is , about two and a half times higher than that of the electron mass method.
International Avogadro Coordination.
The International Avogadro Coordination (IAC), often simply called the "Avogadro project", is a collaboration begun in the early 1990s between various national metrology institutes to measure the Avogadro constant by the X-ray crystal density method to a relative uncertainty of 2 or less. The project is part of the efforts to redefine the kilogram in terms of a universal physical constant, rather than the International Prototype Kilogram, and complements the measurements of the Planck constant using watt balances. Under the current definitions of the International System of Units (SI), a measurement of the Avogadro constant is an indirect measurement of the Planck constant:
The measurements use highly polished spheres of silicon with a mass of one kilogram. Spheres are used to simplify the measurement of the size (and hence the density) and to minimize the effect of the oxide coating that inevitably forms on the surface. The first measurements used spheres of silicon with natural isotopic composition, and had a relative uncertainty of 3.1. These first results were also inconsistent with values of the Planck constant derived from watt balance measurements, although the source of the discrepancy is now believed to be known.
The main residual uncertainty in the early measurements was in the measurement of the isotopic composition of the silicon to calculate the atomic weight so, in 2007, a 4.8-kg single crystal of isotopically-enriched silicon (99.94% 28Si) was grown, and two one-kilogram spheres cut from it. Diameter measurements on the spheres are repeatable to within 0.3 nm, and the uncertainty in the mass is 3 µg. Full results from these determinations were expected in late 2010.
Their paper, published in January 2011, summarized the result of the International Avogadro Coordination and presented a measurement of the Avogadro constant to be mol−1.

</doc>
<doc id="41548" url="https://en.wikipedia.org/wiki?curid=41548" title="Phase-locked loop">
Phase-locked loop

A phase-locked loop or phase lock loop (PLL) is a control system that generates an output signal whose phase is related to the phase of an input signal. While there are several differing types, it is easy to initially visualize as an electronic circuit consisting of a variable frequency oscillator and a phase detector. The oscillator generates a periodic signal. The phase detector compares the phase of that signal with the phase of the input periodic signal and adjusts the oscillator to keep the phases matched. Bringing the output signal back toward the input signal for comparison is called a feedback loop since the output is "fed back" toward the input forming a loop.
Keeping the input and output phase in lock step also implies keeping the input and output frequencies the same. Consequently, in addition to synchronizing signals, a phase-locked loop can track an input frequency, or it can generate a frequency that is a multiple of the input frequency. These properties are used for computer clock synchronization, demodulation, and frequency synthesis.
Phase-locked loops are widely employed in radio, telecommunications, computers and other electronic applications. They can be used to demodulate a signal, recover a signal from a noisy communication channel, generate a stable frequency at multiples of an input frequency (frequency synthesis), or distribute precisely timed clock pulses in digital logic circuits such as microprocessors. Since a single integrated circuit can provide a complete phase-locked-loop building block, the technique is widely used in modern electronic devices, with output frequencies from a fraction of a hertz up to many gigahertz.
Practical analogies.
Automobile race analogy.
For a practical idea of what is going on, consider an auto race. There are many cars, and the driver of each of them wants to go around the track as fast as possible. Each lap corresponds to a complete cycle, and each car will complete dozens of laps per hour. The number of laps per hour (a speed) corresponds to an angular velocity (i.e. a frequency), but the number of laps (a distance) corresponds to a phase (and the conversion factor is the distance around the track loop). 
During most of the race, each car is on its own and the driver of the car is trying to beat the driver of every other car on the course, and the phase of each car varies freely. 
However, if there is an accident, a pace car comes out to set a safe speed. None of the race cars are permitted to pass the pace car (or the race cars in front of them), but each of the race cars wants to stay as close to the pace car as it can. While it is on the track, the pace car is a reference, and the race cars become phase-locked loops. Each driver will measure the phase difference (a distance in laps) between him and the pace car. If the driver is far away, he will increase his engine speed to close the gap. If he's too close to the pace car, he will slow down. The result is all the race cars lock on to the phase of the pace car. The cars travel around the track in a tight group that is a small fraction of a lap.
Clock analogy.
Phase can be proportional to time, so a phase difference can be a time difference. Clocks are, with varying degrees of accuracy, phase-locked (time-locked) to a master clock.
Left on its own, each clock will mark time at slightly different rates. A wall clock, for example, might be fast by a few seconds per hour compared to the reference clock at NIST. Over time, that time difference would become substantial.
To keep the wall clock in sync with the reference clock, each week the owner compares the time on his wall clock to a more accurate clock (a phase comparison), and he resets his clock. Left alone, the wall clock will continue to diverge from the reference clock at the same few seconds per hour rate.
Some clocks have a timing adjustment (a fast-slow control). When the owner compared his wall clock's time to the reference time, he noticed that his clock was too fast. Consequently, he could turn the timing adjust a small amount to make the clock run a little slower(frequency). If things work out right, his clock will be more accurate. Over a series of weekly adjustments, the wall clock's notion of a second would agree with the reference time (locked both in frequency and phase within the wall clock's stability).
An early electromechanical version of a phase-locked loop was used in 1921 in the Shortt-Synchronome clock.
History.
Spontaneous synchronization of weakly coupled pendulum clocks was noted by the Dutch physicist Christiaan Huygens as early as 1673. Around the turn of the 19th century, Lord Rayleigh observed synchronization of weakly coupled organ pipes and tuning forks. In 1919, W. H. Eccles and J. H. Vincent found that two electronic oscillators that had been tuned to oscillate at slightly different frequencies but that were coupled to a resonant circuit would soon oscillate at the same frequency. Automatic synchronization of electronic oscillators was described in 1923 by Edward Victor Appleton. 
Earliest research towards what was later named the phase-locked loop goes back to 1932, when British researchers developed an alternative to Edwin Armstrong's superheterodyne receiver, the Homodyne or direct-conversion receiver. In the homodyne or synchrodyne system, a local oscillator was tuned to the desired input frequency and multiplied with the input signal. The resulting output signal included the original modulation information. The intent was to develop an alternative receiver circuit that required fewer tuned circuits than the superheterodyne receiver. Since the local oscillator would rapidly drift in frequency, an automatic correction signal was applied to the oscillator, maintaining it in the same phase and frequency of the desired signal. The technique was described in 1932, in a paper by Henri de Bellescize, in the French journal "L'Onde Électrique".
In analog television receivers since at least the late 1930s, phase-locked-loop horizontal and vertical sweep circuits are locked to synchronization pulses in the broadcast signal.
When Signetics introduced a line of monolithic integrated circuits like the NE565 that were complete phase-locked loop systems on a chip in 1969, applications for the technique multiplied. A few years later RCA introduced the "CD4046" CMOS Micropower Phase-Locked Loop, which became a popular integrated circuit.
Structure and function.
Phase-locked loop mechanisms may be implemented as either analog or digital circuits. Both implementations use the same basic structure.
Both analog and digital PLL circuits include four basic elements:
Variations.
There are several variations of PLLs. Some terms that are used are analog phase-locked loop (APLL) also referred to as a linear phase-locked loop (LPLL), digital phase-locked loop (DPLL), all digital phase-locked loop (ADPLL), and software phase-locked loop (SPLL).
Applications.
Phase-locked loops are widely used for synchronization purposes; in space communications for coherent demodulation and threshold extension, bit synchronization, and symbol synchronization. Phase-locked loops can also be used to demodulate frequency-modulated signals. In radio transmitters, a PLL is used to synthesize new frequencies which are a multiple of a reference frequency, with the same stability as the reference frequency.
Other applications include:
Clock recovery.
Some data streams, especially high-speed serial data streams (such as the raw stream of data from the magnetic head of a disk drive), are sent without an accompanying clock. The receiver generates a clock from an approximate frequency reference, and then phase-aligns to the transitions in the data stream with a PLL. This process is referred to as clock recovery. In order for this scheme to work, the data stream must have a transition frequently enough to correct any drift in the PLL's oscillator. Typically, some sort of line code, such as 8b/10b encoding, is used to put a hard upper bound on the maximum time between transitions.
Deskewing.
If a clock is sent in parallel with data, that clock can be used to sample the data. Because the clock must be received and amplified before it can drive the flip-flops which sample the data, there will be a finite, and process-, temperature-, and voltage-dependent delay between the detected clock edge and the received data window. This delay limits the frequency at which data can be sent. One way of eliminating this delay is to include a deskew PLL on the receive side, so that the clock at each data flip-flop is phase-matched to the received clock. In that type of application, a special form of a PLL called a delay-locked loop (DLL) is frequently used.
Clock generation.
Many electronic systems include processors of various sorts that operate at hundreds of megahertz. Typically, the clocks supplied to these processors come from clock generator PLLs, which multiply a lower-frequency reference clock (usually 50 or 100 MHz) up to the operating frequency of the processor. The multiplication factor can be quite large in cases where the operating frequency is multiple gigahertz and the reference crystal is just tens or hundreds of megahertz.
Spread spectrum.
All electronic systems emit some unwanted radio frequency energy. Various regulatory agencies (such as the FCC in the United States) put limits on the emitted energy and any interference caused by it. The emitted noise generally appears at sharp spectral peaks (usually at the operating frequency of the device, and a few harmonics). A system designer can use a spread-spectrum PLL to reduce interference with high-Q receivers by spreading the energy over a larger portion of the spectrum. For example, by changing the operating frequency up and down by a small amount (about 1%), a device running at hundreds of megahertz can spread its interference evenly over a few megahertz of spectrum, which drastically reduces the amount of noise seen on broadcast FM radio channels, which have a bandwidth of several tens of kilohertz.
Clock distribution.
Typically, the reference clock enters the chip and drives a phase locked loop (PLL), which then drives the system's clock distribution. The clock distribution is usually balanced so that the clock arrives at every endpoint simultaneously. One of those endpoints is the PLL's feedback input. The function of the PLL is to compare the distributed clock to the incoming reference clock, and vary the phase and frequency of its output until the reference and feedback clocks are phase and frequency matched.
PLLs are ubiquitous—they tune clocks in systems several feet across, as well as clocks in small portions of individual chips. Sometimes the reference clock may not actually be a pure clock at all, but rather a data stream with enough transitions that the PLL is able to recover a regular clock from that stream. Sometimes the reference clock is the same frequency as the clock driven through the clock distribution, other times the distributed clock may be some rational multiple of the reference.
Jitter and noise reduction.
One desirable property of all PLLs is that the reference and feedback clock edges be brought into very close alignment. The average difference in time between the phases of the two signals when the PLL has achieved lock is called the static phase offset (also called the steady-state phase error). The variance between these phases is called tracking jitter. Ideally, the static phase offset should be zero, and the tracking jitter should be as low as possible.
Phase noise is another type of jitter observed in PLLs, and is caused by the oscillator itself and by elements used in the oscillator's frequency control circuit. Some technologies are known to perform better than others in this regard. The best digital PLLs are constructed with emitter-coupled logic (ECL) elements, at the expense of high power consumption. To keep phase noise low in PLL circuits, it is best to avoid saturating logic families such as transistor-transistor logic (TTL) or CMOS.
Another desirable property of all PLLs is that the phase and frequency of the generated clock be unaffected by rapid changes in the voltages of the power and ground supply lines, as well as the voltage of the substrate on which the PLL circuits are fabricated. This is called substrate and supply noise rejection. The higher the noise rejection, the better.
To further improve the phase noise of the output, an injection locked oscillator can be employed following the VCO in the PLL.
Frequency synthesis.
In digital wireless communication systems (GSM, CDMA etc.), PLLs are used to provide the local oscillator up-conversion during transmission and down-conversion during reception. In most cellular handsets this function has been largely integrated into a single integrated circuit to reduce the cost and size of the handset. However, due to the high performance required of base station terminals, the transmission and reception circuits are built with discrete components to achieve the levels of performance required. GSM local oscillator modules are typically built with a frequency synthesizer integrated circuit and discrete resonator VCOs.
Block diagram.
A phase detector compares two input signals and produces an error signal which is proportional to their phase difference. The error signal is then low-pass filtered and used to drive a VCO which creates an output phase. The output is fed through an optional divider back to the input of the system, producing a negative feedback loop. If the output phase drifts, the error signal will increase, driving the VCO phase in the opposite direction so as to reduce the error. Thus the output phase is locked to the phase at the other input. This input is called the reference.
Analog phase locked loops are generally built with an analog phase detector, low pass filter and VCO placed in a negative feedback configuration. A digital phase locked loop uses a digital phase detector; it may also have a divider in the feedback path or in the reference path, or both, in order to make the PLL's output signal frequency a rational multiple of the reference frequency. A non-integer multiple of the reference frequency can also be created by replacing the simple divide-by-"N" counter in the feedback path with a programmable pulse swallowing counter. This technique is usually referred to as a fractional-N synthesizer or fractional-N PLL.
The oscillator generates a periodic output signal. Assume that initially the oscillator is at nearly the same frequency as the reference signal. If the phase from the oscillator falls behind that of the reference, the phase detector changes the control voltage of the oscillator so that it speeds up. Likewise, if the phase creeps ahead of the reference, the phase detector changes the control voltage to slow down the oscillator. Since initially the oscillator may be far from the reference frequency, practical phase detectors may also respond to frequency differences, so as to increase the lock-in range of allowable inputs.
Depending on the application, either the output of the controlled oscillator, or the control signal to the oscillator, provides the useful output of the PLL system.
Elements.
Phase detector.
A phase detector (PD) generates a voltage, which represents the phase difference between two signals. In a PLL, the two inputs of the phase detector are the reference input and the feedback from the VCO. The PD output voltage is used to control the VCO such that the phase difference between the two inputs is held constant, making it a negative feedback system. There are several types of phase detectors in the two main categories of analog and digital.
Different types of phase detectors have different performance characteristics.
For instance, the frequency mixer produces harmonics that adds complexity in applications where spectral purity of the VCO signal is important. The resulting unwanted (spurious) sidebands, also called "reference spurs" can dominate the filter requirements and reduce the capture range well below and/or increase the lock time beyond the requirements. In these applications the more complex digital phase detectors are used which do not have as severe a reference spur component on their output. Also, when in lock, the steady-state phase difference at the inputs using this type of phase detector is near 90 degrees. The actual difference is determined by the DC loop gain.
A bang-bang charge pump phase detector must always have a dead band where the phases of inputs are close enough that the detector detects no phase error. For this reason, bang-bang phase detectors are associated with significant minimum peak-to-peak jitter, because of drift within the dead band. However these types, having outputs consisting of very narrow pulses at lock, are very useful for applications requiring very low VCO spurious outputs. The narrow pulses contain very little energy and are easy to filter out of the VCO control voltage. This results in low VCO control line ripple and therefore low FM sidebands on the VCO.
In PLL applications it is frequently required to know when the loop is out of lock. The more complex digital phase-frequency detectors usually have an output that allows a reliable indication of an out of lock condition.
Filter.
The block commonly called the PLL loop filter (usually a low pass filter) generally has two distinct functions.
The primary function is to determine loop dynamics, also called stability. This is how the loop responds to disturbances, such as changes in the reference frequency, changes of the feedback divider, or at startup. Common considerations are the range over which the loop can achieve lock (pull-in range, lock range or capture range), how fast the loop achieves lock (lock time, lock-up time or settling time) and damping behavior. Depending on the application, this may require one or more of the following: a simple proportion (gain or attenuation), an integral (low pass filter) and/or derivative (high pass filter). Loop parameters commonly examined for this are the loop's gain margin and phase margin. Common concepts in control theory including the PID controller are used to design this function.
The second common consideration is limiting the amount of reference frequency energy (ripple) appearing at the phase detector output that is then applied to the VCO control input. This frequency modulates the VCO and produces FM sidebands commonly called "reference spurs". The low pass characteristic of this block can be used to attenuate this energy, but at times a band reject "notch" may also be useful.
The design of this block can be dominated by either of these considerations, or can be a complex process juggling the interactions of the two. Typical trade-offs are: increasing the bandwidth usually degrades the stability or too much damping for better stability will reduce the speed and increase settling time. Often also the phase-noise is affected.
Oscillator.
All phase-locked loops employ an oscillator element with variable frequency capability. This can be an analog VCO either driven by analog circuitry in the case of an APLL or driven digitally through the use of a digital-to-analog converter as is the case for some DPLL designs. Pure digital oscillators such as a numerically controlled oscillator are used in ADPLLs.
Feedback path and optional divider.
PLLs may include a divider between the oscillator and the feedback input to the phase detector to produce a frequency synthesizer. A programmable divider is particularly useful in radio transmitter applications, since a large number of transmit frequencies can be produced from a single stable, accurate, but expensive, quartz crystal–controlled reference oscillator.
Some PLLs also include a divider between the reference clock and the reference input to the phase detector. If the divider in the feedback path divides by formula_1 and the reference input divider divides by formula_2, it allows the PLL to multiply the reference frequency by formula_3. It might seem simpler to just feed the PLL a lower frequency, but in some cases the reference frequency may be constrained by other issues, and then the reference divider is useful.
Frequency multiplication can also be attained by locking the VCO output to the "N"th harmonic of the reference signal. Instead of a simple phase detector, the design uses a harmonic mixer (sampling mixer). The harmonic mixer turns the reference signal into an impulse train that is rich in harmonics. The VCO output is coarse tuned to be close to one of those harmonics. Consequently, the desired harmonic mixer output (representing the difference between the "N" harmonic and the VCO output) falls within the loop filter passband.
It should also be noted that the feedback is not limited to a frequency divider. This element can be other elements such as a frequency multiplier, or a mixer. The multiplier will make the VCO output a sub-multiple (rather than a multiple) of the reference frequency. A mixer can translate the VCO frequency by a fixed offset. It may also be a combination of these. An example being a divider following a mixer; this allows the divider to operate at a much lower frequency than the VCO without a loss in loop gain.
Modeling.
Time domain model.
The equations governing a phase-locked loop with an analog multiplier
as the phase detector and linear filter may be derived as follows.
Let the input to the phase
detector be formula_4 and the output of the VCO is
formula_5 with phases formula_6 and
formula_7. Functions formula_8 and
formula_9 describe waveforms of signals. Then the
output of the phase detector formula_10 is given by
the VCO frequency is usually taken as a function of the VCO input
formula_12 as
where formula_14 is the "sensitivity" of the VCO and is
expressed in Hz / V;
formula_15 is a free-running frequency of VCO.
The loop filter can be described by system of linear differential equations
where formula_10 is an input of the filter,
formula_12 is an output of the filter, formula_19 is
formula_20-by-formula_20 matrix,
formula_22. formula_23 represents an initial state of
the filter. The star symbol is a conjugate transpose.
Hence the following system describes PLL
where formula_25 is an initial phase shift.
Phase domain model.
Consider the input of PLL formula_4 and VCO output
formula_5 are high frequency signals.
Then for any
piecewise differentiable formula_28-periodic functions
formula_8 and formula_9 there is a function
formula_31 such that the output formula_32 of Filter
in phase domain is asymptotically equal ( the difference formula_34 is small with respect to the frequencies) to the output of the Filter in time domain model. 
Here function formula_31 is a phase detector characteristic.
Denote by formula_36 the phase difference
Then the following dynamical system describes PLL behavior
Here formula_39; formula_40 is a frequency of reference oscillator (we assume that formula_15 is constant).
Example.
Consider sinusoidal signals
and simple one-pole RC circuit as a filter. The time-domain model takes the form
PD characteristics for this signals is equal to
Hence the phase domain model takes form
This system of equations is equivalent to the equation of mathematical pendulum
Linearized phase domain model.
Phase locked loops can also be analyzed as control systems by applying the Laplace transform. The loop response can be written as:
Where
The loop characteristics can be controlled by inserting different types of loop filters. The simplest filter is a one-pole RC circuit. The loop transfer function in this case is:
The loop response becomes:
This is the form of a classic harmonic oscillator. The denominator can be related to that of a second order system:
Where
For the one-pole RC filter,
The loop natural frequency is a measure of the response time of the loop, and the damping factor is a measure of the overshoot and ringing. Ideally, the natural frequency should be high and the damping factor should be near 0.707 (critical damping). With a single pole filter, it is not possible to control the loop frequency and damping factor independently. For the case of critical damping,
A slightly more effective filter, the lag-lead filter includes one pole and one zero. This can be realized with two resistors and one capacitor. The transfer function for this filter is
This filter has two time constants
Substituting above yields the following natural frequency and damping factor
The loop filter components can be calculated independently for a given natural frequency and damping factor
Real world loop filter design can be much more complex e.g. using higher order filters to reduce various types or source of phase noise. (See the D Banerjee ref below)
Implementing a digital phase-locked loop in software.
Digital phase locked loops can be implemented in hardware, using integrated circuits such as a CMOS 4046. However, with microcontrollers becoming faster, it may make sense to implement a phase locked loop in software for applications that do not require locking onto signals in the MHz range or faster, such as precisely controlling motor speeds. Software implementation has several advantages including easy customization of the feedback loop including changing the multiplication or division ratio between the signal being tracked and the output oscillator. Furthermore, a software implementation is useful to understand and experiment with. As an example of a phase-locked loop implemented using a phase frequency detector is presented in MATLAB, as this type of phase detector is robust and easy to implement. This example uses integer arithmetic rather than floating point, as such an example is likely more useful in practice.
In this example, an array tracksig is assumed to contain a reference signal to be tracked. The oscillator is implemented by a counter, with the most significant bit of the counter indicating the on/off status of the oscillator. This code simulates the two D-type flip-flops that comprise a phase-frequency comparator. When either the reference or signal has a positive edge, the corresponding flip-flop switches high. Once both reference and signal is high, both flip-flops are reset. Which flip-flop is high determines at that instant whether the reference or signal leads the other. The error signal is the difference between these two flip-flop values. The pole-zero filter is implemented by adding the error signal and its derivative to the filtered error signal. This in turn is integrated to find the oscillator frequency. 
In practice, one would likely insert other operations into the feedback of this phase-locked loop. For example, if the phase locked loop were to implement a frequency multiplier, the oscillator signal could be divided in frequency before it is compared to the reference signal.

</doc>
<doc id="41549" url="https://en.wikipedia.org/wiki?curid=41549" title="Phase noise">
Phase noise

In signal processing, phase noise is the frequency domain representation of rapid, short-term, random fluctuations in the phase of a waveform, caused by time domain instabilities ("jitter"). Generally speaking, radio frequency engineers speak of the phase noise of an oscillator, whereas digital system engineers work with the jitter of a clock.
Definitions.
Historically there have been two conflicting yet widely used definitions for phase noise. Some authors define phase noise to be the spectral density of a signal's phase only, while the other definition refers to the phase spectrum (which pairs up with the amplitude spectrum, see spectral density#Related concepts) resulting from the spectral estimation of the signal itself. Both definitions yield the same result at offset frequencies well removed from the carrier. At close-in offsets however, the two definitions differ.
The IEEE defines phase noise as where the "phase instability" is the one-sided spectral density of a signal's phase deviation. Although is a one-sided function, it represents "the double-sideband spectral density of phase fluctuation". The phase noise expression is pronounced "script ell of f".
Background.
An ideal oscillator would generate a pure sine wave. In the frequency domain, this would be represented as a single pair of Dirac delta functions (positive and negative conjugates) at the oscillator's frequency, i.e., all the signal's power is at a single frequency. All real oscillators have phase modulated noise components. The phase noise components spread the power of a signal to adjacent frequencies, resulting in noise sidebands. Oscillator phase noise often includes low frequency flicker noise and may include white noise.
Consider the following noise-free signal:
Phase noise is added to this signal by adding a stochastic process represented by φ to the signal as follows:
Phase noise is a type of cyclostationary noise and is closely related to jitter. A particularly important type of phase noise is that produced by oscillators.
Phase noise (ℒ("f")) is typically expressed in units of dBc/Hz, and it represents the noise power relative to the carrier contained in a 1 Hz bandwidth centered at a certain offsets from the carrier. For example, a certain signal may have a phase noise of -80 dBc/Hz at an offset of 10 kHz and -95 dBc/Hz at an offset of 100 kHz. Phase noise can be measured and expressed as single sideband or double sideband values, but as noted earlier, the IEEE has adopted the definition as one-half of the double sideband PSD.
Jitter conversions.
Phase noise is sometimes also measured and expressed as a power obtained by integrating ℒ("f") over a certain range of offset frequencies. For example, the phase noise may be -40 dBc integrated over the range of 1 kHz to 100 kHz. This Integrated phase noise (expressed in degrees) can be converted to jitter (expressed in seconds) using the following formula:
formula_1
In the absence of 1/f noise in a region where the phase noise displays a –20 dBc/decade slope, the
rms cycle jitter can be related to the phase noise by:
formula_2
Likewise:
formula_3
Measurement.
Phase noise can be measured using a spectrum analyzer if the phase noise of the device under test (DUT) is large with respect to the spectrum analyzer's local oscillator. Care should be taken that observed values are due to the measured signal and not the shape factor of the spectrum analyzer's filters. Spectrum analyzer based measurement can show the phase-noise power over many decades of frequency e.g. 1 Hz to 10 MHz. The slope with offset frequency in various offset frequency regions can provide clues as to the source of the noise, e.g. low frequency flicker noise decreasing at 30 dB per decade (=9 dB per octave).
Phase noise measurement systems are alternatives to spectrum analyzers. These systems may use internal and external references and allow measurement of both residual and additive noise. Additionally, these systems can make low-noise, close-to-the-carrier, measurements.
Spectral purity.
The sinewave output of an ideal oscillator is a single line in the frequency spectrum. Such perfect spectral purity is not achievable in a practical oscillator. Spreading of the spectrum line caused by phase noise must be minimised in the local oscillator for a superheterodyne receiver because it defeats the aim of restricting the receiver frequency range by filters in the IF (intermediate frequency) amplifier.

</doc>
<doc id="41550" url="https://en.wikipedia.org/wiki?curid=41550" title="Phase perturbation">
Phase perturbation

Phase perturbation is the shifting, from whatever cause, in the phase of an electronic signal. The shifting is often quite rapid, and may appear to be random or cyclic. The phase departure in phase perturbation usually is larger, but less rapid, than in phase jitter.
Phase perturbation may be expressed in degrees, with any cyclic component expressed in hertz.

</doc>
<doc id="41551" url="https://en.wikipedia.org/wiki?curid=41551" title="Phase-shift keying">
Phase-shift keying

Phase-shift keying (PSK) is a digital modulation scheme that conveys data by changing (modulating) the phase of a reference signal (the carrier wave). The signal is impressed into by varying the sine and cosine inputs at a precise time. It is widely used for wireless LANs, RFID and Bluetooth communication.
Any digital modulation scheme uses a finite number of distinct signals to represent digital data. PSK uses a finite number of phases, each assigned a unique pattern of binary digits. Usually, each phase encodes an equal number of bits. Each pattern of bits forms the symbol that is represented by the particular phase. The demodulator, which is designed specifically for the symbol-set used by the modulator, determines the phase of the received signal and maps it back to the symbol it represents, thus recovering the original data. This requires the receiver to be able to compare the phase of the received signal to a reference signal — such a system is termed coherent (and referred to as CPSK).
Alternatively, instead of operating with respect to a constant reference wave, the broadcast can operate with respect to itself. Changes in phase of a single broadcast waveform can be considered the significant items. In this system, the demodulator determines the changes in the phase of the received signal rather than the phase (relative to a reference wave) itself. Since this scheme depends on the difference between successive phases, it is termed differential phase-shift keying (DPSK). DPSK can be significantly simpler to implement than ordinary PSK, since there is no need for the demodulator to have a copy of the reference signal to determine the exact phase of the received signal (it is a non-coherent scheme). In exchange, it produces more erroneous demodulation.
Introduction.
There are three major classes of digital modulation techniques used for transmission of digitally represented data:
All convey data by changing some aspect of a base signal, the carrier wave (usually a sinusoid), in response to a data signal. In the case of PSK, the phase is changed to represent the data signal. There are two fundamental ways of utilizing the phase of a signal in this way:
A convenient method to represent PSK schemes is on a constellation diagram. This shows the points in the complex plane where, in this context, the real and imaginary axes are termed the in-phase and quadrature axes respectively due to their 90° separation. Such a representation on perpendicular axes lends itself to straightforward implementation. The amplitude of each point along the in-phase axis is used to modulate a cosine (or sine) wave and the amplitude along the quadrature axis to modulate a sine (or cosine) wave. By convention, in-phase modulates cosine and quadrature modulates sine.
In PSK, the constellation points chosen are usually positioned with uniform angular spacing around a circle. This gives maximum phase-separation between adjacent points and thus the best immunity to corruption. They are positioned on a circle so that they can all be transmitted with the same energy. In this way, the moduli of the complex numbers they represent will be the same and thus so will the amplitudes needed for the cosine and sine waves. Two common examples are "binary phase-shift keying" (BPSK) which uses two phases, and "quadrature phase-shift keying" (QPSK) which uses four phases, although any number of phases may be used. Since the data to be conveyed are usually binary, the PSK scheme is usually designed with the number of constellation points being a power of 2.
Definitions.
For determining error-rates mathematically, some definitions will be needed:
formula_9 will give the probability that a single sample taken from a random process with zero-mean and unit-variance Gaussian probability density function will be greater or equal to formula_10. It is a scaled form of the complementary Gaussian error function:
The error-rates quoted here are those in additive white Gaussian noise (AWGN). These error rates are lower than those computed in fading channels, hence, are a good theoretical benchmark to compare with.
Applications.
Owing to PSK's simplicity, particularly when compared with its competitor quadrature amplitude modulation, it is widely used in existing technologies.
The wireless LAN standard, IEEE 802.11b-1999, uses a variety of different PSKs depending on the data rate required. At the basic rate of 1 Mbit/s, it uses DBPSK (differential BPSK). To provide the extended rate of 2 Mbit/s, DQPSK is used. In reaching 5.5 Mbit/s and the full rate of 11 Mbit/s, QPSK is employed, but has to be coupled with complementary code keying. The higher-speed wireless LAN standard, IEEE 802.11g-2003, has eight data rates: 6, 9, 12, 18, 24, 36, 48 and 54 Mbit/s. The 6 and 9 Mbit/s modes use OFDM modulation where each sub-carrier is BPSK modulated. The 12 and 18 Mbit/s modes use OFDM with QPSK. The fastest four modes use OFDM with forms of quadrature amplitude modulation.
Because of its simplicity, BPSK is appropriate for low-cost passive transmitters, and is used in RFID standards such as ISO/IEC 14443 which has been adopted for biometric passports, credit cards such as American Express's ExpressPay, and many other applications.
Bluetooth 2 will use formula_12-DQPSK at its lower rate (2 Mbit/s) and 8-DPSK at its higher rate (3 Mbit/s) when the link between the two devices is sufficiently robust. Bluetooth 1 modulates with Gaussian minimum-shift keying, a binary scheme, so either modulation choice in version 2 will yield a higher data-rate. A similar technology, IEEE 802.15.4 (the wireless standard used by ZigBee) also relies on PSK using two frequency bands: 868–915 MHz with BPSK and at 2.4 GHz with OQPSK.
Both QPSK and 8PSK are widely used in satellite broadcasting. QPSK is still widely used in the streaming of SD satellite channels and some HD channels. High definition programming is delivered almost exclusively in 8PSK due to the higher bitrates of HD video and the high cost of satellite bandwidth. The DVB-S2 standard requires support for both QPSK and 8PSK. The chipsets used in new satellite set top boxes, such as Broadcom's 7000 series support 8PSK and are backward compatible with the older standard.
Historically, voice-band synchronous modems such as the Bell 201, 208, and 209 and the CCITT V.26, V.27, V.29, V.32, and V.34 used PSK.
Binary phase-shift keying (BPSK).
BPSK (also sometimes called PRK, phase reversal keying, or 2PSK) is the simplest form of phase shift keying (PSK). It uses two phases which are separated by 180° and so can also be termed 2-PSK. It does not particularly matter exactly where the constellation points are positioned, and in this figure they are shown on the real axis, at 0° and 180°. This modulation is the most robust of all the PSKs since it takes the highest level of noise or distortion to make the demodulator reach an incorrect decision. It is, however, only able to modulate at 1 bit/symbol (as seen in the figure) and so is unsuitable for high data-rate applications.
In the presence of an arbitrary phase-shift introduced by the communications channel, the demodulator is unable to tell which constellation point is which. As a result, the data is often differentially encoded prior to modulation.
BPSK is functionally equivalent to 2-QAM modulation.
Implementation.
The general form for BPSK follows the equation:
This yields two phases, 0 and π.
In the specific form, binary data is often conveyed with the following signals:
where "f""c" is the frequency of the carrier-wave.
Hence, the signal-space can be represented by the single basis function
where 1 is represented by formula_17 and 0 is represented by formula_18. This assignment is, of course, arbitrary.
This use of this basis function is shown at the end of the next section in a signal timing diagram. The topmost signal is a BPSK-modulated cosine wave that the BPSK modulator would produce. The bit-stream that causes this output is shown above the signal (the other parts of this figure are relevant only to QPSK).
Bit error rate.
The bit error rate (BER) of BPSK in AWGN can be calculated as:
Since there is only one bit per symbol, this is also the symbol error rate.
Quadrature phase-shift keying (QPSK).
Sometimes this is known as "quadriphase PSK", 4-PSK, or 4-QAM. (Although the root concepts of QPSK and 4-QAM are different, the resulting modulated radio waves are exactly the same.) QPSK uses four points on the constellation diagram, equispaced around a circle. With four phases, QPSK can encode two bits per symbol, shown in the diagram with Gray coding to minimize the bit error rate (BER) — sometimes misperceived as twice the BER of BPSK.
The mathematical analysis shows that QPSK can be used either to double the data rate compared with a BPSK system while maintaining the "same" bandwidth of the signal, or to "maintain the data-rate of BPSK" but halving the bandwidth needed. In this latter case, the BER of QPSK is "exactly the same" as the BER of BPSK - and deciding differently is a common confusion when considering or describing QPSK. The transmitted carrier can undergo numbers of phase changes.
Given that radio communication channels are allocated by agencies such as the Federal Communication Commission giving a prescribed (maximum) bandwidth, the advantage of QPSK over BPSK becomes evident: QPSK transmits twice the data rate in a given bandwidth compared to BPSK - at the same BER. The engineering penalty that is paid is that QPSK transmitters and receivers are more complicated than the ones for BPSK. However, with modern electronics technology, the penalty in cost is very moderate.
As with BPSK, there are phase ambiguity problems at the receiving end, and differentially encoded QPSK is often used in practice.
Implementation.
The implementation of QPSK is more general than that of BPSK and also indicates the implementation of higher-order PSK. Writing the symbols in the constellation diagram in terms of the sine and cosine waves used to transmit them:
This yields the four phases π/4, 3π/4, 5π/4 and 7π/4 as needed.
This results in a two-dimensional signal space with unit basis functions
The first basis function is used as the in-phase component of the signal and the second as the quadrature component of the signal.
Hence, the signal constellation consists of the signal-space 4 points
The factors of 1/2 indicate that the total power is split equally between the two carriers.
Comparing these basis functions with that for BPSK shows clearly how QPSK can be viewed as two independent BPSK signals. Note that the signal-space points for BPSK do not need to split the symbol (bit) energy over the two carriers in the scheme shown in the BPSK constellation diagram.
QPSK systems can be implemented in a number of ways. An illustration of the major components of the transmitter and receiver structure are shown below.
Bit error rate.
Although QPSK can be viewed as a quaternary modulation, it is easier to see it as two independently modulated quadrature carriers. With this interpretation, the even (or odd) bits are used to modulate the in-phase component of the carrier, while the odd (or even) bits are used to modulate the quadrature-phase component of the carrier. BPSK is used on both carriers and they can be independently demodulated.
As a result, the probability of bit-error for QPSK is the same as for BPSK:
However, in order to achieve the same bit-error probability as BPSK, QPSK uses twice the power (since two bits are transmitted simultaneously).
The symbol error rate is given by:
If the signal-to-noise ratio is high (as is necessary for practical QPSK systems) the probability of symbol error may be approximated:
The modulated signal is shown below for a short segment of a random binary data-stream. The two carrier waves are a cosine wave and a sine wave, as indicated by the signal-space analysis above. Here, the odd-numbered bits have been assigned to the in-phase component and the even-numbered bits to the quadrature component (taking the first bit as number 1). The total signal — the sum of the two components — is shown at the bottom. Jumps in phase can be seen as the PSK changes the phase on each component at the start of each bit-period. The topmost waveform alone matches the description given for BPSK above.
The binary data that is conveyed by this waveform is: 1 1 0 0 0 1 1 0.
Variants.
Offset QPSK (OQPSK).
"Offset quadrature phase-shift keying" ("OQPSK") is a variant of phase-shift keying modulation using 4 different values of the phase to transmit. It is sometimes called "Staggered quadrature phase-shift keying" ("SQPSK").
Taking four values of the phase (two bits) at a time to construct a QPSK symbol can allow the phase of the signal to jump by as much as 180° at a time. When the signal is low-pass filtered (as is typical in a transmitter), these phase-shifts result in large amplitude fluctuations, an undesirable quality in communication systems. By offsetting the timing of the odd and even bits by one bit-period, or half a symbol-period, the in-phase and quadrature components will never change at the same time. In the constellation diagram shown on the right, it can be seen that this will limit the phase-shift to no more than 90° at a time. This yields much lower amplitude fluctuations than non-offset QPSK and is sometimes preferred in practice.
The picture on the right shows the difference in the behavior of the phase between ordinary QPSK and OQPSK. It can be seen that in the first plot the phase can change by 180° at once, while in OQPSK the changes are never greater than 90°.
The modulated signal is shown below for a short segment of a random binary data-stream. Note the half symbol-period offset between the two component waves. The sudden phase-shifts occur about twice as often as for QPSK (since the signals no longer change together), but they are less severe. In other words, the magnitude of jumps is smaller in OQPSK when compared to QPSK.
"π" /4–QPSK.
This variant of QPSK uses two identical constellations which are rotated by 45° (formula_12 radians, hence the name) with respect to one another. Usually, either the even or odd symbols are used to select points from one of the constellations and the other symbols select points from the other constellation. This also reduces the phase-shifts from a maximum of 180°, but only to a maximum of 135° and so the amplitude fluctuations of formula_12–QPSK are between OQPSK and non-offset QPSK.
One property this modulation scheme possesses is that if the modulated signal is represented in the complex domain, it does not have any paths through the origin. In other words, the signal does not pass through the origin. This lowers the dynamical range of fluctuations in the signal which is desirable when engineering communications signals.
On the other hand, formula_12–QPSK lends itself to easy demodulation and has been adopted for use in, for example, TDMA cellular telephone systems.
The modulated signal is shown below for a short segment of a random binary data-stream. The construction is the same as above for ordinary QPSK. Successive symbols are taken from the two constellations shown in the diagram. Thus, the first symbol (1 1) is taken from the 'blue' constellation and the second symbol (0 0) is taken from the 'green' constellation. Note that magnitudes of the two component waves change as they switch between constellations, but the total signal's magnitude remains constant (constant envelope). The phase-shifts are between those of the two previous timing-diagrams.
SOQPSK.
The license-free shaped-offset QPSK (SOQPSK) is interoperable with Feher-patented QPSK (FQPSK), in the sense that an integrate-and-dump offset QPSK detector produces the same output no matter which kind of transmitter is used.
These modulations carefully shape the I and Q waveforms such that they change very smoothly, and the signal stays constant-amplitude even during signal transitions. (Rather than traveling instantly from one symbol to another, or even linearly, it travels smoothly around the constant-amplitude circle from one symbol to the next.)
The standard description of SOQPSK-TG involves ternary symbols.
DPQPSK.
Dual-polarization quadrature phase shift keying (DPQPSK) or dual-polarization QPSK - involves the polarization multiplexing of two different QPSK signals, thus improving the spectral efficiency by a factor of 2. This is a cost-effective alternative, to utilizing 16-PSK instead of QPSK to double the spectral efficiency.
Higher-order PSK.
Any number of phases may be used to construct a PSK constellation but 8-PSK is usually the highest order PSK constellation deployed. With more than 8 phases, the error-rate becomes too high and there are better, though more complex, modulations available such as quadrature amplitude modulation (QAM). Although any number of phases may be used, the fact that the constellation must usually deal with binary data means that the number of symbols is usually a power of 2 to allow an integer number of bits per symbol.
Bit error rate.
For the general formula_30-PSK there is no simple expression for the symbol-error probability if formula_31. Unfortunately, it can only be obtained from:
where
This may be approximated for high formula_30 and high formula_40 by:
The bit-error probability for formula_30-PSK can only be determined exactly once the bit-mapping is known. However, when Gray coding is used, the most probable error from one symbol to the next produces only a single bit-error and
The graph on the left compares the bit-error rates of BPSK, QPSK (which are the same, as noted above), 8-PSK and 16-PSK. It is seen that higher-order modulations exhibit higher error-rates; in exchange however they deliver a higher raw data-rate.
Bounds on the error rates of various digital modulation schemes can be computed with application of the union bound to the signal constellation.
Differential phase-shift keying (DPSK).
Differential encoding.
Differential phase shift keying (DPSK) is a common form of phase modulation that conveys data by changing the phase of the carrier wave. As mentioned for BPSK and QPSK there is an ambiguity of phase if the constellation is rotated by some effect in the communications channel through which the signal passes. This problem can be overcome by using the data to "change" rather than "set" the phase.
For example, in differentially encoded BPSK a binary '1' may be transmitted by adding 180° to the current phase and a binary '0' by adding 0° to the current phase. 
Another variant of DPSK is Symmetric Differential Phase Shift keying, SDPSK, where encoding would be +90° for a '1' and −90° for a '0'.
In differentially encoded QPSK (DQPSK), the phase-shifts are 0°, 90°, 180°, −90° corresponding to data '00', '01', '11', '10'. This kind of encoding may be demodulated in the same way as for non-differential PSK but the phase ambiguities can be ignored. Thus, each received symbol is demodulated to one of the formula_30 points in the constellation and a comparator then computes the difference in phase between this received signal and the preceding one. The difference encodes the data as described above. 
Symmetric Differential Quadrature Phase Shift Keying (SDQPSK) is like DQPSK, but encoding is symmetric, using phase shift values of −135°, −45°, +45° and +135°.
The modulated signal is shown below for both DBPSK and DQPSK as described above. In the figure, it is assumed that the "signal starts with zero phase", and so there is a phase shift in both signals at formula_45.
Analysis shows that differential encoding approximately doubles the error rate compared to ordinary formula_30-PSK but this may be overcome by only a small increase in formula_40. Furthermore, this analysis (and the graphical results below) are based on a system in which the only corruption is additive white Gaussian noise(AWGN). However, there will also be a physical channel between the transmitter and receiver in the communication system. This channel will, in general, introduce an unknown phase-shift to the PSK signal; in these cases the differential schemes can yield a "better" error-rate than the ordinary schemes which rely on precise phase information.
Demodulation.
For a signal that has been differentially encoded, there is an obvious alternative method of demodulation. Instead of demodulating as usual and ignoring carrier-phase ambiguity, the phase between two successive received symbols is compared and used to determine what the data must have been. When differential encoding is used in this manner, the scheme is known as differential phase-shift keying (DPSK). Note that this is subtly different from just differentially encoded PSK since, upon reception, the received symbols are "not" decoded one-by-one to constellation points but are instead compared directly to one another.
Call the received symbol in the formula_48th timeslot formula_49 and let it have phase formula_50. Assume without loss of generality that the phase of the carrier wave is zero. Denote the AWGN term as formula_51. Then
The decision variable for the formula_53th symbol and the formula_48th symbol is the phase difference between formula_49 and formula_56. That is, if formula_49 is projected onto formula_56, the decision is taken on the phase of the resultant complex number:
where superscript * denotes complex conjugation. In the absence of noise, the phase of this is formula_60, the phase-shift between the two received signals which can be used to determine the data transmitted.
The probability of error for DPSK is difficult to calculate in general, but, in the case of DBPSK it is:
which, when numerically evaluated, is only slightly worse than ordinary BPSK, particularly at higher formula_40 values.
Using DPSK avoids the need for possibly complex carrier-recovery schemes to provide an accurate phase estimate and can be an attractive alternative to ordinary PSK.
In optical communications, the data can be modulated onto the phase of a laser in a differential way. The modulation is a laser which emits a continuous wave, and a Mach-Zehnder modulator which receives electrical binary data. For the case of BPSK for example, the laser transmits the field unchanged for binary '1', and with reverse polarity for '0'. The demodulator consists of a delay line interferometer which delays one bit, so two bits can be compared at one time. In further processing, a photodiode is used to transform the optical field into an electric current, so the information is changed back into its original state.
The bit-error rates of DBPSK and DQPSK are compared to their non-differential counterparts in the graph to the right. The loss for using DBPSK is small enough compared to the complexity reduction that it is often used in communications systems that would otherwise use BPSK. For DQPSK though, the loss in performance compared to ordinary QPSK is larger and the system designer must balance this against the reduction in complexity.
Example: Differentially encoded BPSK.
At the formula_63 time-slot call the bit to be modulated formula_64, the differentially encoded bit formula_65 and the resulting modulated signal formula_66. Assume that the constellation diagram positions the symbols at ±1 (which is BPSK). The differential encoder produces:
where formula_68 indicates binary or modulo-2 addition.
So formula_65 only changes state (from binary '0' to binary '1' or from binary '1' to binary '0') if formula_64 is a binary '1'. Otherwise it remains in its previous state. This is the description of differentially encoded BPSK given above.
The received signal is demodulated to yield formula_71±1 and then the differential decoder reverses the encoding procedure and produces:
Therefore, formula_73 if formula_65 and formula_75 differ and formula_76 if they are the same. Hence, if both formula_65 and formula_75 are "inverted", formula_64 will still be decoded correctly. Thus, the 180° phase ambiguity does not matter.
Differential schemes for other PSK modulations may be devised along similar lines. The waveforms for DPSK are the same as for differentially encoded PSK given above since the only change between the two schemes is at the receiver.
The BER curve for this example is compared to ordinary BPSK on the right. As mentioned above, whilst the error-rate is approximately doubled, the increase needed in formula_40 to overcome this is small. The increase in formula_40 required to overcome differential modulation in coded systems, however, is larger - typically about 3 dB. The performance degradation is a result of noncoherent transmission - in this case it refers to the fact that tracking of the phase is completely ignored.
Channel capacity.
Like all M-ary modulation schemes with M = 2"b" symbols, when given exclusive access to a fixed bandwidth, the channel capacity of any phase shift keying modulation scheme rises to a maximum of "b" bits per symbol as the signal-to-noise ratio increases, due to the Shannon-Hartley Theorem.
References.
The notation and theoretical results in this article are based on material presented in the following sources:

</doc>
<doc id="41552" url="https://en.wikipedia.org/wiki?curid=41552" title="Phonetic alphabet">
Phonetic alphabet

Phonetic alphabet can mean:

</doc>
<doc id="41553" url="https://en.wikipedia.org/wiki?curid=41553" title="Photocurrent">
Photocurrent

Photocurrent is the electric current through a photosensitive device, such as a photodiode, as the result of exposure to radiant power.
The photocurrent may occur as a result of the photoelectric, photoemissive, or photovoltaic effect.
The photocurrent may be enhanced by internal gain caused by interaction among ions and photons under the influence of applied fields, such as occurs in an avalanche photodiode (APD).
When a suitable radiation is used, the photoelectric current

</doc>
<doc id="41555" url="https://en.wikipedia.org/wiki?curid=41555" title="Pilot">
Pilot

Pilot most commonly refers to:
The term may also refer to:

</doc>
<doc id="41556" url="https://en.wikipedia.org/wiki?curid=41556" title="PIN diode">
PIN diode

A PIN diode is a diode with a wide, undoped intrinsic semiconductor region between a p-type semiconductor and an n-type semiconductor region. The p-type and n-type regions are typically heavily doped because they are used for ohmic contacts.
The wide intrinsic region is in contrast to an ordinary PN diode. The wide intrinsic region makes the PIN diode an inferior rectifier (one typical function of a diode), but it makes the PIN diode suitable for attenuators, fast switches, photodetectors, and high voltage power electronics applications.
Operation.
A PIN diode operates under what is known as high-level injection. In other words, the intrinsic "i" region is flooded with charge carriers from the "p" and "n" regions. Its function can be likened to filling up a water bucket with a hole on the side. Once the water reaches the hole's level it will begin to pour out. Similarly, the diode will conduct current once the flooded electrons and holes reach an equilibrium point, where the number of electrons is equal to the number of holes in the intrinsic region. When the diode is forward biased, the injected carrier concentration is typically several orders of magnitude higher than the intrinsic level carrier concentration. Due to this high level injection, which in turn is due to the depletion process, the electric field extends deeply (almost the entire length) into the region. This electric field helps in speeding up of the transport of charge carriers from P to N region, which results in faster operation of the diode, making it a suitable device for high frequency operations.
Characteristics.
A PIN diode obeys the standard diode equation for low frequency signals. At higher frequencies, the diode looks like an almost perfect (very linear, even for large signals) resistor. There is a lot of stored charge in the intrinsic region. At low frequencies, the charge can be removed and the diode turns off. At higher frequencies, there is not enough time to remove the charge, so the diode never turns off. The PIN diode has a poor reverse recovery time.
The high-frequency resistance is inversely proportional to the DC bias current through the diode. A PIN diode, suitably biased, therefore acts as a variable resistor. This high-frequency resistance may vary over a wide range (from 0.1 ohm to 10 kΩ in some cases; the useful range is smaller, though).
The wide intrinsic region also means the diode will have a low capacitance when reverse biased.
In a PIN diode, the depletion region exists almost completely within the intrinsic region. This depletion region is much larger than in a PN diode, and almost constant-size, independent of the reverse bias applied to the diode. This increases the volume where electron-hole pairs can be generated by an incident photon. Some photodetector devices, such as PIN photodiodes and phototransistors (in which the base-collector junction is a PIN diode), use a PIN junction in their construction.
The diode design has some design tradeoffs. Increasing the dimensions of the intrinsic region (and its stored charge) allows the diode to look like a resistor at lower frequencies. It adversely affects the time needed to turn off the diode and its shunt capacitance. PIN diodes will be tailored for a particular use.
Applications.
PIN diodes are useful as RF switches, attenuators, photodetectors, and phase shifters.
RF and microwave switches.
Under zero or reverse bias, a PIN diode has a low capacitance. The low capacitance will not pass much of an RF signal. Under a forward bias of 1 mA, a typical PIN diode will have an RF resistance of about , making it a good RF conductor. Consequently, the PIN diode makes a good RF switch.
Although RF relays can be used as switches, they switch very slowly (on the order of ). A PIN diode switch can switch much more quickly (e.g., ).
The capacitance of an off discrete PIN diode might be . At , the reactance of is about . As a series element in a system, the off-state attenuation would be -20 times the base 10 log of the ratio of the load impedance to the sum of load, diode and source impedances, or roughly , which may not be adequate. In applications where higher isolation is needed, both shunt and series elements may be used, with the shunt diodes biased in complementary fashion to the series elements. Adding shunt elements effectively reduces the source and load impedances, reducing the impedance ratio and increasing the off-state attenuation. However, in addition to the added complexity, the on-state attenuation is increased due to the series resistance of the on-state blocking element and the capacitance of the off-state shunt elements.
PIN diode switches are used not only for signal selection, but they are also used for component selection. For example, some low phase noise oscillators use PIN diodes to range-switch inductors.
RF and microwave variable attenuators.
By changing the bias current through a PIN diode, it is possible to quickly change the RF resistance.
At high frequencies, the PIN diode appears as a resistor whose resistance is an inverse function of its forward current. Consequently, PIN diode can be used in some variable attenuator designs as amplitude modulators or output leveling circuits.
PIN diodes might be used, for example, as the bridge and shunt resistors in a bridged-T attenuator. Another common approach is to use PIN diodes as terminations connected to the 0 degree and -90 degree ports of a quadrature hybrid. The signal to be attenuated is applied to the input port, and the attenuated result is taken from the isolation port. The advantages of this approach over the bridged-T and pi approaches are (1) complementary PIN diode bias drives are not needed—the same bias is applied to both diodes—and (2) the loss in the attenuator equals the return loss of the terminations, which can be varied over a very wide range.
Limiters.
PIN diodes are sometimes used as input protection devices for high frequency test probes. If the input signal is within range, the PIN diode has little impact as a small capacitance. If the signal is large, then the PIN diode starts to conduct and becomes a resistor that shunts most of the signal to ground.
Photodetector and photovoltaic cell.
The PIN photodiode was invented by Jun-ichi Nishizawa and his colleagues in 1950.
PIN photodiodes are used in fibre optic network cards and switches. As a photodetector, the PIN diode is reverse biased. Under reverse bias, the diode ordinarily does not conduct (save a small dark current or Is leakage). When a photon of sufficient energy enters the depletion region of the diode, it creates an electron, hole pair. The reverse bias field sweeps the carriers out of the region creating a current. Some detectors can use avalanche multiplication.
The same mechanism applies to the PIN structure, or p-i-n junction, of a solar cell. In this case, the advantage of using a PIN structure over conventional semiconductor p–n junction is the better long wavelength response of the former. In case of long wavelength irradiation, photons penetrate deep into the cell. But only those electron-hole pairs generated in and near the depletion region contribute to current generation. The depletion region of a PIN structure extends across the intrinsic region, deep into the device. This wider depletion width enables electron-hole pair generation deep within the device. This increases the quantum efficiency of the cell.
Commercially available PIN photodiodes have quantum efficiencies above 80-90% in the telecom wavelength range (~1500 nm), and are typically made of germanium or InGaAs. They feature fast response times (higher than their p-n counterparts), running into several tens of gigahertz, making them ideal for high speed optical telecommunication applications. Similary, silicon p-i-n photodiodes have even higher quantum efficiencies, but can only detect wavelengths below the bandgap of silicon, i.e. ~1100 nm.
Typically, amorphous silicon thin-film cells use PIN structures. On the other hand, CdTe cells use NIP structure, a variation of the PIN structure. In a NIP structure, an intrinsic CdTe layer is sandwiched by n-doped CdS and p-doped ZnTe. The photons are incident on the n-doped layer unlike a PIN diode.
A PIN photodiode can also detect X-ray and gamma ray photons.
Example diodes.
SFH203 or BPW43 are cheap general purpose PIN diodes in 5 mm clear plastic case with bandwidth over
100 MHz. They are used in RONJA telecommunication systems and other circuitry applications.

</doc>
<doc id="41557" url="https://en.wikipedia.org/wiki?curid=41557" title="Planar array">
Planar array

In telecommunications and radar, a planar array is an antenna in which all of the elements, both active and parasitic, are in one plane. A planar array provides a large aperture and may be used for directional beam control by varying the relative phase of each element. A planar array may be used with a reflecting screen behind the active plane.
A planar array with a reflecting screen is related to a radar absorber. Both are supposed not to reflect incoming radiation of the desired wavelength. Radar absorbers have the advantage that they can use magnetic materials to avoid reflections at the interface to air at least for some frequencies. For antennas to be broadband, the transition from air (vacuum) to the metal of the screen has to be a gradual one. Radar absorbers have the advantage that they do not have to collect the received energy. The antenna needs a 3D tree of twin-leads and chokes to connect a single cable to a large number of micro antennas. Micro dipole antennas are only resonant at a single high frequency, to be broadband the arms of adjoining antennas have to be connected.

</doc>
<doc id="41559" url="https://en.wikipedia.org/wiki?curid=41559" title="Plane wave">
Plane wave

In the physics of wave propagation, a plane wave (also spelled planewave) is a field in space-time which takes the form
with an arbitrary (scalar or vector) function formula_2 and formula_3.
The plane wave propagates along the direction formula_4 with velocity "c".
The term is often used to denote the special case where the plane wave is both harmonic and homogeneous. 
A homogeneous and harmonic plane wave is a constant-frequency wave whose wavefronts (surfaces of constant phase) are infinite parallel planes of constant peak-to-peak amplitude normal to the phase velocity vector.
It is not possible in practice to have a true plane wave; only a plane wave of infinite extent will propagate as a plane wave. However, many waves are approximately plane waves in a localized region of space. For example, a localized source such as an antenna produces a field that is approximately a plane wave far from the antenna in its far-field region. Similarly, if the length scales are much longer than the wave’s wavelength, as is often the case for light in the field of optics, one can treat the waves as light rays which correspond locally to plane waves.
__TOC__
Mathematical formalisms.
Two functions that meet the above criteria of having a constant frequency (being harmonic) and constant amplitude are the sine and cosine functions. One of the simplest ways to use such a sinusoid involves defining it along the direction of the x-axis. The equation below, which is illustrated toward the right, uses the cosine function to represent a harmonic and homogeneous plane wave travelling in the positive x direction.
In the above equation:
Other formalisms which directly use the wave’s wavelength formula_18, period formula_19, frequency formula_20 and velocity formula_21 are below.
To appreciate the equivalence of the above set of equations note that formula_25 and formula_26
Arbitrary direction.
A more generalized form is used to describe a plane wave traveling in an arbitrary direction. It uses vectors in combination with the vector dot product.
here:
Complex exponential form.
Many choose to use a more mathematically versatile formulation that utilizes the complex number plane. It requires the use of the natural exponent formula_32 and the imaginary number formula_33.
To appreciate this equation’s relationship to the earlier ones, below is this same equation expressed using sines and cosines. Observe that the first term equals the real form of the plane wave just discussed.
The introduced complex form of the plane wave can be simplified by using a complex-valued amplitude formula_37 substitute the real valued amplitude formula_8. <br>
Specifically, since the complex form…
equals
one can absorb the phase factor formula_41 into a complex amplitude by letting formula_42, resulting in the more compact equation
While the complex form has an imaginary component, after the necessary calculations are performed in the complex plane, its real value can be extracted giving a real valued equation representing an actual plane wave.
The main reason one would choose to work with complex exponential form of plane waves is that complex exponentials are often algebraically easier to handle than the trigonometric sines and cosines. Specifically, the angle-addition rules are extremely simple for exponentials.
Additionally, when using Fourier analysis techniques for waves in a lossy medium, the resulting attenuation is easier to deal with using complex Fourier coefficients. It should be noted however that if a wave is traveling through a lossy medium, the amplitude of the wave is no longer constant, and therefore the wave is strictly speaking no longer a true plane wave.
In quantum mechanics the solutions of the Schrödinger wave equation are by their very nature complex and in the simplest instance take a form identical to the complex plane wave representation above. The imaginary component in that instance however has not been introduced for the purpose of mathematical expediency but is in fact an inherent part of the “wave”.
In special relativity, one can utilize an even more compact expression by using four-vectors.
Thus,
becomes 
Applications.
These waves are solutions for a scalar wave equation in a homogeneous medium. For vector wave equations, such as the ones describing electromagnetic radiation or waves in an elastic solid, the solution for a homogeneous medium is similar: the "scalar" amplitude "Ao" is replaced by a constant "vector" Ao. For example, in electromagnetism Ao is typically the vector for the electric field, magnetic field, or vector potential. A transverse wave is one in which the amplitude vector is orthogonal to k, which is the case for electromagnetic waves in an isotropic medium. By contrast, a longitudinal wave is one in which the amplitude vector is parallel to k, such as for acoustic waves in a gas or fluid.
The plane-wave equation works for arbitrary combinations of "ω" and k, but any real physical medium will only allow such waves to propagate for those combinations of "ω" and k that satisfy the dispersion relation of the medium. The dispersion relation is often expressed as a function, "ω"(k). The ratio "ω"/|k| gives the magnitude of the phase velocity and "dω"/"d"k gives the group velocity. For electromagnetism in an isotropic medium with index of refraction "n", the phase velocity is "c"/"n", which equals the group velocity if the index is not frequency-dependent.
In linear uniform media, a wave solution can be expressed as a superposition of plane waves. This approach is known as the Angular spectrum method. The form of the planewave solution is actually a general consequence of translational symmetry. More generally, for periodic structures having discrete translational symmetry, the solutions take the form of Bloch waves, most famously in crystalline atomic materials but also in photonic crystals and other periodic wave equations. As another generalization, for structures that are only uniform along one direction "x" (such as a waveguide along the "x" direction), the solutions (waveguide modes) are of the form exp["i"("kx"-"ωt")] multiplied by some amplitude function "a"("y","z"). This is a special case of a separable partial differential equation.
Polarized electromagnetic plane waves.
Represented in the first illustration toward the right is a linearly polarized, electromagnetic wave. Because this is a plane wave, each blue vector, indicating the perpendicular displacement from a point on the axis out to the sine wave, represents the magnitude and direction of the electric field for an entire plane that is perpendicular to the axis.
Represented in the second illustration is a circularly polarized, electromagnetic plane wave. Each blue vector indicating the perpendicular displacement from a point on the axis out to the helix, also represents the magnitude and direction of the electric field for an entire plane perpendicular to the axis.
In both illustrations, along the axes is a series of shorter blue vectors which are scaled down versions of the longer blue vectors. These shorter blue vectors are extrapolated out into the block of black vectors which fill a volume of space. Notice that for a given plane, the black vectors are identical, indicating that the magnitude and direction of the electric field is constant along that plane.
In the case of the linearly polarized light, the field strength from plane to plane varies from a maximum in one direction, down to zero, and then back up to a maximum in the opposite direction.
In the case of the circularly polarized light, the field strength remains constant from plane to plane but its direction steadily changes in a rotary type manner.
Not indicated in either illustration is the electric field’s corresponding magnetic field which is proportional in strength to the electric field at each point in space but is at a right angle to it. Illustrations of the magnetic field vectors would be virtually identical to these except all the vectors would be rotated 90 degrees about the axis of propagation so that they were perpendicular to both the direction of propagation and the electric field vector.
The ratio of the amplitudes of the electric and magnetic field components of a plane wave in free space is known as the free-space wave-impedance, equal to 376.730313 ohms.

</doc>
<doc id="41560" url="https://en.wikipedia.org/wiki?curid=41560" title="Plastic-clad silica fiber">
Plastic-clad silica fiber

In telecommunications and fiber optics, a plastic-clad silica fiber or polymer-clad silica fiber (PCS) is an optical fiber that has a silica-based core and a plastic cladding. The cladding of a PCS fiber should not be confused with the polymer overcoat of a conventional all-silica fiber. PCS fibers in general have significantly lower performance characteristics, particularly higher transmission losses and lower bandwidths, than all-glass fibers.
The main applications of plastic-clad silica fiber are industrial, medical or sensing applications where cores that are larger than those used in standard data communications fibers are advantageous.

</doc>
<doc id="41562" url="https://en.wikipedia.org/wiki?curid=41562" title="Ingrid Bergman">
Ingrid Bergman

Ingrid Bergman (; 29 August 191529 August 1982) was a Swedish actress who starred in a variety of European and American films. She won three Academy Awards, two Emmy Awards, four Golden Globe Awards, a BAFTA Award, and the Tony Award for Best Actress. She is best remembered for her roles as Ilsa Lund in "Casablanca" (1942) and as Alicia Huberman in "Notorious" (1946), an Alfred Hitchcock thriller starring Cary Grant.
Before becoming a star in American films, Bergman had been a leading actress in Swedish films. Her introduction to American audiences came with her starring role in the English-language remake of "Intermezzo" (1939). At her insistence, producer David O. Selznick agreed not to sign her to a contract – for four films rather than the then-standard seven-year period, also at her insistence – until after "Intermezzo" had been released.
Selznick's financial problems meant that Bergman was often loaned to other studios. Apart from "Casablanca", her performances from this period include Victor Fleming's remake of "Dr. Jekyll and Mr. Hyde" (1941), "For Whom the Bell Tolls" (1943), "Gaslight" (1944), and "The Bells of St. Mary's" (1945). Her last films for Selznick were Alfred Hitchcock's "Spellbound" (1945) and "Notorious" (1946). Her final film for Hitchcock was "Under Capricorn" (1949).
After a decade in American films, she starred in Roberto Rossellini's "Stromboli" (1950), following the revelation that she was having an extramarital affair with the director. The affair and then marriage with Rossellini created a scandal in the US that forced her to remain in Europe for several years, when she made a successful Hollywood return in "Anastasia" (1956), for which she won her second Academy Award. Many of her personal and film documents can be seen in the Wesleyan University Cinema Archives.
According to the "St. James Encyclopedia of Popular Culture", Bergman quickly became "the ideal of American womanhood" and a contender for Hollywood's greatest leading actress. In the United States, she is considered to have brought a "Nordic freshness and vitality" to the screen, along with exceptional beauty and intelligence; David O. Selznick once called her "the most completely conscientious actress" he had ever worked with. In 2007, the American Film Institute ranked Bergman as the of classic American cinema.
Early years: 1915–38.
Bergman, named after Princess Ingrid of Sweden, was born on 29 August 1915 in Stockholm, to a Swedish father, Justus Bergman, and his German wife, Frieda (née Adler) Bergman. When she was two years of age, her mother died. Her father, who was an artist and photographer, died when she was 13. In the years before he died, he wanted her to become an opera star, and had her take voice lessons for three years. But she always "knew from the beginning that she wanted to be an actress," sometimes wearing her mother's clothes and staging plays in her father's empty studio. Her father documented all her birthdays with a borrowed camera.
After his death, she was sent to live with an aunt, who died of heart disease only six months later. She then moved in with her Aunt Hulda and Uncle Otto, who had five children. Another 
aunt she visited, Elsa Adler, first told Ingrid, when she was 11, that her mother may have had "some Jewish blood," and that her father was aware of that fact long before they married. But her aunt also cautioned her about telling others about her possible ancestry as "there might be some difficult times coming". Biographer Aleksandra Ziolkowska-Boehm, however, notes that the claim of Jewish blood was likely an embellishment. After being forced to do an in-depth genealogical investigation, Bergman's maternal cousin found there to be no Jewish ancestry on Bergman's mother's side.
Later, she received a scholarship to the state-sponsored Royal Dramatic Theatre School, where Greta Garbo had some years earlier earned a similar scholarship. After several months she was given a part in a new play, "Ett Brott" ("A Crime"), written by Sigfrid Siwertz. Chandler notes that this was "totally against procedure" at the school, where girls were expected to complete three years of study before getting such acting roles.
During her first summer break, she was also hired by a Swedish film studio, which led to her leaving the Royal Dramatic Theatre after just one year, to work in films full-time. Her first film role after leaving the Royal Dramatic Theatre was a small part in 1935's "Munkbrogreven" (although she had previously been an extra in the 1932 film "Landskamp"). She went on to act in a dozen films in Sweden, including "En kvinnas ansikte," which was later remade as "A Woman's Face" with Joan Crawford, and one film in Germany, "Die vier Gesellen" ("The Four Companions") (1938).
Hollywood period: 1939–49.
"Intermezzo: A Love Story" (1939).
Bergman's first acting role in the United States came when Hollywood producer David O. Selznick brought her to America to star in "Intermezzo: A Love Story" (1939), an English language remake of her earlier Swedish film, "Intermezzo" (1939). Unable to speak English and uncertain about her acceptance by the American audience, she expected to complete this one film and return home to Sweden. Her husband, Dr. Petter Lindström, remained in Sweden with their daughter Pia (born 1938). In "Intermezzo", she played the role of a young piano accompanist opposite Leslie Howard as a famous violin virtuoso. She arrived in Los Angeles on 6 May 1939, and stayed at the Selznick home until she could find another residence. According to Selznick's son, Danny, who was a child at the time, his father had a few concerns about Ingrid: "She didn't speak English, she was too tall, her name sounded too German, and her eyebrows were too thick."
Bergman was soon accepted without having to modify her looks or name, despite some early suggestions by Selznick. "He let her have her way," notes a story in "Life" magazine. Selznick understood her fear of Hollywood make-up artists, who might turn her into someone she wouldn't recognize, and "instructed them to lay off". He was also aware that her natural good looks would compete successfully with Hollywood's "synthetic razzle-dazzle." During the following weeks, while "Intermezzo" was being filmed, Selznick was also filming "Gone with the Wind." In a letter to William Hebert, his publicity director, Selznick described a few of his early impressions of Bergman:
"Intermezzo" became an enormous success and as a result Bergman became a star. The film's director, Gregory Ratoff, said "She is sensational," as an actress. This was the "sentiment of the entire set," writes "Life," adding that workmen would go out of their way to do things for her, and the cast and crew "admired the quick, alert concentration she gave to direction and to her lines." Film historian David Thomson notes that this would become "the start of an astonishing impact on Hollywood and America" where her lack of makeup contributed to an "air of nobility." According to "Life," the impression that she left on Hollywood, after she returned to Sweden, was of a tall (5 ft. 9 in.) girl "with light brown hair and blue eyes who was painfully shy but friendly, with a warm, straight, quick smile." Selznick appreciated her uniqueness, and with his wife Irene, they remained important friends throughout her career.
"Casablanca" (1942).
After the onset of World War II, Bergman "felt guilty because she had so misjudged the situation in Germany" while she was there filming "Die vier Gesellen" (The Four Companions). According to one of her biographers, Charlotte Chandler (2007), she had at first considered the Nazis only a "temporary aberration, 'too foolish to be taken seriously.' She believed Germany would not start a war." Bergman felt that "The good people there would not permit it." Chandler adds, "Ingrid felt guilty all the rest of her life because when she was in Germany at the end of the war, she had been afraid to go with the others to witness the atrocities of the Nazi extermination camps."
After completing one last film in Sweden and appearing in three moderately successful films ("Adam Had Four Sons", "Rage in Heaven" and "Dr. Jekyll and Mr. Hyde", all 1941) in the United States, Bergman co-starred with Humphrey Bogart in the classic film "Casablanca" (1942), which remains her best-known role. In this film, she played the role of Ilsa, the beautiful Norwegian wife of Victor Laszlo, played by Paul Henreid, an "anti-Nazi underground hero" who is in Casablanca, a haven from the Nazis. Bergman did not consider "Casablanca" to be one of her favorite performances. "I made so many films which were more important, but the only one people ever want to talk about is that one with Bogart." In later years she stated, "I feel about "Casablanca" that it has a life of its own. There is something mystical about it. It seems to have filled a need, a need that was there before the film, a need that the film filled."
"For Whom the Bell Tolls" (1943).
After "Casablanca", with "Selznick's steady boosting," she played the part of Maria in "For Whom the Bell Tolls" (1943), which was also her first color film. For the role she received her first Academy Award nomination for Best Actress. The film was taken from Ernest Hemingway's novel of the same title. When the book was sold to Paramount Pictures, Hemingway stated that "Miss Bergman, and no one else should play the part." His opinion came from seeing her in her first American role, "Intermezzo," although he hadn't yet met her. A few weeks later, they did meet, and after studying her he said, "You "are" Maria!"
"Gaslight" (1944).
The following year, she won the Academy Award for Best Actress for "Gaslight" (1944), a film in which George Cukor directed her as a "wife driven close to madness" by co-star Charles Boyer. The film, according to Thomson, "was the peak of her Hollywood glory." Bergman next played a nun in "The Bells of St. Mary's" (1945) opposite Bing Crosby, for which she received her third consecutive nomination for Best Actress.
Hitchcock films.
Bergman starred in the Alfred Hitchcock films "Spellbound" (1945), "Notorious" (1946), and "Under Capricorn" (1949). "Under Capricorn" was made in England and is a costume drama set in Australia. Bergman was a student of the acting coach Michael Chekhov during the 1940s. Chekhov acted with Bergman in "Spellbound" and received his only Academy Award nomination for his performance.
"Joan of Arc" (1948).
Bergman received another Best Actress nomination for "Joan of Arc" (1948), an independent film based on the Maxwell Anderson play "Joan of Lorraine", produced by Walter Wanger, and initially released through RKO. Bergman had championed the role since her arrival in Hollywood, which was one of the reasons she had played it on the Broadway stage in Anderson's play. The film was not a big hit with the public, partly because of the scandal of Bergman's affair with Italian film director Roberto Rossellini, which broke while the film was still in theatres. Even worse, it received disastrous reviews, and although nominated for several Academy Awards, did not receive a Best Picture nomination. It was subsequently cut by 45 minutes, but restored to full length in 1998 and released in 2004 on DVD.
Between motion pictures, Bergman had appeared in the stage plays "Liliom", "Anna Christie", and "Joan of Lorraine". During a press conference in Washington, D.C. for the promotion of "Joan of Lorraine", she protested against racial segregation after seeing it first hand at the theater she was acting in. This led to a lot of publicity and some hate mail. Bergman went to Alaska during World War II to entertain US Army troops. Soon after the war ended, she also went to Europe for the same purpose, where she was able to see the devastation caused by the war. 
Italian period with Rossellini: 1949–57.
Bergman strongly admired two films by Italian director Roberto Rossellini that she had seen in the United States. In 1949, Bergman wrote to Rossellini, expressing this admiration and suggesting that she make a film with him. This led to her being cast in his film "Stromboli" (1950). During production, Bergman fell in love with Rossellini, and they began an affair. Bergman became pregnant with their son, Renato Roberto Ranaldo Giusto Giuseppe ("Robin") Rossellini (born 2 February 1950).
This affair caused a huge scandal in the United States, where it led to Bergman being denounced on the floor of the United States Senate. Ed Sullivan chose not to have her on his show, despite a poll indicating that the public wanted her to appear. However, Steve Allen, whose show was equally popular, did have her as a guest, later explaining "the danger of trying to judge artistic activity through the prism of one's personal life." Spoto notes that Bergman had, by virtue of her roles and screen persona, placed herself "above all that." She had played a nun in "The Bells of St. Mary's" (1945) and a virgin saint in "Joan of Arc" (1948). Bergman later said, "People saw me in "Joan of Arc" and declared me a saint. I'm not. I'm just a woman, another human being."
As a result of the scandal, Bergman returned to Italy, leaving her husband and daughter (Pia). She went through a publicized divorce and custody battle for their daughter. Bergman and Rossellini were married on 24 May 1950. In addition to Renato, they had twin daughters (born 18 June 1952): Isabella Rossellini, who became an actress and model, and Isotta Ingrid Rossellini, who became a professor of Italian literature. 
"Stromboli" and "neorealism".
Rossellini completed five films starring Bergman between 1949 and 1955: "Stromboli," "Europa '51," "Viaggio in Italia," "Giovanna d'Arco al rogo," and "La Paura" ("Fear").
Rossellini directed her in a brief segment of his 1953 documentary film, "Siamo donne" (We, the Women), which was devoted to film actresses. His biographer Peter Bondanella notes that problems with communication during their marriage may have inspired his films' central themes of "solitude, grace and spirituality in a world without moral values."
Rossellini's use of a Hollywood star in his typically "neorealist" films, in which he normally used non-professional actors, did provoke some negative reactions in certain circles. In Bergman's first film with Rossellini, her character was "defying audience expectations" in that the director preferred to work without a script, forcing Bergman to act "inspired by reality while she worked, a style which Bondanella calls 'a new cinema of psychological introspection'". Bergman was aware of Rossellini's directing style before filming, as the director had earlier written to her explaining that he worked from "a few basic ideas, developing them little by little" as a film progressed.
After separating from Rossellini, Bergman starred in Jean Renoir's "Elena and Her Men" ("Elena et les Hommes," 1956), a romantic comedy in which she played a Polish princess caught up in political intrigue. Although the film was not a success, her performance in it has since come to be regarded as one of her best.
Later years: 1957–82.
"Anastasia" (1956).
With her starring role in 1956's "Anastasia" (1956), Bergman made a triumphant return to the American screen and won the Academy Award for Best Actress for a second time. The award was accepted for her by her friend Cary Grant.
Bergman made her first post-scandal public appearance in Hollywood in the 1958 Academy Awards, when she was the presenter of the Academy Award for Best Picture. She was given a standing ovation, after being introduced by Cary Grant and walking out onto the stage to present the award. She continued to alternate between performances in American and European films for the rest of her career and also made occasional appearances in television dramas such as "The Turn of the Screw" (1959) for the "Ford Startime" TV series—for which she won the Emmy Award for Outstanding Single Performance by an Actress.
During this time, she performed in several stage plays. She married producer Lars Schmidt, a fellow Swede, on 21 December 1958. This marriage ended in divorce in 1975. Schmidt died on 18 October 2009. After a long hiatus, Bergman made the film "Cactus Flower" (1969), with Walter Matthau and Goldie Hawn.
In 1972, U.S. Senator Charles H. Percy entered an apology into the "Congressional Record" for the attack made on Bergman 22 years earlier by Edwin C. Johnson.
Bergman was the President of the Jury at the 1973 Cannes Film Festival.
"Murder on the Orient Express" (1974).
Bergman became one of the few actresses ever to receive three Oscars when she won her third (and first in the category of Best Supporting Actress) for her performance in "Murder on the Orient Express" (1974). Director Sidney Lumet offered Bergman the important part of Princess Dragomiroff, with which he felt she could win an Oscar. She insisted on playing the much smaller role of Greta Ohlsson, the old Swedish missionary. Lumet discussed Bergman's role:
Bergman could speak Swedish (her native language), German (her second language, learned from her German mother and in school), English (learned when brought over to the United States), Italian (learned while living in Italy) and French (her third language, learned in school). She acted in each of these languages at various times. Fellow actor John Gielgud, who had acted with her in "Murder on the Orient Express" and who had directed her in the play "The Constant Wife", playfully commented: "She speaks five languages and can't act in any of them." (This is from a Dorothy Parker quote which became a snowclone, "That woman speaks eighteen languages, and can't say No in any of them.")
Although known chiefly as a film star, Bergman strongly admired the great English stage actors and their craft. She had the opportunity to appear in London's West End, working with such stage stars as Michael Redgrave in "A Month in the Country" (1965), Sir John Gielgud in "The Constant Wife" (1973) and Wendy Hiller in "Waters of the Moon" (1977–78). 
"Autumn Sonata" (1978).
In 1978, Bergman played in Ingmar Bergman's "Autumn Sonata" ("Höstsonaten") for which she received her 7th Academy Award nomination. This was her final performance on the big screen. In the film, Bergman plays a celebrity pianist who travels to Norway to visit her neglected daughter, played by Liv Ullmann. The film was shot in Norway.
In 1979, Bergman hosted the AFI's Life Achievement Award Ceremony for Alfred Hitchcock.
"A Woman Called Golda" (1982) – her final role.
She was offered the starring role in a television mini-series, "A Woman Called Golda" (1982), about the late Israeli prime minister Golda Meir. It was to be her final acting role and she was honored posthumously with a second Emmy Award for Best Actress. Her daughter, Isabella, described Bergman's surprise at being offered the part and the producer trying to explain to her, "People believe you and trust you, and this is what I want, because Golda Meir had the trust of the people." Isabella adds, "Now "that" was interesting to Mother." She was also persuaded that Golda was a "grand-scale person," one that people would assume was much taller than she actually was. Chandler notes that the role "also had a special significance for her, as during World War II, Ingrid felt guilty because she had so misjudged the situation in Germany."
According to Chandler, "Ingrid's rapidly deteriorating health was a more serious problem. Insurance for Bergman was impossible. Not only did she have cancer, but it was spreading, and if anyone had known how bad it was, no one would have gone on with the project." After viewing the series on TV, Isabella commented,
Bergman was frequently ill during the filming although she rarely complained or showed it. Four months after the filming was completed, she died, on her 67th birthday. After her death her daughter Pia accepted her Emmy.
Personal life.
In 1937, at the age of 21, Bergman married dentist Petter Aron Lindström (later to become a neurosurgeon); the couple had a daughter, Friedel Pia Lindström (born 20 September 1938). After returning to the United States in 1940, she acted on Broadway before continuing to do films in Hollywood. The following year, her husband arrived from Sweden with daughter Pia. Lindström stayed in Rochester, New York, where he studied medicine and surgery at the University of Rochester. Bergman would travel to New York and stay at their small rented stucco house between films, her visits lasting from a few days to four months.
According to an article in "Life" magazine, the "doctor regards himself as the undisputed head of the family, an idea that Ingrid accepts cheerfully." He insisted she draw the line between her film and personal life, as he has a "professional dislike for being associated with the tinseled glamor of Hollywood." Lindström later moved to San Francisco, California, where he completed his internship at a private hospital, and they continued to spend time together when she could travel between filming.
Bergman returned to Europe after the scandalous publicity surrounding her affair with Italian director Roberto Rossellini during the filming of "Stromboli" in 1950. In the same month the film was released, she gave birth to a boy, Roberto Ingmar Rossellini (born 2 February 1950). A week after her son was born, she divorced Lindström and married Rossellini in Mexico. On 18 June 1952 she gave birth to the twin daughters Isotta Ingrid Rossellini and Isabella Rossellini. In 1957 she divorced Rossellini. The next year she married Lars Schmidt, a theatrical entrepreneur from a wealthy Swedish shipping family. That marriage lasted nearly two decades, until 1975 when they divorced. 
During her marriage with Lindström, Bergman had a brief affair with "Spellbound" costar Gregory Peck. Unlike the affair with Rossellini, that with Peck was kept private until he confessed it to Brad Darrach of "People" in an interview five years after Bergman's death. Peck said, “All I can say is that I had a real love for her (Bergman), and I think that’s where I ought to stop…. I was young. She was young. We were involved for weeks in close and intense work.”
Death and legacy.
Bergman died in 1982 on her 67th birthday in London, of breast cancer. Her body was cremated at Kensal Green Cemetery, London, and her ashes taken to Sweden. Most of them were scattered in the sea around the islet of Dannholmen off the fishing village of Fjällbacka in Bohuslän, on the west coast of Sweden, where she spent most of the summers from 1958 until her death in 1982. The rest were placed next to her parents' ashes in Norra Begravningsplatsen (Northern Cemetery), Stockholm, Sweden.
According to biographer Donald Spoto, she was "arguably the most international star in the history of entertainment." After her American film debut in the film "Intermezzo: A Love Story" (1939), co-starring Leslie Howard, Hollywood saw her as a unique actress who was completely natural in style and without need of makeup. Film critic James Agee wrote that she "not only bears a startling resemblance to an imaginable human being; she really knows how to act, in a blend of poetic grace with quiet realism."
According to film historian David Thomson, she "always strove to be a 'true' woman", and many filmgoers identified with her:
Writing about her first years in Hollywood, "Life" magazine stated that "All Bergman vehicles are blessed," and "they all go speedily and happily, with no temperament from the leading lady." She was "completely pleased" with her early career's management by David O. Selznick, who always found excellent dramatic roles for her to play, and equally satisfied with her salary, once saying, "I am an actress and I am interested in acting, not in making money." "Life" adds that "she has greater versatility than any actress on the American screen ... her roles have demanded an adaptability and sensitiveness of characterization to which few actresses could rise."
She continued her acting career while suffering from cancer for eight years, and won international honors for her final roles. "Her spirit triumphed with remarkable grace and courage," adds Spoto. Director George Cukor once summed up her contributions to the film media when he said to her, "Do you know what I especially love about you, Ingrid, my dear? I can sum it up as your naturalness. The camera loves your beauty, your acting, and your individuality. A star must have individuality. It makes you a great star. A great star."
For her contributions to the motion picture industry, Bergman has a star on the Hollywood Walk of Fame at 6759 Hollywood Blvd.
Woody Guthrie wrote the erotic song "Ingrid Bergman", which references Bergman's relationship with Roberto Rosselini on the film "Stromboli". It was never recorded by Guthrie but, when later found in the Woody Guthrie archives, it was set to music, and recorded, by Billy Bragg on the album "Mermaid Avenue".
In March 2015, a picture of Bergman photographed by David Seymour was chosen for the main poster for the 2015 Cannes Film Festival. A documentary titled "" was also screened at the festival.
Autobiography.
In 1980, Bergman's autobiography was published under the title "Ingrid Bergman: My Story." It was written with the help of Alan Burgess, and in it she discusses her childhood, her early career, her life during her time in Hollywood, the Rossellini scandal, and subsequent events. The book was written after her children warned her that she would only be known through rumors and interviews if she did not tell her own story. It was through this autobiography that her affair with Robert Capa became known.
Awards.
"Full Article: List of awards and nominations received by Ingrid Bergman"
Bergman won three Academy Awards for acting, two for Best Actress and one for Best Supporting Actress. She ranks tied for second place in terms of Oscars won, with Walter Brennan (all three for Best Supporting Actor), Jack Nicholson (two for Best Actor and one for Best Supporting Actor), Meryl Streep (two for Best Actress and one for Best Supporting Actress), and Daniel Day-Lewis (all three for Best Actor). Katharine Hepburn still holds the record with four (all four for Best Actress).

</doc>
<doc id="41563" url="https://en.wikipedia.org/wiki?curid=41563" title="Polarential telegraph system">
Polarential telegraph system

A polarential telegraph system is a direct-current telegraph system employing polar transmission in one direction and a form of differential duplex transmission in the other. 
Two types of polarential systems, known as types A and B, are in use. In half-duplex operation of a type A polarential system, the direct-current balance is independent of line resistance. In half-duplex operation of a type B polarential system, the direct current is substantially independent of the line leakage. Type A is better for cable loops where leakage is negligible but resistance varies with temperature. Type B is considered better for open wire where variable line leakage is frequent.

</doc>
<doc id="41564" url="https://en.wikipedia.org/wiki?curid=41564" title="Polarization (waves)">
Polarization (waves)

Polarization (also polarisation) is a property of waves that can oscillate with more than one orientation. Electromagnetic waves such as light exhibit polarization, as do some other types of wave, such as gravitational waves. Sound waves in a gas or liquid do not exhibit polarization, since the oscillation is always in the direction the wave travels.
In an electromagnetic wave, both the electric field and magnetic field are oscillating but in different directions; by convention the "polarization" of light refers to the polarization of the electric field. Light which can be approximated as a plane wave in free space or in an isotropic medium propagates as a transverse wave—both the electric and magnetic fields are perpendicular to the wave's direction of travel. The oscillation of these fields may be in a single direction (linear polarization), or the field may rotate at the optical frequency (circular or elliptical polarization). In that case the direction of the fields' rotation, and thus the specified polarization, may be either clockwise or counter clockwise; this is referred to as the wave's chirality or "handedness".
The most common optical materials (such as glass) are isotropic and simply preserve the polarization of a wave but do not differentiate between polarization states. However, there are important classes of materials classified as birefringent or optically active in which this is not the case and a wave's polarization will generally be modified or will affect propagation through it. A polarizer is an optical filter that transmits only one polarization.
Polarization is an important parameter in areas of science dealing with transverse wave propagation, such as optics, seismology, radio, and microwaves. Especially impacted are technologies such as lasers, wireless and optical fiber telecommunications, and radar.
Introduction.
Wave propagation and polarization.
Most sources of light are classified as incoherent and unpolarized (or only "partially polarized") because they consist of a random mixture of waves having different spatial characteristics, frequencies (wavelengths), phases, and polarization states. However, for understanding electromagnetic waves and polarization in particular, it is easiest to just consider coherent plane waves; these are sinusoidal waves of one particular direction (or wavevector), frequency, phase, and polarization state. Characterizing an optical system in relation to a plane wave with those given parameters can then be used to predict its response to a more general case, since a wave with any specified spatial structure can be decomposed into a combination of plane waves (its so-called angular spectrum). And incoherent states can be modeled stochastically as a weighted combination of such uncorrelated waves with some distribution of frequencies (its "spectrum"), phases, and polarizations.
Transverse electromagnetic waves.
Electromagnetic waves (such as light), traveling in free space or another homogeneous isotropic non-attenuating medium, are properly described as transverse waves, meaning that a plane wave's electric field vector E and magnetic field H are in directions perpendicular to (or "transverse" to) the direction of wave propagation; E and H are also perpendicular to each other. Considering a monochromatic plane wave of optical frequency "f" (light of vacuum wavelength λ has a frequency of "f = c/λ" where "c" is the speed of light), let us take the direction of propagation as the "z" axis. Being a transverse wave the E and H fields must then contain components only in the "x" and "y" directions whereas "Ez=Hz=0". Using complex (or phasor) notation, we understand the instantaneous physical electric and magnetic fields to be given by the real parts of the complex quantities occurring in the following equations. As a function of time "t" and spatial position "z" (since for a plane wave in the +"z" direction the fields have no dependence on "x" or "y") these complex fields can be written as:
and
where λ/"n" is the wavelength "in the medium" (whose refractive index is "n") and is the period of the wave. Here 
"ex", "ey", "hx", and "hy" are complex numbers.
In the second more compact form, as these equations are customarily expressed, these factors are described using the wavenumber formula_3 and angular frequency (or "radian frequency") formula_4. In a more general formulation with propagation "not" restricted to the "+z" direction, then the spatial dependence "kz" is replaced by formula_5 where formula_6 is called the wave vector, the magnitude of which is the wavenumber.
Thus the leading vectors e and h each contain up to two nonzero (complex) components describing the amplitude and phase of the wave's "x" and "y" polarization components (again, there can be no "z" polarization component for a transverse wave in the +"z" direction). For a given medium with a characteristic impedance formula_7, h is related to e by:
and
In a dielectric, "η" is real and has the value "η"0/"n", where "n" is the refractive index and "η"0 is the impedance of free space. The impedance will be complex in a conducting medium. Note that given that relationship, the dot product of E and H must be zero:
indicating that these vectors are orthogonal (at right angles to each other), as expected.
So knowing the propagation direction (+"z" in this case) and η, one can just as well specify the wave in terms of just "ex" and "ey" describing the electric field. The vector containing "ex" and "ey" (but without the "z" component which is necessarily zero for a transverse wave) is known as a Jones vector. In addition to specifying the polarization state of the wave, a general Jones vector also specifies the overall magnitude and phase of that wave. Specifically, the Intensity of the light wave is proportional to the sum of the squared magnitudes of the two electric field components:
however the wave's "state of polarization" is only dependent on the (complex) "ratio" of "ey" to "ex". So let us just consider waves whose "|ex|2 + |ey|2 = 1"; this happens to correspond to an intensity of about .00133 watts per square meter in free space (where formula_12 formula_13). And since the absolute phase of a wave is unimportant in discussing its polarization state, let us stipulate that the phase of "ex" is zero, in other words "ex" is a real number while "ey" may be complex. Under these restrictions, "ex" and "ey" can be represented as follows:
where the polarization state is now totally parameterized by the value of "Q" (such that -1 < "Q" < 1) and the relative phase formula_16. By convention when one speaks of a wave's "polarization," if not otherwise specified, reference is being made to the polarization of the electric field. The polarization of the magnetic field always follows that of the electric field but with a 90 degree rotation, as detailed above.
Non-transverse polarization.
In addition to transverse waves, there are many wave motions where the oscillation is not limited to directions perpendicular to the direction of propagation. These cases are beyond the scope of the current article which concentrates on transverse waves (such as most electromagnetic waves in bulk media), however one should be aware of cases where the polarization of a coherent wave cannot be described simply using a Jones vector, as we have just done.
Just considering electromagnetic waves, we note that the preceding discussion strictly applies to plane waves in a homogeneous isotropic non-attenuating medium, whereas in an anisotropic medium (such as birefringent crystals as discussed below) the electric or magnetic field may have longitudinal as well as transverse components. In those cases the electric displacement "D" and magnetic flux density "B" still obey the above geometry but due to anisotropy in the electric susceptibility (or in the magnetic permeability), now given by a tensor, the direction of "E" (or "H") may differ from that of "D" (or "B"). Even in isotropic media, so-called inhomogeneous waves can be launched into a medium whose refractive index has a significant imaginary part (or "extinction coefficient") such as metals; these fields are also not strictly transverse. Surface waves or waves propagating in a waveguide (such as an optical fiber) are generally "not" transverse waves, but might be described as an electric or magnetic transverse mode, or a hybrid mode.
Even in free space, longitudinal field components can be generated in focal regions, where the plane wave approximation breaks down. An extreme example is radially or tangentially polarized light, at the focus of which the electric or magnetic field respectively is "entirely" longitudinal (along the direction of propagation).
For longitudinal waves such as sound waves in fluids, the direction of oscillation is by definition along the direction of travel, so the issue of polarization is not normally even mentioned. On the other hand, sound waves in a bulk solid can be transverse as well as longitudinal, for a total of three polarization components. In this case, the transverse polarization is associated with the direction of the shear stress and displacement in directions perpendicular to the propagation direction, while the longitudinal polarization describes compression of the solid and vibration along the direction of propagation. The differential propagation of transverse and longitudinal polarizations is important in seismology.
Polarization state.
Polarization is best understood by initially considering only pure polarization states, and only a coherent sinusoidal wave at some optical frequency. The vector on the right might describe the oscillation of the electric field emitted by a single-mode laser (whose oscillation frequency would be typically 1015 times faster). The field oscillates in the "x-y" plane, along the page, with the wave propagating in the "z" direction, perpendicular to the page.
The first two diagrams below trace the electric field vector over a complete cycle for linear polarization at two different orientations; these are each considered a distinct "State Of Polarization" (SOP). Note that the linear polarization at 45° can also be viewed as the addition of a horizontally linearly polarized wave (as in the leftmost figure) and a vertically polarized wave of the same amplitude "in the same phase".
Now if one were to introduce a phase shift in between those horizontal and vertical polarization components, one would generally obtain elliptical polarization as is shown in the third figure. When the phase shift is exactly ±90°, then "circular polarization" is produced (fourth and fifth figures). Thus is circular polarization created in practice, starting with linearly polarized light and employing a quarter-wave plate to introduce such a phase shift. The result of two such phase-shifted components in causing a rotating electric field vector is depicted in the animation on the right. Note that circular or elliptical polarization can involve either a clockwise or counterclockwise rotation of the field. These correspond to distinct polarization states, such as the two circular polarizations shown above.
Of course the orientation of the "x" and "y" axes used in this description is arbitrary. The choice of such a coordinate system and viewing the polarization ellipse in terms of the "x" and "y" polarization components, corresponds to the definition of the Jones vector (below) in terms of those basis polarizations. One would typically choose axes to suit a particular problem such as "x" being in the plane of incidence. Since there are separate reflection coefficients for the linear polarizations in and orthogonal to the plane of incidence ("p" and "s" polarizations, see below), that choice greatly simplifies the calculation of a wave's reflection from a surface.
Moreover, one can use as basis functions "any" pair of orthogonal polarization states, not just linear polarizations. For instance, choosing right and left circular polarizations as basis functions simplifies the solution of problems involving circular birefringence (optical activity) or circular dichroism.
Polarization ellipse.
Consider a purely polarized monochromatic wave. If one were to plot the electric field vector over one cycle of oscillation, an ellipse would generally be obtained, as is shown in the figure, corresponding to a particular state of elliptical polarization. Note that linear polarization and circular polarization can be seen as special cases of elliptical polarization.
A polarization state can then be described in relation to the geometrical parameters of the ellipse, and its "handedness", that is, whether the rotation around the ellipse is clockwise or counter clockwise. One parameterization of the elliptical figure specifies the orientation angle ψ, defined as the angle between the major axis of the ellipse and the "x"-axis along with the ellipticity ε=a/b, the ratio of the ellipse's major to minor axis. (also known as the axial ratio). The ellipticity parameter is an alternative parameterization of an ellipse's eccentricity formula_17, or the ellipticity angle, χ = arctan b/a= arctan 1/ε as is shown in the figure. The angle χ is also significant in that the latitude (angle from the equator) of the polarization state as represented on the Poincaré sphere (see below) is equal to ±2χ. The special cases of linear and circular polarization correspond to an ellipticity ε of infinity and unity (or χ of zero and 45°) respectively.
Jones vector.
Full information on a completely polarized state is also provided by the amplitude and phase of oscillations in two components of the electric field vector in the plane of polarization. This representation was used above to show how different states of polarization are possible. The amplitude and phase information can be conveniently represented as a two-dimensional complex vector (the Jones vector):
Here formula_19 and formula_20 denote the amplitude of the wave in the two components of the electric field vector, while formula_21 and formula_22 represent the phases. The product of a Jones vector with a complex number of unit modulus gives a different Jones vector representing the same ellipse, and thus the same state of polarization. The physical electric field, as the real part of the Jones vector, would be altered but the polarization state itself is independent of absolute phase. The basis vectors used to represent the Jones vector need not represent linear polarization states (i.e. be real). In general any two orthogonal states can be used, where an orthogonal vector pair is formally defined as one having a zero inner product. A common choice is left and right circular polarizations, for example to model the different propagation of waves in two such components in circularly birefringent media (see below) or signal paths of coherent detectors sensitive to circular polarization.
Coordinate frame.
Regardless of whether polarization state is represented using geometric parameters or Jones vectors, implicit in the parameterization is the orientation of the coordinate frame. This permits a degree of freedom, namely rotation about the propagation direction. When considering light that is propagating parallel to the surface of the Earth, the terms "horizontal" and "vertical" polarization are often used, with the former being associated with the first component of the Jones vector, or zero azimuth angle. On the other hand, in astronomy the equatorial coordinate system is generally used instead, with the zero azimuth (or position angle, as it is more commonly called in astronomy to avoid confusion with the horizontal coordinate system) corresponding to due north.
"s" and "p" designations.
Another coordinate system frequently used relates to the plane made by the propagation direction and a vector perpendicular to the plane of a reflecting surface. This is known as the "plane of incidence". The component of the electric field parallel to this plane is termed "p-like" (parallel) and the component perpendicular to this plane is termed "s-like" (from "senkrecht", German for perpendicular). Polarized light with its electric field along the plane of incidence is thus denoted "p-polarized", while light whose electric field is normal to the plane of incidence is called "s-polarized". "P" polarization is commonly referred to as "transverse-magnetic" (TM), and has also been termed "pi-polarized" or "tangential plane polarized". "S" polarization is also called "transverse-electric" (TE), as well as "sigma-polarized" or "sagittal plane polarized".
Unpolarized and partially polarized light.
Definition.
Most common sources of visible light, including thermal (black body) radiation and fluorescence (but "not" lasers), produce light described as "incoherent". Radiation is produced independently by a large number of atoms or molecules whose emissions are uncorrelated and generally of random polarizations. In this case the light is said to be "unpolarized". This term is somewhat inexact, since at any instant of time at one location there is a definite direction to the electric and magnetic fields, however it implies that the polarization changes so quickly in time that it will not be measured or relevant to the outcome of an experiment. A so-called depolarizer acts on a polarized beam to create one which is actually "fully" polarized at every point, but in which the polarization varies so rapidly across the beam that it may be ignored in the intended applications.
Light is said to be "partially polarized" when there is more power in one polarization mode than another. At any particular wavelength, partially polarized light can be statistically described as the superposition of a completely unpolarized component, and a completely polarized one. One may then describe the light in terms of the degree of polarization, and the parameters of the polarized component. That polarized component can be described in terms of a Jones vector or polarization ellipse, as is detailed above. However, in order to also describe the degree of polarization, one normally employs Stokes parameters (see below) to specify a state of partial polarization.
Motivation.
The transmission of plane waves through a homogeneous medium are fully described in terms of Jones vectors and 2×2 Jones matrices. However, in practice there are cases in which all of the light cannot be viewed in such a simple manner due to spatial inhomogeneities or the presence of mutually incoherent waves. So-called depolarization, for instance, cannot be described using Jones matrices. For these cases it is usual instead to use a 4×4 matrix that acts upon the Stokes 4-vector. Such matrices were first used by Paul Soleillet in 1929, although they have come to be known as Mueller matrices. While every Jones matrix has a Mueller matrix, the reverse is not true. Mueller matrices are then used to describe the observed polarization effects of the scattering of waves from complex surfaces or ensembles of particles, as shall now be presented.
Coherency matrix.
The Jones vector perfectly describes the state of polarization "and phase" of a single monochromatic wave, representing a pure state of polarization as described above. However any mixture of waves of different polarizations (or even of different frequencies) do "not" correspond to a Jones vector. In so-called partially polarized radiation the fields are stochastic, and the variations and correlations between components of the electric field can only be described statistically. One such representation is the coherency matrix:
where angular brackets denote averaging over many wave cycles. Several variants of the coherency matrix have been proposed: the Wiener coherency matrix and the spectral coherency matrix of Richard Barakat measure the coherence of a spectral decomposition of the signal, while the Wolf coherency matrix averages over all time/frequencies.
The coherency matrix contains all second order statistical information about the polarization. This matrix can be decomposed into the sum of two idempotent matrices, corresponding to the eigenvectors of the coherency matrix, each representing a polarization state that is orthogonal to the other. An alternative decomposition is into completely polarized (zero determinant) and unpolarized (scaled identity matrix) components. In either case, the operation of summing the components corresponds to the incoherent superposition of waves from the two components. The latter case gives rise to the concept of the "degree of polarization"; i.e., the fraction of the total intensity contributed by the completely polarized component.
Stokes parameters.
The coherency matrix is not easy to visualize, and it is therefore common to describe incoherent or partially polarized radiation in terms of its total intensity ("I"), (fractional) degree of polarization ("p"), and the shape parameters of the polarization ellipse. An alternative and mathematically convenient description is given by the Stokes parameters, introduced by George Gabriel Stokes in 1852. The relationship of the Stokes parameters to intensity and polarization ellipse parameters is shown in the equations and figure below.
Here "Ip", 2ψ and 2χ are the spherical coordinates of the polarization state in the three-dimensional space of the last three Stokes parameters. Note the factors of two before ψ and χ corresponding respectively to the facts that any polarization ellipse is indistinguishable from one rotated by 180°, or one with the semi-axis lengths swapped accompanied by a 90° rotation. The Stokes parameters are sometimes denoted "I", "Q", "U" and "V".
Poincaré sphere.
Neglecting the first Stokes parameter "S"0 (or "I"), the three other Stokes parameters can be plotted directly in three dimensional Cartesian coordinates. For a given power in the polarized component given by:
the set of all polarization states are then mapped to points on the surface of the so-called "Poincaré sphere" (but of radius "P"), as shown in the accompanying diagram.
Often the total beam power is not of interest, in which case a normalized Stokes vector is used by dividing the Stokes vector by the total intensity "S"0: 
The normalized Stokes vector formula_32 then has unity power (formula_33) and the three significant Stokes parameters plotted in three dimensions will lie on the unity-radius Poincaré sphere for pure polarization states (where formula_34). Partially polarized states will lie "inside" the Poincaré sphere at a distance of formula_35 from the origin. When the non-polarized component is not of interest, the Stokes vector can be further normalized to obtain 
When plotted, that point will lie on the surface of the unity-radius Poincaré sphere and indicate the state of polarization of the polarized component.
Any two antipodal points on the Poincaré sphere refer to orthogonal polarization states. The overlap between any two polarization states is dependent solely on the distance between their locations along the sphere. This property, which can only be true when pure polarization states are mapped onto a sphere, is the motivation for the invention of the Poincaré sphere and the use of Stokes parameters which are thus plotted on (or beneath) it.
Implications for reflection and propagation.
Polarization in wave propagation.
In a vacuum, the components of the electric field propagate at the speed of light, so that the phase of the wave varies in space and time while the polarization state does not. That is, the electric field vector e of a plane wave in the +"z" direction follows:
where "k" is the wavenumber. As noted above, the instantaneous electric field is the real part of the product of the Jones vector times the phase factor formula_38. When an electromagnetic wave interacts with matter, its propagation is altered according to the material's (complex) index of refraction. When the real or imaginary part of that refractive index is dependent on the polarization state of a wave, properties known as birefringence and polarization dichroism (or diattenuation) respectively, then the polarization state of a wave will generally be altered.
In such media, an electromagnetic wave with any given state of polarization may be decomposed into two orthogonally polarized components that encounter different propagation constants. The effect of propagation over a given path on those two components is most easily characterized in the form of a complex 2×2 transformation matrix J known as a Jones matrix:
The Jones matrix due to passage through a transparent material is dependent on the propagation distance as well as the birefringence. The birefringence (as well as the average refractive index) will generally be dispersive, that is, it will vary as a function of optical frequency (wavelength). In the case of non-birefringent materials, however, the 2×2 Jones matrix is the identity matrix (multiplied by a scalar phase factor and attenuation factor), implying no change in polarization during propagation.
For propagation effects in two orthogonal modes, the Jones matrix can be written as
where "g"1 and "g"2 are complex numbers
describing the phase delay and possibly the amplitude attenuation due to propagation in each of the two polarization eigenmodes. T is a unitary matrix representing a change of basis from these propagation modes to the linear system used for the Jones vectors; in the case of linear birefringence or diattenuation the modes are themselves linear polarization states so T and T−1 can be omitted if the coordinate axes have been chosen appropriately.
Birefringence.
In media termed birefringent, in which the amplitudes are unchanged but a differential phase delay occurs, the Jones matrix is a unitary matrix: |"g"1| = |"g"2| = 1. Media termed diattenuating (or "dichroic" in the sense of polarization), in which only the amplitudes of the two polarizations are affected differentially, may be described using a Hermitian matrix (generally multiplied by a common phase factor). In fact, since "any" matrix may be written as the product of unitary and positive Hermitian matrices, light propagation through any sequence of polarization-dependent optical components can be written as the product of these two basic types of transformations.
In birefringent media there is no attenuation but two modes accrue a differential phase delay. Well known manifestations of linear birefringence (that is, in which the basis polarizations are orthogonal linear polarizations) appear in optical wave plates/retarders and many crystals. If linearly polarized light passes through a birefringent material, its state of polarization will generally change "unless" its polarization direction is identical to one of those basis polarizations. Since the phase shift, and thus the change in polarization state, is usually wavelength dependent, such objects viewed under white light in between two polarizers may give rise to colorful effects, as seen in the accompanying photograph.
Circular birefringence is also termed optical activity especially in chiral fluids, or Faraday rotation when due to the presence of a magnetic field along the direction of propagation. When linearly polarized light is passed through such an object, it will exit still linearly polarized but with the axis of polarization rotated. A combination of linear and circular birefringence will have as basis polarizations two orthogonal elliptical polarizations; the term "elliptical birefringence" however is rarely used.
One can visualize the case of linear birefringence (with two orthogonal linear propagation modes) with an incoming wave linearly polarized at a 45° angle to those modes. As a differential phase starts to accrue, the polarization becomes elliptical, eventually changing to purely circular polarization (90° phase difference), then to elliptical and eventually linear polarization (180° phase) perpendicular to the original polarization, then through circular again (270° phase), then elliptical with the original azimuth angle, and finally back to the original linearly polarized state (360° phase) where the cycle begins anew. In general the situation is more complicated and can be characterized as a rotation in the Poincaré sphere about the axis defined by the propagation modes. Examples for linear (blue), circular (red), and elliptical (yellow) birefringence are shown in the figure on the left. The total intensity and degree of polarization are unaffected. If the path length in the birefringent medium is sufficient, the two polarization components of a collimated beam (or ray) can exit the material with a positional offset, even though their final propagation directions will be the same (assuming the entrance face and exit face are parallel). This is commonly viewed using calcite crystals, which present the viewer with two slightly offset images, in opposite polarizations, of an object behind the crystal. It was this effect that provided the first discovery of polarization, by Erasmus Bartholinus in 1669.
Dichroism.
Media in which transmission of one polarization mode is preferentially reduced are called "dichroic" or "diattenuating". Like birefringence, diattenuation can be with respect to linear polarization modes (in a crystal) or circular polarization modes (usually in a liquid).
Devices that block nearly all of the radiation in one mode are known as "polarizing filters" or simply "polarizers". This corresponds to "g"2=0 in the above representation of the Jones matrix. The output of an ideal polarizer is a specific polarization state (usually linear polarization) with an amplitude equal to the input wave's original amplitude in that polarization mode. Power in the other polarization mode is eliminated. Thus if unpolarized light is passed through an ideal polarizer (where "g"1=1 and "g"2=0) exactly half of its initial power is retained. Practical polarizers, especially inexpensive sheet polarizers, have additional loss so that
"g"1 < 1. However, in many instances the more relevant figure of merit is the polarizer's degree of polarization or extinction ratio, which involve a comparison of "g"1 to "g"2. Since Jone's vectors refer to waves' amplitudes (rather than intensity), when illuminated by unpolarized light the remaining power in the unwanted polarization will be ("g"2/"g"1)2 of the power in the intended polarization.
Specular reflection.
In addition to birefringence and dichroism in extended media, polarization effects describable using Jones matrices can also occur at (reflective) interface between two materials of different refractive index. These effects are treated by the Fresnel equations. Part of the wave is transmitted and part is reflected; for a given material those proportions (and also the phase of reflection) are dependent on the angle of incidence and are different for the "s" and "p" polarizations. Therefore, the polarization state of reflected light (even if initially unpolarized) is generally changed.
Any light striking a surface at a special angle of incidence known as Brewster's angle, where the reflection coefficient for "p" polarization is zero, will be reflected with only the "s"-polarization remaining. This principle is employed in the so-called "pile of plates polarizer" (see figure) in which part of the "s" polarization is removed by reflection at each Brewster angle surface, leaving only the "p" polarization after transmission through many such surfaces. The generally smaller reflection coefficient of the "p" polarization is also the basis of polarized sunglasses; by blocking the "s" (horizontal) polarization, most of the glare due to reflection from a wet street, for instance, is removed.
In the important special case of reflection at normal incidence (not involving anisotropic materials) there is no particular "s" or "p" polarization. Both the "x" and "y" polarization components are reflected identically, and therefore the polarization of the reflected wave is identical to that of the incident wave. However, in the case of circular (or elliptical) polarization, the handedness of the polarization state is thereby reversed, since by convention this is specified relative to the direction of propagation. The circular rotation of the electric field around the "x-y" axes called "right-handed" for a wave in the "+z" direction is "left-handed" for a wave in the "-z" direction. But in the general case of reflection at a nonzero angle of incidence, no such generalization can be made. For instance, right-circularly polarized light reflected from a dielectric surface at a grazing angle, will still be right-handed (but elliptically) polarized. Linear polarized light reflected from a metal at non-normal incidence will generally become elliptically polarized. These cases are handled using Jones vectors acted upon by the different Fresnel coefficients for the "s" and "p" polarization components.
Measurement techniques involving polarization.
Some optical measurement techniques are based on polarization. In many other optical techniques polarization is crucial or at least must be taken into account and controlled; such examples are too numerous to mention.
Ellipsometry.
Ellipsometry is a powerful technique for the measurement of the optical properties of a uniform surface. It involves measuring the polarization state of light following specular reflection from such a surface. This is typically done as a function of incidence angle or wavelength (or both). Since ellipsometry relies on reflection, it is not required for the sample to be transparent to light or for its back side to be accessible.
Ellipsometry can be used to model the (complex) refractive index of a surface of a bulk material. It is also very useful in determining parameters of one or more thin film layers deposited on a substrate. Due to their reflection properties, not only are the predicted magnitude of the "p" and "s" polarization components, but their relative phase shifts upon reflection, compared to measurements using an ellipsometer. A normal ellipsometer does not measure the actual reflection coefficient (which requires careful photometric calibration of the illuminating beam) but the ratio of the "p" and "s" reflections, as well as change of polarization ellipticity (hence the name) induced upon reflection by the surface being studied. In addition to use in science and research, ellipsometers are used in situ to control production processes for instance.
Geology.
The property of (linear) birefringence is widespread in crystalline minerals, and indeed was pivotal in the initial discovery of polarization. In mineralogy, this property is frequently exploited using polarization microscopes, for the purpose of identifying minerals. See optical mineralogy for more details.
Sound waves in solid materials exhibit polarization. Differential propagation of the three polarizations through the earth is a crucial in the field of seismology. Horizontally and vertically polarized seismic waves (shear waves)are termed SH and SV, while waves with longitudinal polarization (compressional waves) are termed P-waves.
Chemistry.
We have seen (above) that the birefringence of a type of crystal is useful in identifying it, and thus detection of linear birefringence is especially useful in geology and mineralogy. Linearly polarized light generally has its polarization state altered upon transmission through such a crystal, making it stand out when viewed in between two crossed polarizers, as seen in the photograph, above. Likewise, in chemistry, rotation of polarization axes in a liquid solution can be a useful measurement. In a liquid, linear birefringence is impossible, however there may be circular birefringence when a chiral molecule is in solution. When the right and left handed enantiomers of such a molecule are present in equal numbers (a so-called racemic mixture) then their effects cancel out. However, when there is only one (or a preponderance of one), as is more often the case for organic molecules, a net circular birefringence (or "optical activity") is observed, revealing the magnitude of that imbalance (or the concentration of the molecule itself, when it can be assumed that only one enantiomer is present). This is measured using a polarimeter in which polarized light is passed through a tube of the liquid, at the end of which is another polarizer which is rotated in order to null the transmission of light through it.
Astronomy.
In many areas of astronomy, the study of polarized electromagnetic radiation from outer space is of great importance. Although not usually a factor in the thermal radiation of stars, polarization is also present in radiation from coherent astronomical sources (e.g. hydroxyl or methanol masers), and incoherent sources such as the large radio lobes in active galaxies, and pulsar radio radiation (which may, it is speculated, sometimes be coherent), and is also imposed upon starlight by scattering from interstellar dust. Apart from providing information on sources of radiation and scattering, polarization also probes the interstellar magnetic field via Faraday rotation. The polarization of the cosmic microwave background is being used to study the physics of the very early universe. Synchrotron radiation is inherently polarised. It has been suggested that astronomical sources caused the chirality of biological molecules on Earth.
Applications and examples.
Polarized sunglasses.
Unpolarized light, after reflection at a specular (shiny) surface, generally obtains a degree of polarization. This phenomenon was observed in 1808 by the mathematician Étienne-Louis Malus after whom Malus' law is named. Polarizing sunglasses exploit this effect to reduce glare from reflections by horizontal surfaces, notably the road ahead viewed at a grazing angle.
Wearers of polarized sunglasses will occasionally observe inadvertent polarization effects such as color-dependent birefringent effects, for example in toughened glass (e.g., car windows) or items made from transparent plastics, in conjunction with natural polarization by reflection or scattering. The polarized light from LCD monitors (see below) is very conspicuous when these are worn.
Sky polarization and photography.
Polarization is observed in the light of the sky, as this is due to sunlight scattered by aerosols as it passes through the earth's atmosphere. The scattered light produces the brightness and color in clear skies. This partial polarization of scattered light can be used to darken the sky in photographs, increasing the contrast. This effect is most strongly observed at points on the sky making a 90° angle to the sun. Polarizing filters use these effects to optimize the results of photographing scenes in which reflection or scattering by the sky is involved.
Sky polarization has been used for orientation in navigation. The "sky compass", was used in the 1950s when navigating near the poles of the Earth's magnetic field when neither the sun nor stars were visible (e.g., under daytime cloud or twilight). It has been suggested, controversially, that the Vikings exploited a similar device (the "sunstone") in their extensive expeditions across the North Atlantic in the 9th–11th centuries, before the arrival of the magnetic compass in Europe in the 12th century. Related to the sky compass is the "polar clock", invented by Charles Wheatstone in the late 19th century.
Display technologies.
The principle of liquid-crystal display (LCD) technology relies on the rotation of the axis of linear polarization by the liquid crystal array. Light from the backlight (or the back reflective layer, in devices not including or requiring a backlight) first passes through a linear polarizing sheet. That polarized light passes through the actual liquid crystal layer which may be organized in pixels (for a TV or computer monitor) or in another format such as a seven-segment display or one with custom symbols for a particular product. The liquid crystal layer is produced with a consistent right (or left) handed chirality, essentially consisting of tiny helices. This causes circular birefringence, and is engineered so that there is a 90 degree rotation of the linear polarization state. However, when a voltage is applied across a cell, the molecules straighten out, lessening or totally losing the circular birefringence. On the viewing side of the display is another linear polarizing sheet, usually oriented at 90 degrees from the one behind the active layer. Therefore, when the circular birefringence is removed by the application of a sufficient voltage, the polarization of the transmitted light remains at right angles to the front polarizer, and the pixel appears dark. With no voltage, however, the 90 degree rotation of the polarization causes it to exactly match the axis of the front polarizer, allowing the light through. Intermediate voltages create intermediate rotation of the polarization axis and the pixel has an intermediate intensity. Displays based on this principle are widespread, and now are used in the vast majority of televisions, computer monitors and video projectors, rendering the previous CRT technology essentially obsolete. The use of polarization in the operation of LCD displays is immediately apparent to someone wearing polarized sunglasses, often making the display unreadable.
In a totally different sense, polarization encoding has become the leading (but not sole) method for delivering separate images to the left and right eye in stereoscopic displays used for 3D movies. This involves separate images intended for each eye either projected from two different projectors with orthogonally oriented polarizing filters or, more typically, from a single projector with time multiplexed polarization (a fast alternating polarization device for successive frames). Polarized 3D glasses with suitable polarizing filters ensure that each eye receives only the intended image. Historically such systems used linear polarization encoding because it was inexpensive and offered good separation. However circular polarization makes separation of the two images insensitive to tilting of the head, and is widely used in 3-D movie exhibition today, such as the system from RealD. Projecting such images requires screens that maintain the polarization of the projected light when viewed in reflection (such as silver screens); a normal diffuse white projection screen causes depolarization of the projected images, making it unsuitable for this application.
Although now obsolete, CRT computer displays suffered from reflection by the glass envelope, causing glare from room lights and consequently poor contrast. Several anti-reflection solutions were employed to ameliorate this problem. One solution utilized the principle of reflection of circularly polarized light. A circular polarizing filter in front of the screen allows for the transmission of (say) only right circularly polarized room light. Now, right circularly polarized light (depending on the convention used) has its electric (and magnetic) field direction rotating clockwise while propagating in the +z direction. Upon reflection, the field still has the same direction of rotation, but now propagation is in the −z direction making the reflected wave "left" circularly polarized. With the right circular polarization filter placed in front of the reflecting glass, the unwanted light reflected from the glass will thus be in very polarization state that is "blocked" by that filter, eliminating the reflection problem. The reversal of circular polarization on reflection and elimination of reflections in this manner can be easily observed by looking in a mirror while wearing 3-D movie glasses which employ left and right handed circular polarization in the two lenses. Closing one eye, the other eye will see a reflection in which it cannot see itself; that lens appears black. However the other lens (of the closed eye) will have the correct circular polarization allowing the closed eye to be easily seen by the open one.
Radio transmission and reception.
All radio (and microwave) antennas used for transmitting or receiving are intrinsically polarized. They transmit in (or receive signals from) a particular polarization, being totally insensitive to the opposite polarization; in certain cases that polarization is a function of direction. Most antennas are nominally linearly polarized, but elliptical and circular polarization is a possibility. As is the convention in optics, the "polarization" of a radio wave is understood to refer to the polarization of its electric field, with the magnetic field being at a 90 degree rotation with respect to it for a linearly polarized wave.
The vast majority of antennas are linearly polarized. In fact it can be shown from considerations of symmetry that an antenna that lies entirely in a plane which also includes the observer, can "only" have its polarization in the direction of that plane. This applies to many cases, allowing one to easily infer such an antenna's polarization at an intended direction of propagation. So a typical rooftop Yagi or log-periodic antenna with horizontal conductors, as viewed from a second station toward the horizon, is necessarily horizontally polarized. But a vertical "whip antenna" or AM broadcast tower used as an antenna element (again, for observers horizontally displaced from it) will transmit in the vertical polarization. A turnstile antenna with its four arms in the horizontal plane, likewise transmits horizontally polarized radiation toward the horizon. However, when that same turnstile antenna is used in the "axial mode" (upwards, for the same horizontally-oriented structure) its radiation is circularly polarized. At intermediate elevations it is elliptically polarized.
Polarization is important in radio communications because, for instance, if one attempts to use a horizontally polarized antenna to receive a vertically polarized transmission, the signal strength will be substantially reduced (or under very controlled conditions, reduced to nothing). This principle is used in satellite television in order to double the channel capacity over a fixed frequency band. The same frequency channel can be used for two signals broadcast in opposite polarizations. By adjusting the receiving antenna for one or the other polarization, either signal can be selected without interference from the other.
Especially due to the presence of the ground, there are some differences in propagation (and also in reflections responsible for TV ghosting) between horizontal and vertical polarizations. AM and FM broadcast radio usually use vertical polarization, while television uses horizontal polarization. At low frequencies especially, horizontal polarization is avoided. That is because the phase of a horizontally polarized wave is reversed upon reflection by the ground. A distant station in the horizontal direction will receive both the direct and reflected wave, which thus tend to cancel each other. This problem is avoided with vertical polarization. Polarization is also important in the transmission of radar pulses and reception of radar reflections by the same or a different antenna. For instance, back scattering of radar pulses by rain drops can be avoided by using circular polarization. Just as specular reflection of circularly polarized light reverses the handedness of the polarization, as discussed above, the same principle applies to scattering by objects much smaller than a wavelength such as rain drops. On the other hand, reflection of that wave by an irregular metal object (such as an airplane) will typically introduce a change in polarization and (partial) reception of the return wave by the same antenna.
The effect of free electrons in the ionosphere, in conjunction with the earth's magnetic field, causes Faraday rotation, a sort of circular birefringence. This is the same mechanism which can rotate the axis of linear polarization by electrons in interstellar space as mentioned below. The magnitude of Faraday rotation caused by such a plasma is greatly exaggerated at lower frequencies, so at the higher microwave frequencies used by satellites the effect is minimal. However medium or short wave transmissions received following refraction by the ionosphere are strongly affected. Since a wave's path through the ionosphere and the earth's magnetic field vector along such a path are rather unpredictable, a wave transmitted with vertical (or horizontal) polarization will generally have a resulting polarization in an arbitrary orientation at the receiver.
Polarization and vision.
Many animals are capable of perceiving some of the components of the polarization of light, e.g., linear horizontally polarized light. This is generally used for navigational purposes, since the linear polarization of sky light is always perpendicular to the direction of the sun. This ability is very common among the insects, including bees, which use this information to orient their communicative dances. Polarization sensitivity has also been observed in species of octopus, squid, cuttlefish, and mantis shrimp. In the latter case, one species measures all six orthogonal components of polarization, and is believed to have optimal polarization vision. The rapidly changing, vividly colored skin patterns of cuttlefish, used for communication, also incorporate polarization patterns, and mantis shrimp are known to have polarization selective reflective tissue. Sky polarization was thought to be perceived by pigeons, which was assumed to be one of their aids in homing, but research indicates this is a popular myth.
The naked human eye is weakly sensitive to polarization, without the need for intervening filters. Polarized light creates a very faint pattern near the center of the visual field, called Haidinger's brush. This pattern is very difficult to see, but with practice one can learn to detect polarized light with the naked eye.
Angular momentum using circular polarization.
It is well known that electromagnetic radiation carries a certain linear momentum in the direction of propagation. In addition, however, light carries a certain angular momentum if it is circularly polarized (or partially so). In comparison with lower frequencies such as microwaves, the amount of angular momentum in light, even of pure circular polarization, compared to the same wave's linear momentum (or radiation pressure) is very small and difficult to even measure. However it was utilized in an experiment to achieve speeds of up to 600 million revolutions per minute.

</doc>
<doc id="41565" url="https://en.wikipedia.org/wiki?curid=41565" title="Polarization-maintaining optical fiber">
Polarization-maintaining optical fiber

In fiber optics, polarization-maintaining optical fiber (PMF or PM fiber) is a single-mode optical fiber in which linearly polarized light, if properly launched into the fiber, maintains a linear polarization during propagation, exiting the fiber in a specific linear polarization state; there is little or no cross-coupling of optical power between the two polarization modes. Such fiber is used in special applications where preserving polarization is essential.
Polarization crosstalk.
In an ordinary (non-polarization-maintaining) fiber, two polarization modes (say vertical and horizontal polarization) have the same nominal phase velocity due to the fiber's circular symmetry. However tiny amounts of random birefringence in such a fiber, or bending in the fiber, will cause a tiny amount of crosstalk from the vertical to the horizontal polarization mode. And since even a short portion of fiber, over which a tiny coupling coefficient may apply, is many thousands of wavelengths long, even that small coupling between the two polarization modes, applied coherently, can lead to a large power transfer to the horizontal mode, completely changing the wave's net state of polarization. Since that coupling coefficient was unintended and a result of arbitrary stress or bending applied to fiber, the output state of polarization will itself be random, and will vary as those stresses or bends vary; it will also vary with wavelength. 
Principle of operation.
Polarization-maintaining fibers work by "intentionally" introducing a systematic linear birefringence in the fiber, so that there are two well defined polarization modes which propagate along the fiber with very distinct phase velocities. The beat length Lb of such a fiber (for a particular wavelength) is the distance (typically a few millimeters) over which the wave in one mode will experience an additional delay of one wavelength compared to the other polarization mode. Thus a length Lb /2 of such fiber is equivalent to a half-wave plate. Now consider that there might be a random coupling between the two polarization states over a significant length of such fiber. At point 0 along the fiber, the wave in polarization mode 1 induces an amplitude into mode 2 at some phase. However at point 1/2 Lb along the fiber, the same coupling coefficient between the polarization modes induces an amplitude into mode 2 which is now 180 degrees "out of phase" with the wave coupled at point zero, leading to cancellation. At point Lb along the fiber the coupling is again in the original phase, but at 3/2 Lb it is again out of phase and so on. The possibility of coherent addition of wave amplitudes through crosstalk over distances much larger than Lb is thus eliminated. Most of the wave's power remains in the original polarization mode, and exits the fiber in that mode's polarization as it is oriented at the fiber end. Optical fiber connectors used for PM fibers are specially keyed so that the two polarization modes are aligned and exit in a specific orientation.
Note that a polarization-maintaining fiber does not polarize light as a polarizer does. Rather, PM fiber maintains the linear polarization of linearly polarized light provided that it is launched into the fiber aligned with one of the fiber's polarization modes. Launching linearly polarized light into the fiber at a different angle will excite both polarization modes, conducting the same wave at a slightly different phase velocities. At most points along the fiber the net polarization will be an elliptically polarized state, with a return to the original polarization state after an integer number of beat lengths. Consequently, if visible laser light is launched into the fiber exciting both polarization modes, scattering of propagating light viewed from the side, is observed with a light and dark pattern periodic over each beat length, since scattering is preferentially perpendicular to the polarization direction.
Designs.
Several different designs are used to create birefringence in a fiber. The fiber may be geometrically asymmetric or have a refractive index profile which is asymmetric such as the design using an elliptical cladding as shown in the diagram. Alternatively, stress permanently induced in the fiber will produce stress birefringence; this may be accomplished using rods of another material included within the cladding. Several different shapes of rod are used, and the resulting fiber is sold under brand names such as "Panda" and "Bow-tie".
It is possible to create a circularly birefringent optical fiber just using an ordinary (circularly symmetric) single-mode fiber and twisting it, thus creating internal torsional stress. That causes the phase velocity of right and left hand circular polarizations to significantly differ. Thus the two circular polarizations propagate with little crosstalk in between them
Applications.
Polarization-maintaining optical fibers are used in special applications, such as in fiber optic sensing, interferometry and quantum key distribution. They are also commonly used in telecommunications for the connection between a source laser and a modulator, since the modulator requires polarized light as input. They are rarely used for long-distance transmission, because PM fiber is expensive and has higher attenuation than singlemode fiber. 
The output of a PM fiber is typically characterized by its polarization extinction ratio (PER)—the ratio of correctly to incorrectly polarized light, expressed in decibels. The quality of PM patchcords and pigtails can be characterized with a PER meter.

</doc>
<doc id="41566" url="https://en.wikipedia.org/wiki?curid=41566" title="Polling, Mühldorf">
Polling, Mühldorf

Polling is a municipality in the district of Mühldorf in Bavaria in Germany.

</doc>
<doc id="41567" url="https://en.wikipedia.org/wiki?curid=41567" title="Power budget">
Power budget

In telecommunication, a power budget (or system budget) is the allocation, within a system, of available transmitter power output to achieve the desired effective radiated power, among the various functions that need to be performed. 
An example of a power budget in a communications satellite is the allocation of available power among various functions, such as maintaining satellite orientation, maintaining orbital control, performing signal reception, and performing signal transmission. 

</doc>
<doc id="41568" url="https://en.wikipedia.org/wiki?curid=41568" title="Power factor">
Power factor

In electrical engineering, the power factor of an AC electrical power system is defined as the ratio of the real power flowing to the load to the apparent power in the circuit, and is a dimensionless number in the closed interval of -1 to 1. A power factor of less than one means that the voltage and current waveforms are not in phase, reducing the instantaneous product of the two waveforms (V x I). Real power is the capacity of the circuit for performing work in a particular time. Apparent power is the product of the current and voltage of the circuit. Due to energy stored in the load and returned to the source, or due to a non-linear load that distorts the wave shape of the current drawn from the source, the apparent power will be greater than the real power. A negative power factor occurs when the device (which is normally the load) generates power, which then flows back towards the source, which is normally considered the generator.
In an electric power system, a load with a low power factor draws more current than a load with a high power factor for the same amount of useful power transferred. The higher currents increase the energy lost in the distribution system, and require larger wires and other equipment. Because of the costs of larger equipment and wasted energy, electrical utilities will usually charge a higher cost to industrial or commercial customers where there is a low power factor.
Linear loads with low power factor (such as induction motors) can be corrected with a passive network of capacitors or inductors. Non-linear loads, such as rectifiers, distort the current drawn from the system. In such cases, active or passive power factor correction may be used to counteract the distortion and raise the power factor. The devices for correction of the power factor may be at a central substation, spread out over a distribution system, or built into power-consuming equipment.
Linear circuits.
In a purely resistive AC circuit, voltage and current waveforms are in step (or in phase), changing polarity at the same instant in each cycle. All the power entering the load is consumed (or dissipated).
Where reactive loads are present, such as with capacitors or inductors, energy storage in the loads results in a phase difference between the current and voltage waveforms. During each cycle of the AC voltage, extra energy, in addition to any energy consumed in the load, is temporarily stored in the load in electric or magnetic fields, and then returned to the power grid a fraction of the period later.
Because high voltage alternating current (HVAC) distribution systems are essentially quasi-linear circuit systems subject to continuous daily variation, there is a continuous "ebb and flow" of nonproductive power. Non productive power increases the current in the line, potentially to the point of failure.
Thus, a circuit with a low power factor will use higher currents to transfer a given quantity of real power than a circuit with a high power factor. A linear load does not change the shape of the waveform of the current, but may change the relative timing (phase) between voltage and current.
Electrical circuits containing dominantly resistive loads (incandescent lamps, heating elements) have a power factor of almost 1.0, but circuits containing inductive or capacitive loads (electric motors, solenoid valves, transformers, fluorescent lamp ballasts, and others) can have a power factor well below 1.
Definition and calculation.
AC power flow has three components:
The VA and var are non-SI units mathematically identical to the Watt, but are used in engineering practice instead of the Watt in order to state what quantity is being expressed. The SI explicitly disallows using units for this purpose or as the only source of information about a physical quantity as used.
The power factor is defined as the ratio of real power to apparent power. As power is transferred along a transmission line, it does not consist purely of real power that can do work once transferred to the load, but rather consists of a combination of real and reactive power, called apparent power. The power factor describes the amount of real power transmitted along a transmission line relative to the total apparent power flowing in the line.
The Power Triangle:
We can relate the various components of AC power by using the power triangle. Real power extends horizontally in the î direction as it represents a purely real component of AC power. Reactive power extends in the direction of ĵ as it represents a purely imaginary component of AC power. Apparent power represents a combination of both real and reactive power, and therefore can be calculated by using the vector sum of these two components. We can conclude that the mathematical relationship between these components is:
formula_1
formula_2
formula_3
Increasing the Power Factor:
As the power factor increases, the ratio of real power to apparent power increases and approaches unity (1), while the angle θ decreases and the reactive power decreases.
Decreasing the Power Factor:
As the power factor decreases, the ratio of real power to apparent power also decreases, as the angle θ increases and reactive power increases.
Lagging and Leading Power Factors:
In addition, there is also a difference between a lagging and leading power factor. A lagging power factor signifies that the load is inductive, as the load will “consume” reactive power, and therefore the reactive component Q is positive as reactive power travels through the circuit and is “consumed” by the inductive load. A leading power factor signifies that the load is capacitive, as the load “supplies” reactive power, and therefore the reactive component Q is negative as reactive power is being supplied to the circuit.
If θ is the phase angle between the current and voltage, then the power factor is equal to the cosine of the angle, formula_4:
Since the units are consistent, the power factor is by definition a dimensionless number between −1 and 1. When power factor is equal to 0, the energy flow is entirely reactive and stored energy in the load returns to the source on each cycle. When the power factor is 1, all the energy supplied by the source is consumed by the load. Power factors are usually stated as "leading" or "lagging" to show the sign of the phase angle. Capacitive loads are leading (current leads voltage), and inductive loads are lagging (current lags voltage).
If a purely resistive load is connected to a power supply, current and voltage will change polarity in step, the power factor will be unity (1), and the electrical energy flows in a single direction across the network in each cycle. Inductive loads such as transformers and motors (any type of wound coil) consume reactive power with current waveform lagging the voltage. Capacitive loads such as capacitor banks or buried cable generate reactive power with current phase leading the voltage. Both types of loads will absorb energy during part of the AC cycle, which is stored in the device's magnetic or electric field, only to return this energy back to the source during the rest of the cycle.
For example, to get 1 kW of real power, if the power factor is unity, 1 kVA of apparent power needs to be transferred (1 kW ÷ 1 = 1 kVA). At low values of power factor, more apparent power needs to be transferred to get the same real power. To get 1 kW of real power at 0.2 power factor, 5 kVA of apparent power needs to be transferred (1 kW ÷ 0.2 = 5 kVA). This apparent power must be produced and transmitted to the load in the conventional fashion, and is subject to the usual distributed losses in the production and transmission processes.
Electrical loads consuming alternating current power consume both real power and reactive power. The vector sum of real and reactive power is the apparent power. The presence of reactive power causes the real power to be less than the apparent power, and so, the electric load has a power factor of less than 1.
A negative power factor (0 to -1) can result from returning power to the source, such as in the case of a building fitted with solar panels when their power is not being fully utilised within the building and the surplus is fed back into the supply.
Power factor correction of linear loads.
A high power factor is generally desirable in a transmission system to reduce transmission losses and improve voltage regulation at the load. It is often desirable to adjust the power factor of a system to near 1.0. When reactive elements supply or absorb reactive power near the load, the apparent power is reduced. Power factor correction may be applied by an electric power transmission utility to improve the stability and efficiency of the transmission network. Individual electrical customers who are charged by their utility for low power factor may install correction equipment to reduce those costs.
Power factor correction brings the power factor of an AC power circuit closer to 1 by supplying reactive power of opposite sign, adding capacitors or inductors that act to cancel the inductive or capacitive effects of the load, respectively. For example, the inductive effect of motor loads may be offset by locally connected capacitors. If a load had a capacitive value, inductors (also known as "reactors" in this context) are connected to correct the power factor. In the electricity industry, inductors are said to "consume" reactive power and capacitors are said to "supply" it, even though the energy is just moving back and forth on each AC cycle.
The reactive elements can create voltage fluctuations and harmonic noise when switched on or off. They will supply or sink reactive power regardless of whether there is a corresponding load operating nearby, increasing the system's no-load losses. In the worst case, reactive elements can interact with the system and with each other to create resonant conditions, resulting in system instability and severe overvoltage fluctuations. As such, reactive elements cannot simply be applied without engineering analysis.
An automatic power factor correction unit consists of a number of capacitors that are switched by means of contactors. These contactors are controlled by a regulator that measures power factor in an electrical network. Depending on the load and power factor of the network, the power factor controller will switch the necessary blocks of capacitors in steps to make sure the power factor stays above a selected value.
Instead of using a set of switched capacitors, an unloaded synchronous motor can supply reactive power. The reactive power drawn by the synchronous motor is a function of its field excitation. This is referred to as a "synchronous condenser". It is started and connected to the electrical network. It operates at a leading power factor and puts vars onto the network as required to support a system's voltage or to maintain the system power factor at a specified level.
The synchronous condenser's installation and operation are identical to large electric motors. Its principal advantage is the ease with which the amount of correction can be adjusted; it behaves like an electrically variable capacitor. Unlike capacitors, the amount of reactive power supplied is proportional to voltage, not the square of voltage; this improves voltage stability on large networks. Synchronous condensers are often used in connection with high-voltage direct-current transmission projects or in large industrial plants such as steel mills.
For power factor correction of high-voltage power systems or large, fluctuating industrial loads, power electronic devices such as the Static VAR compensator or STATCOM are increasingly used. These systems are able to compensate sudden changes of power factor much more rapidly than contactor-switched capacitor banks, and being solid-state require less maintenance than synchronous condensers.
Non-linear loads.
Examples of non-linear loads on a power system are rectifiers (such as used in a power supply), and arc discharge devices such as fluorescent lamps, electric welding machines, or arc furnaces. Because current in these systems is interrupted by a switching action, the current contains frequency components that are multiples of the power system frequency. Distortion power factor is a measure of how much the harmonic distortion of a load current decreases the average power transferred to the load.
Non-sinusoidal components.
In linear circuits having only sinusoidal currents and voltages of one frequency, the power factor arises only from the difference in phase between the current and voltage. This is "displacement power factor".
Non-linear loads change the shape of the current waveform from a sine wave to some other form. Non-linear loads create harmonic currents in addition to the original (fundamental frequency) AC current. This is of importance in practical power systems that contain non-linear loads such as rectifiers, some forms of electric lighting, electric arc furnaces, welding equipment, switched-mode power supplies and other devices. Filters consisting of linear capacitors and inductors can prevent harmonic currents from entering the supplying system.
A typical multimeter will give incorrect results when attempting to measure the AC current in a non-sinusoidal waveform; the instruments sense the average value of the rectified waveform. The average response is then calibrated to the effective RMS value. An RMS sensing multimeter must be used to measure the actual RMS currents and voltages (and therefore apparent power). To measure the real power or reactive power, a wattmeter designed to work properly with non-sinusoidal currents must be used.
Distortion power factor.
The "distortion power factor" is the distortion component associated with the harmonic voltages and currents present in the system.
formula_7 is the total harmonic distortion of the load current. formula_8 is the fundamental component of the current and formula_9 is the total current – both are root mean square-values (distortion power factor can also be used to describe individual order harmonics, using the corresponding current in place of total current). This definition with respect to total harmonic distortion assumes that the voltage stays undistorted (sinusoidal, without harmonics). This simplification is often a good approximation for stiff voltage sources (not being affected by changes in load downstream in the distribution network). Total harmonic distortion of typical generators from current distortion in the network is on the order of 1–2%, which can have larger scale implications but can be ignored in common practice.
The result when multiplied with the displacement power factor (DPF) is the overall, true power factor or just power factor (PF):
Distortion in three-phase networks.
In practice, the local effects of distortion current on devices in a three-phase distribution network rely on the magnitude of certain order harmonics rather than the total harmonic distortion.
For example, the triplen, or zero-sequence, harmonics (3rd, 9th, 15th, etc.) have the property of being in-phase when compared line-to-line. In a delta-wye transformer, these harmonics can result in circulating currents in the delta windings and result in greater resistive heating. In a wye-configuration of a transformer, triplen harmonics will not create these currents, but they will result in a non-zero current in the neutral wire. This could overload the neutral wire in some cases 
and create error in kilowatt-hour metering systems and billing revenue. The presence of current harmonics in a transformer also result in larger eddy currents in the magnetic core of the transformer. Eddy current losses generally increase as the square of the frequency, lowering the transformer's efficiency, dissipating additional heat, and reducing its service life.
Negative-sequence harmonics (5th, 11th, 17th, etc.) combine 120 degrees out of phase, similarly to the fundamental harmonic but in a reversed sequence. In generators and motors, these currents produce magnetic fields which oppose the rotation of the shaft and sometimes result in damaging mechanical vibrations.
Switched-mode power supplies.
A particularly important class of non-linear loads is the millions of personal computers that typically incorporate switched-mode power supplies (SMPS) with rated output power ranging from a few watts to more than 1 kW. Historically, these very-low-cost power supplies incorporated a simple full-wave rectifier that conducted only when the mains instantaneous voltage exceeded the voltage on the input capacitors. This leads to very high ratios of peak-to-average input current, which also lead to a low distortion power factor and potentially serious phase and neutral loading concerns.
A typical switched-mode power supply first converts the AC mains to a DC bus by means of a bridge rectifier or a similar circuit. The output voltage is then derived from this DC bus. The problem with this is that the rectifier is a non-linear device, so the input current is highly non-linear. That means that the input current has energy at harmonics of the frequency of the voltage.
This presents a particular problem for the power companies, because they cannot compensate for the harmonic current by adding simple capacitors or inductors, as they could for the reactive power drawn by a linear load. Many jurisdictions are beginning to legally require power factor correction for all power supplies above a certain power level.
Regulatory agencies such as the EU have set harmonic limits as a method of improving power factor. Declining component cost has hastened implementation of two different methods. To comply with current EU standard EN61000-3-2, all switched-mode power supplies with output power more than 75 W must include passive power factor correction, at least. 80 Plus power supply certification requires a power factor of 0.9 or more.
Power factor correction (PFC) in non-linear loads.
Passive PFC.
The simplest way to control the harmonic current is to use a filter that passes current only at line frequency (50 or 60 Hz). The filter consists of capacitors or inductors, and makes a non-linear device look more like a linear load. An example of passive PFC is a valley-fill circuit.
A disadvantage of passive PFC is that it requires larger inductors or capacitors than an equivalent power active PFC circuit. Also, in practice, passive PFC is often less effective at improving the power factor.
Active PFC.
Active PFC is the use of power electronics to change the waveform of current drawn by a load to improve the power factor. Some types of the active PFC are buck, boost, buck-boost and synchronous condenser. Active power factor correction can be single-stage or multi-stage.
In the case of a switched-mode power supply, a boost converter is inserted between the bridge rectifier and the main input capacitors. The boost converter attempts to maintain a constant DC bus voltage on its output while drawing a current that is always in phase with and at the same frequency as the line voltage. Another switched-mode converter inside the power supply produces the desired output voltage from the DC bus. This approach requires additional semiconductor switches and control electronics, but permits cheaper and smaller passive components. It is frequently used in practice.
For a three-phase SMPS, the Vienna rectifier configuration may be used to substantially improve the power factor.
SMPSs with passive PFC can achieve power factor of about 0.7–0.75, SMPSs with active PFC, up to 0.99 power factor, while a SMPS without any power factor correction have a power factor of only about 0.55–0.65.
Due to their very wide input voltage range, many power supplies with active PFC can automatically adjust to operate on AC power from about 100 V (Japan) to 230 V (Europe). That feature is particularly welcome in power supplies for laptops.
Dynamic PFC.
Dynamic power factor correction (DPFC), sometimes referred to as "real-time power factor correction," is used for electrical stabilization in cases of rapid load changes (e.g. at large manufacturing sites). DPFC is useful when standard power factor correction would cause over or under correction. DPFC uses semiconductor switches, typically thyristors, to quickly connect and disconnect capacitors or inductors from the network in order to improve power factor.
Importance of power factor in distribution systems.
Power factors below 1.0 require a utility to generate more than the minimum volt-amperes necessary to supply the real power (watts). This increases generation and transmission costs. For example, if the load power factor were as low as 0.7, the apparent power would be 1.4 times the real power used by the load. Line current in the circuit would also be 1.4 times the current required at 1.0 power factor, so the losses in the circuit would be doubled (since they are proportional to the square of the current). Alternatively all components of the system such as generators, conductors, transformers, and switchgear would be increased in size (and cost) to carry the extra current.
Utilities typically charge additional costs to commercial customers who have a power factor below some limit, which is typically 0.9 to 0.95. Engineers are often interested in the power factor of a load as one of the factors that affect the efficiency of power transmission.
With the rising cost of energy and concerns over the efficient delivery of power, active PFC has become more common in consumer electronics. Current Energy Star guidelines for computers call for a power factor of ≥ 0.9 at 100% of rated output in the PC's power supply. According to a white paper authored by Intel and the U.S. Environmental Protection Agency, PCs with internal power supplies will require the use of active power factor correction to meet the ENERGY STAR 5.0 Program Requirements for Computers.
In Europe, EN 61000-3-2 requires power factor correction be incorporated into consumer products.
Techniques for measuring the power factor.
The power factor in a single-phase circuit (or balanced three-phase circuit) can be measured with the wattmeter-ammeter-voltmeter method, where the power in watts is divided by the product of measured voltage and current. The power factor of a balanced polyphase circuit is the same as that of any phase. The power factor of an unbalanced poly phase circuit is not uniquely defined.
A direct reading power factor meter can be made with a moving coil meter of the electrodynamic type, carrying two perpendicular coils on the moving part of the instrument. The field of the instrument is energized by the circuit current flow. The two moving coils, A and B, are connected in parallel with the circuit load. One coil, A, will be connected through a resistor and the second coil, B, through an inductor, so that the current in coil B is delayed with respect to current in A. At unity power factor, the current in A is in phase with the circuit current, and coil A provides maximum torque, driving the instrument pointer toward the 1.0 mark on the scale. At zero power factor, the current in coil B is in phase with circuit current, and coil B provides torque to drive the pointer towards 0. At intermediate values of power factor, the torques provided by the two coils add and the pointer takes up intermediate positions.
Another electromechanical instrument is the polarized-vane type. In this instrument a stationary field coil produces a rotating magnetic field, just like a polyphase motor. The field coils are connected either directly to polyphase voltage sources or to a phase-shifting reactor if a single-phase application. A second stationary field coil, perpendicular to the voltage coils, carries a current proportional to current in one phase of the circuit. The moving system of the instrument consists of two vanes that are magnetized by the current coil. In operation the moving vanes take up a physical angle equivalent to the electrical angle between the voltage source and the current source. This type of instrument can be made to register for currents in both directions, giving a four-quadrant display of power factor or phase angle.
Digital instruments can be made that either directly measure the time lag between voltage and current waveforms and so calculate the power factor, or that measure both true and apparent power in the circuit and calculate the quotient. The first method is only accurate if voltage and current are sinusoidal. Loads such as rectifiers distort the waveforms from the sinusoidal shape.
Mnemonics.
English-language power engineering students are advised to remember: 
"ELI the ICE man" or "ELI on ICE" – the voltage E leads the current I in an inductor L; the current leads the voltage in a capacitor C. 
Another common mnemonic is CIVIL – in a capacitor (C) the current (I) leads voltage (V), voltage (V) leads current (I) in an inductor (L).

</doc>
<doc id="41569" url="https://en.wikipedia.org/wiki?curid=41569" title="Power failure transfer">
Power failure transfer

In telecommunication, the term power failure transfer has the following meanings:
Power-failure transfer is an emergency mode of operation in which one and only one instrument may be powered from each trunk line from the subscriber location to the central office.

</doc>
<doc id="41570" url="https://en.wikipedia.org/wiki?curid=41570" title="Power-law index profile">
Power-law index profile

For optical fibers, a power-law index profile is an index of refraction profile characterized by 
where
formula_2
and formula_3 is the nominal refractive index as a function of distance from the fiber axis, formula_4 is the nominal refractive index on axis, formula_5 is the refractive index of the cladding, which is taken to be homogeneous (formula_6), formula_7 is the core radius, and formula_8 is a parameter that defines the shape of the profile. formula_7 is often used in place of formula_8. Hence, this is sometimes called an alpha profile. 
For this class of profiles, multimode distortion is smallest when formula_8 takes a particular value depending on the material used. For most materials, this optimum value is approximately 2. In the limit of infinite formula_8, the profile becomes a step-index profile.

</doc>
<doc id="41571" url="https://en.wikipedia.org/wiki?curid=41571" title="Power margin">
Power margin

In telecommunication, the power margin is the difference between available signal power and the minimum signal power needed to overcome system losses and still satisfy the minimum input requirements of the receiver for a given performance level. 
System power margin reflects the excess signal level, present at the input of the receiver, that is available to compensate for (a) the effects of component aging in the transmitter, receiver, or physical transmission medium, and (b) a deterioration in propagation conditions. "Synonym" system power margin.

</doc>
<doc id="41572" url="https://en.wikipedia.org/wiki?curid=41572" title="Precision">
Precision


</doc>
<doc id="41574" url="https://en.wikipedia.org/wiki?curid=41574" title="Preemphasis improvement">
Preemphasis improvement

In FM broadcasting, preemphasis improvement is the improvement in the signal-to-noise ratio of the high-frequency portion of the baseband, "i.e.," modulating, signal, which improvement results from passing the modulating signal through a preemphasis network before transmission.
The reason that preemphasis is needed is that the process of detecting a frequency-modulated signal in a receiver produces a noise spectrum that rises in frequency (a so-called "triangular" spectrum). Without preemphasis, the received audio would sound unacceptably noisy at high frequencies, especially under conditions of low carrier-to-noise ratio, i.e., during fringe reception conditions. Preemphasis increases the magnitude of the higher signal frequencies, thereby improving the signal-to-noise ratio. At the output of the discriminator in the FM receiver, a deemphasis network restores the original signal power distribution.
"FM improvement factor" is the quotient obtained by dividing the signal-to-noise ratio (SNR) at the output of an FM receiver by the carrier-to-noise ratio (CNR) at the input of the receiver. When the FM improvement factor is greater than unity, the improvement in the SNR is always obtained at the expense of an increased bandwidth in the receiver and the transmission path.
"FM improvement threshold" is the point in an FM (frequency modulation) receiver at which the peaks in the RF signal equal the peaks of the thermal noise generated in the receiver. A baseband signal-to-noise ratio of about 30 dB is typical at the improvement threshold, and this ratio improves 1 dB for each decibel of increase in the signal above the threshold.

</doc>
<doc id="41576" url="https://en.wikipedia.org/wiki?curid=41576" title="Preventive maintenance">
Preventive maintenance

Preventive maintenance (PM) has the following meanings:
The primary goal of maintenance is to avoid or mitigate the consequences of failure of equipment. This may be by preventing the failure before it actually occurs which Planned Maintenance and Condition Based Maintenance help to achieve. It is designed to preserve and restore equipment reliability by replacing worn components before they actually fail. Preventive maintenance activities include partial or complete overhauls at specified periods, oil changes, lubrication, minor adjustments, and so on. In addition, workers can record equipment deterioration so they know to replace or repair worn parts before they cause system failure. The ideal preventive maintenance program would prevent all equipment failure before it occurs.
Preventative maintenance for various equipment and facilities is quite nuanced. For instance, maintaining certain equipment may include a "preventative maintenance checklist" which includes small checks which can significantly extend service life. Furthermore, other considerations such as weather and equipment are taken into account; for instance, in the case of HVAC systems, maintenance is often performed before the hottest time of the year.
There is a controversy of sorts regarding the propriety of the usage “preventative.”

</doc>
<doc id="41577" url="https://en.wikipedia.org/wiki?curid=41577" title="Primary channel">
Primary channel

In telecommunication, the term primary channel has the following meanings: 
A primary channel may support the transfer of information in one direction only, either direction alternately, or both directions simultaneously.

</doc>
<doc id="41578" url="https://en.wikipedia.org/wiki?curid=41578" title="Primary Rate Interface">
Primary Rate Interface

The Primary Rate Interface (PRI) is a telecommunications interface standard used on an Integrated Services Digital Network (ISDN) for carrying multiple DS0 voice and data transmissions between the network and a user.
PRI is the standard for providing telecommunication services to offices. It is based on the T-carrier (T1) line in the US and Canada, and the E-carrier (E1) line in Europe. The T1 line consists of 24 channels, while an E1 has 32.
PRI provides a varying number of channels depending on the standards in the country of implementation. In North America and Japan it consists of 23xB (B channels (bearer channels)) and 1xD (D channel (data channel)) (23 64-kbit/s digital channels + 1 64-kbit/s signaling/control channel) on a T1 (1.544 Mbit/s). In Europe and Australia it is 30B + D on an E1 2.048 Mbit/s. One timeslot(0) on the E1 is used for synchronization purposes and is not considered to be a B or D channel. The D-channel is typically uses timeslot 16 on an E1, vs timeslot 24 for a T1.
Fractional T1.
Fewer active B channels (also called bearer channels) can be used for a fractional T1. Bearer channels may also be known as user channels. More channels can be used with more T1s, within certain design limits.
PRI and BRI.
The Integrated Services Digital Network (ISDN) prescribes two levels of service:
Each B-channel carries data, voice, and other services. The D-channel carries control and signaling information. Larger connections are possible using PRI pairing. A dual T1-PRI could have 24 + 23 = 47 B-channels and 1 D-channel (often called "47B + D"), but more commonly has 46 B-channels and 2 D-channels thus providing a backup signaling channel. The concept applies to E1s as well and both can include more than 2 PRIs. When configuring multiple T1's as ISDN-PRI's, it's possible to use NFAS (non-facility associated signalling) to enable one or two D-channels to support additional B-channels on separate T1 circuits.
Application.
The Primary Rate Interface channels are typically used by medium to large enterprises with digital PBXs to provide them digital access to the Public Switched Telephone Network (PSTN). The 23 (or 30) B-channels can be used flexibly and reassigned when necessary to meet special needs such as video conferences. The Primary Rate user is hooked up directly to the telephone company central office.
PRI channels and direct inward dialing are also common as a means of delivering inbound calls to voice over IP gateways from the PSTN.

</doc>
<doc id="41579" url="https://en.wikipedia.org/wiki?curid=41579" title="Primary station">
Primary station

In a data communication network, the primary station is the station responsible for unbalanced control of a data link. 
The primary station generates commands and interprets responses, and is responsible for initialization of data and control information interchange, organization and control of data flow, retransmission control, and all recovery functions at the link level.

</doc>
<doc id="41580" url="https://en.wikipedia.org/wiki?curid=41580" title="Primary time standard">
Primary time standard

In telecommunications, a primary time standard is a time standard that does not require calibration against another time standard. 
Examples of primary time, ("i.e.", frequency standards) are caesium standards and hydrogen masers. 
The international second is based on the microwave frequency (9,192,631,770 Hz) associated with the atomic resonance of the hyperfine ground state levels of the caesium-133 atom in a magnetically neutral environment. Realizable caesium frequency standards use a strong electromagnet to deliberately introduce a magnetic field which overwhelms that of the Earth. The presence of this strong magnetic field introduces a slight, but known, increase in the atomic resonance frequency. However, very small variations in the calibration of the electric current in the electromagnet introduce minuscule frequency variations among different caesium oscillators.

</doc>
<doc id="41581" url="https://en.wikipedia.org/wiki?curid=41581" title="Principal clock">
Principal clock

In telecommunications, the principal clock of a set of redundant clocks, is the clock that is selected for normal use. The principal clock may be selected because of a property, "e.g." superior accuracy, that makes it a unique member of the set. 
The term ""principal clock"" should not be confused with, or used as a synonym for, the term ""primary frequency standard.""

</doc>
<doc id="41582" url="https://en.wikipedia.org/wiki?curid=41582" title="Priority">
Priority

Priority may refer to:

</doc>
<doc id="41583" url="https://en.wikipedia.org/wiki?curid=41583" title="Priority level">
Priority level

Priority level or priority, in the Telecommunications Service Priority system, is the level that may be assigned to an NS/EP telecommunications service, which level specifies the order in which provisioning or restoration of the service is to occur relative to other NS/EP or non-NS/EP telecommunication services. 
Priority levels authorized are designated (highest to lowest) "E," "1," "2," "3," "4," and "5" for provisioning and "1," "2," "3," "4," and "5" for restoration. 

</doc>
<doc id="41584" url="https://en.wikipedia.org/wiki?curid=41584" title="Private line">
Private line

In wired telephony, a private line or tie line is a service that involves dedicated circuits, private switching arrangements, and/or predefined transmission paths, whether virtual or physical, which provide communications between specific locations. Most private lines connect only two locations though they may be switched at either end or both. Some have multiple drop points. 
Among subscribers to the public switched telephone network, the term "private line" is often used to describe an individual (one-party) telephone line, as opposed to a party line with multiple stations connected.
Proprietary services.
In radio or wireless telephony, "Private Line" is a term trademarked by Motorola to describe their implementation of a Continuous Tone-Coded Squelch System (CTCSS), a method of using low-frequency subaudible tones to share a single radio channel among multiple users. Each user group would use a different low frequency tone. Motorola's trade name, especially the abbreviation "PL", has become a genericized trademark for the method. General Electric used the term "Channel Guard" to describe the same system and other manufacturers used other terms. A later digital version of "Private Line" is called "Digital Private Line" (DPL).

</doc>
<doc id="41585" url="https://en.wikipedia.org/wiki?curid=41585" title="Proceed-to-select">
Proceed-to-select

In telecommunications, proceed-to-select is a signal or event in the call-access phase of a data call, which signal or event confirms the reception of a call-request signal and advises the calling data terminal equipment to proceed with the transmission of the selection signals. 
Examples of proceed-to-select pertain to a dial tone in a telephone system.

</doc>
<doc id="41586" url="https://en.wikipedia.org/wiki?curid=41586" title="Propagation constant">
Propagation constant

The propagation constant of an electromagnetic wave is a measure of the change undergone by the amplitude of the wave as it propagates in a given direction. The quantity being measured can be the voltage or current in a circuit or a field vector such as electric field strength or flux density. The propagation constant itself measures change per unit length but is otherwise dimensionless. In the context of two-port networks and their cascades, propagation constant measures the change undergone by the source quantity as it propagates from one port to the next.
The propagation constant is expressed logarithmically, almost universally to the base "e", rather than the more usual base 10 used in telecommunications in other situations. The quantity measured, such as voltage, is expressed as a sinusoidal phasor. The phase of the sinusoid varies with distance which results in the propagation constant being a complex number, the imaginary part being caused by the phase change.
__TOC__
Alternative names.
The term propagation constant is somewhat of a misnomer as it usually varies strongly with "ω". It is probably the most widely used term but there are a large variety of alternative names used by various authors for this quantity. These include, transmission parameter, transmission function, propagation parameter, propagation coefficient and transmission constant. In plural, it is usually implied that "α" and "β" are being referenced separately but collectively as in transmission parameters, propagation parameters, propagation coefficients, transmission constants and secondary coefficients. This last occurs in transmission line theory, the term "secondary" being used to contrast to the "primary line coefficients". The primary coefficients being the physical properties of the line; R,C,L and G, from which the secondary coefficients may be derived using the telegrapher's equation. Note that, at least in the field of transmission lines, the term transmission coefficient has a different meaning despite the similarity of name. Here it is the corollary of reflection coefficient.
Definition.
The propagation constant, symbol "γ", for a given system is defined by the ratio of the amplitude at the source of the wave to the amplitude at some distance "x", such that,
Since the propagation constant is a complex quantity we can write:
where
That "β" does indeed represent phase can be seen from Euler's formula:
which is a sinusoid which varies in phase as "θ" varies but does not vary in amplitude because
The reason for the use of base e is also now made clear. The imaginary phase constant, "iβ", can be added directly to the attenuation constant, "α", to form a single complex number that can be handled in one mathematical operation provided they are to the same base. Angles measured in radians require base "e", so the attenuation is likewise in base "e".
The propagation constant for copper (or any other conductor) lines can be calculated from the primary line coefficients by means of the relationship
where
Attenuation constant.
In telecommunications, the term attenuation constant, also called attenuation parameter or attenuation coefficient, is the attenuation of an electromagnetic wave propagating through a medium per unit distance from the source. It is the real part of the propagation constant and is measured in nepers per metre. A neper is approximately 8.7 dB. Attenuation constant can be defined by the amplitude ratio
The propagation constant per unit length is defined as the natural logarithmic of ratio of the sending end current or voltage to the receiving end current or voltage.
Copper lines.
The attenuation constant for copper lines (or ones made of any other conductor) can be calculated from the primary line coefficients as shown above. For a line meeting the distortionless condition, with a conductance "G" in the insulator, the attenuation constant is given by
however, a real line is unlikely to meet this condition without the addition of loading coils and, furthermore, there are some frequency dependent effects operating on the primary "constants" which cause a frequency dependence of the loss. There are two main components to these losses, the metal loss and the dielectric loss.
The loss of most transmission lines are dominated by the metal loss, which causes a frequency dependency due to finite conductivity of metals, and the skin effect inside a conductor. The skin effect causes R along the conductor to be approximately dependent on frequency according to
Losses in the dielectric depend on the loss tangent (tan "δ") of the material, which depends inversely on the wavelength of the signal and is directly proportional to the frequency.
Optical fibre.
The attenuation constant for a particular propagation mode in an optical fiber, the real part of the axial propagation constant.
Phase constant.
In electromagnetic theory, the phase constant, also called phase change constant, parameter or coefficient is the imaginary component of the propagation constant for a plane wave. It represents the change in phase per unit length along the path travelled by the wave at any instant and is equal to real part of the angular wavenumber of the wave. It is represented by the symbol "β" and is measured in units of radians per unit length.
From the definition of (angular) wavenumber:
For a transmission line, the Heaviside condition of the telegrapher's equation tells us that the wavenumber must be proportional to frequency for the transmission of the wave to be undistorted in the time domain. This includes, but is not limited to, the ideal case of a lossless line. The reason for this condition can be seen by considering that a useful signal is composed of many different wavelengths in the frequency domain. For there to be no distortion of the waveform, all these waves must travel at the same velocity so that they arrive at the far end of the line at the same time as a group. Since wave phase velocity is given by
it is proved that "β" is required to be proportional to "ω". In terms of primary coefficients of the line, this yields from the telegrapher's equation for a distortionless line the condition
However, practical lines can only be expected to approximately meet this condition over a limited frequency band.
Filters and two-port networks.
The term propagation constant or propagation function is applied to filters and other two-port networks used for signal processing. In these cases, however, the attenuation and phase coefficients are expressed in terms of nepers and radians per network section rather than per unit length. Some authors make a distinction between per unit length measures (for which "constant" is used) and per section measures (for which "function" is used).
The propagation constant is a useful concept in filter design which invariably uses a cascaded section topology. In a cascaded topology, the propagation constant, attenuation constant and phase constant of individual sections may be simply added to find the total propagation constant etc.
Cascaded networks.
The ratio of output to input voltage for each network is given by
The terms formula_18 are impedance scaling terms and their use is explained in the image impedance article.
The overall voltage ratio is given by
Thus for "n" cascaded sections all having matching impedances facing each other, the overall propagation constant is given by
See also.
The concept of penetration depth is one of many ways to describe the absorption of electromagnetic waves. For the others, and their interrelationships, see the article: Mathematical descriptions of opacity.

</doc>
<doc id="41587" url="https://en.wikipedia.org/wiki?curid=41587" title="Propagation path obstruction">
Propagation path obstruction

In telecommunication, a propagation path obstruction is a man-made or natural physical feature that lies near enough to a radio path to cause a measurable effect on path loss, exclusive of reflection effects. An obstruction may lie to the side, above, or below the path. Ridges, bridges, cliffs, buildings, and trees are examples of obstructions. If the clearance from the nearest anticipated path position, over the expected range of Earth radius k-factor, exceeds 0.6 of the first Fresnel zone radius, the feature is not normally considered an obstruction.

</doc>
<doc id="41589" url="https://en.wikipedia.org/wiki?curid=41589" title="Protective distribution system">
Protective distribution system

A protective distribution system (PDS), also called "protected distribution system", is a US government term for wireline or fiber-optics telecommunication system that includes terminals and adequate acoustical, electrical, electromagnetic, and physical safeguards to permit its use for the unencrypted transmission of classified information. At one time these systems were called "approved circuits".
A complete protected distribution system includes the subscriber and terminal equipment and the interconnecting lines.
Description.
The purpose of a PDS is to deter, detect and/or make difficult physical access to the communication lines carrying national security information.
A specification called the National Security Telecommunications and Information Systems Security Instruction (NSTISSI) 7003 was issued in December 1996 by the Committee on National Security Systems.
Approval authority, standards, and guidance for the design, installation, and maintenance for PDS are provided by NSTISSI 7003 to U.S. government departments and agencies and their contractors and vendors. This instruction describes the requirements for all PDS installations within the U.S. and for low and medium threat locations outside the U.S.
PDS is commonly used to protect SIPRNet and JWICS networks.
The document superseded one numbered NASCI 4009 on Protected Distribution Systems, dated December 30, 1981, and part of a document called NACSEM 5203, that covered guidelines for facility design, using the designations "red" and "black".
There are two types of PDS: hardened distribution systems and simple distribution systems. distribution fast as compare to transmission systems due to more load consuming.
Hardened distribution.
Hardened distribution PDSs provide significant physical protection and can be implemented in three forms: hardened carrier PDSs, alarmed carrier PDSs and continuously viewed carrier PDSs.
Hardened carrier.
In a hardened carrier PDS, the data cables are installed in a carrier constructed of electrical metallic tubing (EMT), ferrous conduit or pipe, or ridged sheet steel ducting. All of the connections in a Hardened Carrier System are permanently sealed completely around all surfaces with welds, epoxy or other such sealants. If the hardened carrier is buried under ground, to secure cables running between buildings for example, the carrier containing the cables is encased in concrete.
With a hardened carrier system, detection is accomplished via human inspections that are required to be performed periodically. Therefore, hardened carriers are installed below ceilings or above flooring so they can be visually inspected to ensure that no intrusions have occurred. These periodic visual inspections (PVIs) occur at a frequency dependent upon the level of threat to the environment, the security classification of the data, and the access control to the area.
Alarmed carrier.
As an alternative to conducting human visual inspections, an alarmed carrier PDS may be constructed to automate the inspection process through electronic monitoring with an alarm system. In an Alarmed Carrier PDS, the carrier system is “alarmed” with specialized optical fibers deployed within the conduit for the purpose of sensing acoustic vibrations that usually occur when an intrusion is being attempted on the conduit in order to gain access to the cables.
Alarmed carrier PDS offers several advantages over hardened carrier PDS:
Legacy alarmed carrier Systems monitor the carrier containing the cables being protected. More advanced systems monitor the fibers within, or intrinsic to, the cables being protected to turn those cables into sensors, which detect intrusion attempts.
Depending on the government organization, utilizing an alarmed carrier PDS in conjunction with interlocking armored cable may, in some cases, allow for the elimination of the carrier systems altogether. In these instances, the cables being protected can be installed in existing conveyance (wire basket, ladder rack) or suspended cabling (on D-rings, J-Hooks, etc.).
Continuously viewed carrier.
A Continuously Viewed Carrier PDS is one that is under continuous observation, 24 hours per day (including when operational). Such circuits may be grouped together, but should be separated from all non-continuously viewed circuits ensuring an open field of view. Standing orders should include the requirement to investigate any attempt to disturb the PDS. Appropriate security personnel should investigate the area of attempted penetration within 15 minutes of discovery. This type of hardened carrier is not used for Top Secret or special category information for non-U.S. UAA.
UAA is an Uncontrolled Access Area (UAA). Like definitions include Controlled Access Area (CAA) and Restricted Access Area (RAA). A Secure Room (SR) offers the highest degree of protection.
Therefore from the least protected (least secure) to the most protected is as follows:
UAA
RAA
CAA
SR
Simple distribution.
Simple distribution PDSs are afforded a reduced level of physical security protection as compared to a hardened distribution PDS. They use a simple carrier system and the following means are acceptable under NSTISSI 7003: 

</doc>
<doc id="41590" url="https://en.wikipedia.org/wiki?curid=41590" title="Protocol-control information">
Protocol-control information

In telecommunication, the term protocol-control information or PCI has the following meanings: 

</doc>
<doc id="41591" url="https://en.wikipedia.org/wiki?curid=41591" title="Protocol data unit">
Protocol data unit

In telecommunications, the term protocol data unit (PDU) has the following meanings: 
OSI model.
PDUs are relevant in relation to each of the first 4 layers of the OSI model as follows:
Given a context pertaining to a specific OSI layer, PDU is sometimes used as a synonym for its representation at that layer.
Media access control protocol data unit.
Media access control protocol data unit or MPDU is a message (protocol data unit) exchanged between media access control (MAC) entities in a communication system based on the layered Open Systems Interconnection model.
In systems where the MPDU may be larger than the MAC service data unit (MSDU), the MPDU may include multiple MSDUs as a result of packet aggregation. In systems where the MPDU is smaller than the MSDU, then one MSDU may generate multiple MPDUs as a result of packet segmentation.
Packet-switched data networks.
In the context of packet-switched data networks, a protocol data unit (PDU) is best understood in relation to a service data unit (SDU).
The features or services of the network are implemented in distinct "layers". For example, sending ones and zeros across a wire, fiber, etc. is done by the physical layer, organizing the ones and zeros into chunks of data and getting them safely to the right place on the wire is done by the data link layer, passing data chunks over multiple connected networks is done by the network layer and delivery of the data to the right software application at the destination is done by the transport layer.
Between the layers (and between the application and the top-most layer), the layers pass service data units across the interfaces. The application or higher layer understands the structure of the data in the SDU, but the lower layer at the interface does not; it treats it as payload, undertaking to get it to the same interface at the destination. In order to do this, the protocol layer will add to the SDU certain data it needs to perform its function. For example, it might add a port number to identify the application, a network address to help with routing, a code to identify the type of data in the packet and error-checking information. All this additional information, plus the original service data unit from the higher layer, constitutes the "protocol data unit" at this layer. The significance of this is that the PDU is the structured information that is passed to a matching protocol layer further along on the data's journey that allows the layer to deliver its intended function or service. The matching layer, or "peer", decodes the data to extract the original service data unit, decide if it is error-free and where to send it next, etc. Unless we have already arrived at the lowest (physical) layer, the PDU is passed to the peer using services of the next lower layer in the protocol "stack". When the PDU passes over the interface from the layer that constructed it to the layer that merely delivers it (and therefore does not understand its internal structure), it becomes a service data unit to that layer. The addition of addressing and control information (which is called encapsulation) to an SDU to form a PDU and the passing of that PDU to the next lower layer as an SDU repeats until the lowest layer is reached and the data passes over some medium as a physical signal.
The above process can be likened to the mail system in which a letter (SDU) is placed in an envelope on which is written an address (addressing and control information) making it a PDU. The sending post office might look only at the post code and place the letter in a mail bag so that the address on the envelope can no longer be seen, making it now an SDU. The mail bag is labelled with the destination post code and so becomes a PDU, until it is combined with other bags in a crate, when it is now an SDU, and the crate is labelled with the region to which all the bags are to be sent, making the crate a PDU. When the crate reaches the destination matching its label, it is opened and the bags (SDUs) removed only to become PDUs when someone reads the code of the destination post office. The letters themselves are SDUs when the bags are opened but become PDUs when the address is read for final delivery. When the addressee finally opens the envelope, the top-level SDU, the letter itself, emerges.

</doc>
<doc id="41592" url="https://en.wikipedia.org/wiki?curid=41592" title="Provisioning">
Provisioning

In telecommunication, provisioning involves the process of preparing and equipping a network to allow it to provide (new) services to its users. In National Security/Emergency Preparedness telecommunications services, ""provisioning"" equates to ""initiation"" and includes altering the state of an existing priority service or capability.
The concept of network provisioning or service mediation, mostly used in the telecommunication industry, refers to the provisioning of the customer's services to the network elements. It requires the existence of networking equipment and depends on network planning and design.
In a modern signal infrastructure employing information technology (IT) at all levels, there is no possible distinction between telecommunications services and "higher level" infrastructure. Accordingly, provisioning configures any required systems, provides users with access to data and technology resources, and refers to all enterprise-level information-resource management involved.
Organizationally, a CIO typically manages provisioning, necessarily involving human resources and IT departments cooperating to:
As its core, the provisioning process monitors access rights and privileges to ensure the security of an enterprise's resources and user privacy. As a secondary responsibility, it ensures compliance and minimizes the vulnerability of systems to penetration and abuse. As a tertiary responsibility, it tries to reduce the amount of custom configuration using boot image control and other methods that radically reduce the number of different configurations involved.
Discussion of provisioning often appears in the context of virtualization, orchestration, utility computing, cloud computing, and open-configuration concepts and projects. For instance, the OASIS Provisioning Services Technical Committee (PSTC) defines an XML-based framework for exchanging user, resource, and service-provisioning information - SPML (Service Provisioning Markup Language) for "managing the provisioning and allocation of identity information and system resources within and between organizations".
Once provisioning has taken place, the process of SysOpping ensures the maintenance of services to the expected standards. Provisioning thus refers only to the setup or startup part of the service operation, and SysOpping to the ongoing support.
Network provisioning.
The services which are assigned to the customer in the customer relationship management (CRM) have to be provisioned on the network element which is enabling the service and allows the customer to actually use the service. The relation between a service configured in the CRM and a service on the network elements is not necessarily a 1:1 relation. Some services can be enabled by more than one network element, e.g. the Microsoft Media Server (mms://) service.
During the provisioning, the service mediation device translates the service and the corresponding parameters of the service to one or more services/parameters on the network elements involved.
The algorithm used to translate a system service into network services is called provisioning logic.
Electronic invoice feeds from your carriers can be automatically downloaded directly into the core of the telecom expense management (TEM) software and it will immediately conduct an audit of each single line item charge all the way down to the User Support and Operations Center (USOC) level. The provisioning software will capture each circuit number provided by all of your carriers and if bills outside of the contracted rate an exception rule will trigger a red flag and notify the pre-established staff member to review the billing error.
Server provisioning.
Server provisioning is a set of actions to prepare a server with appropriate systems, data and software, and make it ready for network operation. Typical tasks when provisioning a server are: select a server from a pool of available servers, load the appropriate software (operating system, device drivers, middleware, and applications), appropriately customize and configure the system and the software to create or change a boot image for this server, and then change its parameters, such as IP address, IP Gateway to find associated network and storage resources (sometimes separated as "resource provisioning") to audit the system. By auditing the system, you ensure OVAL compliance with limit vulnerability, ensure compliance, or install patches. After these actions, you restart the system and load the new software. This makes the system ready for operation. Typically an internet service provider (ISP) or Network Operations Center will perform these tasks to a well-defined set of parameters, for example, a boot image that the organization has approved and which uses software it has license to. Many instances of such a boot image create a virtual dedicated host.
There are many software products available to automate the provisioning of servers, services and end-user devices. Examples: BMC Bladelogic Server Automation, HP Server Automation, IBM Tivoli Provisioning Manager, Redhat Kickstart, xCAT, HP Insight CMU, etc. Middleware and applications can be installed either when the operating system is installed or afterwards by using an Application Service Automation tool. Further questions are addressed in academia such as when provisioning should be issued and how many servers are needed in multi-tier, or multi-service applications.
In cloud computing servers may be provisioned via a web user interface or an application programming interface. One of the unique things about cloud computing is how rapidly and easily this can be done. Monitoring software can be used to trigger automatic provisioning when existing resources become too heavily stressed.
In short, server provisioning configures servers based on resource requirements. The use of a hardware or software component (e.g. single/dual processor, RAM, HDD, RAID controller, a number of LAN cards, applications, OS, etc.) depends on the functionality of the server, such as ISP, virtualization, NOS, or voice processing. Server redundancy depends on the availability of servers in the organization. Critical applications have less downtime when using cluster servers, RAID, or a mirroring system.
Service used by most larger scale centers in part to avoid this. Additional resource provisioning may be done per service.
User provisioning.
User provisioning refers to the creation, maintenance and deactivation of user objects and user attributes, as they exist in one or more systems, directories or applications, in response to automated or interactive business processes. User provisioning software may include one or more of the following processes: change propagation, self-service workflow, consolidated user administration, delegated user administration, and federated change control. User objects may represent employees, contractors, vendors, partners, customers or other recipients of a service. Services may include electronic mail, inclusion in a published user directory, access to a database, access to a network or mainframe, etc. User provisioning is a type of identity management software, particularly useful within organizations, where users may be represented by multiple objects on multiple systems and multiple instances.
Self-service provisioning for cloud computing services.
On-demand self-service is described by the National Institute of Standards and Technology (NIST) as an essential characteristic of Cloud computing. The self-service nature of cloud computing lets end users obtain and remove cloud services―including applications, the infrastructure supporting the applications, and configuration― themselves without requiring the assistance of an IT staff member. The automatic self-servicing may target different application goals and constraints (e.g. deadlines and cost), as well as handling different application architectures (e.g., bags-of-tasks and workflows). Cloud users can obtain cloud services through a cloud service catalog or a self-service portal.
However one problem of cloud service provisioning is that it is not instantaneous. A cloud VM can be acquired at any time by the user, but it may take up to several minutes for the acquired VM to be ready to use. The VM startup time is dependent on factors, such as image size, VM type, data center location, number of VMs, etc. Cloud providers have different VM startup performance.
Mobile subscriber provisioning.
Mobile subscriber provisioning refers to the setting up of new services, such as GPRS, MMS and Instant Messaging for an existing subscriber of a mobile phone network, and any gateways to standard Internet chat or mail services. The network operator typically sends these settings to the subscriber's handset using SMS text services or HTML, and less commonly WAP, depending on what the mobile operating systems can accept.
A general example of provisioning is with data services. A mobile user who is using his or her device for voice calling may wish to switch to data services in order to read emails or browse the Internet. The mobile device's services are "provisioned" and thus the user is able to stay connected through push emails and other features of smartphone services.
Device management systems can benefit end-users by incorporating plug and play data services, supporting whatever device the end-user is using.. Such a platform can automatically detect devices in the network, sending them settings for immediate and continued usability. The process is fully automated, keeping the history of used devices and sending settings only to subscriber devices which were not previously set. One method of managing mobile updates is to filter IMEI/IMSI pairs. Some operators report activity of 50 over-the-air settings update files per second. Changed
Mobile content provisioning.
This refers to delivering mobile content, such as mobile internet to a mobile phone, agnostic of the features of said device. These may include operating system type and versions, Java version, browser version, screen form factors, audio capabilities, language settings and a plethora of other characteristics. As of April 2006, an estimated 5000 permutations are relevant. Mobile content provisioning facilitates a common user experience, though delivered on widely different handsets.
Internet access provisioning.
When getting a user / customer online, beyond user provisioning and network provisioning, the client system must be configured. This process may include many steps, depending on the connection technology in question (DSL, Cable, Fibre, etc.). The possible steps are:
There are four approaches to provisioning an internet access:

</doc>
<doc id="41593" url="https://en.wikipedia.org/wiki?curid=41593" title="Pseudorandom noise">
Pseudorandom noise

In cryptography, pseudo random noise (PRN) is a signal similar to noise which satisfies one or more of the standard tests for statistical randomness. 
Although it seems to lack any definite pattern, pseudo random noise consists of a deterministic sequence of pulses that will repeat itself after its period.
In cryptographic devices, the pseudo random noise pattern is determined by a key and the repetition period can be very long, even millions of digits.
Pseudo random noise is used in some electronic musical instruments, either by itself or as an input to subtractive synthesis, and in many white noise machines.
In spread-spectrum systems, the receiver correlates a locally generated signal with the received signal.
Such spread-spectrum systems require a set of one or more "codes" or "sequences" such that
In a direct-sequence spread spectrum system, each bit in the pseudorandom binary sequence is known as a "chip" and the "inverse" of its period as "chip rate"; "compare bit rate and symbol rate."
In a frequency-hopping spread spectrum sequence, each value in the pseudo random sequence is known as a "channel number" and the "inverse" of its period as the "hop rate". FCC Part 15 mandates at least 50 different channels and at least a 2.5 Hz hop rate for narrow band frequency-hopping systems.
GPS satellites broadcast data at a rate of 50 data bits per second – each satellite modulates its data with one PN bit stream at 1.023 million chips per second and the same data with another PN bit stream at 10.23 million chips per second.
GPS receivers correlate the received PN bit stream with a local reference to measure distance. GPS is a receive-only system that uses relative timing measurements from several satellites (and the known positions of the satellites) to determine receiver position.
Other range-finding applications involve two-way transmissions.
A local station generates a pseudo random bit sequence and transmits it to the remote location (using any modulation technique).
Some object at the remote location echoes this PN signal back to the location station – either passively, as in some kinds of radar and sonar systems, or using an active transponder at the remote location, as in the Apollo Unified S-band system.
By correlating a (delayed version of) the transmitted signal with the received signal, a precise round trip time to the remote location can be determined and thus the distance.
PN code.
A pseudo noise code (PN code) or pseudo random noise code (PRN code) is one that has a spectrum similar to a random sequence of bits but is deterministically generated. The most commonly used sequences in direct-sequence spread spectrum systems are maximal length sequences, Gold codes, Kasami codes, and Barker codes.

</doc>
<doc id="41595" url="https://en.wikipedia.org/wiki?curid=41595" title="Psophometer">
Psophometer

In telecommunications, a psophometer is an instrument that provides a visual indication of the audible effects of disturbing voltages of various frequencies. 
A psophometer usually incorporates a weighting network. The characteristics of the weighting network depend on the type of circuit under investigation, such as whether the circuit is used for high-fidelity music or for normal speech.

</doc>
<doc id="41596" url="https://en.wikipedia.org/wiki?curid=41596" title="Psophometric voltage">
Psophometric voltage

Psophometric voltage is a circuit noise voltage measured with a psophometer that includes a CCIF-1951 weighting network. 
""Psophometric voltage"" should not be confused with ""psophometric emf," i.e.", the emf in a generator or line with 600 Ω internal resistance. For practical purposes, the psophometric emf is twice the corresponding psophometric voltage. 
Psophometric voltage readings, "V", in millivolts, are commonly converted to dBm(psoph) by dBm(psoph) = 20 log10"V" – 57.78.

</doc>
<doc id="41597" url="https://en.wikipedia.org/wiki?curid=41597" title="Public data transmission service">
Public data transmission service

In telecommunication, a public data transmission service is a data transmission service that is established and operated by a telecommunication administration, or a recognized private operating agency, and uses a public data network. 
A public data transmission service may include Circuit Switched Data packet-switched, and leased line data transmission.

</doc>
<doc id="41598" url="https://en.wikipedia.org/wiki?curid=41598" title="Public land mobile network">
Public land mobile network

A public land mobile network (PLMN), as defined in telecommunications regulation, is a network that is established and operated by an administration or by a recognized operating agency (ROA) for the specific purpose of providing land mobile telecommunications services to the public.
A PLMN is identified by the Mobile Country Code (MCC) and the Mobile Network Code (MNC). Each operator providing mobile services has its own PLMN. PLMNs interconnect with other PLMNs and Public switched telephone networks (PSTN) for telephone communications or with internet service providers for data and internet access of which links are defined as interconnect links between providers. These links mostly incorporate SDH digital transmission networks via fiber optic on land and digital microwave links.
Access to PLMN services is achieved by means of an "air interface" involving radio communications between mobile phones or other wireless enabled user equipment and land-based radio transmitters or radio base stations or even fiber optic distributed SDH network between mobile base stations and central stations via SDH equipment (ADMs) with integrated IP network services.
Public switched telephone network.
Public Land Mobile Networks need to connect to the Public Switched Telephone Network (PSTN) in order to route calls.
The PSTN is the world's collection of interconnected voice-oriented public telephone networks, in much the same way that the Internet is the concatenation of the world's public IP-based packet-switched networks. It is both commercially- and government-owned. This aggregation of circuit-switching telephone networks has evolved greatly from the days of Alexander Graham Bell, and in the late 20th century became almost entirely digital in nature — except for the final link from the central (local) telephone office to the user (the local loop). It also extends into mobile as well as fixed telephones.
The PSTN also furnishes much of the Internet's long-distance infrastructure and, for the majority of users, the access network as well. Because Internet Service Providers (ISPs) pay the long-distance carriers for access to their infrastructure, and share the circuits among many users through packet switching, the end Internet user avoids having to pay usage tolls to anyone other than their ISP.
The PSTN is largely governed by technical standards created by the ITU-T, and uses E.163/E.164 addresses (usually called telephone numbers) for addressing. A number of large private telephone networks are not connected to the PSTN, and are used for military purposes (such as the Defense Switched Network). There are also private networks run by large companies that are linked to the PSTN, but only through controlled gateways such as private branch exchanges.
Specifications.
A GSM PLMN may be described by a limited set of access interfaces and a limited set of GSM PLMN connection types to support the telecommunication services described in the GSM 02-series of specifications.
PLMN is a network that is established and operated by an administration or by a recognized operating agency (ROA) for the specific purpose of providing land mobile telecommunications services to the public. A PLMN may be considered as an extension of a fixed network, e.g., the Public Switched Telephone Network (PSTN) or as an integral part of the PSTN. This is just one view-point on PLMN. PLMN mostly refers to the whole system of networking hardware and software that enables wireless communication, irrespective of the service area or service provider (cf. Internet backbone). Sometimes a separate PLMN is defined for each country or for each service provider. This systematic ambiguity (of terminological scope) also affects the "PSTN" term. Sometimes it refers to the whole circuit-switched system, while other times it is specific to each country.
PLMN is not a term specific to GSM. In fact, GSM can be treated as an example of a PLMN system. These days (as of January, 2006) many discussions are going on to form the structure of UMTS PLMN for the third-generation systems. Access to PLMN services is achieved by means of an air interface involving radio communications between mobile phones or other wireless-enabled user equipment and land-based radio transmitters or radio base stations. PLMNs interconnect with other PLMNs and PSTNs for telephone communications or with Internet service providers for data and internet access.
A public land mobile network may be defined as a number of mobile services switching center areas within a common numbering plan and a common routing plan. With respect to their functions, the PLMNs may be regarded as independent communications entities, even though different PLMNs may be interconnected through the PSTN/ISDN for the forwarding of calls or network information. The MSCs of a PLMN can be interconnected similarly to allow interaction. A PLMN may have several interfaces with the fixed network (e.g., one for each MSC). Inter-working between two PLMNs may be performed via an international switching center. The PLMN is connected via an NCP to the PSTN/ISDN. If there are two mobile service suppliers in the same country, they can be connected through the same PSTN/ISDN.
Objectives of a GSM PLMN.
The general objective of a PLMN is to facilitate wireless communication and to interlink the wireless network with the fixed wired network. The PLMN was specified by the European Telecommunications Standard Institute (ETSI) following up with their GSM specification. Even as times changed, the GSM PLMN objectives conceptually remained the same.
Architecture.
GSM architecture is basically the PLMN architecture itself as the subject is GSM PLMN. Various interfaces between the GSM subsystems are to be considered, along with the signaling system and the various components (both hardware and software).
Subsystems.
The GSM PLMN is divided into signaling network and mobile network. Each of these has various subsystems, which are grouped under three major systems: the Network and Switching Subsystem (NSS), the Base Station Subsystem (BSS), and the operation and support system (OSS).
Operation and Support System (OSS).
The operations and maintenance center (OMC) is connected to all equipment in the switching system and to the BSC. The implementation of OMC is called the operation and support system (OSS). The OSS is the functional entity from which the network operator monitors and controls the system. The purpose of OSS is to offer the customer cost-effective support for centralized, regional, and local operational and maintenance activities that are required for a GSM network. An important function of OSS is to provide a network overview and support the maintenance activities of different operation and maintenance organizations.
Additional functional elements.
Other functional elements shown are as follows:
There are three viewpoints of interoperability between PLMN and PSTN:
Conclusion.
A PLMN is essential for the effective working of any wireless network, just like the need for PSTN in wireline networks. PLMN facilitates interoperation with its own subsystems in order to perform operation of the GSM system in particular and any wired network in general.

</doc>
<doc id="41599" url="https://en.wikipedia.org/wiki?curid=41599" title="Pulsating direct current">
Pulsating direct current

A pulsating direct current is a type of Direct Current (DC) that changes in value over short periods of time. 
A pulsating direct current may change in value, "i.e.," be always present but at different levels, or it may be interrupted completely. The changes may be irregular or at regular intervals (at a specific frequency), but the current never changes direction.
Pulsating currents are commonly the consequence of using diode rectifiers, or DC sources of lower amplitude connected in series with AC sources. It can be smoother, if a large value capacitor is used in parallel with the rectified source.
Pulsating direct current is used on PWM controllers.
Difference from AC.
Pulsating direct current has an average value equal to a constant (DC) along with a time-dependent pulsating component added to it, while the average value of alternating current is zero in steady state (or a constant if it has a DC offset, value of which will then be equal to that offset). Devices and circuits may respond differently to pulsating DC than they would to non-pulsating DC, such as a battery or regulated power supply and should be evaluated.

</doc>
<doc id="41600" url="https://en.wikipedia.org/wiki?curid=41600" title="Pulse">
Pulse

In medicine, a pulse represents the tactile arterial palpation of the heartbeat by trained fingertips. The pulse may be palpated in any place that allows an artery to be compressed against a bone, such as at the neck (carotid artery), on the inside of the elbow (brachial artery), at the wrist (radial artery), at the groin (femoral artery), behind the knee (popliteal artery), near the ankle joint (posterior tibial artery), and on foot (dorsalis pedis artery). Pulse (or the count of arterial pulse per minute) is equivalent to measuring the heart rate. The heart rate can also be measured by listening to the heart beat directly (auscultation), traditionally using a stethoscope and counting it for a minute. The radial pulse is commonly measured using three fingers. This has a reason: the finger closest to the heart is used to occlude the pulse pressure, the middle finger is used get a crude estimate of the blood pressure, and the finger most distal to the heart (usually the ring finger) is used to nullify the effect of the ulnar pulse as the two arteries are connected via the palmar arches (superficial and deep). 
The study of the pulse is known as sphygmology.
Physiology.
The pulse is a decidedly low tech and high yield and antiquated term still useful at the bedside in an age of computational analysis of cardiac performance. Claudius Galen was perhaps the first physiologist to describe the pulse. The pulse is an expedient tactile method of determination of systolic blood pressure to a trained observer. Diastolic blood pressure is non-palpable and unobservable by tactile methods, occurring between heartbeats.
Pressure waves generated by the heart in systole move the arterial walls. Forward movement of blood occurs when the boundaries are pliable and compliant. These properties form enough to create a palpable pressure wave.
The heart rate may be greater or lesser than the pulse rate depending upon physiologic demand. In this case, the heart rate is determined by auscultation or audible sounds at the heart apex, in which case it is not the pulse. The "pulse deficit" (difference between heart beats and pulsations at the periphery) is determined by simultaneous palpation at the radial artery and auscultation at the heart apex. It may be present in case of premature beats or atrial fibrillation.
Pulse velocity, pulse deficits and much more physiologic data are readily and simplistically visualized by the use of one or more arterial catheters connected to a transducer and oscilloscope. This invasive technique has been commonly used in intensive care since the 1970s.
The rate of the pulse is observed and measured by tactile or visual means on the outside of an artery and is recorded as beats per minute or BPM.
The pulse may be further indirectly observed under light absorbances of varying wavelengths with assigned and inexpensively reproduced mathematical ratios. Applied capture of variances of light signal from the blood component hemoglobin under oxygenated vs. deoxygenated conditions allows the technology of pulse oximetry.
Characteristics of pulse.
Rate.
Normal pulse rates at rest, in beats per minute (BPM):
The pulse rate can be used to check overall heart health and fitness level. Generally lower is better, but bradycardias can be dangerous. Symptoms of a dangerously slow heartbeat include weakness, loss of energy and fainting.
Rhythm.
A normal pulse is regular in rhythm and force. An irregular pulse may be due to sinus arrhythmia, ectopic beats, atrial fibrillation, paroxysmal atrial tachycardia, atrial flutter, partial heart block etc. Intermittent dropping out of beats at pulse is called "intermittent pulse". Examples of "regular" intermittent (regularly irregular) pulse include pulsus bigeminus, second-degree atrioventricular block. An example of "irregular" intermittent (irregularly irregular) pulse is atrial fibrillation.
Volume.
The degree of expansion displayed by artery during diastolic and systolic state is called volume. It is also known as amplitude, expansion or size of pulse.
Hypokinetic pulse.
A weak pulse signifies narrow pulse pressure. It may be due to low cardiac output (as seen in shock, congestive cardiac failure), hypovolemia, valvular heart disease (such as aortic outflow tract obstruction, mitral stenosis, aortic arch syndrome) etc.
Hyperkinetic pulse.
A bounding pulse signifies high pulse pressure. It may be due to low peripheral resistance (as seen in fever, anemia, thyrotoxicosis, , A-V fistula, Paget's disease, beriberi, liver cirrhosis), increased cardiac output, increased stroke volume (as seen in anxiety, exercise, complete heart block, aortic regurgitation), decreased distensibility of arterial system (as seen in atherosclerosis, hypertension and coarctation of aorta).
The strength of the pulse can also be reported:
Force.
Also known as compressibility of pulse. It is a rough measure of systolic blood pressure.
Tension.
It corresponds to diastolic blood pressure. A low tension pulse (pulsus mollis), the vessel is soft or impalpable between beats. In high tension pulse (pulsus durus), vessels feels rigid even between pulse beats.
Form.
A form or contour of a pulse is palpatiory estimation of arteriogram. A quickly rising and quickly falling pulse (pulsus celer) is seen in aortic regurgitation. A slow rising and slowly falling pulse (pulsus tardus) is seen in aortic stenosis.
Equality.
Comparing pulses and different places gives valuable clinical information.
A discrepant or unequal pulse between left and right radial artery is observed in anomalous or aberrant course of artery, coarctation of aorta, aortitis, dissecting aneurysm, peripheral embolism etc. An unequal pulse between upper and lower extremities is seen in coarctation to aorta, aortitis, block at bifurcation of aorta, dissection of aorta, iatrogenic trauma and arteriosclerotic obstruction.
Condition of arterial wall.
A normal artery is not palpable after flattening by digital pressure. A thick radial artery which is palpable 7.5–10 cm up the forearm is suggestive of arteriosclerosis.
Radio-femoral delay.
In coarctation of aorta, femoral pulse may be significantly delayed as compared to radial pulse (unless there is coexisting aortic regurgitation). The delay can also be observed in supravalvar aortic stenosis.
Patterns.
Several pulse patterns can be of clinically significance. These include:
Common palpable sites.
Upper limb.
Chinese medicine has focused on the pulse in the upper limbs for several centuries. The concept of pulse diagnosis is essentially a treatise based upon palpation and observations of the radial and ulnar volar pulses at the readily accessible wrist.
History.
The first person to accurately measure the pulse rate was Santorio Santorii who invented the "pulsilogium", a form of pendulum, based on the work by Galileo Galilei. A century later another physician, de Lacroix, used the pulsilogium to test cardiac function.

</doc>
<doc id="41601" url="https://en.wikipedia.org/wiki?curid=41601" title="Pulse-address multiple access">
Pulse-address multiple access

In telecommunications, pulse-address multiple access (PAMA) is a channel access method that enables the ability of a communication satellite to receive signals from several Earth terminals simultaneously and to amplify, translate, and relay the signals back to Earth, based on the addressing of each station by an assignment of a unique combination of time and frequency slots. 
This ability may be restricted by allowing only some of the terminals access to the satellite at any given time.

</doc>
<doc id="41605" url="https://en.wikipedia.org/wiki?curid=41605" title="Pulse duration">
Pulse duration

In signal processing and telecommunication, pulse duration is the interval between the time, during the first transition, that the amplitude of the pulse reaches a specified fraction (level) of its final amplitude, and the time the pulse amplitude drops, on the last transition, to the same level. 
The interval between the 50% points of the final amplitude is usually used to determine or define pulse duration, and this is understood to be the case unless otherwise specified. Other fractions of the final amplitude, e.g., 90% or 1/e, may also be used, as may the root mean square (rms) value of the pulse amplitude. 
In radar, the pulse duration is the time the radar's transmitter is energized during each cycle.

</doc>
<doc id="41606" url="https://en.wikipedia.org/wiki?curid=41606" title="Pulse link repeater">
Pulse link repeater

In telecommunications, a pulse link repeater (PLR) is a device that interfaces concatenated E and M signaling paths. 
A PLR converts a ground, received from the E lead of one signal path, to −48 VDC, which is applied to the M lead of the concatenated signal path. 
In many commercial carrier systems, the channel bank cards or modules have a "PLR" option that permits the direct concatenation of E&M signaling paths, without the need for separate PLR equipment.

</doc>
<doc id="41607" url="https://en.wikipedia.org/wiki?curid=41607" title="Pulsing">
Pulsing

Pulsing may refer to:

</doc>
<doc id="41608" url="https://en.wikipedia.org/wiki?curid=41608" title="Pumping">
Pumping

Pumping can refer to:

</doc>
<doc id="41610" url="https://en.wikipedia.org/wiki?curid=41610" title="Push-to-type operation">
Push-to-type operation

Push-to-type operation: In telegraph or data transmission systems, that method of communication in which the operator at a station must keep a switch operated in order to send messages. 
Push-to-type operation is used in radio systems where the same frequency is employed for transmission and reception. 
Push-to-type operation is a derivative form of transmission and may be used in simplex, half-duplex, or duplex operation. "Synonym" press-to-type operation.
This is similar to Push-to-talk operation for radio phone communications.

</doc>
<doc id="41611" url="https://en.wikipedia.org/wiki?curid=41611" title="Quadrature">
Quadrature

Quadrature may refer to:
In signal processing:
In mathematics:
In physics:
In astronomy:
Quadrature may also refer to:

</doc>
<doc id="41612" url="https://en.wikipedia.org/wiki?curid=41612" title="Quadruply clad fiber">
Quadruply clad fiber

In fiber optics, a quadruply clad fiber is a single-mode optical fiber that has four claddings. Each cladding has a refractive index lower than that of the core. With respect to one another, their relative refractive indices are, in order of distance from the core: lowest, highest, lower, higher. 
A quadruply clad fiber has the advantage of very low macrobending losses. It also has two zero-dispersion points, and moderately low dispersion over a wider wavelength range than a singly clad fiber or a doubly clad fiber.

</doc>
<doc id="41613" url="https://en.wikipedia.org/wiki?curid=41613" title="Quality control">
Quality control

Quality control, or QC for short, is a process by which entities review the quality of all factors involved in production. ISO 9000 defines quality control as "A part of quality management focused on fulfilling quality requirements".
This approach places an emphasis on three aspects:
Controls include product inspection, where every product is examined visually, and often using a stereo microscope for fine detail before the product is sold into the external market. Inspectors will be provided with lists and descriptions of unacceptable product defects such as cracks or surface blemishes for example.
The quality of the outputs is at risk if any of these three aspects is deficient in any way.
Quality control emphasizes testing of products to uncover defects and reporting to management who make the decision to allow or deny product release, whereas quality assurance attempts to improve and stabilize production (and associated processes) to avoid, or at least minimize, issues which led to the defect(s) in the first place. For contract work, particularly work awarded by government agencies, quality control issues are among the top reasons for not renewing a contract.
Notable approaches to quality control.
There is a tendency for individual consultants and organizations to name their own unique approaches to quality control—a few of these have ended up in widespread use:
Quality control in project management.
In project management, quality control requires the project manager and the project team to inspect the accomplished work to ensure its alignment with the project scope. In practice, projects typically have a dedicated quality control team which focuses on this area.

</doc>
<doc id="41615" url="https://en.wikipedia.org/wiki?curid=41615" title="Quasi-analog signal">
Quasi-analog signal

In telecommunication, a quasi-analog signal is a digital signal that has been converted to a form suitable for transmission over a specified analog channel. 
The specification of the analog channel should include frequency range, bandwidth, signal-to-noise ratio, and envelope delay distortion. When quasi-analog form of signaling is used to convey message traffic over dial-up telephone systems, it is often referred to as voice-data. A modem may be used for the conversion process.

</doc>
