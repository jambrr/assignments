<doc id="47526" url="https://en.wikipedia.org/wiki?curid=47526" title="Convection">
Convection

Convection is the concerted, collective movement of groups or aggregates of molecules within fluids (e.g., liquids, gases) and rheids, through advection or through diffusion or as a combination of both of them. Convection of mass cannot take place in solids, since neither bulk current flows nor significant diffusion can take place in solids. Diffusion of heat can take place in solids, but that is called heat conduction. Convection can be demonstrated by placing a heat source (e.g. a Bunsen burner) at the side of a glass full of a liquid, and observing the changes in temperature in the glass caused by the warmer ghost fluid moving into cooler areas.
Convective heat transfer is one of the major types of heat transfer, and convection is also a major mode of mass transfer in fluids. Convective heat and mass transfer take place both by diffusion – the random Brownian motion of individual particles in the fluid – and by advection, in which matter or heat is transported by the larger-scale motion of currents in the fluid. In the context of heat and mass transfer, the term "convection" is used to refer to the sum of advective and diffusive transfer. In common use the term "convection" may refer loosely to heat transfer by convection, as opposed to mass transfer by convection, or the convection process in general. Sometimes "convection" is even used to refer specifically to "free heat convection" (natural heat convection) as opposed to forced heat convection. However, in mechanics the correct use of the word is the general sense, and different types of convection should be qualified for clarity.
Convection can be qualified in terms of being natural, forced, gravitational, granular, or thermomagnetic. It may also be said to be due to combustion, capillary action, or Marangoni and Weissenberg effects. Heat transfer by natural convection plays a role in the structure of Earth's atmosphere, its oceans, and its mantle. Discrete convective cells in the atmosphere can be seen as clouds, with stronger convection resulting in thunderstorms. Natural convection also plays a role in stellar physics.
Terminology.
The word "convection" may have slightly different but related usages in different scientific or engineering contexts or applications. The broader sense is in fluid mechanics, where "convection" refers to the motion of fluid regardless of cause. However, in thermodynamics "convection" often refers specifically to heat transfer by convection.
Additionally, convection includes fluid movement both by bulk motion (advection) and by the motion of individual particles (diffusion). However, in some cases, convection is taken to mean only advective phenomena. For instance, in the transport equation, which describes a number of different transport phenomena, terms are separated into "convective" and "diffusive" effects, with "convective" meaning purely advective in context.
Examples and applications of convection.
Convection occurs on a large scale in atmospheres, oceans, planetary mantles, and it provides the mechanism of heat transfer for a large fraction of the outermost interiors of our sun and all stars. Fluid movement during convection may be invisibly slow, or it may be obvious and rapid, as in a hurricane. On astronomical scales, convection of gas and dust is thought to occur in the accretion disks of black holes, at speeds which may closely approach that of light.
Heat transfer.
Convective heat transfer is a mechanism of heat transfer occurring because of bulk motion (observable movement) of fluids. Heat is the entity of interest being advected (carried), and diffused (dispersed). This can be contrasted with conductive heat transfer, which is the transfer of energy by vibrations at a molecular level through a solid or fluid, and radiative heat transfer, the transfer of energy through electromagnetic waves.
Heat is transferred by convection in numerous examples of naturally occurring fluid flow, such as: wind, oceanic currents, and movements within the Earth's mantle. Convection is also used in engineering practices of homes, industrial processes, cooling of equipment, etc.
The rate of convective heat transfer may be improved by the use of a heat sink, often in conjunction with a fan. For instance, a typical computer CPU will have a purpose-made fan to ensure its operating temperature is kept within tolerable limits.
Convection cells.
A convection cell, also known as a Bénard cell is a characteristic fluid flow pattern in many convection systems. A rising body of fluid typically loses heat because it encounters a cold surface. In liquid this occurs because it exchanges heat with colder liquid through direct exchange. In the example of the Earth's atmosphere, this occurs because it radiates heat. Because of this heat loss the fluid becomes denser than the fluid underneath it, which is still rising. Since it cannot descend through the rising fluid, it moves to one side. At some distance, its downward force overcomes the rising force beneath it, and the fluid begins to descend. As it descends, it warms again and the cycle repeats itself.
Atmospheric circulation.
Atmospheric circulation is the large-scale movement of air, and is a means by which thermal energy is distributed on the surface of the Earth, together with the much slower (lagged) ocean circulation system. The large-scale structure of the atmospheric circulation varies from year to year, but the basic climatological structure remains fairly constant.
Latitudinal circulation occurs because incident solar radiation per unit area is highest at the heat equator, and decreases as the latitude increases, reaching minima at the poles. It consists of two primary convection cells, the Hadley cell and the polar vortex, with the Hadley cell experiencing stronger convection due to the release of latent heat energy by condensation of water vapor at higher altitudes during cloud formation.
Longitudinal circulation, on the other hand, comes about because the ocean has a higher specific heat capacity than land (and also thermal conductivity, allowing the heat to penetrate further beneath the surface) and thereby absorbs and releases more heat, but the temperature changes less than land. This brings the sea breeze, air cooled by the water, ashore in the day, and carries the land breeze, air cooled by contact with the ground, out to sea during the night. Longitudinal circulation consists of two cells, the Walker circulation and El Niño / Southern Oscillation.
Weather.
Some more localized phenomena than global atmospheric movement are also due to convection, including wind and some of the hydrologic cycle. For example, a foehn wind is a down-slope wind which occurs on the downwind side of a mountain range. It results from the adiabatic warming of air which has dropped most of its moisture on windward slopes. Because of the different adiabatic lapse rates of moist and dry air, the air on the leeward slopes becomes warmer than at the same height on the windward slopes.
A thermal column (or thermal) is a vertical section of rising air in the lower altitudes of the Earth's atmosphere. Thermals are created by the uneven heating of the Earth's surface from solar radiation. The Sun warms the ground, which in turn warms the air directly above it. The warmer air expands, becoming less dense than the surrounding air mass, and creating a thermal low. The mass of lighter air rises, and as it does, it cools by expansion at lower air pressures. It stops rising when it has cooled to the same temperature as the surrounding air. Associated with a thermal is a downward flow surrounding the thermal column. The downward moving exterior is caused by colder air being displaced at the top of the thermal. Another convection-driven weather effect is the sea breeze.
Warm air has a lower density than cool air, so warm air rises within cooler air, similar to hot air balloons. Clouds form as relatively warmer air carrying moisture rises within cooler air. As the moist air rises, it cools, causing some of the water vapor in the rising packet of air to condense. When the moisture condenses, it releases energy known as latent heat of fusion which allows the rising packet of air to cool less than its surrounding air, continuing the cloud's ascension. If enough instability is present in the atmosphere, this process will continue long enough for cumulonimbus clouds to form, which support lightning and thunder. Generally, thunderstorms require three conditions to form: moisture, an unstable airmass, and a lifting force (heat).
All thunderstorms, regardless of type, go through three stages: the developing stage, the mature stage, and the dissipation stage. The average thunderstorm has a diameter. Depending on the conditions present in the atmosphere, these three stages take an average of 30 minutes to go through.
Oceanic circulation.
Solar radiation affects the oceans: warm water from the Equator tends to circulate toward the poles, while cold polar water heads towards the Equator. The surface currents are initially dictated by surface wind conditions. The trade winds blow westward in the tropics, and the westerlies blow eastward at mid-latitudes. This wind pattern applies a stress to the subtropical ocean surface with negative curl across the Northern Hemisphere, and the reverse across the Southern Hemisphere. The resulting Sverdrup transport is equatorward. Because of conservation of potential vorticity caused by the poleward-moving winds on the subtropical ridge's western periphery and the increased relative vorticity of poleward moving water, transport is balanced by a narrow, accelerating poleward current, which flows along the western boundary of the ocean basin, outweighing the effects of friction with the cold western boundary current which originates from high latitudes. The overall process, known as western intensification, causes currents on the western boundary of an ocean basin to be stronger than those on the eastern boundary.
As it travels poleward, warm water transported by strong warm water current undergoes evaporative cooling. The cooling is wind driven: wind moving over water cools the water and also causes evaporation, leaving a saltier brine. In this process, the water becomes saltier and denser. and decreases in temperature. Once sea ice forms, salts are left out of the ice, a process known as brine exclusion. These two processes produce water that is denser and colder (or, more precisely, water that is still liquid at a lower temperature). The water across the northern Atlantic ocean becomes so dense that it begins to sink down through less salty and less dense water. (The convective action is not unlike that of a lava lamp.) This downdraft of heavy, cold and dense water becomes a part of the North Atlantic Deep Water, a southgoing stream.
Mantle convection.
Mantle convection is the slow creeping motion of Earth's rocky mantle caused by convection currents carrying heat from the interior of the earth to the surface. It is one of 3 driving forces that causes tectonic plates to move around the Earth's surface.
The Earth's surface is divided into a number of tectonic plates that are continuously being created and consumed at their opposite plate boundaries. Creation (accretion) occurs as mantle is added to the growing edges of a plate. This hot added material cools down by conduction and convection of heat. At the consumption edges of the plate, the material has thermally contracted to become dense, and it sinks under its own weight in the process of subduction at an ocean trench. This subducted material sinks to some depth in the Earth's interior where it is prohibited from sinking further. The subducted oceanic crust triggers volcanism.
Stack effect.
The Stack effect or chimney effect is the movement of air into and out of buildings, chimneys, flue gas stacks, or other containers due to buoyancy. Buoyancy occurs due to a difference in indoor-to-outdoor air density resulting from temperature and moisture differences. The greater the thermal difference and the height of the structure, the greater the buoyancy force, and thus the stack effect. The stack effect helps drive natural ventilation and infiltration. Some cooling towers operate on this principle; similarly the solar updraft tower is a proposed device to generate electricity based on the stack effect.
Stellar physics.
The convection zone of a star is the range of radii in which energy is transported primarily by convection.
Granules on the photosphere of the Sun are the visible tops of convection cells in the photosphere, caused by convection of plasma in the photosphere. The rising part of the granules is located in the center where the plasma is hotter. The outer edge of the granules is darker due to the cooler descending plasma. A typical granule has a diameter on the order of 1,000 kilometers and each lasts 8 to 20 minutes before dissipating. Below the photosphere is a layer of much larger "supergranules" up to 30,000 kilometers in diameter, with lifespans of up to 24 hours.
Convection mechanisms.
Convection may happen in fluids at all scales larger than a few atoms. There are a variety of circumstances in which the forces required for natural and forced convection arise, leading to different types of convection, described below. In broad terms, convection arises because of body forces acting within the fluid, such as gravity (buoyancy), or surface forces acting at a boundary of the fluid.
The causes of convection are generally described as one of either "natural" ("free") or "forced", although other mechanisms also exist (discussed below). However the distinction between natural and forced convection is particularly important for convective heat transfer.
Natural convection.
Natural convection, or free convection, occurs due to temperature differences which affect the density, and thus relative buoyancy, of the fluid. Heavier (more dense) components will fall, while lighter (less dense) components rise, leading to bulk fluid movement. Natural convection can only occur, therefore, in a gravitational field. A common example of natural convection is the rise of smoke from a fire. It can be seen in a pot of boiling water in which the hot and less-dense water on the bottom layer moves upwards in plumes, and the cool and more dense water near the top of the pot likewise sinks.
Natural convection will be more likely and/or more rapid with a greater variation in density between the two fluids, a larger acceleration due to gravity that drives the convection, and/or a larger distance through the convecting medium. Natural convection will be less likely and/or less rapid with more rapid diffusion (thereby diffusing away the thermal gradient that is causing the convection) and/or a more viscous (sticky) fluid.
The onset of natural convection can be determined by the Rayleigh number (Ra).
Note that differences in buoyancy within a fluid can arise for reasons other than temperature variations, in which case the fluid motion is called gravitational convection (see below). However, all types of buoyant convection, including natural convection, do not occur in microgravity environments. All require the presence of an environment which experiences g-force (proper acceleration).
Forced convection.
In forced convection, also called heat advection, fluid movement results from external surface forces such as a fan or pump. Forced convection is typically used to increase the rate of heat exchange. Many types of mixing also utilize forced convection to distribute one substance within another. Forced convection also occurs as a by-product to other processes, such as the action of a propeller in a fluid or aerodynamic heating. Fluid radiator systems, and also heating and cooling of parts of the body by blood circulation, are other familiar examples of forced convection.
Forced convection may happen by natural means, such as when the heat of a fire causes expansion of air and bulk air flow by this means. In microgravity, such flow (which happens in all directions) along with diffusion is the only means by which fires are able to draw in fresh oxygen to maintain themselves. The shock wave that transfers heat and mass out of explosions is also a type of forced convection.
Although forced convection from thermal gas expansion in zero-g does not fuel a fire as well as natural convection in a gravity field, some types of artificial forced convection are far more efficient than free convection, as they are not limited by natural mechanisms. For instance, a convection oven works by forced convection, as a fan which rapidly circulates hot air forces heat into food faster than would naturally happen due to simple heating without the fan.
Gravitational or buoyant convection.
Gravitational convection is a type of natural convection induced by buoyancy variations resulting from material properties other than temperature. Typically this is caused by a variable composition of the fluid. If the varying property is a concentration gradient, it is known as solutal convection. For example, gravitational convection can be seen in the diffusion of a source of dry salt downward into wet soil due to the buoyancy of fresh water in saline.
Variable salinity in water and variable water content in air masses are frequent causes of convection in the oceans and atmosphere which do not involve heat, or else involve additional compositional density factors other than the density changes from thermal expansion (see "thermohaline circulation"). Similarly, variable composition within the Earth's interior which has not yet achieved maximal stability and minimal energy (in other words, with densest parts deepest) continues to cause a fraction of the convection of fluid rock and molten metal within the Earth's interior (see below).
Gravitational convection, like natural thermal convection, also requires a g-force environment in order to occur.
Granular convection.
Vibration-induced convection occurs in powders and granulated materials in containers subject to vibration where an axis of vibration is parallel to the force of gravity. When the container accelerates upward, the bottom of the container pushes the entire contents upward. In contrast, when the container accelerates downward, the sides of the container push the adjacent material downward by friction, but the material more remote from the sides is less affected. The net result is a slow circulation of particles downward at the sides, and upward in the middle.
If the container contains particles of different sizes, the downward-moving region at the sides is often narrower than the largest particles. Thus, larger particles tend to become sorted to the top of such a mixture. This is one possible explanation of the Brazil nut effect.
Thermomagnetic convection.
Thermomagnetic convection can occur when an external magnetic field is imposed on a ferrofluid with varying magnetic susceptibility. In the presence of a temperature gradient this results in a nonuniform magnetic body force, which leads to fluid movement. A ferrofluid is a liquid which becomes strongly magnetized in the presence of a magnetic field.
This form of heat transfer can be useful for cases where conventional convection fails to provide adequate heat transfer, e.g., in miniature microscale devices or under reduced gravity conditions.
Capillary action.
Capillary action is a phenomenon where liquid spontaneously rises in a narrow space such as a thin tube, or in porous materials. This effect can cause liquids to flow against the force of gravity. It occurs because of inter-molecular attractive forces between the liquid and solid surrounding surfaces; If the diameter of the tube is sufficiently small, then the combination of surface tension and forces of adhesion between the liquid and container act to lift the liquid.
Marangoni effect.
The Marangoni effect is the convection of fluid along an interface between dissimilar substances because of variations in surface tension. Surface tension can vary because of inhomogeneous composition of the substances, and/or the temperature-dependence of surface tension forces. In the latter case the effect is known as thermo-capillary convection.
A well-known phenomenon exhibiting this type of convection is the "tears of wine".
Weissenberg effect.
The Weissenberg effect is a phenomenon that occurs when a spinning rod is placed into a solution of liquid polymer. Entanglements cause the polymer chains to be drawn towards the rod instead of being thrown outward as would happen with an ordinary fluid (i.e., water).
Combustion.
In a zero-gravity environment, there can be no buoyancy forces, and thus no natural (free) convection possible, so flames in many circumstances without gravity smother in their own waste gases. However, flames may be maintained with any type of forced convection (breeze); or (in high oxygen environments in "still" gas environments) entirely from the minimal forced convection that occurs as heat-induced "expansion" (not buoyancy) of gases allows for ventilation of the flame, as waste gases move outward and cool, and fresh high-oxygen gas moves in to take up the low pressure zones created when flame-exhaust water condenses.
Mathematical models of convection.
Mathematically, convection can be described by the convection–diffusion equation, also known as the generic scalar transport equation.
Quantifying natural versus forced convection.
In cases of mixed convection (natural and forced occurring together) one would often like to know how much of the convection is due to external constraints, such as the fluid velocity in the pump, and how much is due to natural convection occurring in the system.
The relative magnitudes of the Grashof and Reynolds numbers squared determine which form of convection dominates. If formula_1, forced convection may be neglected, whereas if formula_2, natural convection may be neglected. If the ratio is approximately one, then both forced and natural convection need to be taken into account.

</doc>
<doc id="47527" url="https://en.wikipedia.org/wiki?curid=47527" title="Cryosphere">
Cryosphere

The cryosphere (from the Greek "kryos", "cold", "frost" or "ice" and "sphaira", "globe, ball") is those portions of Earth's surface where water is in solid form, including sea ice, lake ice, river ice, snow cover, glaciers, ice caps, ice sheets, and frozen ground (which includes permafrost). Thus, there is a wide overlap with the hydrosphere. The cryosphere is an integral part of the global climate system with important linkages and feedbacks generated through its influence on surface energy and moisture fluxes, clouds, precipitation, hydrology, atmospheric and oceanic circulation. Through these feedback processes, the cryosphere plays a significant role in the global climate and in climate model response to global changes. The term deglaciation describes the retreat of cryospheric features. Cryology is the study of cryospheres.
Structure.
Frozen water is found on the Earth’s surface primarily as snow cover, freshwater ice in lakes and rivers, sea ice, glaciers, ice sheets, and frozen ground and permafrost (permanently frozen ground). The residence time of water in each of these cryospheric sub-systems varies widely. Snow cover and freshwater ice are essentially seasonal, and most sea ice, except for ice in the central Arctic, lasts only a few years if it is not seasonal. A given water particle in glaciers, ice sheets, or ground ice, however, may remain frozen for 10-100,000 years or longer, and deep ice in parts of East Antarctica may have an age approaching 1 million years.
Most of the world’s ice volume is in Antarctica, principally in the East Antarctic Ice Sheet. In terms of areal extent, however, Northern Hemisphere winter snow and ice extent comprise the largest area, amounting to an average 23% of hemispheric surface area in January. The large areal extent and the important climatic roles of snow and ice, related to their unique physical properties, indicate that the ability to observe and model snow and ice-cover extent, thickness, and physical properties (radiative and thermal properties) is of particular significance for climate research.
There are several fundamental physical properties of snow and ice that modulate energy exchanges between the surface and the atmosphere. The most important properties are the surface reflectance (albedo), the ability to transfer heat (thermal diffusivity), and the ability to change state (latent heat). These physical properties, together with surface roughness, emissivity, and dielectric characteristics, have important implications for observing snow and ice from space. For example, surface roughness is often the dominant factor determining the strength of radar backscatter . Physical properties such as crystal structure, density, length, and liquid water content are important factors affecting the transfers of heat and water and the scattering of microwave energy.
The surface reflectance of incoming solar radiation is important for the surface energy balance (SEB). It is the ratio of reflected to incident solar radiation, commonly referred to as albedo. Climatologists are primarily interested in albedo integrated over the shortwave portion of the electromagnetic spectrum (~300 to 3500 nm), which coincides with the main solar energy input. Typically, albedo values for non-melting snow-covered surfaces are high (~80-90%) except in the case of forests. The higher albedos for snow and ice cause rapid shifts in surface reflectivity in autumn and spring in high latitudes, but the overall climatic significance of this increase is spatially and temporally modulated by cloud cover. (Planetary albedo is determined principally by cloud cover, and by the small amount of total solar radiation received in high latitudes during winter months.) Summer and autumn are times of high-average cloudiness over the Arctic Ocean so the albedo feedback associated with the large seasonal changes in sea-ice extent is greatly reduced. Groisman "et al." (1994a) observed that snow cover exhibited the greatest influence on the Earth radiative balance in the spring (April to May) period when incoming solar radiation was greatest over snow-covered areas.
The thermal properties of cryospheric elements also have important climatic consequences. Snow and ice have much lower thermal diffusivities than air. Thermal diffusivity is a measure of the speed at which temperature waves can penetrate a substance. Snow and ice are many orders of magnitude less efficient at diffusing heat than air. Snow cover insulates the ground surface, and sea ice insulates the underlying ocean, decoupling the surface-atmosphere interface with respect to both heat and moisture fluxes. The flux of moisture from a water surface is eliminated by even a thin skin of ice, whereas the flux of heat through thin ice continues to be substantial until it attains a thickness in excess of 30 to 40 cm. However, even a small amount of snow on top of the ice will dramatically reduce the heat flux and slow down the rate of ice growth. The insulating effect of snow also has major implications for the hydrological cycle. In non-permafrost regions, the insulating effect of snow is such that only near-surface ground freezes and deep-water drainage is uninterrupted.
While snow and ice act to insulate the surface from large energy losses in winter, they also act to retard warming in the spring and summer because of the large amount of energy required to melt ice (the latent heat of fusion, 3.34 x 105 J/kg at 0 °C). However, the strong static stability of the atmosphere over areas of extensive snow or ice tends to confine the immediate cooling effect to a relatively shallow layer, so that associated atmospheric anomalies are usually short-lived and local to regional in scale. In some areas of the world such as Eurasia, however, the cooling associated with a heavy snowpack and moist spring soils is known to play a role in modulating the summer monsoon circulation. Gutzler and Preston (1997) recently presented evidence for a similar snow-summer circulation feedback over the southwestern United States.
The role of snow cover in modulating the monsoon is just one example of a short-term cryosphere-climate feedback involving the land surface and the atmosphere. From Figure 1 it can be seen that there are numerous cryosphere-climate feedbacks in the global climate system. These operate over a wide range of spatial and temporal scales from local seasonal cooling of air temperatures to hemispheric-scale variations in ice sheets over time-scales of thousands of years. The feedback mechanisms involved are often complex and incompletely understood. For example, Curry "et al." (1995) showed that the so-called “simple” sea ice-albedo feedback involved complex interactions with lead fraction, melt ponds, ice thickness, snow cover, and sea-ice extent.
Snow.
Snow cover has the second-largest areal extent of any component of the cryosphere, with a mean maximum areal extent of approximately 47 million km². Most of the Earth’s snow-covered area (SCA) is located in the Northern Hemisphere, and temporal variability is dominated by the seasonal cycle; Northern Hemisphere snow-cover extent ranges from 46.5 million km² in January to 3.8 million km² in August. North American winter SCA has exhibited an increasing trend over much of this century (Brown and Goodison 1996; Hughes "et al." 1996) largely in response to an increase in precipitation. However, the available satellite data show that the hemispheric winter snow cover has exhibited little interannual variability over the 1972-1996 period, with a coefficient of variation (COV=s.d./mean) for January Northern Hemisphere snow cover of < 0.04. According to Groisman "et al." (1994a) Northern Hemisphere spring snow cover should exhibit a decreasing trend to explain an observed increase in Northern Hemisphere spring air temperatures this century. Preliminary estimates of SCA from historical and reconstructed in situ snow-cover data suggest this is the case for Eurasia, but not for North America, where spring snow cover has remained close to current levels over most of this century. Because of the close relationship observed between hemispheric air temperature and snow-cover extent over the period of satellite data (IPCC 1996), there is considerable interest in monitoring Northern Hemisphere snow-cover extent for detecting and monitoring climate change.
Snow cover is an extremely important storage component in the water balance, especially seasonal snowpacks in mountainous areas of the world. Though limited in extent, seasonal snowpacks in the Earth’s mountain ranges account for the major source of the runoff for stream flow and groundwater recharge over wide areas of the midlatitudes. For example, over 85% of the annual runoff from the Colorado River basin originates as snowmelt. Snowmelt runoff from the Earth’s mountains fills the rivers and recharges the aquifers that over a billion people depend on for their water resources. Further, over 40% of the world’s protected areas are in mountains, attesting to their value both as unique ecosystems needing protection and as recreation areas for humans. Climate warming is expected to result in major changes to the partitioning of snow and rainfall, and to the timing of snowmelt, which will have important implications for water use and management. These changes also involve potentially important decadal and longer time-scale feedbacks to the climate system through temporal and spatial changes in soil moisture and runoff to the oceans.(Walsh 1995). Freshwater fluxes from the snow cover into the marine environment may be important, as the total flux is probably of the same magnitude as desalinated ridging and rubble areas of sea ice. In addition, there is an associated pulse of precipitated pollutants which accumulate over the Arctic winter in snowfall and are released into the ocean upon ablation of the sea-ice .
Sea ice.
Sea ice covers much of the polar oceans and forms by freezing of sea water. Satellite data since the early 1970s reveal considerable seasonal, regional, and interannual variability in the sea-ice covers of both hemispheres. Seasonally, sea-ice extent in the Southern Hemisphere varies by a factor of 5, from a minimum of 3-4 million km² in February to a maximum of 17-20 million km² in September. The seasonal variation is much less in the Northern Hemisphere where the confined nature and high latitudes of the Arctic Ocean result in a much larger perennial ice cover, and the surrounding land limits the equatorward extent of wintertime ice. Thus, the seasonal variability in Northern Hemisphere ice extent varies by only a factor of 2, from a minimum of 7-9 million km² in September to a maximum of 14-16 million km² in March.
The ice cover exhibits much greater regional-scale interannual variability than it does hemispherical. For instance, in the region of the Sea of Okhotsk and Japan, maximum ice extent decreased from 1.3 million km² in 1983 to 0.85 million km² in 1984, a decrease of 35%, before rebounding the following year to 1.2 million km² . The regional fluctuations in both hemispheres are such that for any several-year period of the satellite record some regions exhibit decreasing ice coverage while others exhibit increasing ice cover. The overall trend indicated in the passive microwave record from 1978 through mid-1995 shows that the extent of Arctic sea ice is decreasing 2.7% per decade. Subsequent work with the satellite passive-microwave data indicates that from late October 1978 through the end of 1996 the extent of Arctic sea ice decreased by 2.9% per decade while the extent of Antarctic sea ice increased by 1.3% per decade. The Intergovernmental Panel on Climate Change publication "Climate change 2013: The Physical Science Basis" stated that sea ice extent for the Northern Hemisphere showed a decrease of 3.8% ± 0.3% per decade from November 1978 to December 2012.
Lake ice and river ice.
Ice forms on rivers and lakes in response to seasonal cooling. The sizes of the ice bodies involved are too small to exert other than localized climatic effects. However, the freeze-up/break-up processes respond to large-scale and local weather factors, such that considerable interannual variability exists in the dates of appearance and disappearance of the ice. Long series of lake-ice observations can serve as a proxy climate record, and the monitoring of freeze-up and break-up trends may provide a convenient integrated and seasonally specific index of climatic perturbations. Information on river-ice conditions is less useful as a climatic proxy because ice formation is strongly dependent on river-flow regime, which is affected by precipitation, snow melt, and watershed runoff as well as being subject to human interference that directly modifies channel flow, or that indirectly affects the runoff via land-use practices.
Lake freeze-up depends on the heat storage in the lake and therefore on its depth, the rate and temperature of any inflow, and water-air energy fluxes. Information on lake depth is often unavailable, although some indication of the depth of shallow lakes in the Arctic can be obtained from airborne radar imagery during late winter (Sellman "et al." 1975) and spaceborne optical imagery during summer (Duguay and Lafleur 1997). The timing of breakup is modified by snow depth on the ice as well as by ice thickness and freshwater inflow.
Frozen ground and permafrost.
Frozen ground (permafrost and seasonally frozen ground) occupies approximately 54 million km² of the exposed land areas of the Northern Hemisphere (Zhang et al., 2003) and therefore has the largest areal extent of any component of the cryosphere. Permafrost (perennially frozen ground) may occur where mean annual air temperatures (MAAT) are less than -1 or -2 °C and is generally continuous where MAAT are less than -7 °C. In addition, its extent and thickness are affected by ground moisture content, vegetation cover, winter snow depth, and aspect. The global extent of permafrost is still not completely known, but it underlies approximately 20% of Northern Hemisphere land areas. Thicknesses exceed 600 m along the Arctic coast of northeastern Siberia and Alaska, but, toward the margins, permafrost becomes thinner and horizontally discontinuous. The marginal zones will be more immediately subject to any melting caused by a warming trend. Most of the presently existing permafrost formed during previous colder conditions and is therefore relic. However, permafrost may form under present-day polar climates where glaciers retreat or land emergence exposes unfrozen ground. Washburn (1973) concluded that most continuous permafrost is in balance with the present climate at its upper surface, but changes at the base depend on the present climate and geothermal heat flow; in contrast, most discontinuous permafrost is probably unstable or "in such delicate equilibrium that the slightest climatic or surface change will have drastic disequilibrium effects".
Under warming conditions, the increasing depth of the summer active layer has significant impacts on the hydrologic and geomorphic regimes. Thawing and retreat of permafrost have been reported in the upper Mackenzie Valley and along the southern margin of its occurrence in Manitoba, but such observations are not readily quantified and generalized. Based on average latitudinal gradients of air temperature, an average northward displacement of the southern permafrost boundary by 50-to-150 km could be expected, under equilibrium conditions, for a 1 °C warming.
Only a fraction of the permafrost zone consists of actual ground ice. The remainder (dry permafrost) is simply soil or rock at subfreezing temperatures. The ice volume is generally greatest in the uppermost permafrost layers and mainly comprises pore and segregated ice in Earth material. Measurements of bore-hole temperatures in permafrost can be used as indicators of net changes in temperature regime. Gold and Lachenbruch (1973) infer a 2-4 °C warming over 75 to 100 years at Cape Thompson, Alaska, where the upper 25% of the 400-m thick permafrost is unstable with respect to an equilibrium profile of temperature with depth (for the present mean annual surface temperature of -5 °C). Maritime influences may have biased this estimate, however. At Prudhoe Bay similar data imply a 1.8 °C warming over the last 100 years (Lachenbruch "et al." 1982). Further complications may be introduced by changes in snow-cover depths and the natural or artificial disturbance of the surface vegetation.
The potential rates of permafrost thawing have been established by Osterkamp (1984) to be two centuries or less for 25-meter-thick permafrost in the discontinuous zone of interior Alaska, assuming warming from -0.4 to 0 °C in 3–4 years, followed by a further 2.6 °C rise. Although the response of permafrost (depth) to temperature change is typically a very slow process (Osterkamp 1984; Koster 1993), there is ample evidence for the fact that the active layer thickness quickly responds to a temperature change (Kane "et al." 1991). Whether, under a warming or cooling scenario, global climate change will have a significant effect on the duration of frost-free periods in both regions with seasonally and perennially frozen ground.
Glaciers and ice sheets.
Ice sheets and glaciers are flowing ice masses that rest on solid land. They are controlled by snow accumulation, surface and basal melt, calving into surrounding oceans or lakes and internal dynamics. The latter results from gravity-driven creep flow ("glacial flow") within the ice body and sliding on the underlying land, which leads to thinning and horizontal spreading. Any imbalance of this dynamic equilibrium between mass gain, loss and transport due to flow results in either growing or shrinking ice bodies.
Ice sheets are the greatest potential source of global freshwater, holding approximately 77% of the global total. This corresponds to 80 m of world sea-level equivalent, with Antarctica accounting for 90% of this. Greenland accounts for most of the remaining 10%, with other ice bodies and glaciers accounting for less than 0.5%. Because of their size in relation to annual rates of snow accumulation and melt, the residence time of water in ice sheets can extend to 100,000 or 1 million years. Consequently, any climatic perturbations produce slow responses, occurring over glacial and interglacial periods. Valley glaciers respond rapidly to climatic fluctuations with typical response times of 10–50 years. However, the response of individual glaciers may be asynchronous to the same climatic forcing because of differences in glacier length, elevation, slope, and speed of motion. Oerlemans (1994) provided evidence of coherent global glacier retreat which could be explained by a linear warming trend of 0.66 °C per 100 years.
While glacier variations are likely to have minimal effects upon global climate, their recession may have contributed one third to one half of the observed 20th Century rise in sea level (Meier 1984; IPCC 1996). Furthermore, it is extremely likely that such extensive glacier recession as is currently observed in the Western Cordillera of North America, where runoff from glacierized basins is used for irrigation and hydropower, involves significant hydrological and ecosystem impacts. Effective water-resource planning and impact mitigation in such areas depends upon developing a sophisticated knowledge of the status of glacier ice and the mechanisms that cause it to change. Furthermore, a clear understanding of the mechanisms at work is crucial to interpreting the global-change signals that are contained in the time series of glacier mass balance records.
Combined glacier mass balance estimates of the large ice sheets carry an uncertainty of about 20%. Studies based on estimated snowfall and mass output tend to indicate that the ice sheets are near balance or taking some water out of the oceans. Marinebased studies suggest sea-level rise from the Antarctic or rapid ice-shelf basal melting. Some authors (Paterson 1993; Alley 1997) have suggested that the difference between the observed rate of sea-level rise (roughly 2 mm/y) and the explained rate of sea-level rise from melting of mountain glaciers, thermal expansion of the ocean, etc. (roughly 1 mm/y or less) is similar to the modeled imbalance in the Antarctic (roughly 1 mm/y of sea-level rise; Huybrechts 1990), suggesting a contribution of sea-level rise from the Antarctic.
Relationships between global climate and changes in ice extent are complex. The mass balance of land-based glaciers and ice sheets is determined by the accumulation of snow, mostly in winter, and warm-season ablation due primarily to net radiation and turbulent heat fluxes to melting ice and snow from warm-air advection,(Munro 1990). However, most of Antarctica never experiences surface melting. Where ice masses terminate in the ocean, iceberg calving is the major contributor to mass loss. In this situation, the ice margin may extend out into deep water as a floating ice shelf, such as that in the Ross Sea. Despite the possibility that global warming could result in losses to the Greenland ice sheet being offset by gains to the Antarctic ice sheet, there is major concern about the possibility of a West Antarctic Ice Sheet collapse. The West Antarctic Ice Sheet is grounded on bedrock below sea level, and its collapse has the potential of raising the world sea level 6–7 m over a few hundred years.
Most of the discharge of the West Antarctic Ice Sheet is via the five major ice streams (faster flowing ice) entering the Ross Ice Shelf, the Rutford Ice Stream entering Ronne-Filchner shelf of the Weddell Sea, and the Thwaites Glacier and Pine Island Glacier entering the Amundsen Ice Shelf. Opinions differ as to the present mass balance of these systems (Bentley 1983, 1985), principally because of the limited data. The West Antarctic Ice Sheet is stable so long as the Ross Ice Shelf is constrained by drag along its lateral boundaries and pinned by local grounding.

</doc>
<doc id="47530" url="https://en.wikipedia.org/wiki?curid=47530" title="Cumulonimbus cloud">
Cumulonimbus cloud

Cumulonimbus, from the Latin cumulus ("heap") and nimbus ("rainstorm", "storm cloud"), is a dense towering vertical cloud associated with thunderstorms and atmospheric instability, forming from water vapor carried by powerful upward air currents. If observed during a storm, these clouds may be referred to as thunderheads. Cumulonimbus can form alone, in clusters, or along cold front squall lines. These clouds are capable of producing lightning and other dangerous severe weather, such as tornadoes. Cumulonimbus progress from overdeveloped cumulus congestus clouds and may further develop as part of a supercell. Cumulonimbus is abbreviated Cb and are designated in the D2 family.
Appearance.
Towering cumulonimbus clouds are typically accompanied by smaller cumulus clouds. The cumulonimbus base may extend several miles across and occupy low to middle altitudes- formed at altitude from approximately . Peaks typically reach to as much as , with extreme instances as high as . Well-developed cumulonimbus clouds are characterized by a flat, anvil-like top (anvil dome), caused by wind shear or inversion near the tropopause. The shelf of the anvil may precede the main cloud's vertical component for many miles, and be accompanied by lightning. Occasionally, rising air parcels surpass the equilibrium level (due to momentum) and form an overshooting top culminating at the maximum parcel level. When vertically developed, this largest of all clouds usually extends through all three cloud regions. Even the smallest cumulonimbus cloud dwarfs its neighbors in comparison.
Effects.
Cumulonimbus storm cells can produce torrential rain of a convective nature (often in the form of a rain shaft) and flash flooding, as well as straight-line winds. Most storm cells die after about 20 minutes, when the precipitation causes more downdraft than updraft, causing the energy to dissipate. If there is enough solar energy in the atmosphere, however (on a hot summer's day, for example), the moisture from one storm cell can evaporate rapidly—resulting in a new cell forming just a few miles from the former one. This can cause thunderstorms to last for several hours. Cumulonimbus clouds can also bring dangerous winter storms (called "blizzards") which bring lightning, thunder, and torrential snow. However, cumulonimbus clouds are most common in tropical regions.
Life cycle or stages.
In general, cumulonimbus require moisture, an unstable air mass, and a lifting force (heat) in order to form. Cumulonimbus typically go through three stages: the developing stage, the mature stage (where the main cloud may reach supercell status in favorable conditions), and the dissipation stage. The average thunderstorm has a diameter. Depending on the conditions present in the atmosphere, these three stages take an average of 30 minutes to go through.
Cloud types.
Clouds form when the dewpoint of water is reached in the presence of condensation nuclei in the troposphere. The atmosphere is a dynamic system, and the local conditions of turbulence, uplift and other parameters give rise to many types of clouds. Various types of cloud occur frequently enough to have been categorized. Furthermore, some atmospheric processes can make the clouds organize in distinct patterns such as wave clouds or actinoform clouds. These are large-scale structures and are not always readily identifiable from single point of view.

</doc>
<doc id="47532" url="https://en.wikipedia.org/wiki?curid=47532" title="Cumulus cloud">
Cumulus cloud

Cumulus clouds are often precursors of other types of cloud, such as cumulonimbus, when influenced by weather factors such as instability, moisture, and temperature gradient. Normally, cumulus clouds produce little or no precipitation, but they can grow into the precipitation-bearing congestus or cumulonimbus clouds. Cumulus clouds can be formed from water vapor, supercooled water droplets, or ice crystals, depending upon the ambient temperature. They come in many distinct subforms, and generally cool the earth by reflecting the incoming solar radiation. Cumulus clouds are part of the larger category of free-convective cumuliform clouds, which include cumulonimbus clouds. The latter genus-type is sometimes categorized separately as cumulonimbiform due to its more complex structure that often includes a cirriform or anvil top. There are also cumuliform clouds of limited convection that comprise stratocumulus (low-étage), altocumulus (middle-étage) and cirrocumulus. (high-étage). These last three genus-types are sometimes classified separately as stratocumuliform.
Formation.
Cumulus clouds form via atmospheric convection as air warmed by the surface begins to rise. As the air rises, the temperature drops (following the lapse rate), causing the relative humidity (RH) to rise. If convection reaches a certain level the RH reaches one hundred percent, and the "wet-adiabatic" phase begins. At this point a positive feedback ensues: since the RH is above 100%, water vapour condenses, releasing latent heat, warming the air and spurring further convection.
In this phase, water vapor condenses on various nuclei present in the air, forming the cumulus cloud. This creates the characteristic flat-bottomed puffy shape associated with cumulus clouds. The size of the cloud depends on the temperature profile of the atmosphere and the presence of any inversions. During the convection, surrounding air is entrained (mixed) with the thermal and the total mass of the ascending air increases.
Rain forms in a cumulus cloud via a process involving two non-discrete stages. The first stage occurs after the droplets coalesce onto the various nuclei. Langmuir writes that surface tension in the water droplets provides a slightly higher pressure on the droplet, raising the vapor pressure by a small amount. The increased pressure results in those droplets evaporating and the resulting water vapor condensing on the larger droplets. Due to the extremely small size of the evaporating water droplets, this process becomes largely meaningless after the larger droplets have grown to around 20 to 30 micrometres, and the second stage takes over. In the accretion phase, the raindrop begins to fall, and other droplets collide and combine with it to increase the size of the raindrop. Langmuir was able to develop a formula which predicted that the droplet radius would grow unboundedly within a discrete time period.
Description.
The liquid water density within a cumulus cloud has been found to change with height above the cloud base rather than being approximately constant throughout the cloud. At the cloud base, the concentration was 0 grams of liquid water per kilogram of air. As altitude increased, the concentration rapidly increased to the maximum concentration near the middle of the cloud. The maximum concentration was found to be anything up to 1.25 grams of water per kilogram of air. The concentration slowly dropped off as altitude increased to the height of the top of the cloud, where it immediately dropped to zero again.
Cumulus clouds can form in lines stretching over long called cloud streets. These cloud streets cover vast areas and may be broken or continuous. They form when wind shear causes horizontal circulation in the atmosphere, producing the long, tubular cloud streets. They generally form during high-pressure systems, such as after a cold front.
The height at which the cloud forms depends on the amount of moisture in the thermal that forms the cloud. Humid air will generally result in a lower cloud base. In temperate areas, the base of the cumulus clouds is usually below above ground level, but it can range up to in altitude. In arid and mountainous areas, the cloud base can be in excess of .
Cumulus clouds can be composed of ice crystals, water droplets, supercooled water droplets, or a mixture of them. The water droplets form when water vapor condenses on the nuclei, and they may then coalesce into larger and larger droplets. In temperate regions, the cloud bases studied ranged from above ground level. These clouds were normally above , and the concentration of droplets ranged from 23 to 1300 droplets per cubic centimeter (380 to 21,300 droplets per cubic inch). This data was taken from growing isolated cumulus clouds that were not precipitating. The droplets were very small, ranging down to around 5 micrometers in diameter. Although smaller droplets may have been present, the measurements were not sensitive enough to detect them. The smallest droplets were found in the lower portions of the clouds, with the percentage of large droplets (around 20 to 30 micrometers) rising dramatically in the upper regions of the cloud. The droplet size distribution was slightly bimodal in nature, with peaks at the small and large droplet sizes and a slight trough in the intermediate size range. The skew was roughly neutral. Furthermore, large droplet size is roughly inversely proportional to the droplet concentration per unit volume of air. In places, cumulus clouds can have "holes" where there are no water droplets. These can occur when winds tear the cloud and incorporate the environmental air or when strong downdrafts evaporate the water.
Subforms.
Cumulus clouds come in four distinct species, "cumulis humilis", "mediocris", "congestus", and "fractus". These species may be arranged into the variety, "cumulus radiatus"; and may be accompanied by up to seven supplementary features, "cumulus pileus", "velum", "virga", "praecipitatio", "arcus", "pannus", and "tuba".
The species "Cumulus fractus" is ragged in appearance and can form in clear air as a precursor to cumulus humilis and larger cumulus species-types; or it can form in precipitation as the supplementary feature "pannus" (also called scud) which can also include stratus fractus of bad weather. "Cumulus humilis" clouds look like puffy, flattened shapes. "Cumulus mediocris" clouds look similar, except that they have some vertical development. "Cumulus congestus" clouds have a cauliflower-like structure and tower high into the atmosphere, hence their alternate name "towering cumulus". The variety "Cumulus radiatus" forms in radial bands called cloud streets and can comprise any of the four species of cumulus.
Cumulus supplementary features are most commonly seen with the species congestus. "Cumulus virga" clouds are cumulus clouds producing virga (precipitation that evaporates while aloft), and "cumulus praecipitatio" produce precipitation that reaches the Earth's surface. "Cumulus pannus" comprise shredded clouds that normally appear beneath the parent cumulus cloud during precipitation. "Cumulus arcus" clouds have a gust front, and "cumulus tuba" clouds have funnel clouds or tornadoes. "Cumulus pileus" clouds refer to cumulus clouds that have grown so rapidly as to force the formation of pileus over the top of the cloud. "Cumulus velum" clouds have an ice crystal veil over the growing top of the cloud.
Forecast.
Cumulus humilis clouds usually indicate fair weather. Cumulus mediocris clouds are similar, except that they have some vertical development, which implies that they can grow into cumulus congestus or even cumulonimbus clouds, which can produce heavy rain, lightning, severe winds, hail, and even tornadoes. Cumulus congestus clouds, which appear as towers, will often grow into cumulonimbus storm clouds. They can produce precipitation. Glider pilots often pay close attention to cumulus clouds, as they can be indicators of rising air drafts or thermals underneath that can suck the plane high into the sky—a phenomenon known as cloud suck.
Cumulus clouds can also produce acid rain or possibly a tornado. The acidity is largely formed by the oxidation of sulfur dioxide, the most plentiful acidifying gas, into sulfate ions. The main oxidizing compounds are hydrogen peroxide and ozone. Various nitrogen oxides can also react with hydroxide ions to form acids.
Effects on climate.
Due to reflectivity, clouds cool the earth by around , an effect largely caused by stratocumulus clouds. However, at the same time, they heat the earth by around by reflecting emitted radiation, an effect largely caused by cirrus clouds. This averages out to a net loss of . Cumulus clouds, on the other hand, have a variable effect on heating the earth's surface. The more vertical "cumulus congestus" species and cumulonimbus genus of clouds grow high into the atmosphere, carrying moisture with them, which can lead to the formation of cirrus clouds. The researchers speculated that this might even produce a positive feedback, where the increasing upper atmospheric moisture further warms the earth, resulting in an increasing number of "cumulus congestus" clouds carrying more moisture into the upper atmosphere.
Relation to other clouds.
Cumulus clouds are a genus of free-convective low-étage cloud along with the related limited-convective cumuliform or stratocumuliform cloud stratocumulus. These clouds form from ground level to at all latitudes. Stratus clouds are also low-étage. In the middle étage are the alto clouds, which consist of the limiited-convective cumuliform or stratocumuliform cloud altocumulus and the stratiform cloud altostratus. Middle-étage clouds form from to in polar areas, in temperate areas, and in tropical areas. The high-étage clouds are all cirriform, one of which, cirrocumulus, is also cumuliform of limited convection or stratocumuliform. The other clouds in this étage are cirrus and cirrostratus. High-étage clouds form in high latitudes, in temperate latitudes, and in low, tropical latitudes. Cumulonimbus clouds, like cumulus congestus, extend vertically rather than remaining confined to one étage.
Cirrocumulus clouds.
Cirrocumulus clouds form in patches and cannot cast shadows. They commonly appear in regular, rippling patterns or in rows of clouds with clear areas between. Cirrocumulus are, like other members of the cumuliform and stratocumuliform categories, formed via convective processes. Significant growth of these patches indicates high-altitude instability and can signal the approach of poorer weather. The ice crystals in the bottoms of cirrocumulus clouds tend to be in the form of hexagonal cylinders. They are not solid, but instead tend to have stepped funnels coming in from the ends. Towards the top of the cloud, these crystals have a tendency to clump together. These clouds do not last long, and they tend to change into cirrus because as the water vapor continues to deposit on the ice crystals, they eventually begin to fall, destroying the upward convection. The cloud then dissipates into cirrus. Cirrocumulus clouds come in four species which are common to all three genus-types that have limited-convective or stratocumuliform characteristics: "stratiformis", "lenticularis", "castellanus", and "floccus". They are iridescent when the constituent supercooled water droplets are all about the same size.
Altocumulus clouds.
Altocumulus clouds are a middle-étage cloud that forms from high to in polar areas, in temperate areas, and in tropical areas. They can have precipitation and are commonly composed of a mixture of ice crystals, supercooled water droplets, and water droplets in temperate latitudes. However, the liquid water concentration was almost always significantly greater than the concentration of ice crystals, and the maximum concentration of liquid water tended to be at the top of the cloud while the ice concentrated itself at the bottom. The ice crystals in the base of the altocumulus clouds and in the virga were found to be dendrites or conglomerations of dendrites while needles and plates resided more towards the top. Altocumulus clouds can form via convection or via the forced uplift caused by a warm front. Because Altocumulus is a genus-type of limited convection, it is divided into the same four species as cirrocumulus.
Stratocumulus clouds.
A stratocumulus cloud is another type of a cumuliform or stratocumuliform cloud. Like cumulus clouds, they form at low levels and via convection. However, unlike cumulus clouds, their growth is almost completely retarded by a strong inversion. As a result, they flatten out like stratus clouds, giving them a layered appearance. These clouds are extremely common, covering on average around twenty-three percent of the earth's oceans and twelve percent of the earth's continents. They are less common in tropical areas and commonly form after cold fronts. Additionally, stratocumulus clouds reflect a large amount of the incoming sunlight, producing a net cooling effect. Stratocumulus clouds can produce drizzle, which stabilizes the cloud by warming it and reducing turbulent mixing. Being a cloud of limited convection, stratocumulus is divided into three species; stratiformis, lenticularis, and castellanus, that are common to the higher stratocumuliform genus-types.
Cumulonimbus clouds.
Cumulonimbus clouds are the final form of growing cumulus clouds. They form when "cumulus congestus" clouds develop a strong updraft that propels their tops higher and higher into the atmosphere until they reach the tropopause at in altitude. Cumulonimbus clouds, commonly called thunderheads, can produce high winds, torrential rain, lightning, gust fronts, waterspouts, funnel clouds, and tornadoes. They commonly have anvil clouds.
Extraterrestrial.
Some cumuliform clouds have been discovered on most other planets in the solar system. On Mars, the Viking Orbiter detected cirrocumulus and stratocumulus clouds forming via convection primarily near the polar icecaps. The Galileo space probe detected massive cumulonimbus clouds near the Great Red Spot on Jupiter. Cumuliform clouds have also been detected on Saturn. In 2008, the Cassini spacecraft determined that cumulus clouds near Saturn's south pole were part of a cyclone over in diameter. The Keck Observatory detected whitish cumulus clouds on Uranus. Like Uranus, Neptune has methane cumulus clouds. Venus, however, does not appear to have cumulus clouds.

</doc>
<doc id="47535" url="https://en.wikipedia.org/wiki?curid=47535" title="Haptophyte">
Haptophyte

The haptophytes, classified either as the Prymnesiophyta (named for "Prymnesium") or Haptophyta, are a division of algae.
The names Haptophyceae or Prymnesiophyceae are sometimes used instead. This ending implies classification at the class 
rank rather than as a division. Although the phylogenetics of this group has become much better understood in recent years, there remains some dispute over which rank is most appropriate.
Characteristics.
The chloroplasts are pigmented similarly to those of the heterokonts, but the structure of the rest of the cell is different, so it may be that they are a separate line whose chloroplasts are derived from similar red algal endosymbionts.
The cells typically have two slightly unequal flagella, both of which are smooth, and a unique organelle called a "haptonema", which is superficially similar to a flagellum but differs in the arrangement of microtubules and in its use. The name comes from the Greek "hapsis", touch, and "nema", thread. The mitochondria have tubular cristae.
Economic importance.
Haptophytes are economically important as "Pavlova lutheri" and "Isochrysis sp." are widely used in the aquaculture industries.
Examples and classification.
The haptophytes were first placed in the class Chrysophyceae (golden algae) but ultrastructural data have provided evidence to classify them separately. The best-known haptophytes are coccolithophores, which have an exoskeleton of calcareous plates called coccoliths. Coccolithophores are some of the most abundant marine phytoplankton, especially in the open ocean and are extremely abundant as microfossils. Other planktonic haptophytes of note include "Chrysochromulina" and "Prymnesium", which periodically form toxic marine algal blooms, and "Phaeocystis" blooms of which can produce unpleasant foam which often accumulates on beaches. Both molecular and morphological evidence supports their division into five orders; coccolithophores make up the Isochrysidales and Coccolithales. Very small (2-3μm) uncultured pico-prymnesiophytes are ecologically important
Haptophytes was discussed to be closely related to cryptomonads.
Haptophytes are closely related to the SAR clade.

</doc>
<doc id="47537" url="https://en.wikipedia.org/wiki?curid=47537" title="Configuration">
Configuration

Configuration may refer to:
In computing:
In physics:
Other uses:

</doc>
<doc id="47541" url="https://en.wikipedia.org/wiki?curid=47541" title="United Nations Security Council Resolution 242">
United Nations Security Council Resolution 242

United Nations Security Council Resolution 242 (S/RES/242) was adopted unanimously by the UN Security Council on November 22, 1967, in the aftermath of the Six-Day War. It was adopted under Chapter VI of the UN Charter. The resolution was sponsored by British ambassador Lord Caradon and was one of five drafts under consideration.
The preamble refers to the "inadmissibility of the acquisition of territory by war and the need to work for a just and lasting peace in the Middle East in which every State in the area can live in security."
Operative Paragraph One "Affirms that the fulfillment of Charter principles requires the establishment of a just and lasting peace in the Middle East which should include the application of both the following principles:
Egypt, Jordan, Israel and Lebanon entered into consultations with the UN Special representative over the implementation of 242. After denouncing it in 1967, Syria "conditionally" accepted the resolution in March 1972. Syria formally accepted UN Security Council Resolution 338, the cease-fire at the end of the Yom Kippur War (in 1973), which embraced resolution 242. 
On 1 May 1968, the Israeli ambassador to the UN expressed Israel's position to the Security Council: "My government has indicated its acceptance of the Security Council resolution for the promotion of agreement on the establishment of a just and lasting peace. I am also authorized to reaffirm that we are willing to seek agreement with each Arab State on all matters included in that resolution."
In a statement to the General Assembly on 15 October 1968, the PLO rejected Resolution 242, saying "the implementation of said resolution will lead to the loss of every hope for the establishment of peace and security in Palestine and the Middle East region." In September 1993, the PLO agreed that Resolutions 242 and 338 should be the basis for negotiations with Israel when it signed the Declaration of Principles.
Resolution 242 is one of the most widely affirmed resolutions on the Arab–Israeli conflict and formed the basis for later negotiations between the parties. These led to Peace Treaties between Israel and Egypt (1979) and Jordan (1994), as well as the 1993 and 1995 agreements with the Palestinians.
Context.
The resolution is the formula proposed by the Security Council for the successful resolution of the Arab-Israeli conflict, in particular, ending the state of belligerency then existing between the 'States concerned', Israel and Egypt, Jordan, Syria and Lebanon. The resolution deals with five principles; withdrawal of Israeli forces, 'peace within secure and recognized boundaries', freedom of navigation, a just settlement of the refugee problem and security measures including demilitarized zones. It also provided for the appointment of a Special Representative to proceed to the Middle East in order to promote agreement on a peaceful and accepted settlement in accordance with the principles outlined in the resolution.
Upon presenting the draft resolution to the Security Council, the U.K. representative Lord Caradon said:
Secretary of State Dean Rusk commented on the most significant area of disagreement regarding the resolution:There was much bickering over whether that resolution should say from "the" territories or from "all" territories. In the French version, which is equally authentic, it says withdrawal de territory, with de meaning "the." We wanted that to be left a little vague and subject to future negotiation because we thought the Israeli border along the West Bank could be "rationalized"; certain anomalies could easily be straightened out with some exchanges of territory, making a more sensible border for all parties. We also wanted to leave open demilitarization measures in the Sinai and the Golan Heights and take a fresh look at the old city of Jerusalem. But we never contemplated any significant grant of territory to Israel as a result of the June 1967 war. On that point we and the Israelis to this day remain sharply divided. This situation could lead to real trouble in the future. Although every President since Harry Truman has committed the United States to the security and independence of Israel, I'm not aware of any commitment the United States has made to assist Israel in retaining territories seized in the Six-Day War.
A memorandum from the President's Special Assistant, Walt Rostow, to President Johnson said: "What's on the Arab Ambassadors' minds boils down to one big question: Will we make good on our pledge to support the territorial integrity of all states in the Middle East? Our best answer is that we stand by that pledge, but the only way to make good on it is to have a genuine peace. The tough question is whether we'd force Israel back to 4 June borders if the Arabs accepted terms that amounted to an honest peace settlement. Secretary Rusk told the Yugoslav Foreign Minister: 'The US had no problem with frontiers as they existed before the outbreak of hostilities. If we are talking about national frontiers--in a state of peace--then we will work toward restoring them.' But we all know that could lead to a tangle with the Israelis."
Rusk met with Foreign Minister Nikezic on August 30, 1967. However, according to telegram 30825 to Belgrade, September 1, which summarizes the conversation, Rusk said the key to a settlement was to end the state of war and belligerence and that if a way could be found to deal with this, other things would fall into place; the difference between pre-June 5 positions and secure national boundaries was an important difference.
President Johnson responded to a complaint from President Tito that Israel could change the frontiers without Arab consent: "You note that the Arabs feel the US interprets the draft resolution to imply a change of frontiers to their detriment. We have no preconceptions on frontiers as such. What we believe to be important is that the frontiers be secure. For this the single most vital condition is that they be acceptable to both sides. It is a source of regret to us that the Arabs appear to misunderstand our proposal and misread our motives."
Furthermore, Secretary Rusk's Telegram dated March 2, 1968 to the U.S. Interests Section of the Spanish Embassy in Cairo summarizing Undersecretary of State for Political Affairs Eugene Rostow’s conversation with Soviet Ambassador Anatoly Dobrynin states:
In an address delivered on September 1, 1982 President Ronald Reagan said:
According to Michael Lynk, there are three schools of thought concerning the proper legal interpretation of the withdrawal phrase. Some of the parties involved have suggested that the indefinite language is a “perceptible loophole”, that authorizes “territorial revision” for Israel’s benefit. Some have stated that the indefinite language was used to permit insubstantial and mutually beneficial alterations to the 1949 armistices lines, but that unilateral annexation of the captured territory was never authorized. Other parties have said that no final settlement obtained through force or the threat of force could be considered valid. They insist that the Security Council cannot create loopholes in peremptory norms of international law or the UN Charter, and that any use of indefinite language has to be interpreted in line with the overriding legal principles regarding the “inadmissibility of the acquisition of territory by war” and the prohibitions on mass deportations or displacement in connection with the settlement of the refugee problem.
Alexander Orakhelashvili says that the Security Council manifestly lacks the competence to validate agreements imposed through coercion, not least because the peremptory prohibition of the use of force is a limitation on the Council’s powers and the voidness of coercively imposed treaties is the clear consequence of jus cogens and the conventional law as reflected in the Vienna Convention on the Law of Treaties. A recent South African study concluded that the ultimate status and boundaries will require negotiation between the parties, according to Security Council Resolutions 242 and 338. The same study also found that the provisions of the Fourth Geneva Convention which govern ‘special agreements’ that can adversely affect the rights of protected persons precludes any change in status of the territory obtained through an agreement concluded during a state of belligerent occupation.
Content.
Preamble.
The second preambular reference states: 
John McHugo says that by the 1920s, international law no longer recognized that a state could acquire title to territory by conquest. Article 2 of the Charter of the United Nations requires all members to refrain in their international relations from the threat or use of force against the territorial integrity or political independence of any state, or in any other manner inconsistent with the purposes of the United Nations.
Michael Lynk says that article 2 of the Charter embodied a prevailing legal principle that there could be "no title by conquest". He says that principle had been expressed through numerous international conferences, doctrines and treaties since the late 19th Century. Lynk cites the examples of the First International Conference of American States in 1890; the United States Stimson Doctrine of 1932; the 1932 League of Nations resolution on Japanese aggression in China; the Buenos Aires Declaration of 1936; and the Atlantic Charter of 1941. Surya Sharma says that a war in self-defense cannot result in acquisition of title by conquest. He says that even if a war is lawful in origin it cannot exceed the limits of legitimate self-defense.
Land for peace.
The resolution also calls for the implementation of the "land for peace" formula, calling for Israeli withdrawal from "territories" it had occupied in 1967 in exchange for peace with its neighbors. This was an important advance at the time, considering that there were no peace treaties between any Arab state and Israel until the Israel-Egypt Peace Treaty of 1979. "Land for peace" served as the basis of the Israel-Egypt Peace Treaty, in which Israel withdrew from the Sinai peninsula (Egypt withdrew its claims to the Gaza Strip in favor of the Palestine Liberation Organization). Jordan renounced its claims regarding the West Bank in favor of the Palestine Liberation Organization, and has signed the Israel-Jordan Treaty of Peace in 1994, that established the Jordan River as the boundary of Jordan.
Throughout the 1990s, there were Israeli-Syrian negotiations regarding a normalization of relations and an Israeli withdrawal from the Golan Heights. But a peace treaty was not made, mainly due to Syria's desire to recover and retain 25 square kilometers of territory in the Jordan River Valley which it seized in 1948 and occupied until 1967. As the United Nations recognizes only the 1948 borders, there is little support for the Syrian position outside the Arab bloc nor in resolving the Golan Heights issue.
The UN resolution does not specifically mention the Palestinians. The United Kingdom had recognized the union between the West Bank and Transjordan. Lord Caradon said that the parties assumed that withdrawal from occupied territories as provided in the resolution was applicable to East Jerusalem. "Nevertheless so important is the future of Jerusalem that it might be argued that we should have specifically dealt with that issue in the 1967 Resolution. It is easy to say that now, but I am quite sure that if we had attempted to raise or settle the question of Jerusalem as a separate issue at that time our task in attempting to find a unanimous decision would have been far greater if not impossible."
Judge Higgins of the International Court of Justice explained "from Security Council resolution 242 (1967) through to Security Council Resolution 1515 (2003), the key underlying requirements have remained the same - that Israel is entitled to exist, to be recognized, and to security, and that the Palestinian people are entitled to their territory, to exercise self-determination, and to have their own State. Security Council resolution 1515 (2003)
envisages that these long-standing obligations are to be secured (...) by negotiation"
Secretary of State Madeleine Albright told the U.N. Security Council: "We simply do not support the description of the territories occupied by Israel in 1967 as 'Occupied Palestinian Territory'. In the view of my Government, this language could be taken to indicate sovereignty, a matter which both Israel and the PLO have agreed must be decided in negotiations on the final status of the territories. "Had this language appeared in the operative paragraphs of the resolution, let me be clear: we would have exercised our veto. In fact, we are today voting against a resolution in the Commission on the Status of Women precisely because it implies that Jerusalem is "occupied Palestinian territory".
The Palestinians were represented by the Palestine Liberation Organization in negotiations leading to the Oslo Accords. They envisioned a 'permanent settlement based on Security Council Resolution 242'. The main premise of the Oslo Accords was the eventual creation of Palestinian autonomy in some or all of the territories captured during the Six-Day War, in return for Palestinian recognition of Israel. However, the Foreign Minister of the Palestinian Authority, Nabil Shaath, said: "Whether a state is announced now or after liberation, its borders must be those of 4 June 1967. We will not accept a state without borders or with borders based on UN Resolution 242, which we believe is no longer suitable. On the contrary, Resolution 242 has come to be used by Israel as a way to procrastinate."
The Security Council subsequently adopted resolution 1515 (2003), which recalled resolution 242 and endorsed the Middle East Quartet’s Road Map towards a permanent, two-State solution to the Israeli-Palestinian conflict. The Quartet Plan calls for direct, bilateral negotiations as part of a comprehensive resolution of the Arab-Israeli conflict, on the basis of UN Security Council Resolutions 242, 338, 1397, 1515, 1850, and the Madrid principles. The Quartet has reiterated that the only viable solution to the Israeli-Palestinian conflict is an agreement that ends the occupation that began in 1967; resolves all permanent status issues as previously defined by the parties; and fulfils the aspirations of both parties for independent homelands through two states for two peoples, Israel and an independent, contiguous and viable state of Palestine, living side by side in peace and security.
On April 14, 2004, US President George W. Bush said to Israeli Prime Minister Ariel Sharon, "The United States reiterates its steadfast commitment to Israel's security, including secure, defensible borders." Israeli officials argue that the pre-1967 armistice line is not a defensible border, since Israel would be nine miles wide at the thinnest point, subjected to rocket fire from the highlands of the West Bank, and unable to stop smuggling from Jordan across the Jordan Valley. Thus, Israeli officials have been arguing for the final-status borders to be readjusted to reflect security concerns.
Resolution 1860 (2009) recalled resolution 242 and stressed that the Gaza Strip constitutes an integral part of the territory occupied in 1967 that will be a part of the Palestinian state.
Settlement of the refugee problem.
The resolution advocates a "just settlement of the refugee problem". Lord Caradon said "It has been said that in the Resolution we treated Palestinians only as refugees, but this is unjustified. We provided that Israel should withdraw from occupied territories and it was together with that requirement for a restoration of Arab territory that we also called for a settlement of the refugee problem." Upon the adoption of Resolution 242, French President Charles de Gaulle stressed this principle during a press conference on November 27, 1967 and confirmed it in his letter of January 9, 1968 to David Ben-Gurion. De Gaulle cited "the pitiful condition of the Arabs who had sought refuge in Jordan or were relegated to Gaza" and stated that provided Israel withdrew her forces, it appeared it would be possible to reach a solution "within the framework of the United Nations that included the assurance of a dignified and fair future for the refugees and minorities in the Middle East."
Alexander Orakhelashvili said that ‘Just settlement’ can only refer to a settlement guaranteeing
the return of displaced Palestinians. He explained that it must be presumed that the Council did not adopt decisions that validated mass deportation or displacement, since expulsion or deportation are crimes against humanity or an exceptionally serious war crime.
According to M. Avrum Ehrlich, 'Resolution 242 called for "a just solution to the refugee problem," a term covering Jewish refugees from Arab countries as stated by President Carter in 1978 at Camp David'.
According to John Quigley, however, it is clear from the context in which it was adopted, and from the statements recounted by the delegates, that Resolution 242 contemplates the Palestine Arab refugees only.
Complicating the issue, Arthur Goldberg, one of the key drafters of the final language present in the resolution, pointed out that the language intentionally refers to both Jewish and Arab refugees.
French version vs. English version of text.
The French version of the clause reads:
The difference between the two versions lies in the absence of a definite article ("the") in the English version, while the word "des" present in the French version in the expression "des territoires occupés" can only mean "from the occupied territories" (the "des" in front of "territoires occupés" can only be the contraction "from the" because of the use of the word "retrait" which entails an object - "des forces israéliennes" where the "des" is the contraction of "of the" (of the Israeli forces) and a location "des territoires occupés" where the "des" is the contraction of "from the" (from the occupied territories)). If the meaning of "from some occupied territories" were intended, the only way to say so in French would have been "de territoires occupés".
Although some have dismissed the controversy by suggesting that the use of the word "des" in the French version is a translation error and should therefore be ignored in interpreting the document, the debate has retained its force since both versions are of equal legal force, as recognized languages of the United Nations and in international law.
Solicitor John McHugo, a partner at Trowers & Hamlins and a visiting fellow at the Scottish Centre for International Law at Edinburgh University, draws a comparison to phrases such as:
In spite of the lack of definite articles, according to McHugo, it is clear that such an instruction cannot legitimately be taken to imply that some dogs need not be kept on the lead or that the rule applies only near some ponds. Further, McHugo points out a potential consequence of the logic employed by advocates of a "some" reading. Paragraph 2 (a) of the Resolution, which guarantees "freedom of navigation through international waterways in the area," may allow Arab states to interfere with navigation through "some" international waterways of their choosing.
Glenn Perry asserts that because the French version resolves ambiguities in the English text, and is more consistent with the other clauses of the treaty, it is the correct interpretation. He argues that "it is an accepted rule that the various language versions must be considered together, with the ambiguities of one version elucidated by the other". He cites Article 33 of the Vienna Convention on the Law of Treaties, which states that except when a treaty provides that one text shall prevail "the meaning which best reconciles the texts, having regard to the object and purpose of the treaty, shall be adopted". He furthermore argues that the context of the passage, in a treaty that reaffirms "'territorial integrity', 'territorial inviolability,' and 'the inadmissibility of the acquisition of territory by war' - taken together cannot be reconciled with anything less than full withdrawal". He argues that the reference to "secure and recognized borders" can be interpreted in several ways, and only one of them contradicts the principle of full withdrawal.
Shabtai Rosenne, former Permanent Representative of Israel to the United Nations Office at Geneva and member of the UN's International Law Commission, notes that:
Only English and French were the Security Council's working languages (Arabic, Russian, Spanish and Chinese were official but not the working languages).
The Committee for Accuracy in Middle East Reporting in America argues the practice at the UN is that the binding version of any resolution is the one voted upon. In the case of 242 that version was in English, so they assert the English version the only binding one.David A. Korn asserts that this was indeed the position held by the United States and United Kingdom:
The French representative to the Security Council, in the debate immediately after the vote, asserted:
Opponents of the "all territories" reading remind that the UN Security Council declined to adopt a draft resolution, including the definite article, far prior to the adoption of Resolution 242. They argue that, in interpreting a resolution of an international organization, one must look to the process of the negotiation and adoption of the text. This would make the text in English, the language of the discussion, take precedence.
The negotiating and drafting process.
A Congressional Research Service (CRS) Issue Brief quotes policy statements made by President Johnson in a speech delivered on September 10, 1968, and by Secretary of State Rogers in a speech delivered on December 9, 1969: "The United States has stated that boundaries should be negotiated and mutually recognized, 'should not reflect the weight of conquest,' and that adjustments in the pre-1967 boundaries should be 'insubstantial.'"
President Carter asked for a State Department report "to determine if there was any justice to the Israeli position that the resolution did not include all the occupied territories". The State Department report concluded:Support for the concept of total withdrawal was widespread in the Security Council, and it was only through intensive American efforts that a resolution was adopted which employed indefinite language in the withdrawal clause. In the process of obtaining this result, the United States made clear to the Arab states and several other members of the Security Council that the United States envisioned only insubstantial revisions of the 1949 armistice lines. Israel did not protest the approach.
Ruth Lapidoth describes the view, adopted by Israel, which holds that the resolution allowed Israel to retain "some territories". She argues "The provision on the establishment of “secure and recognized boundaries” would have been meaningless if there had been an obligation to withdraw from all the territories.
U.S. Secretary of State Henry Kissinger recalled the first time he heard someone invoke "the sacramental language of United Nations Security Council Resolution 242, mumbling about the need for a just and lasting peace within secure and recognized borders". He said the phrase was so platitudinous that he thought the speaker was pulling his leg. Kissinger said that, at that time, he did not appreciate how the flood of words used to justify the various demands obscured rather than illuminated the fundamental positions. Kissinger said those "clashing perspectives" prevented any real bargaining and explained:
However, speaking to Henry Kissinger, President Richard Nixon said "You and I both know they can’t go back to the other borders. But we must not, on the other hand, say that because the Israelis win this war, as they won the '67 War, that we just go on with status quo. It can't be done." Kissinger replied "I couldn't agree more"
Moreover, President Gerald Ford said: "The U.S. further supports the position that a just and lasting peace, which remains our objective, must be acceptable to both sides. The U.S. has not developed a final position on the borders. Should it do so it will give great weight to Israel's position that any peace agreement with Syria must be predicated on Israel remaining on the Golan Heights."
Furthermore, Secretary of State George Shultz declared: "Israel will never negotiate from, or return to, the lines of partition or to the 1967 borders."
Secretary of State Christopher's letter to Netanyahu states: "I would like to reiterate our position that Israel is entitled to secure and defensible borders, which should be directly negotiated and agreed with its neighbors."
A key part of the case in favour of a "some territories" reading is the claim that British and American officials involved in the drafting of the Resolution omitted the definite article deliberately in order to make it less demanding on the Israelis. As George Brown, British Foreign Secretary in 1967, said:
Lord Caradon, chief author of the resolution, takes a subtly different slant. His focus seems to be that the lack of a definite article is intended to deny permanence to the "unsatisfactory" pre-1967 border, rather than to allow Israel to retain land taken by force. Such a view would appear to allow for the possibility that the borders could be varied through negotiation:
Arthur J. Goldberg, another of the resolution's drafters, concurred that Resolution 242 does not dictate the extent of the withdrawal, and added that this matter should be negotiated between the parties:
Mr. Michael Stewart, Secretary of State for Foreign and Commonwealth Affairs, in a reply to a question in Parliament, 9 December 1969: "As I have explained before, there is reference, in the vital United Nations Security Council Resolution, both to withdrawal from territories and to secure and recognized boundaries. As I have told the House previously, we believe that these two things should be read concurrently and that the omission of the word 'all' before the word 'territories' is deliberate."
Mr. Joseph J. Sisco, Assistant Secretary of State, 12 July 1970 (NBC "Meet the Press"): "That Resolution did not say 'withdrawal to the pre-June 5 lines'. The Resolution said that the parties must negotiate to achieve agreement on the so-called final secure and recognized borders. In other words, the question of the final borders is a matter of negotiations between the parties." Mr. Sisco was actively involved in drafting the Resolution in his capacity as Assistant Secretary of State for International Organization Affairs in 1967.
President Lyndon B. Johnson:
U.S. position.
On June 19, 1967 President Johnson declared the five principles, including land for peace, that he believed comprised the components of any United Nations settlement of the Middle East crisis. He pledged the U.S. Government would "do its part for peace in every forum, at every level, at every hour". On July 12, 1967, Secretary of State Rusk announced that the U.S. position on the Near East crisis was outlined in the President's statement of June 19 and that it provided the basis for a just and equitable settlement between the Arab states and Israel. On August 16, 1967 the Israeli Foreign Office stated that Israel agreed with the principles set forth by the President on June 19 and indicated that no resolution would be acceptable if it deviated from them.
On June 9, 1967, Israeli Foreign Minister Eban assured Arthur Goldberg, US Ambassador to the UN, that Israel was not seeking territorial aggrandizement and had no "colonial" aspirations. Secretary of State Rusk stressed to the Government of Israel that no settlement with Jordan would be accepted by the world community unless it gave Jordan some special position in the Old City of Jerusalem. The US also assumed Jordan would receive the bulk of the West Bank as that was regarded as Jordanian territory.
On November 3, 1967 Ambassador Goldberg, accompanied by Mr. Sisco and Mr. Pedersen, called on King Hussein of Jordan. Goldberg said the US was committed to the principle of political independence and territorial integrity and was ready to reaffirm it bilaterally and publicly in the Security Council resolution. Goldberg said the US believes in territorial integrity, withdrawal, and recognition of secure boundaries. Goldberg said the principle of territorial integrity has two important sub-principles: there must be a withdrawal to recognized and secure frontiers for all countries, not necessarily the old armistice lines, and there must be mutuality in adjustments.
Walt Rostow advised President Johnson that Secretary Rusk had explained to Mr. Eban that US support for secure permanent frontiers does not mean the US supports territorial changes. The record of a meeting between Under Secretary of State Eugene Rostow and Israeli Ambassador Harmon stated that Rostow made clear the US view that there should be movement from General Armistice Agreements to conditions of peace and that this would involve some adjustments of armistice lines as foreseen in the Armistice Agreements. Rostow told Harmon that he had already stressed to Foreign Minister Eban that the US expected the thrust of the settlement would be toward security and demilitarization arrangements rather than toward major changes in the Armistice lines. Harmon said the Israeli position was that Jerusalem should be an open city under unified administration but that the Jordanian interest in Jerusalem could be met through arrangements including "sovereignty". Rostow said the US government assumed (and Harman confirmed) that despite public statements to the contrary, the Government of Israel position on Jerusalem was that which Eban, Harman, and Evron had given several times, that Jerusalem was negotiable.
Ambassador Goldberg briefed King Hussein on US assurances regarding territorial integrity. Goldberg said the US did not view Jordan as a country that consisted only of the East Bank, and that the US was prepared to support a return of the West Bank to Jordan with minor boundary rectifications. The US would use its influence to obtain compensation to Jordan for any territory it would be required to give up. Finally, although as a matter of policy the US did not agree with Jordan's position on Jerusalem, nor with the Israeli position on Jerusalem, the US was prepared to use its influence to obtain for Jordan a role in Jerusalem. Secretary Rusk advised President Johnson that he confirmed Goldberg's pledge regarding territorial integrity to King Hussein.
During a subsequent meeting between President Johnson, King Hussein, and Secretary of State Rusk, Hussein said the phrasing of the resolution calling for withdrawal from occupied territories could be interpreted to mean that the Egyptians should withdraw from Gaza and the Jordanians should withdraw from the West Bank. He said this possibility was evident from a speech given by Prime Minister Eshkol in which it had been claimed that both Gaza and the West Bank had been "occupied territory". The President agreed, and promised he would talk to Ambassador Goldberg about inserting Israel in that clause. Ambassador Goldberg told King Hussein that after taking into account legitimate Arab concerns and suggestions, the US would be willing to add the word "Israeli" before "Armed Forces" in the first operative paragraph.
A State Department study noted that when King Hussein met on 8 November with President Johnson, who had been briefed by Secretary Rusk on the US interpretation, the Jordanian monarch asked how soon the Israeli troops would withdraw from most of the occupied lands. The President replied "In six months."
William Quandt wrote about Johnson's meeting with Eban on October 24, 1967, and noted that Israel had annexed East Jerusalem. He said Johnson forcefully told Eban he thought Israel had been unwise when it went to war and that he still thought they were unwise. The President stressed the need to respect the territorial integrity of the Arab states. Quandt said "'The President wished to caution the Israelis that the further they get from June 5 the further they get from peace.' Meaning the more territory they insisted on holding beyond the 1967 lines, the worse would be the odds of getting a peace agreement with the Arabs."
Interpretations.
Israel interprets Resolution 242 as calling for withdrawal from territories as part of a negotiated peace and full diplomatic recognition. The extent of withdrawal would come as a result of comprehensive negotiations that led to durable peace not before Arabs start to meet their own obligations under Resolution 242.
Initially, the resolution was accepted by Egypt, Jordan and Israel but not by the Palestine Liberation Organization. The Arab position was initially that the Resolution called for Israel to withdraw from all the territory it occupied during the Six-Day War prior to peace agreements.
Israel and the Arab states have negotiated before the Israeli withdrawal. Israel and Jordan made peace without Israel withdrawing from the West Bank, since Jordan had already renounced its claims and recognized the PLO as the sole representative of the Palestinians. Egypt began negotiations before Israel withdrew from the Sinai. Negotiations ended without Egypt ever resuming control of the Gaza Strip, which Egypt held until 1967.
Supporters of the "Palestinian viewpoint" focus on the phrase in the resolution's preamble emphasizing the "inadmissibility of the acquisition of territory by war", and note that the French version called for withdrawal from "des territoires occupés" - ""the" territories occupied". The French UN delegation insisted on this interpretation at the time, but both English and French are the Secretariat's working languages.
Supporters of the "Israeli viewpoint" note that the second part of that same sentence in the preamble explicitly recognizes the need of existing states to live in security.
They focus on the operative phrase calling for "secure and recognized boundaries" and note that the resolution calls for a withdrawal "from territories" rather than "from the territories" or "from all territories," as the Arabs and others proposed; the latter two terms were rejected from the final draft of Resolution 242.
Alexander Orakhelashvili cites a number cases in which international tribunals have ruled that international organizations, including the Security Council, are bound by general international law. He says that inclusion of explicit clauses about the inadmissibility of acquisition of territory by war and requiring respect of territorial integrity and sovereignty of a state demonstrates that the Council does not intend to offend peremptory norms in these specific ways. The resolution also acknowledges that these principles must be part of an accepted settlement. That is confirmed by the Vienna Convention on the Law of Treaties which reiterates the prohibition on the use of force and provides that any settlement obtained by the threat or use of force in violation of the principles of international law embodied in the Charter of the United Nations or conflicting with a peremptory norm of general international law is invalid. According to Hans-Paul Gasser, ‘doubtful’ wording of the Council’s resolutions must always be construed in such a way as to avoid conflict with fundamental international obligations.
The USSR, India, Mali, Nigeria and Arab States all proposed that the resolution be changed to read "all territories" instead of "territories." Their request was discussed by the UN Security Council and "territories" was adopted instead of "all territories", after President Johnson told Premier Alexei Kosygin that the delegates should not try to negotiate the details of a Middle East settlement in the corridors and meeting halls of the United Nations, and Ambassador Goldberg stipulated that the exact wording of the resolution would not affect the position of any of the parties. Per Lord Caradon, the chief author of the resolution:
Lord Caradon also maintained,
During a symposium on the subject Lord Caradon said that Israel was in clear defiance of resolution 242. He specifically cited the "annexation of East Jerusalem" and "the creeping colonialism on the West Bank and in Gaza and in the Golan."
However, British Foreign Secretary George Brown said:
Statements by Security Council representatives.
The representative for India stated to the Security Council:
The representatives from Nigeria, France, USSR, Bulgaria, United Arab Republic (Egypt), Ethiopia, Jordan, Argentina and Mali supported this view, as worded by the representative from Mali: " wishes its vote today to be interpreted in the light of the clear and unequivocal interpretation which the representative of India gave of the provisions of the United Kingdom text." The Russian representative Vasili Kuznetsov stated:
Israel was the only country represented at the Security Council to express a contrary view. The USA, United Kingdom, Canada, Denmark, China and Japan were silent on the matter, but the US and UK did point out that other countries' comments on the meaning of 242 were simply their own views. The Syrian representative was strongly critical of the text's "vague call on Israel to withdraw".
The statement by the Brazilian representative perhaps gives a flavour of the complexities at the heart of the discussions:
However, the Soviet delegate Vasily Kuznetsov argued: " ... phrases such as 'secure and recognized boundaries'. ... make it possible for Israel itself arbitrarily to establish new boundaries and to withdraw its forces only to those lines it considers appropriate." meeting, para. 152.
U.S. Supreme Court Justice Arthur Goldberg, who represented the US in discussions, later stated: "The notable omissions in regard to withdrawal are the word 'the' or 'all' and 'the June 5, 1967 lines' the resolution speaks of withdrawal from occupied territories, without defining the extent of withdrawal".
Implementation.
On November 23, 1967, the Secretary General appointed Gunnar Jarring as Special Envoy to negotiate the implementation of the resolution with the parties, the so-called Jarring Mission. The governments of Israel, Egypt, Jordan and Lebanon recognized Jarring's appointment and agreed to participate in his shuttle diplomacy, although they differed on key points of interpretation of the resolution. The government of Syria rejected Jarring's mission on grounds that total Israeli withdrawal was a prerequisite for further negotiations. The talks under Jarring's auspices lasted until 1973, but bore no results. After 1973, the Jarring mission was replaced by bilateral and multilateral peace conferences.

</doc>
<doc id="47542" url="https://en.wikipedia.org/wiki?curid=47542" title="Buffy the Vampire Slayer">
Buffy the Vampire Slayer

Buffy the Vampire Slayer is an American television series created by Joss Whedon under his production tag, Mutant Enemy Productions with later co-executive producers being Jane Espenson, David Fury, David Greenwalt, Doug Petrie, Marti Noxon, and David Solomon. The series premiered on March 10, 1997, on The WB and concluded on May 20, 2003, on UPN. The series narrative follows Buffy Summers (played by Sarah Michelle Gellar), the latest in a line of young women known as "Vampire Slayers", or simply "Slayers". In the story, Slayers are "called" (chosen by fate) to battle against vampires, demons, and other forces of darkness. Like previous Slayers, Buffy is aided by a Watcher, who guides, teaches, and trains her. Unlike her predecessors, Buffy surrounds herself with a circle of loyal friends who become known as the "Scooby Gang".
The series received critical and popular acclaim and usually reached between four and six million viewers on original airings. Although such ratings are lower than successful shows on the "big four" networks (ABC, CBS, NBC, and Fox), they were a success for the relatively new and smaller WB Television Network. The show was ranked 41st on "TV Guide"'s list of 50 Greatest TV Shows of All Time, second on "Empire"s "50 Greatest TV Shows of All Time", 27th on "The Hollywood Reporter"s "Hollywood's 100 Favorite TV Shows", voted third in 2004 and 2007 on "TV Guide"s "Top Cult Shows Ever" and listed in "Time" magazine's "100 Best TV Shows of All-"Time"". In 2013, "TV Guide" also included it in its list of "The 60 Greatest Dramas of All Time" and ranked it #38 on its list of the "60 Best Series of All Time". "Buffy" was also named the third Best School Show of All Time by AOL TV. It was nominated for Emmy and Golden Globe awards, winning a total of three Emmys. However, snubs in lead Emmy categories resulted in outrage among TV critics and the decision by the academy to hold a tribute event in honor of the series after it had gone off the air in 2003.
"Buffy"'s success has led to hundreds of tie-in products, including novels, comics, and video games. The series has received attention in fandom (including fan films), parody, and academia, and has influenced the direction of other television series.
Premise.
Characters.
Buffy Summers (played by Sarah Michelle Gellar) is "the Slayer", one in a long line of young women chosen by fate to battle evil forces. This mystical calling endows her with dramatically increased physical strength, endurance, agility, accelerated healing, intuition, and a limited degree of clairvoyance, usually in the form of prophetic dreams. Buffy receives guidance from her Watcher, Rupert Giles (Anthony Stewart Head). Giles, rarely referred to by his first name (it is later revealed that in his misspent younger days he was called "Ripper"), is a member of the Watchers' Council, whose job is to train and assist the Slayers. Giles researches the supernatural creatures that Buffy must face, offering insights into their origins and advice on how to defeat them.
Buffy is also helped by friends she meets at Sunnydale High: Willow Rosenberg (Alyson Hannigan) and Xander Harris (Nicholas Brendon). Willow is originally a wallflower who excels at academics, providing a contrast to Buffy's outgoing personality and less-than-stellar educational record. They share the social isolation that comes with being different, and especially from being exceptional young women. As the series progresses, Willow becomes a more assertive character and a powerful witch, and comes out as a lesbian. In contrast, Xander, with no supernatural skills but very athletic, provides comic relief and a grounded perspective. It is Xander who often provides the heart to the series, and in season six, becomes the hero in place of Buffy who defeats the "Big Bad." Buffy and Willow are the only characters who appear in all 144 episodes; Xander is missing in only one.
The cast of characters grew over the course of the series. Buffy first arrives in Sunnydale with her mother, Joyce Summers (portrayed by Kristine Sutherland), who functions as an anchor of normality in the Summers' lives even after she learns of Buffy's role in the supernatural world ("Becoming, Part Two"). Buffy's younger sister Dawn Summers (Michelle Trachtenberg) is introduced in season five. A vampire with a soul, Angel (portrayed by David Boreanaz), is Buffy's love interest throughout the first three seasons. He leaves Buffy as he believes he isn't good enough for her. He goes on to make amends for his sins and to search for redemption in his own spin-off, "Angel". He makes several guest appearances in the remaining seasons, including the last episode.
At Sunnydale High, Buffy meets several other students besides Willow and Xander willing to join her fight for good, an informal group eventually tagged the "Scooby Gang" or "Scoobies." Cordelia Chase (Charisma Carpenter), the archetypal shallow cheerleader, reluctantly becomes involved, and Daniel "Oz" Osbourne (Seth Green), a fellow student, rock guitarist and werewolf, joins the group through his relationship with Willow. Jenny Calendar (Robia LaMorte), Sunnydale's computer science teacher, joins the group after helping destroy a demon trapped in cyberspace during season 1. She later becomes Giles' love interest. Anya (Emma Caulfield), a former vengeance demon (Anyanka) who specialized in avenging scorned women, becomes Xander's lover after losing her powers and joins the group in season four.
In Buffy's senior year at high school, she meets Faith (Eliza Dushku), the other current Slayer, who was brought forth when Slayer Kendra Young (Bianca Lawson) was killed by vampire Drusilla (Juliet Landau), in season two. Although she initially fights on the side of good with Buffy and the rest of the group, she comes to stand against them and sides with Mayor Richard Wilkins (Harry Groener) after accidentally killing a human in season three. She reappears briefly in the fourth season, looking for vengeance, and moves to "Angel" where she voluntarily goes to jail for her murders. Faith reappears in season seven of "Buffy", after having helped Angel and his crew, and fights alongside Buffy against The First Evil.
Buffy gathers other allies: Spike (James Marsters), a vampire, is an old companion of Angelus and one of Buffy's major enemies in early seasons, although they later become allies and lovers. At the end of season six, Spike regains his soul. Spike is known for his Billy Idol-style peroxide blond hair and his black leather coat, stolen from a previous Slayer, Nikki Wood; her son, Robin Wood (D. B. Woodside), joined the group in the final season. Tara Maclay (Amber Benson) is a fellow member of Willow's Wicca group during season four, and their friendship eventually turns into a romantic relationship. Buffy became involved personally and professionally with Riley Finn (Marc Blucas), a military operative in "the Initiative," which hunts demons using science and technology. The final season sees geeky wannabe-villain Andrew Wells (Tom Lenk) come to side with the Scoobies, who regard him more as a nuisance than an ally.
"Buffy" featured dozens of recurring characters, both major and minor. For example, the "Big Bad" (villain) characters were featured for at least one season (for example, Glorificus was a character who appeared in 12 episodes, spanning much of season five). Similarly, characters who allied themselves to the group and characters which attended the same institutions were sometimes featured in multiple episodes.
Setting and filming locations.
The show is set in the fictional California town of Sunnydale, whose suburban Sunnydale High School sits on top of a "Hellmouth," a gateway to demon realms. The Hellmouth, located beneath the school library, is a source of mystical energies as well as a nexus for a wide variety of evil creatures and supernatural phenomena. In addition to being an open-ended plot device, Joss Whedon has cited the Hellmouth and "High school as Hell" as one of the primary metaphors in creating the series.
Most of "Buffy" was shot on location in Los Angeles, California. The main exterior set of the town of Sunnydale, including the "sun sign," was in a lot on Olympic Boulevard in Santa Monica, California. The high school used in the first three seasons is actually Torrance High School, in Torrance, California. In addition to the high school and its library, scenes take place in the town's cemeteries, a local nightclub (The Bronze), and Buffy's home (located in Torrance), where many of the characters live at various points in the series.
Some of the exterior shots of the college Buffy attends, UC Sunnydale, were filmed at UCLA. Several episodes include shots from the Oviatt Library at CSUN.
Format.
"Buffy" is told in a serialized format, with each episode involving a self-contained story while contributing to a larger storyline, which is broken down into season-long narratives marked by the rise and defeat of a powerful antagonist, commonly referred to as the "Big Bad". While the show is mainly a drama with frequent comic relief, most episodes blend different genres, including horror, martial arts, romance, melodrama, farce, science fiction, comedy, and even, in one episode, musical comedy.
The series' narrative revolves around Buffy and her friends, collectively dubbed the "Scooby Gang," who struggle to balance the fight against supernatural evils with their complex social lives. The show mixes complex, season-long storylines with a villain-of-the-week format; a typical episode contains one or more villains, or supernatural phenomena, that are thwarted or defeated by the end of the episode. Though elements and relationships are explored and ongoing subplots are included, the show focuses primarily on Buffy and her role as an archetypal heroine.
In the first few seasons, the most prominent monsters in the "Buffy" bestiary are vampires, which are based on traditional myths, lore, and literary conventions. As the series continues, Buffy and her companions fight an increasing variety of demons, as well as ghosts, werewolves, zombies, and unscrupulous humans. They frequently save the world from annihilation by a combination of physical combat, magic, and detective-style investigation, and are guided by an extensive collection of ancient and mystical reference books. 
Storylines.
Season one exemplifies the "high school is hell" concept. Buffy Summers has just moved to Sunnydale after burning down her old school's gym, and hopes to escape her Slayer duties. Her plans are complicated by Rupert Giles, her new Watcher, who reminds her of the inescapable presence of evil. Sunnydale High is built atop a Hellmouth, a portal to demon dimensions that attracts supernatural phenomena to the area. Buffy befriends two schoolmates, Xander Harris and Willow Rosenberg, who help her fight evil throughout the series, but they must first prevent The Master, an ancient and especially threatening vampire, from opening the Hellmouth and taking over Sunnydale.
The emotional stakes are raised in season two. Vampires Spike and Drusilla (weakened from a mob in Prague, which, it is implied, caused her debilitating injury), come to town along with a new slayer, Kendra Young, who was activated as a result of Buffy's brief death in the season one finale. Xander becomes involved with Cordelia, while Willow becomes involved with witchcraft and Daniel "Oz" Osbourne, who is a werewolf. The romantic relationship between Buffy and the vampire Angel develops over the course of the season, but after they sleep together, Angel's soul, given to him by a Gypsy curse in the past, is lost, and he once more becomes Angelus, a sadistic killer. Kendra is killed by a restored Drusilla. Angelus torments much of the "Scooby Gang" throughout the rest of the season and murders multiple innocents and Giles's new girlfriend Jenny Calendar, a gypsy who was sent to maintain Angel's curse. To avert an apocalypse, Buffy is forced to banish Angelus to a demon dimension just moments after Willow has restored his soul. The ordeal leaves Buffy emotionally shattered, and she leaves Sunnydale.
After attempting to start a new life in Los Angeles, Buffy returns to town in season three. Angel has mysteriously been released from the demon dimension, but is close to insanity due to the torment he suffered there, and is nearly driven to suicide by the First Evil. He and Buffy realize that a relationship between them can never happen; he eventually leaves Sunnydale at the end of the season. A new watcher named Wesley is put in Giles's place when Giles is fired from the Watcher's Council because he has developed a "father's love" for Buffy; and towards the end of the season, Buffy announces that she will no longer be working for the Council. Early in the season, she meets Faith, the Slayer activated after Kendra's death. She also encounters the affable Mayor Richard Wilkins, who secretly has plans to "ascend" (become a "pure" demon) on Sunnydale High's Graduation Day. Although Faith initially works well with Buffy, she becomes increasingly unstable after accidentally killing a human and forms a relationship with the paternal yet manipulative Mayor, eventually landing in a coma after a fight with Buffy. At the end of the season, after the Mayor becomes a huge snake-like demon, Buffy and the entire graduating class destroy him by blowing up Sunnydale High.
Season four sees Buffy and Willow enroll at UC Sunnydale, while Xander joins the workforce and begins dating Anya, a former vengeance demon. Spike returns as a series regular and is abducted by The Initiative, a top-secret military installation based beneath the UC Sunnydale campus. They implant a microchip in his head that punishes him whenever he tries to harm a human. He makes a truce with the Scooby Gang and begins to fight on their side, for the joy of fighting, upon learning that he can still harm other demons. Oz leaves town after realizing that he is too dangerous as a werewolf, and Willow falls in love with Tara Maclay, another witch. Buffy begins dating Riley Finn, a graduate student and member of The Initiative. Although appearing to be a well-meaning anti-demon operation, The Initiative's sinister plans are revealed when Adam, a monster secretly built from parts of humans, demons and machinery, escapes and begins to wreak havoc on the town. Adam is destroyed by a magical composite of Buffy and her three friends, and The Initiative is shut down.
During season five, a younger sister, Dawn, suddenly appears in Buffy's life; although she is new to the series, to the characters it is as if she has always been there. Buffy is confronted by Glory, an exiled Hell God who is searching for a "Key" that will allow her to return to her Hell dimension and in the process blur the lines between dimensions and unleash Hell on Earth. It is later discovered that the Key's protectors have turned the Key into human form – Dawn – concurrently implanting everybody with lifelong memories of her. The Watcher's Council aids in Buffy's research on Glory, and she and Giles are both reinstated on their own terms. Riley leaves early in the season after realizing that Buffy does not love him and joins a military demon-hunting operation. Spike, still implanted with the Initiative chip, realizes he is in love with Buffy and increasingly helps the Scoobies in their fight. Buffy's mother Joyce dies of a brain aneurysm, while at the end of the season, Xander proposes to Anya. Glory finally discovers that Dawn is the key and kidnaps her. To save Dawn, Buffy sacrifices her own life by diving into the portal to the Hell dimension and thus closes it with her death.
At the beginning of season six, Buffy has been dead for 147 days, but Buffy's friends resurrect her through a powerful spell, believing they have rescued her from the Hell dimension. Buffy returns in a deep depression, explaining that she had been in Heaven and is devastated to be pulled back to earth. Giles returns to England because he has concluded that Buffy has become too reliant on him, while Buffy takes up a fast-food job to support herself and Dawn, and develops a secret, mutually abusive relationship with Spike. Dawn suffers from kleptomania and feelings of alienation, Xander leaves Anya at the altar (after which she once again becomes a vengeance demon), and Willow becomes addicted to magic, causing Tara to temporarily leave her. They also begin to deal with The Trio, a group of nerds led by Warren Mears who use their technological proficiency to attempt to kill Buffy and take over Sunnydale. Warren is shown to be the only competent villain of the group and, after Buffy thwarts his plans multiple times and the Trio breaks apart, he becomes unhinged and attacks Buffy with a gun, killing Tara in the process. This causes Willow to descend into a nihilistic darkness and unleash all of her dark magical powers, killing Warren and attempting to kill his friends. Giles returns to face her in battle and infuses her with light magic, tapping into her remaining humanity. This overwhelms Willow with guilt and pain, whereupon she attempts to destroy the world to end everyone's suffering, although it eventually allows Xander to reach through her pain and end her rampage. Late in the season, after losing control and trying to rape Buffy, Spike leaves Sunnydale and travels to see a demon and asks him to "return him to what he used to be" so that he can "give Buffy what she deserves." After Spike passes a series of brutal tests, the demon restores his soul.
During season seven, it is revealed that Buffy's second resurrection caused an instability that is allowing the First Evil to begin tipping the balance between good and evil. It begins by hunting down and killing inactive Potential Slayers, and soon raises an army of ancient, powerful Turok-Han vampires. After the Watchers' Council is destroyed, a number of Potential Slayers (some brought by Giles) take refuge in Buffy's house. Faith returns to help fight the First Evil, and the new Sunnydale High School's principal, Robin Wood, also joins the cause. The Turok-Han vampires and a sinister, misogynistic preacher known as Caleb begin causing havoc for the Scoobies. As the Hellmouth becomes more active, nearly all of Sunnydale's population – humans and demons alike – flee. In the series finale, Buffy kills Caleb, and Angel returns to Sunnydale with an amulet, which Buffy gives to Spike; the Scoobies then surround the Hellmouth and the Potential Slayers descend into its cavern, while Willow casts a spell that activates their Slayer powers. Anya dies in the fight, as do some of the new Slayers. Spike's amulet channels the power of the sun to destroy the Hellmouth and all the vampires within it, including himself. The collapse of the cavern creates a crater that swallows all of Sunnydale, while the survivors of the battle escape in a school bus. In the final scene, as the survivors survey the crater, Dawn asks, "What are we going to do now?" Buffy slowly begins to enigmatically smile as she contemplates the future ahead of her, ending the series on a hopeful note.
Production.
Origins.
Writer Joss Whedon says that "Rhonda the Immortal Waitress" was really the first incarnation of the "Buffy" concept, "the idea of some woman who seems to be completely insignificant who turns out to be extraordinary." This early, unproduced idea evolved into "Buffy", which Whedon developed to invert the Hollywood formula of "the little blonde girl who goes into a dark alley and gets killed in every horror movie." Whedon wanted "to subvert that idea and create someone who was a hero." He explained, "The very first mission statement of the show was the joy of female power: having it, using it, sharing it."
The idea was first visited through Whedon's script for the 1992 movie "Buffy the Vampire Slayer", which featured Kristy Swanson in the title role. The director, Fran Rubel Kuzui, saw it as a "pop culture comedy about what people think about vampires." Whedon disagreed: "I had written this scary film about an empowered woman, and they turned it into a broad comedy. It was crushing." The script was praised within the industry, but the movie was not.
Several years later, Gail Berman (later a Fox executive, but at that time President and CEO of the production company Sandollar Television, who owned the TV rights to the movie) approached Whedon to develop his "Buffy" concept into a television series. Whedon explained that "They said, 'Do you want to do a show?' And I thought, 'High school as a horror movie.' And so the metaphor became the central concept behind "Buffy", and that's how I sold it." The supernatural elements in the series stood as metaphors for personal anxieties associated with adolescence and young adulthood. Early in its development, the series was going to be simply titled "Slayer". Whedon went on to write and partly fund a 25-minute non-broadcast pilot that was shown to networks and eventually sold to the WB Network. The latter promoted the premiere with a series of "History of the Slayer" clips, and the first episode aired on March 10, 1997.
Executive producers.
Joss Whedon was credited as executive producer throughout the run of the series, and for the first five seasons (1997–2001) he was also the showrunner, supervising the writing and all aspects of production. Marti Noxon took on the role for seasons six and seven (2001–2003), but Whedon continued to be involved with writing and directing "Buffy" alongside projects such as "Angel", "Fray", and "Firefly". Fran Rubel Kuzui and her husband, Kaz Kuzui, were credited as executive producers but were not involved in the show. Their credit, rights, and royalties over the franchise relate to their funding, producing, and directing of the original movie version of "Buffy".
Writing.
Script-writing was done by Mutant Enemy, a production company created by Whedon in 1997. The writers with the most writing credits are Joss Whedon, Steven S. DeKnight, Jane Espenson, David Fury, Drew Goddard, Drew Greenberg, David Greenwalt, Rebecca Rand Kirshner, Marti Noxon and Doug Petrie. Other authors with writing credits include Dean Batali, Carl Ellsworth, Tracey Forbes, Ashley Gable, Howard Gordon, Diego Gutierrez, Elin Hampton, Rob Des Hotel, Matt Kiene, Ty King, Thomas A. Swyden, Joe Reinkemeyer, Dana Reston and Dan Vebber.
Jane Espenson has explained how scripts came together. First, the writers talked about the emotional issues facing Buffy Summers and how she would confront them through her battle against evil supernatural forces. Then the episode's story was "broken" into acts and scenes. Act breaks were designed as key moments to intrigue viewers so that they would stay with the episode following the commercial break. The writers collectively filled in scenes surrounding these act breaks for a more fleshed-out story. A whiteboard marked their progress by mapping brief descriptions of each scene. Once "breaking" was done, the credited author wrote an outline for the episode, which was checked by Whedon or Noxon. The writer then wrote a full script, which went through a series of drafts, and finally a quick rewrite from the show runner. The final article was used as the shooting script.
Broadcast history and syndication.
"Buffy the Vampire Slayer" first aired on March 10, 1997, (as a mid season replacement for the show "Savannah") on the WB network, and played a key role in the growth of the Warner Bros. television network in its early years. After five seasons, it transferred to the United Paramount Network (UPN) for its final two seasons. In 2001, the show went into syndication in the United States on local stations and on cable channel FX; the local airings ended in 2005, and the FX airings lasted until 2008 but returned to the network in 2013. Beginning in January 2010, it began to air in syndication in the United States on Logo. Reruns also briefly aired on MTV. In March 2010, it began to air in Canada on MuchMusic and MuchMore. On November 7, 2010, it began airing on Chiller with a 24-hour marathon; the series airs weekdays. Chiller also aired a 14-hour Thanksgiving Day marathon on November 25, 2010. In 2011, it began airing on Oxygen and TeenNick. On June 22, 2015, it began airing on ABC Family.
While the seventh season was still being broadcast, Sarah Michelle Gellar told "Entertainment Weekly" she was not going to sign on for an eighth year; "When we started to have such a strong year this year, I thought: 'This is how I want to go out, on top, at our best.'" Whedon and UPN gave some considerations to production of a spin-off series that would not require Gellar, including a rumored Faith series, but nothing came of those plans. The "Buffy" canon continued outside the television medium in the Dark Horse Comics series, "Buffy" Season Eight. This was produced starting March 2007 by Whedon, who also wrote the first story arc, "The Long Way Home".
As of July 15, 2008, "Buffy the Vampire Slayer" episodes are available to download for PlayStation 3 and PlayStation Portable video game consoles via the PlayStation Network.
In the United Kingdom, the entire series aired on Sky1 and BBC Two. After protests from fans about early episodes being edited for their pre-watershed time-slot, from the second run (mid-second season onwards), the BBC gave the show two time slots: the early-evening slot (typically Thursday at 6:45 pm) for a family-friendly version with violence, objectionable language and other stronger material cut out, and a late-night uncut version (initially late-night Sundays, but for most of the run, late-night Fridays; exact times varied). Sky1 aired the show typically at 8:00 pm on Thursdays. From the fourth season onwards, the BBC aired the show in anamorphic 16:9 widescreen format. Whedon later said that "Buffy" was never intended to be viewed this way. Despite his claims, Sky1 and Syfy now air repeat showings in the widescreen format.
In August 2014, Pivot announced that, for the first time, episodes of "Buffy" would be broadcast in high-definition which was remastered by 20th Century Fox. The transfer was poorly received by some fans, owing to a number of technical and format changes that were viewed as detrimental to the show's presentation; various scenes were heavily cropped to fit the 16:9 format, and shots were altered to have a brighter look, often with color levels altered. Other problems included missing filters, editing errors, and poorly re-rendered CGI. Series creator Joss Whedon and other members of the original team also expressed their displeasure.
Music.
"Buffy" features a mix of original, indie, rock and pop music. The composers spent around seven days scoring between fourteen to thirty minutes of music for each episode. Christophe Beck revealed that the "Buffy" composers used computers and synthesizers and were limited to recording one or two "real" samples. Despite this, their goal was to produce "dramatic" orchestration that would stand up to film scores.
Alongside the score, most episodes featured indie rock music, usually at the characters' venue of choice, The Bronze. "Buffy" music supervisor John King explained that "we like to use unsigned bands" that "you would believe would play in this place." For example, the fictional group Dingoes Ate My Baby were portrayed on screen by front group Four Star Mary. Pop songs by famous artists were rarely featured prominently, but several episodes spotlighted the sounds of more famous artists such as Sarah McLachlan, The Brian Jonestown Massacre, Blink-182, Third Eye Blind, Aimee Mann (who also had a line of dialogue), The Dandy Warhols, Cibo Matto, Coldplay, Lisa Loeb, K's Choice and Michelle Branch. The popularity of music used in "Buffy" has led to the release of four soundtrack albums: ', ', the ""Once More, with Feeling" Soundtrack", and "".
Inspirations and metaphors.
During the first year of the series, Whedon described the show as "My So-Called Life" meets "The X-Files."" "My So-Called Life" gave a sympathetic portrayal of teen anxieties; in contrast, "The X-Files" delivered a supernatural "monster of the week" storyline. Alongside these series, Whedon has cited cult film "Night of the Comet" as a "big influence," and credited the "X-Men" character Kitty Pryde as a significant influence on the character of Buffy. The authors of the unofficial guidebook "Dusted" point out that the series was often a pastiche, borrowing elements from previous horror novels, movies, and short stories and from such common literary stock as folklore and mythology. Nevitt and Smith describe "Buffy"'s use of pastiche as "post modern Gothic." For example, the Adam character parallels the "Frankenstein" monster, the episode "Bad Eggs" parallels "Invasion of the Body Snatchers", "Out of Mind, Out of Sight" parallels "The Invisible Man", and so on.
"Buffy" episodes often include a deeper meaning or metaphor as well. Whedon explained, "We think very carefully about what we're trying to say emotionally, politically, and even philosophically while we're writing it... it really is, apart from being a pop-culture phenomenon, something that is deeply layered textually episode by episode." Academics Wilcox and Lavery provide examples of how a few episodes deal with real life issues turned into supernatural metaphors:
The love affair between the vampire Angel and Buffy was fraught with metaphors. For example, their night of passion cost the vampire his soul. Sarah Michelle Gellar said: "That's the ultimate metaphor. You sleep with a guy and he turns bad on you."
Buffy struggles throughout the series with her calling as Slayer and the loss of freedom this entails, frequently sacrificing teenage experiences for her Slayer duties. Her difficulties and eventual empowering realizations are reflections of several dichotomies faced by modern women and echo feminist issues within society.
In the episode "Becoming (Part 2)," when Joyce learns that Buffy is the Slayer, her reaction has strong echoes of a parent discovering their child is gay, including denial, suggesting that she try "not being a Slayer," and ultimately kicking Buffy out of the house.
Casting.
Actresses who auditioned for Buffy Summers and got other roles include Julie Benz (Darla), Elizabeth Anne Allen (Amy Madison), Julia Lee (Chantarelle/Lily Houston), Charisma Carpenter (Cordelia Chase), and Mercedes McNab (Harmony Kendall). Bianca Lawson, who played vampire slayer Kendra Young in season 2 of the show, originally auditioned for the role of Cordelia Chase before Charisma Carpenter was cast in the role.
The title role went to Sarah Michelle Gellar, who had appeared as Sydney Rutledge on "Swans Crossing" and Kendall Hart on "All My Children". At age 18 in 1995, Gellar had already won a Daytime Emmy Award for Outstanding Younger Leading Actress in a Drama Series. In 1996, she was originally cast as Cordelia Chase during a week of auditioning. She decided to keep trying for the role of Buffy, and after several more auditions, she landed the lead.
Nathan Fillion originally auditioned for the role of Angel back in early 1996. David Boreanaz had already been cast at the time of the unaired "Buffy" pilot, but did not appear. Fillion would later portray Caleb in the show's seventh season, and would also work with Whedon on several other occasions, including "Firefly".
Anthony Stewart Head had already led a prolific acting and singing career, but remained best known in the United States for a series of twelve coffee commercials with Sharon Maughan for Nescafé. He accepted the role of Rupert Giles.
Nicholas Brendon, unlike other "Buffy" regulars, had little acting experience, instead working various jobs—including production assistant, plumber's assistant, veterinary janitor, food delivery, script delivery, day care counselor, and waiter—before breaking into acting and overcoming his stutter. He landed his Xander Harris role following only four days of auditioning. Ryan Reynolds and Danny Strong also auditioned for the part. Strong later played the role of Jonathan Levinson, a recurring character for much of the series run.
Alyson Hannigan was the last of the original six to be cast. Following her role in "My Stepmother Is an Alien", she appeared in commercials and supporting roles on television shows throughout the early 1990s. In 1996, the role of Willow Rosenberg was originally played by Riff Regan for the unaired "Buffy" pilot, but Hannigan auditioned when the role was being recast for the series proper. Hannigan described her approach to the character through Willow's reaction to a particular moment: Willow sadly tells Buffy that her Barbie doll was taken from her as a child. Buffy asks her if she ever got it back. Willow's line was to reply "most of it." Hannigan decided on an upbeat and happy delivery of the line "most of it," as opposed to a sad, depressed delivery. Hannigan figured Willow would be happy and proud that she got "most of it" back. That indicated how she was going to play the rest of the scene, and the role, for that matter, and defined the character. Her approach subsequently got her the role.
Opening sequence.
The "Buffy" opening sequence provides credits at the beginning of each episode, with the accompanying music performed by Californian rock band Nerf Herder. In the DVD commentary for the first "Buffy" episode, Whedon said his decision to go with Nerf Herder's theme was influenced by Hannigan, who had urged him to listen to the band's music. Janet Halfyard, in her essay "Music, Gender, and Identity in "Buffy the Vampire Slayer" and "Angel"," describes the opening:
But the theme quickly changes: "It removes itself from the sphere of 1960s and 70s horror by replaying the same motif, the organ now supplanted by an aggressively strummed electric guitar, relocating itself in modern youth culture ..." Halfyard describes sequences, in which the action and turbulence of adolescence are depicted, as the visual content of the opening credits, and which provide a postmodern twist on the horror genre.
Spin-offs.
"Buffy" has inspired a range of official and unofficial works, including television shows, books, comics and games. This expansion of the series encouraged use of the term "Buffyverse" to describe the fictional universe in which "Buffy" and related stories take place.
The franchise has inspired "Buffy" action figures and merchandise such as official "Buffy/Angel" magazines and "Buffy" companion books. Eden Studios has published a "Buffy" role-playing game, while Score Entertainment has released a "Buffy" Collectible Card Game.
Series continuation.
The storyline is being continued in a comic book series produced by Joss Whedon and published by Dark Horse Comics. The series, which began in 2007 with "Buffy the Vampire Slayer Season Eight", followed by "Buffy the Vampire Slayer Season Nine" in 2011, and "Buffy the Vampire Slayer Season Ten" in 2014, serves as a canonical continuation of the television series.
Joss Whedon was interested in a film continuation in 1998, but such a film has yet to materialize.
"Angel".
The spin-off "Angel" was introduced in October 1999, at the start of "Buffy" season four. The series was created by "Buffy"'s creator Joss Whedon in collaboration with David Greenwalt. Like "Buffy", it was produced by the production company Mutant Enemy. At times, it performed better in the Nielsen ratings than its parent series did.
The series was given a darker tone focusing on the ongoing trials of Angel in Los Angeles. His character is tormented by guilt following the return of his soul, punishment for more than a century of murder and torture. During the first four seasons of the show, he works as a private detective in a fictionalized version of Los Angeles, California, where he and his associates work to "help the helpless" and to restore the faith and "save the souls" of those who have lost their way. Typically, this mission involves doing battle with evil demons or demonically allied humans (primarily the law firm Wolfram & Hart), while Angel must also contend with his own violent nature. In season five, the Senior Partners of Wolfram and Hart take a bold gamble in their campaign to corrupt Angel, giving him control of their Los Angeles office. Angel accepts the deal as an opportunity to fight evil from the inside.
In addition to Boreanaz, "Angel" inherited "Buffy" series cast regular Charisma Carpenter (Cordelia Chase). When Glenn Quinn (Doyle) left the series during its first season, Alexis Denisof (Wesley Wyndam-Pryce), who had been a recurring character in the last nine episodes of season three of "Buffy", took his place. Carpenter and Denisof were followed later by Mercedes McNab (Harmony Kendall) and James Marsters (Spike). Several actors and actresses who played "Buffy" characters made guest appearances on "Angel", including Seth Green (Daniel "Oz" Osbourne), Sarah Michelle Gellar (Buffy Summers), Eliza Dushku (Faith), Tom Lenk (Andrew Wells), Alyson Hannigan (Willow Rosenberg), Julie Benz (Darla), Mark Metcalf (The Master), Julia Lee (Anne Steele), and Juliet Landau (Drusilla). Angel also continued to appear occasionally on "Buffy".
The storyline has been continued in the comic book series "" published by IDW Publishing and later "Angel and Faith" published by Dark Horse Comics.
Expanded universe.
Outside of the TV series, the Buffyverse has been officially expanded and elaborated on by authors and artists in the so-called "Buffyverse Expanded Universe." The creators of these works may or may not keep to established continuity. Similarly, writers for the TV series were under no obligation to use information which had been established by the Expanded Universe, and sometimes contradicted such continuity.
Dark Horse has published the "Buffy" comics since 1998. In 2003, Whedon wrote an eight-issue miniseries for Dark Horse Comics titled "Fray", about a Slayer in the future. Following the publication of "Tales of the Vampires" in 2004, "Dark Horse Comics" halted publication on Buffyverse-related comics and graphic novels. The company produced Whedon's "Buffy the Vampire Slayer Season Eight" with forty issues from March 2007 to January 2011, picking up where the television show left off—taking the place of an eighth canonical season. The first story arc is also written by Whedon, and is called "The Long Way Home" which has been widely well-received, with circulation rivalling industry leaders DC and Marvel's top-selling titles. Also after "The Long Way Home" came other story arcs like Faith's return in "No Future for You" and a "Fray" cross-over in "Time of Your Life." Dark Horse later followed "Season Eight" with "Buffy the Vampire Slayer Season Nine", starting in 2011, and "Buffy the Vampire Slayer Season Ten", which began in 2014.
Pocket Books hold the license to produce "Buffy" novels, of which they have published more than sixty since 1998. These sometimes flesh out background information on characters; for example, "Go Ask Malice" details the events that lead up to Faith arriving in Sunnydale. The most recent novels include "Carnival of Souls", "Blackout", "Portal Through Time", "Bad Bargain", and "The Deathless".
Five official "Buffy" video games have been released on portable and home consoles. Most notably, "Buffy the Vampire Slayer" for Xbox in 2002 and "" for GameCube, Xbox and PlayStation 2 in 2003.
Undeveloped spinoffs.
The popularity of "Buffy" and "Angel" has led to attempts to develop more on-screen ventures in the fictional 'Buffyverse'. These projects remain undeveloped and may never be greenlit. In 2002, two potential spinoffs were in discussion: "Buffy the Animated Series" and "Ripper". "Buffy the Animated Series" was a proposed animated TV show based on "Buffy"; Whedon and Jeph Loeb were to be executive producers for the show, and most of the cast from "Buffy" were to return to voice their characters. 20th Century Fox showed an interest in developing and selling the show to another network. A three-minute pilot was completed in 2004, but was never picked up. Whedon revealed to "The Hollywood Reporter": "We just could not find a home for it. We had six or seven hilarious scripts from our own staff – and nobody wanted it." Neither the pilot nor the scripts have been seen outside of the entertainment industry, though writer Jane Espenson has teasingly revealed small extracts from some of her scripts for the show.
"Ripper" was originally a proposed television show based upon the character of Rupert Giles portrayed by Anthony Stewart Head. More recent information has suggested that if "Ripper" were ever made, it would be a TV movie or a DVD movie. There was little heard about the series until 2007 when Joss Whedon confirmed that talks were almost completed for a 90 minute "Ripper" special on the BBC with both Head and the BBC completely on board.
In 2003, a year after the first public discussions on "Buffy the Animated Series" and "Ripper", "Buffy" was nearing its end. Espenson has said that during this time spinoffs were discussed, "I think Marti talked with Joss about "Slayer School" and Tim Minear talked with him about Faith on a motorcycle. I assume there was some back-and-forth pitching." Espenson has revealed that "Slayer School" might have used new slayers and potentially included Willow Rosenberg, but Whedon did not think that such a spinoff felt right.
Dushku declined the pitch for a Buffyverse TV series based on Faith and instead agreed to a deal to produce "Tru Calling". Dushku explained to IGN: "It would have been a really hard thing to do, and not that I would not have been up for a challenge, but with it coming on immediately following "Buffy", I think that those would have been really big boots to fill." Tim Minear explained some of the ideas behind the aborted series: "The show was basically going to be Faith meets "Kung Fu". It would have been Faith, probably on a motorcycle, crossing the earth, trying to find her place in the world."
Finally, during the summer of 2004 after the end of "Angel", a movie about Spike was proposed. The movie would have been directed by Tim Minear and starred Marsters and Amy Acker and featured Alyson Hannigan. Outside the 2006 Saturn Awards, Whedon announced that he had pitched the concept to various bodies but had yet to receive any feedback.
In September 2008, "Sci-Fi Wire" ran an interview with Sarah Michelle Gellar in which she said she would not rule out returning to her most iconic role: "Never say never," she said. "One of the reasons the original "Buffy" movie did not really work on the big screen–and people blamed Kristy, but that's not what it was–the story was better told over a long arc," Gellar said. "And I worry about Buffy as a 'beginning, middle and end' so quickly. ... You show me a script; you show me that it works, and you show me that audience can accept that, [and I'd probably be there. Those are what my hesitations are."
Cultural impact.
Academia.
"Buffy" is notable for attracting the interest of scholars of popular culture, as a subset of popular culture studies, and some academic settings include the show as a topic of literary study and analysis. National Public Radio describes "Buffy" as having a "special following among academics, some of whom have staked a claim in what they call 'Buffy Studies.'" Though not widely recognized as a distinct discipline, the term "Buffy studies" is commonly used amongst the peer-reviewed academic "Buffy"-related writings. The influence of "Buffy" on the depiction of vampires across popular culture has also been noted by anthropologists such as A. Asbjørn Jøn.
Critics have emerged in response to the academic attention the series has received. For example, Jes Battis, who authored "", admits that study of the Buffyverse "invokes an uneasy combination of enthusiasm and ire" and meets "a certain amount of disdain from within the halls of the academy." Nonetheless, "Buffy" eventually led to the publication of around twenty books and hundreds of articles examining the themes of the show from a wide range of disciplinary perspectives, including sociology, Speech Communication, psychology, philosophy, and women's studies. In a 2012 study by "Slate", "Buffy the Vampire Slayer" was named the most studied pop culture work by academics, with more than 200 papers, essays, and books devoted to the series.
The Whedon Studies Association produces the online academic journal "Slayage" and sponsors a biennial academic conference on the works of Whedon. The sixth "Biennial Slayage Conference", titled "Much Ado About Whedon", was held at California State University-Sacramento in late June 2014.
Fandom and fan films.
The popularity of "Buffy" has led to websites, online discussion forums, works of "Buffy" fan fiction and several unofficial fan-made productions. Since the end of the series, Whedon has stated that his intention was to produce a "cult" television series and has acknowledged a "rabid, almost insane fan base" that the show has created.
"Buffy" in popular culture.
The series, which employed pop culture references as a frequent humorous device, has itself become a frequent pop culture reference in video games, comics and television shows and has been frequently parodied and spoofed. Sarah Michelle Gellar has participated in several parody sketches, including a "Saturday Night Live" sketch in which the Slayer is relocated to the "Seinfeld" universe, and adding her voice to an episode of "Robot Chicken" that parodied a would-be eighth season of "Buffy".
"Buffy" was the code-name used for an early HTC mobile phone which integrated the social networking website Facebook.
U.S. television ratings.
"Buffy" helped put The WB on the ratings map, but by the time the series landed on UPN in 2001, viewing figures had fallen. The series' high came during the third season, with 5.3 million viewers (including repeats). This was probably due to the fact that both Gellar and Hannigan had hit movies out during the season ("Cruel Intentions" and "American Pie" respectively). The series' low was in season one at 3.7 million. The show's series finale "Chosen" pulled in a season high of 4.9 million viewers on the UPN network.
"Buffy" did not compete with shows on the main four networks (CBS, ABC, NBC, and Fox), but The WB was impressed with the young audience that the show was bringing in. Because of this, The WB ordered a full season of 22 episodes for the series' second season. Beginning with the episode "Innocence," which was watched by 8.2 million people, "Buffy" was moved from Monday at 9:00 pm to launch The WB's new night of programming on Tuesday. Due to its large success in that time slot, it remained on Tuesdays at 8:00 pm for the remainder of its original run. With its new timeslot on The WB, the show quickly climbed to the top of The WB ratings and became one of their highest-rated shows for the remainder of its time on the network. The show always placed in the top 3, usually only coming in behind "7th Heaven". Between seasons three and five, "Buffy" flip-flopped with "Dawson's Creek" and "Charmed" as the network's second highest-rated show.
In the 2001–2002 season, the show had moved to UPN after a negotiation dispute with The WB. While it was still one of their highest rated shows on their network, The WB felt that the show had already peaked and was not worth giving a salary increase to the cast and crew. UPN on the other hand, had strong faith in the series and picked up it for a two-season renewal. UPN dedicated a two-hour premiere to the series to help re-launch it. The relaunching had effect, as the season premiere attracted the second highest rating of the series, with 7.7 million viewers.
Impact on television.
Commentators of the entertainment industry including "The Village Voice", "PopMatters", "Allmovie", "The Hollywood Reporter", "The Washington Post" have cited "Buffy" as "influential." Some citing it as the ascent of television into its golden age. Stephanie Zacharek, in the "Village Voice", wrote "If we really are in a golden age of television, "Buffy the Vampire Slayer" was a harbinger." Robert Moore of "Popmatters" also expressed these sentiments, writing "TV was not art before "Buffy", but it was afterwards," suggesting that it was responsible for re-popularizing long story arcs on primetime television.
Its effect on programming was quickly evident. Autumn 2003 saw several new shows going into production in the U.S. that featured strong females who are forced to come to terms with supernatural power or destiny while trying to maintain a normal life. These post-"Buffy" shows include "Dead Like Me", "Joan of Arcadia" and "Teen Wolf". Bryan Fuller, the creator of "Dead Like Me", said that ""Buffy" showed that young women could be in situations that were both fantastic and relatable, and instead of shunting women off to the side, it puts them at the center." In the United Kingdom, the lessons learned from the impact of "Buffy" influenced the revived "Doctor Who" series (2005–present), and executive producer Russell T Davies has said:
As well as influencing "Doctor Who", "Buffy" influenced its spinoff series "Torchwood".
Several "Buffy" alumni have gone on to write for or create other shows. Such endeavors include "Tru Calling" (Douglas Petrie, Jane Espenson and lead actress Eliza Dushku), "Wonderfalls" (Tim Minear), "Point Pleasant" (Marti Noxon), "Jake 2.0" (David Greenwalt), "The Inside" (Tim Minear), "Smallville" (Steven S. DeKnight), "Once Upon a Time" (Jane Espenson), and "Lost" (Drew Goddard and David Fury).
Meanwhile, the Parents Television Council complained of efforts to "deluge their young viewing audiences with adult themes." The U.S. Federal Communications Commission (FCC), however, rejected the Council's indecency complaint concerning the violent sex scene between Buffy and Spike in "Smashed." The BBC, however, chose to censor some of the more controversial sexual content when it was shown on the pre-watershed 6:45 pm slot.
Series information.
The first season was introduced as a mid-season replacement for the short-lived night-time soap opera "Savannah", and therefore was made up of only 12 episodes. Each subsequent season was built up of 22 episodes. Discounting the unaired "Buffy" pilot, the seven seasons make up a total of 144 "Buffy" episodes aired between 1997 and 2003.
Awards and nominations.
"Buffy" has gathered a number of awards and nominations which include an Emmy Award nomination for the 1999 episode "Hush", which featured an extended sequence with no character dialogue. The 2001 episode "The Body" was filmed with no musical score, only diegetic music; it was nominated for a Nebula Award in 2002. The 2001 musical episode "Once More, with Feeling" received plaudits, but was omitted from Emmy nomination ballots by "accident". It since was featured on "Channel 4's "100 Greatest Musicals."" In 2001, Sarah Michelle Gellar received a Golden Globe-nomination for Best Actress in a TV Series-Drama for her role in the show, as well nominations for the Teen Choice Awards and the Saturn Award for Best Genre TV Actress. The series won the Drama Category for Television's Most Memorable Moment at the 60th Primetime Emmy Awards for "The Gift" beating "The X-Files", "Grey's Anatomy", "Brian's Song" and "Dallas", although the sequence for this award was not aired.

</doc>
<doc id="47543" url="https://en.wikipedia.org/wiki?curid=47543" title="Zero Wing">
Zero Wing

It enjoyed a degree of success in arcades and was subsequently ported to the Mega Drive by Toaplan on May 31, 1991, in Japan, and by Sega during the following year in Europe, followed by a Japan-only release by Naxat Soft on September 18, 1992, for the PC Engine's CD-ROM².
The European version of the Mega Drive port was the source for "All your base are belong to us", an Internet meme which plays off the poorly translated English in the game's introduction.
Synopsis.
Set in 2101, the game follows the signing of a peace treaty between the United Nations and CATS, an alien cyborg. However CATS breaks the covenant and takes control of the Japanese space colonies. The protagonist leads the ZIG spacecraft, managed to escape from the mother ship destroyed by CATS, with the aim to defeat enemy forces and liberate the Earth.
Gameplay.
As with other scrolling shooters, the aim of the game is to shoot all enemies that appear on screen and avoid getting obliterated by enemy fire, crashing into enemies or into foreground scenery. There are mid-level and end-of-level boss enemies that stay with the player until they are defeated. The game features eight levels.
Ports.
After it became fairly successful in the arcades and game centers, "Zero Wing" was ported to the Mega Drive in 1991 by Toaplan themselves and the CD-ROM², an add-on for the PC Engine, by Naxat Soft in 1992. The Mega Drive version was also released in Europe by Sega in 1992. The home console versions of "Zero Wing" were never released in North America due to the release of the arcade version distributed by Williams Electronics. The Japanese release will play fine on American consoles. Like most early titles it had no region protection, nor had the European release been PAL-optimized.
In the Mega Drive version, to expand on the game's plot, Toaplan added an introductory cut scene to the game. This introductory scene was translated by Sega of Europe to English from Japanese rather poorly for the European release (a phenomenon dubbed Engrish), resulting in dialogue such as "Somebody set up us the bomb", "All your base are belong to us", and "You have no chance to survive make your time". The introduction does not appear in the arcade nor CD-ROM² versions, rather, a different intro takes place with a blue-windowed ZIG.
In PC Engine version two new levels were added - 5th (Deeva) and 10th (Vacura).
Reception.
The game received positive critical reception upon release. "Computer and Video Games" scored it 93%, including ratings of 92% for graphics, 93% for sound, 90% for playability, and 89% for lastability. They praised "the great intro sequence", "super-smooth gameplay, beautifully defined graphics, rocking sound track, amazing explosions and incredible end-of-level bosses", concluding that it is "the game which breaths new life into shoot 'em ups on the Megadrive". "Mean Machines" scored it 91%, including ratings of 92% for presentation and graphics, 88% for sound, 90% for playability, and 89% for lastability, concluding that it is one of "the best Megadrive blasts in ages." "Sega Force" scored it 86%, including ratings of 84% for presentation, 89% for visuals, 83% for sound, 89% for playability, and 82% for lastability, concluding that it is "almost as good as "Hellfire"" but "not quite."
The game has received mixed reception from retrospective reviews. GameTrailers listed the Mega Drive version of Zero Wing as the seventh-worst video game in its "10 Best and Worst Video Games", though the focus was on its bad translation. However, a later ScrewAttack review said the game was "not that bad". It praised its soundtrack, stating that it contains "some of the best 16-bit rock music you'll ever hear". Retro review site HonestGamers said that "Much is made of this game, all things considered. And it's funny, because there's not a whole lot to it," before giving it a lackluster score of 4/10.
""All your base are belong to us"".
In 1999, "Zero Wing"'s introduction was re-discovered, culminating in the wildly popular "All your base are belong to us" Internet meme.

</doc>
<doc id="47544" url="https://en.wikipedia.org/wiki?curid=47544" title="Carrying capacity">
Carrying capacity

The carrying capacity of a biological species in an environment is the maximum population size of the species that the environment can sustain indefinitely, given the food, habitat, water, and other necessities available in the environment. In population biology, carrying capacity is defined as the environment's maximal load, which is different from the concept of population equilibrium. Its effect on population dynamics may be approximated in a logistic model, although this simplification ignores the possibility of overshoot which real systems may exhibit.
For the human population, more complex variables such as sanitation and medical care are sometimes considered as part of the necessary establishment. As population density increases, birth rate often decreases and death rate typically increases. The difference between the birth rate and the death rate is the "natural increase". The carrying capacity could support a positive natural increase, or could require a negative natural increase. Thus, the carrying capacity is the number of individuals an environment can support without significant negative impacts to the given organism and its environment. Below carrying capacity, populations typically increase, while above, they typically decrease. A factor that keeps population size at equilibrium is known as a regulating factor. Population size decreases above carrying capacity due to a range of factors depending on the species concerned, but can include insufficient space, food supply, or sunlight. The carrying capacity of an environment may vary for different species and may change over time due to a variety of factors, including: food availability, water supply, environmental conditions and living space.
The origins of the term "carrying capacity" are uncertain, with researchers variously stating that it was used "in the context of international shipping" or that it was first used during 19th-century laboratory experiments with micro-organisms. A recent review finds the first use of the term in an 1845 report by the US Secretary of State to the Senate.
Humans.
Several estimates of the carrying capacity have been made with a wide range of population numbers. A 2001 UN report said that two-thirds of the estimates fall in the range of 4 billion to 16 billion (with unspecified standard errors), with a median of about 10 billion. More recent estimates are much lower, particularly if resource depletion and increased consumption are considered.
The application of the concept of carrying capacity for the human population has been criticized for not successfully capturing the multi-layered processes between humans and the environment, which have a nature of fluidity and non-equilibrium, and for sometimes being employed in a blame-the-victim framework.
Supporters of the concept argue that the idea of a finite carrying capacity is just as valid when applied to humans as when applied to any other species. Animal population size, living standards, and resource depletion vary, but the concept of carrying capacity still applies. The carrying capacity of Earth has been studied by computer simulation models like World3.
Numbers of people are not the only factor. Waste and over-consumption, especially by wealthy people and nations, is putting more strain on the environment than overpopulation.
Factors that govern carrying capacity.
Some aspects of a system's carrying capacity may involve matters such as available supplies of food, water, raw materials, and/or other similar "resources." In addition, there are other factors that govern carrying capacity which may be less-instinctive or less-intuitive in nature, such as ever-increasing and/or ever-accumulating levels of wastes, damage, and/or eradication of essential components of any complex functioning system. Eradication of, for example, large or critical portions of any complex system (envision a space vehicle, for instance, or an airplane, or an automobile, or computer code, or the body components of a living vertebrate) can interrupt essential processes and dynamics in ways that induce systems-failures or unexpected collapse. (As an example of these latter factors, the "carrying capacity" of a complex system such an airplane is more than a matter of available food, or water, or available seating, but also reflects total weight carried and presumes that its passengers do not damage, destroy, or eradicate parts, doors, windows, wings, engine parts, fuel, and oil, and so forth.) Thus, on a global scale, food and similar resources may affect planetary carrying capacity to some extent so long as Earth's human passengers do not dismantle, eradicate, or otherwise destroy critical biospheric life-support capacities for essential processes of self-maintenance, self-perpetuation, and self-repair.
Thus, carrying capacity interpretations that focus solely on resource-limitations alone (such as food) may neglect wider functional factors. If the humans neither gain nor lose weight in the long run, the calculation is fairly accurate. If the quantity of food is invariably equal to the "Y" amount, carrying capacity has been reached. Humans, with the need to enhance their reproductive success (see Richard Dawkins' "The Selfish Gene"), understand that food supply can vary and also that other factors in the environment can alter humans' need for food. A house, for example, might mean that one does not need to eat as much to stay warm as one otherwise would. Over time, monetary transactions have replaced barter and local production, and consequently modified local human carrying capacity. However, purchases also impact regions thousands of miles away. For example, carbon dioxide from an automobile travels to the upper atmosphere. This led Paul R. Ehrlich to develop the I = PAT equation
where:
Technology can play a role in the dynamics of carrying capacity and while this can sometimes be positive, in other cases its influence can be problematic. For example, it has been suggested that in the past that the Neolithic revolution increased the carrying capacity of the world relative to humans through the invention of agriculture. In a similar way, viewed from the perspective of foods, the use of fossil fuels has been alleged to artificially increase the carrying capacity of the world by the use of stored sunlight, even though that food production does not guarantee the capacity of the Earth's climatic and biospheric life-support systems to withstand the damage and wastes arising from such fossil fuels. Again, however, such interpretations presume the continued and uninterrupted functioning of all other critical components of the global system. It has also been suggested that other technological advances that have increased the carrying capacity of the world relative to humans are: polders, fertilizer, composting, greenhouses, land reclamation, and fish farming. In an adverse way, however, many technologies enable economic entities and individual humans to inflict far more damage and eradication, far more quickly, efficiently, and on a wider-scale than ever. (Machine guns, chain saws, earth-movers, and the capacity of industrialized fishing fleets to capture and harvest targeted fish species faster than the fish themselves can reproduce are examples of such problematic outcomes of technology.)
Agricultural capability on Earth expanded in the last quarter of the 20th century. But now there are many projections of a continuation of the decline in world agricultural capability (and hence carrying capacity) which began in the 1990s. Most conspicuously, China's food production is forecast to decline by 37% by the last half of the 21st century, placing a strain on the entire carrying capacity of the world, as China's population could expand to about 1.5 billion people by the year 2050. This reduction in China's agricultural capability (as in other world regions) is largely due to the world water crisis and especially due to mining groundwater beyond sustainable yield, which has been happening in China since the mid-20th century.
Lester Brown of the Earth Policy Institute, has said: "It would take 1.5 Earths to sustain our present level of consumption. Environmentally, the world is in an overshoot mode."
Ecological footprint.
One way to estimate human demand compared to ecosystem's carrying capacity is "ecological footprint" accounting. Rather than speculating about future possibilities and limitations imposed by carrying capacity constraints, Ecological Footprint accounting provides empirical, non-speculative assessments of the past. It compares historic regeneration rates (biocapacity) against historical human demand (Ecological Footprint) in the same year. One result shows that humanity's demand footprint in 1999 exceeded the planet's biocapacity by over 20 percent.

</doc>
<doc id="47546" url="https://en.wikipedia.org/wiki?curid=47546" title="Vardar Macedonia">
Vardar Macedonia

Vardar Macedonia is an area in the north of the geographical region of Macedonia, corresponding with the area of today's Republic of Macedonia. It covers an area of . It usually refers to the part of Macedonia region attributed to the Kingdom of Serbia by the Treaty of Bucharest in 1913. It is named after the Vardar, the major river in the area.

</doc>
<doc id="47548" url="https://en.wikipedia.org/wiki?curid=47548" title="Daylight saving time">
Daylight saving time

[[File:DaylightSaving-World-Subdivisions.png|upright=1.67|thumb|alt=World map. Europe, most of North America, parts of southern South America and southeastern Australia, and a few other places use DST. Most of equatorial Africa and a few other places near the equator have never used DST. The rest of the landmass is marked as formerly using DST.|
Most of the world's countries do not use daylight saving time, but it is common in Europe and North America.
]]
Daylight saving time (DST) or summer time is the practice of advancing clocks during summer months by one hour so that evening daylight lasts an hour longer, while sacrificing normal sunrise times. Typically, regions with summer time adjust clocks forward one hour close to the start of spring and adjust them backward in the autumn to standard time. People use the terms" spring forward" and " fall back " when referring to this.
New Zealander George Hudson proposed the idea of daylight saving in 1895. The German Empire and Austria-Hungary organized the first nationwide implementation, starting on April 30, 1916. Many countries have used it at various times since then, particularly since the energy crisis of the 1970s.
The practice has both advocates and critics. Putting clocks forward benefits retailing, sports, and other activities that exploit sunlight after working hours, but can cause problems for outdoor entertainment and other activities tied to sunlight, such as farming. Though some early proponents of DST aimed to reduce evening use of incandescent lighting—once a primary use of electricity— today's heating and cooling usage patterns differ greatly, and research about how DST affects energy use is limited or contradictory.
DST clock shifts sometimes complicate timekeeping, and can disrupt travel, billing, record keeping, medical devices, heavy equipment, and sleep patterns. Computer software often adjusts clocks automatically, but policy changes by various jurisdictions of DST dates and timings may be confusing.
Rationale.
Industrialized societies generally follow a clock-based schedule for daily activities that do not change throughout the course of the year. The time of day that individuals begin and end work or school, and the coordination of mass transit, for example, usually remain constant year-round. In contrast, an agrarian society's daily routines for work and personal conduct are more likely governed by the length of daylight hours and by solar time, which change seasonally because of the Earth's axial tilt. North and south of the tropics daylight lasts longer in summer and shorter in winter, the effect becoming greater as one moves away from the tropics.
By synchronously resetting all clocks in a region to one hour ahead of Standard Time (one hour "fast"), individuals who follow such a year-round schedule will wake an hour earlier than they would have otherwise; they will begin and complete daily work routines an hour earlier, and they will have available to them an extra hour of daylight after their workday activities. However, they will have one less hour of daylight at the start of each day, making the policy less practical during winter.
While the times of sunrise and sunset change at roughly equal rates as the seasons change, proponents of Daylight Saving Time argue that most people prefer a greater increase in daylight hours after the typical "nine-to-five" workday. Supporters have also argued that DST decreases energy consumption by reducing the need for lighting and heating, but the actual effect on overall energy use is heavily disputed.
The manipulation of time at higher latitudes (for example Iceland, Nunavut or Alaska) has little impact on daily life, because the length of day and night changes more extremely throughout the seasons (in comparison to other latitudes), and thus sunrise and sunset times are significantly out of phase with standard working hours regardless of manipulations of the clock. DST is also of little use for locations near the equator, because these regions see only a small variation in daylight in the course of the year.
History.
[[File:Clepsydra-Diagram-Fancy.jpeg|thumb|upright|alt=A water clock. A small human figurine holds a pointer to a cylinder marked by the hours. The cylinder is connected by gears to a water wheel driven by water that also floats, a part that supports the figurine.|
Ancient water clock that lets hour lengths vary with season.]]
Although they did not fix their schedules to the clock in the modern sense, ancient civilizations adjusted daily schedules to the sun more flexibly than DST does, often dividing daylight into twelve hours regardless of day length, so that each daylight hour was longer during summer. For example, Roman water clocks had different scales for different months of the year: at Rome's latitude the third hour from sunrise, "hora tertia", started by modern standards at 09:02 solar time and lasted 44 minutes at the winter solstice, but at the summer solstice it started at 06:58 and lasted 75 minutes (see also: Roman timekeeping). After ancient times, equal-length civil hours eventually supplanted unequal, so civil time no longer varies by season. Unequal hours are still used in a few traditional settings, such as some Mount Athos monasteries and all Jewish ceremonies.
During his time as an American envoy to France, Benjamin Franklin, publisher of the old English proverb, "Early to bed, and early to rise, makes a man healthy, wealthy and wise", anonymously published a letter suggesting that Parisians economize on candles by rising earlier to use morning sunlight. This 1784 satire proposed taxing shutters, rationing candles, and waking the public by ringing church bells and firing cannons at sunrise. Despite common misconception, Franklin did "not" actually propose DST; 18th-century Europe did not even keep precise schedules. However, this soon changed as rail and communication networks came to require a standardization of time unknown in Franklin's day.
Modern DST was first proposed by the New Zealand entomologist George Hudson, whose shift-work job gave him leisure time to collect insects, and led him to value after-hours daylight. In 1895 he presented a paper to the Wellington Philosophical Society proposing a two-hour daylight-saving shift, and after considerable interest was expressed in Christchurch, he followed up in an 1898 paper. Many publications credit DST's proposal to the prominent English builder and outdoorsman William Willett, who independently conceived DST in 1905 during a pre-breakfast ride, when he observed with dismay how many Londoners slept through a large part of a summer's day. An avid golfer, he also disliked cutting short his round at dusk. His solution was to advance the clock during the summer months, a proposal he published two years later. The proposal was taken up by the Liberal Member of Parliament (MP) Robert Pearce, who introduced the first Daylight Saving Bill to the House of Commons on February 12, 1908. A select committee was set up to examine the issue, but Pearce's bill did not become law, and several other bills failed in the following years. Willett lobbied for the proposal in the UK until his death in 1915.
William Sword Frost, mayor of Orillia, Ontario, introduced daylight saving time in the municipality during his tenure from 1911 to 1912.
Starting on April 30, 1916, Germany and its World War I ally Austria-Hungary were the first to use DST () as a way to conserve coal during wartime. Britain, most of its allies, and many European neutrals soon followed suit. Russia and a few other countries waited until the next year and the United States adopted it in 1918.
Broadly speaking, Daylight Saving Time was abandoned in the years after the war (with some notable exceptions including Canada, the UK, France, and Ireland for example). However, it was brought back for periods of time in many different places during the following decades, and commonly during the Second World War. It became widely adopted, particularly in North America and Europe starting in the 1970s as a result of the 1970s energy crisis.
Since then, the world has seen many enactments, adjustments, and repeals. For specific details, an overview is available at Daylight saving time by country.
Procedure.
In the case of the United States where a one-hour shift occurs at 02:00 local time, in spring the clock jumps forward from the last moment of 01:59 standard time to 03:00 DST and that day has 23 hours, whereas in autumn the clock jumps backward from the last moment of 01:59 DST to 01:00 standard time, repeating that hour, and that day has 25 hours. A digital display of local time does not read 02:00 exactly at the shift to summer time, but instead jumps from 01:59:59.9 forward to 03:00:00.0.
Clock shifts are usually scheduled near a weekend midnight to lessen disruption to weekday schedules. A one-hour shift is customary, but Australia's Lord Howe Island uses a half-hour shift. Twenty-minute and two-hour shifts have been used in the past.
Coordination strategies differ when adjacent time zones shift clocks. The European Union shifts all at once, at 01:00 UTC or 02:00 CET or 03:00 EET; for example, Eastern European Time is always one hour ahead of Central European Time. Most of North America shifts at 02:00 local time, so its zones do not shift at the same time; for example, Mountain Time is temporarily (for one hour) zero hours ahead of Pacific Time, instead of one hour ahead, in the autumn and two hours, instead of one, ahead of Pacific Time in the spring. In the past, Australian districts went even further and did not always agree on start and end dates; for example, in 2008 most DST-observing areas shifted clocks forward on October 5 but Western Australia shifted on October 26. In some cases only part of a country shifts; for example, in the US, Hawaii and most of Arizona do not observe DST.
Start and end dates vary with location and year. Since 1996, European Summer Time has been observed from the last Sunday in March to the last Sunday in October; previously the rules were not uniform across the European Union. Starting in 2007, most of the United States and Canada observe DST from the second Sunday in March to the first Sunday in November, almost two-thirds of the year. The 2007 US change was part of the Energy Policy Act of 2005; previously, from 1987 through 2006, the start and end dates were the first Sunday in April and the last Sunday in October, and Congress retains the right to go back to the previous dates now that an energy-consumption study has been done. Proponents for permanently retaining November as the month for ending DST point to Halloween as a reason to delay the change—to provide extra daylight on October 31.
Beginning and ending dates are roughly the reverse in the southern hemisphere. For example, mainland Chile observed DST from the second Saturday in October to the second Saturday in March, with transitions at local time. The time difference between the United Kingdom and mainland Chile could therefore be five hours during the Northern summer, three hours during the Southern summer and four hours a few weeks per year because of mismatch of changing dates.
DST is generally not observed near the equator, where sunrise times do not vary enough to justify it. Some countries observe it only in some regions; for example, southern Brazil observes it while equatorial Brazil does not. Only a minority of the world's population uses DST because Asia and Africa generally do not observe it.
Politics.
Daylight saving has caused controversy since it began. Winston Churchill argued that it enlarges "the opportunities for the pursuit of health and happiness among the millions of people who live in this country" and pundits have dubbed it "Daylight Slaving Time". Historically, retailing, sports, and tourism interests have favored daylight saving, while agricultural and evening entertainment interests have opposed it, and its initial adoption had been prompted by energy crisis and war.
The fate of Willett's 1907 proposal illustrates several political issues involved. The proposal attracted many supporters, including Balfour, Churchill, Lloyd George, MacDonald, Edward VII (who used half-hour DST at Sandringham), the managing director of Harrods, and the manager of the National Bank. However, the opposition was stronger: it included Prime Minister H. H. Asquith, Christie (the Astronomer Royal), George Darwin, Napier Shaw (director of the Meteorological Office), many agricultural organizations, and theatre owners. After many hearings the proposal was narrowly defeated in a Parliament committee vote in 1909. Willett's allies introduced similar bills every year from 1911 through 1914, to no avail. The US was even more skeptical: Andrew Peters introduced a DST bill to the US House of Representatives in May 1909, but it soon died in committee.
After Germany led the way with starting DST () during World War I on April 30, 1916 together with its allies to alleviate hardships from wartime coal shortages and air raid blackouts, the political equation changed in other countries; the United Kingdom used DST first on May 21, 1916. US retailing and manufacturing interests led by Pittsburgh industrialist Robert Garland soon began lobbying for DST, but were opposed by railroads. The US's 1917 entry to the war overcame objections, and DST was established in 1918.
The war's end swung the pendulum back. Farmers continued to dislike DST, and many countries repealed it after the war. Britain was an exception: it retained DST nationwide but over the years adjusted transition dates for several reasons, including special rules during the 1920s and 1930s to avoid clock shifts on Easter mornings. The US was more typical: Congress repealed DST after 1919. President Woodrow Wilson, like Willett an avid golfer, vetoed the repeal twice but his second veto was overridden. Only a few US cities retained DST locally thereafter, including New York so that its financial exchanges could maintain an hour of arbitrage trading with London, and Chicago and Cleveland to keep pace with New York. Wilson's successor Warren G. Harding opposed DST as a "deception". Reasoning that people should instead get up and go to work earlier in the summer, he ordered District of Columbia federal employees to start work at 08:00 rather than 09:00 during summer 1922. Some businesses followed suit though many others did not; the experiment was not repeated.
Since Germany's adoption in 1916, the world has seen many enactments, adjustments, and repeals of DST, with similar politics involved.
The history of time in the United States includes DST during both world wars, but no standardization of peacetime DST until 1966. In May 1965, for two weeks, St. Paul, Minnesota and Minneapolis, Minnesota were on different times, when the capital city decided to join most of the nation by starting Daylight Saving Time while Minneapolis opted to follow the later date set by state law. In the mid-1980s, Clorox (parent of Kingsford Charcoal) and 7-Eleven provided the primary funding for the Daylight Saving Time Coalition behind the 1987 extension to US DST, and both Idaho senators voted for it based on the premise that during DST fast-food restaurants sell more French fries, which are made from Idaho potatoes.
In 1992, after a three-year trial of daylight saving in Queensland, Australia, a referendum on daylight saving was held and defeated with a 54.5% 'no' vote – with regional and rural areas strongly opposed, while those in the metropolitan south-east were in favor. In 2005, the Sporting Goods Manufacturers Association and the National Association of Convenience Stores successfully lobbied for the 2007 extension to US DST. In December 2008, the Daylight Saving for South East Queensland (DS4SEQ) political party was officially registered in Queensland, advocating the implementation of a dual-time zone arrangement for Daylight Saving in South East Queensland while the rest of the state maintains standard time. DS4SEQ contested the March 2009 Queensland State election with 32 candidates and received one percent of the statewide primary vote, equating to around 2.5% across the 32 electorates contested. After a three-year trial, more than 55% of Western Australians voted against DST in 2009, with rural areas strongly opposed. On April 14, 2010, after being approached by the DS4SEQ political party, Queensland Independent member Peter Wellington, introduced the Daylight Saving for South East Queensland Referendum Bill 2010 into Queensland Parliament, calling for a referendum at the next State election on the introduction of daylight saving into South East Queensland under a dual-time zone arrangement. The Bill was defeated in Queensland Parliament on June 15, 2011.
In the UK the Royal Society for the Prevention of Accidents supports a proposal to observe SDST's additional hour year-round, but is opposed in some industries, such as postal workers and farmers, and particularly by those living in the northern regions of the UK.
In some Muslim countries DST is temporarily abandoned during Ramadan (the month when no food should be eaten between sunrise and sunset), since the DST would delay the evening dinner. Ramadan took place in July and August in 2012. This concerns at least Morocco and Palestine, although Iran keeps DST during Ramadan. Most Muslim countries do not use DST, partially for this reason.
The 2011 declaration by Russia that it would not turn its clocks back and stay in DST all year long was subsequently followed by a similar declaration from Belarus. The plan generated widespread complaints due to the dark of wintertime morning, and thus was abandoned in 2014. The country changed its clocks to Standard Time on October 26, 2014 - and intends to stay there permanently.
Dispute over benefits and drawbacks.
Proponents of DST generally argue that it saves energy, promotes outdoor leisure activity in the evening (in summer), and is therefore good for physical and psychological health, reduces traffic accidents, reduces crime, or is good for business. Groups that tend to support DST are urban workers, retail businesses, outdoor sports enthusiasts and businesses, tourism operators, and others who benefit from increased light during the evening in summer.
Opponents argue that actual energy savings are inconclusive, that DST increases health risks such as heart attack, that DST can disrupt morning activities, and that the act of changing clocks twice a year is economically and socially disruptive and cancels out any benefit. Farmers have tended to oppose DST.
Common agreement about the day's layout or schedule confers so many advantages that a standard DST schedule has generally been chosen over ad hoc efforts to get up earlier. The advantages of coordination are so great that many people ignore whether DST is in effect by altering their nominal work schedules to coordinate with television broadcasts or daylight. DST is commonly not observed during most of winter, because its mornings are darker; workers may have no sunlit leisure time, and children may need to leave for school in the dark. Since DST is applied to many varying communities, its effects may be very different depending on their culture, light levels, geography, and climate; that is why it is hard to make generalized conclusions about the absolute effects of the practice. Some areas may adopt DST simply as a matter of coordination with others rather than for any direct benefits.
Energy use.
DST's potential to save energy comes primarily from its effects on residential lighting, which consumes about 3.5% of electricity in the United States and Canada. Delaying the nominal time of sunset and sunrise reduces the use of artificial light in the evening and increases it in the morning. As Franklin's 1784 satire pointed out, lighting costs are reduced if the evening reduction outweighs the morning increase, as in high-latitude summer when most people wake up well after sunrise. An early goal of DST was to reduce evening usage of incandescent lighting, once a primary use of electricity. Although energy conservation remains an important goal, energy usage patterns have greatly changed since then, and recent research is limited and reports contradictory results. --> Electricity use is greatly affected by geography, climate, and economics, making it hard to generalize from single studies.
Several studies have suggested that DST increases motor fuel consumption. The 2008 DOE report found no significant increase in motor gasoline consumption due to the 2007 United States extension of DST.
Economic effects.
Retailers, sporting goods makers, and other businesses benefit from extra afternoon sunlight, as it induces customers to shop and to participate in outdoor afternoon sports. In 1984, "Fortune" magazine estimated that a seven-week extension of DST would yield an additional $30 million for 7-Eleven stores, and the National Golf Foundation estimated the extension would increase golf industry revenues $200 million to $300 million. A 1999 study estimated that DST increases the revenue of the European Union's leisure sector by about 3%.
Conversely, DST can adversely affect farmers, parents of young children, and others whose hours are set by the sun and they have traditionally opposed the practice, although some farmers are neutral. One reason why farmers oppose DST is that grain is best harvested after dew evaporates, so when field hands arrive and leave earlier in summer their labor is less valuable. Dairy farmers are another group who complain of the change. Their cows are sensitive to the timing of milking, so delivering milk earlier disrupts their systems. Today some farmers' groups are in favor of DST.
DST also hurts prime-time television broadcast ratings, drive-ins and other theaters.
Changing clocks and DST rules has a direct economic cost, entailing extra work to support remote meetings, computer applications and the like. For example, a 2007 North American rule change cost an estimated $500 million to $1 billion, and Utah State University economist William F. Shughart II has estimated the lost opportunity cost at around $1.7 billion USD. Although it has been argued that clock shifts correlate with decreased economic efficiency, and that in 2000 the daylight-saving effect implied an estimated one-day loss of $31 billion on US stock exchanges, the estimated numbers depend on the methodology. The results have been disputed, and the original authors have refuted the points raised by disputers.
Public safety.
In 1975 the US DOT conservatively identified a 0.7% reduction in traffic fatalities during DST, and estimated the real reduction at 1.5% to 2%, but the 1976 NBS review of the DOT study found no differences in traffic fatalities. In 1995 the Insurance Institute for Highway Safety estimated a reduction of 1.2%, including a 5% reduction in crashes fatal to pedestrians. Others have found similar reductions. Single/Double Summer Time (SDST), a variant where clocks are one hour ahead of the sun in winter and two in summer, has been projected to reduce traffic fatalities by 3% to 4% in the UK, compared to ordinary DST. However, accidents do increase by as much as 11% during the two weeks that follow the end of British Summer Time. It is not clear whether sleep disruption contributes to fatal accidents immediately after the spring clock shifts. A correlation between clock shifts and traffic accidents has been observed in North America and the UK but not in Finland or Sweden. If this effect exists, it is far smaller than the overall reduction in traffic fatalities. A 2009 US study found that on Mondays after the switch to DST, workers sleep an average of 40 minutes less, and are injured at work more often and more severely.
In the 1970s the US Law Enforcement Assistance Administration (LEAA) found a reduction of 10% to 13% in Washington, D.C.'s violent crime rate during DST. However, the LEAA did not filter out other factors, and it examined only two cities and found crime reductions only in one and only in some crime categories; the DOT decided it was "impossible to conclude with any confidence that comparable benefits would be found nationwide". Outdoor lighting has a marginal and sometimes even contradictory influence on crime and fear of crime.
In several countries, fire safety officials encourage citizens to use the two annual clock shifts as reminders to replace batteries in smoke and carbon monoxide detectors, particularly in autumn, just before the heating and candle season causes an increase in home fires. Similar twice-yearly tasks include reviewing and practicing fire escape and family disaster plans, inspecting vehicle lights, checking storage areas for hazardous materials, reprogramming thermostats, and seasonal vaccinations. Locations without DST can instead use the first days of spring and autumn as reminders.
Health.
[[File:Greenwich GB DaylightChart.png|upright=1.25|thumb|
alt=Graph of sunrise and sunset times for 2007. The horizontal axis is the date; the vertical axis is the times of sunset and sunrise. There is a bulge in the centre during summer, when sunrise is early and sunset late. There are step functions in spring and fall, when DST starts and stops.|
Clock shifts affecting apparent sunrise and sunset times at Greenwich in 2007]]
DST has mixed effects on health. In societies with fixed work schedules it provides more afternoon sunlight for outdoor exercise. It alters sunlight exposure; whether this is beneficial depends on one's location and daily schedule, as sunlight triggers vitamin D synthesis in the skin, but overexposure can lead to skin cancer. DST may help in depression by causing individuals to rise earlier, but some argue the reverse. The Retinitis Pigmentosa Foundation Fighting Blindness, chaired by blind sports magnate Gordon Gund, successfully lobbied in 1985 and 2005 for US DST extensions.
Clock shifts were found to increase the risk of heart attack by 10 percent, and to disrupt sleep and reduce its efficiency. Effects on seasonal adaptation of the circadian rhythm can be severe and last for weeks. A 2008 study found that although male suicide rates rise in the weeks after the spring transition, the relationship weakened greatly after adjusting for season. A 2008 Swedish study found that heart attacks were significantly more common the first three weekdays after the spring transition, and significantly less common the first weekday after the autumn transition. The government of Kazakhstan cited health complications due to clock shifts as a reason for abolishing DST in 2005. In March 2011, Dmitri Medvedev, president of Russia, claimed that "stress of changing clocks" was the motivation for Russia to stay in DST all year long. Officials at the time talked about an annual increase in suicides.
An unexpected adverse effect of daylight saving time may lie in the fact that an extra part of morning rush hour traffic occurs before dawn and traffic emissions then cause higher air pollution than during daylight hours.
Complexity.
DST's clock shifts have the obvious disadvantage of complexity. People must remember to change their clocks; this can be time-consuming, particularly for mechanical clocks that cannot be moved backward safely. People who work across time zone boundaries need to keep track of multiple DST rules, as not all locations observe DST or observe it the same way. The length of the calendar day becomes variable; it is no longer always 24 hours. Disruption to meetings, travel, broadcasts, billing systems, and records management is common, and can be expensive. During an autumn transition from 02:00 to 01:00, a clock reads times from 01:00:00 through 01:59:59 twice, possibly leading to confusion.
Damage to a German steel facility occurred during a DST transition in 1993, when a computer timing system linked to a radio time synchronization signal allowed molten steel to cool for one hour less than the required duration, resulting in spattering of molten steel when it was poured. Medical devices may generate adverse events that could harm patients, without being obvious to clinicians responsible for care. These problems are compounded when the DST rules themselves change; software developers must test and perhaps modify many programs, and users must install updates and restart applications. Consumers must update devices such as programmable thermostats with the correct DST rules, or manually adjust the devices' clocks. A common strategy to resolve these problems in computer systems is to express time using the Coordinated Universal Time (UTC) rather than the local time zone. For example, Unix-based computer systems use the UTC-based Unix time internally.
Some clock-shift problems could be avoided by adjusting clocks continuously or at least more gradually—for example, Willett at first suggested weekly 20-minute transitions—but this would add complexity and has never been implemented.
DST inherits and can magnify the disadvantages of standard time. For example, when reading a sundial, one must compensate for it along with time zone and natural discrepancies. Also, sun-exposure guidelines such as avoiding the sun within two hours of noon become less accurate when DST is in effect.
Terminology.
As explained by Richard Meade in the English Journal of the (American) National Council of Teachers of English, the form "daylight savings time" (with an "s") was already in 1978 much more common than the older form "daylight saving time" in American English ("the change has been virtually accomplished"). Nevertheless, even dictionaries such as Merriam-Webster's, American Heritage, and Oxford, which describe actual usage instead of prescribing outdated usage (and therefore also list the newer form), still list the older form first. This is because the older form is still very common in print and preferred by many editors. ("Although "daylight saving time" is considered correct, "daylight savings time" (with an "s") is commonly used.") The first two words are sometimes hyphenated ("daylight-saving time"). Merriam-Webster's also lists the forms daylight saving (without "time"), daylight savings (without "time"), and daylight time.
In Britain, Willett's 1907 proposal used the term "daylight saving", but by 1911 the term "summer time" replaced "daylight saving time" in draft legislation. Continental Europe uses similar phrases, such as "Sommerzeit" in Germany, "zomertijd" in Dutch-speaking regions, "kesäaika" in Finland, "horario de verano" or "hora de verano" in Spain and "heure d'été" in France, whereas in Italy the term is "ora legale", that is, legal time (legally enforced time) as opposed to "ora solare", solar time, in winter.
The name of local time typically changes when DST is observed. American English replaces "standard" with "daylight": for example, "Pacific Standard Time" ("PST") becomes "Pacific Daylight Time" ("PDT"). In the United Kingdom, the standard term for UK time when advanced by one hour is "British Summer Time" (BST), and British English typically inserts "summer" into other time zone names, e.g. "Central European Time" ("CET") becomes "Central European Summer Time" ("CEST").
The North American mnemonic "spring forward, fall back" (also "spring ahead ...", "spring up ...", and "... fall behind") helps people remember which direction to shift clocks.
Computing.
[[File:Daylightsavings.svg|thumb|upright|
alt=Strong man in sandals and with shaggy hair, facing away from audience/artist, grabbing a hand of a clock bigger than he is and attempting to force it backwards. The clock uses Roman numerals and the man is dressed in stripped-down Roman gladiator style. The text says "You can't stop time... But you can turn it back one hour at 2 a.m. on Oct. 28 when daylight-saving time ends and standard time begins."|
A 2001 US public service advertisement reminded people to adjust clocks.]]
Changes to DST rules cause problems in existing computer installations. For example, the 2007 change to DST rules in North America required that many computer systems be upgraded, with the greatest impact on e-mail and calendar programs. The upgrades required a significant effort by corporate information technologists.
Some applications standardize on UTC to avoid problems with clock shifts and time zone differences.
Likewise, most modern operating systems internally handle and store all times as UTC and only convert to local time for display.
However, even if UTC is used internally, the systems still require information on time zones to correctly calculate local time where it is needed. Many systems in use today base their date/time calculations from data derived from the IANA time zone database also known as zoneinfo.
IANA time zone database.
The IANA time zone database maps a name to the named location's historical and predicted clock shifts. This database is used by many computer software systems, including most Unix-like operating systems, Java, and the Oracle RDBMS; HP's "tztab" database is similar but incompatible. When temporal authorities change DST rules, zoneinfo updates are installed as part of ordinary system maintenance. In Unix-like systems the TZ environment variable specifies the location name, as in codice_1. In many of those systems there is also a system-wide setting that is applied if the TZ environment variable isn't set: this setting is controlled by the contents of the /etc/localtime file, which is usually a symbolic link or hard link to one of the zoneinfo files. Internal time is stored in timezone-independent epoch time; the TZ is used by each of potentially many simultaneous users and processes to independently localize time display.
Older or stripped-down systems may support only the TZ values required by POSIX, which specify at most one start and end rule explicitly in the value. For example, codice_2 specifies time for the eastern United States starting in 2007. Such a TZ value must be changed whenever DST rules change, and the new value applies to all years, mishandling some older timestamps.
Microsoft Windows.
As with zoneinfo, a user of Microsoft Windows configures DST by specifying the name of a location, and the operating system then consults a table of rule sets that must be updated when DST rules change. Procedures for specifying the name and updating the table vary with release. Updates are not issued for older versions of Microsoft Windows. Windows Vista supports at most two start and end rules per time zone setting. In a Canadian location observing DST, a single Vista setting supports both 1987–2006 and post-2006 time stamps, but mishandles some older time stamps. Older Microsoft Windows systems usually store only a single start and end rule for each zone, so that the same Canadian setting reliably supports only post-2006 time stamps.
These limitations have caused problems. For example, before 2005, DST in Israel varied each year and was skipped some years. Windows 95 used rules correct for 1995 only, causing problems in later years. In Windows 98, Microsoft marked Israel as not having DST, forcing Israeli users to shift their computer clocks manually twice a year. The 2005 Israeli Daylight Saving Law established predictable rules using the Jewish calendar but Windows zone files could not represent the rules' dates in a year-independent way. Partial workarounds, which mishandled older time stamps, included manually switching zone files every year and a Microsoft tool that switches zones automatically. In 2013, Israel standardized its daylight saving time according to the Gregorian calendar.
Microsoft Windows keeps the system real-time clock in local time. This causes several problems, including compatibility when multi booting with operating systems that set the clock to UTC, and double-adjusting the clock when multi booting different Windows versions, such as with a rescue boot disk. This approach is a problem even in Windows-only systems: there is no support for per-user timezone settings, only a single system-wide setting. In 2008 Microsoft hinted that future versions of Windows will partially support a Windows registry entry RealTimeIsUniversal that had been introduced many years earlier, when Windows NT supported RISC machines with UTC clocks, but had not been maintained. Since then at least two fixes related to this feature have been published by Microsoft.
The NTFS file system used by recent versions of Windows stores the file with a UTC time stamp, but displays it corrected to local—or seasonal—time. However, the FAT filesystem commonly used on removable devices stores only the local time. Consequently, when a file is copied from the hard disk onto separate media, its time will be set to the current local time. If the time adjustment is changed, the timestamps of the original file and the copy will be different. The same effect can be observed when compressing and uncompressing files with some file archivers. It is the NTFS file that changes seen time. This effect should be kept in mind when trying to determine if a file is a duplicate of another, although there are other methods of comparing files for equality (such as using a checksum algorithm).
Permanent daylight saving time.
A move to "permanent daylight saving time" (staying on summer hours all year with no time shifts) is sometimes advocated, and has in fact been implemented in some jurisdictions such as Argentina, Chile, Iceland, Singapore, Uzbekistan and Belarus. Advocates cite the same advantages as normal DST without the problems associated with the twice yearly time shifts. However, many remain unconvinced of the benefits, citing the same problems and the relatively late sunrises, particularly in winter, that year-round DST entails. Russia switched to permanent DST from 2011 to 2014, but the move proved unpopular because of the late sunrises in winter, so the country switched permanently back to "standard" or "winter" time in 2014.
Xinjiang, China; Argentina; Chile; Iceland; Russia and other areas skew time zones westward, in effect observing DST year-round without complications from clock shifts. For example, Saskatoon, Saskatchewan, is at longitude, slightly west of center of the idealized Mountain Time Zone , but the time in Saskatchewan is Central Standard Time year-round, so Saskatoon is always about 67 minutes ahead of mean solar time, thus effectively observing daylight saving time year-round. Conversely, northeast India and a few other areas skew time zones eastward, in effect observing negative DST. The United Kingdom and Ireland experimented with year-round DST from 1968 to 1971 but abandoned it because of its unpopularity, particularly in northern regions.
Western France, Spain, and other areas skew time zones and shift clocks, in effect observing DST in winter with an extra hour in summer. Nome, Alaska, is at longitude, which is just west of center of the idealized Samoa Time Zone , but Nome observes Alaska Time with DST, so it is slightly more than two hours ahead of the sun in winter and three in summer. Double daylight saving time has been used on occasion; for example, it was used in some European countries during and shortly after World War II when it was referred to as "Double Summer Time". See British Double Summer Time and Central European Midsummer Time for details.

</doc>
<doc id="47552" url="https://en.wikipedia.org/wiki?curid=47552" title="Let's roll">
Let's roll

"Let's roll" is a colloquial catchphrase that has been used extensively as a command to move and start an activity, attack, mission or project.
Pre–September 11, 2001 usage.
The phrase may have its origins as early as 1908 in the cadence song now called "The Army Goes Rolling Along", which likely extended into tank usage. "The Roads Must Roll", a science fiction story written in 1940 by Robert A. Heinlein, mentions a re-worded version of "The Roll of the Caissons" called "Road Songs of the Transport Cadets". The protagonist of the 1937 supernatural comedy, "Topper", played by Cary Grant uses the phrase "Let's roll" to his wife, played by Constance Bennett, to indicate they should immediately exit their friend's stuffy office and find a drink. The pair are lighthearted, youthful, irresponsible and impossibly glamorous types, and the line delivery has a decisive insouciance about it. The protagonist of Ernest Hemingway's 1950 novel "Across the River and into the Trees", Colonel Dick Cantwell, based on World War II commander Charles "Buck" Lanham, uses the phrase to his driver. He knows he is facing imminent death, but tries to maintain decency, grace, and a sense of humor. The verb "roll" has been used in both the film and recording industry to signal the beginning of a film or audio recording. "Let's roll" was in common use on 1950s and 1960s police television series such as "Adam-12" and (the original) "Dragnet". It was used at the end of roll call at the beginning of each episode of 1980s TV series "Hill Street Blues". It has appeared, among other places, in "The Transformers" animated series by Optimus Prime before entering battle or embarking on a group journey. The exact phrase was used in Season 1, Episode 3 "More than Meets the Eye" (1984) in preparation for the final showdown with Megatron and the Decepticons (as well as in the 2009 feature film ""). It was used in the 1986 film "Ferris Bueller's Day Off", and the 1987 film "Matewan", where it was used by Baldwin–Felts agents just before a violent attack on striking coal miners. The term was used at the end of the film "Matilda" when the title character was given up for adoption. The toys use the phrase when setting out to rescue Woody in the 1999 animated children's film "Toy Story 2". In the late 1990s, the term "let's roll" was frequently used to initiate a departure from any given place. Hence, the term and true context of the term "let's roll" during this time period was to initiate action from an individual to a group of friends.
Flight 93.
Todd Beamer, a passenger on the hijacked United Airlines Flight 93, tried to place a credit card call through a phone located on the back of a plane seat but was routed to a customer-service representative instead, who passed him on to supervisor Lisa Jefferson. Beamer reported that one passenger was killed and, later, that a flight attendant had told him the pilot and co-pilot had been forced from the cockpit and may have been wounded. He was also on the phone when the plane made its turn in a southeasterly direction, a move that had him briefly panicking. Later, he told the operator that some of the plane's passengers were planning to attack the hijackers and take control of the aircraft. According to Jefferson, Beamer's last audible words were "Are you ready? Okay. Let's roll."

</doc>
<doc id="47556" url="https://en.wikipedia.org/wiki?curid=47556" title="Summertime">
Summertime

Summertime may refer to:

</doc>
<doc id="47563" url="https://en.wikipedia.org/wiki?curid=47563" title="Idiom">
Idiom

An idiom (, "special property", from , "special feature, special phrasing, a peculiarity", f. , "one’s own") is a phrase or a fixed expression that has a figurative, or sometimes literal, meaning. An idiom's figurative meaning is different from the literal meaning. There are thousands of idioms, and they occur frequently in all languages. It is estimated that there are at least twenty-five thousand idiomatic expressions in the English language. Idioms fall into the category of formulaic language.
Examples.
The following sentences contain idioms. The fixed words constituting the idiom in each case are bolded:
Each of the word combinations in bold has at least two meanings: a literal meaning and a figurative meaning. Such expressions that are typical for a language can appear as words, combinations of words, phrases, entire clauses, and entire sentences.
Expressions such as these have figurative meaning. When one says "The devil is in the details", one is not expressing a belief in demons, but rather one means that things may look good on the surface, but upon scrutiny, undesirable aspects are revealed. Similarly, when one says "The early bird gets the worm", one is not suggesting that there is only one worm, rather one means there are plenty of worms, but for the sake of the idiom one plays along, and imagines that there is only one worm; alternatively, the figurative translation of this phrase is that the most attentive and astute individual, or perhaps the hardest working (or simply the first one) gets the desired outcome to a situation or the better product, depending on the context. On the other hand, "Waste not, want not" is completely devoid of a figurative meaning. It counts as an idiom, however, because it has a literal meaning and people keep saying it.
Derivations.
Many idiomatic expressions, in their original use were not figurative but had literal meaning.
For instance:
"spill the beans" meaning to let out a secret originates from an ancient method of democratic voting, wherein a voter would put a bean into one of several cups to indicate which candidate he wanted to cast his vote for. If the jars were spilled before the counting of votes was complete, anyone would be able to see which jar had more beans, and therefore which candidate was the winner. Over time, the practice was discontinued and the idiom became figurative.
"break a leg": meaning good luck in a performance/presentation etc. This common idiom comes from belief in superstitions. The term 'break a leg' appears to come from the belief that one ought not to utter the words 'good luck' to an actor. By wishing someone bad luck, it is supposed that the opposite will occur.
Compositionality.
In linguistics, idioms are usually presumed to be figures of speech contradicting the principle of compositionality. This principle states that the meaning of a whole should be constructed from the meanings of the parts that make up the whole. In other words, one should be in a position to understand the whole if one understands the meanings of each of the parts that make up the whole. The following example is widely employed to illustrate the point:
Understood compositionally, Fred has literally kicked an actual, physical bucket. The much more likely idiomatic reading, however, is non-compositional: Fred is understood to have died. Arriving at the idiomatic reading from the literal reading is unlikely for most speakers. What this means is that the idiomatic reading is, rather, stored as a single lexical item that is now largely independent of the literal reading.
In phraseology, idioms are defined as a sub-type of , the meaning of which is not the regular sum of the meanings of its component parts. John Saeed defines an idiom as collocated words that became affixed to each other until metamorphosing into a fossilised term. This collocation of words redefines each component word in the word-group and becomes an "idiomatic expression". Idioms usually do not translate well; in some cases, when an idiom is translated directly word-for-word into another language, either its meaning is changed or it is meaningless.
When two or three words are often used together in a particular sequence, the words are said to be irreversible binomials, or Siamese twins. Usage will prevent the words from being displaced or rearranged. For example, a person may be left "high and dry" but never "dry and high". This idiom in turn means that the person is left in their former condition rather than being assisted so that their condition improves. Not all Siamese twins are idioms, however. "Chips and dip" is an irreversible binomial, but it refers to literal food items, not idiomatic ones.
Translating idioms.
A literal translation (word-by-word) of opaque idioms will most likely not convey the same meaning in other languages. The following list shows idioms from other languages that are analogous to "kick the bucket" in the English language.
Some idioms are transparent. Much of their meaning does get through if they are taken (or translated) literally. For example, "lay one's cards on the table" meaning to reveal previously unknown intentions, or to reveal a secret. Transparency is a matter of degree; "spill the beans" (to let secret information become known) and "leave no stone unturned" (to do everything possible in order to achieve or find something) are not entirely literally interpretable, but only involve a slight metaphorical broadening. Another category of idioms is a word having several meanings, sometimes simultaneously, sometimes discerned from the context of its usage. This is seen in the (mostly uninflected) English language in polysemes, the common use of the same word for an activity, for those engaged in it, for the product used, for the place or time of an activity, and sometimes for a verb.
Idioms tend to confuse those unfamiliar with them; students of a new language must learn its idiomatic expressions as vocabulary. Many natural language words have "idiomatic origins", but are assimilated, so losing their figurative senses, for example, in Portuguese, the expression "saber de coração" 'to know by heart', with the same meaning as in English, was shortened to 'saber de cor', and, later, to the verb "decorar", meaning "memorize".
In 2015, TED collected 40 examples of bizarre idioms that cannot be translated literally. They include the Swedish saying "to slide in on a shrimp sandwich", which refers to somebody who didn't have to work to get where they are."
Dealing with non-compositionality.
The non-compositionality of meaning of idioms challenges theories of syntax. The fixed words of many idioms do not qualify as constituents in any sense, e.g.
The fixed words of this idiom (in bold) do not form a constituent in any theory's analysis of syntactic structure because the object of the preposition (here "this situation") is not part of the idiom (but rather it is an argument of the idiom). One can know that it is not part of the idiom because it is variable, e.g. "How do we get to the bottom of this situation / the claim / the phenomenon / her statement /" etc. What this means is that theories of syntax that take the constituent to be the fundamental unit of syntactic analysis are challenged. The manner in which units of meaning are assigned to units of syntax remains unclear. This problem has motivated a tremendous amount of discussion and debate in linguistics circles and it is a primary motivator behind the Construction Grammar framework.
A relatively recent (1998) development in the syntactic analysis of idioms departs from a constituent-based account of syntactic structure, preferring instead the catena-based account. Any word or any combination of words that are linked together by dependencies qualifies as a catena. The words constituting idioms are stored as catenae in the lexicon, and as such, they are concrete units of syntax. The dependency grammar trees of a few sentences containing non-constituent idioms illustrate the point:
The fixed words of the idiom (in orange) in each case are linked together by dependencies; they form a catena. The material that is outside of the idiom (in normal black script) is not part of the idiom. The following two trees illustrate proverbs:
The fixed words of the proverbs (in orange) again form a catena each time. The adjective "nitty-gritty" and the adverb "always" are not part of the respective proverb and their appearance does not interrupt the fixed words of the proverb. A caveat concerning the catena-based analysis of idioms concerns their status in the lexicon. Idioms are lexical items, which means they are stored as catenae in the lexicon. In the actual syntax, however, some idioms can be broken up by various functional constructions.
The catena-based analysis of idioms provides a basis for an understanding of meaning compositionality. The Principle of Compositionality can in fact be maintained. Units of meaning are being assigned to catenae, whereby many of these catenae are not constituents.

</doc>
<doc id="47565" url="https://en.wikipedia.org/wiki?curid=47565" title="European chub">
European chub

The chub ("Squalius cephalus") is a European species of freshwater fish in the carp family Cyprinidae. It frequents both slow and moderate rivers, as well as canals and still waters of various kinds. In North America, this species is referred to as the European chub. Other names used for the species include round chub, fat chub, chevin, and pollard.
Distribution.
The species is distributed in most of the countries of Europe.
Fishing for chub.
European chub are popular with anglers due to their readiness to feed, and thus to be caught, in almost any conditions. Small chub are freely biting fish which even inexperienced anglers find easy to catch. As they become larger, however, chub become more wary and are easily spooked by noise or visual disturbance. Consequently, large chub (in excess of 2 kg) are keenly sought by anglers who prefer to target specific fish.
The British angling record for chub was broken in May 2007 when Steve White caught a 4.2-kg (9.2-lb) fish from a southern stillwater on a mainline boilie. The chub can reach a maximum length of 60-80 cm (24-31.5 in).
Tackle and tactics.
Smaller chub are not too difficult to catch and on small or medium-sized rivers, a stick-float fishing approach can be adopted or even a swim-feeder and using almost any bait including maggots, luncheon meat, sweetcorn and even small lures and flies. Catching the larger specimens however requires a patient and stealthy approach as most larger chub are caught on the smaller, clearer rivers and as a result, the angler must make their presence as subtle as possible and yet again, not a lot of tackle is required and most anglers may even set their tackle up before they get their favored spot as there is less noise from tackle being set up that may disturb the fish. A classic chub spot is just hanging off (or even inside) branches/bushes brushing through the water as chub are quite sensitive to sunlight and most anglers may fish at sunrise or sunset when the chub leave their entangled home. An angler should also look for where the current is being pushed out, causing a re-circulation pattern behind what ever is pushing the current outwards and this is where lots of food will wash around and where there will probably be feeding fish. Like with the smaller chub, a range of baits can be used but smaller baits such as maggots may attract small fish like minnows (especially on smaller rivers) so a larger bait such as luncheon meat is best used. In terms of the line setup, line ratings of a range of 4-8 pounds breaking-strain is ideal, less experienced anglers should use the tougher rating until they have gained knowledge about 'playing' the fish.
Traditionally a quiver-tip rod is used with at least 4 pound line rating due to the weight of the ledger/feeder (heavier weights need heavier line). In feeder fishing, bait will be put on a hook and inside a swim-feeder which the current will cause to flow out and attract fish to the hook's position so it involves fishing upstream of where the fish are, this usually involves smaller baits like sweetcorn or maggots. The same applies to ledgering except there is a weight (called a ledger) instead of a swim-feeder and usually heavier baits are used here such as luncheon meat. Another method known as touch-ledgering can be used which involves not using a quiver-tip but instead holding the line that is loose off the reel and feeling for any pulls or the line going loose. Some anglers do this without any weights and let the bait slowly drift downstream with the line steadily moving through their hands, slugs and luncheon meat are excellent for this method.
This method could involve fishing under the rod-tip in deep water or letting the float gently drift to where the fish may be situated (known as trotting) whilst throwing portions of bait in the stream to encourage feeding. Usually a lighter rod may be used (no more than 10 foot) and sometimes a centrepin reel is used as it allows the line to smoothly come off the reel. Anglers must strike quickly when trotting as bites can be easy to miss sometimes. Drifting baits such as bread, sweetcorn and maggots are usually used here.
This method is usually for the larger chub, a light spinning/lure rod with a fixed-spool reel of at least 10 pounds line rating as it is easy to snag onto debris when doing this method. Small lures such as bar-spoons and spoons or even small soft-plastics can imitate the smaller fish such as minnows that the larger chub feed on.
Using flies such as damselfly patterns or even larger, dark patterns such as those that imitate slugs can be very good if presented correctly. More obvious, shiny flies that imitate small fish may work for more aggressive chub, this method is all-year but best in warmer months. Nymph patterns also do well. 

</doc>
<doc id="47568" url="https://en.wikipedia.org/wiki?curid=47568" title="Low Earth orbit">
Low Earth orbit

Low-Earth Orbit (LEO) is the region of space around the Earth below an altitude of . This is where the ISS conducts operations. One complete orbit in LEO takes about 90 minutes.
With the exception of the manned lunar flights of the Apollo program, all human spaceflights have taken place in LEO (or were suborbital). The altitude record for a human spaceflight in LEO was Gemini 11 with an apogee of . All manned space stations to date, as well as the majority of satellites, have been in LEO.
Orbital characteristics.
Objects in LEO encounter atmospheric drag in the form of gases in the thermosphere (approximately 80–500 km up) or exosphere (approximately 500 km and up), depending on orbit height. Objects in LEO orbit Earth between the atmosphere and below the inner Van Allen radiation belt. The altitude is usually not less than 300 km for satellites, as that would be impractical due to atmospheric drag.
The orbital velocity needed to maintain a stable low Earth orbit is about 7.8 km/s, but reduces with increased orbital altitude. Calculated for circular orbit of 200 km it is 7.79 km/s and for 1500 km it is 7.12 km/s. The delta-v needed to achieve low Earth orbit starts around 9.4 km/s. Atmospheric and gravity drag associated with launch typically adds 1.3–1.8 km/s to the launch vehicle delta-v required to reach normal LEO orbital velocity of around .
Equatorial low Earth orbits (ELEO) are a subset of LEO. These orbits, with low inclination to the Equator, allow rapid revisit times and have the lowest delta-v requirement (i.e., fuel spend) of any orbit. Orbits with a high inclination angle to the equator are usually called polar orbits.
Higher orbits include medium Earth orbit (MEO), sometimes called intermediate circular orbit (ICO), and further above, geostationary orbit (GEO). Orbits higher than low orbit can lead to early failure of electronic components due to intense radiation and charge accumulation.
Use of LEO.
Although the Earth's pull due to gravity in LEO is not much less than on the surface of the Earth, people and objects in orbit experience weightlessness because they are in free fall.
A low Earth orbit is simplest and cheapest for satellite placement. It provides high bandwidth and low communication time lag (latency), but satellites in LEO will not be visible from any given point on the Earth at all times.
Space debris.
The LEO environment is becoming congested with space debris due to the frequency of object launches. This has caused growing concern in recent years, since collisions at orbital velocities can easily be dangerous, and even deadly. Collisions can produce even more space debris in the process, creating a domino effect, something known as Kessler Syndrome. The Joint Space Operations Center, part of United States Strategic Command (formerly the United States Space Command), currently tracks more than 8,500 objects larger than 10 cm in LEO. However, a limited Arecibo Observatory study suggested there could be approximately one million objects larger than 2 millimeters, which are too small to be visible from Earth-based observatories.

</doc>
<doc id="47569" url="https://en.wikipedia.org/wiki?curid=47569" title="Appian Way">
Appian Way

The Appian Way (Latin and Italian: Via Appia) was one of the earliest and strategically most important Roman roads of the ancient republic. It connected Rome to Brindisi, in southeast Italy. Its importance is indicated by its common name, recorded by Statius:
The road is named after Appius Claudius Caecus, the Roman censor who began and completed the first section as a military road to the south in 312 BC during the Samnite Wars.
Origins.
The need for roads.
The Appian Way was used as a main route for military supplies since its construction for that purpose in 312 B.C. 
The Appian Way was the first long road built specifically to transport troops outside the smaller region of greater Rome (this was essential to the Romans). The few roads outside the early city were Etruscan and went mainly to Etruria. By the late Republic, the Romans had expanded over most of Italy and were masters of road construction. Their roads began at Rome, where the master , or list of destinations along the roads, was located, and extended to the borders of their domain — hence the expression, "All roads lead to Rome". 
The Samnite Wars.
Rome had an affinity for the people of Campania, who, like themselves, traced their backgrounds to the Etruscans. The Samnite Wars were instigated by the Samnites when Rome attempted to ally itself with the city of Capua in Campania. The Italic speakers in Latium had long ago been subdued and incorporated into the Roman state. They were responsible for changing Rome from a primarily Etruscan to a primarily Italic state.
Dense populations of sovereign Samnites remained in the mountains north of Capua, which is just north of the Greek city of Neapolis. Around 343 BC, Rome and Capua attempted to form an alliance, a first step toward a closer unity. The Samnites reacted with military force.
The barrier of the Pontine Marshes.
Between Capua and Rome lay the Pontine Marshes (Pomptinae paludes), a swamp infested with malaria. A tortuous coastal road wound between Ostia at the mouth of the Tiber and Neapolis. The via Latina followed its ancient and scarcely more accessible path along the foothills of Monti Laziali and Monti Lepini, which are visible towering over the former marsh.
In the First Samnite War (343–341 BC) the Romans found they could not support or resupply troops in the field against the Samnites across the marsh. A revolt of the Latin League drained their resources further. They gave up the attempted alliance and settled with Samnium.
Colonization to the southeast.
The Romans were only biding their time while they looked for a solution. The first answer was the colonia, a "cultivation" of settlers from Rome, who would maintain a permanent base of operations. The Second Samnite War (327–304 BC) erupted when Rome attempted to place a colony at Cales in 334 and again at Fregellae in 328 on the other side of the marshes. The Samnites, now a major power after defeating the Greeks of Tarentum, occupied Neapolis to try to ensure its loyalty. The Neapolitans appealed to Rome, which sent an army and expelled the Samnites from Neapolis.
Appius Claudius' beginning of the works.
In 312 BC, Appius Claudius Caecus became censor at Rome. He was of the gens Claudia, who were patricians descended from the Sabines taken into the early Roman state. He had been given the name of the founding ancestor of the gens. He was a populist, i.e., an advocate of the common people. A man of inner perspicacity, in the years of success he was said to have lost his outer vision and thus acquired the name , "blind".
Without waiting to be told what to do by the Senate, Appius Claudius began bold public works to address the supply problem. An aqueduct (the Aqua Appia) secured the water supply of the city of Rome. By far the best known project was the road, which ran across the Pontine Marshes to the coast northwest of Naples, where it turned north to Capua. On it, any number of fresh troops could be sped to the theatre of operations, and supplies could be moved en masse to Roman bases without hindrance by either enemy or terrain. It is no surprise that, after his term as censor, Appius Claudius became consul twice, subsequently held other offices, and was a respected consultant to the state even during his later years.
The success of the road.
The road achieved its purpose. The outcome of the Second Samnite War was at last favorable to Rome. In a series of blows the Romans reversed their fortunes, bringing Etruria to the table in 311 BC, the very year of their revolt, and Samnium in 304. The road was the main factor that allowed them to concentrate their forces with sufficient rapidity and to keep them adequately supplied, wherein they became a formidable opponent.
Construction of the road.
The main part of the Appian Way was started and finished in 312 BC.
The road began as a leveled dirt road upon which small stones and mortar were laid. Gravel was laid upon this, which was finally topped with tight fitting, interlocking stones to provide a flat surface. The historian Procopius said that the stones fit together so securely and closely that they appeared to have grown together rather than to have been fitted together. The road was cambered in the middle (for water runoff) and had ditches on either side of the road which were protected by retaining walls.
Between Rome and Lake Albano.
The road began in the Forum Romanum, passed through the Servian Wall at the porta Capena, went through a cutting in the clivus Martis, and left the city. For this stretch of the road, the builders used the via Latina. The building of the Aurelian Wall centuries later required the placing of another gate, the Porta Appia. Outside of Rome the new via Appia went through well-to-do suburbs along the via Norba, the ancient track to the Alban hills, where Norba was situated. The road at the time was a via glarea, a gravel road. The Romans built a high-quality road, with layers of cemented stone over a layer of small stones, cambered, drainage ditches on either side, low retaining walls on sunken portions, and dirt pathways for sidewalks. The via Appia is believed to have been the first Roman road to feature the use of lime cement. The materials were volcanic rock. The surface was said to have been so smooth that you could not distinguish the joints. The Roman section still exists and is lined with monuments of all periods, although the cement has eroded out of the joints, leaving a very rough surface.
Across the marsh.
The road concedes nothing to the Alban hills, but goes straight through them over cuts and fills. The gradients are steep. Then it enters the former Pontine Marshes. A stone causeway of about led across stagnant and foul-smelling pools blocked from the sea by sand dunes. Appius Claudius planned to drain the marsh, taking up earlier attempts, but he failed. The causeway and its bridges subsequently needed constant repair. No one enjoyed crossing the marsh. In 162 BC, Marcus Cornelius Cathegus had a canal constructed along the road to relieve the traffic and provide an alternative when the road was being repaired. Romans preferred using the canal.
Along the coast.
The via Appia picked up the coastal road at Tarracina. However, the Romans straightened it somewhat with cuttings, which form cliffs today. From there the road swerved north to Capua, where, for the time being, it ended. Caudine Forks was not far to the north. The itinerary was Aricia (Ariccia), Tres Tabernae, Forum Appii, Tarracina (Terracina), Fundi (Fondi), Formiae (Formia), Minturnae (Minturno), Suessa (), Casilinum and Capua, but some of these were colonies added after the Samnite Wars. The distance was . The original road had no milestones, as they were not yet in use. A few survive from later times, including a first milestone near the porta Appia.
Extension to Beneventum.
The Third Samnite War (298–290 BC) is perhaps misnamed. It was an all-out attempt by all the neighbors of Rome: Italics, Etruscans and Gauls, to check the power of Rome. The Samnites were the leading people of the conspiracy. Rome dealt the northerners a crushing blow at the Battle of Sentinum in Umbria in 295. The Samnites fought on alone. Rome now placed 13 colonies in Campania and Samnium. It must have been during this time that they extended the via Appia 35 miles beyond Capua past the Caudine forks to a place the Samnites called Maloenton, "passage of the flocks". The itinerary added Calatia, Caudium and Beneventum (not yet called that). Here also ended the via Latina. 
Extension to Apulia and Calabria.
By 290 BC, the sovereignty of the Samnites had ended. The heel of Italy lay open to the Romans. The dates are somewhat uncertain and there is considerable variation in the sources, but during the Third Samnite War the Romans seem to have extended the road to Venusia, where they placed a colony of 20,000 men. After that they were at Tarentum.
Roman expansion alarmed Tarentum, the leading city of the Greek presence (Magna Graecia) in southern Italy. They hired the mercenary, King Pyrrhus of Epirus, in neighboring Greece to fight the Romans on their behalf. In 280 BC the Romans suffered a defeat at the hands of Pyrrhus at the Battle of Heraclea on the coast west of Tarentum. The battle was costly for both sides, prompting Pyrrhus to remark "One more such victory and I am lost." Making the best of it, the Roman army turned on Greek Rhegium and effected a massacre of Pyrrhian partisans there.
Rather than pursue them, Pyrrhus went straight for Rome along the via Appia and then the via Latina. He knew that if he continued on the via Appia he could be trapped in the marsh. Wary of such entrapment on the via Latina also, he withdrew without fighting after encountering opposition at Anagni. Wintering in Campania, he withdrew to Apulia in 279 BC, where, pursued by the Romans, he won a second costly victory at the Battle of Asculum. Withdrawing from Apulia for a Sicilian interlude, he returned to Apulia in 275 BC and started for Campania up the nice Roman road.
Supplied by that same road, the Romans successfully defended the region against Pyrrhus, crushing his army in a two-day fight at the Battle of Beneventum in 275 BC. The Romans renamed the town from "Maleventum" ("site of bad events") to Beneventum ("site of good events") as a result. Pyrrhus withdrew to Greece, where he died in a street fight in Argos in 272 BC. Tarentum fell to the Romans that same year, who proceeded to consolidate their rule over all of Italy.
The Romans pushed the via Appia to the port of Brundisium in 264 BC. The itinerary from Beneventum was now Venusia, Silvium, Tarentum, Uria and Brundisium. The Roman Republic was the government of Italy, for the time being. Appius Claudius died in 273, but in extending the road a number of times, no one has tried to displace his name upon it.
Extension by Trajan.
The emperor Trajan built the Via Traiana, an extension of the Via Appia from Beneventum, reaching Brundisium via Canusium and Barium rather than via Tarentum. This was commemorated by an arch at Beneventum.
Notable historical events along the road.
The crucifixion of Spartacus' army.
In 73 BC, a slave revolt (known as the Third Servile War) under the ex-gladiator of Capua, Spartacus, began against the Romans. Slavery accounted for roughly every third person in Italy.
Spartacus defeated many Roman armies in a conflict that lasted for over two years. While trying to escape from Italy at Brundisium he unwittingly moved his forces into the historic trap in Apulia/Calabria. The Romans were well acquainted with the region. Legions were brought home from abroad and Spartacus was pinned between armies.
On his defeat the Romans judged that the slaves had forfeited their right to live. In 71 BC, 6,000 slaves were crucified along the Via Appia from Rome to Capua.
The World War II battle of Anzio.
In 1943, during World War II, the Allies fell into the same trap Pyrrhus had retreated to avoid, in the Pomptine fields, the successor to the Pomptine marshes. The marsh remained, despite many efforts to drain it, until engineers working for Benito Mussolini finally succeeded. (Even so, the fields were infested with malarial mosquitos until the advent of DDT in 1950s.)
Hoping to break a stalemate at Monte Cassino, the Allies landed on the coast of Italy at Nettuno, ancient Antium, which was midway between Ostia and Terracina. They found that the place was undefended. They intended to move along the line of the via Appia to take Rome, outflanking Monte Cassino, but they did not do so quickly enough. The Germans occupied Mounts Laziali and Lepini along the track of the old Via Latina, from which they rained down shells on Anzio. Even though the Allies expanded into all the Pomptine region, they gained no ground. The Germans counterattacked down the via Appia from the Alban hills in a front four miles wide, but could not retake Anzio. The battle lasted for four months, one side being supplied by sea, the other by land through Rome. In May 1944, the Allies broke out of Anzio and took Rome. The German forces escaped to the north of Florence.
1960 Summer Olympics.
For the 1960 Summer Olympics, it served as part of the men's marathon course that was won by Abebe Bikila of Ethiopia.
Main sights.
Via Appia antica.
After the fall of the Western Roman Empire, the road fell out of use; Pope Pius VI ordered its restoration. A new Appian Way was built in parallel with the old one in 1784 as far as the Alban Hills region. The new road is the "Via Appia Nuova" ("New Appian Way") as opposed to the old section, now known as "Via Appia Antica". The old Appian Way close to Rome is now a free tourist attraction. It was extensively restored for Rome's Millennium and Great Jubilee celebrations. The first are still heavily used by cars, buses and coaches but from then on traffic is very light and the ruins can be explored on foot in relative safety. The Church of Domine Quo Vadis is in the second mile of the road. Along or close to the part of the road closest to Rome, there are three catacombs of Roman and early Christian origin and one of Jewish origin.
The construction of Rome's ring road, the Grande Raccordo Anulare or GRA, in 1951 caused the Appian Way to be cut in two. More recent improvements to the GRA have rectified this through the construction of a tunnel under the Appia, so that it is now possible to follow the Appia on foot for about from its beginning near the Baths of Caracalla.
Many parts of the original road beyond Rome's environs have been preserved, and some are now used by cars (for example, in the area of Velletri). The road inspires the last movement of Ottorino Respighi's "Pini di Roma". To this day the Via Appia contains the longest stretch of straight road in Europe, totaling .
Monuments along the Via Appia.
Roman bridges along the road.
There are the remains of several Roman bridges along the road, including the Ponte di Tre Ponti, Ponte di Vigna Capoccio, Viadotta di Valle Ariccia, Ponte Alto and Ponte Antico.

</doc>
<doc id="47571" url="https://en.wikipedia.org/wiki?curid=47571" title="American pickerel">
American pickerel

The American pickerels are two subspecies of "Esox americanus", a species of freshwater fish in the pike family (family Esocidae) of order Esociformes: the redfin pickerel, "E. americanus americanus" Gmelin, 1789, and the grass pickerel, "E. americanus vermiculatus" Lesueur, 1846.
Both subspecies are native to North America. They are not to be confused with their aggressive counterpart the Northern pike. The redfin pickerel's range extends from the Saint Lawrence drainage in Quebec down to the Gulf Coast, from Mississippi to Florida, while the grass pickerel's range is further west, extending from the Great Lakes Basin, from Ontario to Michigan, down to the western Gulf Coast, from eastern Texas to Mississippi.
The two subspecies are very similar, but the grass pickerel lacks the redfin's distinctive orange to red fin coloration, its fins having dark leading edges and amber to dusky coloration. In addition, the light areas between the dark bands are generally wider on the grass pickerel and narrower on the redfin pickerel. These pickerels grow to a maximum overall length of and a maximum weight of 2.25 pounds
The redfin and grass pickerels occur primarily in sluggish, vegetated waters of pools, lakes, and swamps, and are carnivorous, feeding on smaller fish. Larger fishes, such as the striped bass ("Morone saxatilis"), bowfin ("Amia calva"), and gray weakfish ("Cynoscion regalis"), in turn, prey on the pickerels when they venture into larger rivers or estuaries.
These fishes reproduce by scattering spherical, sticky eggs in shallow, heavily vegetated waters. The eggs hatch in 11–15 days; the adults guard neither the eggs nor the young.
The "E. americanus" subspecies are not as highly prized as a game fish as their larger cousins, the northern pike and muskellunge, but they are caught by anglers. "McClane's Standard Fishing Encyclopedia" describes ultralight tackle as a sporty if overlooked method to catch these small but voracious pikes.
Lesueur originally classified the grass pickerel as "E. vermiculatus," but it is now considered a subspecies of "E. americanus."
"E. americanus americanus" is sometimes called the brook pickerel. There is no widely accepted English common collective name for the two "E. americanus" subspecies; "American pickerel" is a translation of the systematic name and the French "brochet d'Amérique."

</doc>
<doc id="47572" url="https://en.wikipedia.org/wiki?curid=47572" title="Redfish">
Redfish

Redfish is a common name for several species of fish. It is most commonly applied to certain deep-sea rockfish in the genus "Sebastes", or the reef dwelling snappers in the genus "Lutjanus". It is also applied to the slimeheads or roughies (family Trachichthyidae), and the alfonsinos (Berycidae).

</doc>
<doc id="47574" url="https://en.wikipedia.org/wiki?curid=47574" title="Porgy (novel)">
Porgy (novel)

Porgy is a novel written by the American author DuBose Heyward and published by the George H. Doran Company in 1925.
The novel tells the story of Porgy, a crippled street-beggar in the black tenements of Charleston, South Carolina, in the 1920s. The character was based on the real-life Charlestonian Samuel Smalls. Some passages in the novel have the characters speaking in the Gullah language.
The novel was adapted for a 1927 play by Heyward and his wife, playwright Dorothy Heyward. Even before the play had been fully written, Heyward was in discussions with George Gershwin for an operatic version of his novel, which appeared in 1935 as "Porgy and Bess" (renamed to distinguish it from the play).

</doc>
<doc id="47575" url="https://en.wikipedia.org/wiki?curid=47575" title="Weakfish">
Weakfish

The weakfish, "Cynoscion regalis", is a marine fish of the drum family Sciaenidae.
A medium-large, slender, marine fish, it is found along the east coast of North America. The head and back of this fish are dark brown in color with a greenish tinge. The sides have a faint silvery hue with dusky specks, and the belly is white. The origin of its name is based on the weakness of the mouth muscles, which often cause a hook to tear free, allowing the fish to escape. The weakfish grows to 1 m (3 feet) in length and 9 kg (20 pounds) in weight. It is found along the eastern coast of North America from Nova Scotia, Canada to northern Florida, where it is fished both commercially and recreationally. 
Weakfish are also known by the American Indian name Squeteague. In the mid-Atlantic states, the fish is sometimes referred to by the name sea trout, though it is not related to the fishes properly called trout, which are in the family Salmonidae.
The weakfish is the state fish of Delaware.
Management.
Weakfish stocks have been generally low in recent years due to fishing and natural mortality increasing. Management of the species includes gear regulations, seasonal fishing, bycatch limitations, minimum size limits, commercial creed limits, and bycatch reduction gear. It is hoped that these regulations incorporated with others will help weakfish populations come back to a sustainable point.

</doc>
<doc id="47576" url="https://en.wikipedia.org/wiki?curid=47576" title="Striped bass">
Striped bass

The striped bass ("Morone saxatilis"), also called Atlantic striped bass, striper, linesider, pimpfish, rock, or rockfish, is an anadromous Perciforme fish of the Moronidae family found primarily along the Atlantic coast of North America. It has also been widely introduced into inland recreational fisheries across the United States.
The striped bass is the state fish of Maryland, Rhode Island, and South Carolina, and the state saltwater (marine) fish of New York, New Jersey, Virginia, and New Hampshire. They are also found in the Minas Basin, Gaspereau River, and Northumberland Strait in Nova Scotia, Canada, and the Miramichi River and Saint John River in New Brunswick, Canada. 
The history of the striped bass fishery in North America dates back to the Colonial period. Many written accounts by some of the first European settlers describe the immense abundance of striped bass along with alewives traveling and spawning up most rivers in the coastal Northeast.
Morphology and lifespan.
The striped bass is a typical member of the Moronidae family in shape, having a streamlined, silvery body marked with longitudinal dark stripes running from behind the gills to the base of the tail. The maximum scientifically recorded weight is . Common mature size is . Striped bass are believed to live for up to 30 years. The maximum length is . The average size is about and .
Distribution.
Natural distribution.
Striped bass are native to the Atlantic coastline of North America from the St. Lawrence River into the Gulf of Mexico to approximately Louisiana. They are anadromous fish that migrate between fresh and salt water. Spawning takes place in fresh water.
Introductions outside their natural range.
Striped bass have been introduced to the Pacific Coast of North America and into many of the large reservoir impoundments across the United States by state game and fish commissions for the purposes of recreational fishing and as a predator to control populations of gizzard shad. These include: Elephant Butte Lake in New Mexico; Lake Ouachita, Lake Norman in North Carolina; Lake Norfork, Beaver Lake and Lake Hamilton in Arkansas; Lake Thunderbird in Illinois; Lake Pleasant, and Lake Havasu in Arizona; Lake Powell along the Arizona/Utah border; Castaic Lake, Pyramid Lake, Silverwood Lake, Diamond Valley Lake, and San Francisco Bay-Delta in California; Lewis Smith Lake in Alabama; Lake Cumberland in Kentucky; Lake George in Florida; Lake Murray in South Carolina; Lake Lanier in Georgia; Watts Bar Lake, in Tennessee; Lake Mead, Nevada; Lake Texoma, Lake Tawakoni, Lake Whitney, Possum Kingdom Lake, and Lake Buchanan in Texas; Raystown Lake in Pennsylvania; and in Virginia's Smith Mountain Lake and Leesville Lake.
Striped bass have also been introduced into waters in Ecuador, Iran, Latvia, Mexico, Russia, South Africa, and Turkey, primarily for sport fishing and aquaculture.
Environmental factors.
The spawning success of striped bass has been studied in the San Francisco Bay-Delta water system, with a finding that high total dissolved solids (TDS) reduce spawning. At levels as low as 200 mg/l TDS, an observable diminution of spawning productivity occurs. They can be found in lakes, ponds, streams, and wetlands.
Though the population of striped bass was growing and repopulating in the late 1980s and throughout the 1990s, a study executed by the Wildlife and Fisheries Program of the West Virginia University found that the rapid growth of the striped bass population was exerting a tremendous pressure on its prey (river herring, shad, and blueback herring). This pressure on their food source was putting their own population at risk due to the population of prey naturally not coming back to the same spawning areas.
In the United States, the striped bass was designated as a protected game fish in 2007, and executive agencies were directed to use existing legal authorities to prohibit the sale of striped bass caught in federal waters in the Atlantic Ocean and Gulf of Mexico.
In Canada, the province of Quebec designated the striped bass population of the Saint Lawrence as extirpated in 1996. Analysis of available data implicated overfishing and dredging in the disappearance. In 2002, a reintroduction program was successful.
Lifecycle.
Striped bass spawn in fresh water, and although they have been successfully adapted to freshwater habitat, they naturally spend their adult lives in saltwater (i.e., they are anadromous). Four important bodies of water with breeding stocks of striped bass are: Chesapeake Bay, Massachusetts Bay/Cape Cod, Hudson River, and Delaware River. Many of the rivers and tributaries that emptied into the Atlantic, had at one time, breeding stock of striped bass. This occurred until the 1860s. One of the largest breeding areas is the Chesapeake Bay, where populations from Chesapeake and Delaware bays have intermingled. The very few successful spawning populations of freshwater striped bass include Lake Texoma, Lake Weiss (Coosa River), the Colorado River and its reservoirs downstream from and including Lake Powell, and the Arkansas River, as well as Lake Marion (South Carolina) that retained a landlocked breeding population when the dam was built; other freshwater fisheries must be restocked with hatchery-produced fish annually. Stocking of striped bass was discontinued at Lake Mead in 1973 once natural reproduction was verified.
Hybrids with other bass.
Striped bass have also been hybridized with white bass to produce hybrid striped bass also known as wiper, whiterock bass, sunshine bass, and Cherokee bass. These hybrids have been stocked in many freshwater areas across the US.
Fishing for striped bass.
Striped bass are of significant value for sport fishing, and have been introduced to many waterways outside their natural range. A variety of angling methods are used, including trolling and surf casting with topwater lures a good pick for surf casting, as well as bait casting with live and deceased bait. Striped bass will take a number of live and fresh baits, including bunker, clams, eels, sandworms, herring, bloodworms, mackerel, and shad, with the last being an excellent bait for freshwater fishing.
The largest striped bass ever taken by angling was an 81.88-lb (37.14-kg) specimen taken from a boat in Long Island Sound, near the Outer Southwest Reef, off the coast of Westbrook, Connecticut. The all-tackle world record fish was taken by Gregory Myerson on the night of August 4, 2011. The fish took a drifted live eel bait, and fought for 20 minutes before being boated by Myerson. A second hook and leader was discovered in the fish's mouth when it was boated, indicating it had been previously hooked by another angler. The fish measured 54 in length and had a girth of 36 in. The International Game Fish Association declared Myerson's catch the new all-tackle world record striped bass on October 19, 2011. In addition to now holding the All-Tackle record, Meyerson's catch also landed him the new IGFA men’s 37-kg (80-lb) line class record for striped bass, which previously stood at 70 lb. The previous all-tackle world record fish was a 78.5-lb (35.6-kg) specimen taken in Atlantic City, New Jersey on September 21, 1982 by Albert McReynolds, who fought the fish from the beach for 1:20 after it took his Rebel artificial lure.
Recreational bag limits vary by state and province.
Landlocked striped bass.
Striped bass are an anadromous fish, so their spawning ritual of traveling up rivers to spawn led some of them to become landlocked during lake dam constructions. The first area where they became landlocked was documented to be in the Santee-Cooper River during the construction of the two dams that impounded Lakes Moultrie and Marion, and because of this, the state game fish of South Carolina is the striped bass.
Recently, biologists came to believe that striped bass stayed in rivers for long periods of time, with some not returning to sea unless temperature changes forced migration. Once fishermen and biologists caught on to rising striped bass populations, many state natural resources departments started stocking striped bass in local lakes. Striped bass still continue the natural spawn run in freshwater lakes, traveling up river and blocked at the next dam, which is why they are landlocked. Landlocked stripers have a hard time reproducing naturally, and one of the few and most successful rivers they have been documented reproducing successfully is the Coosa River in Alabama and Georgia.
A 70.6-lb (32.0-kg) landlocked bass was caught in February 2013 by James Bramlett on the Warrior River in Alabama, a current world record. This fish had a length of and a girth of .
One of the only landlocked striped bass populations in Canada is located in the Grand Lake, Nova Scotia. They migrate out in early April into the Shubenacadie River to spawn. These bass also spawn in the Stewiacke River (a tributary of the Shubenacadie). The Shubenacadie River system is one of five known spawning areas in Canada for striped bass, with the others being the St. Lawerence River, Miramichi River, Saint John River, Annapolis River and Shubenacadie/Stewiacke Rivers.
Management.
The striped bass population declined to less than 5 million by 1982, but efforts by fishermen and management programs to rebuild the stock proved successful, and in 2007, there were nearly 56 million fish, including all ages. Recreational anglers and commercial fisherman caught an unprecedented 3.8 million fish in 2006. The management of the species includes size limits, commercial quotas, and biological reference points for the health of the species. The Atlantic States Marine Fisheries Commission states that striped bass are "not overfished and overfishing is not occurring." Another way to replenish and help repopulate the striped bass population is to reintroduce the species back to original spawning grounds in coastal rivers and estuaries in the Northeast.
As food.
Striped bass has white meat with a mild flavor and a medium texture. It is extremely versatile in that it can be pan-seared, grilled, steamed, poached, roasted, broiled, sautéed, and deep fried (including batter-frying). The flesh can also be eaten raw or pickled.
The primary market forms for fresh bass include headed and gutted (with the head and organs removed) and filets; the primary market forms for frozen bass include headed and gutted and loins. It can also be found in steaks, chunks, or whole. Fresh striped bass is available year-round, and is typically sold in sizes from two to fifteen pounds, and can be sold up to fifty pounds.
Striped bass has firm and flavorful flesh with a large flake. The hybrid striped bass yields more meat, has a more fragile texture, and a blander flavor than wild striped bass. The fish has a mild and distinctive flavor. In recipes, it can be substituted for milder fish like cod, as well as for stronger fish like bluefish. Other fish can substitute it, including weakfish, tilefish, blackfish, small bluefish, catfish, salmon, swordfish, and shark. Striped bass is easily grilled in fillets, and is therefore popular in beach communities.

</doc>
<doc id="47578" url="https://en.wikipedia.org/wiki?curid=47578" title="Muskellunge">
Muskellunge

The muskellunge "(Esox masquinongy)", also known as muskelunge, muscallonge, milliganong, or maskinonge (and often abbreviated "muskie" or "musky"), is a species of large, relatively uncommon freshwater fish native to North America. The muskellunge is the largest member of the pike family, Esocidae. The common name comes from the Ojibwa word "maashkinoozhe", meaning "ugly pike", by way of French "masque allongé" (modified from the Ojibwa word by folk etymology), "elongated face." The French common name is "masquinongé" or "maskinongé".
The muskellunge is known by a wide variety of trivial names including Ohio muskellunge, Great Lakes muskellunge, barred muskellunge, Ohio River pike, Allegheny River pike, jack pike, unspotted muskellunge and the Wisconsin muskellunge.
Description.
Muskellunge closely resemble other esocids such as the northern pike and American pickerel in both appearance and behavior. Like the northern pike and other aggressive pikes, the body plan is typical of ambush predators with an elongated body, flat head, and dorsal, pelvic and anal fins set far back on the body. Muskellunge are typically long and weigh , though some have reached up to and almost . A fish reported at 88 in (224 cm) and 110 lb (50 kg) reportedly caught around 1908 has been identified as a hoax or legend. A fish with a weight of 61.25 lb (27.8 kg) was caught in November 2000 in Georgian Bay, Ontario. The fish are a light silver, brown, or green, with dark vertical stripes on the flank, which may tend to break up into spots. In some cases, markings may be absent altogether, especially in fish from turbid waters. This is in contrast to northern pike, which have dark bodies with light markings. A reliable method to distinguish the two similar species is by counting the sensory pores on the underside of the mandible. A muskie will have seven or more per side, while the northern pike never has more than six. The lobes of the caudal (tail) fin in muskellunge come to a sharper point, while those of northern pike are more generally rounded. In addition, unlike pike, muskies have no scales on the lower half of their opercula.
Habitat.
Muskellunge are found in oligotrophic and mesotrophic lakes and large rivers from northern Michigan, northern Wisconsin, and northern Minnesota through the Great Lakes region, north into Canada, throughout most of the St Lawrence River drainage, and northward throughout the upper Mississippi valley, although the species also extends as far south as Chattanooga in the Tennessee River valley. Also, a small population is found in the Broad River in South Carolina. Several North Georgia reservoirs also have healthy stocked populations of muskie. They are also found in the Red River drainage of the Hudson Bay basin. Muskie were introduced to western Saint John River in the late 1960s and have now spread to many connecting waterways in northern Maine.
They prefer clear waters where they lurk along weed edges, rock outcrops, or other structures to rest. A fish forms two distinct home ranges in summer: a shallow range and a deeper one. The shallow range is generally much smaller than the deeper range due to shallow water heating up. A muskie continually patrols the ranges in search of available food in the appropriate conditions of water temperature.
Diet.
Most of their diets consist of fish, but can also include crayfish, frogs, ducklings, snakes, muskrats, mice, other small mammals, and small birds. The mouth is large with many long, needle-like teeth. Muskies will attempt to take their prey head-first, sometimes in a single gulp. They will take prey items up to 30% of their total length. In the spring, they tend to prefer smaller bait since their metabolism is slower, while large bait are preferred in fall as preparation for winter.
Length and weight.
As muskellunge grow longer they increase in weight, but the relationship between length and weight is not linear. The relationship between them can be expressed by a power-law equation:
The exponent b is close to 3.0 for all species, and c is a constant for each species. For muskellunge, b = 3.325, higher than for many common species, and c = 0.000089 pounds/inch.
This equation implies that a 30-in (76-cm) muskellunge will weigh about 8 lb (3.6 kg), while a 40-in muskellunge will weigh about 18 lb.
Behavior.
Muskellunge are sometimes gregarious, forming small schools in distinct territories. They spawn in mid to late spring, somewhat later than northern pike, over shallow, vegetated areas. A rock or sand bottom is preferred for spawning so the eggs do not sink into the mud and suffocate. The males arrive first and attempt to establish dominance over a territory. Spawning may last from five to 10 days and occurs mainly at night. The eggs are negatively buoyant and slightly adhesive; they adhere to plants and the bottom of the lake. Soon afterward, they are abandoned by the adults. Those embryos which are not eaten by fish, insects, or crayfish hatch within two weeks. The larvae live on yolk until the mouth is fully developed, when they begin to feed on copepods and other zooplankton. They soon begin to prey upon fish. Juveniles generally attain a length of by November of their first year.
Predators.
Adult muskellunge are apex predators where they occur naturally. Only humans pose a threat to an adult but juveniles are consumed by other muskies, northern pike, bass, trout, and occasionally birds of prey. The musky's low reproductive rate and slow growth render populations highly vulnerable to overfishing. This has prompted some jurisdictions to institute artificial propagation programs in an attempt to maintain otherwise unsustainably high rates of angling effort and habitat destruction.
Angling.
Anglers seek large muskies as trophies or for sport. The fish attain impressive swimming speeds, but are not particularly maneuverable. The highest-speed runs are usually fairly short, but they can be quite intense. The muskie can also do headshaking in an attempt to rid itself of a hook. Muskies are known for their strength and for their tendency to leap from the water in stunning acrobatic displays. A challenging fish to catch, the muskie has been called "the fish of ten thousand casts". Anglers tend to use smaller lures in spring or during cold-front conditions and larger lures in fall or the heat of summer. The average lure is long, but longer lures of are not uncommon. Many times, live bait is used in the form of "muskie minnows" or 8- to 12-in-long fish strung on treble hooks. Anglers in many areas are strongly encouraged to practice catch and release when fishing for muskellunge due to their low population. In places where muskie are not native, such as in Maine, anglers are encouraged not to release the fish back into the water because of their alleged negative impact on the populations of trout and other smaller fish species. One strategy for securing this fish is called the figure eight, which is usually done as a muskie follows a cast until the end of a retrieve. 
Subspecies and hybrids.
Though interbreeding with other pike species can complicate the classification of some individuals, zoologists usually recognize up to three subspecies of muskellunge.
The tiger muskellunge ("E. masquinongy" × "lucius" or "E. lucius" × "masquinongy") is a hybrid of the muskie and northern pike. Hybrids are sterile, although females sometimes unsuccessfully engage in spawning motions. Some hybrids are artificially produced and planted for anglers to catch. Tiger muskies grow faster than pure muskies, but do not attain the ultimate size of their pure relatives, as the tiger muskie does not live as long. The body is often quite silvery and largely or entirely without spots, but with indistinct longitudinal bands.
Muskellunge fishing records.
By information from International Game Fish Association IGFA the most outstanding record:

</doc>
<doc id="47579" url="https://en.wikipedia.org/wiki?curid=47579" title="Pollock">
Pollock

Pollock (alternatively spelled pollack; pronounced ) is the common name used for either of the two species of North Atlantic marine fish in the genus Pollachius ("P."). Both "P. pollachius" and "P. virens" are commonly referred to as pollock. Other names for "P. pollachius" include the Atlantic pollock, European pollock, lieu jaune, and lythe; while "P. virens" is sometimes known as Boston blues (distinct from bluefish), coalfish (or coley), silver bills or saithe.
Species.
There are currently two recognized species in this genus:
Description.
Both species can grow to and can weigh up to . "P. virens" has a strongly defined, silvery lateral line running down the sides. Above the lateral line, the color is a greenish black. The belly is white, while "P. pollachius" has a distinctly crocked lateral line, grayish to golden belly and a dark brown back. "P. pollachius" also has a strong under-bite. It can be found in water up to deep over rocks, and anywhere in the water column. Pollock are a "whitefish".
Other fish called pollock.
One member of the genus "Gadus" is also commonly referred to as pollock. This is the Alaska pollock or walleye pollock ("Gadus chalcogrammus") including the form known as the Norwegian pollock. While related (they are also members of the family "Gadidae") to the above pollock species, they are not members of the genus "Pollachius". Alaska pollock generally spawn in late winter and early spring in the southeastern Bering Sea. The Alaska pollock is a significant part of the commercial fishery in the Gulf of Alaska.
Parasites.
Pollock and other species of gadids are plagued by parasites, one of which is the cod worm, "Lernaeocera branchialis", a copepod crustacean. At its final stage, the female parasite, with fertilized eggs, clings to the gills of the fish and metamorphoses into a plump, sinusoidal, wormlike body, with a coiled mass of egg strings at the rear.
As food.
Atlantic pollock is largely considered to be a whitefish, although it is a fairly strongly flavored one. Traditionally a popular source of food in some countries, such as Norway, in the United Kingdom it has previously been largely consumed as a cheaper and versatile alternative to cod and haddock. However, in recent years pollock has become more popular due to over-fishing of cod and haddock. It can now be found in most supermarkets as fresh fillets or prepared freezer items. For example, it is used minced in fish fingers or as an ingredient in imitation crab meat.
Because of its slightly gray color, pollock is often prepared, as in Norway, as fried fish balls, or if juvenile sized, breaded with oatmeal and fried, as in Shetland. Year-old fish are traditionally split, salted and dried over a peat hearth in Orkney, where their texture becomes wooden. The fish can also be salted and smoked and achieve a salmon-like orange color (although it is not closely related to the salmon), as is the case in Germany where the fish is commonly sold as "Seelachs" or sea salmon. In Korea, pollock may be repeatedly frozen and defrosted to create "hwangtae", half-dried to create "ko-da-ri", or fully dried and eaten as "book-o".
In 2009, U.K. supermarket Sainsbury's renamed pollock 'Colin' in a bid to boost of the fish as an alternative to cod. Sainsbury's, which said the new name was derived from the French for cooked pollock ("colin"), launched the product under the banner "Colin and chips can save British cod."
In the U.S. and worldwide, Alaska pollock is the primary fish used by the McDonald's chain in their Filet-O-Fish sandwich.

</doc>
<doc id="47580" url="https://en.wikipedia.org/wiki?curid=47580" title="Kingfish">
Kingfish

Kingfish may refer to:

</doc>
<doc id="47581" url="https://en.wikipedia.org/wiki?curid=47581" title="Archosargus probatocephalus">
Archosargus probatocephalus

Archosargus probatocephalus, the sheepshead, is a marine fish that grows to 30 in (760 mm), but commonly reaches 10 to 20 in. It is deep and compressed in body shape, with five or six dark bars on the side of the body over a gray background. It has sharp dorsal spines. Its diet consists of oysters, clams, and other bivalves, and barnacles, fiddler crabs, and other crustaceans. It has a hard mouth, with several rows of stubby teeth, which help crush the shells of prey.
Range.
The sheepshead is found in coastal waters along the western Atlantic, from Nova Scotia to Brazil, but the greatest concentration is around southwest Florida. Although the Sheepshead Bay section of Brooklyn, in New York City, was named after the fish, it is now rarely found that far north.
Fishing.
As sheepshead feed on bivalves and crustaceans, successful baits include shrimp, sand fleas (mole crabs), clams, fiddler crabs, and mussels. Sheepshead have a knack for stealing bait, so a small hook is necessary. Locating sheepshead with a boat is not difficult: Fishermen look for rocky bottoms or places with obstructions, jetties, and the pilings of bridges and piers. The average weight of a sheepshead is 3 to 4 lb, but some individuals reach the range of 10 to 15 lb.

</doc>
<doc id="47582" url="https://en.wikipedia.org/wiki?curid=47582" title="Blackfish">
Blackfish

Blackfish is a common name of fish.
Species which are called blackfish include:

</doc>
<doc id="47586" url="https://en.wikipedia.org/wiki?curid=47586" title="Fort Montgomery (Hudson River)">
Fort Montgomery (Hudson River)

Fort Montgomery is the name of a fortification built in 1776 by the Continental Army on West Bank of the Hudson River during the American Revolution. It is part of the Hudson River Valley National Heritage Area. Fort Montgomery was one of the first major investments by the Americans in strategic construction projects. Declared a National Historic Landmark, it is owned and operated by the state of New York as the Fort Montgomery State Historic Site.
Background.
The strategic importance of the ability to control navigation along the Hudson River was obvious to both the Americans and the British from the outbreak of open hostilities. The Hudson was the major means for transportation of supplies and troops throughout a large portion of the northeast. The fort was constructed at a site noted as early as the seventeenth century for its strategic advantage in controlling navigation along the river.
A month after the first open armed conflict in Lexington, the Continental Congress resolved on May 25, 1775 to build fortifications in the Hudson highlands for the purpose of protecting and maintaining control of the Hudson River. It noted that "…a post be also taken in the Highlands on each side of Hudson’s River and batteries erected in such a manner as will most effectually prevent any vessels passing that may be sent to harass the inhabitants on the borders of said river…"
Fort Constitution.
James Clinton and Christopher Tappan, both lifetime residents of the area, were sent to scout appropriate locations. The initial site chosen was further to the north, on Martlaer's Island, across from West Point. Plans for the fort called for four bastions. Construction of the fortifications began in the summer of 1775. By November it had 70 cannons. However, difficulties in construction and management of the original plan of fortifications, together with escalating costs, led to abandonment of that project. The site on the north side of Popolopen Creek across from Anthony's Nose was proposed, and in January 1776, the materials and resources from Fort Constitution were redirected to the construction at the new site. Construction began on the new Fort Montgomery in March 1776.
Fort Montgomery in the American Revolution.
Fort Montgomery was located at the confluence of Popolopen Creek with the Hudson River near Bear Mountain in Orange County, New York. The fortifications included a river battery of six 32-pound cannons, a cable chain supported by a boom across the Hudson River (see Hudson River Chain), and landward redoubts connected by ramparts, all situated on a cliff promontory rising 100 feet (30 m) above the river. The fort was commanded by General George Clinton, also the newly appointed governor of the state. Fort Montgomery and its companion fortification Fort Clinton, on the southern bank of the Popolopen, held a combined garrison of roughly 700 American soldiers. These men were from the 5th NY Regiment, Lamb's Artillery, Orange County Militia, and Ulster County Militia.
The strategic importance of the opposite bank of Popolopen Creek was quickly realized, as it was an elevated cliff terrace with a full view of the Fort Montgomery site and could not be left undefended. The Army built the smaller fortification named Fort Clinton at that site. These two forts and their associated cannon batteries effectively controlled this stretch of the Hudson River. The Army also conceived a major engineering project to effectively blockade any naval traffic headed north on the river. In 1776 a chain and boom were built across the river to provide a physical barrier to ships, in addition to the combined firepower of the fortifications, which could be massed against ships.
In July 1776, the New York convention appointed a committee, including John Jay, Robert Livingston, George Clinton and Robert Yates, to "devise and carry into execution" measures for "obstructing the channel of Hudson's river, or annoying the navigation of the said River." Worried about lack of arms, the committee worked to buy more cannons.
Battle of Fort Montgomery.
On October 6, 1777, a combined force of roughly 2,100 Loyalists, Hessians, and British regulars led by Lieutenant General Sir Henry Clinton attacked forts Montgomery and Clinton from the landward side (where the defenses were only partially completed). They had support from cannon fire from British ships on the Hudson River that had passed through the chevaux de frise on the lower river. The land columns attacking from west of the fort consisted of the New York Volunteers, the Loyal American Regiment, Emmerich's Chasseurs, the 57th and the 52nd Regiments of Foot. By the end of the day, both forts had fallen to the British, who burned the forts and tore down the stonework buildings.
The battle was a pyrrhic victory for the British, however. Their campaign against the forts caused delays in reinforcing General John Burgoyne at Saratoga. Americans gained the upper hand at the Battle of Bemis Heights and forced the surrender of Burgoyne ten days later at the Battle of Saratoga, when the reinforcements were still far to the south.
Historic Site.
The site was declared a National Historic Landmark in 1972. A system of trails and interpretive signs guides visitors through the ruins of the fort.
Designed by the architect Salvatore Cuciti, the Visitor Center opened in October 2006. The timber frame building is oriented to provide visitors with a "gun sight" view down the Hudson. Operated as a state museum, it contains artifacts from the site, mannequins representing military units and a detailed model of the fort.

</doc>
<doc id="47588" url="https://en.wikipedia.org/wiki?curid=47588" title="Neurosis">
Neurosis

Neurosis is a class of functional mental disorders involving distress but neither delusions nor hallucinations. Neurosis may also be called psychoneurosis or neurotic disorder.
Symptoms and causes.
There are many different neuroses: obsessive–compulsive disorder, obsessive–compulsive personality disorder, impulse control disorder, anxiety disorder, hysteria, and a great variety of phobias.
According to C. George Boeree, professor emeritus at Shippensburg University, the symptoms of neurosis may involve:
Neurosis may be defined simply as a "poor ability to adapt to one's environment, an inability to change one's life patterns, and the inability to develop a richer, more complex, more satisfying personality."
Jung's theory.
Carl Jung found his approach particularly effective for patients who are well adjusted by social standards but are troubled by existential questions.
Jung found that the unconscious finds expression primarily through an individual's inferior psychological function, whether it is thinking, feeling, sensation, or intuition. The characteristic effects of a neurosis on the dominant and inferior functions are discussed in "Psychological Types".
Jung saw collective neuroses in politics: "Our world is, so to speak, dissociated like a neurotic." (Jung (1964) p. 85)
Psychoanalytical theory.
According to psychoanalytic theory, neuroses may be rooted in ego defense mechanisms, but the two concepts are not synonymous. Defense mechanisms are a normal way of developing and maintaining a consistent sense of self (i.e., an ego). But only those thoughts and behaviors that produce difficulties in one's life should be called neuroses.
A neurotic person experiences emotional distress and unconscious conflict, which are manifested in various physical or mental illnesses. The definitive symptom is anxiety.
Neurotic tendencies are common and may manifest themselves as acute or chronic anxiety, depression, an obsessive–compulsive disorder, a phobia, or a personality disorder.
Neurosis should not be mistaken for psychosis, which refers to a loss of touch with reality. Neither should it be mistaken for neuroticism, which is a fundamental personality trait according to psychological theory.
Horney's theory.
In her final book, "Neurosis and Human Growth", Karen Horney laid out a complete theory of the origin and dynamics of neurosis.
In her theory, neurosis is a distorted way of looking at the world and at oneself, which is determined by compulsive needs rather than by a genuine interest in the world as it is.
Horney proposed that neurosis is transmitted to a child from his or her early environment and that there are many ways in which this can occur:
The child's initial reality is then distorted by his or her parents' needs and pretenses. Growing up with neurotic caretakers, the child quickly becomes insecure and develops basic anxiety. To deal with this anxiety, the child's imagination creates an idealized self-image:
Once he identifies himself with his idealized image, a number of effects follow. He will make claims on others and on life based on the prestige he feels entitled to because of his idealized self-image. He will impose a rigorous set of standards upon himself in order to try to measure up to that image. He will cultivate pride, and with that will come the vulnerabilities associated with pride that lacks any foundation. Finally, he will despise himself for all his limitations. Vicious circles will operate to strengthen all of these effects.
Eventually, as he grows to adulthood, a particular "solution" to all the inner conflicts and vulnerabilities will solidify. He will be expansive and will display symptoms of narcissism, perfectionism, or vindictiveness. Or he will be self-effacing and compulsively compliant; he will display symptoms of neediness or codependence. Or he will be resigned and will display schizoid tendencies.
In Horney's view, mild anxiety disorders and full-blown personality disorders all fall under her basic scheme of neurosis. These are considered to be variations in the degree of severity and in the individual dynamics.
The opposite of neurosis is a condition which Horney calls self-realization, which is a state of being in which the person responds to the world with the full depth of his or her spontaneous feelings, rather than with anxiety-driven compulsion. Thus the person grows to actualize his or her inborn potentialities. Horney compares this process to an acorn that grows and becomes a tree.
History and etymology.
The term "neurosis" was coined by the Scottish doctor William Cullen in 1769 to refer to "disorders of sense and motion" caused by a "general affection of the nervous system." Cullen used the term to describe various nervous disorders and symptoms that could not be explained physiologically. Physical features, however, were almost inevitably present, and physical diagnostic tests, such as exaggerated knee-jerks, loss of the gag reflex and dermatographia, were used into the 20th century. The meaning of the term was redefined by Carl Jung and Sigmund Freud over the early and middle 20th century. It has continued to be used in psychology and philosophy.
The "Diagnostic and Statistical Manual of Mental Disorders" (DSM) has eliminated the category "neurosis" because of a decision by its editors to provide descriptions of behavior rather than descriptions of hidden psychological mechanisms. This change has been controversial.
According to the "American Heritage Medical Dictionary", "neurosis" is "no longer used in psychiatric diagnosis."
The term is derived from the Greek word νεῦρον ("neuron", "nerve") and the suffix -ωσις "-osis" (diseased or abnormal condition).

</doc>
<doc id="47589" url="https://en.wikipedia.org/wiki?curid=47589" title="480s BC">
480s BC


</doc>
<doc id="47592" url="https://en.wikipedia.org/wiki?curid=47592" title="Waveform">
Waveform

A waveform is the shape and form of a signal such as a wave moving in a physical medium or an abstract representation.
In many cases the medium in which the wave is being propagated does not permit a direct visual image of the form. In these cases, the term "waveform" refers to the shape of a graph of the varying quantity against time or distance. An instrument called an oscilloscope can be used to pictorially represent a wave as a repeating image on a screen.
To be more specific, a waveform is a set graph that represents an audio signal or recording which shows the changes in amplitude over a certain period of time. The amplitude of the signal is measured on the y-axis (vertical), as time is measured below on the x-axis (horizontal).
Most programs show waveforms to give the user a visual aid of what has been recorded by imagery. If the waveform is low, the recording was most likely soft. If the waveform is large and covering most of the image, the recording may have been recorded with the levels high. Waveforms can vary, they may be small when there is just a vocalist singing, but may become much larger when the drums and guitar come in. 
Examples of waveforms.
Common periodic waveforms include ("t" is time):
Other waveforms are often called composite waveforms and can often be described as a combination of a number of sinusoidal waves or other basis functions added together.
The Fourier series describes the decomposition of periodic waveforms, such that any periodic waveform can be formed by the sum of a (possibly infinite) set of fundamental and harmonic components. Finite-energy non-periodic waveforms can be analyzed into sinusoids by the Fourier transform.

</doc>
<doc id="47595" url="https://en.wikipedia.org/wiki?curid=47595" title="Manchuria">
Manchuria

Manchuria () is a modern name given to a large geographic region in Northeast Asia. Depending on the context, Manchuria can either refer to a region that falls entirely within China, or a larger region divided between China and Russia. The region that falls entirely within China is now usually referred to as Northeast China () in China, although "Manchuria" is widely used outside of China to denote the geographical and historical region. This region is the traditional homeland of the Xianbei, Khitan, and Jurchen peoples, who built several states historically. (The Jurchen, later called the Manchus, are those after whom Manchuria is named.)
Extent of Manchuria.
Manchuria can refer to any one of several regions of various size. These are, from smallest to largest:
Etymology and names.
Three centuries and a half must now pass away before entering upon the next act of the Manchu drama. The Nü-chêns had been scotched, but not killed, by their Mongol conquerors, who, one hundred and thirty-four years later (1368), were themselves driven out of China, a pure native dynasty being re-established under the style of Ming, "Bright." During the ensuing two hundred years the Nü-chêns were scarcely heard of, the House of Ming being busily occupied in other directions. Their warlike spirit, however, found scope and nourishment in the expeditions organised against Japan and Tan-lo, or Quelpart, as named by the Dutch, a large island to the south of the Korean peninsula; while on the other hand the various tribes scattered over a portion of the territory known to Europeans as Manchuria, availed themselves of long immunity from attack by the Chinese to advance in civilization and prosperity. It may be noted here that "Manchuria" is unknown to the Chinese or to the Manchus themselves as a geographical expression. The present extensive home of the Manchus is usually spoken of as the Three Eastern Provinces, namely, (1) Shêngking, or Liao-tung, or Kuan-tung, (2) Kirin, and (3) Heilungchiang or Tsitsihar. — Herbert A. Giles, "China and the Manchus", 1912
"Manchuria" is a translation of the Japanese word "Manshū", which dates from the 19th century. The name "Manju" (Manzhou) was invented and given to the Jurchen people by Hong Taiji in 1635 as a new name for their ethnic group; however, the name "Manchuria" was never used by the Manchus or the Qing dynasty itself to refer to their homeland. According to the Japanese scholar Junko Miyawaki-Okada, the Japanese geographer Takahashi Kageyasu was the first to use the term "满洲" (Manshū) as a place name in 1809 in the "Nippon Henkai Ryakuzu", and it was from that work that Westerners adopted the name. "满洲" (Manshū) then began to appear as a place names in more maps created by Japanese like Kondi Jūzō, Takahashi Kageyasu, Baba Sadayoshi and Yamada Ren, and these maps were brought to Europe by the Dutch Philipp von Siebold. According to Nakami Tatsuo, Philip Franz von Siebold was the one who brought the usage of the term Manchuria to Europeans after borrowing it from the Japanese, who were the first to use it in a geographic manner in the eighteenth century although neither the Manchu nor Chinese languages had a term in their own language equivalent to "Manchuria" as a geographic place name. The Manchu and Chinese languages had no such word as "Manchuria" and the word has imperialist connotations. According to Bill Sewell, it was Europeans who first started using the name Manchuria to refer to the location and it is "not a genuine geographic term". The historian Gavan McCormack agreed with Robert H. G. Lee's statement that "The term Manchuria or Man-chou is a modern creation used mainly by westerners and Japanese", with McCormack writing that the term Manchuria is imperialistic in nature and has no "precise meaning" since the Japanese deliberately promoted the use of "Manchuria" as a geographic name to promote its separation from China at the time they were setting up their puppet state of Manchukuo. The Japanese had their own motive for deliberately spreading the usage of the term Manchuria. The historian Norman Smith wrote that "The term 'Manchuria' is controversial". Professor Mariko Asano Tamanoi said that she "should use the term in quotation marks" when referring to Manchuria. In his 2012 dissertation on the Jurchen people to obtain a Doctor of Philosophy degree in History from the University of Washington, Professor Chad D. Garcia noted that usage of the term "Manchuria" is out of favor in "current scholarly practice" and that he had ceased using the term, instead using "the northeast" or referring to specific geographical features.
In the 18th-century Europe, the region later known as "Manchuria" was most commonly referred to as " Tartary". However, the term Manchuria ("Mantchourie", in French) started appearing by the end of the century; French missionaries used it as early as 1800. The French-based geographers Conrad Malte-Brun and Edme Mentelle promoted the use of the term Manchuria ("Mantchourie", in French), along with "Mongolia", "Kalmykia", etc., as more precise terms than Tartary, in their world geography work published in 1804.
During the Qing dynasty, the area of Manchuria was known as the "three eastern provinces" () since 1683 when Jilin and Heilongjiang were separated even though it was not until 1907 that they were turned into actual provinces. The area of Manchuria was then converted into three provinces by the late Qing government in 1907. Since then, the phrase "Three Northeast Provinces" was officially used by the Qing government in China to refer to this region, and the post of Viceroy of the Three Northeast Provinces was established to take charge of these provinces. After the 1911 revolution, which resulted in the collapse of the Manchu-established Qing dynasty, the name of the region where the Manchus originated was known as "the Northeast" in official documents in the newly founded Republic of China, in addition to the "Three Northeast Provinces".
In current Chinese parlance, an inhabitant of "the Northeast", or Northeast China, is a "Northeasterner" (). "The Northeast" is a term that expresses the entire region, encompassing its history, culture, traditions, dialects, cuisines and so forth, as well as the "Three East Provinces" or "Three Northeast Provinces". In China, the term Manchuria () is rarely used today, and the term is often negatively associated with the Japanese imperial legacy in the puppet state of Manchukuo ().
During the Ming dynasty the area where the Jurchens lived was referred to as Nurgan. Nurgan was the area of modern Jilin province in Manchuria.
Manchuria has historically also been referred to as Guandong (), which literally means "east of the pass", a reference to Shanhai Pass in Qinhuangdao in today's Hebei province, at the eastern end of the Great Wall of China. This usage is seen in the expression "Chuǎng Guāndōng" (literally "Rushing into Guandong") referring to the mass migration of Han Chinese to Manchuria in the 19th and 20th centuries. An alternate name, Guanwai (關外; 关外; "Guānwài"; "outside of the pass"), was also used for the region. The name Guandong later came to be used more narrowly for the area of the Kwantung Leased Territory on the Liaodong Peninsula. It is not to be confused with the southern province of Guangdong.
Geography and climate.
Manchuria consists mainly of the northern side of the funnel-shaped North China Craton, a large area of tilled and overlaid Precambrian rocks spanning 100 million hectares. The North China Craton was an independent continent before the Triassic period and is known to have been the northernmost piece of land in the world during the Carboniferous. The Khingan Mountains in the west are a Jurassic mountain range formed by the collision of the North China Craton with the Siberian Craton, which marked the final stage of the formation of the supercontinent Pangaea.
No part of Manchuria was glaciated during the Quaternary, but the surface geology of most of the lower-lying and more fertile parts of Manchuria consists of very deep layers of loess, which have been formed by wind-borne movement of dust and till particles formed in glaciated parts of the Himalayas, Kunlun Shan and Tien Shan, as well as the Gobi and Taklamakan Deserts. Soils are mostly fertile Mollisols and Fluvents except in the more mountainous parts where they are poorly developed Orthents, as well as in the extreme north where permafrost occurs and Orthels dominate.
The climate of Manchuria has extreme seasonal contrasts, ranging from humid, almost tropical heat in the summer to windy, dry, Arctic cold in the winter. This pattern occurs because the position of Manchuria on the boundary between the great Eurasian continental landmass and the huge Pacific Ocean causes complete monsoonal wind reversal.
In the summer, when the land heats faster than the ocean, low pressure forms over Asia and warm, moist south to southeasterly winds bring heavy, thundery rain, yielding annual rainfall ranging from 400 mm (16 in.), or less in the west, to over 1150 mm (45 in.) in the Changbai Mountains. Temperatures in the summer are very warm to hot, with July average maxima ranging from 31 °C (88 °F) in the south to 24 °C (75 °F) in the extreme north. Except in the far north near the Amur River, high humidity causes major discomfort at this time of year.
In the winter, however, the vast Siberian High causes very cold, north to northwesterly winds that bring temperatures as low as −5 °C (23 °F) in the extreme south and −30 °C (−22 °F) in the north where the zone of discontinuous permafrost reaches northern Heilongjiang. However, because the winds from Siberia are exceedingly dry, snow falls only on a few days every winter, and it is never heavy. This explains why corresponding latitudes of North America were fully glaciated during glacial periods of the Quaternary while Manchuria, though even colder, always remained too dry to form glaciers – a state of affairs enhanced by stronger westerly winds from the surface of the ice sheet in Europe.
History.
Early history.
Manchuria was the homeland of several ethnic groups, including the Manchu, Ulchs and Hezhen. Various ethnic groups and their respective kingdoms, including the Sushen, Donghu, Xianbei, Wuhuan, Mohe, Khitan and Jurchens have risen to power in Manchuria. At various times, Han dynasty, Cao Wei dynasty, Western Jin dynasty, Tang dynasty and some other minor kingdoms of China established control in parts of Manchuria and in some cases tributary relations with peoples in the area. Various kingdoms of Korea such as Gojoseon, Buyeo, Goguryeo and Balhae were also established in parts of this area. Finnish linguist Juha Janhunen believes that it was likely that a "Tungusic-speaking elite" ruled Goguryeo and Balhae, describing them as "protohistorical Manchurian states" with part of their population Tungusic, and that the area of southern Manchuria was the origin of Tungusic peoples and was inhabited continuously by them since ancient times. Janhunen rejected opposing theories of Goguryeo and Balhae's ethnic composition. With the Song dynasty to the south, the Khitan people of Inner Mongolia created the Liao dynasty in the region, which went on to control adjacent parts of Northern China as well. The Liao dynasty was the first state to control all of Manchuria.
In the early 12th century the Tungusic Jurchen people, who were Liao's tributaries, overthrew the Liao and formed the Jin dynasty (1115–1234), which went on to control parts of Northern China and Mongolia after a series of successful military campaigns. During the Yuan dynasty (1271–1368), Manchuria was administered under the Liaoyang province. In 1375, Naghachu, a Mongol official of the Mongolia-based Northern Yuan dynasty in Liaoyang province invaded Liaodong, but later surrendered to the Ming dynasty in 1387. In order to protect the northern border areas, the Ming decided to "pacify" the Jurchens in order to deal with its problems with Yuan remnants along its northern border. The Ming solidified control over Manchuria under the Yongle Emperor (1402–1424), establishing the Nurgan Regional Military Commission. Starting in the 1580s, a Jianzhou Jurchen chieftain, Nurhaci (1558–1626), started to unify Jurchen tribes of the region. Over the next several decades, the Jurchen took control of most of Manchuria. In 1616, Nurhaci founded the Later Jin dynasty, later became known as the Qing dynasty.
Chinese cultural and religious influence such as Chinese New Year, the "Chinese god", motifs such as the dragon, spirals, and scrolls, agriculture, husbandry, methods of heating, and material goods such as iron cooking pots, silk, and cotton spread among the Amur natives including the Udeghes, Ulchis, and Nanais.
In 1644, after the Ming dynasty's capital of Beijing was sacked by the peasant rebels, the Jurchens (now called Manchus) allied with Ming general Wu Sangui and seized control of Beijing, overthrowing the short-lived Shun dynasty and establishing Qing dynasty rule (1644–1912) over all of China. The Willow Palisade was a system of ditches and embankments built by the Qing dynasty during the later 17th century to restrict the movement of Han civilians into Jilin and Heilongjiang. Only bannermen, including Chinese bannermen, were allowed to settle in Jilin and Heilongjiang.
After conquering the Ming, the Qing identified their state as "China" (中國, Zhongguo; "Middle Kingdom"), and referred to it as "Dulimbai Gurun" in Manchu. The Qing equated the lands of the Qing state (including Manchuria and present-day Xinjiang, Mongolia, Tibet and other areas) as "China" in both the Chinese and Manchu languages, defining China as a multi-ethnic state, rejecting the idea that China only meant Han areas, proclaiming that both Han and non-Han peoples were part of "China", and using "China" to refer to the Qing in official documents, international treaties, and foreign affairs. The term "Chinese language" (Dulimbai gurun i bithe) referred to Chinese, Manchu, and Mongol languages, and the term "Chinese people" (中國人 Zhongguo ren; Manchu: Dulimbai gurun i niyalma) referred to all Han, Manchus, and Mongol subjects of the Qing. The lands in Manchuria were explicitly stated by the Qing to belong to "China" (Zhongguo, Dulimbai gurun) in Qing edicts and in the Treaty of Nerchinsk.
However Qing rule saw a massively increasing amount of Han Chinese both illegally and legally streaming into Manchuria and settling down to cultivate land as Manchu landlords desired Han Chinese peasants to rent on their land and grow grain, most Han Chinese migrants were not evicted as they went over the Great Wall and Willow Palisade, during the eighteenth century Han Chinese farmed 500,000 hectares of privately owned land in Manchuria and 203,583 hectares of lands which were part of coutrier stations, noble estates, and Banner lands, in garrisons and towns in Manchuria Han Chinese made up 80% of the population.
Han Chinese farmers were resettled from north China by the Qing to the area along the Liao River in order to restore the land to cultivation. Wasteland was reclaimed by Han Chinese squatters in addition to other Han who rented land from Manchu landlords. Despite officially prohibiting Han Chinese settlement on Manchu and Mongol lands, by the 18th century the Qing decided to settle Han refugees from northern China who were suffering from famine, floods, and drought into Manchuria and Inner Mongolia, so that Han Chinese farmed 500,000 hectares in Manchuria and tens of thousands of hectares in Inner Mongolia by the 1780s. The Qianlong Emperor allowed Han Chinese peasants suffering from drought to move into Manchuria despite his having issued edicts in favor of banning them from 1740–1776. Chinese tenant farmers rented or even claimed title to land from the "imperial estates" and Manchu Bannerlands in the area. Besides moving into the Liao area in southern Manchuria, the path linking Jinzhou, Fengtian, Tieling, Changchun, Hulun, and Ningguta was settled by Han Chinese during the Qianlong Emperor's reign, and Han Chinese were the majority in urban areas of Manchuria by 1800. To increase the Imperial Treasury's revenue, the Qing sold formerly Manchu-only lands along the Sungari to Han Chinese at the beginning of the Daoguang Emperor's reign, and Han Chinese filled up most of Manchuria's towns by the 1840s, according to Abbe Huc.
The Russian conquest of Siberia was accompanied by massacres due to indigenous resistance to colonization by the Russian Cossacks, who savagely crushed the natives. At the hands of people like Vasilii Poyarkov in 1645 and Yerofei Khabarov in 1650 some peoples like the Daur were slaughtered by the Russians to the extent that it is now considered to have been genocide. The Daurs initially deserted their villages since they heard about the cruelty of the Russians the first time Khabarov came. The second time he came, the Daurs decided to do battle against the Russians instead but were slaughtered by Russian guns. The indigenous peoples of the Amur region were attacked by Russians who came to be known as "red-beards". The Russian Cossacks were named luocha (羅剎), after Demons found in Buddhist mythology, by the Amur natives because of their cruelty towards the Amur tribes people, who were subjects of the Qing. The Russian proselytization of Eastern Orthodox Christianity to the indigenous peoples along the Amur River was viewed as a threat by the Qing.
In 1858, a weakening Qing Empire was forced to cede Manchuria north of the Amur to Russia under the Treaty of Aigun. In 1860, at the Treaty of Peking, the Russians managed to obtain a further large slice of Manchuria, east of the Ussuri River. As a result, Manchuria was divided into a Russian half known as "Outer Manchuria", and a remaining Chinese half known as "Inner Manchuria". In modern literature, "Manchuria" usually refers to Inner (Chinese) Manchuria. As a result of the Treaties of Aigun and Peking, China lost access to the Sea of Japan.
History after 1860.
Inner Manchuria also came under strong Russian influence with the building of the Chinese Eastern Railway through Harbin to Vladivostok. In the "Chuang Guandong" movement, many Han farmers, mostly from the Shandong peninsula moved there. By 1921, Harbin, northern Manchuria's largest city, had a population of 300,000, including 100,000 Russians. Japan replaced Russian influence in the southern half of Inner Manchuria as a result of the Russo-Japanese War in 1904–1905. Most of the southern branch of the Chinese Eastern Railway was transferred from Russia to Japan, and became the South Manchurian Railway. Japanese influence extended into Outer Manchuria in the wake of the Russian Revolution of 1917, but Outer Manchuria had reverted to Soviet control by 1925. Manchuria was an important region for its rich mineral and coal reserves, and its soil is perfect for soy and barley production. For pre–World War II Japan, Manchuria was an essential source of raw materials. Without occupying Manchuria, the Japanese probably could not have carried out their plan for conquest over Southeast Asia or taken the risk to attack Pearl Harbor and the British Empire in 1941.
It was reported that among Banner people, both Manchu and Chinese (Hanjun) in Aihun, Heilongjiang in the 1920s, would seldom marry with Han civilians, but they (Manchu and Chinese Bannermen) would mostly intermarry with each other. Owen Lattimore reported that during his January 1930 visit to Manchuria, he studied a community in Jilin (Kirin), where both Manchu and Chinese bannermen were settled at a town called Wulakai, and eventually the Chinese Bannermen there could not be differentiated from Manchus since they were effectively Manchufied. The Han civilian population was in the process of absorbing and mixing with them when Lattimore wrote his article.
Around the time of World War I, Zhang Zuolin established himself as a powerful warlord with influence over most of Manchuria. During his rule, the Manchurian economy grew tremendously, backed by immigration of Chinese from other parts of China. The Japanese assassinated him on June 2, 1928, in what is known as the Huanggutun Incident. Following the Mukden Incident in 1931 and the subsequent Japanese invasion of Manchuria, the Japanese declared Inner Manchuria an "independent state", and appointed the deposed Qing emperor Puyi as puppet emperor of Manchukuo. Under Japanese control Manchuria was one of the most brutally run regions in the world, with a systematic campaign of terror and intimidation against the local Russian and Chinese populations including arrests, organised riots and other forms of subjugation. Manchukuo was used as a base to invade the rest of China.
After the atomic bombing of Hiroshima, Japan in 1945, the Soviet Union invaded from Soviet Outer Manchuria as part of its declaration of war against Japan. Soon afterwards, the Chinese communists and nationalists started fighting for control over Manchuria. The communists won in the Liaoshen Campaign and took complete control over Manchuria. With the encouragement of the Soviet Union, Manchuria was then used as a staging ground during the Chinese Civil War for the Communist Party of China, which emerged victorious in 1949. Ambiguities in the treaties that ceded Outer Manchuria to Russia led to dispute over the political status of several islands. This led to armed conflict in 1969, called the Sino-Soviet border conflict. In 2004, Russia agreed to transfer Yinlong Island and one half of Heixiazi Island to the PRC, ending an enduring border dispute.

</doc>
<doc id="47596" url="https://en.wikipedia.org/wiki?curid=47596" title="Korean Peninsula Energy Development Organization">
Korean Peninsula Energy Development Organization

__NOTOC__
The Korean Peninsula Energy Development Organization (KEDO) is an organization founded on March 15, 1995, by the United States, South Korea, and Japan to implement the 1994 U.S.-North Korea Agreed Framework that froze North Korea's indigenous nuclear power plant development centered at the Yongbyon Nuclear Scientific Research Center, that was suspected of being a step in a nuclear weapons program.
KEDO's principal activity is to construct two light water reactor nuclear power plants in North Korea to replace North Korea's Magnox type reactors. The original target year for completion was 2003.
Since then, other members have joined:
KEDO discussions took place at the level of a U.S. Assistant Secretary of State, South Korea's deputy foreign minister, and the head of the Asian bureau of Japan's Foreign Ministry.
The KEDO Secretariat is located in New York.
History.
Formal ground breaking on the site for two light water reactors (LWR) was on August 19, 1997, at Kumho, 30 km north of Sinpo. The Kumho site had been previously selected for two similar sized reactors that had been promised in the 1980s by the Soviet Union, before its collapse.
Soon after the Agreed Framework was signed, U.S. Congress control changed to the Republican Party, who did not support the agreement. Some Republican Senators were strongly against the agreement, regarding it as appeasement. KEDO's first director, Stephen Bosworth, later commented "The Agreed Framework was a political orphan within two weeks after its signature".
Arranging project financing was not easy, and formal invitations to bid were not issued until 1998, by which time the delays were infuriating North Korea. Significant spending on the LWR project did not commence until 2000, with "First Concrete" pouring at the construction site on August 7, 2002. Construction of both reactors was well behind the original schedule.
In the wake of the breakdown of the Agreed Framework in 2003, KEDO has largely lost its function. KEDO ensured that the nuclear power plant project assets at the construction site at Kumho in North Korea and at manufacturers’ facilities around the world ($1.5 billion invested to date) were preserved and maintained. The project was reported to be about 30% complete. One reactor containment building was about 50% complete and another about 15% finished. No key equipment for the reactors has been moved yet to the site.
In 2005 there were reports indicating that KEDO had agreed in principle to terminate the light-water reactor project. On January 9, 2006, it was announced that the project was over and the workers would be returning to their home countries. North Korea demanded compensation and has refused to return the approximately $45 million worth of equipment left behind.

</doc>
<doc id="47599" url="https://en.wikipedia.org/wiki?curid=47599" title="Unconscious">
Unconscious

Unconscious may refer to:

</doc>
<doc id="47600" url="https://en.wikipedia.org/wiki?curid=47600" title="Simple group">
Simple group

In mathematics, a simple group is a nontrivial group whose only normal subgroups are the trivial group and the group itself. A group that is not simple can be broken into two smaller groups, a normal subgroup and the quotient group, and the process can be repeated. If the group is finite, then eventually one arrives at uniquely determined simple groups by the Jordan–Hölder theorem. The complete classification of finite simple groups, completed in 2008, is a major milestone in the history of mathematics.
Examples.
Finite simple groups.
The cyclic group "G" = Z/3Z of congruence classes modulo 3 (see modular arithmetic) is simple. If "H" is a subgroup of this group, its order (the number of elements) must be a divisor of the order of "G" which is 3. Since 3 is prime, its only divisors are 1 and 3, so either "H" is "G", or "H" is the trivial group. On the other hand, the group "G" = Z/12Z is not simple. The set "H" of congruence classes of 0, 4, and 8 modulo 12 is a subgroup of order 3, and it is a normal subgroup since any subgroup of an abelian group is normal. Similarly, the additive group Z of integers is not simple; the set of even integers is a non-trivial proper normal subgroup.
One may use the same kind of reasoning for any abelian group, to deduce that the only simple abelian groups are the cyclic groups of prime order. The classification of nonabelian simple groups is far less trivial. The smallest nonabelian simple group is the alternating group "A"5 of order 60, and every simple group of order 60 is isomorphic to "A"5. The second smallest nonabelian simple group is the projective special linear group PSL(2,7) of order 168, and it is possible to prove that every simple group of order 168 is isomorphic to PSL(2,7).
Infinite simple groups.
The infinite alternating group, i.e. the group of even permutations of the integers, formula_1 is simple. This group can be defined as the increasing union of the finite simple groups formula_2 with respect to standard embeddings formula_3. Another family of examples of infinite simple groups is given by formula_4, where formula_5 is a field and formula_6.
It is much more difficult to construct "finitely generated" infinite simple groups. The first example is due to Graham Higman and is a quotient of the Higman group. Other examples include the infinite Thompson groups "T" and "V". Finitely presented torsion-free infinite simple groups were constructed by Burger-Mozes.
Classification.
There is as yet no known classification for general simple groups.
Finite simple groups.
The finite simple groups are important because in a certain sense they are the "basic building blocks" of all finite groups, somewhat similar to the way prime numbers are the basic building blocks of the integers. This is expressed by the Jordan–Hölder theorem which states that any two composition series of a given group have the same length and the same factors, up to permutation and isomorphism. In a huge collaborative effort, the classification of finite simple groups was declared accomplished in 1983 by Daniel Gorenstein, though some problems surfaced (specifically in the classification of quasithin groups, which were plugged in 2004).
Briefly, finite simple groups are classified as lying in one of 18 families, or being one of 26 exceptions:
Structure of finite simple groups.
The famous theorem of Feit and Thompson states that every group of odd order is solvable. Therefore, every finite simple group has even order unless it is cyclic of prime order.
The Schreier conjecture asserts that the group of outer automorphisms of every finite simple group is solvable. This can be proved using the classification theorem.
History for finite simple groups.
There are two threads in the history of finite simple groups – the discovery and construction of specific simple groups and families, which took place from the work of Galois in the 1820s to the construction of the Monster in 1981; and proof that this list was complete, which began in the 19th century, most significantly took place 1955 through 1983 (when victory was initially declared), but was only generally agreed to be finished in 2004. , work on improving the proofs and understanding continues; see for 19th century history of simple groups.
Construction.
Simple groups have been studied at least since early Galois theory, where Évariste Galois realized that the fact that the alternating groups on five or more points are simple (and hence not solvable), which he proved in 1831, was the reason that one could not solve the quintic in radicals. Galois also constructed the projective special linear group of a plane over a prime finite field, PSL(2,"p"), and remarked that they were simple for "p" not 2 or 3. This is contained in his last letter to Chevalier, and are the next example of finite simple groups.
The next discoveries were by Camille Jordan in 1870. Jordan had found 4 families of simple matrix groups over finite fields of prime order, which are now known as the classical groups.
At about the same time, it was shown that a family of five groups, called the Mathieu groups and first described by Émile Léonard Mathieu in 1861 and 1873, were also simple. Since these five groups were constructed by methods which did not yield infinitely many possibilities, they were called "sporadic" by William Burnside in his 1897 textbook.
Later Jordan's results on classical groups were generalized to arbitrary finite fields by Leonard Dickson, following the classification of complex simple Lie algebras by Wilhelm Killing. Dickson also constructed exception groups of type G2 and E6 as well, but not of types F4, E7, or E8 . In the 1950s the work on groups of Lie type was continued, with Claude Chevalley giving a uniform construction of the classical groups and the groups of exceptional type in a 1955 paper. This omitted certain known groups (the projective unitary groups), which were obtained by "twisting" the Chevalley construction. The remaining groups of Lie type were produced by Steinberg, Tits, and Herzig (who produced 3"D"4("q") and 2"E"6("q")) and by Suzuki and Ree (the Suzuki–Ree groups).
These groups (the groups of Lie type, together with the cyclic groups, alternating groups, and the five exceptional Mathieu groups) were believed to be a complete list, but after a lull of almost a century since the work of Mathieu, in 1964 the first Janko group was discovered, and the remaining 20 sporadic groups were discovered or conjectured in 1965–1975, culminating in 1981, when Robert Griess announced that he had constructed Bernd Fischer's "Monster group". The Monster is the largest sporadic simple group having order of 808,017,424,794,512,875,886,459,904,961,710,757,005,754,368,000,000,000. The Monster has a faithful 196,883-dimensional representation in the 196,884-dimensional Griess algebra, meaning that each element of the Monster can be expressed as a 196,883 by 196,883 matrix.
Classification.
The full classification is generally accepted as starting with the Feit–Thompson theorem of 1962/63, largely lasting until 1983, but only being finished in 2004.
Soon after the construction of the Monster in 1981, a proof, totaling more than 10,000 pages, was supplied that group theorists had successfully listed all finite simple groups, with victory declared in 1983 by Daniel Gorenstein. This was premature – some gaps were later discovered, notably in the classification of quasithin groups, which were eventually replaced in 2004 by a 1,300 page classification of quasithin groups, which is now generally accepted as complete.
Tests for nonsimplicity.
"Sylow's test": Let "n" be a positive integer that is not prime, and let "p" be a prime divisor of "n". If 1 is the only divisor of "n" that is equal to 1 modulo p, then there does not exist a simple group of order "n".
Proof: If "n" is a prime-power, then a group of order "n" has a nontrivial center and, therefore, is not simple. If "n" is not a prime power, then every Sylow subgroup is proper, and, by Sylow's Third Theorem, we know that the number of Sylow p-subgroups of a group of order "n" is equal to 1 modulo "p" and divides "n". Since 1 is the only such number, the Sylow p-subgroup is unique, and therefore it is normal. Since it is a proper, non-identity subgroup, the group is not simple.
"Burnside": A non-Abelian finite simple group has order divisible by at least three distinct primes. This follows from Burnside's p-q theorem.

</doc>
<doc id="47604" url="https://en.wikipedia.org/wiki?curid=47604" title="Eleanor of Castile (disambiguation)">
Eleanor of Castile (disambiguation)

Leonora of Castile or Eleanor of Castile may refer to:

</doc>
<doc id="47607" url="https://en.wikipedia.org/wiki?curid=47607" title="Suspension bridge">
Suspension bridge

A suspension bridge is a type of bridge in which the deck (the load-bearing portion) is hung below suspension cables on vertical suspenders. The first modern examples of this type of bridge were built in the early 19th century. Simple suspension bridges, which lack vertical suspenders, have a long history in many mountainous parts of the world.
This type of bridge has cables suspended between towers, plus vertical "suspender cables" that carry the weight of the deck below, upon which traffic crosses. This arrangement allows the deck to be level or to arc upward for additional clearance. Like other suspension bridge types, this type often is constructed without falsework.
The suspension cables must be anchored at each end of the bridge, since any load applied to the bridge is transformed into a tension in these main cables. The main cables continue beyond the pillars to deck-level supports, and further continue to connections with anchors in the ground. The roadway is supported by vertical suspender cables or rods, called hangers. In some circumstances, the towers may sit on a bluff or canyon edge where the road may proceed directly to the main span, otherwise the bridge will usually have two smaller spans, running between either pair of pillars and the highway, which may be supported by suspender cables or may use a truss bridge to make this connection. In the latter case there will be very little arc in the outboard main cables.
History.
The earliest suspension bridges were ropes slung across a chasm, with a deck possibly at the same level or hung below the ropes such that the rope had a catenary shape.
Precursor.
The Tibetan saint and bridge-builder Thangtong Gyalpo originated the use of iron chains in his version of simple suspension bridges. In 1433, Gyalpo built eight bridges in eastern Bhutan. The last surviving chain-linked bridge of Gyalpo's was the Thangtong Gyalpo Bridge in Duksum en route to Trashi Yangtse, which was finally washed away in 2004. Gyalpo's iron chain bridges did not include a suspended deck bridge which is the standard on all modern suspension bridges today. Instead, both the railing and the walking layer of Gyalpo's bridges used wires. The stress points that carried the screed were reinforced by the iron chains. Before the use of iron chains it is thought that Gyalpo used ropes from twisted willows or yak skins. He may have also used tightly bound cloth.
First.
The first design for a bridge resembling the modern suspension bridge is attributed to Croatian polymath Fausto Veranzio, whose 1595 book "Machinae Novae" included drawings both for a timber and rope suspension bridge, and a hybrid suspension and cable-stayed bridge using iron chains (see gallery below).
The first American iron chain suspension bridge was the Jacob's Creek Bridge (1801) in Westmoreland County, Pennsylvania, designed by inventor James Finley. Finley's bridge was the first to incorporate all of the necessary components of a modern suspension bridge, including a suspended deck which hung by trusses. Finley patented his design in 1808, and published it in the Philadelphia journal, The Port Folio, in 1810.
Early British chain bridges included the Dryburgh Abbey Bridge (1817) and 137 m Union Bridge (1820), with spans rapidly increasing to 176 m with the Menai Bridge (1826), "the first important modern suspension bridge". 
The Clifton Suspension Bridge (designed in 1831, completed in 1864 with a 214 m central span) is one of the longest of the parabolic arc chain type. The current Marlow suspension bridge was designed by William Tierney Clark and was built between 1829 and 1832, replacing a wooden bridge further downstream which collapsed in 1828. It is the only suspension bridge across the non-tidal Thames. The Széchenyi Chain Bridge, spanning the River Danube in Budapest, was also designed by William Clark and it is a larger scale version of Marlow bridge.
Wire-cable.
The first wire-cable suspension bridge was the Spider Bridge at Falls of Schuylkill (1816), a modest and temporary footbridge built following the collapse of James Finley's nearby Chain Bridge at Falls of Schuylkill (1808). The footbridge's span was 124 m, although its deck was only 0.45 m wide.
Development of wire-cable suspension bridges dates to the temporary simple suspension bridge at Annonay built by Marc Seguin and his brothers in 1822. It spanned only 18 m. The first permanent wire cable suspension bridge was Guillaume Henri Dufour's Saint Antoine Bridge in Geneva of 1823, with two 40 m spans. The first with cables assembled in mid-air in the modern method was Joseph Chaley's Grand Pont Suspendu in Fribourg, in 1834.
In the United States, the first major wire-cable suspension bridge was the in Philadelphia, Pennsylvania. Designed by Charles Ellet, Jr. and completed in 1842, it had a span of 109 m. Ellet's Niagara Falls Suspension Bridge (1847–48) was abandoned before completion. It was used as scaffolding for John A. Roebling's double decker railroad and carriage bridge (1855).
The Otto Beit Bridge (1938–39) was the first modern suspension bridge outside the United States built with parallel wire cables.
Structural behavior.
Structural analysis.
The main forces in a suspension bridge of any type are tension in the cables and compression in the pillars. Since almost all the force on the pillars is vertically downwards and they are also stabilized by the main cables, the pillars can be made quite slender, as on the Severn Bridge, on the Wales-England border.
In a suspended deck bridge, cables suspended via towers hold up the road deck. The weight is transferred by the cables to the towers, which in turn transfer the weight to the ground.
Assuming a negligible weight as compared to the weight of the deck and vehicles being supported, the main cables of a suspension bridge will form a parabola (very similar to a catenary, the form the unloaded cables take before the deck is added). One can see the shape from the constant increase of the gradient of the cable with linear (deck) distance, this increase in gradient at each connection with the deck providing a net upward support force. Combined with the relatively simple constraints placed upon the actual deck, this makes the suspension bridge much simpler to design and analyze than a cable-stayed bridge, where the deck is in compression.
Variations.
Underspanned.
In an underspanned suspension bridge, the main cables hang entirely below the bridge deck, but are still anchored into the ground in a similar way to the conventional type. Very few bridges of this nature have been built, as the deck is inherently less stable than when suspended below the cables. Examples include the Pont des Bergues of 1834 designed by Guillaume Henri Dufour; James Smith's Micklewood Bridge; and a proposal by Robert Stevenson for a bridge over the River Almond near Edinburgh.
Roebling's Delaware Aqueduct (begun 1847) consists of three sections supported by cables.
The timber structure essentially hides the cables; and from a quick view, it is not immediately apparent that it is even a suspension bridge.
Suspension cable types.
The main suspension cable in older bridges was often made from chain or linked bars, but modern bridge cables are made from multiple strands of wire. This contributes greater redundancy; a few flawed strands in the hundreds used pose very little threat, whereas a single bad link or eyebar can cause failure of the entire bridge. (The failure of a single eyebar was found to be the cause of the collapse of the Silver Bridge over the Ohio River). Another reason is that as spans increased, engineers were unable to lift larger chains into position, whereas wire strand cables can be largely prepared in mid-air from a temporary walkway.
Deck structure types.
Most suspension bridges have open truss structures to support the roadbed, particularly owing to the unfavorable effects of using plate girders, discovered from the Tacoma Narrows Bridge (1940) bridge collapse. Recent developments in bridge aerodynamics have allowed the re-introduction of plate structures. In the picture of the Yichang Bridge, note the very sharp entry edge and sloping undergirders in the suspension bridge shown. This enables this type of construction to be used without the danger of vortex shedding and consequent aeroelastic effects, such as those that destroyed the original Tacoma Narrows bridge.
Forces.
Three kinds of forces operate on any bridge: the dead load, the live load, and the dynamic load. Dead load refers to the weight of the bridge itself. Like any other structure, a bridge has a tendency to collapse simply because of the gravitational forces acting on the materials of which the bridge is made. Live load refers to traffic that moves across the bridge as well as normal environmental factors such as changes in temperature, precipitation, and winds. Dynamic load refers to environmental factors that go beyond normal weather conditions, factors such as sudden gusts of wind and earthquakes. All three factors must be taken into consideration when building a bridge.
Use other than road and rail.
The principles of suspension used on the large scale may also appear in contexts less dramatic than road or rail bridges. Light cable suspension may prove less expensive and seem more elegant for a cycle or footbridge than strong girder supports. An example of this is the Nescio Bridge in the Netherlands.
Where such a bridge spans a gap between two buildings, there is no need to construct special towers, as the buildings can anchor the cables. Cable suspension may also be augmented by the inherent stiffness of a structure that has much in common with a tubular bridge.
Construction sequence (wire strand cable type).
Typical suspension bridges are constructed using a sequence generally described as follows. Depending on length and size, construction may take anywhere between a year and a half (construction on the original Tacoma Narrows Bridge took only 19 months) up to as long as a decade (the Akashi-Kaikyō Bridge's construction began in May 1986 and was opened in May, 1998 – a total of twelve years).
Longest spans.
Suspension bridges are typically ranked by the length of their main span. These are the ten bridges with the longest spans, followed by the length of the span and the year the bridge opened for traffic:
2016

</doc>
<doc id="47610" url="https://en.wikipedia.org/wiki?curid=47610" title="Great Belt Fixed Link">
Great Belt Fixed Link

The Great Belt Fixed Link () runs between the Danish islands of Zealand and Funen. It consists of three structures: a road suspension bridge and a railway tunnel between Zealand and the small island Sprogø located in the middle of the Great Belt, and a box girder bridge for both road and rail traffic between Sprogø and Funen. The "Great Belt Bridge" (Danish: "Storebæltsbroen") commonly refers to the suspension bridge, although it may also be used to mean the box-girder bridge or the link in its entirety. The suspension bridge, officially known as the East Bridge, has the world's third longest main span (1.6 km), and the longest outside of Asia. It was designed by the Danish engineering firms COWI and Ramboll.
The link replaced the ferry service that had been the primary means of crossing the Great Belt. After more than five decades of speculation and debate, the decision to construct the link was made in 1986; the original intent was to complete the railway link three years before opening the road connection, but the link opened to rail traffic in 1997 and road traffic in 1998. At an estimated cost of DKK 21.4 billion (1988 prices), the link is the largest construction project in Danish history.
Operation and maintenance are performed by "A/S Storebælt" under "Sund & Bælt". Construction and maintenance are financed by tolls on vehicles and trains.
The link has reduced travel times significantly; previously taking about an hour by ferry, the Great Belt can now be crossed in about ten minutes. The construction of the link and the Øresund Bridge have together enabled driving from mainland Europe to Sweden and the rest of Scandinavia through Denmark. Cyclists are not permitted to use the bridge, but cycles may be transported by train or bus.
History.
The Great Belt ferries entered service between the coastal towns of Korsør and Nyborg in 1883, connecting the railway lines on either side of the Belt. In 1957, road traffic was moved to the Halsskov–Knudshoved route, about 1.5 kilometres to the north and close to the fixed link.
Construction drafts for a fixed link were presented as early as the 1850s, with several suggestions appearing in the following decades. The Danish State Railways, responsible for the ferry service, presented plans for a bridge in 1934. The concepts of bridges over Øresund (152m DKK) and Storebælt (257m DKK) were calculated around 1936. In 1948, the Ministry for Public Works (now the Ministry of Transport) established a commission to investigate the implications of a fixed link.
The first law concerning a fixed link was enacted in 1973, but the project was put on hold in 1978 as the Venstre (Liberal) party demanded postponing public spending. Political agreement to restart work was reached in 1986, with a construction law () being passed in 1987.
The design was carried out by the engineering firms COWI and Ramboll together with Dissing+Weitling architecture practice.
Construction of the link commenced in 1988. In 1991, Finland sued Denmark at the International Court of Justice, on the grounds that Finnish-built mobile offshore drilling units would be unable to pass beneath the bridge. The two countries negotiated a financial compensation of 90 million Danish kroner, and Finland withdrew the lawsuit.
The link is estimated to have created a value of 379 billion DKK after 50 years of use.
Construction.
The construction of the fixed link became the biggest building project in the history of Denmark. In order to connect Halsskov on Zealand with Knudshoved on Funen, 18 kilometres to its west, a two-track railway and a four-lane motorway had to be built, via the small island of Sprogø in the middle of the Great Belt. The project comprised three different tasks: the East Bridge for road transport, the East Tunnel for rail transport and the West Bridge for road and rail transport combined. The construction work was carried out by Sundlink Contractors, a consortium of Skanska, Hochtief, Højgaard & Schultz (which built the West Bridge) and Monberg & Thorsen (which built the eight-kilometre section under the Great Belt). The work of lifting and placing the elements was carried out by Ballast Nedam using a floating crane.
The East Bridge.
Built between 1991 and 1998 at a cost of US$950 million, the East Bridge ("Østbroen") is a suspension bridge between Halsskov and Sprogø. It is long with a free span of , making it the world's third-longest suspension bridge span, surpassed only by the Akashi Kaikyō Bridge and Xihoumen Bridge. The Akashi-Kaikyo Bridge was opened two months earlier. The East Bridge had been planned to be completed in time to be the longest bridge, but it was delayed. The vertical clearance for ships is , meaning the world's largest cruise ship, an Oasis-class cruise ship, just fits under with its smokestack folded.
At above sea level, the two pylons of the East Bridge are the highest points on self-supporting structures in Denmark. Some radio masts, such as Tommerup transmitter, are taller.
To keep the main cables tensioned, an anchorage structure on each side of the span is placed below the road deck. After 15 years, the cables have no rust. They were scheduled for a 15million DKK paint job, but due to corroding cables on other bridges, the decision was made to instead install a 70million DKK sealed de-humidifying system in the cables.
Nineteen concrete pillars (12 on the Zealand side, seven by Sprogø), apart, carry the road deck outside the span.
The West Bridge.
The West Bridge ("Vestbroen") is a box girder bridge between Sprogø and Knudshoved. It is long, and has a vertical clearance for ships of . It is actually two separate, adjacent bridges: the northern one carries rail traffic and the southern one road traffic. The pillars of the two bridges rest on common foundations below sea level. The West Bridge was built between 1988 and 1994; its road/rail deck comprises 63 sections, supported by 62 pillars.
The East Tunnel.
The twin bored tunnel tubes of the East Tunnel ("Østtunnelen") are each long. There are 31 connecting tunnels between the two main tunnels, at intervals. The equipment that is necessary for train operation in the tunnels is installed in the connecting tunnels, which also serve as emergency escape routes.
There were delays and cost overruns in the tunnel construction. The plan was to open it in 1993, giving the trains a head start of three years over road traffic, but train traffic started in 1997 and road traffic in 1998. During construction the sea bed gave way and one of the tunnels was flooded. The water continued to rise and reached the end at Sprogø, where it continued into the (still dry) other tunnel. The water damaged two of the four tunnel boring machines, but no workers were injured. Only by placing a clay blanket on the sea bed was it possible to dry out the tunnels. The two damaged machines were repaired and the majority of the tunnelling was undertaken from the Sprogø side. The machines on the Zealand side tunnelled through difficult ground and made little progress. A major fire on one of the Zealand machines in June 1994 stopped these drives and the tunnels were completed by the two Sprogø machines.
A total of 320 compressed air workers were involved in 9018 pressure exposures in the four tunnel-boring machines. The project had a decompression sickness incidence of 0.14% with two workers having long-term residual symptoms.
Traffic implications.
Prior to the opening of the link, an average of 8,000 cars used the ferries across the Great Belt every day. This increased 127 percent the first year of opening due to the so-called traffic leap; new traffic generated by the improved ease, facility and lower price of crossing the Great Belt.
In 2008, an average of 30,200 cars used the link each day. The increase in traffic is partly caused by the general growth of traffic, partly diversion of traffic volume from other ferry services and air services.
The fixed link has produced considerable time savings between eastern and western Denmark. Previously, it took approximately 90 minutes on average to cross the Great Belt in a car with transfer by ferry, including the waiting time at the ports. It took considerably longer during peak periods, such as at weekends and holidays. With the opening of the link, the journey is now between ten and 15 minutes.
By train the time savings are significant as well. The journey has been reduced by 60 minutes, and there are many more seats available because more carriages may be added to a train as the train does not have to fit onto a ferry. The seating capacity offered by DSB across the Great Belt on an ordinary Wednesday has risen from 11,060 seats to 37,490 seats. On Fridays the seating capacity exceeds 40,000 seats.
On the following stretches the shortest travel times are as follows: Copenhagen–Odense 1 hour 15 minutes, Copenhagen–Aarhus 2 hours 30 minutes, Copenhagen–Aalborg 3 hours 55 minutes and Copenhagen–Esbjerg 2 hours 35 minutes.
Flights between Copenhagen and Odense, and between Copenhagen and Esbjerg have ceased, and the train now has the largest market share between Copenhagen and Aarhus.
Together with the Øresund Bridge, the link provides a direct fixed connection between western Continental Europe and northern Scandinavia, eventually connecting all parts of the European Union except Ireland, Malta and Cyprus and outlying islands. Most people from Zealand still prefer taking the ferry between Puttgarden and Rødby, as it is a much shorter distance and provides a needed break for those travelling a long distance.
For freight trains, the fixed links are a large improvement between Sweden and Germany, and between Sweden and the UK. The Sweden-to-Germany ferry system is still used to some extent owing to limited rail capacity, with heavy passenger traffic over the bridges and some single track stretches in southern Denmark and northern Germany.
The Great Belt was used by now defunct night passenger trains between Copenhagen and Germany, which were too long to fit on the ferries. Day trains on the Copenhagen-Hamburg route use the Fehmarn Belt ferries, with short diesel trains.
In 2020 the Fehmarn Belt Fixed Link is expected to be complete with much of this international traffic being shifted from the Great Belt Fixed Link. This more direct route will reduce the rail journey from Hamburg to Copenhagen from 4¾ to 3½ hours.
Toll charge.
In 2014 the vehicle tolls were as follows:
Environmental effects.
Environmental considerations have been an integral part of the project, and have been of decisive significance for the choice of alignment and determination of the design. Environmental considerations were the reason why Great Belt A/S established an environmental monitoring programme in 1988, and initiated co-operation with authorities and external consultants on the definition of environmental concerns during the construction work and the professional requirements to the monitoring programme. This co-operation issued in a report published at the beginning of 1997 on the state of the environment in the Great Belt. The conclusion of the report was that the marine environment was at least as good as before construction work began.
As concerns the water flows, the link must comply with the so-called zero-solution. This has been achieved by deepening parts of the Great Belt, so that the water flow cross section has been increased. This excavation compensates for the blocking effect caused by the bridge pylons and approach ramps. The conclusion of the report is that water flows are now almost at the level they were before the bridge was built.
The fixed link has generated increased road traffic volume, which has meant increased air pollution. However, there has been significant savings in the energy consumption by switching from ferries to the fixed link. Train and car ferries consume much energy for propulsion, high-speed ferries consume large amounts of energy at high speeds, and air transport is highly energy consuming. Domestic air travel over the Great Belt was greatly reduced after the opening of the bridge, with the former air travellers now using trains and private cars.
The larger energy consumption by ferries as opposed to via the fixed link is most clearly seen when comparing short driving distances from areas immediately east or west of the link. For more extended driving distances the difference in energy consumption is smaller, but any transport within Denmark across the link shows very clear energy savings.
During 2009, seven large wind turbines, likely Vestas 3MWs totalling 21MW capacity, were erected in the sea north of Sprogø to contribute to the electrical demand of the Great Belt Link. Their hub heights are about the same level as the road deck of the suspension bridge. Part of the project was to showcase sea wind at the December 2009 Copenhagen climate meeting.
Accidents.
During construction 479 work-related accidents were reported, of which 53 resulted in serious injuries or death. At least seven workers died as a result of work-related accidents.
The West Bridge has been struck by sea traffic twice. While the link was still under construction on 14 September 1993, the ferry M/F Romsø drifted off course in bad weather and hit the West Bridge. At 19:17 on 3 March 2005, the 3,500-ton freighter MV Karen Danielsen crashed into the West Bridge 800 metres from Funen. All traffic across the bridge was halted, effectively cutting Denmark in two. The bridge was re-opened shortly after midnight, after the freighter was pulled free and inspectors had found no structural damage to the bridge.
The East Bridge has so far been in the clear, although on 16 May 2001, the bridge was closed for 10 minutes as the Cambodian 27,000-ton bulk carrier "Bella" was heading straight for one of the anchorage structures. The ship was deflected by a swift response from the navy.
On 5 June 2006, a maintenance vehicle burst into flames in the east-bound railway tunnel at about 21:30. Nobody was hurt; its crew of three fled to the other tunnel and escaped. The fire was put out shortly before midnight, and the vehicle was removed from the tunnel the next day. Train service resumed on 6 June at reduced speed, and normal service was restored on 12 June.

</doc>
<doc id="47611" url="https://en.wikipedia.org/wiki?curid=47611" title="Doomsday Clock">
Doomsday Clock

The Doomsday Clock is a symbolic clock face, representing a countdown to possible global catastrophe (e.g. nuclear war or climate change). It has been maintained since 1947 by the members of the Science and Security Board of the "Bulletin of the Atomic Scientists", who are in turn advised by the Governing Board and the Board of Sponsors, including 18 Nobel Laureates. The closer they set the Clock to midnight, the closer the scientists believe the world is to global disaster.
Originally, the Clock, which hangs on a wall in the Bulletin's office in the University of Chicago, represented an analogy for the threat of global nuclear war; however, since 2007 it has also reflected climate change and new developments in the life sciences and technology that could inflict irrevocable harm to humanity. The most recent officially announced setting—three minutes to midnight (23:57)—was made in January 2015 due to "climate change, global nuclear weapons modernizations, and outsized nuclear weapons arsenals". This setting was retained in January 2016.
History.
The origin of the Clock can be traced to the international group of researchers called the Chicago Atomic Scientists who had participated in the Manhattan Project. After the atomic bombing of Hiroshima and Nagasaki, they started to publish a mimeographed newsletter and then a bulletin. Since its inception, the Clock has been depicted on every cover of the "Bulletin of the Atomic Scientists". Its first representation was in 1947, when bulletin co-founder Hyman Goldsmith asked artist Martyl Langsdorf (wife of Manhattan Project research associate and Szilárd petition signatory Alexander Langsdorf, Jr.) to design a cover for the magazine's June 1947 issue. As Eugene Rabinowitch, another co-founder of the Bulletin, explained later, 
In January 2007, designer Michael Bierut, who was on the "Bulletin's" Governing Board, redesigned the Clock to give it a more modern feel. In 2009, the "Bulletin" ceased its print edition and was one of the first print publications in the US to become entirely digital; the Clock is now found as part of the logo on the "Bulletin's" website. Information about the Doomsday Clock Symposium, a timeline of the Clock's settings, and multimedia shows about the Clock's history and culture can also be found on the "Bulletin"'s website.
The 5th Doomsday Clock Symposium was held on November 14, 2013 in Washington, D.C.; it was a daylong event that was open to the public and featured panelists discussing various issues on the topic "Communicating Catastrophe." There was also an evening event at the Hirshhorn Museum and Sculpture Garden in conjunction with the Hirshhorn's current exhibit, "Damage Control: Art and Destruction Since 1950." The panel discussions, held at the American Association for the Advancement of Science, were streamed live from the "Bulletin's" website, and can still be viewed there. Reflecting international events dangerous to humankind, the Clock's hands have been adjusted twenty one times since its inception in 1947, when the Clock was initially set to seven minutes to midnight (23:53).
Symbolic timepiece changes.
In 1947, during the Cold War, the Clock was started at seven minutes to midnight and was subsequently advanced or rewound per the state of the world and nuclear war prospects. The Clock's setting is decided by the Science and Security Board of the "Bulletin of the Atomic Scientists" and is an adjunct to the essays in the "Bulletin" on global affairs. The Clock is not set and reset in real time as events occur; rather than respond to each and every crisis as it happens, the Science and Security Board meets twice annually to discuss global events in a deliberative manner. The closest nuclear war threat, the Cuban Missile Crisis in 1962, reached crisis, climax, and resolution before the Clock could be set to reflect that possible doomsday. 

</doc>
<doc id="47612" url="https://en.wikipedia.org/wiki?curid=47612" title="Hexactinellid">
Hexactinellid

Hexactinellid sponges are sponges with a skeleton made of four- and/or six-pointed siliceous spicules, often referred to as glass sponges. They are usually classified along with other sponges in the phylum Porifera, but some researchers consider them sufficiently distinct to deserve their own phylum, Symplasma.
Biology.
Glass sponges are relatively uncommon and are mostly found at depths from although the species "Oopsacas minuta" has been found in shallow water, while others have been found much deeper. They are found in all oceans of the world, although they are particularly common in Antarctic and Northern Pacific waters.
They are more-or-less cup-shaped animals, ranging from in height, with sturdy lattice-like internal skeletons made up of fused spicules of silica. The body is relatively symmetrical, with a large central cavity that, in many species, opens to the outside through a sieve formed from the skeleton. Some species of glass sponges are capable of fusing together to create reefs or bioherms. They are generally pale in colour, ranging from white to orange.
Much of the body is composed of syncitial tissue, extensive regions of multinucleate cytoplasm. In particular, the epidermal cells of other sponges are absent, being replaced by a syncitial net of amoebocytes, through which the spicules penetrate. Unlike other sponges, they do not possess the ability to contract.
One ability they do possess is a unique system for rapidly conducting electrical impulses across their bodies, making it possible for them to respond quickly to external stimuli. Glass sponges like "Venus' Flower Basket" have a tuft of fibers that extends outward like an inverted crown at the base of their skeleton. These fibers are long and about the thickness of a human hair.
Glass sponges are different from other sponges in a variety of other ways. For example, most of the cytoplasm is not divided into separate cells by walls but forms a syncytium or continuous mass of cytoplasm with many nuclei (e.g., Reiswig and Mackie, 1983).
These creatures are long lived, but the exact age is hard to measure; one study based on modelling gave an estimated age of a specimen of "Scolymastra joubini" as 23,000 years, which is thought impossible, but is the basis for a listing of ~15,000 years in the AnAge Database.
The shallow water occurrence of hexactinellids is rare world wide. In the Antarctic two species occur as shallow as 33 meters under the ice. In the Mediterranean one species occurs as shallow as in a cave with deep water upwelling (Boury-Esnault & Vacelet (1994))
Reefs.
The sponges form reefs off the coast of British Columbia and Washington State, which are studied in the Sponge Reef Project.
Classification.
The earliest known hexactinellids are from the earliest Cambrian or late Neoproterozoic. They are fairly common relative to demosponges as fossils, but this is thought to be, at least in part, because their spicules are sturdier than spongin and fossilize better. 
Like almost all sponges, the hexactinellids draw water in through a series of small pores by the whip like beating of a series of hairs or flagella in chambers which in this group line the sponge wall. (Sponge Gardens)
The class is divided into six orders, in two subclasses:
Class Hexactinellida

</doc>
<doc id="47615" url="https://en.wikipedia.org/wiki?curid=47615" title="Rook (chess)">
Rook (chess)

A rook (♖ ♜ borrowed from Persian رخ "rokh", Sanskrit रथ "ratha", "chariot") is a piece in the strategy board game of chess. Formerly the piece was called the "tower", "marquess", "rector", and "comes" . The term "castle" is considered informal, incorrect, or old-fashioned.
Each player starts the game with two rooks, one in each of the corner squares on their own side of the board.
Initial placement and movement.
In algebraic notation, the white rooks start on squares "a1" and "h1", while the black rooks start on "a8" and "h8". The rook moves horizontally or vertically, through any number of unoccupied squares (see diagram). As with captures by other pieces, the rook captures by occupying the square on which the enemy piece sits. The rook also participates, with the king, in a special move called castling.
History.
In the medieval shatranj, the rook symbolized a chariot. The Persian word "rukh" means chariot , and the corresponding pieces in east Asian chess games such as xiangqi and shogi have names also meaning chariot (車).
Persian war chariots were heavily armoured, carrying a driver and at least one ranged-weapon bearer, such as an archer. The sides of the chariot were built to resemble fortified stone work, giving the impression of small, mobile buildings, causing terror on the battlefield. However, in the West the rook is almost universally represented as a crenellated turret. One possible explanation is that when the game was imported to Italy, the Persian "rukh" became the Italian word "rocca," meaning fortress, and from there spread in the rest of Europe. Another possible explanation is that rooks represent siege towers – the piece is called "torre", meaning tower, in Italian, Portuguese, and Spanish; "tour" in French; "toren" in Dutch; "Turm" in German; and "torn" in Swedish. An alternative name in (pronounced as toura). Finally, the chariot was sometimes represented as a silhouette, a square with two points above representing the horse's heads, which may have been seen to resemble a building with arrowports to the medieval imagination. An exception is seen in the British Museum's collection of the medieval Lewis chess pieces in which the rooks appear as stern warders or wild-eyed Berzerker warriors. Rooks usually are similar in appearance to small castles, and as a result a rook is sometimes called a "castle" . This usage was common in the past ("The Rook, or Castle, is next in power to the Queen" – Howard Staunton, 1847) but today it is rarely if ever used in chess literature or among players, except in the expression "castling".
The Russian name for the rook ("ladya") means a sailing boat or longship of Northern cultures such as the Vikings.
Strategy.
Relative value.
In general, rooks are stronger than bishops or knights (which are called "minor pieces") and are considered greater in value than either of those pieces by nearly two pawns but less valuable than two minor pieces by approximately a pawn. Two rooks are generally considered to be worth slightly more than a queen (see chess piece relative value). Winning a rook for a bishop or knight is referred to as winning "the exchange". Rooks and queens are called "heavy pieces" or "major pieces", as opposed to bishops and knights, the minor pieces.
Placement.
In the opening, the rooks are blocked in by other pieces and cannot immediately participate in the game; so it is usually desirable to "connect" one's rooks on the first rank by clearing all pieces except the king and rooks from the first rank and then castling. In that position, the rooks support each other, and can more easily move to occupy and control the most favorable files.
A common strategic goal is to place a rook on the first rank of an open file (i.e. one unobstructed by pawns of either player), or a half-open file (i.e., one unobstructed by friendly pawns). From this position, the rook is relatively unexposed to risk but can exert control on every square on the file. If one file is particularly important, a player might advance one rook on it, then position the other rook behind – "doubling" the rooks.
A rook on the seventh rank (the opponent's second rank) is typically very powerful, as it threatens the opponent's unadvanced pawns and hems in the enemy king. A rook on the seventh rank is often considered sufficient compensation for a pawn . In the diagrammed position from a game between Lev Polugaevsky and Larry Evans, the rook on the seventh rank enables White to draw, despite being a pawn down .
"Two" rooks on the seventh rank are often enough to force victory, or at least a draw by perpetual check.
Endgame.
Rooks are most powerful towards the end of a game (i.e., the endgame), when they can move unobstructed by pawns and control large numbers of squares. They are somewhat clumsy at restraining enemy pawns from advancing towards promotion, unless they can occupy the file behind the advancing pawn. As well, a rook best supports a friendly pawn towards promotion from behind it on the same file (see Tarrasch rule).
In a position with a rook and one or two minor pieces versus two rooks, generally in addition to pawns, and possibly other pieces – Lev Alburt advises that the player with the single rook should avoid exchanging the rook for one of his opponent's rooks .
The rook is a very powerful piece to deliver checkmate. Below are a few examples of rook checkmates that are easy to force.
Heraldry.
In Canadian heraldry, the chess rook is the cadency mark of a fifth daughter.
Unicode.
Unicode defines two codepoints for rook:
♖ U+2656 White Chess Rook (HTML &#9814;)
♜ U+265C Black Chess Rook (HTML &#9820;)

</doc>
<doc id="47616" url="https://en.wikipedia.org/wiki?curid=47616" title="Bishop (chess)">
Bishop (chess)

A bishop (♗,♝) is a piece in the board game of chess. Each player begins the game with two bishops. One starts between the king's knight and the king, the other between the queen's knight and the queen. In algebraic notation the starting squares are c1 and f1 for White's bishops, and c8 and f8 for Black's bishops.
Movement.
The bishop has no restrictions in distance for each move, but is limited to diagonal movement. Bishops, like all other pieces except the knight, cannot jump over other pieces. A bishop captures by occupying the square on which an enemy piece sits.
The bishops may be differentiated according to which wing they begin on, i.e. the "king's bishop" and "queen's bishop". As a consequence of its diagonal movement, each bishop always remains on either the white or black squares, and so it is also common to refer to them as "light-squared" or "dark-squared" bishops.
History.
The bishop's predecessor in shatranj (medieval chess) was the alfil, meaning "the elephant," which could leap two squares along any diagonal, and could jump over an intervening piece. As a consequence, each fil was restricted to eight squares, and no fil could attack another. The modern bishop first appeared shortly after 1200 in Courier chess. A piece with this move, called a "cocatriz" or crocodile, is part of the Grande Acedrex in the game book compiled in 1283 for King Alfonso X of Castile. The game is attributed to "India", then a very vague term. About half a century later Muḥammad ibn Maḥmud al-Āmulī, in his "Treasury of the Sciences", describes an expanded form of chess with two pieces moving "like the rook but obliquely".
Derivatives of "alfil" survive in the languages of the two countries where chess were first introduced within Western Europe—Italian ("alfiere") and Spanish ("alfil"). It was known as the "aufin" in French, or the aufin, alphin, or archer in early English.
The term "bishop" first entered the English language in the 16th century, with the first known written example dating back to 1560s. In all other Germanic languages, except for Icelandic, it is called various names, all of which directly translate to English as "runner" or "messenger" (e.g. in Norwegian "Løper", in Danish "Løber", in Swedish "Löpare", in German "Läufer" and in Dutch "loper"; in Finnish, the word is "lähetti", and in Polish, "goniec", both with the same meaning). In Romanian, it is known as "nebun" which refers to a crazy person (similarly to the French name "Fou" (fool) which is most likely derived from "Fou du roi", a jester). In Icelandic, however, it is called "biskup", with the same meaning as in English. Interestingly, the use of the term in Icelandic predates that of the English language, as the first mentioning of "biskup" in Icelandic texts dates back to the early part of the 14th century, while the 12th-century Lewis Chessmen portray the bishop as an unambiguously ecclesiastical figure. In The Saga of Earl Mágus, which was written in Iceland somewhere between 1300–1325, it is described how an emperor was checkmated by a bishop. This has led to some speculations as to the origin of the English use of the term "bishop".
The canonical chessmen date back to the Staunton chess set of 1849. The piece's deep groove symbolizes a bishop's (or abbot's) mitre. Some have written that the groove originated from the original form of the piece, an elephant with the groove representing the elephant's tusks (see photo in the history section). The British chose to call the piece a bishop because the projections at the top resembled a mitre. This groove was interpreted differently in different countries as the game moved to Europe; in France, for example, the groove was taken to be a jester's cap, hence in France the bishop is called "fou" (the jester; the word can also mean madman or gannet).
In some Slavic languages (e.g. Czech/Slovak) the bishop is called "střelec/strelec", which directly translates to English as a "shooter" meaning an archer, while in others it is still known as "elephant" (e.g. Russian "slon"). In South Slavic languages it is usually known as "lovac", meaning "hunter", or "laufer", taken from the German name for the same piece (laufer is also alternative Polish name). An alternative name for bishop in Russian is officer ().
Name translations.
The original name of the bishop was "elephant" (i.e. "war elephant").
Comparison to other pieces.
Versus rook.
A rook is generally worth about two pawns more than a bishop (see Chess piece relative value and the exchange). The bishop has access to only half of the squares on the board, whereas all squares of the board are accessible to the rook. On an empty board, a rook always attacks fourteen squares, whereas a bishop attacks no more than thirteen and sometimes as few as seven, depending on how near it is to the center. Also, a king and rook can force checkmate against a lone king, while a king and bishop cannot.
Versus knight.
In general bishops are approximately equal in strength to knights, but depending on the game situation either may have a distinct advantage.
Less experienced players tend to underrate the bishop compared to the knight because the knight can reach all squares and is more adept at forking. More experienced players understand the power of the bishop .
Bishops usually gain in relative strength towards the endgame as more pieces are captured and more open lines become available on which they can operate. A bishop can easily influence both wings simultaneously, whereas a knight is less capable of doing so. In an open endgame, a pair of bishops is decidedly superior to either a bishop and a knight, or two knights. A player possessing a pair of bishops has a strategic weapon in the form of a long-term threat to trade down to an advantageous endgame.
Two bishops vs King can force checkmate, whereas two knights cannot. A bishop and knight can force mate, but with far greater difficulty than two bishops. 
In certain positions a bishop can by itself lose a move (see triangulation and tempo), while a knight can never do so. The bishop is capable of skewering or pinning a piece, while the knight can do neither. A bishop can in some situations hinder a knight from moving. In these situations, the bishop is said to be "dominating" the knight.
On the other hand, in the opening and middle game a bishop may be hemmed in by pawns of both players, and thus be inferior to a knight which can hop over them. Furthermore, on a crowded board a knight has many tactical opportunities to fork two enemy pieces. A bishop can fork, but opportunities are more rare. One such example occurs in the position at right, which arises from the Ruy Lopez: 1.e4 e5 2.Nf3 Nc6 3.Bb5 a6 4.Ba4 Nf6 5.0-0 b5 6.Bb3 Be7?! 7.d4 d6 8.c3 Bg4 9.h3!? Bxf3 10.Qxf3 exd4 11.Qg3 g6 12.Bh6!
Game use.
Good bishop and bad bishop.
In the middle game, a player with only one bishop should generally place his pawns on squares of the color that the bishop cannot move to. This allows the player to control squares of both colors, allows the bishop to move freely among the pawns, and helps fix enemy pawns on squares on which they can be attacked by the bishop. Such a bishop is often referred to as a "good" bishop.
Conversely, a bishop which is impeded by friendly pawns is often referred to as a "bad bishop" (or sometimes, disparagingly, a "tall pawn"). The black light-squared bishop in the French Defense is a notorious example of this concept. However, a "bad" bishop need not always be a weakness, especially if it is outside its own pawn chains. In addition, having a "bad" bishop may be advantageous in an opposite-colored bishops endgame. Even if the bad bishop is passively placed, it may serve a useful defensive function; a well-known quip from GM Mihai Suba is that "Bad bishops protect good pawns."
In the position from the game Krasenkow versus Zvjaginsev, a thicket of black pawns hems in Black's bishop on c8, so Black is effectively playing with one piece fewer than White. Although the black pawns also obstruct the white bishop on e2, it has many more attacking possibilities, and thus is a good bishop vis-à-vis Black's bad bishop. Black resigned after another ten moves.
Fianchetto.
A bishop may be "fianchettoed", for example after moving the g2 pawn to g3 and the bishop on f1 to g2. This can form a strong defense for the castled king on g1 and the bishop can often exert strong pressure on the long diagonal (here h1-a8). A fianchettoed bishop should generally not be given up lightly, since the resulting holes in the pawn formation may prove to be serious weaknesses, particularly if the king has castled on that side of the board.
There are nonetheless some modern opening lines where a fianchettoed bishop is given up for a knight in order to double the opponent's pawns, for example 1.d4 g6 2.c4 Bg7 3.Nc3 c5 4.d5 Bxc3+!? 5.bxc3 f5, a sharp line originated by Roman Dzindzichashvili. Giving up a fianchettoed queen bishop for a knight is usually less problematic. For example, in Karpov–Browne, San Antonio 1972, after 1.c4 c5 2.b3 Nf6 3.Bb2 g6?!, Karpov gave up his fianchettoed bishop with 4.Bxf6! exf6 5.Nc3, doubling Black's pawns and giving him a hole on d5.
Endgame.
An endgame in which each player has only one bishop, one controlling the dark squares and the other the light, will often result in a draw even if one player has a pawn or sometimes two more than the other. The players tend to gain control of squares of opposite colors, and a deadlock results. In endgames with same-colored bishops, however, even a positional advantage may be enough to win .
Bishops on opposite colors.
Endgames in which each player has only one bishop (and no other pieces besides the king) and the bishops are on opposite colors are often drawn, even when one side has an extra pawn or two. Many of these positions would be a win if the bishops were on the same color.
The position from Wolf versus Leonhardt (see diagram), shows an important defensive setup. Black can make no progress, since the white bishop ties the black king to defending the pawn on "g4" and it also prevents the advance  ...f3+ because it would simply capture the pawn – then either the other pawn is exchanged for the bishop (an immediate draw) or the pawn advances (an easily drawn position). Otherwise the bishop alternates between the squares "d1" and "e2" .
If two pawns are connected, they normally win if they reach their sixth rank, otherwise the game may be a draw (as above). If two pawns are separated by one file they usually draw, but win if they are farther apart .
In some cases with more pawns on the board, it is actually advantageous to have the bishops on opposite colors if one side has weak pawns. In the 1925 game of Efim Bogoljubov versus Max Blümich, (see diagram) White wins because of the bishops being on opposite colors making Black weak on the black squares, the weakness of Black's isolated pawns on the queenside, and the weak doubled pawns on the kingside . The game continued
Wrong bishop.
In an endgame with a bishop, in some cases the bishop is the "wrong bishop", meaning that it is on the wrong color of square for some purpose (usually promoting a pawn). For example, with just a bishop and a rook pawn, if the bishop cannot control the promotion square of the pawn, it is said to be the "wrong bishop" or the pawn is said to be the wrong rook pawn. This results in some positions being drawn (by setting up a fortress) which otherwise would be won.
Unicode.
Unicode defines two codepoints for bishop:
♗ U+2657 White Chess Bishop (HTML &#9815;)
♝ U+265D Black Chess Bishop (HTML &#9821;)

</doc>
<doc id="47617" url="https://en.wikipedia.org/wiki?curid=47617" title="Rook">
Rook

Rook or rooks may refer to:

</doc>
<doc id="47618" url="https://en.wikipedia.org/wiki?curid=47618" title="Chess piece">
Chess piece

A chess piece, or chessman, is any of the 32 movable objects deployed on a chessboard used to play the game of chess. In a standard game of chess, each of the two players begins a game with the following 16 pieces:
In playing chess, the players take turns moving one of their own chess pieces. The rules of chess prescribe the types of move a player can make with each type of chess piece.
The pieces that belong to each player are distinguished by color. The lighter colored pieces, and the player that plays them, are referred to as white. The darker colored pieces and their player are referred to as black.
Terminology.
In chess, the word "piece" has three meanings, depending on the context.
The context should make the intended meaning clear
Moves of the pieces.
Each piece type moves in a different way.
Pieces other than pawns capture in the same way that they move. A capturing piece replaces the opponent piece on its square, except for an "en passant" capture. Captured pieces are immediately removed from the game. A square may hold only one piece at any given time. Except for castling and the knight's move, no piece may jump over another piece .
Chess sets.
Table sets.
The variety of designs available is broad, from small cosmetic changes to highly abstract representations, to themed designs such as those that emulate the drawings from the works of Lewis Carroll, or modern treatments such as Star Trek or "The Simpsons". Themed designs are generally intended for display purposes rather than actual play . Some works of art are designs of chess sets, such as the modernist chess set by chess enthusiast and dadaist Man Ray, that is on display in the Museum of Modern Art in New York City.
Chess pieces used for play are usually figurines that are taller than they are wide. For example, a set of pieces designed for a chessboard with squares typically have a king around tall. Chess sets are available in a variety of designs, with the most well-known Staunton design, named after Howard Staunton, a 19th-century English chess player, and designed by Nathaniel Cook. The first Staunton style sets were made in 1849 by Jaques of London (also known as "John Jaques of London" and "Jaques and Son of London") .
Wooden White chess pieces are normally made of a light wood, boxwood, or sometimes maple. Black wooden pieces are made of a dark wood such as rosewood, ebony, red sandalwood, or walnut. Sometimes they are made of boxwood and stained or painted black, brown, or red. Plastic white pieces are made of white or off-white plastic, and plastic black pieces are made of black or red plastic. Sometimes other materials are used, such as bone, ivory, or a composite material .
For actual play, pieces of the Staunton chess set design are standard. The height of the king should be between . United States Chess Federation rules call for a king height between 3.375 and 4.5 inches (86 to 114 mm). A height of about is preferred by most players. The diameter of the king should be 40–50% of its height. The size of the other pieces should be in proportion to the king. The pieces should be well balanced. The length of the sides of the squares of the chessboard should be about 1.25–1.3 times the diameter of the base of the king, or . Squares of about are normally well suited for pieces with the kings in the preferred size range. These criteria are from the United States Chess Federation's "Official Rules of Chess", which is based on the Fédération Internationale des Échecs rules .
The Grandmaster Larry Evans offered this advice on buying a set :Make sure the one you buy is easy on the eye, felt-based, and heavy (weighted). The men should be constructed so they don't come apart. ... The regulation board used by the U. S. Chess Federation is green and buff—never red and black. However, there are several good inlaid wood boards on the market. ... Avoid cheap equipment. Chess offers a lifetime of enjoyment for just a few dollars well spent at the outset.
Pocket and travel sets.
Some small magnetic sets, designed to be compact and/or for travel, have pieces more like those used in shogi and xiangqi – each piece being a similar flat token, with a symbol printed on it to identify the piece type.
Computer images.
On computers, chess pieces are often 2D symbols on a 2D board, although some programs have 3D graphics engines with more traditional designs of chess pieces.
Unicode contains symbols for chess pieces in both white and black.
Relative value.
The value assigned to a piece attempts to represent the potential strength of the piece in the game. As the game develops, the relative values of the pieces will also change. A bishop positioned to control long, open diagonal spaces is usually more valuable than a knight stuck in a corner. Similar ideas apply to placing rooks on open files and knights on active, central squares. The standard valuation is one point for a pawn, three points for a knight or bishop, five points for a rook, and nine points for a queen . These values are general throughout a game; in specific circumstances the values may be quite different—a knight can be more valuable than a queen in a particular decisive attack.

</doc>
<doc id="47619" url="https://en.wikipedia.org/wiki?curid=47619" title="Knight (chess)">
Knight (chess)

The knight (♘ ♞) is a piece in the game of chess, representing a knight (armored cavalry). It is normally represented by a horse's head and neck. Each player starts with two knights, which begin on the row closest to the player, one square from each corner. 
Movement.
The knight move is unusual among chess pieces. When it moves, it can move to a square that is two squares horizontally and one square vertically, or two squares vertically and one square horizontally. The complete move therefore looks like the letter "L". Unlike all other standard chess pieces, the knight can 'jump over' all other pieces (of either color) to its destination square. It captures an enemy piece by replacing it on its square. The knight's ability to "jump over" other pieces means it tends to be at its most powerful in closed positions, in contrast to that of a bishop. The move is one of the longest-surviving moves in chess, having remained unchanged since before the seventh century. Because of this it also appears in most chess-related regional games. The knight moves alternately to light and dark squares.
A knight should always be close to where the action is, meaning it is best used on areas of the board where the opponent's pieces are clustered or close together. Pieces are generally more powerful if placed near the center of the board, but this is particularly true for a knight. A knight on the edge of the board attacks only three or four squares (depending on its exact location) and a knight in the corner only two. Moreover, it takes more moves for an uncentralized knight to switch operation to the opposite side of the board than an uncentralized bishop, rook, or queen. The mnemonic phrases "A knight on the rim is grim" or "A knight on the rim is dim" are often used in chess instruction to reflect this principle.
The knight is the only piece that can move at the beginning of the game without first moving a pawn. For the reasons above, the best square for the initial move of each knight is usually one towards the center. Knights are usually brought into play slightly sooner than the bishops and much sooner than the rooks and the queen.
Because of its move pattern, the knight is especially well-suited for executing a fork.
In the numbered diagram, the numbers represent how many moves it takes for a knight to reach each square on the chessboard from its location on the f5-square. 
Value.
A knight is approximately equal in strength and value to a bishop. The bishop has longer range, but is restricted to only half the squares on the board. Since the knight can jump over pieces which obstruct other pieces, it is usually more valuable when the board is more crowded (closed positions, and early in the game). A knight is best when it has a 'support point' or outpost – a relatively sheltered square where it can be positioned to exert its strength remotely. On the fourth rank a knight is comparable in power to a bishop, and on the fifth it is often superior to the bishop, and on the sixth rank it can be a decisive advantage. This is assuming the knight is taking part in the action; a knight on the sixth rank which is not doing anything useful is not a well-placed piece.
Properties.
Enemy pawns are very effective at harassing knights because a pawn attacking a knight is not itself attacked by the knight. For this reason, a knight is most effective when placed in a weakness in the opponent's pawn structure, i.e. a square which cannot be attacked by enemy pawns. In the diagram at right, White's knight on d5 is very powerful – more powerful than Black's bishop on g7.
Whereas two bishops cover each other's weaknesses, two knights tend not to cooperate with each other as efficiently. As such, a pair of bishops is usually considered better than a pair of knights . World Champion José Raúl Capablanca considered that a queen and a knight is usually a better combination than a queen and a bishop. However, Glenn Flear found no game of Capablanca's that supported his statement and statistics do not support the statement either . In an endgame without other pieces or pawns, two knights generally have a better chance against a queen than two bishops or a bishop and a knight would (see fortress (chess)).
Compared to a bishop, a knight is often not as good in an endgame. The knight's potential range of movement is more limited, which often makes it less suitable in endgames with pawns on both sides of the board. However, this limitation is less important in endgames with pawns on only one side of the board. Knights are superior to bishops in an endgame if all the pawns are on one side of the board. Furthermore, knights have the advantage of being able to control squares of either color, unlike a lone bishop.
Nonetheless, a disadvantage of the knight (compared to the other pieces) is that by itself it cannot lose a move to put the opponent in zugzwang (see triangulation and tempo), while a bishop can. In this position, if the knight is on a white square and it is White's turn to move, White cannot win. Similarly, if the knight was on a black square and it was Black's turn to move, White cannot win. In the other two cases, White would win. If instead of the knight, White had a bishop on either color of square, White would win with either side to move .
At the end of the game, if one side has only a king and a knight while the other side has only a king, the game is a draw since a checkmate is impossible. When a bare king faces a king and two knights, checkmate can occur only if the opponent commits a blunder by moving his king to a square where it may be checkmated on the next move. Otherwise, a checkmate can never be forced. However checkmate can be forced with a bishop and knight, or with two bishops, even though the bishop and knight are in general about equal in value. Paradoxically, checkmate with two knights sometimes can be forced if the weaker side has a single extra pawn, but this is a curiosity of little practical value (see two knights endgame). Pawnless endings are a rarity, and if the stronger side has even a single pawn, an extra knight should give him an easy win. A bishop can trap (although it cannot then capture) a knight on the rim (diagram), especially in the endgame.
Notation.
In algebraic notation, the usual modern way of recording chess games, the letter "N" stands for the knight ("K" is reserved for the king); in descriptive chess notation, "Kt" is sometimes used instead, mainly in older literature. In chess problems and endgame studies, the letter "S", standing for the German name for the piece, "Springer", is often used (and in some variants of fairy chess "N" is used for the popular fairy chess piece, the nightrider).
Unicode.
Unicode defines two codepoints for knight:
♘ U+2658 White Chess Knight (HTML &#9816;)
♞ U+265E Black Chess Knight (HTML &#9822;)

</doc>
<doc id="47620" url="https://en.wikipedia.org/wiki?curid=47620" title="Henry Clay">
Henry Clay

Henry Clay, Sr. (April 12, 1777 – June 29, 1852) was an American lawyer and planter, politician, and skilled orator who represented Kentucky in both the United States Senate and House of Representatives. He served three non-consecutive terms as Speaker of the House of Representatives and served as Secretary of State under President John Quincy Adams from 1825 to 1829. Clay ran for the presidency in 1824, 1832 and 1844, while also seeking the Whig Party nomination in 1840 and 1848. However, he was unsuccessful.
Clay was a dominant figure in both the First and Second Party systems. As a leading war hawk in 1812, he favored war with Britain and played a significant role in leading the nation into the War of 1812. In 1824 he ran for president and lost, but maneuvered House voting in favor of John Quincy Adams, who appointed him as Secretary of State. Opposing candidate Andrew Jackson denounced the actions of Clay and Adams as part of a "corrupt bargain." Clay ran for president again, and lost the general election in 1832, as the candidate of the National Republican Party, and in 1844 as the candidate of the Whig Party. He was a strong proponent of the American System, fighting for an increase in tariffs to foster industry in the United States, the use of federal funding to build and maintain infrastructure, and a strong national bank. He opposed the annexation of Texas and "Manifest Destiny" policy of Democrats, fearing it would inject the slavery issue into politics. This cost him votes in the close presidential election of 1844. Clay later opposed the Mexican-American War.
Known as "The Great Compromiser", Clay brokered important agreements during the Nullification Crisis and on the slavery issue. As part of the "Great Triumvirate" or "Immortal Trio," along with his colleagues Daniel Webster and John C. Calhoun, he was instrumental in formulating the Missouri Compromise of 1820, the Compromise Tariff of 1833, and the Compromise of 1850 to ease sectional tensions. He was viewed as the primary representative of Western interests in this group, and was given the names "Henry of the West" and "The Western Star." As a plantation owner, Clay held slaves during his lifetime, but freed them in his will.
Abraham Lincoln, the Whig leader in Illinois, was a great admirer of Clay, saying he was "my ideal of a great man." Lincoln wholeheartedly supported Clay's economic programs. In 1957, a Senate Committee selected Clay as one of the five greatest U.S. Senators, along with Daniel Webster, John C. Calhoun, Robert La Follette, and Robert A. Taft.
Early life and education.
Childhood.
Henry Clay was born on April 12, 1777, at the Clay homestead in Hanover County, Virginia, in a story-and-a-half frame house. It was an above-average home for a "common" Virginia planter of that time. At the time of his death, Clay's father owned more than 22 slaves, making him part of the planter class in Virginia (those men who owned 20 or more slaves).
Henry was the seventh of nine children of the Reverend John Clay and Elizabeth (née Hudson) Clay. His father, a Baptist minister nicknamed "Sir John," died four years after the boy's birth (1781). The father left Henry and his brothers two slaves each, and his wife 18 slaves and of land. Henry Clay was a second cousin of Cassius Marcellus Clay, who became a politician and an abolitionist in Kentucky.
The widow Elizabeth Clay married Capt. Henry Watkins, who was an affectionate stepfather. Henry Watkins moved the family to Richmond, Virginia. Elizabeth had seven more children with Watkins, bearing a total of sixteen.
Education.
His stepfather secured Clay employment in the office of the Virginia Court of Chancery, where the youth displayed an aptitude for law. There he became friends with George Wythe. Hampered by a crippled hand, Wythe chose Clay as his secretary. After Clay was employed as Wythe's amanuensis for four years, the chancellor took an active interest in Clay's future; he arranged a position for him with the Virginia attorney general, Robert Brooke. Clay read law by working and studying with Wythe, Chancellor of the Commonwealth of Virginia (also a mentor to Thomas Jefferson and John Marshall, among others), and Brooke. Clay was admitted to the bar to practice law in 1797.
Marriage and family.
After beginning his law career, on April 11, 1799, Clay married Lucretia Hart at the Hart home in Lexington, Kentucky. One of her brothers was Nathaniel G. S. Hart, who later served as a captain in the War of 1812 and was killed in the Massacre of the River Raisin.
Clay and his wife had eleven children (six daughters and five sons): Henrietta (1800–1801), Theodore (1802–1870), Thomas (1803–1871), Susan (1805–1825), Anne (1807–1835), Lucretia (1809–1823), Henry, Jr. (1811–1847), Eliza (1813–1825), Laura (1815–1817), James Brown Clay (1817–1864), and John Morrison Clay (1821–1887).
Seven of Clay's children died before him. By 1835 all six daughters had died of varying causes, two when very young, two as children, and the last two as young women: from whooping cough, yellow fever, and complications of childbirth, respectively. Henry Clay, Jr. was killed during the Mexican–American War at the Battle of Buena Vista.
Lucretia Hart Clay survived her husband, dying in 1864 at the age of 83. She is interred with her husband in the vault of his monument at the Lexington Cemetery. Henry and Lucretia Clay were great-grandparents of the suffragette Madeline McDowell Breckinridge.
Early law and political career.
Legal career.
In November 1797, Clay relocated to Lexington, Kentucky, the growing town near where his parents and siblings then resided in Woodford County. He soon established a reputation for his legal skills and courtroom oratory. Some of his clients paid him with horses and others with land. Clay came to own town lots and the Kentucky Hotel.
By 1812, Clay owned a productive plantation, which he called "Ashland," and numerous slaves to work the land. He held 60 slaves at the peak of operations, and likely produced tobacco and hemp, the two chief commodity crops of the Bluegrass Region.
One of Clay's clients was his father-in-law, Colonel Thomas Hart, an early settler of Kentucky and a prominent businessman. Clay's most notable client was Aaron Burr in 1806, after the US District Attorney Joseph Hamilton Daveiss indicted him for planning an expedition into Spanish Territory west of the Mississippi River. Clay and his law partner John Allen successfully defended Burr. Some years later Thomas Jefferson convinced Clay that Daveiss had been right in his charges. Clay was so upset that many years later, when he met Burr again, Clay refused to shake his hand.
State legislator.
In 1803, although not old enough to be elected, Clay was appointed a representative of Fayette County in the Kentucky General Assembly. As a legislator, Clay advocated a liberal interpretation of the state's constitution and the gradual emancipation of slavery in Kentucky. The political realities of the time forced him to abandon that position, as slaveholders formed the elite in Kentucky. Clay also advocated moving the state capitol from Frankfort to Lexington. He defended the Kentucky Insurance Company, which he saved from an attempt in 1804 by Felix Grundy to repeal its monopolistic charter.
First Senate appointment and eligibility.
Clay's influence in Kentucky state politics was such that in 1806 the Kentucky legislature elected him to the Senate seat of John Breckinridge, who had resigned when appointed as US Attorney General. The legislature first chose John Adair to complete Breckinridge's term, but he had to resign over his alleged role in the Burr Conspiracy. On December 29, 1806, Clay was sworn in as senator, serving for slightly more than two months that first time.
When elected by the legislature, Clay was below the constitutionally required age of thirty. His age did not appear to have been noticed by any other Senator, and perhaps not by Clay. His term ended before his thirtieth birthday. Such an age qualification issue has occurred with only two other U.S. Senators, Armistead Thomson Mason (aged 28 in 1816), and John Eaton (aged 28 in 1818). Such an occurrence, however, has not been repeated since. In 1934, Rush D. Holt, Sr. was elected to the Senate at the age of 29; he waited until he turned 30 (on the following June 19) to take the oath of office. In November 1972, Joe Biden was elected to the Senate at the age of 29, but he reached his 30th birthday before the swearing-in ceremony for incoming senators in January 1973.
Speaker of the State House and duel with Humphrey Marshall.
When Clay returned to Kentucky in 1807, he was elected as the Speaker of the state House of Representatives. On January 3, 1809, Clay introduced a resolution to require members to wear homespun suits rather than those made of imported British broadcloth. Two members voted against the measure. One was Humphrey Marshall, an "aristocratic lawyer who possessed a sarcastic tongue," who had been hostile toward Clay in 1806 during the trial of Aaron Burr.
On January 4, 1809 Clay and Marshall nearly came to blows on the Assembly floor, and Clay challenged Marshall to a duel. It was still used as a method of settling disputes of honor. It took place on January 19. Apparently to keep any blood from being spilled in their home state of Kentucky, they had chosen a dueling ground in Indiana, directly across the Ohio River from what was then Shippingport, Kentucky and near the mouth of Silver Creek.
They each had three turns to shoot. Clay grazed Marshall once, just below the chest. Marshall hit Clay once in the thigh. Both survived.
Second Senate appointment.
In 1810, United States Senator Buckner Thruston resigned to serve as a judge on the United States Circuit Court, and Clay was again selected by the legislature to fill his seat.
Speaker of the House.
Early years.
In the summer of 1811, Clay was elected to the United States House of Representatives. He was chosen Speaker of the House on the first day of his first session, something never done before or since (except for the first ever session of Congress back in 1789). During the fourteen years following his first election, Clay was re-elected five times to the House and to the speakership. Like other Southern Congressmen, Clay took domestic slaves to Washington, DC to support his household. They included Aaron and Charlotte Dupuy, their son Charles, and daughter Mary Ann.
Changes as Speaker.
Before Clay's election as Speaker of the House, the position had been that of a rule enforcer and mediator. Clay made the position one of political power second only to the President of the United States. He immediately appointed members of the War Hawk faction (of which he was the "guiding spirit") to all the important committees, effectively giving him control of the House. This was a singular achievement for a 34-year-old House freshman. During his early House service, Clay strongly opposed the creation of a National Bank, in part because of his personal ownership in several small banks in his hometown of Lexington. Later he changed his position. When he was seeking the presidency, Clay gave strong support for the Second Bank of the United States.
War Hawks.
The War Hawks, mostly from the South and the West, strongly supported war against Great Britain due to British violations of United States maritime rights and impressment of U.S. sailors, while also fearing British designs on U.S. territory in the Old Northwest and wishing to expand. As the Congressional leader of the Democratic-Republican Party, Clay took charge of the agenda, especially as a "War Hawk." Later, as one of the peace commissioners, Clay helped negotiate the Treaty of Ghent and signed it on December 24, 1814. In 1815, while still in Europe, he helped negotiate a commerce treaty with Great Britain.
Work in Liberia.
Henry Clay helped establish and became president in 1816 of the American Colonization Society, a group that wanted to establish a colony for free American blacks in Africa; it founded Monrovia, in what became Liberia, for that purpose. The group was made up of both abolitionists from the North, who wanted to end slavery, and slaveholders, who wanted to deport free blacks to reduce what they considered a threat to the stability of slave society. On the "amalgamation" of the black and white races, Clay said that "The God of Nature, by the differences of color and physical constitution, has decreed against it." Clay presided at the founding meeting of the ACS on December 21, 1816, at the Davis Hotel in Washington, D.C. Attendees included Robert Finley, James Monroe, Bushrod Washington, Andrew Jackson, Francis Scott Key, and Daniel Webster.
The "American System".
Henry Clay and John C. Calhoun helped to pass the Tariff of 1816 as part of the national economic plan Clay called "The American System," which is rooted in Alexander Hamilton's American School. Described later by Friedrich List, it was designed to allow the fledgling American manufacturing sector, largely centered on the eastern seaboard, to compete with British manufacturing through the creation of tariffs.
After the conclusion of the War of 1812, British factories were overwhelming American ports with inexpensive goods. To persuade voters in the western states to support the tariff, Clay advocated federal government support for internal improvements to infrastructure, principally roads and canals. These internal improvements would be financed by the tariff and by sale of the public lands, prices for which would be kept high to generate revenue. Finally, a national bank would stabilize the currency and serve as the nexus of a truly national financial system.
Foreign policy.
In foreign policy, Clay was the leading American supporter of independence movements and revolutions in Latin America after 1817. Between 1821 and 1826, the U.S. recognized all the new countries, except Uruguay (whose independence was debated and recognized only later). When in 1826 the U.S. was invited to attend the Columbia Conference of new nations, opposition emerged, and the American delegation never arrived. Clay supported the Greek independence revolutionaries in 1824 who wished to separate from the Ottoman Empire, an early move into European affairs.
Clay also strongly desired the office of Secretary of State in the administration of James Monroe, who was elected president in 1816. When Monroe, largely in an effort to placate New England Federalists, gave the office to John Quincy Adams instead, Clay became so bitter that he declined Monroe's offer for him to become Secretary of War, refused to allow Monroe's inauguration to take place in the House Chamber, and subsequently did not attend Monroe's outdoor inauguration.
The Missouri Compromise and 1820s.
In 1820 a dispute erupted over the extension of slavery in Missouri Territory. Clay helped settle this dispute by gaining Congressional approval for a plan called the "Missouri Compromise". It brought in Maine as a free state and Missouri as a slave state (thus maintaining the balance in the Senate, which had included 11 free and 11 slave states), and it forbade slavery north of 36° 30' (the northern boundary of Arkansas and the latitude line) except in Missouri.
Presidential Election of 1824 and Secretary of State.
By 1824, the unparalleled success of the Democratic-Republican Party had driven all other parties from the field. Four major candidates-Adams, Andrew Jackson, William H. Crawford, and Clay-sought the office of president. Because of the unusually large number of candidates receiving electoral votes, no candidate secured a majority of votes in the electoral college. According to the terms of the Twelfth Amendment to the United States Constitution, the top three electoral vote-getters advanced to the runoff in the House of Representatives. Having finished fourth, Clay was eliminated from contention; the top three were Jackson, Adams, and Crawford. Clay, who was Speaker of the House, supported Adams, and his endorsement ultimately secured Adams' win in the House.
Clay used his political clout to secure the victory for Adams, who he felt would be both more sympathetic to Clay's political views and more likely to appoint Clay to a cabinet position. When Clay was appointed Secretary of State, his maneuver was called a "corrupt bargain" by many of Jackson's supporters and tarnished Clay's reputation. It also marked the beginning of an intense personal rivalry between Clay and Jackson.
Slave freedom suit.
As Secretary of State, Clay lived with his family and slaves in Decatur House on Lafayette Square. As he was preparing to return to Lexington in 1829, his slave Charlotte Dupuy sued Clay for her freedom and that of her two children, based on a promise by an earlier owner. Her legal challenge to slavery preceded the more famous Dred Scott case by 27 years. The "freedom suit" received a fair amount of attention in the press at the time. Dupuy's attorney gained an order from the court for her to remain in DC until the case was settled, and she worked for wages for 18 months for Martin Van Buren, the successor to Secretary of State and the Decatur House. Clay returned to Ashland with Aaron, Charles and Mary Ann Dupuy.
The jury ruled against Dupuy, deciding that any agreement with her previous master Condon did not bear on Clay. Because Dupuy refused to return voluntarily to Kentucky, Clay had his agent arrest her. She was imprisoned in Alexandria, Virginia, before Clay arranged for her transport to New Orleans, where he placed her with his daughter and son-in-law Martin Duralde. Mary Ann Dupuy was sent to join her mother, and they worked as domestic slaves for the Duraldes for another decade.
In 1840 Henry Clay finally gave Charlotte and her daughter Mary Ann Dupuy their freedom. He kept her son Charles Dupuy as a personal servant, frequently citing him as an example of how well he treated his slaves. Clay granted Charles Dupuy his freedom in 1844. While no deed of emancipation has been found for Aaron Dupuy, in 1860 he and Charlotte were living together as free black residents in Fayette County, Kentucky. He may have been freed or "given his time" by one of Clay's sons, as Dupuy continued to work at Ashland, for pay.
Today, Decatur House, in Washington, DC, is a National Historic Landmark and museum on Lafayette Square near the White House and has exhibits on urban slavery and Charlotte Dupuy's freedom suit against Henry Clay.
Senate career.
The Nullification Crisis.
After the passage of the Tariff of 1828, dubbed the "tariff of abominations" which raised tariffs considerably in an attempt to protect fledgling factories built under previous tariff legislation, South Carolina declared its right to nullify federal tariff legislation and stopped assessing the tariff on imports. It threatened to secede from the Union if the Federal government tried to enforce the tariff laws. Furious, President Jackson threatened to lead an army to South Carolina and hang any man who refused to obey the law.
The crisis worsened until 1833. Clay was by that time a U.S. Senator again, having been re-elected by Kentucky in 1831. His return to the U.S. Senate, after 20 years, 8 months, 7 days out of office, marks the fourth longest gap in service to the chamber in history.
In 1833, Clay helped to broker a deal in Congress to lower the tariff gradually, known as the Compromise Tariff of 1833. This measure helped to preserve the supremacy of the Federal government over the states, but the crisis was indicative of the developing conflict between the northern and southern United States over economics and slavery.
Opposition to Jackson and creation of Whig Party.
After the election of Andrew Jackson, Clay led the opposition to Jackson's policies. His supporters included the National Republicans, who were beginning to identify as "Whigs" in honor of ancestors during the Revolutionary War. They opposed the "tyranny" of Jackson, as their ancestors had opposed the tyranny of King George III. Clay strongly opposed Jackson's refusal to renew the charter of the Second Bank of the United States, and advocated passage of a resolution to censure Jackson for his actions.
Clay's American System ran into strong opposition from President Jackson's administration. A key point of contention between the two men was over the Maysville Road. It would authorize federal funding for a project to construct a road linking Lexington and the Ohio River; however all of the road would be inside Kentucky. Jackson vetoed it because he felt that it did not constitute interstate commerce, and he feared it would fund corruption. Jackson opposed using the federal government to promote economic modernization, thereby appealing to his agrarian base that wanted rural expansion but distrusted cities.
Presidential campaigns.
In 1832 the National Republicans unanimously nominated Clay for the presidency, while the Democrats nominated the sitting President Jackson. The main issue was the policy of continuing the Second Bank of the United States. Clay lost by a wide margin to the highly popular Jackson (55% to 37%).
In 1840, Clay was a candidate for the Whig nomination, but he was defeated at the party convention by supporters of war hero William Henry Harrison. Harrison was chosen because his war record was attractive, and he was seen as more likely to win than Clay.
In 1844, Clay was nominated by the Whigs against James K. Polk, the Democratic candidate. Polk won by 170 to 105 electoral votes, carrying 15 of the 26 states. Polk's populist stances on territorial expansion figured prominently—particularly his opinion on US control over the entire Oregon Country and his support for the annexation of Texas. Clay opposed annexing Texas on the grounds that it would once again bring the issue of slavery to the forefront of the nation's political dialog and would draw the ire of Mexico, from which Texas had declared its independence in 1836. Despite Polk's populism, the election was close; New York's 36 electoral votes proved the difference, and went to Polk by a slim 5,000 vote margin. Liberty Party candidate James G. Birney won slightly more than 15,000 votes in New York and likely attracted votes that might have gone to Clay. His warnings about Texas proved prescient. The US annexation of Texas led to the Mexican–American War (1846–1848) (in which his namesake son died). The North and South came to increased tensions during Polk's Presidency over the extension of slavery into Texas and beyond.
Return to the Senate.
After losing the Whig Party presidential nomination to Zachary Taylor in 1848, Clay decided to retire to his Ashland estate in Kentucky. Retired for less than a year, he was in 1849 again elected to the U.S. Senate from Kentucky. During his term, the controversy over the expansion of slavery in new lands had reemerged with the addition of the lands ceded to the United States by Mexico in the Treaty of Guadalupe Hidalgo at the conclusion of the Mexican–American War. In 1846, David Wilmot, a Pennsylvania congressman, had proposed preventing the extension of slavery into any of the new territory in a failed proposal referred to as the "Wilmot Proviso."
The Compromise of 1850.
Clay played a central role in designing a compromise in 1850 between North and South to resolve the increasingly dangerous slavery question. He acquired the help of Illinois Senator Stephen Arnold Douglas, a Democrat, to help guide the measures through Congress and thus prevent civil war.
On January 29, 1850, Clay proposed a series of resolutions, which he considered to reconcile Northern and Southern interests, what would widely be called the Compromise of 1850. Clay originally intended the resolutions to be voted on separately, but at the urging of southerners he agreed to the creation of a Committee of Thirteen to consider the measures. The committee was formed on April 17. On May 8, as chair of the committee, Clay presented an omnibus bill linking all of the resolutions. The resolutions included:
The bill was opposed for different reasons by hardliners on both sides. Senator John C. Calhoun, Clay's former ally, composed a speech arguing against the compromise and warning of the possibility of disunion. Calhoun was too ill to read the speech, and Virginia Senator James Murray Mason did so for him on March 4. Many of the more anti-slavery northerners, such as New York Senator William H. Seward, opposed the compromise as well. However, on March 7, Daniel Webster gave a speech in which he argued in favor of the compromise.
The omnibus bill, despite Clay's efforts, failed in a crucial vote on July 31 with the majority of his Whig Party and many Southern Democrats opposed. He announced on the Senate floor the next day that he intended to persevere and pass each individual part of the bill. Clay was physically exhausted; the tuberculosis that would eventually kill him began to take its toll. Clay left the Senate to recuperate in Newport, Rhode Island. Douglas separated the bills and guided them through the Senate. President Millard Fillmore, a Whig who took office following the death of Taylor, who had not supported the compromise, signed the bills into law.
Clay was given much of the credit for the Compromise's success. It quieted the controversy between Northerners and Southerners over the expansion of slavery, and delayed secession and civil war for another decade. Senator Henry S. Foote of Mississippi, who had suggested the creation of the Committee of Thirteen, later said, "Had there been one such man in the Congress of the United States as Henry Clay in 1860–'61 there would, I feel sure, have been no civil war."
Death and estate.
Clay continued to serve both the Union and his home state of Kentucky. On June 29, 1852, he died of tuberculosis in Washington, D.C., at the age of 75.
He was buried in Lexington Cemetery, and Theodore Frelinghuysen, Clay's vice-presidential candidate in the election of 1844, gave the eulogy. Clay's headstone reads: "I know no North—no South—no East—no West." Even though the 1852 pro-slavery novel "Life at the South; or, "Uncle Tom's Cabin" As It Is", by W.L.G. Smith, is dedicated to his memory, Clay's Will freed all the slaves he held.
Ashland, named for the many ash trees on the property, was Clay's plantation and mansion for many years. He held as many as 60 slaves at the peak of the plantation operations. It was there he introduced the Hereford livestock breed to the United States.
By the time of his death, his only surviving children were sons Theodore, Thomas, James Brown Clay and John Morrison Clay, who inherited the estate and took portions for use. For several years (1866–1878), James Clay allowed the mansion to be used as a residence for the regent of Kentucky University, forerunner of the University of Kentucky and present-day Transylvania University. The mansion and estate were later rebuilt and remodeled by Clay's descendants. John Clay designated his portion of the estate as Ashland Stud, which he devoted to breeding thoroughbred horses.
Maintained and operated as a museum, today Ashland includes of the original estate grounds. It is located on Richmond Road (US 25) in Lexington. It is open to the public (admission charged).
Henry Clay is credited with introducing the mint julep drink to Washington, D.C., at the Willard Hotel during his residence as a senator in the city.
Memberships and other honors.
Elected a member of the American Antiquarian Society in 1820.

</doc>
<doc id="47621" url="https://en.wikipedia.org/wiki?curid=47621" title="Isabella of France">
Isabella of France

Isabella of France (1295 – 22 August 1358), sometimes described as the She-Wolf of France, was Queen of England as the wife of Edward II. She was the youngest surviving child and only surviving daughter of Philip IV of France and Joan I of Navarre. Queen Isabella was notable at the time for her beauty, diplomatic skills, and intelligence.
Isabella arrived in England at the age of 12 during a period of growing conflict between the king and the powerful baronial factions. Her new husband was notorious for the patronage he lavished on his favourite, Piers Gaveston, but the queen supported Edward during these early years, forming a working relationship with Piers and using her relationship with the French monarchy to bolster her own authority and power. After the death of Gaveston at the hands of the barons in 1312, however, Edward later turned to a new favourite, Hugh Despenser the younger, and attempted to take revenge on the barons, resulting in the Despenser War and a period of internal repression across England. Isabella could not tolerate Hugh Despenser and by 1325 her marriage to Edward was at a breaking point.
Travelling to France under the guise of a diplomatic mission, Isabella began an affair with Roger Mortimer, and the two agreed to depose Edward and oust the Despenser family. The Queen returned to England with a small mercenary army in 1326, moving rapidly across England. The King's forces deserted him. Isabella deposed Edward, becoming regent on behalf of her son, Edward III. Many have believed that Isabella then arranged the murder of Edward II. Isabella and Mortimer’s regime began to crumble, partly because of her lavish spending, but also because the Queen successfully, but unpopularly, resolved long-running problems such as the wars with Scotland.
In 1330, Isabella’s son Edward III deposed Mortimer in turn, taking back his authority and executing Isabella’s lover. The Queen was not punished, however, and lived for many years in considerable style—although not at Edward III’s court—until her death in 1358. Isabella became a popular "femme fatale" figure in plays and literature over the years, usually portrayed as a beautiful but cruel, manipulative figure.
Early life and marriage: 1295–1308.
Isabella was born in Paris on an uncertain date – on the basis of the chroniclers and the eventual date of her marriage, she was probably born between May and November 1295. She is described as born in 1292 in the Annals of Wigmore, and Piers Langtoft agrees, claiming that she was 7 years old in 1299. The French chroniclers Guillaume de Nangis and Thomas Walsingham describe her as 12 years old at the time of her marriage in January 1308, placing her birth between the January of 1295 and of 1296. A Papal dispensation by Clement V in November 1305 permitted her immediate marriage by proxy, despite the fact that she was probably only 10 years old. Since she had to reach the canonical age of 7 before her betrothal in May 1303, and that of 12 before her marriage in January 1308, the evidence suggests that she was born between May and November 1295. Her parents were King Philip IV of France and Queen Joan I of Navarre; her brothers Louis, Philip and Charles became kings of France.
Isabella was born into a royal family that ruled the most powerful state in Western Europe. Her father, King Philip, known as ""le Bel"" (the Fair) because of his good looks, was a strangely unemotional man; contemporaries described him as "neither a man nor a beast, but a statue"; modern historians have noted that he "cultivated a reputation for Christian kingship and showed few weaknesses of the flesh". Philip built up centralised royal power in France, engaging in a sequence of conflicts to expand or consolidate French authority across the region, but remained chronically short of money throughout his reign. Indeed, he appeared almost obsessed about building up wealth and lands, something that his daughter was also accused of in later life. Isabella's mother died when Isabella was still quite young; some contemporaries suspected Philip IV of her murder, albeit probably incorrectly.
Isabella was brought up in and around the Château du Louvre and the "Palais de la Cité" in Paris. Isabella was cared for by Théophania de Saint-Pierre, her nurse, given a good education and taught to read, developing a love of books. As was customary for the period, all of Philip's children were married young for political benefit. Isabella was promised in marriage by her father to King Edward II of England whilst she was still an infant, with the intention to resolve the conflicts between France and England over the latter's continental possession of Gascony and claims to Anjou, Normandy and Aquitaine. Pope Boniface VIII had urged the marriage as early as 1298 but was delayed by wrangling over the terms of the marriage contract. The English king, Edward I attempted to break the engagement several times for political advantage, and only after he died in 1307 did the wedding proceed.
Isabella and Edward were finally married at Boulogne-sur-Mer on 25 January 1308. Isabella's wardrobe gives some indications of her wealth and style – she had gowns of baudekyn, velvet, taffeta and cloth, along with numerous furs; she had over 72 headdresses and coifs; she brought with her two gold crowns, gold and silver dinnerware and 419 yards of linen. At the time of her marriage, Isabella was probably about twelve and was described by Geoffrey of Paris as ""the beauty of beauties... in the kingdom if not in all Europe."" This description was probably not simply flattery by a chronicler, since both Isabella's father and brothers were considered very handsome men by contemporaries, and her husband was to nickname her "Isabella the Fair". Isabella was said to resemble her father, and not her mother, queen regnant of Navarre, a plump, plain woman. This indicates that Isabella was slender and pale-skinned, although the fashion at the time was for blonde, slightly full-faced women, and Isabella may well have followed this stereotype instead. Throughout her career, Isabella was noted as charming and diplomatic, with a particular skill at convincing people to follow her courses of action. Unusual for the medieval period, contemporaries also commented on her high intelligence.
Queenship.
As queen, the young Isabella faced numerous challenges. Edward was handsome, but highly unconventional, possibly forming close romantic attachments to first Piers Gaveston and then Hugh Despenser the younger. Edward found himself at odds with the barons, too, in particular the Lancastrians under Thomas of Lancaster, whilst continuing the war against the Scots that he had inherited from Edward I. Using her own supporters at court, and the patronage of her French family, Isabella attempted to find a political path through these challenges; she successfully formed an alliance with Gaveston, but after his death at the hands of the barons her position grew increasingly precarious. Edward began to take revenge on his enemies, using an ever more brutal alliance with the Despenser family, in particular his new favourite, Hugh Despenser the younger. By 1326 Isabella found herself at increasing odds with both Edward and Hugh, ultimately resulting in Isabella's own bid for power and the invasion of England.
Fall of Gaveston: 1308–12.
Isabella's new husband Edward was an unusual character by medieval standards. Edward looked the part of a Plantagenet king to perfection. He was tall, athletic, and wildly popular at the beginning of his reign. He rejected most of the traditional pursuits of a king for the period – jousting, hunting and warfare – and instead enjoyed music, poetry and many rural crafts. Furthermore, there is the question of Edward's sexuality. He can be considered to have been bisexual, in a period when homosexuality of any sort was considered a very serious crime, but there is no complicit evidence which comments directly on his sexual orientation. Contemporary chroniclers made much of his close affinity with a succession of male favourites; some condemned Edward for loving them "beyond measure" and "uniquely", others explicitly referring to an "illicit and sinful union". Nonetheless, Isabella bore four children by Edward, leading to an opinion amongst some historians that Edward's affairs with his male favourites may have been platonic.
When Isabella first arrived in England following her marriage, her husband was already in the midst of a relationship with Piers Gaveston, an "arrogant, ostentatious" soldier, with a "reckless and headstrong" personality that clearly appealed to Edward. Isabella, then aged twelve, was effectively sidelined by the pair. Edward chose to sit with Gaveston rather than Isabella at their wedding celebration, causing grave offence to her uncles Louis, Count of Évreux, and Charles, Count of Valois, and then refused to grant her either her own lands or her own household. Edward also gave Isabella's own jewelry to Gaveston, which he wore publicly. It took the intervention of Isabella's father, Philip IV, before Edward began to provide for her more appropriately.
Isabella's relationship with Gaveston was a complex one. Baronial opposition to Gaveston, championed by Thomas of Lancaster, was increasing, and Philip IV began to covertly fund this grouping, using Isabella and her household as intermediaries. Edward was forced to exile Gaveston to Ireland for a period, and began to show Isabella much greater respect and assigning her significant lands and patronage; in turn, Philip ceased his support for the barons. Gaveston eventually returned from Ireland, and by 1309–11 the three seemed to be co-existing together relatively comfortably. Indeed, Gaveston's key enemy, Thomas of Lancaster, considered Isabella to be an ally of Gaveston's. Isabella had begun to build up her own supporters at court, principally the de Beaumont family, itself opposed to the Lancastrians; originating, like her, from France, the senior member of the family, Isabella de Vesci, had been a close confidant of Queen Eleanor; supported by her brother, Henry de Beaumont, Isabella de Vesci became a close friend of Isabella in turn.
During 1311, however, Edward conducted a failed campaign against the Scots, during which Isabella and he only just escaped capture. In the aftermath, the barons rose up, signing the Ordinances of 1311, which promised action against Gaveston and expelled Isabella de Vesci and Henry de Beaumont from court. 1312 saw a descent into civil war against the king and his lover – Isabella stood with Edward, sending angry letters to her uncles d'Évreux and de Valois asking for support. Edward left Isabella, rather against her will, at Tynemouth Priory in Northumberland whilst he unsuccessfully attempted to fight the barons. The campaign was a disaster, and although Edward escaped, Gaveston found himself stranded at Scarborough Castle, where his baronial enemies first surrounded and captured him. Guy de Beauchamp and Thomas of Lancaster ensured Gaveston's execution as he was being taken south to rejoin Edward. Isabella's rival for Edward's affections was gone, but the situation in England was deeply unstable.
Tensions grow: 1312–21.
Tensions mounted steadily over the decade. In 1312, Isabella gave birth to the future Edward III, but by the end of the year Edward's court was beginning to change. Edward was still relying upon his French relatives – Isabella's uncle, Louis d'Évreux, for example had been sent from Paris to assist him – but Hugh Despenser the elder now formed part of the inner circle, marking the beginning of the Despensers' increased prominence at Edward's court. The Despensers were opposed to both the Lancastrians and their other allies in the Welsh Marches, making an easy alliance with Edward, who sought revenge for the death of Gaveston.
In 1313, Isabella travelled to Paris with Edward to garner further French support, which resulted in the Tour de Nesle Affair. The journey was a pleasant one, with lots of festivities, although Isabella was injured when her tent burned down. During the visit Louis and Charles had had a satirical puppet show put on for their guests, and after this Isabella had given new embroidered purses both to her brothers and to their wives. Isabella and Edward then returned to England with new assurances of French support against the English barons. Later in the year, however, Isabella and Edward held a large dinner in London to celebrate their return and Isabella apparently noticed that the purses she had given to her sisters-in-law were now being carried by two Norman knights, Gautier and Philippe d'Aunay. Isabella concluded that the pair must have been carrying on an illicit affair, and appears to have informed her father of this during her next visit to France in 1314. The consequence of this was the Tour de Nesle Affair in Paris, which led to legal action against all three of Isabella's sisters-in-law; Blanche and Margaret of Burgundy were imprisoned for life for adultery. Joan of Burgundy was imprisoned for a year. Isabella's reputation in France suffered somewhat as a result of her perceived role in the affair.
In the north, however, the situation was turning worse. Edward attempted to quash the Scots in a fresh campaign in 1314, resulting in the disastrous defeat at the battle of Bannockburn. Edward was blamed by the barons for the catastrophic failure of the campaign. Thomas of Lancaster reacted to the defeats in Scotland by taking increased power in England and turning against Isabella, cutting off funds and harassing her household. To make matters worse, the "Great Famine" descended on England during 1316–7, causing widespread loss of life and financial problems.
Despite Isabella giving birth to her second son, John, in 1316, Edward's position was precarious. Indeed, John Deydras, a royal Pretender, appeared in Oxford, claiming to have been switched with Edward at birth, and to be the real king of England himself. Given Edward's unpopularity, the rumours spread considerably before Deydras' eventual execution, and appear to have greatly upset Isabella. Isabella responded by deepening her alliance with Lancaster's enemy Henry de Beaumont and by taking up an increased role in government herself, including attending council meetings and acquiring increased lands. Henry's sister, Isabella de Vesci, continued to remain a close adviser to the Queen. The Scottish general Sir James Douglas, war leader for Robert I of Scotland, made a bid to capture Isabella personally in 1319, almost capturing her at York – Isabella only just escaped. Suspicions fell on Lancaster, and one of Edward's knights, Edmund Darel, was arrested on charges of having betrayed her location, but the charges were essentially unproven. In 1320, Isabella accompanied Edward to France, to try and convince her brother, Charles IV, to provide fresh support to crush the English barons.
Meanwhile, Hugh de Despenser the younger became an increasing favourite of Isabella's husband, and was widely believed to have begun a sexual relationship with him around this time. Hugh was the same age as Edward. His father, Hugh the elder, had supported Edward and Gaveston a few years previously. The Despensers were bitter enemies of Lancaster, and with Edward's support began to increase their power base in the Welsh Marches, in the process making enemies of Roger Mortimer de Chirk and his nephew, Roger Mortimer of Wigmore, their rival Marcher lords. Whilst Isabella had been able to work with Gaveston, Edward's previous favourite, it became increasingly clear that Hugh the younger and Isabella could not work out a similar compromise. Unfortunately for Isabella, she was still estranged from Lancaster's rival faction, giving her little room to manoeuvre. In 1321, Lancaster's alliance moved against the Despensers, sending troops into London and demanding their exile. Aymer de Valence, 2nd Earl of Pembroke, a moderate baron with strong French links, asked Isabella to intervene in an attempt to prevent war; Isabella publicly went down on her knees to appeal to Edward to exile the Despensers, providing him with a face-saving excuse to do so, but Edward intended to arrange their return at the first opportunity.
Return of the Despensers, 1321-6.
Despite the momentary respite delivered by Isabella, by the autumn of 1321, the tensions between the two factions of Edward, Isabella and the Despenser, opposing the baronial opposition led by Thomas of Lancaster, were extremely high, with forces still mobilised across the country. At this point, Isabella undertook a pilgrimage to Canterbury, during which she left the traditional route to stop at Leeds Castle in Kent, a fortification held by Bartholomew de Badlesmere, steward of the King's household who had by 1321 joined the ranks of Edward's opponents. Historians believe that the pilgrimage was a deliberate act by Isabella on Edward's behalf to create a "casus belli". Lord Badlesmere was away at the time, having left his wife Margaret in charge of the castle. When the latter adamantly refused the Queen admittance, fighting broke out outside the castle between Isabella's guards and the garrison, marking the beginning of the Despenser War. Whilst Edward mobilised his own faction and placed Leeds castle under siege, Isabella was given the Great Seal and assumed control of the royal Chancery from the Tower of London. After surrendering to Edward's forces on 31 October 1321, Margaret, Baroness Badlesmere and her children were sent to the Tower, and 13 of the Leeds garrison were hanged. By January 1322, Edward's army, reinforced by the Despensers returning from exile, had forced the surrender of the Mortimers, and by March Lancaster himself had been captured after the battle of Boroughbridge; Lancaster was promptly executed, leaving Edward and the Despensers victorious.
Hugh Despenser the younger was now firmly ensconced as Edward's new favourite and lover, and together over the next four years Edward and the Despensers imposed a harsh rule over England, a "sweeping revenge" characterised by land confiscation, large-scale imprisonment, executions and the punishment of extended family members, including women and the elderly. This was condemned by contemporary chroniclers, and is felt to have caused concern to Isabella as well; some of those widows being persecuted included her friends. Isabella's relationship with Despenser the younger continued to deteriorate; the Despensers refused to pay her monies owed to her, or return her castles at Marlborough and Devizes. Indeed, various authors have suggested that there is evidence that Hugh Despenser the younger may have attempted to assault Isabella herself in some fashion. Certainly, immediately after the battle of Boroughbridge, Edward began to be markedly less generous in his gifts towards Isabella, and none of the spoils of the war were awarded to her. Worse still, later in the year Isabella was caught up in the failure of another of Edward's campaigns in Scotland, in a way that permanently poisoned her relationship with both Edward and the Despensers.
Isabella and Edward had travelled north together at the start of the autumn campaign; before the disastrous battle of Old Byland, Edward had ridden south, apparently to raise more men, sending Isabella east to Tynemouth Priory. With the Scottish army marching south, Isabella expressed considerable concern about her personal safety and requested assistance from Edward. Her husband initially proposed sending Despenser forces to secure her, but Isabella rejected this outright, instead requesting friendly troops. Rapidly retreating south with the Despensers, Edward failed to grip the situation, with the result that Isabella found herself and her household cut off from the south by the Scottish army, with the coastline patrolled by Flemish naval forces allied to the Scots. The situation was precarious and Isabella was forced to use a group of squires from her personal retinue to hold off the advancing army whilst other of her knights commandeered a ship; the fighting continued as Isabella and her household retreated onto the vessel, resulting in the death of two of her ladies-in-waiting. Once aboard, Isabella evaded the Flemish navy, landing further south and making her way to York. Isabella was furious, both with Edward for, from her perspective, abandoning her to the Scots, and with Despensers for convincing Edward to retreat rather than sending help. For his part, Edward blamed Lewis de Beaumont, the Bishop of Durham and an ally of Isabella, for the fiasco.
Isabella effectively separated from Edward from here onwards, leaving him to live with Hugh Despenser. At the end of 1322, Isabella left the court on a ten-month-long pilgrimage around England by herself. On her return in 1323 she visited Edward briefly, but refused to take a loyalty oath to the Despensers and was removed from the process of granting royal patronage. At the end of 1324, as tensions grew with Isabella's homeland of France, Edward and the Despensers confiscated all of Isabella's lands, took over the running of her household and arrested and imprisoned all of her French staff. Isabella's youngest children were removed from her and placed into the custody of the Despensers. At this point, Isabella appears to have realised that any hope of working with Edward was effectively over and begun to consider radical solutions.
The invasion of England.
By 1325, Isabella was facing increasing pressure from Hugh Despenser the Younger, Edward's new royal favourite. With her lands in England seized, her children taken away from her and her household staff arrested, Isabella began to pursue other options. When her brother, King Charles IV of France, seized Edward's French possessions in 1325, she returned to France, initially as a delegate of the King charged with negotiating a peace treaty between the two nations. However, her presence in France became a focal point for the many nobles opposed to Edward's reign. Isabella gathered an army to oppose Edward, in alliance with Roger Mortimer, whom she took as a lover. Isabella and Mortimer returned to England with a mercenary army, seizing the country in a lightning campaign. The Despensers were executed and Edward was forced to abdicate – his eventual fate and possible murder remains a matter of considerable historical debate. Isabella ruled as regent until 1330, when her son, Edward deposed Mortimer in turn and ruled directly in his own right.
Tensions in Gascony, 1323-5.
Isabella's husband Edward, as the Duke of Aquitaine, owed homage to the King of France for his lands in Gascony. Isabella's three brothers each had only short reigns, and Edward had successfully avoided paying homage to Louis X, and had only paid homage to Philip V under great pressure. Once Charles IV took up the throne, Edward had attempted to avoid doing so again, increasing tensions between the two. One of the elements in the disputes was the border province of Agenais, part of Gascony and in turn part of Aquitaine. Tensions had risen in November 1323 after the construction of a bastide, a type of fortified town, in Saint-Sardos, part of the Agenais, by a French vassal. Gascon forces destroyed the bastide, and in turn Charles attacked the English-held Montpezat: the assault was unsuccessful, but in the subsequent War of Saint-Sardos Isabella's uncle, Charles of Valois, successfully wrestled Aquitaine from English control; by 1324, Charles had declared Edward's lands forfeit and had occupied the whole of Aquitaine apart from the coastal areas.
Edward was still unwilling to travel to France to give homage; the situation in England was febrile; there had been an assassination plot against Edward and Hugh Despenser in 1324, there had been allegations that the famous magician John of Nottingham had been hired to kill the pair using necromancy in 1325, and criminal gangs were occupying much of the country. Edward was deeply concerned that should he leave England, even for a short while, the barons would take the chance to rise up and take their revenge on the Despensers. Charles sent a message through Pope John XXII to Edward, suggesting that he was willing to reverse the forfeiture of the lands if Edward ceded the Agenais and paid homage for the rest of the lands: the Pope proposed Isabella as an ambassador. Isabella, however, saw this as a perfect opportunity to resolve her situation with Edward and the Despensers.
Having promised to return to England by the summer, Isabella reached Paris in March 1325, and rapidly agreed a truce in Gascony, under which Prince Edward, then thirteen years old, would come to France to give homage on his father's behalf. Prince Edward arrived in France, and gave homage in September. At this point, however, rather than returning, Isabella remained firmly in France with her son. Edward began to send urgent messages to the Pope and to Charles IV, expressing his concern about his wife's absence, but to no avail. For his part, Charles replied that the, "queen has come of her own will and may freely return if she wishes. But if she prefers to remain here, she is my sister and I refuse to expel her." Charles went on to refuse to return the lands in Aquitaine to Edward, resulting in a provisional agreement under which Edward resumed administration of the remaining English territories in early 1326 whilst France continued to occupy the rest.
Meanwhile, the messages brought back by Edward's agent Walter de Stapledon, Bishop of Exeter and others grew steadily worse: Isabella had publicly snubbed Stapledon; Edward's political enemies were gathering at the French court, and threatening his emissaries; Isabella was dressed as a widow, claiming that Hugh Despenser had destroyed her marriage with Edward; Isabella was assembling a court-in-exile, including Edmund of Kent and John of Brittany, Earl of Richmond. By this stage Isabella had begun a romantic relationship with the English exile Roger Mortimer.
Roger Mortimer, 1325-6.
Roger Mortimer of Wigmore was a powerful Marcher lord, married to the wealthy heiress Joan de Geneville, and the father of twelve children. Mortimer had been imprisoned in the Tower of London in 1322 following his capture by Edward during the Despenser wars. Mortimer's uncle, Roger Mortimer de Chirk finally died in prison, but Mortimer managed to escape the Tower in August 1323, making a hole in the stone wall of his cell and then escaping onto the roof, before using rope ladders provided by an accomplice to get down to the River Thames, across the river and then on eventually to safety in France. Victorian writers suggested that, given later events, Isabella might have helped Mortimer escape and some historians continue to argue that their relationship had already begun at this point, although most believe that there is no hard evidence for their having had a substantial relationship before meeting in Paris.
Isabella was reintroduced to Mortimer in Paris by her cousin, Joan, Countess of Hainault, who appears to have approached Isabella suggesting a marital alliance between their two families, marrying Prince Edward to Joan's daughter, Philippa. Mortimer and Isabella began a passionate relationship from December 1325 onwards; Isabella was taking a huge risk in doing so – female infidelity was a very serious offence in medieval Europe, as shown during the Tour de Nesle Affair – both Isabella's former French sisters-in-law had died by 1326 as a result of their imprisonment for exactly this offence. Isabella's motivation has been the subject of discussion by historians; most agree that there was a strong sexual attraction between the two, that they shared an interest in the Arthurian legends and that they both enjoyed fine art and high living. One historian has described their relationship as one of the "great romances of the Middle Ages". They also shared a common enemy – the regime of Edward II and the Despensers.
Taking Prince Edward with them, Isabella and Mortimer left the French court in summer 1326 and travelled north to William I, Count of Hainaut. As Joan had suggested the previous year, Isabella betrothed Prince Edward to Philippa, the daughter of the Count, in exchange for a substantial dowry. She then used this money plus an earlier loan from Charles to raise a mercenary army, scouring Brabant for men, which were added to a small force of Hainaut troops. William also provided eight men of war ships and various smaller vessels as part of the marriage arrangements. Although Edward was now fearing an invasion, secrecy remained key, and Isabella convinced William to detain envoys from Edward. Isabella also appears to have made a secret agreement with the Scots for the duration of the forthcoming campaign. On 22 September, Isabella, Mortimer and their modest force set sail for England.
Seizure of power, 1326.
Having evaded Edward's fleet, which had been sent to intercept them, Isabella and Mortimer landed at Orwell on the east coast of England on 24 September with a small force; estimates of Isabella's army vary from between 300 to around 2,000 soldiers, with 1,500 being a popular middle figure. After a short period of confusion during which they attempted to work out where they had actually landed, Isabella moved quickly inland, dressed in her widow's clothes. The local levies mobilised to stop them immediately changed sides, and by the following day Isabella was in Bury St Edmunds and shortly afterwards had swept inland to Cambridge. Thomas, Earl of Norfolk, joined Isabella's forces and Henry of Lancaster – the brother of the late Thomas, and Isabella's uncle – also announced he was joining Isabella's faction, marching south to join her.
By the 27th, word of the invasion had reached the King and the Despensers in London. Edward issued orders to local sheriffs to mobilise opposition to Isabella and Mortimer, but London itself was becoming unsafe because of local unrest and Edward made plans to leave. Isabella struck west again, reaching Oxford on 2 October where she was "greeted as a saviour" – Adam Orleton, the bishop of Hereford, emerged from hiding to give a lecture to the university on the evils of the Despensers. Edward fled London on the same day, heading west toward Wales. Isabella and Mortimer now had an effective alliance with the Lancastrian opposition to Edward, bringing all of his opponents into a single coalition.
Isabella now marched south towards London, pausing at Dunstable, outside the city on 7 October. London was now in the hands of the mobs, although broadly allied to Isabella. Bishop Stapledon failed to realise the extent to which royal power had collapsed in the capital and tried to intervene militarily to protect his property against rioters; a hated figure locally, he was promptly attacked and killed – his head was later sent to Isabella by her local supporters. Edward, meanwhile, was still fleeing west, reaching Gloucester by the 9th. Isabella responded by marching swiftly west herself in an attempt to cut him off, reaching Gloucester a week after Edward, who slipped across the border into Wales the same day.
Hugh de Despenser the elder continued to hold Bristol against Isabella and Mortimer, who placed it under siege between 18–26 October; when it fell, Isabella was able to recover her daughters Eleanor and Joan, who had been kept in the Despenser's custody. By now desperate and increasingly deserted by their court, Edward and Hugh Despenser the younger attempted to sail to Lundy, a small island just off the Devon coast, but the weather was against them and after several days they were forced to land back in Wales. With Bristol secure, Isabella moved her base of operations up to the border town of Hereford, from where she ordered Henry of Lancaster to locate and arrest her husband. After a fortnight of evading Isabella's forces in South Wales, Edward and Hugh were finally caught and arrested near Llantrisant on 16 November.
The retribution began immediately. Hugh Despenser the elder had been captured at Bristol, and despite some attempts by Isabella to protect him, was promptly executed by his Lancastrian enemies – his body was hacked to pieces and fed to the local dogs. The remainder of the former regime were brought to Isabella. Edmund Fitzalan, a key supporter of Edward II and who had received many of Mortimer's confiscated lands in 1322, was executed on 17 November. Hugh Despenser the younger was sentenced to be brutally executed on 24 November, and a huge crowd gathered in anticipation at seeing him die. They dragged him from his horse, stripped him, and scrawled Biblical verses against corruption and arrogance on his skin. He was then dragged into the city, presented to Queen Isabella, Roger Mortimer, and the Lancastrians. Despenser was then condemned to hang as a thief, be castrated, and then to be drawn and quartered as a traitor, his quarters to be dispersed throughout England. Simon of Reading, one of the Despensers' supporters, was hanged next to him, on charges of insulting Isabella. Once the core of the Despenser regime had been executed, Isabella and Mortimer began to show restraint. Lesser nobles were pardoned and the clerks at the heart of the government, mostly appointed by the Despensers and Stapleton, were confirmed in office. All that was left now was the question of Edward II, still officially Isabella's legal husband and lawful king.
The death of Edward, 1327.
As an interim measure, Edward II was held in the custody of Henry of Lancaster, who surrendered Edward's Great Seal to Isabella. The situation remained tense, however; Isabella was clearly concerned about Edward's supporters staging a counter-coup, and in November she seized the Tower of London, appointed one of her supporters as mayor and convened a council of nobles and churchmen in Wallingford to discuss the fate of Edward. The council concluded that Edward would be legally deposed and placed under house arrest for the rest of his life. This was then confirmed at the Parliament of England, dominated by Isabella and Mortimer's followers. The session was held in January 1327, with Isabella's case being led by her supporter Adam Orleton, Bishop of Hereford. Isabella's son, Prince Edward, was confirmed as Edward III, with his mother appointed regent. Isabella's position was still precarious, as the legal basis for deposing Edward was minimal and many lawyers of the day maintained that Edward was still the rightful king, regardless of the declaration of the Parliament. The situation could be reversed at any moment and Edward was known to be a vengeful ruler.
Edward II's subsequent fate, and Isabella's role in it, remains hotly contested by historians. The minimally agreed version of events is that Isabella and Mortimer had Edward moved from Kenilworth Castle in the Midlands to the safer location of Berkeley Castle in the Welsh borders, where he was put into the custody of Lord Berkeley. On 23 September, Isabella and Edward III were informed by messenger that Edward had died whilst imprisoned at the castle, because of a "fatal accident". Edward's body was apparently buried at Gloucester Cathedral, with his heart being given in a casket to Isabella. After the funeral, there were rumours for many years that Edward had survived and was really alive somewhere in Europe, some of which were captured in the famous Fieschi Letter written in the 1340s, although no concrete evidence ever emerged to support the allegations. There are, however, various historical interpretations of the events surrounding this basic sequence of events.
According to legend, Isabella and Mortimer famously plotted to murder Edward in such a way as not to draw blame on themselves, sending a famous order (in ) which, depending on where the comma was inserted, could mean either "Do not be afraid to kill Edward; it is good" or "Do not kill Edward; it is good to fear". In actuality, there is little evidence of anyone deciding to have Edward assassinated, and none whatsoever of the note having been written. Similarly, accounts of Edward being killed with a red-hot poker have no strong contemporary sources to support them. The conventional 20th-century view has been that Edward did die at Berkeley Castle, either murdered on Isabella's orders or of ill-health brought on by his captivity, and that subsequent accounts of his survival were simply rumours, similar to those that surrounded Joan of Arc and other near contemporaries after their deaths.
Three recent historians, however, have offered an alternative interpretation of events. Paul Doherty, drawing extensively on the Fieschi Letter of the 1340s, has argued that Edward in fact escaped from Berkeley Castle with the help of William Ockle, a knight whom Doherty argues subsequently pretended to be Edward in disguise around Europe, using the name "William the Welshman" to draw attention away from the real Edward himself. In this interpretation, a look-alike was buried at Gloucester. Ian Mortimer, focusing more on contemporary documents from 1327 itself, argues that Roger de Mortimer engineered a fake "escape" for Edward from Berkeley Castle; after this Edward was kept in Ireland, believing he was really evading Mortimer, before finally finding himself free, but politically unwelcome, after the fall of Isabella and Mortimer. In this version, Edward makes his way to Europe, before subsequently being buried at Gloucester. Finally, Alison Weir, again drawing on the Fieschi Letter, has recently argued that Edward II escaped his captors, killing one in the process, and lived as a hermit for many years; in this interpretation, the body in Gloucester Cathedral is of Edward's dead captor. In all of these versions, it is argued that it suited Isabella and Mortimer to publicly claim that Edward was dead, even if they were aware of the truth. Other historians, however, including David Carpenter, have criticised the methodology behind this revisionist approach and disagree with the conclusions.
Later years.
Isabella and Mortimer ruled together for four years, with Isabella's period as regent marked by the acquisition of huge sums of money and land. When their political alliance with the Lancastrians began to disintegrate, Isabella continued to support Mortimer, her lover. Isabella fell from power when her son, Edward III deposed Mortimer in a coup, taking back royal authority for himself. Unlike Mortimer, Isabella survived the transition of power, however, remaining a wealthy and influential member of the English court, albeit never returning directly to active politics.
As regent, 1326–30.
Isabella's reign as regent lasted only four years, before the fragile political alliance that had brought her and Mortimer to power disintegrated. 1328 saw the marriage of Isabella's son, Edward III to Philippa of Hainault, as agreed before the invasion of 1326; the lavish ceremony was held in London to popular acclaim. Isabella and Mortimer had already begun a trend that continued over the next few years, in starting to accumulate huge wealth. With her lands restored to her, Isabella was already exceptionally rich, but she began to accumulate yet more. Within the first few weeks, Isabella had granted herself almost £12,000; finding that Edward's royal treasury contained £60,000, a rapid period of celebratory spending then ensued. Isabella soon awarded herself another £20,000, allegedly to pay off foreign debts. At Prince Edward's coronation, Isabella then extended her land holdings from a value of £4,400 each year to the huge sum of £13,333, making her one of the largest landowners in the kingdom. Isabella also refused to hand over her dower lands to Philippa after her marriage to Edward III, in contravention of usual custom. Isabella's lavish lifestyle matched her new incomes. Mortimer, as her lover and effective first minister, after a restrained beginning, also began to accumulate lands and titles at a tremendous rate, particularly in the Marcher territories.
The new regime also faced some key foreign policy dilemmas, which Isabella approached from a realist perspective. The first of these was the situation in Scotland, where Edward II's unsuccessful policies had left an unfinished, tremendously expensive war. Isabella was committed to bringing this issue to a conclusion by diplomatic means. Edward III initially opposed this policy, before eventually relenting, leading to the Treaty of Northampton. Under this treaty, Isabella's daughter Joan would marry David Bruce (heir apparent to the Scottish throne) and Edward III would renounce any claims on Scottish lands, in exchange for the promise of Scottish military aid against any enemy except the French, and £20,000 in compensation for the raids across northern England. No compensation would be given to those earls who had lost their Scottish estates, and the compensation would be taken by Isabella. Although strategically successful and, historically at least, "a successful piece of policy making", Isabella's Scottish policy was by no means popular and contributed to the general sense of discontent with the regime. Secondly, the Gascon situation, still unresolved from Edward II's reign, also posed an issue. Isabella reopened negotiations in Paris, resulting in a peace treaty under which the bulk of Gascony, minus the Agenais, would be returned to England in exchange for a 50,000 mark penalty. The treaty was not popular in England because of the Agenais clause.
Henry of Lancaster was amongst the first to break with Isabella and Mortimer. By 1327 Lancaster was irritated by Mortimer's behaviour and Isabella responded by beginning to sideline him from her government. Lancaster was furious over the passing of the Treaty of Northampton, and refused to attend court, mobilising support amongst the commoners of London. Isabella responded to the problems by undertaking a wide reform of royal administration, local law enforcement. In a move guaranteed to appeal to domestic opinion, Isabella also decided to pursue Edward III's claim on the French throne, sending her advisers to France to demand official recognition of his claim. The French nobility were unimpressed and, since Isabella lacked the funds to begin any military campaign, she began to court the opinion of France's neighbours, including proposing the marriage of her son John to the Castilian royal family.
By the end of 1328 the situation had descended into near civil war once again, with Lancaster mobilising his army against Isabella and Mortimer. In January 1329 Isabella's forces under Mortimer's command took Lancaster's stronghold of Leicester, followed by Bedford; Isabella – wearing armour, and mounted on a warhorse – and Edward III marched rapidly north, resulting in Lancaster's surrender. He escaped death but was subjected to a colossal fine, effectively crippling his power. Isabella was merciful to those who had aligned themselves with him, although some – such as her old supporter Henry de Beaumont, whose family had split from Isabella over the peace with Scotland, which had lost them huge land holdings in Scotland – fled to France.
Despite Lancaster's defeat, however, discontent continued to grow. Edmund of Kent had sided with Isabella in 1326, but had since begun to question his decision and was edging back towards Edward II, his half-brother. Edmund of Kent was in conversations with other senior nobles questioning Isabella's rule, including Henry de Beaumont and Isabella de Vesci. Edmund was finally involved in a conspiracy in 1330, allegedly to restore Edward II, whom he claimed was still alive: Isabella and Mortimer broke up the conspiracy, arresting Edmund and other supporters – including Simon Mepeham, Archbishop of Canterbury. Edmund may have expected a pardon, possibly from Edward III, but Isabella was insistent on his execution. The execution itself was a fiasco after the executioner refused to attend and Edmund of Kent had to be killed by a local dung-collector, who had been himself sentenced to death and was pardoned as a bribe to undertake the beheading. Isabella de Vesci escaped punishment, despite have been closely involved in the plot.
Mortimer's fall from power, 1330.
By mid-1330, Isabella and Mortimer's regime was increasingly insecure, and Isabella's son, Edward III, was growing frustrated at Mortimer's grip on power. Various historians, with different levels of confidence, have also suggested that in late 1329 Isabella became pregnant. A child of Mortimer's with royal blood would have proved both politically inconvenient for Isabella, and challenging to Edward's own position.
Edward quietly assembled a body of support from the Church and selected nobles, whilst Isabella and Mortimer moved into Nottingham Castle for safety, surrounding themselves with loyal troops. In the autumn, Mortimer was investigating another plot against him, when he challenged a young noble, Montague, during an interrogation. Mortimer declared that his word had priority over the king's, an alarming statement that Montague reported back to Edward. Edward was convinced that this was the moment to act, and on 19 October, Montague led a force of twenty three armed men into the castle by a secret tunnel. Up in the keep, Isabella, Mortimer and other council members were discussing how to arrest Montague, when Montague and his men appeared. Fighting broke out on the stairs and Mortimer was overwhelmed in his chamber. Isabella threw herself at Edward's feet, famously crying ""Fair son, have pity on gentle Mortimer!"" Lancastrian troops rapidly took the rest of the castle, leaving Edward in control of his own government for the first time.
Parliament was convened the next month, where Mortimer was put on trial for treason. Isabella was portrayed as an innocent victim during the proceedings, and no mention of her sexual relationship with Mortimer was made public. Isabella's lover was executed at Tyburn, but Edward III showed leniency and he was not quartered or disembowelled.
In retirement, 1330–58.
After the coup, Isabella was initially transferred to Berkhamsted Castle, and then held under house arrest at Windsor Castle until 1332, when she then moved back to her own Castle Rising in Norfolk. Agnes Strickland, a Victorian historian, argued that Isabella suffered from occasional fits of madness during this period but modern interpretations suggest, at worst, a nervous breakdown following the death of her lover. Isabella remained extremely wealthy; despite being required to surrender most of her lands after losing power, in 1331 she was reassigned a yearly income of £3000, which increased to £4000 by 1337. She lived an expensive lifestyle in Norfolk, including minstrels, huntsmen, grooms and other luxuries, and was soon travelling again around England. In 1342, there were suggestions that she might travel to Paris to take part in peace negotiations, but eventually this plan was quashed. She was also appointed to negotiate with France in 1348 and was involved in the negotiations with Charles II of Navarre in 1358.
As the years went by, Isabella became very close to her daughter Joan, especially after Joan left her unfaithful husband, King David II of Scotland. Joan also nursed her just before she died. She doted on her grandchildren, including Edward, the Black Prince. She became increasingly interested in religion as she grew older, visiting a number of shrines. She remained, however, a gregarious member of the court, receiving constant visitors; amongst her particular friends appear to have been Roger Mortimer's daughter Agnes Mortimer, Countess of Pembroke, and Roger Mortimer's grandson, also called Roger Mortimer, whom Edward III restored to the Earldom of March. King Edward and his children often visited her as well. She remained interested in Arthurian legends and jewellery; in 1358 she appeared at the St George's Day celebrations at Windsor wearing a dress made of silk, silver, 300 rubies, 1800 pearls and a circlet of gold. She may also have developed an interest in astrology or geometry towards the end of her life, receiving various presents relating to these disciplines.
Isabella took the habit of the Poor Clares before she died on 22 August 1358, and her body was returned to London for burial at the Franciscan church at Newgate, in a service overseen by Archbishop Simon Islip. She was buried in the mantle she had worn at her wedding and at her request, Edward's heart, placed into a casket thirty years before, was interred with her. Isabella left the bulk of her property, including Castle Rising, to her favourite grandson, the Black Prince, with some personal effects being granted to her daughter Joan
Legacy.
Literature and theatre.
Queen Isabella appeared with a major role in Christopher Marlowe's play "Edward II" (c. 1592) and thereafter has been frequently used as a character in plays, books, and films, often portrayed as beautiful but manipulative or wicked. Thomas Gray, the 18th-century poet, combined Marlowe's depiction of Isabella with William Shakespeare's description of Margaret of Anjou (the wife of Henry VI) as the "She-Wolf of France", to produce the anti-French poem "The Bard" (1757), in which Isabella rips apart the bowels of Edward II with her "unrelenting fangs". The "She-Wolf" epithet stuck, and Bertolt Brecht re-used it in "The Life of Edward II of England" (1923).
Film.
In Derek Jarman's film "Edward II" (1991), based on Marlowe's play, Isabella is portrayed (by actress Tilda Swinton) as a "femme fatale" whose thwarted love for Edward causes her to turn against him and steal his throne. In contrast to the negative depictions, Mel Gibson's film "Braveheart" (1995) portrays Isabella (played by the French actress Sophie Marceau) more sympathetically. In the film, an adult Isabella is fictionally depicted as having a romantic affair with the Scottish hero William Wallace. However, in reality, she was 9-years-old at the time of Wallace's death. Additionally, Wallace is incorrectly suggested to be the father of her son, Edward III, despite Wallace's death many years before Edward's birth.
Issue.
Edward and Isabella had four children, and she suffered at least one miscarriage. Their itineraries demonstrate that they were together 9 months prior to the births of all four surviving offspring. Their children were:
Ancestry.
Isabella is descended from Gytha of Wessex through King Andrew II of Hungary and thus brought the bloodline of the last Saxon King of England, Harold Godwinson, back into the English Royal family.

</doc>
<doc id="47622" url="https://en.wikipedia.org/wiki?curid=47622" title="Caernarfon">
Caernarfon

Caernarfon (; ) is a royal town, community, and port in Gwynedd, Wales, with a population of 9,615 (this figure does not include nearby Bontnewydd or Caeathro, as they are in separate communities). It lies along the A487 road, on the eastern shore of the Menai Strait, opposite the Isle of Anglesey. The city of Bangor is to the north-east, while Snowdonia fringes Caernarfon to the east and south-east. Carnarvon and Caernarvon are Anglicised spellings that were superseded in 1926 and 1974, respectively.
Abundant natural resources in and around the Menai Straits enabled human habitation in prehistoric Britain. The Ordovices, a Celtic tribe, lived in the region during the period known as Roman Britain. The Roman fort Segontium was established around  80 to subjugate the Ordovices during the Roman conquest of Britain. The Romans occupied the region until the end of Roman rule in Britain in 382, after which Caernarfon became part of the Kingdom of Gwynedd. In the late 11th century, William the Conqueror ordered the construction of a motte-and-bailey castle at Caernarfon as part of the Norman invasion of Wales. He was unsuccessful, and Wales remained independent until around 1283.
In the 13th century, Llywelyn ap Gruffudd, ruler of Gwynedd, refused to pay homage to Edward I of England, prompting the English conquest of Gwynedd. This was followed by the construction of Caernarfon Castle, one of the largest and most imposing fortifications built by the English in Wales. In 1284, the English-style county of Caernarfonshire was established by the Statute of Rhuddlan; the same year, Caernarfon was made a borough, a county and market town, and the seat of English government in North Wales.
The ascent of the House of Tudor to the throne of England eased hostilities between the English and resulted in Caernarfon Castle falling into a state of disrepair. The city has flourished, leading to its status as a major tourist centre and seat of Gwynedd Council, with a thriving harbour and marina. Caernarfon has expanded beyond its medieval walls and experienced heavy suburbanisation. Its population includes the largest percentage of Welsh-speaking citizens anywhere in Wales. The status of Royal Borough was granted by Queen Elizabeth II in 1963 and emended to Royal Town in 1974. The castle and town walls are part of a World Heritage Site described as the Castles and Town Walls of King Edward in Gwynedd.
History.
The present city of Caernarfon grew up around and owes its name to its Norman and Edwardian fortifications. The earlier British and Romano-British settlement at Segontium was named for the nearby Afon Seiont. After the end of Roman rule in Britain around 410, the settlement continued to be known as ' ("Fort Seiont") and as ' ("Fort Constantius or Constantine"), of the "History of the Britons", cited by James Ussher in Newman's life of Germanus of Auxerre, both of whose names appear among the 28 "civitates" of sub-Roman Britain in the "Historia Brittonum" traditionally ascribed to Nennius. The work states that the inscribed tomb of "Constantius the Emperor" (presumably Constantius Chlorus, father of Constantine the Great) was still present in the 9th century. (Constantius actually died at York; Ford credited the monument to a different Constantine, the supposed son of Saint Elen and Magnus Maximus, who was said to have ruled northern Wales before being removed by the Irish.) The medieval romance about Maximus and Elen, "Macsen's Dream", calls her home ' ("Fort Seiontmouth" or "the caer at the mouth of the Seiont") and other pre-conquest poets such as Hywel ab Owain Gwynedd also used the name '.
The Norman motte was erected apart from the existing settlement and came to be known as ', "the fortress in Arfon". (The region of Arfon itself derived its name from its position opposite Anglesey, known as ' in Welsh.) A 1221 charter by Llywelyn the Great to the canons of Penmon priory on Anglesey mentions '; In 1283, King Edward I completed his conquest of Wales which he secured by a chain of castles and walled towns. The construction of a new stone Caernarfon Castle seems to have started as soon as the campaign had finished. Edward's architect, James of St. George, may well have modelled the castle on the walls of Constantinople, possibly being aware of the town's legendary associations. Edward's fourth son, Edward of Caernarfon, later Edward II of England, was born at the castle in April 1284 and made Prince of Wales in 1301. A story recorded in the 16th century suggests that the new prince was offered to the native Welsh on the premise "that [he was borne in Wales and could speake never a word of English", however there is no contemporary evidence to support this.
Caernarfon was constituted a borough in 1284 by charter of Edward I. The charter, which was confirmed on a number of occasions, appointed the mayor of the borough Constable of the Castle ex officio. The former municipal borough was designated a royal borough in 1963. The borough was abolished by the Local Government Act 1972 in 1974, and the status of "royal town" was granted to the community which succeeded it. Caernarfon was the county town of the historic county of Caernarfonshire.
In 1911, David Lloyd George, then Member of Parliament for Caernarfon boroughs, which included various towns from Llŷn to Conwy, agreed to the British Royal Family's idea of holding the investiture of the Prince of Wales at Caernarfon Castle. The ceremony took place on 13 July, with the royal family paying a rare visit to Wales, and the future Edward VIII was duly invested.
In 1955 Caernarfon was in the running for the title of Capital of Wales on historical grounds but the town's campaign was heavily defeated in a ballot of Welsh local authorities, with 11 votes compared to Cardiff's 136. Cardiff therefore became the Welsh capital.
On 1 July 1969 the investiture ceremony for Charles, Prince of Wales was again held at Caernarfon Castle. The ceremony itself went ahead without incident despite terrorist threats and protests, which culminated in the death of two members of Mudiad Amddiffyn Cymru (Welsh Defence Movement), Alwyn Jones and George Taylor, who were killed when their bomb – intended for the railway line at Abergele in order to stop the British Royal Train – exploded prematurely. The bomb campaign (one in Abergele, two in Caernarfon and finally one on Llandudno Pier) was organised by the leader of Mudiad Amddiffyn Cymru, John Jenkins. He was later arrested after a tip-off and was sentenced to ten years' imprisonment.
The history of Caernarfon as an example where the rise and fall of different civilizations can be seen from one hilltop, are discussed in John Michael Greer's book "The Long Descent". He writes of Caernarfon: 
Geography.
Caernarfon is situated on the southern bank of the Menai Strait facing the Isle of Anglesey. It is situated south-west of Bangor, north of Porthmadog and approximately west of Llanberis and Snowdonia National Park. The mouth of the River Seiont is in the town, creating a natural harbour where it flows into the Menai Strait. Caernarfon Castle stands at the mouth of the river. The A487 passes directly through Caernarfon, with Bangor to the north and Porthmadog to the south. Llanberis at the foot of Snowdon can be reached via the A4086, which heads east out of the town to Capel Curig. Heading north out of the town is the Lôn Las Menai cycle path to nearby Y Felinheli. Heading south out of the town is the Lôn Eifion cycle path, which leads to Bryncir, near Criccieth. The route provides views into the Snowdonia mountains, down along the Llŷn Peninsula and across to the Isle of Anglesey. The restoration of the Welsh Highland Railway or Rheilffordd Eryri, a narrow gauge heritage railway, was completed in 2011 and runs from Caernarfon to Porthmadog where it connects with the Festiniog Railway.
Economy.
Caernarfon's historical prominence and landmarks have made it a major tourist centre. As a result, many of the local businesses cater for the tourist trade. Caernarfon is home to numerous guest houses, inns and pubs, hotels, restaurants and shops. The majority of shops in the town are located either in the centre of town around Pool Street and Castle Square (Maes), or on Doc Fictoria. A number of shops are also located within the Town Walls.
The majority of the retail and residential section of Doc Fictoria (Victoria Dock) was opened in 2008. The retail and residential section of Doc Fictoria is built directly beside a Blue Flag beach marina. It contains numerous homes, bars and bistros, cafés and restaurants, an award- winning arts centre, a maritime museum and a range of shops and stores.
Pool Street and Castle Square (Maes) contain a number of large, national retail shops and smaller independent stores. Pool Street is a pedestrianised street and, as such, serves as the town's main shopping street. Castle Square, commonly referred to as the 'Maes' by both Welsh and English speakers, is the market square of the town. A market is held every Saturday throughout the year and also on Mondays in the Summer. The square was revamped at a cost of £2.4 million in 2009. However, since its revamp the square has caused controversy due to traffic and parking difficulties. During the revamp, it was decided to remove barriers between traffic and pedestrians creating a 'shared space', to try and force road users to be more considerate of pedestrians and other vehicles. This is the first use of this kind of arrangement in Wales, but it has been described by councillor Bob Anderson as being 'too ambiguous' for road users. Another controversy caused by the revamp of the Maes was that a historic feature of the town was taken down, namely a very old oak tree, situated outside the HSBC bank. When the Maes was re-opened in July 2009 by the local politician and Heritage Minister of Wales, Alun Ffred Jones AM, he said, "the use of beautiful local slate is very prominent in the new Maes."
There are many old public houses serving the town, including The Four Alls, The Anglesey Arms Hotel, The Castle Hotel, The Crown, Morgan Lloyd, Pen Deitch and The Twthill Vaults. The oldest public house in Caernarfon is the Black Boy Inn, which remained in the same family for over 40 years until sold in 2003 to a local independent family business. The pub has stood inside Caernarfon's Town Walls since the 16th century, and many ghosts have been sighted within the building.
In and around the Town Walls are numerous restaurants, public houses and inns, and guest houses and hostels.
Local government.
Gwynedd Council's head offices are situated in the town. The local court serves the town and the rest of north-west Wales, and in 2009 moved to a multimillion-pound court complex on Llanberis Road. The Caernarfon UK Parliament constituency was a former electoral area centred on Caernarfon. Caernarfon is now part of the Arfon constituency for both the UK Parliament and the National Assembly for Wales. The town is twinned with Landerneau in Brittany.
Demography.
The population of Caernarfon Community Parish in 2001 was 9,611. Caernarfon residents are known colloquially as "Cofis". The word "Cofi" is also used locally in Caernarfon to describe the local Welsh dialect, notable for a number of words not in use elsewhere.
Within Wales, Gwynedd has the highest proportion of speakers of the Welsh language. The greatest concentration of Welsh speakers in Gwynedd is found in and around Caernarfon. According to the 2001 Census, 86.1% of the population could speak Welsh; the largest majority of Welsh speakers was found in the 10-14 age group, where 97.7% could speak it fluently. The town is nowadays a rallying-point for the Welsh nationalist cause.
Landmarks.
Caernarfon Castle.
The present castle building was constructed between 1283 and 1330 by order of King Edward I. The banded stonework and polygonal towers are thought to have been in imitation of the Walls of Constantinople. The impressive curtain wall with nine towers and two gatehouses survive largely intact. Caernarfon Castle is now under the care of Cadw and is open to the public. The castle includes the regimental museum of the Royal Welch Fusiliers.
Caernarfon town walls.
The medieval town walls, including eight towers and two twin-towered gateways form a complete circuit of 730 metres around the old town and were built between 1283 and 1285. The walls are in the care of Cadw but only a small section is accessible to the public. The town walls and castle at Caernarfon were declared part of a World Heritage Site in 1986. According to UNESCO, the castle and walls together with other royal castles in Gwynedd "are the finest examples of late 13th century and early 14th century military architecture in Europe".
Church of St Peblig.
Dedicated to Saint Peblig, the son of Saint Elen and Macsen Wledig (Magnus Maximus), the church is built on an important early Christian site, itself built on a Roman Mithraeum or temple of Mithras, close to the Segontium Roman Fort. A Roman altar was found in one of the walls during 19th century restoration work. The present church dates mainly from the 14th century and is a Grade I listed building.
Statue of David Lloyd George.
The statue in Castle Square was sculpted by W. Goscombe John and was erected in 1921 when Lloyd George was Prime Minister. David Lloyd George was the Member of Parliament for the area from 1890 to 1945.
The Old Market Hall.
The Old Market Hall in Hole-in-the-Wall Street and Crown Street was built in 1832, but the interior and roof were rebuilt later in that century. It is a Grade II listed building.
Others.
The old court buildings, replaced in 2009 by a new complex designed by HOK on the former Segontium School site in Llanberis Road, are situated inside the castle walls, next door to the Anglesey Arms Hotel and to the Gwynedd County Council buildings in Pendeitch. They are grand buildings, especially the exterior of the former magistrates' court, which features a gothic architecture style of decoration. The old buildings adjoin what used to be Caernarfon gaol, which has been closed since about the early 20th century and has now been converted into further council offices.
There is a small hospital in the town, 'Ysbyty Eryri' (i.e. "Snowdon Hospital"). The nearest large regional hospital is Ysbyty Gwynedd, in Bangor.
Previously, Caernarfon had been chosen as the location of a new prison. HMP Caernarfon would have held up to 800 adult males when constructed, and would have taken prisoners from all over the North Wales area. However, in September 2009 the UK Government withdrew plans to construct the prison.
Transport.
Caernarfon was at one time an important port, exporting slate from the Dyffryn Nantlle quarries.
Caernarvon railway station served the town from 1852 to 1970 and was one of the last passenger services to be closed under the Beeching Axe; it is now the site of a Morrisons supermarket. The site served as the terminus of the Bangor and Carnarvon Railway, and an end-on junction with the Carnarvonshire Railway and the Carnarvon and Llanberis Railway. All three companies were operated by and absorbed into the London and North Western Railway by 1871.
The route of the line southwards passed through a tunnel under central Caernarfon that was converted in 1995 for road traffic. The new Caernarfon railway station in St. Helen's Road is the northern terminus of the narrow gauge Rheilffordd Eryri / Welsh Highland Railway.
Bus services in the town are provided by Arriva Buses Wales, GHA Coaches, Express Motors and Padarn Bus.
Caernarfon Airport is to the south west, and offers pleasure flights and an aviation museum.
Education.
There are four primary schools in Caernarfon, Ysgol yr Hendre being the largest. The others are Ysgol y Gelli, Ysgol Santes Helen and Ysgol Maesincla.
The single secondary school serving Caernarfon and the surrounding areas – Ysgol Syr Hugh Owen – currently has between 900 and 1000 pupils from ages 11 to 18.
Ysgol Pendalar, a school for children with special needs, serves all of Arfon.
Coleg Menai is a further education college for adult learners.
Sport.
Caernarfon Town F.C. is a football team that plays at The Oval, in Division One of the Welsh Alliance League. Caernarfon Wanderers play in Division Two of the Welsh Alliance. There is a rugby union club, Clwb Rygbi Caernarfon, which plays in Division One North of the Swalec League. The club's home ground is Y Morfa. A Caernarfon Cricket Club was formed in 1841, making it one of the oldest clubs in north Wales.
Culture.
Caernarfon hosted the National Eisteddfod in 1862, 1894, 1906, 1921, 1935, 1959 and 1979. Unofficial National Eisteddfod events were also held there in 1877 and 1880. Caernarfon also hosted the 30th annual Celtic Media Festival in March 2009.
Cultural destinations include Galeri, Bocs and Oriel Pendeitsh.
Galeri is a creative enterprise centre that houses a gallery, a concert hall, cinema, a number of companies, and a range of other creative and cultural spaces.
Bocs is a young artists' co-operative and an arts centre that holds exhibitions and a range of cultural and creative events.
Oriel Pendeitsh is a ground-floor exhibition space adjoining the Tourist Information Centre opposite Caernarfon Castle. The gallery has a varied and changing programme of exhibitions throughout the year.

</doc>
<doc id="47623" url="https://en.wikipedia.org/wiki?curid=47623" title="Free Trade Area of the Americas">
Free Trade Area of the Americas

The Free Trade Area of the Americas (FTAA; , ALCA; , ZLÉA; , ALCA; ) was a proposed agreement to eliminate or reduce the trade barriers among all countries in the Americas, excluding Cuba.
History.
In the latest round of negotiations, trade ministers from 34 countries met in Miami, Florida, in the United States, in November 2003 to discuss the proposal. The proposed agreement was an extension of the North American Free Trade Agreement (NAFTA) between Canada, Mexico, and the United States. Opposing the proposal were Cuba, Venezuela, Bolivia, Ecuador, Dominica, and Nicaragua (all of which entered the Bolivarian Alternative for the Americas in response), and Mercosur member states.
Discussions have faltered over similar points as the Doha Development Round of World Trade Organization (WTO) talks; developed nations seek expanded trade in services and increased intellectual property rights, while less developed nations seek an end to agricultural subsidies and free trade in agricultural goods. Similar to the WTO talks, Brazil has taken a leadership role among the less developed nations, while the United States has taken a similar role for the developed nations.
Beginning.
Free Trade Area of the Americas began with the Summit of the Americas in Miami, Florida, on December 11, 1994, but the FTAA came to public attention during the Quebec City Summit of the Americas, held in Canada in 2001, a meeting targeted by massive anti-corporatization and anti-globalization protests. The Miami negotiations in 2003 met similar protests, though perhaps not as large.
Disagreements.
In previous negotiations, the United States had pushed for a single comprehensive agreement to reduce trade barriers for goods, while increasing intellectual property protection. Specific intellectual property protections could include Digital Millennium Copyright Act-style copyright protections similar to the U.S.-Australia Free Trade Agreement. Another protection would likely restrict the reimportation or cross-importation of pharmaceuticals, similar to the proposed agreement between the United States and Canada. Brazil proposed a three-track approach that calls for a series of bilateral agreements to reduce specific tariffs on goods, a hemispheric pact on rules of origin, and a dispute resolution process; Brazil proposed to omit the more controversial issues from the FTAA, leaving them to the WTO.
The location of the FTAA Secretariat was to have been determined in 2005. The contending cities are: Atlanta, Chicago, Galveston, Houston, San Juan, and Miami in the United States; Cancún and Puebla in Mexico; Panama City, Panama; and Port of Spain, Trinidad and Tobago. The U.S. city of Colorado Springs also submitted its candidacy in the early days but subsequently withdrew. Miami, Panama City and Puebla served successively as interim secretariat headquarters during the negotiation process.
The last summit was held at Mar del Plata, Argentina, in November 2005, but no agreement on FTAA was reached. Of the 34 countries present at the negotiations, 26 pledged to meet again in 2006 to resume negotiations, but no such meeting took place. The failure of the Mar del Plata summit to establish a comprehensive FTAA agenda augured poorly.
Current status.
The FTAA missed the targeted deadline of 2005, which followed the stalling of useful negotiations of the World Trade Organization Ministerial Conference of 2005. Over the next few years, some governments, most notably the United States, not wanting to lose any chance of hemispheric trade expansion moved in the direction of establishing a series of bilateral trade deals. The leaders however, planned further discussions at the Sixth Summit of the Americas in Cartagena, Colombia, in 2012.
Membership.
The following countries are in the plans of the Free Trade Area of the Americas:
Current support and opposition.
A vocal critic of the FTAA was Venezuelan president Hugo Chávez, who has described it as an "annexation plan" and a "tool of imperialism" for the exploitation of Latin America. As a counterproposal to this initiative, Chávez promoted the Bolivarian Alternative for the Americas ("Alternativa Bolivariana para las Américas", ALBA), based mostly on the model of the Eurasian Economic Community, which makes emphasis on energy and infrastructure agreements that are gradually extended to other areas finally to include the total economic, political and military integration of the member states. Also, Evo Morales of Bolivia has referred to the U.S.-backed Free Trade Area of the Americas, as "an agreement to legalize the colonization of the Americas".
On the other hand, the then presidents of Brazil, Luiz Inácio Lula da Silva, and Argentina, Néstor Kirchner, have stated that they do not oppose the FTAA but they do demand that the agreement provide for the elimination of U.S. agriculture subsidies, the provision of effective access to foreign markets and further consideration towards the needs and sensibilities of its members.
One of the most contentious issues of the treaty proposed by the United States is with concerns to patents and copyrights. Critics claim that if the measures proposed by the United States were implemented and applied this would reduce scientific research in Latin America. On the left-wing Council of Canadians web site, Barlow wrote: "This agreement sets enforceable global rules on patents, copyrights and trademark. It has gone far beyond its initial scope of protecting original inventions or cultural products and now permits the practice of patenting plants and animal forms as well as seeds. It promotes the private rights of corporations over local communities and their genetic heritage and traditional medicines".
On the weekend of April 20, 2001, the 3rd Summit of the Americas was a summit held in Quebec City, Canada. This international meeting was a round of negotiations regarding a proposed FTAA.
Agreements.
There are currently 34 countries in the Western Hemisphere, stretching from Canada to Chile that still have the FTAA as a long term goal. The Implementation of a full multilateral FTAA between all parties could be made possible by enlargement of existing agreements. North America, with the exception of Cuba and Haiti (which has participated in economic integration with the Caricom since 2002) has come close to setting up a subcontinental free trade area. At this point Agreements within the Area of the Americas include:

</doc>
<doc id="47624" url="https://en.wikipedia.org/wiki?curid=47624" title="Pawn (chess)">
Pawn (chess)

The pawn (♙♟) is the most numerous piece in the game of chess, and in most circumstances, also the weakest. It historically represents infantry, or more particularly, armed peasants or pikemen. Each player begins a game of chess with eight pawns, one on each square of the rank immediately in front of the other pieces. (In algebraic notation, the white pawns start on a2, b2, c2, ..., h2, while black pawns start on a7, b7, c7, ..., h7.)
Individual pawns are referred to by the file on which they stand. For example, one speaks of "White's f-pawn" or "Black's b-pawn", or less commonly (using descriptive notation), "White's king bishop pawn" or "Black's queen knight pawn". It is also common to refer to a "rook pawn", meaning any pawn on the a- or h-file, a "knight pawn" (on the b- or g-file), a "bishop pawn" (on the c- or f-file), a "queen pawn" (on the d-file), a "king pawn" (on the e-file), and a "central pawn" (on either the d- or e-file).
Movement.
Unlike the other pieces, pawns may not move backwards. Normally a pawn moves by advancing a single square, but the first time a pawn is moved, it has the option of advancing two squares. Pawns may not use the initial two-square advance to jump over an occupied square, or to capture. Any piece directly in front of a pawn, friend or foe, blocks its advance. In the diagram at right, the pawn on c4 may move to c5, while the pawn on e2 may move to either e3 or e4.
Capturing.
Unlike other pieces, the pawn does not capture in the same direction as it otherwise moves. A pawn captures diagonally, one square forward and to the left or right.
In the diagram to the left, the white pawn may capture either the black rook or the black knight.
Another unusual move is the "en passant" capture.
This arises when a pawn uses its initial move option to advance two squares instead of one, and in so doing passes over a square that is attacked by an enemy pawn. That enemy pawn, which would have been able to capture the moving pawn had it advanced only one square, is entitled to capture the moving pawn "in passing" "as if" it had advanced only one square. The capturing pawn moves into the empty square over which the moving pawn passed, and the moving pawn is removed from the board. In the diagram at right, the black pawn has just moved c7 to c5, so the white pawn may capture it by going from d5 to c6. The option to capture "en passant" must be exercised on the move immediately following the double-square pawn advance, or it is lost for the rest of the game. The "en passant" move was added to the pawn's repertoire in the 15th century to compensate for the then newly added two-square initial move rule . Without "en passant", a pawn could simply march past squares guarded by opposing pawns; "en passant" preserves the restrictive ability of pawns that have reached the fifth rank.
Promotion.
A pawn that advances all the way to the opposite side of the board (the opposing player's first rank) is "promoted" to another piece of that player's choice: a queen, rook, bishop, or knight of the same color. The pawn is immediately (before the opposing player's next move) replaced by the new piece. Since it is uncommon for a piece other than a queen to be chosen, promotion is often called "queening". When some other piece is chosen it is known as "underpromotion", and the piece selected is most often a knight, used to execute a checkmate or a fork giving the player a net increase in material compared to promoting to a queen. Underpromotion is also used in situations where promoting to a queen would give immediate stalemate.
The choice of promotion is not limited to pieces that have been captured. Thus a player could in theory have as many as ten knights, ten bishops, ten rooks or nine queens on the board at the same time. While this extreme would almost never occur in practice, in game 11 of their 1927 world championship match, José Raúl Capablanca and Alexander Alekhine each had two queens in play at once (from move 65 through the end on move 66). While some finer sets do include an extra queen of each color, most standard chess sets do not come with additional pieces, so the physical piece used to replace a promoted pawn on the board is usually one that was previously captured. When the correct piece is not available, some substitute is used: a second queen is often indicated by inverting a previously captured rook, or a piece is borrowed from another set.
Strategy.
The "pawn structure", the configuration of pawns on the chessboard, mostly determines the strategic flavor of a game. While other pieces can usually be moved to a more favorable position if they are temporarily badly placed, a poorly positioned pawn is limited in its movement and often cannot be so relocated.
Because pawns capture diagonally and can be blocked from moving straight forward, opposing pawns can become locked in diagonal of two or more pawns of each color, where each player controls squares of one color. In the diagram, Black and White have locked their d- and e-pawns.
Here, White has a long-term space advantage. White will have an easier time than Black in finding good squares for his pieces, particularly with an eye to the kingside. Black, in contrast, suffers from a on c8, which is prevented by the black pawns from finding a good square or helping out on the kingside. On the other hand, White's central pawns are somewhat and vulnerable to attack. Black can undermine the white pawn chain with an immediate c7–c5 and perhaps a later f7–f6.
Isolated pawn.
Pawns on adjacent files can support each other in attack and defense. A pawn which has no friendly pawns in adjacent files is an "isolated pawn". The square in front of an isolated pawn may become an enduring weakness. Any piece placed directly in front not only blocks the advance of that pawn, but cannot be driven away by other pawns.
In the diagram at right, Black has an isolated pawn on d5. If all the pieces except the kings and pawns were removed, the weakness of that pawn might prove fatal to Black in the endgame. In the middlegame, however, Black has slightly more freedom of movement than White, and may be able to trade off the isolated pawn before an endgame ensues.
Passed pawn.
A pawn which cannot be blocked or captured by enemy pawns in its advance to promotion is a "passed pawn". In the diagram at right, White has a "protected" passed pawn on c5 and Black has an "outside" passed pawn on h5. Because endgames are often won by the player who can promote a pawn first, having a passed pawn in an endgame can be decisive – especially a protected passed pawn (a passed pawn that is protected by a pawn). In this vein, a "pawn majority", a greater number of pawns belonging to one player on one side of the chessboard, is strategically important because it can often be converted into a passed pawn.
The diagrammed position might appear roughly equal, because each side has a king and three pawns, and the positions of the kings are about equal. In truth, White wins this endgame on the strength of the protected passed pawn, regardless which player moves first. The black king cannot be on both sides of the board at once – to defend the isolated h-pawn and to stop White's c-pawn from advancing to promotion. Thus White can capture the h-pawn and then win the game .
Doubled pawn.
After a capture with a pawn, a player may end up with two pawns on the same file, called "doubled pawns". Doubled pawns are substantially weaker than pawns which are side by side, because they cannot defend each other, they usually cannot both be defended by adjacent pawns, and the front pawn blocks the advance of the back one. In the diagram at right, Black is playing at a strategic disadvantage due to the doubled c-pawns.
There are situations where doubled pawns confer some advantage, typically when the guarding of consecutive squares in a file by the pawns prevents an invasion by the opponent's pieces.
Pawns which are both doubled and isolated are typically a tangible weakness. A single piece or pawn in front of doubled isolated pawns blocks both of them, and cannot be easily dislodged. It is rare for a player to have three pawns in a file, i.e. "tripled" pawns. Depending on the position, tripled pawns may be more or less valuable than two pawns which are side by side.
Wrong rook pawn.
In chess endgames with a bishop, a rook pawn may be the "wrong rook pawn", depending on the square-color of the bishop. This causes some positions to be draws which would otherwise be wins.
History.
The pawn has its origins in the oldest version of chess, chaturanga, and it is present in all other significant versions of the game as well. In chaturanga, this piece moved directly forward, capturing to the sides (one square diagonally forward to the left or right).
In medieval chess, an attempt was made to make the pieces more interesting, each file's pawn being given the name of a commoner's occupation. On the board, from left to right, those titles were:
The most famous example of this is found in the second book ever printed in the English language, "The Game and Playe of the Chesse". Purportedly, this book was viewed to be as much a political commentary on society as a chess book, and was printed by William Caxton.
The ability to move two spaces, and the related ability to capture "en passant", were only introduced in 15th-century Europe (see En-passant (historical context)). The rule for promotion has changed through history, see History of the promotion rule.
Etymology and word usage.
Although the name origin of most other chess pieces is obvious, the etymology of "pawn" is fairly obscure. It is derived from the Old French word "paon," which comes from the Medieval Latin term for "foot soldier" and is cognate with "peon".
"Pawn" is often taken to mean "one who is easily manipulated" or "one who is sacrificed for a larger purpose". Because the pawn is the weakest piece, it is often used metaphorically to indicate unimportance or outright disposability; for example, "She's only a pawn in their game."
In most other languages, the word for pawn is similarly derived from "paon", its Latin ancestor or some other word for foot soldier. Exceptions include the Irish "fichillín", which means "little chess", and the German "Bauer", meaning "farmer" or "peasant".
Unicode.
Unicode defines two codepoints for pawn:
♙ U+2659 White Chess Pawn (HTML &#9817;)
♟ U+265F Black Chess Pawn (HTML &#9823;)
References.
Notes
References

</doc>
<doc id="47625" url="https://en.wikipedia.org/wiki?curid=47625" title="Yarkovsky effect">
Yarkovsky effect

The Yarkovsky effect is a force acting on a rotating body in space caused by the anisotropic emission of thermal photons, which carry momentum. It is usually considered in relation to meteoroids or small asteroids (about 10 cm to 10 km in diameter), as its influence is most significant for these bodies.
History of discovery.
The effect was discovered by the Russian civil engineer Ivan Osipovich Yarkovsky (1844–1902), who worked on scientific problems in his spare time. Writing in a pamphlet around the year 1900, Yarkovsky noted that the diurnal heating of a rotating object in space would cause it to experience a force that, while tiny, could lead to large long-term effects in the orbits of small bodies, especially meteoroids and small asteroids. Yarkovsky's insight would have been forgotten had it not been for the Estonian astronomer Ernst J. Öpik (1893–1985), who read Yarkovsky's pamphlet sometime around 1909. Decades later, Öpik, recalling the pamphlet from memory, discussed the possible importance of the Yarkovsky effect on movement of meteoroids about the Solar System.
Mechanism.
The Yarkovsky effect is a consequence of the fact that change in the temperature of an object warmed by radiation (and therefore the intensity of thermal radiation from the object) lags behind changes in the incoming radiation. That is, the surface of the object takes time to become warm when first illuminated; and takes time to cool down when illumination stops. In general there are two components to the effect:
In general, the effect is size dependent, and will affect the semi-major axis of smaller asteroids, while leaving large asteroids practically unaffected. For kilometre-sized asteroids, the Yarkovsky effect is minuscule over short periods: the force on asteroid 6489 Golevka has been estimated at about 0.25 newton, for a net acceleration of 10−10 m/s². But it is steady; over millions of years an asteroid's orbit can be perturbed enough to transport it from the asteroid belt to the inner Solar System.
The above details can become more complicated for bodies in strongly eccentric orbits.
Measurement.
The effect was first measured in 1991–2003 on the asteroid 6489 Golevka. The asteroid drifted 15 km from its predicted position over twelve years (the orbit was established with great precision by a series of radar observations in 1991, 1995 and 1999) from the Arecibo radio telescope.
Without direct measurement, it is very hard to predict the exact impact of the Yarkovsky effect on a given asteroid's orbit. This is because the magnitude of the effect depends on many variables that are hard to determine from the limited observational information that is available. These include the exact shape of the asteroid, its orientation, and its albedo. Calculations are further complicated by the effects of shadowing and thermal "reillumination", whether caused by local craters or a possible overall concave shape. The Yarkovsky effect also competes with radiation pressure, whose net effect may cause similar small long-term forces for bodies with albedo variations and/or non-spherical shapes.
As an example, even for the simple case of the pure seasonal Yarkovsky effect on a spherical body in a circular orbit with 90° obliquity, semi-major axis changes could differ by as much as a factor of two between the case of a uniform albedo and the case of a strong north/south albedo asymmetry. Depending on the object's orbit and spin axis, the Yarkovsky change of the semi-major axis may be reversed simply by changing from a spherical to a non-spherical shape.
Despite these difficulties, utilizing the Yarkovsky effect is one scenario under investigation to alter the course of potentially Earth-impacting near-Earth asteroids. Possible asteroid deflection strategies include "painting" the surface of the asteroid or focusing solar radiation onto the asteroid to alter the intensity of the Yarkovsky effect and so alter the orbit of the asteroid away from a collision with Earth. The OSIRIS-REx mission, to be launched in September 2016, will study the Yarkovsky effect on asteroid Bennu.

</doc>
