<doc id="47437" url="https://en.wikipedia.org/wiki?curid=47437" title="Pope Paul III">
Pope Paul III

Pope Paul III (; 29 February 1468 – 10 November 1549), born Alessandro Farnese, was Pope from 13 October 1534 to his death in 1549.
He came to the papal throne in an era following the sack of Rome in 1527 and rife with uncertainties in the Catholic Church following the Protestant Reformation. During his pontificate, and in the spirit of the Counter-Reformation, new Catholic religious orders and societies, such as the Jesuits, the Barnabites, and the Congregation of the Oratory, attracted a popular following.
He convened the Council of Trent in 1545. He was a significant patron of the arts and employed nepotism to advance the power and fortunes of his family. It is to Pope Paul III that Nicolaus Copernicus dedicated "De revolutionibus orbium coelestium" ("On the Revolutions of the Celestial Spheres").
Biography.
Early life and career.
Born in 1468 at Canino, Latium (then part of the Papal States), Alessandro Farnese was the oldest son of Pier Luigi I Farnese, Signore di Montalto (1435–1487) and his wife Giovanna Caetani, a member of the Caetani family which had also produced Pope Boniface VIII. The Farnese family had prospered over the centuries but it was Alessandro’s ascendency to the papacy and his dedication to family interests which brought about the most significant increase in the family’s wealth and power.
Alessandro’s humanist education was at the University of Pisa and the court of Lorenzo de' Medici. Initially trained as an apostolic notary, he joined the Roman Curia in 1491 and in 1493 Pope Alexander VI appointed him Cardinal-Deacon of "Santi Cosma e Damiano". Farnese’s sister, Giulia was reputedly a mistress of Alexander VI and may have been instrumental in securing this appointment for her brother. For this reason, he was sometimes mockingly referred to as the "Borgia brother-in-law," just as Giulia was mocked as "the Bride of Christ." More disparagingly he was referred to as "Cardinal Fregnese" (translated as Cardinal Cunt). As Bishop of Parma, he came under the influence of his vicar general, Bartolomeo Guidiccioni. This led to the future pope breaking off the relationship with his mistress and committing himself to reform in his Parma diocese. Under Pope Clement VII (1523–34) he became Cardinal Bishop of Ostia and dean of the College of Cardinals, and on the death of Clement VII in 1534, was elected as Pope Paul III.
Patron of the arts and family interests.
As a young cleric, Alessandro lived a notably dissolute life, taking for himself a mistress and having three sons and two daughters with her. By Silvia Ruffini, he fathered Pier Luigi Farnese, whom he created Duke of Parma; others included Ranuccio Farnese and Costanza Farnese. The elevation to the cardinalate of his grandsons, Alessandro Farnese, aged fourteen, and Guido Ascanio Sforza, aged sixteen, displeased the reform party and drew a protest from the emperor, but this was forgiven, when shortly after, he introduced into the Sacred College men of the calibre of Reginald Pole, Gasparo Contarini, Jacopo Sadoleto, and Giovanni Pietro Caraffa, who became Pope Paul IV.
One of the most significant artistic works of Paul's reign was the depiction of the Last Judgement by Michelangelo in the Sistine Chapel of the Vatican Palace. Although the work was commissioned by Paul III’s predecessor, it was finished in 1541.
As a cardinal, Alessandro had begun construction of a palace, the Palazzo Farnese, in central Rome. On his election to the papacy, the size and magnificence of this building programme was increased to reflect his change in status. The palace was initially designed by the architect Antonio da Sangallo the Younger, received further architectural refinement from Michelangelo, and was completed by Giacomo della Porta. Like other Farnese family buildings, the palace imposes its presence on its surroundings in an expression of the family’s power and wealth. Alessandro's Villa Farnese at Caprarola has a similar presence.
In 1546, after the death of Sangallo, Paul appointed the elderly Michelangelo to take over the supervision of the building of St. Peter's Basilica. Michelangelo was also commissioned by Paul to paint the 'Crucifixion of St. Peter' and the 'Conversion of St. Paul' (1542–50), Michelangelo's last frescoes, in the Pauline Chapel of the Vatican.
Paul III's artistic and architectural commissions were numerous and varied. The Venetian artist Titian painted a portrait of the Pope in 1543, and in 1546, the well-known portrait of Paul III with his grandsons Cardinal Alessandro Farnese and Ottavio Farnese, Duke of Parma. Both are now in the Capodimonte Museum, Naples. The military fortifications in Rome and the Papal States were strengthened during his reign. He had Michelangelo relocate the ancient bronze of the Emperor Marcus Aurelius to the Capitoline Hill, where it became the centerpiece to the Piazza del Campidoglio.
Paul III’s bronze tomb, executed by Guglielmo della Porta, is in St. Peter's.
Politics and religion during the papacy of Paul III.
The fourth pope during the period of the Protestant Reformation, Paul III became the first to take active reform measures in response to Protestantism. Soon after his elevation, 2 June 1536, Paul III summoned a general council to meet at Mantua in the following May; but the opposition of the Protestant princes and the refusal of the Duke of Mantua to assume the responsibility of maintaining order frustrated the project. Paul III first deferred for a year and then discarded the whole project.
In 1536, Paul III invited nine eminent prelates, distinguished by learning and piety alike, to act in committee and to report on the reformation and rebuilding of the Church. In 1537 they turned in their celebrated Consilium de emendenda ecclesia, exposing gross abuses in the Curia, in the church administration and public worship; and proffering many a bold and earnest word on behalf of abolishing such abuses. This report was printed not only at Rome, but at Strasburg and elsewhere.
But to the Protestants it seemed far from thorough; Martin Luther had his edition (1538) prefaced with a vignette showing the cardinals cleaning the Augean stable of the Roman Church with foxtails instead of brooms. Yet the Pope was in earnest when he took up the problem of reform. He clearly perceived that the emperor, Charles V would not rest until the problems were grappled in earnest, and a council was an unequivocal procedure that should leave no room for doubt of his own readiness to make changes. Yet it is clear that the "Concilium" bore no fruit in the actual situation, and that in Rome no results followed from the committee's recommendations.
On the other hand, serious political complications resulted. In order to vest his grandson Ottavio Farnese with the dukedom of Camerino, Paul forcibly wrested the same from the duke of Urbino (1540). He also incurred virtual war with his own subjects and vassals by the imposition of burdensome taxes. Perugia, renouncing its obedience, was besieged by Paul's son, Pier Luigi, and forfeited its freedom entirely on its surrender. The burghers of Colonna were duly vanquished, and Ascanio was banished (1541). After this the time seemed ripe for annihilating heresy.
In 1540, the Church officially recognized the young society forming about Ignatius of Loyola, (founder of the Society of Jesus).
The second visible stage in the process becomes marked by the institution, or reorganization, in 1542, of the Congregation of the Holy Office of the Inquisition (see Inquisition).
On another side, the Emperor was insisting that Rome should forward his designs toward a peaceable recovery of the German Protestants. Accordingly, the Pope despatched Giovanni Morone (not yet a cardinal) as nuncio to Hagenau and Worms, in 1540; while, in 1541, Cardinal Gasparo Contarini took part in the adjustment proceedings at the Conference of Regensburg. It was Contarini who led to the stating of a definition in connection with the article of justification in which occurs the famous formula "by faith alone are we justified," with which was combined, however, the Roman Catholic doctrine of good works. At Rome, this definition was rejected in the consistory of 27 May, and Luther declared that he could accept it only provided the opposers would admit that hitherto they had taught differently from what was meant in the present instance.
Yet, even now, and particularly after the Regensburg Conference had proved in vain, the Emperor did not cease to insist on convening the council, the final result of his insistence being the Council of Trent, which, after several postponements, was finally convoked by the bull "Laetare Hierusalem", 15 March 1545.
Meanwhile, after the peace of Crespy (September 1544), the situation had so shaped itself that Emperor Charles V (1519–56) began to put down Protestantism by force. Pending the diet of 1545 in Worms, the emperor concluded a covenant of joint action with the papal legate, Cardinal Alessandro Farnese. Paul III was to aid in the projected war against the German Evangelical princes and estates. The prompt acquiescence of Paul III in the war project was probably grounded on personal motives. The moment now seemed opportune for him, since the Emperor was sufficiently preoccupied in the German realm, to acquire for his son Pier Luigi the duchies of Parma and Piacenza. Although these belonged to the Papal States, Paul III thought to overcome the reluctance of the Cardinals by exchanging the duchies for the less valuable domains of Camerino and Nepi. The Emperor agreed, because of his prospective compensation to the extent of 12,000 infantry, 500 mounted troops, and considerable sums of money.
In Germany the campaign began in the west, where Protestant movements had been at work in the archbishopric of Cologne since 1542. The Reformation was not a complete success there, because the city council and the majority of the chapter opposed it; whereas on 16 April 1546, Hermann of Wied was excommunicated, his rank forfeited, and, in February 1547, was compelled by the Emperor to abdicate.
In the meantime open warfare had begun against the Evangelical princes, estates, and cities allied in the Schmalkaldic League (see Philip of Hesse). By the close of 1546, Charles V succeeded in subjugating South Germany, while the victory at the Battle of Mühlberg, on 24 April 1547, established his imperial sovereignty everywhere in Germany and delivered into his hands the two leaders of the league.
But while north of the Alps, in virtue of his preparations for the Augsburg Interim and its enforcement, the Emperor was widely instrumental in recovering Germany to Roman Catholicism, the Pope now held aloof from him because Charles V himself had stood aloof in the matter of endowing Pier Luigi with Parma and Piacenza, and the situation came to a total rupture when the imperial vice-regent, Ferrante Gonzaga, proceeded forcibly to expel Pier Luigi.
The Pope's son was assassinated, 1547, at Piacenza, and Paul III believed that this had not come to pass without the emperor's foreknowledge. In the same year, however, and after the death of Francis I of France (1515–47), with whom the Pope had once again sought an alliance, the stress of circumstances compelled him to do the Emperor's will and accept the ecclesiastical measures adopted during the Interim.
With reference to the assassinated prince's inheritance, the restitution of which Paul III demanded ostensibly in the name and for the sake of the Church, the Pope's design was thwarted by the Emperor, who refused to surrender Piacenza, and by Pier Luigi's heir in Parma, Ottavio Farnese.
In consequence of a violent altercation on this account with Cardinal Farnese, Paul III, at the age of eighty-one years, became so overwrought that an attack of sickness ensued from which he died, 10 November 1549.
Paul III proved unable to suppress the Protestant Reformation, although it was during his pontificate that the foundation was laid for the Counter-Reformation. He decreed the second and final excommunication of King Henry VIII of England in December 1538.
Pope Paul III and slavery.
In May–June 1537 Paul issued three documents: the bulls "Sublimus Dei" (also known as "Unigenitus" and "Veritas ipsa"); "Altituda divini consolii"; and "Pastorale officium", the brief for the execution of "Sublimus Dei".
"Altituda divini consolii" was essentially a bull to settle a difference between the Franciscans and Dominicans over baptism, but "Sublimus Dei" is described by Prein (2008) as the "Magna Carta" for the human rights of the indigenous peoples of the Americas in its declaration that "the Indians were human beings and they were not to be robbed of their freedom or possessions".
"Pastorale officium" declared automatic excommunication for anyone who failed to abide by the new ruling. Stogre (1992) notes that "Sublimus Dei" is not present in Denzinger, the authoritative compendium of official teachings of the Catholic Church, and that the executing brief for it ("Pastorale officium") was annulled the following year in "Non Indecens Videtur". Davis (1988) asserts it was annulled due to a dispute with the Spanish crown. The Council of The West Indies and the Crown concluded that the documents broke their patronato rights and the Pope withdrew them, though they continued to circulate and be quoted by Las Casas and others who supported Indian rights.
According to Falkowski (2002) "Sublimus Dei" had the effect of revoking the bull of Alexander VI "Inter Caetera" but still leaving the colonizers the duty of converting the native people. Prein (2008) observes the difficulty in reconciling these decrees with "Inter Caetera".
Father Gustavo Gutierrez describes "Sublimus Dei" as "the most important papal document relating to the condition of native Indians and that it was addressed to all Christians". Maxwell (1975) notes that the bull did not change the traditional teaching that the enslavement of Indians was permissible if they were considered "enemies of Christendom" as this would be considered by the Church as a "just war". He further argues that the Indian nations had every right to self-defense. Stark (2003) describes the bull as "magnificent" and believes the reason that, in his opinion, it has belatedly come to light is due to the neglect of Protestant historians. Falola notes that the bull related to the native populations of the New World and did not condemn the transatlantic slave trade stimulated by the Spanish monarchy and the Holy Roman Emperor.
In 1537, he also issued "In nomine Sancte", a bull in which he talks about evangelism and conversion tasks.
In 1545 Paul repealed an ancient law that allowed slaves to claim their freedom under the Emperor's statue on Capital Hill, in view of the number of homeless people and tramps in the city of Rome. The decree included those who had become Christians after their enslavement and those born to Christian slaves. The right of inhabitants of Rome to publicly buy and sell slaves of both sexes was affirmed. Stogre (1992) asserts that the lifting of restrictions was due to a shortage of slaves in Rome. In 1548 Paul authorized the purchase and possession of Muslim slaves in the Papal states.
Fictional portrayals.
The character of Pope Paul III, played by Peter O'Toole in the Showtime series "The Tudors", is loosely inspired by him.
The young Alessandro Farnese is played by Diarmuid Noyes in the StudioCanal serial "Borgia", and Cyron Melville in Showtime's "The Borgias".

</doc>
<doc id="47442" url="https://en.wikipedia.org/wiki?curid=47442" title="Roc">
Roc

Roc, ROC or R.O.C. may refer to following:
As an acronym or initialism.
ROC or R.O.C. may refer to:

</doc>
<doc id="47443" url="https://en.wikipedia.org/wiki?curid=47443" title="Vera Wang">
Vera Wang

Vera Ellen Wang (, ; born June 27, 1949) is an American fashion designer based in New York City.
Early life and education.
Vera Ellen Wang was born and raised in New York City, and is of Chinese descent. Her parents were born in China, and came to the United States in the mid-1940s. Her mother, Florence Wu, worked as a translator for the United Nations, while her father, Cheng Ching Wang, owned a medicine company. Wang has one younger brother, Kenneth.
Wang graduated from The Chapin School in 1967, attended the University of Paris and earned a degree in art history from Sarah Lawrence College. Wang began figure skating at the age of eight. While in high school, she trained with pairs partner James Stuart, and competed at the 1968 U.S. Figure Skating Championships. She was featured in "Sports Illustrated"'s Faces in the Crowd in the January 9, 1968 issue. When she failed to make the US Olympics team, she entered the fashion industry. Wang continues to enjoy skating, saying, "Skating is multidimensional".
Career.
Beginning in 1970, Wang was a senior fashion editor for "Vogue" but left "Vogue" after being turned down for the editor-in-chief position currently filled by Anna Wintour and joined Ralph Lauren as a design director.
Wang has made wedding gowns for many well-known public figures, such as Chelsea Clinton, Karenna Gore, Ivanka Trump, Campbell Brown, Alicia Keys, Mariah Carey, Victoria Beckham, Avril Lavigne, Jennifer Lopez, Jennifer Garner, Sharon Stone, Sarah Michelle Gellar, Hilary Duff, Uma Thurman, Holly Hunter, Kate Hudson, Khloe Kardashian and Kim Kardashian. Wang's evening wear has also been worn by Michelle Obama.
She has designed costumes for figure skaters, including Nancy Kerrigan, Michelle Kwan and Evan Lysacek. Silver medalist Nancy Kerrigan wore a unique design of Vera's for the 1994 Olympics. She designed the two-piece uniforms currently worn by the Philadelphia Eagles Cheerleaders.
On October 23, 2001, her book, "Vera Wang on Weddings", was released. In June 2005, she won the CFDA (Council of Fashion Designers of America) Womenswear Designer of the Year. On May 27, 2006, Wang was awarded the André Leon Talley Lifetime Achievement Award from the Savannah College of Art and Design. Wang was inducted into the U.S. Figure Skating Hall of Fame in 2009, and was honored for her contribution to the sport as a costume designer.
Twenty years after opening her first bridal boutique, Wang was awarded the Leadership in the Arts Award by the Harvard-Radcliffe Asian American Association. She accepted the award on April 17, 2010 at "Identities", the Harvard association's annual charity fashion show.
Wang's evening wear has been worn by stars at many red carpet events, including Viola Davis at the 2012 Academy Awards, Sandra Bullock at the Oscars in 2011 and Sofia Vergara at the 65th Emmy Awards.
She was honored with the Geoffrey Beene Lifetime Achievement Award in 2013.
Retail.
In 1990, she opened her own design salon in the Carlyle Hotel in New York that features her trademark bridal gowns. She has since opened bridal boutiques in New York, London, Tokyo and Sydney. Wang has also expanded her brand name through her fragrance, jewellery, eyewear, shoe and homeware collections.
'White by Vera Wang' launched on February 11, 2011 at David's Bridal. Prices of the bridal gowns range from $600–$1,400 which gives more brides a more affordable way to wear Vera's designs. In 2002, Wang began to enter the home fashion industry and launched The Vera Wang China and Crystal Collection, followed by the 2007 release of her diffusion line called Simply Vera, which are sold exclusively by Kohl's.
In Spring of 2012, Wang teamed up with Men's Wearhouse to offer two tuxedo styles available in both the retail and rental areas of their inventory. 
In June 2012, she expanded in Australia with the opening of "Vera Wang Bride Sydney" and her first Asian flagship store 'Vera Wang Bridal Korea', helmed by President Jung Mi-ri, in upmarket neighbourhood Cheongdam-dong in Gangnam-gu, Seoul.
Personal life.
In 1989, she married Arthur Becker in an interfaith Baptist and Jewish ceremony. They resided in Manhattan with their two adopted daughters: Cecilia (born 1990), who currently resides in New York City, and Josephine (born 1993), who attended The Chapin School and currently attends Harvard University. Becker was the CEO of an information technology services company called NaviSite until August 2010. In July 2012, Vera Wang Co. announced that the couple have separated. The separation was amicable.
In popular culture.
Several movies and television shows have mentioned Wang's works.
In the "Sex and the City" TV series, Charlotte York found Wang's wedding dress to be the perfect wedding dress, and wore it for her wedding to Trey MacDougal. Wang's design was mentioned in the NBC television show "The West Wing" in the episode "The Black Vera Wang". In ABC's "Ugly Betty" TV series, Vera Wang makes a cameo as herself, designing a dress for Wilhelmina Slater's wedding to Bradford Meade.
In the film "Sex and the City", Vera Wang was among the bridal gowns Carrie Bradshaw wore in her "Vogue" photo shoot. In the film "Bride Wars", Anne Hathaway and Kate Hudson wore custom-made Vera Wang gowns.
In one of Totally Spies! episodes, "The Wedding Crasher", a villain named Wera Van (parody of Vera Wang), desires revenge on those who rejected her wedding dress designs.
Vera Wang also designed a wedding dress for Sarah Michelle Gellar's character Buffy Summers in the TV Series "Buffy The Vampire Slayer" episode "The Prom".
In the series, "How to Get Away with Murder", the character Michaela Pratt mentioned a custom Vera Wang wedding gown when confronting her fiancé, Aiden Walker, about his sexuality.
In the TV Series, "Gossip Girl," Vera Wang is mentioned in multiple episodes and is one of Blair Warldorf's favourite designers. Blair has a wedding dress designed for her wedding to Prince Louis of Monaco by Vera Wang. Blair decides she can no longer wear it after losing her and Louis' baby in a car accident, and has another designed instead. The three bridesmaids in Blair's wedding wear Vera Wang bridemaids short dress.
Also in Revenge (TV series), Victoria Grayson played by Madeleine Stowe wears a steel gray Vera Wang mermaid gown in her second wedding with Conrad Grayson (Henry Czerny). The dress was actually shown in black at Wang's Fall 2012 show.

</doc>
<doc id="47444" url="https://en.wikipedia.org/wiki?curid=47444" title="Copycat suicide">
Copycat suicide

A copycat suicide is defined as an emulation of another suicide that the person attempting suicide knows about either from local knowledge or due to accounts or depictions of the original suicide on television and in other media.
A spike of emulation suicides after a widely publicized suicide is known as the Werther effect, following Goethe's novel "The Sorrows of Young Werther".
The well-known suicide serves as a model, in the absence of protective factors, for the next suicide. This is referred to as suicide contagion. They occasionally spread through a school system, through a community, or in terms of a celebrity suicide wave, nationally. This is called a suicide cluster. Suicide clusters are caused by the social learning of suicide related behaviors, or "copycat suicides". Point clusters are clusters of suicides in both time and space, and have been linked to direct social learning from nearby individuals. Mass clusters are clusters of suicides in time but not space, and have been linked to the broadcasting of information concerning celebrity suicides via the mass media Examples of celebrities whose suicides have inspired suicide clusters include Ruan Lingyu, the Japanese musicians Yukiko Okada, Miyu Uehara and hide, and Marilyn Monroe, whose death was followed by an increase of 200 more suicides than average for that August month.
Another famous case is the self-immolation of Mohamed Bouazizi, a Tunisian street vendor who set himself on fire on December 17, 2010, an act that was a catalyst for the Tunisian Revolution and sparked the Arab Spring, including several men who emulated Bouazizi's act.
To prevent this type of suicide, it is customary in some countries for the media to discourage suicide reports except in special cases.
History.
One of the earliest known associations between the media and suicide arose from Goethe's novel "Die Leiden des jungen Werthers" ("The Sorrows of Young Werther"). Soon after its publication in 1774, young men began to mimic the main character by dressing in yellow pants and blue jackets. In the novel, Werther shoots himself with a pistol after he is rejected by the woman he loves, and shortly after its publication there were many reports of young men using the same method to kill themselves in an act of hopelessness.
This resulted in the book being banned in several places. Hence the term "Werther effect", used in the technical literature to designate copycat suicides. The term was coined by researcher David Phillips in 1974.
Two centuries after Goethe's novel was published, David Phillips confirmed imitative suicides as the "Werther effect." Reports in 1985 and 1989 by Phillips and his colleagues found that suicides and other accidents seem to rise after a well publicized suicide. Copycat suicide is mostly blamed on the media. "Hearing about a suicide seems to make those who are vulnerable feel they have permission to do it," Phillips said. He cited studies that showed that people were more likely to engage in dangerous deviant behavior, such as drug taking, if someone else had set the example first.
Factors in suicide reporting.
The Werther effect not only predicts an increase in suicide, but the majority of the suicides will take place in the same or a similar way as the one publicized. The more similar the person in the publicized suicide is to the people exposed to the information about it, the more likely the age group or demographic is to die by suicide. The increase generally happens only in areas where the suicide story was highly publicized. Upon learning of someone else's suicide, many people decide that action is appropriate for them as well, especially if the publicized suicide was of someone in a similar situation as them.
Publishing the means of suicides, romanticized and sensationalized reporting, particularly about celebrities, suggestions that there is an epidemic, glorifying the deceased and simplifying the reasons all lead to increases in the suicide rate. People may see suicide as a glamorous ending—with youth getting a lot of attention, lots of sympathy, lots of national concern that they never got in life. The second possible factor is that vulnerable youth may feel like, "If they couldn't cut it, neither can I". Increased rate of suicides has been shown to occur up to ten days after a television report. Studies in Japan and Germany have replicated findings of an imitative effect. Etzersdorfer et al. in an Austrian study showed a strong correlation between the number of papers distributed in various areas and the number of subsequent firearm suicides in each area after a related media report. Higher rates of copycat suicides have been found in those with similarities in race, age, and gender to the victim in the original report.
Stack analyzed the results from 42 studies and found that those measuring the effect of a celebrity suicide story were 14.3 times more likely to find a copycat effect than studies that did not. Studies based on a real as opposed to fictional story were 4.03 times more likely to uncover a copycat effect and research based on televised stories was 82% less likely to report a copycat effect than research based on newspapers. Other scholars have been less certain about whether copycat suicides truly happen or are selectively hyped. For instance, fears of a suicide wave following the death of Kurt Cobain never materialized in an actual increase in suicides. Furthermore, there is evidence for an indirect Werther effect, i.e. the perception that suicidal media content influences others which, in turn, can concurrently or additionally influence one person's own future thoughts and behaviors. Similarly the researcher Gerard Sullivan has critiqued research on copycat suicides, suggesting that data analyses have been selective and misleading, and that the evidence for copycat suicides are much less consistent than suggested by some researchers.
Many people interviewed after the suicide of a relative or friend have a tendency to simplify the issues; their grief can lead to their minimizing or ignoring significant factors. Studies show a high incidence of psychiatric disorders in suicide victims at the time of their death with the total figure ranging from 98% to 87.3% with mood disorders and substance abuse being the two most common. These are often undiagnosed or untreated and treatment can result in reductions in the suicide rate. Reports that minimize the effect of psychiatric disorders contribute to copycat suicides whereas reports that mention this factor and provide help-line contact numbers and advice for where sufferers may gain assistance can reduce suicides.
Social proof model.
An alternate model to explain copycat suicide, called "social proof" by Cialdini, goes beyond the theories of glorification and simplification of reasons to look at why copycat suicides are so similar, demographically and in actual methods, to the original publicized suicide. In the social proof model, people imitate those who seem similar, despite or even because of societal disapproval. This model is important because it has nearly opposite ramifications for what the media ought to do about the copycat suicide effect than the standard model does. To deal with this problem, Alex Mesoudi of Queen Mary University, London, developed a computer model of a community of 1000 people, to examine how copycat suicides occur.
These were divided into 100 groups of 10, in a model designed to represent different levels of social organization, such as schools or hospitals within a town or state.
Mesoudi then circulated the simulation through 100 generations. He found the simulated people acted just as sociologists' theory predicted. They were more likely to die by suicide in clusters, either because they had learned this trait from their friends, or because suicidal people are more likely to be like one another.
Journalism codes.
Various countries have national journalism codes which range from one extreme of, "Suicide and attempted suicide should in general never be given any mention" (Norway) to a more moderate, "In cases of suicide, publishing or broadcasting information in an exaggerated way that goes beyond normal dimensions of reporting with the purpose of influencing readers or spectators should not occur." The study's author, University of London psychologist Alex Mesoudi, recommends that reporters follow the sort of guidelines the World Health Organization and others endorse for coverage of any suicide: Use extreme restraint in covering these deaths—keep the word "suicide" out of the headline, don't romanticize the death, and limit the number of stories. Photography, pictures, visual images or film depicting such cases should not be made public" (Turkey). While many countries do not have national codes, media outlets still often have in-house guidelines along similar lines. In the United States there are no industrywide standards and a survey of inhouse guides of 16 US daily newspapers showed that only three mentioned the word "suicide" and none gave guidelines about publishing the method of suicide. Craig Branson, online director of the American Society of News Editors (ASNE), has been quoted as saying, "Industry codes are very generic and totally voluntary. Most ethical decisions are left to individual editors at individual papers. The industry would fight any attempt to create more specific rules or standards, and editors would no doubt ignore them." Guidelines on the reporting of suicides in Ireland were introduced recently which attempt to remove any positive connotations the act might have (e.g. using the term "completed" rather than "successful" when describing a suicide attempt which resulted in a death).
The Canadian Broadcasting Corporation's journalistic standards and practices manual discourages the reporting of the details of suicide.
Journalist training.
Australia is one of the few countries where there is a concerted effort to teach journalism students about this subject. The Mindframe national media initiative followed an ambivalent response by the Australian Press Council to an earlier media resource kit issued by Suicide Prevention Australia and the Australian Institute for Suicide Research and Prevention. The UK-based media ethics charity MediaWise provides training for journalists on reporting suicide and related issues.
Headline is Ireland's media monitoring programme for suicide and mental health issues, set up by Shine and the Health Service Executives National Office for Suicide Prevention as part of 'Reach Out: National Strategy for action on Suicide Prevention.' Headline works with media professionals and students to find ways to collaborate to ensure that suicide, mental health and mental illness are responsibly covered in the media and provides information on reporting on mental health and suicidal behavior, literature and daily analysis of news stories. Headline also serves as a vehicle for the public to become involved in helping to monitor the Irish media on issues relating to mental health and suicide.

</doc>
<doc id="47449" url="https://en.wikipedia.org/wiki?curid=47449" title="Chessboard">
Chessboard

A chessboard is the type of checkerboard used in the classic board game chess, and consists of 64 squares (eight rows and eight columns) and 32 pieces.The squares are arranged in two alternating colors (light and dark). Wooden boards may use naturally light and dark brown woods, while plastic and vinyl boards often use brown or green for the dark squares and shades such as buff or cream for the light squares. Materials vary widely; while wooden boards are generally used in high-level games; vinyl, plastic, and cardboard are common for low-level and informal play. Decorative glass and marble boards are available but rarely accepted for games rated by national or international chess federations. Each square on the board has a name from a1 to h8.
In "modern commentary", the columns (called "files") are labeled by the letters a to h from left to right from the white player's point of view, and the rows (called "ranks") by the numbers 1 to 8, with 1 being closest to the white player, thus providing a standard notation called algebraic chess notation.
In older "English commentary", the files are labeled by the piece originally occupying its first rank (e.g. queen, king's rook, queen's bishop), and ranks by the numbers 1 to 8 from each player's point of view, depending on the move being described. This is called descriptive chess notation and is no longer commonly used.

</doc>
<doc id="47454" url="https://en.wikipedia.org/wiki?curid=47454" title="Stratosphere">
Stratosphere

The stratosphere () is the second major layer of Earth's atmosphere, just above the troposphere, and below the mesosphere. It is stratified in temperature, with warmer layers higher up and cooler layers farther down. This is in contrast to the troposphere near the Earth's surface, which is cooler higher up and warmer farther down. The border of the troposphere and stratosphere, the tropopause, is marked by where this inversion begins, which in terms of atmospheric thermodynamics is the equilibrium level. At moderate latitudes the stratosphere is situated between about and altitude above the surface, while at the poles it starts at about altitude, and near the equator it may start at altitudes as high as .
Ozone and temperature.
Within this layer, temperature increases as altitude increases "(see temperature inversion)"; the top of the stratosphere has a temperature of about 270 K (−3°C or 26.6°F), just slightly below the freezing point of water. The stratosphere is layered in temperature because ozone (O3) here absorbs high energy ultraviolet (UVB and UVC) radiation from the Sun and is broken down into the allotropes of atomic oxygen (O1) and common molecular oxygen (O2). The mid stratosphere has less UV light passing through it; O and O2 are able to combine, and this is where the majority of natural ozone is produced. It is when these two forms of oxygen recombine to form ozone that they release the heat found in the stratosphere. The lower stratosphere receives very low amounts of UVC; thus atomic oxygen is not found here and ozone is not formed (with heat as the byproduct). This vertical stratification, with warmer layers above and cooler layers below, makes the stratosphere dynamically stable: there is no regular convection and associated turbulence in this part of the atmosphere. The top of the stratosphere is called the stratopause, above which the temperature decreases with height.
Methane (CH4), while not a direct cause of ozone destruction in the stratosphere, does lead to the formation of compounds that destroy ozone. Monatomic oxygen (O) in the upper stratosphere reacts with methane (CH4) to form a hydroxyl radical (OH·). This hydroxyl radical is then able to interact with non-soluble compounds like chlorofluorocarbons, and UV light breaks off chlorine radicals (Cl·). These chlorine radicals break off an oxygen atom from the ozone molecule, creating an oxygen molecule (O2) and a hypochloryl radical (ClO·). The hypochloryl radical then reacts with an atomic oxygen creating another oxygen molecule and another chlorine radical, thereby preventing the reaction of monatomic oxygen with O2 to create natural ozone.
Aircraft flight.
Commercial airliners typically cruise at altitudes of in temperate latitudes (in the lower reaches of the stratosphere). This optimizes fuel burn, mostly due to the low temperatures encountered near the tropopause and low air density, reducing parasitic drag on the airframe. (Stated another way, it allows the airliner to fly faster for the same amount of drag.) It also allows them to stay above hard weather (extreme turbulence).
Concorde would cruise at mach 2 at about , and the SR-71 would cruise at mach 3 at , all still in the stratosphere.
Because the temperature in the tropopause and lower stratosphere remains constant (or slightly decreases) with increasing altitude, very little convective turbulence occurs at these altitudes. Though most turbulence at this altitude is caused by variations in the jet stream and other local wind shears, areas of significant convective activity (thunderstorms) in the troposphere below may produce convective overshoot.
Although a few gliders have achieved great altitudes in the powerful thermals in thunderstorms, this is dangerous. Most high altitude flights by gliders use lee waves from mountain ranges and were used to set the current record of .
On October 24, 2014, Alan Eustace became the record holder for reaching the altitude record for a manned balloon at . Mr Eustace also broke the world records for vertical speed reached with a peak velocity of 1,321 km/h (822 mph) and total freefall distance of - lasting four minutes and 27 seconds.
Circulation and mixing.
The stratosphere is a region of intense interactions among radiative, dynamical, and chemical processes, in which the horizontal mixing of gaseous components proceeds much more rapidly than in vertical mixing.
An interesting feature of stratospheric circulation is the quasi-biennial oscillation (QBO) in the tropical latitudes, which is driven by gravity waves that are convectively generated in the troposphere. The QBO induces a secondary circulation that is important for the global stratospheric transport of tracers, such as ozone or water vapor.
In northern hemispheric winter, sudden stratospheric warmings, caused by the absorption of Rossby waves in the stratosphere, can be observed in approximately half of winters when easterly winds develop in the stratosphere. These events often precede unusual winter weather and may even be responsible for the cold European winters of the 1960s.
Life.
Bacteria.
Bacterial life survives in the stratosphere, making it a part of the biosphere. In 2001 an Indian experiment, involving a high-altitude balloon, was carried out at a height of 41 kilometres and a sample of dust was collected with bacterial material inside.
Birds.
Also, some bird species have been reported to fly at the lower levels of the stratosphere. On November 29, 1973, a Rüppell's vulture was ingested into a jet engine above the Ivory Coast, and bar-headed geese reportedly overfly Mount Everest's summit, which is .
Discovery.
Léon Teisserenc de Bort from France and Richard Assmann from Germany, in separate publications and following years of observations, announced the discovery of an isothermal layer at around 11–14 km, which is the base of the lower stratosphere. This was based on temperature profiles from unmanned instrumented balloons.

</doc>
<doc id="47456" url="https://en.wikipedia.org/wiki?curid=47456" title="Pater Noster (disambiguation)">
Pater Noster (disambiguation)

Pater Noster or "Our Father" is probably the best-known prayer in Christianity.
Pater Noster or Paternoster may also refer to:

</doc>
<doc id="47459" url="https://en.wikipedia.org/wiki?curid=47459" title="James T. Kirk">
James T. Kirk

James Tiberius "Jim" Kirk is a fictional character in the "Star Trek" media franchise, appearing in numerous television episodes, films, books, comics, and video games. As the captain of the starship USS "Enterprise", Kirk leads his crew as they explore "where no man has gone before". Often, the characters of Spock and Leonard McCoy act as his logical and emotional sounding boards, respectively.
Kirk, played by William Shatner, first appears in the broadcast pilot episode of ', "The Man Trap", originally broadcast on September 8, 1966. Shatner continued in the role for the show's three seasons, and later provided the voice of the animated version of Kirk in ' (1973–74). Shatner returned to the role for "" (1979) and in six subsequent films. Chris Pine portrays a young version of the character in the 2009 reboot "Star Trek" film, with Jimmy Bennett playing Kirk as a child. Pine reprised his role in 2013 "Star Trek Into Darkness" and in 2016 "Star Trek Beyond". Other actors have played the character in fan-created media, and the character has been the subject of multiple spoofs and satires. The character has been praised for his leadership traits, and also criticized for his relationships with women.
Depiction.
James Tiberius Kirk was born on March 22, 2233, in Riverside, Iowa. He was raised there by his parents, George and Winona Kirk. Although born on Earth, Kirk lived for a time on , where he was one of nine surviving witnesses to the massacre of 4,000 colonists by . James Kirk's brother, George Samuel Kirk, is first mentioned in "What Are Little Girls Made Of?" and introduced and killed in "", leaving behind three children.
Kirk became the only student at Starfleet Academy to defeat the "Kobayashi Maru" test, garnering a commendation for original thinking for reprogramming the computer to make the "no-win scenario" winnable. Kirk was granted a field commission as an ensign and posted to advanced training aboard the USS "Republic". He was then promoted to lieutenant junior grade and returned to Starfleet Academy as a student instructor. Students could either "think or sink" in his class, and Kirk himself was "a stack of books with legs". Upon graduating in the top five percent, Kirk was promoted to lieutenant and served aboard the USS "Farragut". While assigned to the "Farragut", Kirk commanded his first planetary survey and survived a deadly attack that killed a large portion of the "Farragut"s crew, including his commanding officer, Captain Garrovick. He received his first command, a spaceship roughly equivalent to a destroyer, while still quite young.
Kirk became Starfleet's youngest captain when he received command of the for a five-year mission, three years of which are depicted in the original "Star Trek" series. Kirk's most significant relationships in the television series are with first officer Spock and chief medical officer Dr. Leonard "Bones" McCoy. McCoy is someone to whom Kirk unburdens himself and is a foil to Spock. Robert Jewett and John Shelton Lawrence's "The Myth of the American Superhero" describes Kirk as "a hard-driving leader who pushes himself and his crew beyond human limits". Terry J. Erdman and Paula M. Block, in their "Star Trek 101" primer, note that while "cunning, courageous and confident", Kirk also has a "tendency to ignore Starfleet regulations when he feels the end justifies the means"; he is "the quintessential officer, a man among men and a hero for the ages". Although Kirk throughout the series becomes romantically involved with various women, when confronted with a choice between a woman and the "Enterprise", "his ship always won". Roddenberry wrote in a production memo that Kirk is not afraid of being fallible, but rather is afraid of the consequences to his ship and crew should he make an error in judgment. Roddenberry wrote:
J. M. Dillard's novel "The Lost Years" describes Kirk's promotion to rear admiral and unfulfilling duties as a diplomatic troubleshooter after the "Enterprise"s five-year mission. In ', Kirk is chief of Starfleet operations, and he takes command of the "Enterprise" from Captain Willard Decker. "Star Trek" creator Gene Roddenberry's novelization of "The Motion Picture" depicts Kirk married to a Starfleet officer killed during a transporter accident. At the beginning of ', Kirk takes command of the "Enterprise" from Captain Spock to pursue his enemy from "Space Seed", Khan Noonien Singh. The movie introduces Kirk's son, David Marcus. Spock, who notes that "commanding a starship is [Kirk's] first, best destiny", dies at the end of "Star Trek II". In ', Kirk leads his surviving officers in a successful mission to rescue Spock from a planet on which he is reborn. Although Kirk is demoted to captain in ' for disobeying Starfleet orders, he also receives command of a new USS "Enterprise". The ship is ordered decommissioned at the end of "".
In "Star Trek Generations", Captain Picard finds Kirk alive in the timeless Nexus, despite the fact that history recorded his death during the "Enterprise"-B's maiden voyage, Kirk having fallen into the Nexus in the incident that caused his "death". Picard convinces Kirk to return to Picard's present to help stop the villain Soran from destroying Veridian III's sun. Although Kirk initially refuses the offer, he agrees when he realises that the Nexus cannot give him the one thing he has always sought: the ability to make a difference. The two leave the Nexus and stop Soran. However, Kirk is mortally wounded; as he dies, Picard assures him that he helped to "make a difference". Picard buries Kirk on the planet. Shatner and Judith and Garfield Reeves-Stevens wrote a series of novels that depict Kirk's resurrection by the Borg and his ongoing adventures after the events of "Generations".
Reboot films.
The 2009 film "Star Trek" introduces an alternative timeline that reveals different origins for Kirk, the formation of his association with Spock, and how they came to serve together on the "Enterprise". The point of divergence between "The Original Series" and the film occurs on the day of Kirk's birth in 2233. Although the film treats specific details from Star Trek as mutable, characterizations are meant to "remain the same". In the film, George and Winona Kirk name their son "James Tiberius" after his maternal and paternal grandfathers, respectively. He is born on a shuttle escaping the starship USS "Kelvin", on which his father is killed. The character begins as "a reckless, bar-fighting rebel" who eventually matures. According to Pine, the character is "a 25-year-old acts like a 15-year-old" and who is "angry at the world". Kirk and Spock clash at Starfleet Academy, but, over the course of the film, Kirk focuses his "passion and obstinance and the spectrum of emotions" and becomes captain of the "Enterprise". The alternate timeline continues in the 2013 sequel "Star Trek Into Darkness", and the 2016 sequel "Star Trek Beyond", in which Pine reprises his role.
Development.
Conception and television.
Jeffrey Hunter played the commanding officer of the USS "Enterprise", Captain Christopher Pike, in the rejected "Star Trek" television pilot "". In developing a new pilot episode, called "Where No Man Has Gone Before", series creator Gene Roddenberry changed the captain's name to "James Kirk" after rejecting other options like Hannibal, Timber, Flagg and Raintree. The name was inspired by Captain James Cook, whose journal entry "ambition leads me ... farther than any other man has been before me" inspired the episode title. The character is in part based on C. S. Forester's Horatio Hornblower hero, and NBC wanted the show to emphasize the captain's "rugged individualism". Jack Lord was Desilu Productions' original choice to play Kirk, but his demand for fifty-percent ownership of the show led to him not being hired. The second pilot episode was successful, and "Where No Man Has Gone Before" was broadcast as the third episode of "Star Trek" on September 22, 1966.
William Shatner tried to imbue the character with qualities of "awe and wonder" absent from "The Cage". He also drew upon his experiences as a Shakespearean actor to invigorate the character, whose dialogue at times is laden with jargon. Not only did Shatner take inspiration from Roddenberry's suggestion of Hornblower, but also from Alexander the Great – "the athlete and the intellectual of his time" – whom Shatner had played for an unsold television pilot two years earlier. In addition, the actor based Kirk partly on himself because "the fatigue factor weeks of daily filming is such that you try to be as honest about yourself as possible". A comedy veteran, Shatner suggested making the show's characters as comfortable working in space as they would be at sea, thus having Kirk be a humorous "good-pal-the-captain, who in time of need would snap to and become the warrior". Changing the character to be "a man with very human emotions" also allowed for the development of the Spock character. Shatner wrote that "Kirk was a man who marveled and greatly appreciated the endless surprises presented to him by the universe ... He didn't take things for granted and, more than anything else, respected life in every one of its weird weekly adventure forms".
Films.
Shatner did not expect "Star Trek" to become a success, so when it was cancelled in 1969, he assumed it would be the end of his association with the show. However, Shatner went on to voice Kirk in the animated "Star Trek" series, star in the first seven "Star Trek" films, and provide voice acting for several games. "" director and writer Nicholas Meyer, who had never seen an episode of "Star Trek" before he was assigned to direct, conceived a ""Hornblower" in outer space" atmosphere, unaware that those books had been an influence on the show. Meyer also emphasized parallels to Sherlock Holmes, in that both characters waste away in the absence of their stimuli: new cases for Holmes; starship adventures for Kirk.
Meyer's "The Wrath of Khan" script focuses on Kirk's age, with McCoy giving Kirk a pair of glasses as a birthday present. The script states that Kirk is 49, but Shatner was unsure about being specific about Kirk's age because he was hesitant to portray a middle-aged version of himself. Shatner changed his mind when producer Harve Bennett convinced Shatner that he could age gracefully like Spencer Tracy. Spock's sacrifice at the end of the film allows for Kirk's spiritual rebirth; after commenting earlier that he feels old and worn out, Kirk states in the final scene that he feels "young." Additionally, Spock's self-sacrificing solution to the no-win "Kobayashi Maru" scenario, which Kirk had cheated his way through, forces Kirk to confront death and to grow as a character.
Both Shatner and test audiences were dissatisfied that Kirk was fatally shot in the back in the original ending of the film "Star Trek Generations". An addendum inserted while Shatner's "Star Trek Movie Memories" memoir was being printed expresses his enthusiasm at being called back to film a rewritten ending. Despite the rewrite, "Generations" co-writer Ronald D. Moore said that Kirk's death, which was intended to "resonate throughout the Star Trek franchise", failed to "pay off the themes death and mortality in the way we wanted". Malcolm McDowell, whose character kills Kirk, was dissatisfied with both versions of Kirk's death: he believed Kirk should have been killed "in a big way". McDowell claims to have received death threats after "Generations" was released.
Franchise "reboot".
In the 2009 film "Star Trek", screenwriters Alex Kurtzman and Roberto Orci focused their story on Kirk and Spock in the movie's alternative timeline while attempting to preserve key character traits from the previous depictions. Kurtzman said casting someone whose portrayal of Kirk would show that the character "is being honored and protected" was "tricky", but that the "spirit of Kirk is very much alive and well" in Pine's depiction. Due to his belief that he could not take himself seriously as a leader, Pine recalled having difficulty with his audition, which required him "to bark "Trek" jargon'", but his charisma impressed director J. J. Abrams. Pine's chemistry with Zachary Quinto, playing Spock, led Abrams to offer Pine the role. Jimmy Bennett played Kirk in scenes depicting the character's childhood. The writers turned to material "Best Destiny" for inspiration as to Kirk's childhood.
In preparing to play Kirk, Pine decided to embrace the character's key traits – "charming, funny, leader of men" – rather than try to fit the "predigested image" of Shatner's portrayal. Pine specifically did not try to mirror Shatner's cadence, believing that doing so would become "an impersonation". Pine said he wanted his portrayal of Kirk to most resemble Harrison Ford's Indiana Jones or Han Solo characters, highlighting their humor and "accidental hero" traits.
A misunderstanding arose during the film's production about the possibility of Shatner making a cameo appearance. According to Abrams, the production team considered ways to resurrect Shatner's deceased Kirk character, but could not devise a way that was not "lame". However, Abrams believed Shatner misinterpreted language about trying to get "him" into the movie as a reference to Shatner, and not his character. Shatner released a YouTube video expressing disappointment at not being approached for a cameo. Although Shatner questioned the wisdom of not including him in the film, he predicted the movie would be "wonderful" and that he was "kidding" about Abrams not offering him a cameo.
Reception.
According to Shatner, early "Star Trek" reviews called his performance "wooden", with most of the show's acting praise and media interest going to Nimoy. However, Shatner's mannerisms when portraying Kirk have become "instantly recognizable" and Shatner won a Saturn Award for Best Actor in 1982 for "The Wrath of Khan". "Star Trek II" director Nicholas Meyer said Shatner "gives the best performance of his life" in "The Wrath of Khan". "The Guardian" called Pine's performance of Kirk an "unqualified success", and "The Boston Globe" said Pine is "a fine, brash boy Kirk". "Slate", which called Pine "a jewel", described his performance as "channel" Shatner without being an impersonation.
Slate.com described Shatner's depiction of Kirk as an "expansive, randy, faintly ridiculous, and yet supremely capable leader of men, Falstaffian in his love of life and largeness of spirit". "The Myth of the American Superhero" refers to Kirk as a "superhuman redeemer" who "like a true superhero ... regularly escapes after risking battle with monsters or enemy spaceships". Although some episodes question Kirk's position as a hero, "Star Trek" "never left the viewer in doubt for long". Others have commented that Kirk's exaggerated "strength, intelligence, charm, and adventurousness" make him unrealistic. Kirk is described as able to find ways "through unanticipated problems to reach goals" and his leadership style is most "appropriate in a tight, geographically identical team with a culture of strong leadership." Although Roddenberry conceived the character as being "in a very real sense ... 'married' " to the "Enterprise", Kirk has been noted for "his sexual exploits with gorgeous females of every size, shape and type"; he has been called "promiscuous" and labeled a "womanizer". "The Last Lecture" author Randy Pausch believed he became a better teacher, colleague, and husband because he watched Kirk run the "Enterprise"; Pausch wrote that "for ambitious boys with a scientific bent, there could be no greater role model than James T. Kirk". David A. Goodman commented that Kirk "has as much reality as possible for a fictional character."
Cultural impact.
The town of Riverside, Iowa, petitioned Roddenberry and Paramount Pictures in 1985 for permission to "adopt" Kirk as their town's "Future Son". Paramount wanted $40,000 for a license to reproduce a bust of Kirk, but the city instead set a plaque and built a replica of the "Enterprise" (named the "USS "Riverside""), and the Riverside Area Community Club holds an annual "Trek Fest" in anticipation of Kirk's birthday.
Kirk has been the subject of a wide range of television spoofs that aired in many countries, including "The Carol Burnett Show" and KI.KA's "Bernd das Brot". John Belushi's impression of Kirk for "Saturday Night Live", which he described as his favorite role, was "dead-on". Jim Carrey has been praised for his satire of the character in a 1992 episode of "In Living Color". Comedian Kevin Pollak is well known for his impressions of Shatner as Kirk. Kirk has also been mentioned in song, the 1984 English adaptation of "99 Luftballons" by Nena and the 1979 song "Where's Captain Kirk?" by Spizzenergi.
Kirk has been merchandised in a variety of ways, including collectible busts, action figures, mugs, t-shirts, and Christmas tree ornaments. A Kirk Halloween mask was altered and used as the mask worn by the character Michael Myers in the "Halloween" film franchise. In 2002, Kirk's captain's chair from the original "Star Trek" was auctioned for $304,000.
In a 2010 Space Foundation survey, Kirk was ranked as the No. 6 (tied with cosmonaut Yuri Gagarin) most popular space hero.
Fan productions.
The "" fan production portrays the further voyages of the original "Enterprise" crew. The series' creators feel that "Kirk, Spock, McCoy and the rest should be treated as 'classic' characters like Willy Loman from "Death of a Salesman", Gandalf from "The Lord of the Rings" or even Hamlet, Othello or Romeo. Many actors have and can play the roles, each offering a different interpretation of said character".
James Cawley has played Kirk in the "Phase II" series since it began in 2004. "Wired" observes that while Cawley's depiction "lacks Shatner's vulnerability", the actor has enough swagger "to be passable in the role of Captain Kirk". Cawley's portrayal was well-known enough at Paramount that a group of "" writers called for Cawley's attention at a science fiction convention by shouting "Hey, Kirk!" at him while Shatner sat nearby.

</doc>
<doc id="47460" url="https://en.wikipedia.org/wiki?curid=47460" title="Mesosphere">
Mesosphere

The mesosphere (; from Greek "mesos" "middle" and "sphaira" "ball") is the layer of the Earth's atmosphere that is directly above the stratopause and directly below the mesopause. In the mesosphere temperature decreases as the altitude increases. The upper boundary of the mesosphere is the mesopause, which can be the coldest naturally occurring place on Earth with temperatures below . The exact upper and lower boundaries of the mesosphere vary with latitude and with season, but the lower boundary of the mesosphere is usually located at heights of about above the Earth's surface and the mesopause is usually at heights near , except at middle and high latitudes in summer where it descends to heights of about .
The stratosphere, mesosphere and lowest part of the thermosphere are collectively referred to as the "middle atmosphere", which spans heights from approximately to . The mesopause, at an altitude of , separates the mesosphere from the thermosphere—the second-outermost layer of the Earth's atmosphere. This is also around the same altitude as the turbopause, below which different chemical species are well mixed due to turbulent eddies. Above this level the atmosphere becomes non-uniform; the scale heights of different chemical species differ by their molecular masses.
Temperature.
Within the mesosphere, temperature decreases with increasing height. This is due to decreasing solar heating and increasing cooling by CO2 radiative emission. The top of the mesosphere, called the mesopause, is the coldest part of Earth's atmosphere. Temperatures in the upper mesosphere fall as low as , varying according to latitude and season.
Dynamic features.
The main dynamic features in this region are strong zonal (East-West) winds, atmospheric tides, internal atmospheric gravity waves (commonly called "gravity waves") and planetary waves. Most of these tides and waves are excited in the troposphere and lower stratosphere, and propagate upward to the mesosphere. In the mesosphere, gravity-wave amplitudes can become so large that the waves become unstable and dissipate. This dissipation deposits momentum into the mesosphere and largely drives global circulation.
Noctilucent clouds are located in the mesosphere. The upper mesosphere is also the region of the ionosphere known as the "D layer". The D layer is only present during the day, when some ionization occurs with nitric oxide being ionized by Lyman series-alpha hydrogen radiation. The ionization is so weak that when night falls, and the source of ionization is removed, the free electron and ion form back into a neutral molecule. The mesosphere is also known as the "Ignorosphere" because it is poorly studied compared to the stratosphere (which can be accessed with high-altitude balloons) and the thermosphere (in which satellites can orbit).
A deep sodium layer is located between . Made of unbound, non-ionized atoms of sodium, the sodium layer radiates weakly to contribute to the airglow.
Millions of meteors enter the Earth's atmosphere, an average of 40 tons per year.
Uncertainties.
The mesosphere lies above the maximum altitude for aircraft and nearly all balloons, and below the minimum altitude for orbital spacecraft. Above the 53.0 km balloon altitude record,
the mesosphere has only been accessed through the use of sounding rockets. As a result, it is the most poorly understood part of the atmosphere. The presence of red sprites and blue jets (electrical discharges or lightning within the lower mesosphere), noctilucent clouds and density shears within this poorly understood layer are of current scientific interest.

</doc>
<doc id="47463" url="https://en.wikipedia.org/wiki?curid=47463" title="Thermosphere">
Thermosphere

The thermosphere is the layer of the Earth's atmosphere directly above the mesosphere and directly below the exosphere. Within this layer of the atmosphere, ultraviolet radiation causes photoionization/photodissociation of molecules, creating the ions in the ionosphere. Called from the Greek θερμός ("pronounced thermos") meaning heat, the thermosphere begins about above the Earth. At these high altitudes, the residual atmospheric gases sort into strata according to molecular mass (see turbosphere). Thermospheric temperatures increase with altitude due to absorption of highly energetic solar radiation. Temperatures are highly dependent on solar activity, and can rise to . Radiation causes the atmosphere particles in this layer to become electrically charged (see ionosphere), enabling radio waves to bounce off and be received beyond the horizon. In the exosphere, beginning at above the Earth's surface, the atmosphere turns into space.
The highly diluted gas in this layer can reach during the day. Even though the temperature is so high, one would not feel warm in the thermosphere, because it is so near vacuum that there is not enough contact with the few atoms of gas to transfer much heat. A normal thermometer would be significantly below , because the energy lost by thermal radiation would exceed the energy acquired from the atmospheric gas by direct contact. In the anacoustic zone above , the density is so low that molecular interactions are too infrequent to permit the transmission of sound. 
The dynamics of the thermosphere are dominated by atmospheric tides, which are driven by the very significant diurnal heating. Atmospheric waves dissipate above this level because of collisions between the neutral gas and the ionospheric plasma.
The International Space Station orbits the Earth within the middle of the thermosphere, between (decaying by 2 km/month and raised by periodic reboosts), whereas the Gravity Field and Steady-State Ocean Circulation Explorer satellite at utilized winglets and an innovative ion engine to maintain a stable orientation and orbit.
Neutral gas constituents.
It is convenient to separate the atmospheric regions according to the two temperature minima at about 12 km altitude (the tropopause) and at about 85 km (the mesopause) (Figure 1). The thermosphere (or the upper atmosphere) is the height region above 85 km, while the region between the tropospause and the mesopause is the middle atmosphere (stratosphere and mesosphere) where absorption of solar UV radiation generates the temperature maximum near 45 km altitude and causes the ozone layer.
The density of the Earth's atmosphere decreases nearly exponentially with altitude. The total mass of the atmosphere is M = ρA H  ≃ 1 kg/cm2 within a column of one square centimeter above the ground (with ρA = 1.29 kg/m3 the atmospheric density on the ground at z = 0 m altitude, and H ≃ 8 km the average atmospheric scale height). 80% of that mass already concentrated within the troposphere. The mass of the thermosphere above about 85 km is only 0.002% of the total mass. Therefore, no significant energetic feedback from the thermosphere to the lower atmospheric regions can be expected.
Turbulence causes the air within the lower atmospheric regions below the turbopause at about 110 km to be a mixture of gases that does not change its composition. Its mean molecular weight is 29 g/mol with molecular oxygen (O2) and nitrogen (N2) as the two dominant constituents. Above the turbopause, however, diffusive separation of the various constituents is significant, so that each constituent follows its own barometric height structure with a scale height inversely proportional to its molecular weight. The lighter constituents atomic oxygen (O), helium (He), and hydrogen (H) successively dominate above about 200 km altitude and vary with geographic location, time, and solar activity. The ratio
N2/O which is a measure of the electron density at the ionospheric F region is highly affected by these variations. These changes follow from the diffusion of the minor constituents through the major gas component during dynamic processes.
Energy input.
Energy budget.
The thermospheric temperature can be determined from density observations as well as from direct satellite measurements. The temperature vs. altitude z in Fig. 1 can be simulated by the so-called Bates profile 
(1) formula_1
with T∞ the exospheric temperature above about 400 km altitude, 
To = 355 K, and zo = 120 km reference temperature and height, and s an empirical parameter depending on T∞ and decreasing with T∞. That formula is derived from a simple equation of heat conduction. One estimates a total heat input of qo≃ 0.8 to 1.6 mW/m2 above zo = 120 km altitude. In order to obtain equilibrium conditions, that heat input qo above zo is lost to the lower atmospheric regions by heat conduction.
The exospheric temperature T∞ is a fair measurement of the solar XUV radiation. Since solar radio emission F at 10.7 cm wavelength is a good indicator of solar activity, one can apply the empirical formula for quiet magnetospheric conditions.
(2) formula_2
with T∞ in K, Fo in 10−2 W m−2 Hz−1 (the Covington index) a value of F averaged over several solar cycles. The Covington index varies typically between 70 and 250 during a solar cycle, and never drops below about 50. Thus, T∞ varies between about 740 and 1350 K. During very quiet magnetospheric conditions, the still continuously flowing magnetospheric energy input contributes by about 250 K to the residual temperature of 500 K in eq.(2). The rest of 250 K in eq.(2) can be attributed to atmospheric waves generated within the troposphere and dissipated within the lower thermosphere.
Solar XUV radiation.
The solar X-ray and extreme ultraviolet radiation (XUV) at wavelengths < 170 nm is almost completely absorbed within the thermosphere. This radiation causes the various ionospheric layers as well as a temperature increase at these heights (Figure 1).
While the solar visible light (380 to 780 nm) is nearly constant with a variability of not more than about 0.1% of the solar constant, the solar XUV radiation is highly variable in time and space. For instance, X-ray bursts associated with solar flares can dramatically increase their intensity over preflare levels by many orders of magnitude over a time span of tens of minutes. In the extreme ultraviolet, the Lyman α line at 121.6 nm represents an important source of ionization and dissociation at ionospheric D layer heights. During quiet periods of solar activity, it alone contains more energy than the rest of the XUV spectrum. Quasi-periodic changes of the order of 100% and more with period of 27 days and 11 years belong to the prominent variations of solar XUV radiation. However, irregular fluctuations over all time scales are present all the time. During low solar activity, about one half of the total energy input into the thermosphere is thought to be solar XUV radiation. Evidently, that solar XUV energy input occurs only during daytime conditions, maximizing at the equator during equinox.
Solar wind.
A second source of energy input into the thermosphere is solar wind energy which is transferred to the magnetosphere by mechanisms that are not completely understood. One possible way to transfer energy is via a hydrodynamic dynamo process. Solar wind particles penetrate into the polar regions of the magnetosphere where the geomagnetic field lines are essentially vertically directed. An electric field is generated, directed from dawn to dusk. Along the last closed geomagnetic field lines with their footpoints within the auroral zones, field aligned electric currents can flow into the ionospheric dynamo region where they are closed by electric Pedersen and Hall currents. Ohmic losses of the Pedersen currents heat the lower thermosphere (see e.g., Magnetospheric electric convection field). In addition, penetration of high energetic particles from the magnetosphere into the auroral regions enhance drastically the electric conductivity, further increasing the electric currents and thus Joule heating. During quiet magnetospheric activity, the magnetosphere contributes perhaps by a quarter to the energy budget of the thermosphere. This is about 250 K of the exospheric temperature in eq.(2). During very large activity, however, this heat input can increase substantially, by a factor of four or more. That solar wind input occurs mainly in the auroral regions during the day as well as during the night.
Atmospheric waves.
Two kinds of large scale atmospheric waves within the lower atmosphere exist: internal waves with finite vertical wavelengths which can transport wave energy upward and external waves with infinitely large wavelengths which cannot transport wave energy. Atmospheric gravity waves and most of the atmospheric tides generated within the troposphere belong to the internal waves. Their density amplitudes increase exponentially with height, so that at the mesopause these waves become turbulent and their enery is dissipated (similar to breaking of ocean waves at the coast), thus contributing to the heating of the thermosphere by about 250 K in eq.(2). On the other hand, the fundamental diurnal tide labelled (1, −2) which is most efficiently excited by solar irradiance is an external wave and plays only a marginal role within lower and middle atmosphere. However, at thermospheric altitudes, it becomes the predominant wave. It drives the electric Sq-current within the ionospheric dynamo region between about 100 and 200 km height.
Heating, predominately by tidal waves, occurs mainly at lower and middle latitudes. The variability of this heating depends in general on the meteorological conditions within troposphere and middle atmosphere, and may not exceed about 50%.
Dynamics.
Within the thermosphere above about 150 km height, all atmospheric waves successively become external waves, and no signifiant vertical wave structure is visible. The atmospheric wave modes degenerate to the spherical functions Pnm with m a meridional wave number and n the zonal wave number (m = 0: zonal mean flow; m = 1: diurnal tides; m = 2: semidiurnal tides; etc.). The thermosphere becomes a damped oscillator system with low pass filter characteristics. This means that smaller scale waves (greater numbers of (n,m)) and higher frequencies are suppressed in favor of large scale waves and lower frequencies. If one considers very quiet magnetospheric disturbances and a constant mean exospheric temperature (averaged over the sphere), the observed temporal and spatial distribution of the exospheric temperature distribution can be described by a sum of spheric functions:
(3) formula_3
Here, it is φ latitude, λ longitude, and t time, ωa the angular frequency of one year, ωd the angular frequency of one solar day, and τ = ωdt + λ the local time. ta = June, 21 is the time of northern summer solstice, and τd = 15:00 is the local time of maximum diurnal temperature.
The first term in (3) on the right is the global mean of the exospheric temperature (of the order of 1000 K). The second term P20 = 0.5(3 sin2(φ)−1) represents the heat surplus at lower latitudes and a corresponding heat deficit at higher latitudes (Fig. 2a). A thermal wind system develops with winds toward the poles in the upper level and wind away from the poles in the lower level. The coefficient ΔT20 ≈ 0.004 is small because Joule heating in the aurora regions compensates that heat surplus even during quiet magnetospheric conditions. During disturbed conditions, however, that term becomes dominant changing sign so that now heat surplus is transported from the poles to the equator. The third term (with P10 = sin φ) represents heat surplus on the summer hemisphere and is responsible for the transport of excess heat from the summer into the winter hemisphere (Fig. 2b). Its relative amplitude is of the order ΔT10 ≃ 0.13. The fourth term (with P11(φ) = cos φ) is the dominant diurnal wave (the tidal mode (1,−2)). It is responsible for the transport of excess heat from the day time hemisphere into the night time hemisphere (Fig. 2d). Its relative amplitude is ΔT11≃ 0.15, thus of the order of 150 K. Additional terms (e.g., semiannual, semidiurnal terms and higher order terms) must be added to eq.(3). They are, however, of minor importance. Corresponding sums can be developed for density, pressure, and the various gas constituents.
Thermospheric storms.
In contrast to solar XUV radiation, magnetospheric disturbances, indicated on the ground by geomagnetic variations, show an unpredictable impulsive character, from short periodic disturbances of the order of hours to long standing giant storms of several day's duration. 
The reaction of the thermosphere to a large magnetospheric storm is called thermospheric storm. Since the heat input into the thermosphere occurs at high latitudes (mainly into the auroral regions), the heat transport represented by the term P20 in eq.(3) is reversed. In addition, due to the impulsive form of the disturbance, higher order terms are generated which, however, possess short decay times and thus quickly disappear. The sum of 
these modes determines the "travel time" of the disturbance to the lower latitudes, and thus the response time of the thermosphere with respect to the magnetospheric disturbance. Important for the development of an ionospheric storm is the increase of the ratio N2/O
during a thermospheric storm at middle and higher latitude. An increase of 
N2 increases the loss process of the ionospheric plasma and causes therefore a decrease of the electron density within the ionospheric F-layer (negative ionospheric storm).

</doc>
<doc id="47465" url="https://en.wikipedia.org/wiki?curid=47465" title="Aten">
Aten

Aten (also Aton, Egyptian "jtn") is the disk of the sun in ancient Egyptian mythology, and originally an aspect of the god Ra. The deified Aten is the focus of the monolatristic, henotheistic, monistic or monotheistic religion of Atenism established by Amenhotep IV, who later took the name Akhenaten (died ca. 1335 BCE) in worship and recognition of Aten. In his poem "Great Hymn to the Aten", Akhenaten praises Aten as the creator, giver of life, and nurturing spirit of the world. Aten does not have a Creation Myth or family, but is mentioned in the Book of the Dead. The worship of Aten was eradicated by Horemheb.
Overview.
The Aten, the sun-disk, is first referred to as a deity in The Story of Sinuhe from the 12th dynasty, in which the deceased king is described as rising as god to the heavens and uniting "with the sun-disk, the divine body merging with its maker." By analogy, the term "silver aten" was sometimes used to refer to the moon. The solar Aten was extensively worshipped as a god in the reign of Amenhotep III, when it was depicted as a falcon-headed man much like Ra. In the reign of Amenhotep III's successor, Amenhotep IV, the Aten became the central god of Egyptian state religion, and Amenhotep IV changed his name to Akhenaten to reflect his close link with the new supreme deity.
The full title of Akhenaten's god was "Ra-Horakhty who rejoices in the horizon, in his Name as the Light which is in the sun disc." (This is the title of the god as it appears on the numerous stelae which were placed to mark the boundaries of Akhenaten's new capital at Akhetaten, modern Amarna.) 
This lengthy name was often shortened to "Ra-Horus-Aten" or just "Aten" in many texts, but the god of Akhenaten raised to supremacy is considered a synthesis of very ancient gods viewed in a new and different way. The god is also considered to be both masculine and feminine simultaneously. All creation was thought to emanate from the god and to exist within the god. In particular, the god was not depicted in anthropomorphic (human) form, but as rays of light extending from the sun's disk.
Furthermore, the god's name came to be written within a cartouche, along with the titles normally given to a Pharaoh, another break with ancient tradition. Ra-Horus, more usually referred to as "Ra-Horakhty" ("Ra, who is Horus of the two horizons"), is a synthesis of two other gods, both of which are attested from very early on. During the Amarna period, this synthesis was seen as the invisible source of energy of the sun god, of which the visible manifestation was the Aten, the solar disk. Thus Ra-Horus-Aten was a development of old ideas which came gradually. The real change, as some see it, was the apparent abandonment of all other gods, especially Amun, and the debatable introduction of monotheism by Akhenaten. The syncretism is readily apparent in the Great Hymn to the Aten in which Re-Herakhty, Shu and Aten are merged into the creator god. Others see Akhenaten as a practitioner of an Aten monolatry, as he did not actively deny the existence of other gods; he simply refrained from worshipping any but the Aten.
Religion.
Principles of Aten's religion were recorded on the rock tomb walls of Akhetaten.
In the religion of Aten (Atenism), night is a time to fear. Work is done best when the sun, Aten, is present. Aten cares for every creature, and created a Nile river in the sky (rain) for the Syrians. Aten created all countries and people. The rays of the sun disk only holds out life to the royal family; everyone else receives life from Akhenaten and Nefertiti in exchange of loyalty for Aten.
When a good person dies, he/she continues to live in the City of Light for the dead in Akhetaten. The conditions are the same after death. Akhenaten judged whether someone should be granted an afterlife, and operated the scale of justice.
The explanation as to why Aten could not be fully represented was that the god has gone beyond creation.
Worship.
The cult centre of Aten was at the new city Akhetaten; some other cult cities include Thebes and Heliopolis. The principles of Aten's cult were recorded on the rock walls of tombs of Tall al-Amarnah. Significantly different from other ancient Egyptian temples, temples of Aten were colorful and open-roofed to allow the rays of the sun. Doorways had broken lintels and raised thresholds. No statues of Aten were allowed; those were seen as idolatry. Priests had less to do, since offerings (fruits, flowers, cakes) were limited, and oracles were not needed. Temples of Aten did not collect tax.
In the worship of Aten, the daily service of purification, anointment and clothing of the divine image was not performed. Incense was burnt several times a day. Hymns sung to Aten were accompanied by harp music. Aten's ceremonies in Akhetaten involved giving offerings to Aten with a swipe of the royal scepter.
Instead of barque processions, the royal family rode on a chariot on festival days.
Royal Titulary.
During the Amarna Period, the Aten was given a Royal Titulary (as he was considered to be king of all), with his names drawn in a cartouche. There were two forms of this title, the first had the names of other gods, and the second later one which was more 'singular' and referred only to the Aten himself. The early form has Re-Horakhti who rejoices in the Horizon, in his name Shu which is the Aten. The later form has Re, ruler of the two horizons who rejoices in the Horizon, in his name of light which is the Aten.
Variant vocalizations.
Egyptologists have vocalized the word variously as Aten, Aton, Atonu, and Itn.

</doc>
<doc id="47466" url="https://en.wikipedia.org/wiki?curid=47466" title="Mesopause">
Mesopause

The mesopause is the temperature minimum at the boundary between the mesosphere and the thermosphere atmospheric regions. Due to the lack of solar heating and very strong radiative cooling from carbon dioxide, the mesosphere is the coldest region on Earth with temperatures as low as -100 °C (-148 °F or 173 K). The altitude of the mesopause for many years was assumed to be at around 85 km (53 mi.), but observations to higher altitudes and modeling studies in the last 10 years have shown that in fact the mesopause consists of two minima - one at about 85 km and a stronger minimum at about 100 km. (62 mi.)
An interesting feature is that the summer mesopause is cooler than the winter (sometimes referred to as the "mesopause anomaly"). It is due to a summer-to-winter circulation giving rise to upwelling at the summer pole and downwelling at the winter pole. Air rising will expand and cool resulting in a cold summer mesopause and conversely downwelling air results in compression and associated increase in temperature at the winter mesopause. In the mesosphere the summer-to-winter circulation is due to gravity wave dissipation, which deposits momentum against the mean east-west flow, resulting in a small north-south circulation.
In recent years the mesopause has also been the focus of studies on global climate change associated with increases in CO2. Unlike the troposphere, where greenhouse gases result in the atmosphere heating up, increased CO2 in the mesosphere acts to cool the atmosphere due to increased radiative emission. This results in a measurable effect - the mesopause should become cooler with increased CO2. Observations do show a decrease of temperature of the mesopause, though the magnitude of this decrease varies and is subject to further study. Modeling studies of this phenomenon have also been carried out.

</doc>
<doc id="47469" url="https://en.wikipedia.org/wiki?curid=47469" title="Amor">
Amor

Amor may refer to:

</doc>
<doc id="47470" url="https://en.wikipedia.org/wiki?curid=47470" title="List of Apollo asteroids">
List of Apollo asteroids

[[File:Minor Planets - Apollo.svg|thumb|285px|The Apollo asteroid group compared to the orbits of the terrestrial planets of the Solar System.
]]
The Apollo asteroids are a group of near-Earth asteroids named after 1862 Apollo, the first asteroid of this group which was discovered by German astronomer Karl Reinmuth in the 1930s. They are Earth-crosser asteroids that have orbital semi-major axis greater than that of the Earth (> 1 AU) but perihelion distances less than the Earth's aphelion distance (q < 1.017 AU).
, the Apollo asteroid group includes a total of 6,923 known asteroids of which 991 are currently numbered. Asteroids are not numbered until they have been observed at two or more oppositions. More than a thousand Apollo asteroids are large enough and may get close enough to Earth to be known as potentially hazardous asteroids.
The closer their semi-major axis is to Earth's, the less eccentricity is needed for the orbits to cross. The February 15, 2013 Chelyabinsk meteor that exploded over the city of Chelyabinsk in the southern Urals region of Russia, injuring an estimated one thousand people with flying glass from broken windows, was an Apollo class asteroid.
List.
The largest known Apollo asteroid is 1866 Sisyphus, with a diameter of about 8.5 km. Examples of known Apollo asteroids include:

</doc>
<doc id="47473" url="https://en.wikipedia.org/wiki?curid=47473" title="Algal bloom">
Algal bloom

An algal bloom is a rapid increase or accumulation in the population of algae (typically microscopic) in a water system. Cyanobacteria blooms are often called blue-green algae. Algal blooms may occur in freshwater as well as marine environments. Typically, only one or a small number of phytoplankton species are involved, and some blooms may be recognized by discoloration of the water resulting from the high density of pigmented cells. Some algal blooms can result in extreme issues, such as the Lake Erie blooming in 2011, which lead to a nearly full-scale trophic cascade.
Blooming.
Since 'algae' is a broad term including organisms of widely varying sizes, growth rates and nutrient requirements, there is no officially recognized threshold level as to what is defined as a bloom. For some species, algae can be considered to be blooming at concentrations reaching millions of cells per milliliter, while others form blooms of tens of thousands of cells per liter. The photosynthetic pigments in the algal cells determine the color of the algal bloom, and are thus often a greenish color, but they can also be a wide variety of other colors such as yellow, brown or red, depending on the species of algae and the type of pigments contained therein.
Bright green blooms in freshwater systems are frequently a result of cyanobacteria (colloquially known as blue-green algae) such as "Microcystis". Blooms may also consist of macroalgal (non-phytoplanktonic) species. These blooms are recognizable by large blades of algae that may wash up onto the shoreline.
Of particular note are harmful algal blooms (HABs), which are algal bloom events involving toxic or otherwise harmful phytoplankton such as dinoflagellates of the genus "Alexandrium" and "Karenia", or diatoms of the genus "Pseudo-nitzschia". Such blooms often take on a red or brown hue and are known colloquially as red tides.
Freshwater algal blooms.
Freshwater algal blooms are the result of an excess of nutrients, particularly some phosphates. The excess of nutrients may originate from fertilizers that are applied to land for agricultural or recreational purposes. They may also originate from household cleaning products containing phosphorus. These nutrients can then enter watersheds through water runoff. Excess carbon and nitrogen have also been suspected as causes. Presence of residual sodium carbonate acts as catalyst for the algae to bloom by providing dissolved carbon dioxide for enhanced photosynthesis in the presence of nutrients.
When phosphates are introduced into water systems, higher concentrations cause increased growth of algae and plants. Algae tend to grow very quickly under high nutrient availability, but each alga is short-lived, and the result is a high concentration of dead organic matter which starts to decay. The decay process consumes dissolved oxygen in the water, resulting in hypoxic conditions. Without sufficient dissolved oxygen in the water, animals and plants may die off in large numbers. Use of an Olszewski tube can help combat these problems with hypolimnetic withdrawal.
Blooms may be observed in freshwater aquariums when fish are overfed and excess nutrients are not absorbed by plants. These are generally harmful for fish, and the situation can be corrected by changing the water in the tank and then reducing the amount of food given.
Harmful algal blooms.
A "harmful algal bloom" (HAB) is an algal bloom that causes negative impacts to other organisms via production of natural toxins, mechanical damage to other organisms, or by other means. HABs are often associated with large-scale marine mortality events and have been associated with various types of shellfish poisonings.
Background.
In the marine environment, single-celled, microscopic, plant-like organisms naturally occur in the well-lit surface layer of any body of water. These organisms, referred to as phytoplankton or microalgae, form the base of the food web upon which nearly all other marine organisms depend. Of the 5000+ species of marine phytoplankton that exist worldwide, about 2% are known to be harmful or toxic. Blooms of harmful algae can have large and varied impacts on marine ecosystems, depending on the species involved, the environment where they are found, and the mechanism by which they exert negative effects.
Harmful algal blooms have been observed to cause adverse effects to a wide variety of aquatic organisms, most notably marine mammals, sea turtles, seabirds and finfish. The impacts of HAB toxins on these groups can include harmful changes to their developmental, immunological, neurological, or reproductive capacities. The most conspicuous effects of HABs on marine wildlife are large-scale mortality events associated with toxin-producing blooms. For example, a mass mortality event of 107 bottlenose dolphins occurred along the Florida panhandle in the spring of 2004 due to ingestion of contaminated menhaden with high levels of brevetoxin. Manatee mortalities have also been attributed to brevetoxin but unlike dolphins, the main toxin vector was endemic seagrass species ("Thalassia testudinum") in which high concentrations of brevetoxins were detected and subsequently found as a main component of the stomach contents of manatees.
Additional marine mammal species, like the highly endangered North Atlantic Right Whale, have been exposed to neurotoxins by preying on highly contaminated zooplankton. With the summertime habitat of this species overlapping with seasonal blooms of the toxic dinoflagellate "Alexandrium fundyense", and subsequent copepod grazing, foraging right whales will ingest large concentrations of these contaminated copepods. Ingestion of such contaminated prey can affect respiratory capabilities, feeding behavior, and ultimately the reproductive condition of the population.
Immune system responses have been affected by brevetoxin exposure in another critically endangered species, the Loggerhead sea turtle. Brevetoxin exposure, via inhalation of aerosolized toxins and ingestion of contaminated prey, can have clinical signs of increased lethargy and muscle weakness in loggerhead sea turtles causing these animals to wash ashore in a decreased metabolic state with increases of immune system responses upon blood analysis.
Examples of common harmful effects of HABs include:
Due to their negative economic and health impacts, HABs are often carefully monitored.
HABs occur in many regions of the world, and in the United States are recurring phenomena in multiple geographical regions. The Gulf of Maine frequently experiences blooms of the dinoflagellate "Alexandrium fundyense", an organism that produces saxitoxin, the neurotoxin responsible for paralytic shellfish poisoning. The well-known "Florida red tide" that occurs in the Gulf of Mexico is a HAB caused by "Karenia brevis", another dinoflagellate which produces brevetoxin, the neurotoxin responsible for neurotoxic shellfish poisoning. California coastal waters also experience seasonal blooms of "Pseudo-nitzschia", a diatom known to produce domoic acid, the neurotoxin responsible for amnesic shellfish poisoning. Off the west coast of South Africa, HABs caused by "Alexandrium catanella" occur every spring. These blooms of organisms cause severe disruptions in fisheries of these waters as the toxins in the phytoplankton cause filter-feeding shellfish in affected waters to become poisonous for human consumption.
If the HAB event results in a high enough concentration of algae the water may become discoloured or murky, varying in colour from purple to almost pink, normally being red or green. Not all algal blooms are dense enough to cause water discolouration.
Red tides.
Red tide is a term often used synonymously with HABs in marine coastal areas, however the term is misleading since algal blooms can be a wide variety of colors and growth of algae is unrelated to the tides. The term 'algal bloom' or 'harmful algal bloom' has since replaced 'red tide' as the appropriate description of this phenomenon.
Causes of HABs.
It is unclear what causes HABs; their occurrence in some locations appears to be entirely natural, while in others they appear to be a result of human activities. Furthermore, there are many different species of algae that can form HABs, each with different environmental requirements for optimal growth. The frequency and severity of HABs in some parts of the world have been linked to increased nutrient loading from human activities. In other areas, HABs are a predictable seasonal occurrence resulting from coastal upwelling, a natural result of the movement of certain ocean currents. The growth of marine phytoplankton (both non-toxic and toxic) is generally limited by the availability of nitrates and phosphates, which can be abundant in coastal upwelling zones as well as in agricultural run-off. The type of nitrates and phosphates available in the system are also a factor, since phytoplankton can grow at different rates depending on the relative abundance of these substances (e.g. ammonia, urea, nitrate ion). A variety of other nutrient sources can also play an important role in affecting algal bloom formation, including iron, silica or carbon. Coastal water pollution produced by humans (including iron fertilization) and systematic increase in sea water temperature have also been suggested as possible contributing factors in HABs. Other factors such as iron-rich dust influx from large desert areas such as the Sahara are thought to play a role in causing HABs. Some algal blooms on the Pacific coast have also been linked to natural occurrences of large-scale climatic oscillations such as El Niño events. While HABs in the Gulf of Mexico have been occurring since the time of early explorers such as Cabeza de Vaca, it is unclear what initiates these blooms and how large a role anthropogenic and natural factors play in their development. It is also unclear whether the apparent increase in frequency and severity of HABs in various parts of the world is in fact a real increase or is due to increased observation effort and advances in species identification technology. However recent research found that the warming of summer surface temperatures of lakes, which rose by 0.34°C decade per decade between 1985 and 2009 due to global warming, also will likely increase algal blooming by 20 % over the next century.
Researching Solutions.
The decline of filter-feeding shellfish populations, such as oysters, likely contribute to HAB occurrence. As such, numerous research projects are assessing the potential of restored shellfish populations to reduce HAB occurrence.
Since many Algal blooms are caused by a major influx of nutrient-rich runoff into a water body, programs to treat wastewater, reduce the overuse of fertilizers in agriculture and reducing the bulk flow of runoff can be effective for reducing severe algal blooms at river mouths, estuaries, and the ocean directly in front of the river's mouth.

</doc>
<doc id="47474" url="https://en.wikipedia.org/wiki?curid=47474" title="Aperture">
Aperture

In optics, an aperture is a hole or an opening through which light travels. More specifically, the aperture and focal length of an optical system determine the cone angle of a bundle of rays that come to a focus in the image plane. The aperture determines how collimated the admitted rays are, which is of great importance for the appearance at the image plane. If an aperture is narrow, then highly collimated rays are admitted, resulting in a sharp focus at the image plane. If an aperture is wide, then uncollimated rays are admitted, resulting in a sharp focus only for rays with a certain focal length. This means that a wide aperture results in an image that is sharp for things at the correct distance. The aperture also determines how many of the incoming rays are actually admitted and thus how much light reaches the image plane (the narrower the aperture, the darker the image for a given exposure time). In the human eye, the pupil is the aperture.
An optical system typically has many openings, or structures that limit the ray bundles (ray bundles are also known as "pencils" of light). These structures may be the edge of a lens or mirror, or a ring or other fixture that holds an optical element in place, or may be a special element such as a diaphragm placed in the optical path to limit the light admitted by the system. In general, these structures are called stops, and the aperture stop is the stop that determines the ray cone angle, or equivalently the brightness, at an image point.
In some contexts, especially in photography and astronomy, "aperture" refers to the "diameter" of the aperture stop rather than the physical stop or the opening itself. For example, in a telescope the aperture stop is typically the edges of the objective lens or mirror (or of the mount that holds it). One then speaks of a telescope as having, for example, a 100 centimeter "aperture". Note that the aperture stop is not necessarily the smallest stop in the system. Magnification and demagnification by lenses and other elements can cause a relatively large stop to be the aperture stop for the system. In astrophotography the aperture may be given as a linear measure (for example in inches or mm) or as the dimensionless ratio between that measure and the focal length. In other photography it is usually given as a ratio.
Sometimes stops and diaphragms are called apertures, even when they are not the aperture stop of the system.
The word aperture is also used in other contexts to indicate a system which blocks off light outside a certain region. In astronomy for example, a photometric aperture around a star usually corresponds to a circular window around the image of a star within which the light intensity is assumed.
Application.
The aperture stop is an important element in most optical designs. Its most obvious feature is that it limits the amount of light that can reach the image/film plane. This can be either unavoidable, as in a telescope where one wants to collect as much light as possible; or deliberate, to prevent saturation of a detector or overexposure of film. In both cases, the size of the aperture stop is constrained by things other than the amount of light admitted; however:
In addition to an aperture stop, a photographic lens may have one or more "field stops", which limit the system's field of view. When the field of view is limited by a field stop in the lens (rather than at the film or sensor) vignetting results; this is only a problem if the resulting field of view is less than was desired.
The biological pupil of the eye is its aperture in optics nomenclature; the iris is the diaphragm that serves as the aperture stop. Refraction in the cornea causes the effective aperture (the entrance pupil in optics parlance) to differ slightly from the physical pupil diameter. The entrance pupil is typically about 4 mm in diameter, although it can range from 2 mm () in a brightly lit place to 8 mm () in the dark.
In astronomy, the diameter of the aperture stop (called the "aperture") is a critical parameter in the design of a telescope. Generally, one would want the "aperture" to be as large as possible, to collect the maximum amount of light from the distant objects being imaged. The size of the aperture is limited, however, in practice by considerations of cost and weight, as well as prevention of aberrations (as mentioned above).
Apertures are also used in laser energy control, focusing, diffractions/patterns, and beam cleaning. Laser applications include spatial filters, Q-switching, high intensity x-ray control.
In light microscopy, the word aperture may be used with reference to either the condenser (changes angle of light onto specimen field), field iris (changes area of illumination) or possibly objective lens (forms primary image). "See" Optical microscope.
In photography.
The aperture stop of a photographic lens can be adjusted to control the amount of light reaching the film or image sensor. In combination with variation of shutter speed, the aperture size will regulate the film's or image sensor's degree of exposure to light. Typically, a fast shutter will require a larger aperture to ensure sufficient light exposure, and a slow shutter will require a smaller aperture to avoid excessive exposure.
A device called a diaphragm usually serves as the aperture stop, and controls the aperture. The diaphragm functions much like the iris of the eye – it controls the effective diameter of the lens opening. Reducing the aperture size increases the depth of field, which describes the extent to which subject matter lying closer than or farther from the actual plane of focus appears to be in focus. In general, the smaller the aperture (the larger the number), the greater the distance from the plane of focus the subject matter may be while still appearing in focus.
The lens aperture is usually specified as an f-number, the ratio of focal length to effective aperture diameter. A lens typically has a set of marked "f-stops" that the f-number can be set to. A lower f-number denotes a greater aperture opening which allows more light to reach the film or image sensor. The photography term "one f-stop" refers to a factor of √2 (approx. 1.41) change in f-number, which in turn corresponds to a factor of 2 change in light intensity.
Aperture priority is a semi-automatic shooting mode used in cameras. It permits the photographer to select an aperture setting and let the camera to decide the shutter speed and sometimes also ISO sensitivity for the correct exposure. This is also referred to as Aperture Priority Auto Exposure, A mode, AV mode (aperture-value mode), or semi-auto mode.
Typical ranges of apertures used in photography are about 2.8–22 or 2–16, covering 6 stops, which may be divided into wide, middle, and narrow of 2 stops each, roughly (using round numbers) 2–4, 4–8, and 8–16 or (for a slower lens) 2.8–5.6, 5.6–11, and 11–22. These are not sharp divisions, and ranges for specific lenses vary.
Maximum and minimum apertures.
The specifications for a given lens typically include the maximum and minimum aperture sizes, for example, 1.4–22. In this case 1.4 is the maximum aperture (the widest opening), and 22 is the minimum aperture (the smallest opening). The maximum aperture opening tends to be of most interest, and is always included when describing a lens. This value is also known as the lens "speed", as it affects the exposure time. The aperture is proportional to the square root of the light admitted, and thus inversely proportional to the square root of required exposure time, such that an aperture of 2 allows for exposure times one quarter that of 4.
Lenses with apertures opening 2.8 or wider are referred to as "fast" lenses, although the specific point has changed over time (for example, in the 1911 Encyclopaedia Britannica aperture openings wider than 6 were considered fast). The fastest lenses for the common 35 mm film format in general production have apertures of 1.2 or 1.4, with more at 1.8 and 2.0, and many at 2.8 or slower; 1.0 is unusual, though sees some use. When comparing "fast" lenses, the image format used must be considered. Lenses designed for a small format such as half frame or APS-C need to project a much smaller image circle than a lens used for large format photography. Thus the optical elements built into the lens can be far smaller and cheaper.
In exceptional circumstances lenses can have even wider apertures with f-numbers smaller than 1.0; see lens speed: fast lenses for a detailed list. For instance, both the current Leica Noctilux-M 50mm ASPH and a 1960s-era Canon 50mm rangefinder lens have a maximum aperture of 0.95. Such lenses tend to be optically complex and very expensive; at launch, in September 2008, the Leica Noctilux retailed for $11,000. However, significantly more affordable examples have appeared in recent years, such as the Cosina Voigtländer 17.5mm 0.95, 25mm 0.95 and 42.5mm 0.95 manual focus lenses for the Micro Four Thirds System, each of which retails for approximately US$1,000.
Professional lenses for some movie cameras have f-numbers as small as 0.75. Stanley Kubrick's film "Barry Lyndon" has scenes shot by candlelight with a NASA/Zeiss 50mm f/0.7, the fastest lens in film history. Beyond the expense, these lenses have limited application due to the correspondingly shallower depth of field – the scene must either be shallow, shot from a distance, or will be significantly defocused, though this may be a desired effect.
Zoom lenses typically have a maximum relative aperture (minimum f-number) of 2.8 to 6.3 through their range. High-end lenses will have a constant aperture, such as 2.8 or 4, which means that the relative aperture will stay the same throughout the zoom range. A more typical consumer zoom will have a variable maximum relative aperture, since it is harder and more expensive to keep the maximum relative aperture proportional to focal length at long focal lengths; 3.5 to 5.6 is an example of a common variable aperture range in a consumer zoom lens.
By contrast, the minimum aperture does not depend on the focal length – it is limited by how narrowly the aperture closes, not the lens design – and is instead generally chosen based on practicality: very small apertures have lower sharpness due to diffraction, while the added depth of field is not generally useful, and thus there is generally little benefit in using such apertures. Accordingly, DSLR lens typically have minimum aperture of 16, 22, or 32, while large format may go down to 64, as reflected in the name of Group f/64. Depth of field is a significant concern in macro photography, however, and there one sees smaller apertures. For example, the Canon MP-E 65mm can have effective aperture (due to magnification) as small as 96. The pinhole optic for Lensbaby creative lenses has an aperture of just 177.
Aperture area.
The amount of light captured by a lens is proportional to the area of the aperture, equal to:
Where the two equivalent forms are related via the f-number "N = f / D", with focal length "f" and aperture diameter "D".
The focal length value is not required when comparing two lenses of the same focal length; a value of 1 can be used instead, and the other factors can be dropped as well, leaving area proportion to the reciprocal square of the f-number "N".
If two cameras of different format sizes and focal lengths have the same angle of view, and the same aperture area, they gather the same amount of light from the scene. In that case, the relative focal-plane illuminance, however, would depend only on the f-number "N", so it is less in the camera with the larger format, longer focal length, and higher f-number. This assumes both lenses have identical transmissivity.
Aperture control.
Most SLR cameras provide "automatic aperture control", which allows viewing and metering at the lens's maximum aperture, but stops the lens down to the working aperture during exposure, and returns the lens to maximum aperture afterwards.
The first SLR cameras with internal (“through-the-lens” or “TTL”) meters (e.g., the Pentax Spotmatic) required that the lens be stopped down to the working aperture when taking a meter reading. With a small aperture, this darkened the viewfinder, making viewing, focusing, and composition difficult. Subsequent models soon incorporated mechanical coupling between the lens and the camera body, indicating the working aperture to the camera while allowing the lens to be at its maximum aperture for composition and focusing; this feature became known as automatic aperture control or automatic diaphragm control.
For some lenses, including a few long telephotos, lenses mounted on bellows, and perspective-control and tilt/shift lenses, the mechanical linkage was impractical, and automatic aperture control was not provided. Many such lenses incorporated a feature known as a "preset" aperture, which allows the lens to be set to working aperture and then quickly switched between working aperture and full aperture without looking at the aperture control. Typical operation might be to establish rough composition, set the working aperture for metering, return to full aperture for a final check of focus and composition, and focusing, and finally, return to working aperture just before exposure. Although slightly easier than stopped-down metering, operation is less convenient than automatic operation. Preset aperture controls have taken several forms; the most common has been the use of essentially two lens aperture rings, with one ring setting the aperture and the other serving as a limit stop when switching to working aperture. Examples of lenses with this type of preset aperture control are the Nikon PC Nikkor 28 mm 3.5 and the SMC Pentax Shift 6×7 75 mm 4.5. The Nikon PC Micro-Nikkor 85 mm 2.8D lens incorporates a mechanical pushbutton that sets working aperture when pressed and restores full aperture when pressed a second time.
Canon EF lenses, introduced in 1987, have electromagnetic diaphragms, eliminating the need for a mechanical linkage between the camera and the lens, and allowing automatic aperture control with the Canon TS-E tilt/shift lenses. Nikon PC-E perspective-control lenses, introduced in 2008, also have electromagnetic diaphragms. Automatic aperture control is provided with the newer Nikon digital SLR cameras; with some earlier cameras, the lenses offer preset aperture control by means of a pushbutton that controls the electromagnetic diaphragm.
Optimal aperture.
Optimal aperture depends both on optics (the depth of the scene versus diffraction), and on the performance of the lens.
Optically, as a lens is stopped down, the defocus blur at the Depth of Field (DOF) limits decreases but diffraction blur increases. The presence of these two opposing factors implies a point at which the combined blur spot is minimized (Gibson 1975, 64); at that point, the f-number is optimal for image sharpness, for this given depth of field – a wider aperture (lower "f"-number) causes more defocus, while a narrower aperture (higher "f"-number) causes more diffraction.
As a matter of performance, lenses often do not perform optimally when fully opened, and thus generally have better sharpness when stopped down some – note that this is sharpness in the plane of critical focus, setting aside issues of depth of field. Beyond a certain point there is no further sharpness benefit to stopping down, and the diffraction begins to become significant. There is accordingly a sweet spot, generally in the 4 – 8 range, depending on lens, where sharpness is optimal, though some lenses are designed to perform optimally when wide open. How significant this is varies between lenses, and opinions differ on how much practical impact this has.
While optimal aperture can be determined mechanically, how much sharpness is "required" depends on how the image will be used – if the final image is viewed under normal conditions (e.g., an 8″×10″ image viewed at 10″), it may suffice to determine the f-number using criteria for minimum required sharpness, and there may be no practical benefit from further reducing the size of the blur spot. But this may not be true if the final image is viewed under more demanding conditions, e.g., a very large final image viewed at normal distance, or a portion of an image enlarged to normal size (Hansma 1996). Hansma also suggests that the final-image size may not be known when a photograph is taken, and obtaining the maximum practicable sharpness allows the decision to make a large final image to be made at a later time; see also critical sharpness.
Equivalent aperture range.
In digital photography, the 35mm-equivalent aperture range is sometimes considered to be more important than the actual f-number. Equivalent aperture is the f-number adjusted to correspond to the f-number of the same size absolute aperture diameter on a lens with a 35mm equivalent focal length. Smaller equivalent f-numbers are expected to lead to higher image quality based on more total light from the subject, as well as lead to reduced depth of field. For example, a Sony Cyber-shot DSC-RX10 uses a 1" sensor, 24–200 mm with maximum aperture constant along the zoom range; 2.8 has equivalent aperture range 7.6, which is a lower equivalent f-number than some other 2.8 cameras with smaller sensors.
In scanning or sampling.
The terms "scanning aperture" and "sampling aperture" are often used to refer to the opening through which an image is sampled, or scanned, for example in a Drum scanner, an image sensor, or a television pickup apparatus. The sampling aperture can be a literal optical aperture, that is, a small opening in space, or it can be a time-domain aperture for sampling a signal waveform.
For example, film grain is quantified as "graininess" via a measurement of film density fluctuations as seen through a 0.048 mm sampling aperture.

</doc>
<doc id="47476" url="https://en.wikipedia.org/wiki?curid=47476" title="Altimeter">
Altimeter

An altimeter or an altitude meter is an instrument used to measure the altitude of an object above a fixed level. The measurement of altitude is called altimetry, which is related to the term bathymetry, the measurement of depth under water.
Pressure altimeter.
Altitude can be determined based on the measurement of atmospheric pressure. The greater the altitude, the lower the pressure. When a barometer is supplied with a nonlinear calibration so as to indicate altitude, the instrument is called a pressure altimeter or barometric altimeter. A pressure altimeter is the altimeter found in most aircraft, and skydivers use wrist-mounted versions for similar purposes. Hikers and mountain climbers use wrist-mounted or hand-held altimeters, in addition to other navigational tools such as a map, magnetic compass, or GPS receiver.
The calibration of an altimeter follows the equation
where c is a constant, T is the absolute temperature, P is the pressure at altitude z, and Po is the pressure at sea level. The constant c depends on the acceleration of gravity and the molar mass of the air.
However, one must be aware that this type of altimeter relies on "density altitude" and its readings can vary by hundreds of feet owing to a sudden change in air pressure, such as from a cold front, without any actual change in altitude.
Use in hiking and climbing.
A barometric altimeter, used along with a topographic map, can help to verify one's location. It is more reliable, and often more accurate, than a GPS receiver for measuring altitude; the GPS signal may be unavailable, for example, when one is deep in a canyon, or it may give wildly inaccurate altitudes when all available satellites are near the horizon. Because barometric pressure changes with the weather, hikers must periodically re-calibrate their altimeters when they reach a known altitude, such as a trail junction or peak marked on a topographical map.
Skydiving.
An altimeter is the most important piece of skydiving equipment, after the parachute itself. Altitude awareness is crucial at all times during the jump, and determines the appropriate response to maintain safety.
Since altitude awareness is so important in skydiving, there is a wide variety of altimeter designs made specifically for use in the sport, and a non-student skydiver will typically use two or more altimeters in a single jump:
The exact choice of altimeters depends heavily on the individual skydiver's preferences, experience level, primary disciplines, as well as the type of the jump. On one end of the spectrum, a low-altitude demonstration jump with water landing and no free fall might waive the mandated use of altimeters and use none at all. In contrast, a jumper doing freeflying jumps and flying a high performance canopy might use a mechanical analogue altimeter for easy reference in free fall, an in-helmet audible for breakaway altitude warning, additionally programmed with swoop guide tones for canopy flying, as well as a digital altimeter on an armband for quickly glancing the precise altitude on approach. Another skydiver doing similar types of jumps might wear a digital altimeter for their primary visual one, preferring the direct altitude readout of a numeric display.
Use in aircraft.
In aircraft, an aneroid barometer measures the atmospheric pressure from a static port outside the aircraft. Air pressure decreases with an increase of altitude—approximately 100 hectopascals per 800 meters or one inch of mercury per 1000 feet near sea level.
The aneroid altimeter is calibrated to show the pressure directly as an altitude above mean sea level, in accordance with a mathematical model atmosphere defined by the International Standard Atmosphere (ISA). Older aircraft used a simple aneroid barometer where the needle made less than one revolution around the face from zero to full scale. This design evolved to altimeters with a primary needle and one or more secondary needles that show the number of revolutions, similar to a clock face. In other words, each needle points to a different digit of the current altitude measurement. However this design has fallen out of favor due to the risk of misreading in stressful situations. The design evolved further to drum-type altimeters, the final step in analogue instrumentation, where each revolution of a single needle accounted for 1,000 feet, with thousand foot increments recorded on a numerical odometer-type drum. To determine altitude, a pilot had first to read the drum to determine the thousands of feet, then look at the needle for the hundreds of feet. Modern analogue altimeters in transport aircraft are typically drum-type. The latest development in clarity is an Electronic flight instrument system with integrated digital altimeter displays. This technology has trickled down from airliners and military planes until it is now standard in many general aviation aircraft.
Modern aircraft use a "sensitive altimeter". On a sensitive altimeter, the sea-level reference pressure can be adjusted with a setting knob. The reference pressure, in inches of mercury in Canada and the United States, and hectopascals (previously millibars) elsewhere, is displayed in the small "Kollsman window," on the face of the aircraft altimeter. This is necessary, since sea level reference atmospheric pressure at a given location varies over time with temperature and the movement of pressure systems in the atmosphere.
In aviation terminology, the regional or local air pressure at mean sea level (MSL) is called the QNH or "altimeter setting", and the pressure that will calibrate the altimeter to show the height above ground at a given airfield is called the QFE of the field. An altimeter cannot, however, be adjusted for variations in air temperature. Differences in temperature from the ISA model will accordingly cause errors in indicated altitude.
In aerospace, the mechanical stand-alone altimeters which are based on diaphragm bellows were replaced by integrated measurement systems which are called air data computers (ADC). This module measures altitude, speed of flight and outside temperature to provide more precise output data allowing automatic flight control and flight level division. Multiple altimeters can be used to design a pressure reference system to provide information about the airplane's position angles to further support inertial navigation system calculations.
Use in ground effect vehicle.
After extensive research and experimentation, it has been shown that "phase radio-altimeters" are most suitable for ground effect vehicles, as compared to laser, isotropic or ultrasonic altimeters.
Sonic altimeter.
In 1931, the US Army Air Corps and General Electric tested a sonic altimeter for aircraft, which was considered more reliable and accurate than one that relied on air pressure, when heavy fog or rain was present. The new altimeter used a series of high-pitched sounds like those made by a bat to measure the distance from the aircraft to the surface, which on return to the aircraft was converted to feet shown on a gauge inside the aircraft cockpit.
Radar altimeter.
A radar altimeter measures altitude more directly, using the time taken for a radio signal to reflect from the surface back to the aircraft. Alternatively, Frequency Modulated Continuous-wave radar can be used. The greater the frequency shift the further the distance traveled. This method can achieve much better accuracy than the pulsed radar for the same outlay and radar altimeters that use frequency modulation are industry standard. The radar altimeter is used to measure height above ground level during landing in commercial and military aircraft. Radar altimeters are also a component of terrain avoidance warning systems, warning the pilot if the aircraft is flying too low, or if there is rising terrain ahead. Radar altimeter technology is also used in terrain-following radar allowing fighter aircraft to fly at very low altitude.
Global Positioning System.
Global Positioning System (GPS) receivers can also determine altitude by trilateration with four or more satellites. In aircraft, altitude determined using autonomous GPS is not reliable enough to supersede the pressure altimeter without using some method of augmentation. In hiking and climbing, it is not uncommon to find that the altitude measured by GPS is off by as much as 400 feet depending on satellite orientation.
Other modes of transport.
The altimeter is an instrument optional in off-road vehicles to aid in navigation. Some high-performance luxury cars that were never intended to leave paved roads, such as the Duesenberg in the 1930s, have also been equipped with altimeters.
Hikers and mountaineers use hand-held or wrist-mounted barometric altimeters, as do skydivers.
Diesel submarines have barometers installed on them to monitor vacuum being pulled in the event that the snorkel closes while the diesels are running and, as a consequence, sucking the air out of the boat.
Satellites.
Satellites such as Seasat and TOPEX/Poseidon use advanced dual-band radar altimeters to measure height from a spacecraft. That measurement, coupled with orbital elements (possibly augmented by GPS), enables determination of the terrain. The two different wavelengths of radio waves used permit the altimeter to automatically correct for varying delays in the ionosphere.
Spaceborne radar altimeters have proven to be superb tools for mapping ocean-surface topography, the hills and valleys of the sea surface. These instruments send a microwave pulse to the ocean's surface and record the time it takes to return. A microwave radiometer corrects any delay that may be caused by water vapor in the atmosphere. Other corrections are also required to account for the influence of electrons in the ionosphere and the dry air mass of the atmosphere. Combining these data with the precise location of the spacecraft makes it possible to determine sea-surface height to within a few centimeters (about one inch). The strength and shape of the returning signal also provides information on wind speed and the height of ocean waves. These data are used in ocean models to calculate the speed and direction of ocean currents and the amount and location of heat stored in the ocean, which in turn reveals global climate variations.

</doc>
<doc id="47477" url="https://en.wikipedia.org/wiki?curid=47477" title="Ames Research Center">
Ames Research Center

Ames Research Center (ARC), commonly known as NASA Ames, is a major NASA research center at Moffett Federal Airfield in California's Silicon Valley. Originally founded as the second National Advisory Committee for Aeronautics (NACA) laboratory, that agency was dissolved and its assets and personnel transferred to the newly created National Aeronautics and Space Administration (NASA) on October 1, 1958. NASA Ames is named in honor of Joseph Sweetman Ames, a physicist and one of the founding members of NACA. At last estimate NASA Ames has over US$3.0 billion in capital equipment, 2,300 research personnel and a US$860 million annual budget.
Ames was originally founded to conduct wind-tunnel research on the aerodynamics of propeller-driven aircraft; however, its role has expanded to encompass spaceflight and information technology. Ames plays a role in many NASA missions. It provides leadership in astrobiology; small satellites; robotic lunar exploration; the search for habitable planets; supercomputing; intelligent/adaptive systems; advanced thermal protection; and airborne astronomy. Ames also develops tools for a safer, more efficient national airspace. The center's current director is Eugene Tu.
The site is mission center for several key current missions ("Kepler", the "Lunar Crater Observation and Sensing Satellite" (LCROSS) mission, "Stratospheric Observatory for Infrared Astronomy" (SOFIA), Interface Region Imaging Spectrograph) and a major contributor to the "new exploration focus" as a participant in the Orion crew exploration vehicle.
Missions.
Although Ames is a NASA Research Center, and not a flight center, it has nevertheless been closely involved in a number of astronomy and space missions.
The Pioneer program's eight successful space missions from 1965 to 1978 were managed by Charles Hall at Ames, initially aimed at the inner solar system. By 1972, it supported the bold flyby missions to Jupiter and Saturn with Pioneer 10 and Pioneer 11. Those two missions were trail blazers (radiation environment, new moons, gravity-assist flybys) for the planners of the more complex Voyager 1 and Voyager 2 missions, launched five years later. In 1978, the end of the program saw a return to the inner solar system, with the Pioneer Venus Orbiter and Multiprobe, this time using orbital insertion rather than flyby missions.
Lunar Prospector was the third mission selected by NASA for full development and construction as part of the Discovery Program. At a cost of $62.8 million, the 19-month mission was put into a low polar orbit of the Moon, accomplishing mapping of surface composition and possible polar ice deposits, measurements of magnetic and gravity fields, and study of lunar outgassing events. Based on Lunar Prospector Neutron Spectrometer (NS) data, mission scientists have determined that there is indeed water ice in the polar craters of the Moon. The mission ended July 31, 1999 when the orbiter was guided to an impact into a crater near the lunar south pole in an (unsuccessful) attempt to analyze lunar polar water by vaporizing it to allow spectroscopic characterization from Earth telescopes.
The 11-pound (5 kg) GeneSat-1, carrying bacteria inside a miniature laboratory, was launched on December 16, 2006. The very small NASA satellite has proven that scientists can quickly design and launch a new class of inexpensive spacecraft—and conduct significant science.
The "Lunar CRater Observation and Sensing Satellite" (LCROSS) mission to look for water on the moon was a 'secondary payload spacecraft.' LCROSS began its trip to the moon on the same rocket as the Lunar Reconnaissance Orbiter (LRO), which continues to conduct a different lunar task. It launched in April 2009 on an Atlas V rocket from Kennedy Space Center, Florida.
"Kepler" is NASA's first mission capable of finding Earth-size and smaller planets. The Kepler mission will monitor the brightness of stars to find planets that pass in front of them during the planets' orbits. During such passes or 'transits,' the planets will slightly decrease the star's brightness.
"Stratospheric Observatory for Infrared Astronomy" (SOFIA) is a joint venture of the U.S. and German aerospace agencies, NASA and the DLR to make an infrared telescope platform that can fly at altitudes high enough to be in the infrared-transparent regime above the water vapor in the Earth's atmosphere. The aircraft is supplied by the U.S., and the infrared telescope by Germany. Modifications of the Boeing 747SP airframe to accommodate the telescope, mission-unique equipment and large external door were made by L-3 Communications Integrated Systems of Waco, Texas.
The Interface Region Imaging Spectrograph mission is a partnership with the Lockheed Martin Solar and Astrophysics Laboratory to understand the processes at the boundary between the Sun's chromosphere and corona. This mission is sponsored by the NASA Small Explorer program.
The "Lunar Atmosphere Dust Environment Explorer" (LADEE) mission has been developed by NASA Ames. This successfully launched to the Moon on September 6, 2013.
In addition, Ames has played a support role in a number of missions, most notably the Mars Pathfinder and Mars Exploration Rover missions, where the Ames Intelligent Robotics Laboratory played a key role. NASA Ames was a partner on the Mars "Phoenix", a Mars Scout Program mission to send a high-latitude lander to Mars, deployed a robotic arm to dig trenches up to 1.6 feet (one half meter) into the layers of water ice and analyzing the soil composition. Ames is also a partner on the Mars Science Laboratory, a next generation Mars rover to explore for signs of organics and complex molecules.
Air traffic control automation research.
The Aviation Systems Division conducts research and development in two primary areas: air traffic management, and high-fidelity flight simulation. For air traffic management, researchers are creating and testing concepts to allow for up to three times today's level of aircraft in the national airspace. Automation and its attendant safety consequences are key foundations of the concept development. Historically, the division has developed products that have subsequently been implemented for the flying public, such as the Traffic Management Advisor, which is being deployed nationwide. For high-fidelity flight simulation, the division operates the world's largest flight simulator (the Vertical Motion Simulator), a Level-D 747-400 simulator, and a panoramic air traffic control tower simulator. These simulators have been used for a variety of purposes including continued training for space shuttle pilots, development of future spacecraft handling qualities, helicopter control system testing, Joint Strike Fighter evaluations, and accident investigations. Personnel in the division have a variety of technical backgrounds, including guidance and control, flight mechanics, flight simulation, and computer science. Customers outside NASA have included the FAA, DOD, DHS, DOT, NTSB, Lockheed Martin, and Boeing.
Information technology.
Ames is the home of NASA's large research and development divisions in Advanced Supercomputing, "Human Factors", and Artificial Intelligence ( Intelligent Systems). These Research & Development organizations support NASA's Exploration efforts, as well as the continued operations of the International Space Station, and the space science and Aeronautics work across NASA. The center also runs and maintains the E Root nameserver of the DNS System.
The Intelligent Systems Division is NASA's leading R&D Division developing advanced intelligent software and systems for all of NASA Mission Directorates. It provides software expertise for aeronautics, space science missions, International Space Station, and the Crewed Exploration Vehicle (CEV. The first AI in space (Deep Space 1) was developed from Code TI, as is the MAPGEN software that daily plans the activities for the Mars Exploration Rovers, the same core reasoner is used for Ensemble to operate Phoenix Lander, and the planning system for the International Space Station's solar arrays. Integrated System Health Management for the International Space Station's control moment gyroscopes, collaborative systems with semantic search tools, and robust software engineering round out the scope of Code TI's work.
The Human Systems Integration Division "advances human-centered design and operations of complex aerospace systems through analysis, experimentation, and modeling of human performance and human-automation interaction to make dramatic improvements in safety, efficiency and mission success". For decades, the Human Systems Integration Division has been on the leading edge of human-centered aerospace research. The Division is home to over 100 researchers, contractors and administrative staff.
Ames operates one of the world′s fastest supercomputers, Pleiades, which will be further enhanced and is scheduled to reach 10 petaflops of processing power by 2012.
In September 2009, Ames launched NEBULA as a fast and powerful Cloud Computing Platform to handle NASA's massive data sets that complied with security requirements. This innovative pilot uses open-source components, complies with FISMA and can scale to Government-sized demands while being extremely energy efficient. In July 2010, NASA CTO Chris C. Kemp open sourced Nova, the technology behind the NEBULA Project in collaboration with Rackspace, launching OpenStack. OpenStack has subsequently become one of the largest and fastest growing open source projects in the history of computing, and has been included in most major distributions of linux including Red Hat, Oracle, HP, SUSE, and Canonical.
Image processing.
NASA Ames was one of the first locations in the world to conduct research on image processing of satellite-platform aerial photography. Some of the pioneering techniques of contrast enhancement using Fourier analysis were developed at Ames in conjunction with researchers at ESL Inc.
Wind tunnels.
The NASA Ames Research Center wind tunnels are known not only for their immense size, but also for their diverse characteristics that enable various kinds of scientific and engineering research.
ARC Unitary Plan Wind Tunnel.
The Unitary Plan Wind Tunnel (UPWT) was completed in 1956 at a cost of $27 million under the Unitary Plan Act of 1949. Since its completion, the UPWT facility has been the most heavily used NASA wind tunnel. Every major commercial transport and almost every military jet built in the United States over the last 40 years has been tested in this facility. Mercury, Gemini, and Apollo spacecraft space shuttle models were also tested in this tunnel complex.
National Full-Scale Aerodynamics Complex (NFAC).
Ames Research Center also houses the world's largest wind tunnel, part of the National Full-Scale Aerodynamic Complex (NFAC): it is large enough to test full-sized planes, rather than scale models.
The 40 by 80 foot wind tunnel circuit was originally constructed in the 1940s and is now capable of providing test velocities up to . It is used to support an active research program in aerodynamics, dynamics, model noise, and full-scale aircraft and their components. The aerodynamic characteristics of new configurations are investigated with an emphasis on estimating the accuracy of computational methods. Aeromechanical stability boundaries of advanced rotorcraft and rotor-fuselage interactions are explored. Stability and control derivatives are also determined, including the static and dynamic characteristics of new aircraft configurations. The acoustic characteristics of most of the full-scale vehicles are also determined, as well as acoustic research aimed at discovering and reducing aerodynamic sources of noise. In addition to the normal data gathering methods (e.g., balance system, pressure measuring transducers, and temperature sensing thermocouples), state-of-the-art, non-intrusive instrumentation (e.g., laser velocimeters and shadowgraphs) are available to help determine flow direction and velocity in and around the lifting surfaces of models or aircraft undergoing investigation. The 40 by 80 Foot Wind Tunnel is primarily used for determining the low- and medium-speed aerodynamic characteristics of high-performance aircraft, rotorcraft, and fixed wing, powered-lift V/STOL aircraft.
The 80 by 120 Foot Wind Tunnel at NASA Ames Research Center is the largest wind tunnel test section in the world. This open circuit leg was added and a new fan drive system was installed in the 1980s. The 80 by 120 Foot Wind Tunnel is used to support an active research program in aerodynamics, dynamics, model noise, and full-scale aircraft and their components. The aerodynamic characteristics of new configurations are investigated, with an emphasis on estimating the accuracy of computational methods. Aeromechanical stability boundaries of advanced rotorcraft and rotor-fuselage interactions are explored. The acoustic characteristics of most of the full-scale vehicles are also determined, as well as acoustic research aimed at discovering and reducing aerodynamic sources of noise. In addition to the normal data gathering methods (e.g., balance system, pressure measuring transducers, and temperature sensing thermocouples), state-of-the-art non-intrusive instrumentation (e.g., laser velocimeters and shadowgraphs) are available to help determine flow direction and velocity in and around the lifting surfaces of models or aircraft undergoing investigation. Some of the test programs that have come through the 80 by 120 Foot include: F-18 High Angle of Attack Vehicle, DARPA/Lockheed Common Affordable Lightweight Fighter, XV-15 Tilt Rotor, and Advance Recovery System Parafoil. The 80 by 120 foot test section is capable of testing a full size Boeing 737 at velocities up to .
Although decommissioned by NASA in 2003, the NFAC is now being operated by the United States Air Force as a satellite facility of the Arnold Engineering Development Complex (AEDC).
Arc Jet Complex.
The Ames Arc Jet Complex is an advanced thermophysics facility where sustained hypersonic- and hyperthermal testing of vehicular thermoprotective systems takes place under a variety of simulated flight- and re-entry conditions. Of its seven available test bays, four currently contain Arc Jet units of differing configurations, serviced by common facility support equipment. These are the Aerodynamic Heating Facility (AHF), the Turbulent Flow Duct (TFD), the Panel Test Facility (PTF), and the Interaction Heating Facility (IHF). The support equipment includes two D.C. power supplies, a steam ejector-driven vacuum system, a water-cooling system, high-pressure gas systems, data acquisition system, and other auxiliary systems.
The magnitude and capacity of these systems makes the Ames Arc Jet Complex unique in the world. The largest power supply can deliver 75 megawatts (MW) for a 30-minute duration or 150 MW for a 15-second duration. This power capacity, in combination with a high-volume 5-stage steam ejector vacuum-pumping system, enables facility operations to match high-altitude atmospheric flight conditions with samples of relatively large size. The Thermo-Physics Facilities Branch operates four arc jet facilities. The Interaction Heating Facility (IHF), with an available power of over 60-MW, is one of the highest-power arc jets available. It is a very flexible facility, capable of long run times of up to one hour, and able to test large samples in both a stagnation and flat plate configuration. The Panel Test Facility (PTF) uses a unique semielliptic nozzle for testing panel sections. Powered by a 20-MW arc heater, the PTF can perform tests on samples for up to 20 minutes. The Turbulent Flow Duct provides supersonic, turbulent high temperature air flows over flat surfaces. The TFD is powered by a 20-MW Hüls arc heater and can test samples in size. The Aerodynamic Heating Facility (AHF) has similar characteristics to the IHF arc heater, offering a wide range of operating conditions, samples sizes and extended test times. A cold-air-mixing plenum allows for simulations of ascent or high-speed flight conditions. Catalycity studies using air or nitrogen can be performed in this flexible rig. A 5-arm model support system allows the user to maximize testing efficiency. The AHF can be configured with either a Hüls or segmented arc heater, up to 20-MW. 1 MW is enough power to supply 750 homes.
Range complex.
Ames Vertical Gun Range.
The Ames Vertical Gun Range (AVGR) was designed to conduct scientific studies of lunar impact processes in support of the Apollo missions. In 1979, it was established as a National Facility, funded through the Planetary Geology and Geophysics Program. In 1995, increased scientific needs across various disciplines resulted in joint core funding by three different science programs at NASA Headquarters (Planetary Geology and Geophysics, Exobiology, and Solar System Origins). In addition, the AVGR provides programmatic support for various proposed and ongoing planetary missions (e.g. Stardust, Deep Impact).
Using its 0.30 cal light-gas gun and powder gun, the AVGR can launch projectiles to velocities ranging from . By varying the gun’s angle of elevation with respect to the target vacuum chamber, impact angles from 0° to 90° relative to the gravitational vector are possible. This unique feature is extremely important in the study of crater formation processes.
The target chamber is approximately in diameter and height and can accommodate a wide variety of targets and mounting fixtures. It can maintain vacuum levels below , or can be back filled with various gases to simulate different planetary atmospheres. Impact events are typically recorded with high-speed video/film, or Particle Image Velocimetry (PIV).
Hypervelocity Free-Flight Range.
The Hypervelocity Free-Flight (HFF) Range currently comprises two active facilities: the Aerodynamic Facility (HFFAF) and the Gun Development Facility (HFFGDF). The HFFAF is a combined Ballistic Range and Shock-tube Driven Wind Tunnel. Its primary purpose is to examine the aerodynamic characteristics and flow-field structural details of free-flying aeroballistic models.
The HFFAF has a test section equipped with 16 shadowgraph-imaging stations. Each station can be used to capture an orthogonal pair of images of a hypervelocity model in flight. These images, combined with the recorded flight time history, can be used to obtain critical aerodynamic parameters such as lift, drag, static and dynamic stability, flow characteristics, and pitching moment coefficients. For very high Mach number (M > 25) simulations, models can be launched into a counter-flowing gas stream generated by the shock tube. The facility can also be configured for hypervelocity impact testing and has an aerothermodynamic capability as well. The HFFAF is currently configured to operate the light-gas gun in support of continuing thermal imaging and transition research for NASA's hypersonics program.
The HFFGDF is used for gun performance enhancement studies, and occasional impact testing. The Facility uses the same arsenal of light-gas and powder guns as the HFFAF to accelerate particles that range in size from diameter to velocities ranging from 0.5 to 8.5 km/s (1,500 to 28,000 ft/s). Most of the research effort to date has centered on Earth atmosphere entry configurations (Mercury, Gemini, Apollo, and Shuttle), planetary entry designs (Viking, Pioneer Venus, Galileo and MSL), and aerobraking (AFE) configurations. The facility has also been used for scramjet propulsion studies (National Aerospace Plane (NASP)) and meteoroid/orbital debris impact studies (Space Station and RLV). In 2004, the facility was utilized for foam-debris dynamics testing in support of the Return To Flight effort. As of March 2007, the GDF has been reconfigured to operate a cold gas gun for subsonic CEV capsule aerodynamics.
Electric Arc Shock Tube.
The Electric Arc Shock Tube (EAST) Facility is used to investigate the effects of radiation and ionization that occur during very high velocity atmospheric entries. In addition, the EAST can also provide air-blast simulations requiring the strongest possible shock generation in air at an initial pressure loading of or greater. The facility has three separate driver configurations, to meet a range of test requirements: the driver can be connected to a diaphragm station of either a or a shock tube, and the high-pressure shock tube can also drive a shock tunnel. Energy for the drivers is supplied by a 1.25-MJ-capacitor storage system.
Education.
NASA Ames Exploration Center.
The NASA Ames Exploration Center is a science museum and education center for NASA. There are displays and interactive exhibits about NASA technology, missions and space exploration. A moon rock, meteorite and other geologic samples are on display. The theater shows movies with footage from NASA's explorations of Mars and the planets, and about the contributions of the scientists at Ames. 
Robotics Alliance Project.
In 1999, Mark León developed NASA's Robotics Education Project — now called the Robotics Alliance Project — under his mentor Dave Lavery, which has reached over 100,000 students nationwide using FIRST robotics and BOTBALL robotics competitions. The Project's FIRST branch originally comprised FRC Team 254: "The Cheesy Poofs", an all-male team from Bellarmine High School in San José, California. In 2006, Team 1868: "The Space Cookies", an all-female team, was founded in collaboration with the Girl Scouts. In 2012, Team 971: "Spartan Robotics" of Mountain View High School joined the Project, though the team continues to operate at their school. All three teams are highly decorated. All three have won Regional competitions, two have won the FIRST Championship, two of which have won the Regional Chairman's Award, and one is a Hall of Fame team. The three teams are collectively referred to as "House teams".
The mission of the project is "To create a human, technical, and programmatic resource of robotics capabilities to enable the implementation of future robotic space exploration missions."
Recent events.
Although the Bush administration slightly increased funding
for NASA overall, the substantial realignment in research priorities that followed the announcement of the Vision for Space Exploration in 2004 led to a significant number of layoffs at Ames.
On October 22, 2006, NASA opened the Carl Sagan Center for the Study of Life in the Cosmos. The center continued work that Sagan undertook, including the Search for Extraterrestrial Intelligence.
In 2008, the Lunar Orbiter Image Recovery Project (LOIRP) was given space in the old McDonald's (the building was renamed McMoons) to digitize data tapes from the five 1966 and 1967 Lunar Orbiter spacecraft that were sent to the Moon.
Also in 2008, it was announced that former Ames director Henry McDonald was a 60th Anniversary Class inductee of the Ames Hall of Fame for providing, "...exceptional leadership and keen technical insight to NASA Ames as the Center re-invented itself in the late 1990s."
In 2010, scientists at the Fluid Mechanics Laboratory at Ames studied the aerodynamics of the Jabulani World Cup soccer ball, concluding that it tends to "knuckle under" at speeds of . Aerospace engineer Rabi Mehta attributed this effect to asymmetric flow due to the ball's seam construction.
In March 2015, scientists at Ames announced that they had synthesized "...uracil, cytosine, and thymine, all three components of RNA and DNA, non-biologically in a laboratory under conditions found in space." 
Public-private partnerships.
The federal government has re-tasked portions of the facility and human resources to support private sector industry, research, and education.
HP became the first corporate affiliate of a new Bio-Info-Nano Research and Development Institute (BIN-RDI); a collaborative venture established by the University of California Santa Cruz and NASA, based at Ames. The Bio|Info|Nano R&D Institute is dedicated to creating scientific breakthroughs by the convergence of biotechnology, information technology, and nanotechnology.
Singularity University hosts its leadership and educational program at the facility. The Organ Preservation Alliance[http://www.organpreservationalliance.org/] is also headquartered there; the Alliance is a nonprofit organization that works in partnership with the Methuselah Foundation's New Organ Prize "to catalyze breakthroughs on the remaining obstacles towards the long-term storage of organs" to overcome the drastic unmet medical need for viable organs for transplantation. Kleenspeed Technologies is headquartered there.
Google.
On September 28, 2005, both Google and Ames Research Center disclosed details to a long-term research partnership. In addition to pooling engineering talent, Google planned to build a facility on the ARC campus. One of the projects between Ames, Google, and Carnegie Mellon University is the Gigapan Project—a robotic platform for creating, sharing, and annotating terrestrial gigapixel images. The Planetary Content Project seeks to integrate and improve the data that Google uses for its Google Moon and Google Mars projects. On June 4, 2008 Google announced it had leased from NASA, at Moffett Field, for use as office space and employee housing.
Construction of the new Google project which is near Google's Googleplex headquarters began in 2013 and has a target opening date of 2015. It is called "Bay View" as it overlooks San Francisco Bay.
In May 2013, Google Inc announced that it was launching the Quantum Artificial Intelligence Lab, to be hosted by NASA’s Ames Research Center. The lab will house a 512-qubit quantum computer from D-Wave Systems, and the USRA (Universities Space Research Association) will invite researchers from around the world to share time on it. The goal being to study how quantum computing might advance machine learning.
Announced on 10 November 2014, Planetary Ventures LLC (a Google subsidiary) will lease the Moffett Federal Airfield from NASA Ames, a site of about 1,000 acres formerly costing the agency $6.3 million annually in maintenance and operation costs. The lease includes the restoration of the site's historic landmark Hangar One, as well as hangars Two and Three. The lease went into effect in March 2015, and spans 60 years.
Living and working at Ames.
There are a myriad of activities inside the research center and around for full-time workers and interns alike. The website new2nasa.wikispaces.com offers updated information. There was a Parcourse trail, also known as a fitness trail inside the base, but sections of it are inaccessible due to changes in base layout since it was installed. An official NASA ID is required to enter Ames.

</doc>
<doc id="47481" url="https://en.wikipedia.org/wiki?curid=47481" title="Aquifer">
Aquifer

An aquifer is an underground layer of water-bearing permeable rock, rock fractures or unconsolidated materials (gravel, sand, or silt) from which groundwater can be extracted using a water well. The study of water flow in aquifers and the characterization of aquifers is called hydrogeology. Related terms include aquitard, which is a bed of low permeability along an aquifer, and aquiclude (or "aquifuge"), which is a solid, impermeable area underlying or overlying an aquifer. If the impermeable area overlies the aquifer, pressure could cause it to become a confined aquifer.
Depth.
Aquifers may occur at various depths. Those closer to the surface are not only more likely to be used for water supply and irrigation, but are also more likely to be topped up by the local rainfall. Many desert areas have limestone hills or mountains within them or close to them that can be exploited as groundwater resources. Part of the Atlas Mountains in North Africa, the Lebanon and Anti-Lebanon ranges between Syria and Lebanon, the Jebel Akhdar (Oman) in Oman, parts of the Sierra Nevada and neighboring ranges in the United States' Southwest, have shallow aquifers that are exploited for their water. Overexploitation can lead to the exceeding of the practical sustained yield; i.e., more water is taken out than can be replenished. Along the coastlines of certain countries, such as Libya and Israel, increased water usage associated with population growth has caused a lowering of the water table and the subsequent contamination of the groundwater with saltwater from the sea.
The beach provides a model to help visualize an aquifer. If a hole is dug into the sand, very wet or saturated sand will be located at a shallow depth. This hole is a crude well, the wet sand represents an aquifer, and the level to which the water rises in this hole represents the water table.
In 2013 large freshwater aquifers were discovered under continental shelves off Australia, China, North America and South Africa. They contain an estimated half a million cubic kilometers of “low salinity” water that could be economically processed into potable water. The reserves formed when ocean levels were lower and rainwater made its way into the ground in land areas that were not submerged until the ice age ended 20,000 years ago. The volume is estimated to be 100x the amount of water extracted from other aquifers since 1900.
Classification.
The above diagram indicates typical flow directions in a cross-sectional view of a simple confined or unconfined aquifer system. The system shows two aquifers with one aquitard (a confining or impermeable layer) between them, surrounded by the bedrock "aquiclude", which is in contact with a gaining stream (typical in humid regions). The water table and unsaturated zone are also illustrated. 
An "aquitard" is a zone within the earth that restricts the flow of groundwater from one aquifer to another. An aquitard can sometimes, if completely impermeable, be called an "aquiclude" or "aquifuge". Aquitards are composed of layers of either clay or non-porous rock with low hydraulic conductivity.
Saturated versus unsaturated.
Groundwater can be found at nearly every point in the Earth's shallow subsurface to some degree, although aquifers do not necessarily contain fresh water. The Earth's crust can be divided into two regions: the "saturated zone" or "phreatic zone" (e.g., aquifers, aquitards, etc.), where all available spaces are filled with water, and the "unsaturated zone" (also called the vadose zone), where there are still pockets of air that contain some water, but can be filled with more water.
Saturated means the pressure head of the water is greater than atmospheric pressure (it has a gauge pressure > 0). The definition of the water table is the surface where the pressure head is equal to atmospheric pressure (where gauge pressure = 0).
Unsaturated conditions occur above the water table where the pressure head is negative (absolute pressure can never be negative, but gauge pressure can) and the water that incompletely fills the pores of the aquifer material is under suction. The water content in the unsaturated zone is held in place by surface adhesive forces and it rises above the water table (the zero-gauge-pressure isobar) by capillary action to saturate a small zone above the phreatic surface (the capillary fringe) at less than atmospheric pressure. This is termed tension saturation and is not the same as saturation on a water-content basis. Water content in a capillary fringe decreases with increasing distance from the phreatic surface. The capillary head depends on soil pore size. In sandy soils with larger pores, the head will be less than in clay soils with very small pores. The normal capillary rise in a clayey soil is less than 1.80 m (six feet) but can range between 0.3 and 10 m (one and 30 ft).
The capillary rise of water in a small-diameter tube involves the same physical process. The water table is the level to which water will rise in a large-diameter pipe (e.g., a well) that goes down into the aquifer and is open to the atmosphere.
Aquifers versus aquitards.
Aquifers are typically saturated regions of the subsurface that produce an economically feasible quantity of water to a well or spring (e.g., sand and gravel or fractured bedrock often make good aquifer materials).
An aquitard is a zone within the earth that restricts the flow of groundwater from one aquifer to another. A completely impermeable aquitard is called an aquiclude or aquifuge. Aquitards comprise layers of either clay or non-porous rock with low hydraulic conductivity.
In mountainous areas (or near rivers in mountainous areas), the main aquifers are typically unconsolidated alluvium, composed of mostly horizontal layers of materials deposited by water processes (rivers and streams), which in cross-section (looking at a two-dimensional slice of the aquifer) appear to be layers of alternating coarse and fine materials. Coarse materials, because of the high energy needed to move them, tend to be found nearer the source (mountain fronts or rivers), whereas the fine-grained material will make it farther from the source (to the flatter parts of the basin or overbank areas - sometimes called the pressure area). Since there are less fine-grained deposits near the source, this is a place where aquifers are often unconfined (sometimes called the forebay area), or in hydraulic communication with the land surface.
Confined versus unconfined.
There are two end members in the spectrum of types of aquifers; "confined" and "unconfined" (with semi-confined being in between). Unconfined aquifers are sometimes also called "water table" or "phreatic" aquifers, because their upper boundary is the water table or phreatic surface. (See Biscayne Aquifer.) Typically (but not always) the shallowest aquifer at a given location is unconfined, meaning it does not have a confining layer (an aquitard or aquiclude) between it and the surface. The term "perched" refers to ground water accumulating above a low-permeability unit or strata, such as a clay layer. This term is generally used to refer to a small local area of ground water that occurs at an elevation higher than a regionally extensive aquifer. The difference between perched and unconfined aquifers is their size (perched is smaller). Confined aquifers are aquifers that are overlain by a confining layer, often made up of clay. The confining layer might offer some protection from surface contamination.
If the distinction between confined and unconfined is not clear geologically (i.e., if it is not known if a clear confining layer exists, or if the geology is more complex, e.g., a fractured bedrock aquifer), the value of storativity returned from an aquifer test can be used to determine it (although aquifer tests in unconfined aquifers should be interpreted differently than confined ones). Confined aquifers have very low storativity values (much less than 0.01, and as little as 10−5), which means that the aquifer is storing water using the mechanisms of aquifer matrix expansion and the compressibility of water, which typically are both quite small quantities. Unconfined aquifers have storativities (typically then called specific yield) greater than 0.01 (1% of bulk volume); they release water from storage by the mechanism of actually draining the pores of the aquifer, releasing relatively large amounts of water (up to the drainable porosity of the aquifer material, or the minimum volumetric water content).
Isotropic versus anisotropic.
In isotropic aquifers or aquifer layers the hydraulic conductivity (K) is equal for flow in all directions, while in anisotropic conditions it differs, notably in horizontal (Kh) and vertical (Kv) sense.
Semi-confined aquifers with one or more aquitards work as an anisotropic system, even when the separate layers are isotropic, because the compound Kh and Kv values are different (see hydraulic transmissivity and hydraulic resistance).
When calculating flow to drains or flow to wells in an aquifer, the anisotropy is to be taken into account lest the resulting design of the drainage system may be faulty.
Groundwater in rock formations.
Groundwater may exist in "underground rivers" (e.g., caves where water flows freely underground). This may occur in eroded limestone areas known as karst topography, which make up only a small percentage of Earth's area. More usual is that the pore spaces of rocks in the subsurface are simply saturated with water — like a kitchen sponge — which can be pumped out for agricultural, industrial, or municipal uses.
If a rock unit of low porosity is highly fractured, it can also make a good aquifer (via fissure flow), provided the rock has a hydraulic conductivity sufficient to facilitate movement of water. Porosity is important, but, "alone", it does not determine a rock's ability to act as an aquifer. Areas of the Deccan Traps (a basaltic lava) in west central India are good examples of rock formations with high porosity but low permeability, which makes them poor aquifers. Similarly, the micro-porous (Upper Cretaceous) Chalk of south east England, although having a reasonably high porosity, has a low grain-to-grain permeability, with its good water-yielding characteristics mostly due to micro-fracturing and fissuring.
Human dependence on groundwater.
Most land areas on Earth have some form of aquifer underlying them, sometimes at significant depths. In some cases, these aquifers are rapidly being depleted by the human population.
Fresh-water aquifers, especially those with limited recharge by snow or rain, also known as meteoric water, can be over-exploited and depending on the local hydrogeology, may draw in non-potable water or saltwater intrusion from hydraulically connected aquifers or surface water bodies. This can be a serious problem, especially in coastal areas and other areas where aquifer pumping is excessive. In some areas, the ground water can become contaminated by arsenic and other mineral poisons.
Aquifers are critically important in human habitation and agriculture. Deep aquifers in arid areas have long been water sources for irrigation (see Ogallala below). Many villages and even large cities draw their water supply from wells in aquifers.
Municipal, irrigation, and industrial water supplies are provided through large wells. Multiple wells for one water supply source are termed "wellfields", which may withdraw water from confined or unconfined aquifers. Using ground water from deep, confined aquifers provides more protection from surface water contamination. Some wells, termed "collector wells," are specifically designed to induce infiltration of surface (usually river) water.
Aquifers that provide sustainable fresh groundwater to urban areas and for agricultural irrigation are typically close to the ground surface (within a couple of hundred metres) and have some recharge by fresh water. This recharge is typically from rivers or meteoric water (precipitation) that percolates into the aquifer through overlying unsaturated materials.
Occasionally, sedimentary or "fossil" aquifers are used to provide irrigation and drinking water to urban areas. In Libya, for example, Muammar Gaddafi's Great Manmade River project has pumped large amounts of groundwater from aquifers beneath the Sahara to populous areas near the coast. Though this has saved Libya money over the alternative, desalination, the aquifers are likely to run dry in 60 to 100 years. Aquifer depletion has been cited as one of the causes of the food price rises of 2011.
Subsidence.
In unconsolidated aquifers, groundwater is produced from pore spaces between particles of gravel, sand, and silt. If the aquifer is confined by low-permeability layers, the reduced water pressure in the sand and gravel causes slow drainage of water from the adjoining confining layers. If these confining layers are composed of compressible silt or clay, the loss of water to the aquifer reduces the water pressure in the confining layer, causing it to compress from the weight of overlying geologic materials. In severe cases, this compression can be observed on the ground surface as subsidence. Unfortunately, much of the subsidence from groundwater extraction is permanent (elastic rebound is small). Thus, the subsidence is not only permanent, but the compressed aquifer has a permanently reduced capacity to hold water.
Saltwater intrusion.
Aquifers near the coast have a lens of freshwater near the surface and denser seawater under freshwater. Seawater penetrates the aquifer diffusing in from the ocean and is denser than freshwater. For porous (i.e., sandy) aquifers near the coast, the thickness of freshwater atop saltwater is about for every of freshwater head above sea level. This relationship is called the Ghyben-Herzberg equation. If too much ground water is pumped near the coast, salt-water may intrude into freshwater aquifers causing contamination of potable freshwater supplies. Many coastal aquifers, such as the Biscayne Aquifer near Miami and the New Jersey Coastal Plain aquifer, have problems with saltwater intrusion as a result of overpumping and sea level rise.
Salination.
Aquifers in surface irrigated areas in semi-arid zones with reuse of the unavoidable irrigation water losses percolating down into the underground by supplemental irrigation from wells run the risk of salination.
Surface irrigation water normally contains salts in the order of 0.5 g/l or more and the annual irrigation requirement is in the order of 10000 m³/ha or more so the annual import of salt is in the order of 5000 kg/ha or more.
Under the influence of continuous evaporation, the salt concentration of the aquifer water may increase continually and eventually cause an environmental problem.
For salinity control in such a case, annually an amount of drainage water is to be discharged from the aquifer by means of a subsurface drainage system and disposed of through a safe outlet. The drainage system may be "horizontal" (i.e. using pipes, tile drains or ditches) or "vertical" (drainage by wells). To estimate the drainage requirement, the use of a groundwater model with an agro-hydro-salinity component may be instrumental, e.g. SahysMod.
Examples.
The Great Artesian Basin situated in Australia is arguably the largest groundwater aquifer in the world (over 1.7 million km²). It plays a large part in water supplies for Queensland and remote parts of South Australia.
The Guarani Aquifer, located beneath the surface of Argentina, Brazil, Paraguay, and Uruguay, is one of the world's largest aquifer systems and is an important source of fresh water. Named after the Guarani people, it covers 1,200,000 km², with a volume of about 40,000 km³, a thickness of between 50 m and 800 m and a maximum depth of about 1,800 m.
Aquifer depletion is a problem in some areas, and is especially critical in northern Africa; see the Great Manmade River project of Libya for an example. However, new methods of groundwater management such as artificial recharge and injection of surface waters during seasonal wet periods has extended the life of many freshwater aquifers, especially in the United States.
The Ogallala Aquifer of the central United States is one of the world's great aquifers, but in places it is being rapidly depleted by growing municipal use, and continuing agricultural use. This huge aquifer, which underlies portions of eight states, contains primarily fossil water from the time of the last glaciation. Annual recharge, in the more arid parts of the aquifer, is estimated to total only about 10 percent of annual withdrawals. According to a 2013 report by research hydrologist Leonard F. Konikow at the United States Geological Survey (USGC), the depletion between 2001–2008, inclusive, is about 32 percent of the cumulative depletion during the entire 20th century (Konikow 2013:22)." In the United States, the biggest users of water from aquifers include agricultural irrigation and oil and coal extraction. "Cumulative total groundwater depletion in the United States accelerated in the late 1940s and continued at an almost steady linear rate through the end of the century. In addition to widely recognized environmental consequences, groundwater depletion also adversely impacts the long-term sustainability of groundwater supplies to help meet the Nation’s water needs."
An example of a significant and sustainable carbonate aquifer is the Edwards Aquifer in central Texas. This carbonate aquifer has historically been providing high quality water for nearly 2 million people, and even today, is full because of tremendous recharge from a number of area streams, rivers and lakes. The primary risk to this resource is human development over the recharge areas.
Discontinuous sand bodies at the base of the McMurray Formation in the Athabasca Oil Sands region of northeastern Alberta, Canada, are commonly referred to as the Basal Water Sand (BWS) aquifers. Saturated with water, they are confined beneath impermeable bitumen-saturated sands that are exploited to recover bitumen for synthetic crude oil production. Where they are deep-lying and recharge occurs from underlying Devonian formations they are saline, and where they are shallow and recharged by meteoric water they are non-saline. The BWS typically pose problems for the recovery of bitumen, whether by open-pit mining or by "in situ" methods such as steam-assisted gravity drainage (SAGD), and in some areas they are targets for waste-water injection.

</doc>
<doc id="47484" url="https://en.wikipedia.org/wiki?curid=47484" title="Atmospheric pressure">
Atmospheric pressure

Atmospheric pressure, sometimes also called barometric pressure, is the pressure exerted by the weight of air in the atmosphere of Earth (or that of another planet). In most circumstances atmospheric pressure is closely approximated by the hydrostatic pressure caused by the weight of air above the measurement point. Low-pressure areas have less atmospheric mass above their location, whereas high-pressure areas have more atmospheric mass above their location. Likewise, as elevation increases, there is less overlying atmospheric mass, so that atmospheric pressure decreases with increasing elevation. On average, a column of air one square centimeter in cross-section, measured from sea level to the top of the atmosphere, has a mass of about 1.03 kg and weight of about 10.1 N (2.27 lbf) That force over one square centimeter is a pressure of 10.1 N/cm2 or 101000 N/m2. (A column one square inch in cross-section would have a weight of about 14.7 lb, or about 65.4 N.)
Standard atmosphere.
The standard atmosphere (symbol: atm) is a unit of pressure defined as , equivalent to 760 mmHg (torr), 29.92 inHg and 14.696 psi.
Mean sea level pressure.
The mean sea level pressure (MSLP) is the atmospheric pressure at sea level. This is the atmospheric pressure normally given in weather reports on radio, television, and newspapers or on the Internet. When barometers in the home are set to match the local weather reports, they measure pressure adjusted to sea level, not the actual local atmospheric pressure.
The "altimeter setting" in aviation, is an atmospheric pressure adjustment.
Average "sea-level pressure" is 101.325 kPa (1013.25 hPa or mbar) or 29.92 inches (inHg) or 760 millimetres of mercury (mmHg). In aviation weather reports (METAR), QNH is transmitted around the world in hectopascals or millibars (1 hectopascal = 1 millibar), except in the United States, Canada, and Colombia where it is reported in inches (to two decimal places) of mercury. (The United States and Canada also report "sea level pressure" SLP, which is adjusted to sea level by a different method, in the remarks section, not in the internationally transmitted part of the code, in hectopascals or millibars. However, in Canada's public weather reports, sea level pressure is instead reported in kilopascals.
In the US weather code remarks, three digits are all that are transmitted; decimal points and the one or two most significant digits are omitted: 1013.2 mbar or 101.32 kPa is transmitted as 132; 1000.0 mbar or 100.00 kPa is transmitted as 000; 998.7 mbar or 99.87 kPa is transmitted as 987; etc. The highest "sea-level pressure" on Earth occurs in Siberia, where the Siberian High often attains a "sea-level pressure" above 1050.0 mbar (105.00 kPa, 30.01 inHg), with record highs close to 1085.0 mbar (108.50 kPa, 32.04 inHg). The lowest measurable "sea-level pressure" is found at the centers of tropical cyclones and tornadoes, with a record low of 870 mbar (87 kPa, 25.69 inHg) (see Atmospheric pressure records).
Altitude variation.
Pressure varies smoothly from the Earth's surface to the top of the mesosphere. Although the pressure changes with the weather, NASA has averaged the conditions for all parts of the earth year-round. As altitude increases, atmospheric pressure decreases. One can calculate the atmospheric pressure at a given altitude. Temperature and humidity also affect the atmospheric pressure, and it is necessary to know these to compute an accurate figure. The graph at right was developed for a temperature of 15 °C and a relative humidity of 0%.
At low altitudes above the sea level, the pressure decreases by about 1.2 kPa for every 100 meters. For higher altitudes within the troposphere, the following equation (the barometric formula) relates atmospheric pressure "p" to altitude "h"
where the constant parameters are as described below:
Local variation.
Atmospheric pressure varies widely on Earth, and these changes are important in studying weather and climate. See pressure system for the effects of air pressure variations on weather.
Atmospheric pressure shows a diurnal or semidiurnal (twice-daily) cycle caused by global atmospheric tides. This effect is strongest in tropical zones, with an amplitude of a few millibars, and almost zero in polar areas. These variations have two superimposed cycles, a circadian (24 h) cycle and semi-circadian (12 h) cycle.
Records.
The highest adjusted-to-sea level barometric pressure ever recorded on Earth (above 750 meters) was measured in Tosontsengel, Mongolia on 19 December 2001. The highest adjusted-to-sea level barometric pressure ever recorded (below 750 meters) was at Agata, Evenkiyskiy, Russia 93°28’E, elevation: 261 m (856.3 ft) on 31 December 1968 of . The discrimination is due to the problematic assumptions (assuming a standard lapse rate) associated with reduction of sea level from high elevations. 
The Dead Sea, the lowest place on Earth at 425 metres (1400 feet) below sea level, has a correspondingly high typical atmospheric pressure of 1065 hPa.
The lowest non-tornadic atmospheric pressure ever measured was 0.858 atm (25.69 inHg), set on 12 October 1979, during Typhoon Tip in the western Pacific Ocean. The measurement was based on an instrumental observation made from a reconnaissance aircraft.
Measurement based on depth of water.
One atmosphere (101 kPa or 14.7 psi) is the pressure caused by the weight of a column of fresh water of approximately 10.3 m (33.8 ft). Thus, a diver 10.3 m underwater experiences a pressure of about 2 atmospheres (1 atm of air plus 1 atm of water). Conversely, 10.3 m is the maximum height to which water can be raised using suction under standard atmospheric conditions.
Low pressures such as natural gas lines are sometimes specified in inches of water, typically written as "w.c." (water column) or "w.g." (inches water gauge). A typical gas-using residential appliance in the US is rated for a maximum of 14 w.c., which is approximately 35 hPa. Similar metric units with a wide variety of names and notation based on millimetres, centimetres or metres are now less commonly used.
Boiling point of water.
Clean fresh water boils at about at earth's standard atmospheric pressure. The boiling point is the temperature at which the vapor pressure is equal to the atmospheric pressure around the water. Because of this, the boiling point of water is lower at lower pressure and higher at higher pressure. This is why cooking at high elevations requires adjustments to recipes. A rough approximation of elevation can be obtained by measuring the temperature at which water boils; in the mid-19th century, this method was used by explorers.
Measurement and maps.
An important application of the knowledge that atmospheric pressure varies directly with altitude was in determining the height of hills and mountains thanks to the availability of reliable pressure measurement devices. While in 1774 Maskelyne was confirming Newton's theory of gravitation at and on Schiehallion in Scotland (using plumb bob deviation to show the effect of "gravity") and accurately measure elevation, William Roy using barometric pressure was able to confirm his height determinations, the agreement being to within one meter (3.28 feet). This was then a useful tool for survey work and map making and long has continued to be useful. It was part of the "application of science" which gave practical people the insight that applied science could easily and relatively cheaply be "useful".

</doc>
<doc id="47485" url="https://en.wikipedia.org/wiki?curid=47485" title="King (chess)">
King (chess)

In chess, the king (♔, ♚) is the most important piece. The object of the game is to trap the opponent's king so that its escape is not possible (checkmate). If a player's king is threatened with capture, it is said to be "in check", and the player must remove the threat of capture on the next move. If this cannot be done, the king is said to be in checkmate. Although the king is the most important piece, it is usually the weakest piece in the game until a later phase, the endgame.
Movement.
White starts with the king on the first rank to the right of the queen. Black starts with the king directly across from the white king. With the squares labeled as in algebraic notation, the white king starts on e1 and the black king on e8.
A king can move one square in any direction (horizontally, vertically, or diagonally) unless the square is already occupied by a friendly piece or the move would place the king in check. As a result, the opposing kings may never occupy adjacent squares (see opposition), but the king can give discovered check by unmasking a bishop, rook, or queen. The king is also involved in the special move of castling. 
Castling.
In conjunction with a rook, the king may make a special move called castling, in which the king moves two squares toward one of its rooks and then the rook is placed on the square over which the king crossed. Castling is allowed only when neither the king nor the castling rook has previously moved, when no squares between them are occupied, when the king is not in check, and when the king will not move across or end its movement on a square that is under enemy attack.
Status in games.
Check and checkmate.
If a player's move places the opponent's king under attack, that king is said to be "in check", and the player in check is required to immediately remedy the situation. There are three possible methods to remove the king from check:
If none of these three options are possible, the player's king has been "checkmated" and the player loses the game.
Stalemate.
A stalemate occurs when, for the player with the move:
If this happens, the king is said to have been stalemated and the game ends in a draw. A player who has very little or no chance of winning will often try to entice the opponent to inadvertently place the player's king in stalemate in order to avoid a loss.
Role in gameplay.
In the opening and middlegame, the king will rarely play an active role in the development of an offensive or defensive position. Instead, a player will normally try to castle and seek safety on the edge of the board behind friendly pawns. In the endgame, however, the king emerges to play an active role as an offensive piece as well as assisting in the promotion of their remaining pawns.
It is not meaningful to assign a value to the king relative to the other pieces, as it cannot be captured or exchanged. In this sense, its value could be considered infinite. As an assessment of the king's capability as an offensive piece in the endgame, it is often considered to be slightly stronger than a bishop or knight – Emanuel Lasker gave it the value of a knight plus a pawn (i.e. four points on the scale of chess piece relative value) . It is better at defending nearby pawns than the knight is, and it is better at attacking them than the bishop is .
Unicode.
Unicode defines two codepoints for king:
♔ U+2654 White Chess King (HTML &#9812;)
♚ U+265A Black Chess King (HTML &#9818;)

</doc>
<doc id="47486" url="https://en.wikipedia.org/wiki?curid=47486" title="Atoll">
Atoll

An atoll (, , , , or ), sometimes called a coral atoll, is a ring-shaped coral reef including a coral rim that encircles a lagoon partially or completely. There may be coral islands/cays on the rim.
Usage.
The word "atoll" comes from the Dhivehi (an Indo-Aryan language spoken on the Maldive Islands) word "atholhu" (Dhivehi: , ), meaning an administrative subdivision.OED Its first recorded use in English was in 1625 as "atollon" – Charles Darwin recognized its indigenous origin and coined, in his "The Structure and Distribution of Coral Reefs", the definition of atolls as "circular groups of coral islets" that is synonymous with "lagoon-island".
More modern definitions of "atoll" describe them as "annular reefs enclosing a lagoon in which there are no promontories other than reefs and islets composed of reef detritus" or "in an exclusively morphological sense, a ring-shaped ribbon reef enclosing a lagoon".
Distribution and size.
The distribution of atolls around the globe is instructive: most of the world's atolls are in the Pacific Ocean (with concentrations in the Tuamotu Islands, Caroline Islands, Marshall Islands, Coral Sea Islands, and the island groups of Kiribati, Tuvalu and Tokelau) and Indian Ocean (the Atolls of the Maldives, the Lakshadweep Islands, the Chagos Archipelago and the Outer Islands of the Seychelles). The Atlantic Ocean has no large groups of atolls, other than eight atolls east of Nicaragua that belong to the Colombian department of San Andres and Providencia in the Caribbean.
Reef-building corals will thrive only in warm tropical and subtropical waters of oceans and seas, and therefore atolls are only found in the tropics and subtropics. The northernmost atoll of the world is Kure Atoll at 28°24' N, along with other atolls of the Northwestern Hawaiian Islands. The southernmost atolls of the world are Elizabeth Reef at 29°58' S, and nearby Middleton Reef at 29°29' S, in the Tasman Sea, both of which are part of the Coral Sea Islands Territory. The next southerly atoll is Ducie Island in the Pitcairn Islands Group, at 24°40' S. Bermuda is sometimes claimed as the "northernmost atoll" at a latitude of 32°24' N. At this latitude coral reefs would not develop without the warming waters of the Gulf Stream. However, Bermuda is termed a "pseudo-atoll" because its general form, while resembling that of an atoll, has a very different mode of formation. While there is no atoll directly on the equator, the closest atoll to the Equator is Aranuka of Kiribati, with its southern tip just 12 km north of the equator.
The largest atolls by total area (lagoon plus reef and dry land) are listed below:
In most cases, the land area of an atoll is very small in comparison to the total area. Atoll islands are low lying, with their elevations less than 5 meters (9). Measured by total area, Lifou (1146 km²) is the largest raised coral atoll of the world, followed by Rennell Island (660 km²). More sources however list as the largest atoll in the world in terms of land area Kiritimati, which is also a raised coral atoll (321.37 km² land area; according to other sources even 575 km²), 160 km² main lagoon, 168 km² other lagoons (according to other sources 319 km² total lagoon size). The remains of an ancient atoll as a hill in a limestone area is called a reef knoll. The second largest atoll by dry land area is Aldabra with 155 km². The largest atoll in terms of island numbers is Huvadhu Atoll in the south of the Maldives with 255 islands.
Formation.
In 1842, Charles Darwin explained the creation of coral atolls in the southern Pacific Ocean based upon observations made during a five-year voyage aboard the from 1831 to 1836. Accepted as basically correct, his explanation involved considering that several tropical island types—from high volcanic island, through barrier reef island, to atoll—represented a sequence of gradual subsidence of what started as an oceanic volcano. He reasoned that a fringing coral reef surrounding a volcanic island in the tropical sea will grow upwards as the island subsides (sinks), becoming an "almost atoll", or barrier reef island, as typified by an island such as Aitutaki in the Cook Islands, Bora Bora and others in the Society Islands. The fringing reef becomes a barrier reef for the reason that the outer part of the reef maintains itself near sea level through biotic growth, while the inner part of the reef falls behind, becoming a lagoon because conditions are less favorable for the coral and calcareous algae responsible for most reef growth. In time, subsidence carries the old volcano below the ocean surface and the barrier reef remains. At this point, the island has become an atoll.
Atolls are the product of the growth of tropical marine organisms, and so these islands are only found in warm tropical waters. Volcanic islands located beyond the warm water temperature requirements of hermatypic (reef-building) organisms become seamounts as they subside and are eroded away at the surface. An island that is located where the ocean water temperatures are just sufficiently warm for upward reef growth to keep pace with the rate of subsidence is said to be at the Darwin Point. Islands in colder, more polar regions evolve towards seamounts or guyots; warmer, more equatorial islands evolve towards atolls, for example Kure Atoll.
Reginald Aldworth Daly offered a somewhat different explanation for atoll formation: islands worn away by erosion, by ocean waves and streams, during the last glacial stand of the sea of some below present sea level developed as coral islands (atolls), or barrier reefs on a platform surrounding a volcanic island not completely worn away, as sea level gradually rose from melting of the glaciers. Discovery of the great depth of the volcanic remnant beneath many atolls such as at Midway Atoll favors the Darwin explanation, although there can be little doubt that fluctuating sea level has had considerable influence on atolls and other reefs.
Coral atolls are also an important place where dolomitization of calcite occurs. At certain depths water is undersaturated in calcium carbonate but saturated in dolomite. Convection created by tides and sea currents enhance this change. Hydrothermal currents created by volcanoes under the atoll may also play an important role.
Investigation by the Royal Society of London into the formation of coral reefs.
In 1896, 1897 and 1898, the Royal Society of London carried out drilling on Funafuti atoll in Tuvalu for the purpose of investigating the formation of coral reefs to determine whether traces of shallow water organisms could be found at depth in the coral of Pacific atolls. This investigation followed the work on the structure and distribution of coral reefs conducted by Charles Darwin in the Pacific.
The first expedition in 1896 was led by Professor William Johnson Sollas of the University of Oxford. The geologists included Walter George Woolnough and Edgeworth David of the University of Sydney. Professor Edgeworth David led the expedition in 1897. The third expedition in 1898 was led by Alfred Edmund Finckh.
United States national monuments.
On January 6, 2009, U.S. President George W. Bush announced that several remote Pacific islands under U.S. jurisdiction were now national monuments, protecting coral reefs.

</doc>
<doc id="47487" url="https://en.wikipedia.org/wiki?curid=47487" title="Azimuth">
Azimuth

An azimuth () (from Arabic "al-sumūt", meaning "the directions") is an angular measurement in a spherical coordinate system. The vector from an observer (origin) to a point of interest is projected perpendicularly onto a reference plane; the angle between the projected vector and a reference vector on the reference plane is called the azimuth.
An example is the position of a star in the sky. The star is the point of interest, the reference plane is the horizon or the surface of the sea, and the reference vector points north. The azimuth is the angle between the north vector and the perpendicular projection of the star down onto the horizon.
Azimuth is usually measured in degrees (°). The concept is used in navigation, astronomy, engineering, mapping, mining and artillery.
Navigation.
Today, the reference plane for an azimuth is typically true north, measured as a 0° azimuth, though other angular units (grad, mil) can be used. Moving clockwise on a 360 degree circle, east has azimuth 90°, south 180°, and west 270°. There are exceptions: some navigation systems use south as the reference vector. Any direction can be the reference vector, as long as it is clearly defined.
Quite commonly, azimuths or compass bearings are stated in a system in which either north or south can be the zero, and the angle may be measured clockwise or anticlockwise from the zero. For example, a bearing might be described as "(from) south, (turn) thirty degrees (toward the) east" (the words in brackets are usually omitted), abbreviated "S30°E", which is the bearing 30 degrees in the eastward direction from south, i.e. the bearing 150 degrees clockwise from north. The reference direction, stated first, is always north or south, and the turning direction, stated last, is east or west. The directions are chosen so that the angle, stated between them, is positive, between zero and 90 degrees. If the bearing happens to be exactly in the direction of one of the cardinal points, a different notation, e.g. "due east", is used instead.
Cartographical azimuth.
The cartographical azimuth (in decimal degrees) can be calculated when the coordinates of 2 points are known in a flat plane (cartographical coordinates):
Remark that the reference axes are swapped relative to the (counterclockwise) mathematical polar coordinate system and that the azimuth is clockwise relative to the north.
This is the reason why the X and Y axis in the above formula are swapped.
If the azimuth becomes negative, one can always add 360°.
The formula in radians would be slightly easier:
Calculating Coordinates.
When the coordinates (X1, Y1) of one point, the distance L, and the azimuth α to another point (X2, Y2) are known, one can calculate its coordinates:
This is typically used in triangulation.
Calculating azimuth.
We are standing at latitude formula_5, longitude zero; we want to find the azimuth from our viewpoint to Point 2 at latitude formula_6, longitude L (positive eastward). We can get a fair approximation by assuming the Earth is a sphere, in which case the azimuth formula_7 is given by
A better approximation assumes the Earth is a slightly-squashed sphere (an "oblate spheroid"); "azimuth" then has at least two very slightly different meanings. "Normal-section azimuth" is the angle measured at our viewpoint by a theodolite whose axis is perpendicular to the surface of the spheroid; "geodetic azimuth" is the angle between north and the "geodesic"; that is, the shortest path on the surface of the spheroid from our viewpoint to Point 2. The difference is usually immeasurably small; if Point 2 is not more than 100 km away, the difference will not exceed 0.03 arc second.
Various websites will calculate geodetic azimuth; e.g., GeoScience Australia site. Formulas for calculating geodetic azimuth are linked in the distance article.
Normal-section azimuth is simpler to calculate; Bomford says Cunningham's formula is exact for any distance . If formula_9 is the flattening for the chosen spheroid (e.g., 1/298.257223563 for WGS84) then
If formula_11 = 0 then
To calculate the azimuth of the sun or a star given its declination and hour angle at our location, we modify the formula for a spherical earth. Replace formula_13 with declination and longitude difference with hour angle, and change the sign (since the hour angle is positive westward instead of east).
Mapping.
There is a wide variety of azimuthal map projections. They all have the property that directions (the azimuths) from a central point are preserved. Some navigation systems use south as the reference plane. However, any direction can serve as the plane of reference, as long as it is clearly defined for everyone using that system.
Astronomy.
Used in celestial navigation, an "azimuth" is the direction of a celestial body from the observer. In astronomy, an "azimuth" is sometimes referred to as a bearing. In modern astronomy azimuth is nearly always measured from the north.
(The article on coordinate systems, for example, uses a convention measuring from the south.) In former times, it was common to refer to azimuth from the south, as it was then zero at the same time that the hour angle of a star was zero. This assumes, however, that the star (upper) culminates in the south, which is only true if the star's declination is less than (i.e. further south than) the observer's latitude.
Other systems.
Right ascension.
If, instead of measuring from and along the horizon, the angles are measured from and along the celestial equator, the angles are called right ascension if referenced to the Vernal Equinox, or hour angle if referenced to the celestial meridian.
Horizontal coordinate.
In the horizontal coordinate system, used in celestial navigation and satellite dish installation, azimuth is one of the two coordinates. The other is altitude, sometimes called elevation above the horizon. See also: Sat finder.
Polar coordinate.
In mathematics, the azimuth angle of a point in cylindrical coordinates or spherical coordinates is the anticlockwise angle between the positive x-axis and the projection of the vector onto the xy-plane. The angle is the same as an angle in polar coordinates of the component of the vector in the xy-plane and is normally measured in radians rather than degrees. As well as measuring the angle differently, in mathematical applications theta, formula_14, is very often used to represent the azimuth rather than the symbol phi formula_15.
Other uses of the word.
For magnetic tape drives, "azimuth" refers to the angle between the tape head(s) and tape.
In sound localization experiments and literature, the "azimuth" refers to the angle the sound source makes compared to the imaginary straight line that is drawn from within the head through the area between the eyes.
An azimuth thruster in shipbuilding is a propeller that can be rotated horizontally.
Etymology of the word.
The word azimuth is in all European languages today. It originates from medieval Arabic "al-sumūt", pronounced "as-sumūt" in Arabic, meaning "the directions" (plural of Arabic "al-samt" = "the direction"). The Arabic word entered late medieval Latin in an astronomy context and in particular in the use of the Arabic version of the Astrolabe astronomy instrument. The word's first record in English is in the 1390s in "Treatise on the Astrolabe" by Geoffrey Chaucer. The first known record in any Western language is in Spanish in the 1270s in an astronomy book that was largely derived from Arabic sources, the "Libros del saber de astronomía" commissioned by King Alfonso X of Castile.

</doc>
<doc id="47488" url="https://en.wikipedia.org/wiki?curid=47488" title="Barometer">
Barometer

A barometer is a scientific instrument used in meteorology to measure atmospheric pressure. Pressure tendency can forecast short term changes in the weather. Numerous measurements of air pressure are used within surface weather analysis to help find surface troughs, high pressure systems and frontal boundaries.
Barometers and pressure altimeters (the most basic and common type of altimeter) are essentially the same instrument, but used for different purposes. An altimeter is intended to be transported from place to place matching the atmospheric pressure to the corresponding altitude, while a barometer is kept stationary and measures subtle pressure changes caused by weather. The main exception to this is ships at sea, which can use a barometer because their elevation does not change. Due to the presence of weather systems, aircraft altimeters may need to be adjusted as they fly between regions of varying normalized atmospheric pressure.
History.
Although Evangelista Torricelli is universally credited with inventing the barometer in 1643, historical documentation also suggests Gasparo Berti, an Italian mathematician and astronomer, unintentionally built a water barometer sometime between 1640 and 1643. French scientist and philosopher René Descartes described the design of an experiment to determine atmospheric pressure as early as 1631, but there is no evidence that he built a working barometer at that time. This was a restatement of the theory of "horror vacui" ("nature abhors a vacuum"), which dates to Aristotle, and which Galileo restated as "resistenza del vacuo".
Galileo's ideas reached Rome in December 1638 in his "Discorsi". Raffaele Magiotti and Gasparo Berti were excited by these ideas, and decided to seek a better way to attempt to produce a vacuum than with a siphon. Magiotti devised such an experiment, and sometime between 1639 and 1641, Berti (with Magiotti, Athanasius Kircher and Niccolò Zucchi present) carried it out.
Four accounts of Berti's experiment exist, but a simple model of his experiment consisted of filling with water a long tube that had both ends plugged, then standing the tube in a basin already full of water. The bottom end of the tube was opened, and water that had been inside of it poured out into the basin. However, only part of the water in the tube flowed out, and the level of the water inside the tube stayed at an exact level, which happened to be 10.3 m, the same height Baliani and Galileo had observed that was limited by the siphon. What was most important about this experiment was that the lowering water had left a space above it in the tube which had no intermediate contact with air to fill it up. This seemed to suggest the possibility of a vacuum existing in the space above the water.
Torricelli, a friend and student of Galileo, interpreted the results of the experiments in a novel way. He proposed that the weight of the atmosphere, not an attracting force of the vacuum, held the water in the tube. In a letter to Michelangelo Ricci in 1644 concerning the experiments, he wrote:
Many have said that a vacuum does not exist, others that it does exist in spite of the repugnance of nature and with difficulty; I know of no one who has said that it exists without difficulty and without a resistance from nature. I argued thus: If there can be found a manifest cause from which the resistance can be derived which is felt if we try to make a vacuum, it seems to me foolish to try to attribute to vacuum those operations which follow evidently from some other cause; and so by making some very easy calculations, I found that the cause assigned by me (that is, the weight of the atmosphere) ought by itself alone to offer a greater resistance than it does when we try to produce a vacuum.
It was traditionally thought (especially by the Aristotelians) that the air did not have lateral weight: that is, that the kilometers of air above the surface did not exert any weight on the bodies below it. Even Galileo had accepted the weightlessness of air as a simple truth. Torricelli questioned that assumption, and instead proposed that air had weight and that it was the latter (not the attracting force of the vacuum) which held (or rather, pushed) up the column of water. He thought that the level the water stayed at (c. 10.3 m) was reflective of the force of the air's weight pushing on it (specifically, pushing on the water in the basin and thus limiting how much water can fall from the tube into it). In other words, he viewed the barometer as a balance, an instrument for measurement (as opposed to merely being an instrument to create a vacuum), and because he was the first to view it this way, he is traditionally considered the inventor of the barometer (in the sense in which we use the term now).
Because of rumors circulating in Torricelli's gossipy Italian neighborhood, which included that he was engaged in some form of sorcery or witchcraft, Torricelli realized he had to keep his experiment secret to avoid the risk of being arrested. He needed to use a liquid that was heavier than water, and from his previous association and suggestions by Galileo, he deduced by using mercury, a shorter tube could be used. With mercury, which is about 14 times heavier than water, a tube only 80 cm was now needed, not 10.5 m.
In 1646, Blaise Pascal along with Pierre Petit, had repeated and perfected Torricelli's experiment after hearing about it from Marin Mersenne, who himself had been shown the experiment by Torricelli toward the end of 1644. Pascal further devised an experiment to test the Aristotelian proposition that it was vapors from the liquid that filled the space in a barometer. His experiment compared water with wine, and since the latter was considered more "spiritous", the Aristotelians expected the wine to stand lower (since more vapors would mean more pushing down on the liquid column). Pascal performed the experiment publicly, inviting the Aristotelians to predict the outcome beforehand. The Aristotelians predicted the wine would stand lower. It did not.
However, Pascal went even further to test the mechanical theory. If, as suspected by mechanical philosophers like Torricelli and Pascal, air had lateral weight, the weight of the air would be less at higher altitudes. Therefore, Pascal wrote to his brother-in-law, Florin Perier, who lived near a mountain called the Puy de Dome, asking him to perform a crucial experiment. Perier was to take a barometer up the Puy de Dome and make measurements along the way of the height of the column of mercury. He was then to compare it to measurements taken at the foot of the mountain to see if those measurements taken higher up were in fact smaller. In September 1648, Perier carefully and meticulously carried out the experiment, and found that Pascal's predictions had been correct. The mercury barometer stood lower the higher one went.
Types.
Water-based barometers.
The concept that decreasing atmospheric pressure predicts stormy weather, postulated by Lucien Vidi, provides the theoretical basis for a weather prediction device called a "storm glass" or a "Goethe barometer" (named for Johann Wolfgang Von Goethe, the renowned German writer and polymath who developed a simple but effective weather ball barometer using the principles developed by Torricelli). The French name, "le baromètre Liègeois", is used by some English speakers. This name reflects the origins of many early weather glasses - the glass blowers of Liège, Belgium.
The weather ball barometer consists of a glass container with a sealed body, half filled with water. A narrow spout connects to the body below the water level and rises above the water level. The narrow spout is open to the atmosphere. When the air pressure is lower than it was at the time the body was sealed, the water level in the spout will rise above the water level in the body; when the air pressure is higher, the water level in the spout will drop below the water level in the body. A variation of this type of barometer can be easily made at home.
Mercury barometers.
A mercury barometer has a glass tube with a height of at least 84 cm, closed at one end, with an open mercury-filled reservoir at the base. The weight of the mercury creates a vacuum in the top of the tube known as Torricellian vacuum. Mercury in the tube adjusts until the weight of the mercury column balances the atmospheric force exerted on the reservoir. High atmospheric pressure places more force on the reservoir, forcing mercury higher in the column. Low pressure allows the mercury to drop to a lower level in the column by lowering the force placed on the reservoir. Since higher temperature levels around the instrument will reduce the density of the mercury, the scale for reading the height of the mercury is adjusted to compensate for this effect.
Torricelli documented that the height of the mercury in a barometer changed slightly each day and concluded that this was due to the changing pressure in the atmosphere.
The mercury barometer's design gives rise to the expression of atmospheric pressure in inches or millimeters or feet (torr): the pressure is quoted as the level of the mercury's height in the vertical column. Typically, atmospheric pressure is measured between 26.5 and 31.5 inches of Hg. One atmosphere (1 atm) is equivalent to 29.92 inches of mercury.
Design changes to make the instrument more sensitive, simpler to read, and easier to transport resulted in variations such as the basin, siphon, wheel, cistern, Fortin, multiple folded, stereometric, and balance barometers. Fitzroy barometers combine the standard mercury barometer with a thermometer, as well as a guide of how to interpret pressure changes. Fortin barometers use a variable displacement mercury cistern, usually constructed with a thumbscrew pressing on a leather diaphragm bottom. This compensates for displacement of mercury in the column with varying pressure. To use a Fortin barometer, the level of mercury is set to the zero level before the pressure is read on the column. Some models also employ a valve for closing the cistern, enabling the mercury column to be forced to the top of the column for transport. This prevents water-hammer damage to the column in transit.
On June 5, 2007, a European Union directive was enacted to restrict the sale of mercury, thus effectively ending the production of new mercury barometers in Europe.
Vacuum pump oil barometer.
Using vacuum pump oil as the working fluid in a barometer has led to the creation of the new "World's Tallest Barometer" in February 2013. The barometer at Portland State University (PSU) uses doubly distilled vacuum pump oil and has a nominal height of ~12.4 m for the oil column height; expected excursions are in the range of ±0.4 m over the course of a year. Vacuum pump oil has very low vapor pressure and it is available in a range of densities; the lowest density vacuum oil was chosen for the PSU barometer to maximize the oil column height.
Aneroid barometers.
An aneroid barometer is an instrument for measuring pressure as a method that does not involve liquid. Invented in 1844 by French scientist Lucien Vidi, the aneroid barometer uses a small, flexible metal box called an aneroid cell (capsule), which is made from an alloy of beryllium and copper. The evacuated capsule (or usually more capsules) is prevented from collapsing by a strong spring. Small changes in external air pressure cause the cell to expand or contract. This expansion and contraction drives mechanical levers such that the tiny movements of the capsule are amplified and displayed on the face of the aneroid barometer. Many models include a manually set needle which is used to mark the current measurement so a change can be seen. In addition, the mechanism is made deliberately "stiff" so that tapping the barometer reveals whether the pressure is rising or falling as the pointer moves. This type of barometer is common in homes and in recreational boats, as well as small aircraft. It is also used in meteorology, mostly in barographs and as a pressure instrument in radiosondes.
Barographs.
A barograph records a graph of some atmospheric pressure and uses an aneroid barometer mechanism to move a needle on a smoked foil or to move a pen upon paper, both of which are attached to a drum moved by clockwork.
MEMS Barometers.
Microelectromechanical systems (or MEMS) barometers are extremely small devices between 1 to 100 micrometres in size (i.e. 0.001 to 0.1 mm). They are created via photolithography or photochemical machining. Typical applications include miniaturized weather stations, electronic barometers and altimeters.
More unusual barometers.
There are many other more unusual types of barometer. From variations on the storm barometer, such as the Collins Patent Table Barometer, to more traditional looking designs such as Hooke's Otheometer and the Ross Sympiesometer. Some, such as the Shark Oil barometer, work only in a certain temperature range, achieved in warmer climates.
A barometer can also be found in smartphones such as the Samsung Galaxy Nexus, Samsung Galaxy S3-S6, Motorola Xoom and Apple iPhone 6 smartphones, based on MEMS and piezoresistive pressure-sensing technologies. Inclusion of barometers on smartphones was originally intended to provide a faster GPS lock. However, third party researchers were unable to confirm additional GPS accuracy or lock speed due to barometric readings. The researchers suggest that the inclusion of barometers in smartphones may provide a solution to determining a user's elevation, but also suggest that several pitfalls must first be overcome.
Applications.
Barometric pressure and the pressure tendency (the change of pressure over time) have been used in weather forecasting since the late 19th century. When used in combination with wind observations, reasonably accurate short-term forecasts can be made. Simultaneous barometric readings from across a network of weather stations allow maps of air pressure to be produced, which were the first form of the modern weather map when created in the 19th century. Isobars, lines of equal pressure, when drawn on such a map, give a contour map showing areas of high and low pressure. Localized high atmospheric pressure acts as a barrier to approaching weather systems, diverting their course. Atmospheric lift caused by low-level wind convergence into the surface low brings clouds and sometimes precipitation. The larger the change in pressure, especially if more than 3.5 hPa, the greater the change in weather that can be expected. If the pressure drop is rapid, a low pressure system is approaching, and there is a greater chance of rain. Rapid pressure rises, such as in the wake of a cold front, are associated with improving weather conditions, such as clearing skies.
With falling air pressure, gases trapped within the coal in deep mines can escape more freely. Thus low pressure increases the risk of firedamp accumulating. Collieries therefore keep track of the pressure. In the case of the Trimdon Grange colliery disaster of 1882 the mines inspector drew attention to the records and in the report stated "the conditions of atmosphere and temperature may be taken to have reached a dangerous point".
Aneroid barometers are used in scuba diving. A Submersible pressure gauge is used to keep track of the contents of the diver's air tank. Another gauge is used to measure the hydrostatic pressure, usually expressed as a depth of sea water. Either or both gauges may be replaced with electronic variants or a dive computer.
Compensations.
Temperature.
The density of mercury will change with temperature, so a reading must be adjusted for the temperature of the instrument. For this purpose a mercury thermometer is usually mounted on the instrument. Temperature compensation of an aneroid barometer is accomplished by including a bi-metal element in the mechanical linkages. Aneroid barometers sold for domestic use typically have no compensation under the assumption that they will be used within a controlled room temperature range.
Altitude.
As the air pressure decreases at altitudes above sea level (and increases below sea level) the uncorrected reading of the barometer will depend on its location. The reading is then adjusted to an equivalent sea-level pressure for purposes of reporting. For example, if a barometer located at sea level and under fair weather conditions is moved to an altitude of 1,000 feet (305 m), about 1 inch of mercury (~35 hPa) must be added on to the reading. The barometer readings at the two locations should be the same if there are negligible changes in time, horizontal distance, and temperature. If this were not done, there would be a false indication of an approaching storm at the higher elevation.
Aneroid barometers have a mechanical adjustment that allows the equivalent sea level pressure to be read directly and without further adjustment if the instrument is not moved to a different altitude. Setting an aneroid barometer is similar to setting an analog clock that is not at the correct time. Its dial is rotated so that the current atmospheric pressure from a known accurate and nearby barometer (such as the local weather station) is displayed. No calculation is needed, as the source barometer reading has already been converted to equivalent sea-level pressure, and this is transferred to the barometer being set—regardless of its altitude. Though somewhat rare, a few aneroid barometers intended for monitoring the weather are calibrated to manually adjust for altitude. In this case, knowing "either" the altitude or the current atmospheric pressure would be sufficient for future accurate readings.
The table below shows examples for three locations in the city of San Francisco, California. Note the corrected barometer readings are identical, and based on equivalent sea-level pressure. (Assume a temperature of 15 °C.)
Barometers and atmospheric pressure calculations.
When atmospheric pressure is measured by a barometer, the pressure is also referred to as the "barometric pressure". Assume a barometer with a cross-sectional area "A", a height "h", filled with mercury from the bottom at Point B to the top at Point C. The pressure at the bottom of the barometer, Point B, is equal to the atmospheric pressure. The pressure at the very top, Point C, can be taken as zero because there is only mercury vapor above this point and its pressure is very low relative to the atmospheric pressure. Therefore, one can find the atmospheric pressure using the barometer and this equation:
Patm = ρgh
where ρ is the density of mercury, g is the gravitational acceleration, and h is the height of the mercury column above the free surface area. The physical dimensions (length of tube and cross-sectional area of the tube) of the barometer itself have no effect on the height of the fluid column in the tube.
In thermodynamic calculations, a commonly used pressure unit is the "standard atmosphere". This is the pressure resulting from a column of mercury of 760 mm in height at 0 °C. For the density of mercury, use ρHg = 13,595 kg/m3 and for gravitational acceleration use g = 9.807 m/s2.
If water were used (instead of mercury) to meet the standard atmospheric pressure, a water column of roughly 10.3 m (33.8 ft) would be needed.
Standard atmospheric pressure as a function of elevation:
Note: 1 torr = 133.3 Pa = 0.03937 In Hg

</doc>
<doc id="47489" url="https://en.wikipedia.org/wiki?curid=47489" title="Bioassay">
Bioassay

Bioassay (commonly used shorthand for biological assay or assessment), or biological standardization is a type of scientific experiment. A bioassay involves the use of live animal or plant ("in vivo") or tissue or cell ("in vitro") to determine the biological activity of a substance, such as a hormone or drug. Bioassays are typically conducted to measure the effects of a substance on a living organism and are essential in the development of new drugs and in monitoring environmental pollutants. Both are procedures by which the potency or the nature of a substance is estimated by studying its effects on living matter. A bioassay can also be used to determine the concentration of a particular constitution of a mixture that may cause harmful effects on organisms or the environment.
Use.
Bioassays are procedures that can determine the concentration or purity or biological activity of a substance such as vitamin, hormone or plant growth factor by measuring the effect on an organism, tissue, cells, enzyme or receptor. Bioassays may be or . Qualitative bioassays are used for assessing the physical effects of a substance that may not be quantified, such as seeds fail to germinate or develop abnormally deformity. An example of a qualitative bioassay includes Arnold Adolph Berthold's famous experiment on castrated chickens. This analysis found that by removing the testicles of a chicken, it would not develop into a rooster because the endocrine signals necessary for this process were not available. Quantitative bioassays involve estimation of the dose-response curve, how the response changes with increasing dose. That dose-response relation allows estimation of the dose or concentration of a substance associated with a specific biological response, such as the LC50 (concentration killing 50% of the exposed organisms). Quantitative bioassays are typically analyzed using the methods of biostatistics. For more information Look up Basic and Clinical Pharmacology by Bertram G. Katzung.
Definition.
""The determination of the relative strength of a substance (e.g., a drug or hormone or toxicant) by comparing its effect on a test organism with that of a standard preparation." is called bioassay.
Similarly, Bioassay is a method of developing toxicological information on organisms whose physiology is considered similar to the organisms of direct concern to a known level of toxic chemical compound in an environmentally concerned chamber.
Types.
Bioassays are of two types:
Quantal.
A quantal assay involves an "all or none response".
Graded.
Graded assays are based on the observation that there is a proportionate increase in the observed response following an increase in the concentration or dose. The parameters employed in such bioassays are based on the nature of the effect the substance is expected to produce. For example: contraction of smooth muscle preparation for assaying histamine or the study of blood pressure response in case of adrenaline.
A graded bioassay can be performed by employing any of the below-mentioned techniques. The choice of procedure depends on:
Techniques.
Matching Bioassay:
It is the simplest type of the bioassay. In this type of bioassay, response of the test substance taken first and the observed response is tried to match with the standard response. Several responses of the standard drug are recorded till a close matching point to that of the test substance is observed. A corresponding concentration is thus calculated. This assay is applied when the sample size is too small. Since the assay does not involve the recording of concentration response curve, the sensitivity of the preparation is not taken into consideration. Therefore, precision and reliability is not very good.
Interpolation bioassay:
Bioassays are conducted by determining the amount of preparation of unknown potency required to produce a definite effect on suitable test animals or organs or tissue under standard conditions. This effect is compared with that of a standard. Thus the amount of the test substance required to produce the same biological effect as a given quantity the unit of a standard preparation is compared and the potency of the unknown is expressed as a % of that of the standard by employing a simple formula.
Many times, a reliable result cannot be obtained using this calculation. Therefore it may be necessary to adopt more precise methods of calculating potency based upon observations of relative, but not necessarily equal effects, likewise, statistical methods may also be employed.
The data (obtained from either of assay techniques used) on which bioassay are based may be classified as quantal or graded response.
Both these depend ultimately on plotting or making assumption concerning the form of DRC.
Environmental bioassays.
Environmental bioassays are generally a broad-range survey of toxicity. A toxicity identification evaluation is conducted to determine what the relevant toxicants are. Although bioassays are beneficial in determining the biological activity within an organism, they can often be time-consuming and laborious. Organism-specific factors may result in data that are not applicable to others in that species. For these reasons, other biological techniques are often employed, including radioimmunoassays. "See bioindicator."
Water pollution control requirements in the United States require some industrial dischargers and municipal sewage treatment plants to conduct bioassays. These procedures, called whole effluent toxicity tests, include acute toxicity tests as well as chronic test methods. The methods involve exposing living aquatic organisms to samples of wastewater for a specific length of time. For example the bioassay ECOTOX uses the microalgae Euglena gracilis to test the toxicity of water samples. (See Bioindicator#Microalgae as bioindicators and water quality)

</doc>
<doc id="47490" url="https://en.wikipedia.org/wiki?curid=47490" title="Biodegradation">
Biodegradation

Biodegradation is the disintegration of materials by bacteria, fungi, or other biological means. Although often conflated, biodegradable is distinct in meaning from compostable. While biodegradable simply means to be consumed by microorganisms, "compostable" makes the specific demand that the object break down under composting conditions. The term is often used in relation to ecology, waste management, biomedicine, and the natural environment (bioremediation) and is now commonly associated with environmentally friendly products that are capable of decomposing back into natural elements. Organic material can be degraded aerobically with oxygen, or anaerobically, without oxygen. Biosurfactant, an extracellular surfactant secreted by microorganisms, enhances the biodegradation process.
Biodegradable matter is generally organic material that serves as a nutrient for microorganisms. Microorganisms are so numerous and diverse that, a huge range of compounds are biodegraded, including hydrocarbons (e.g. oil), polychlorinated biphenyls (PCBs), polyaromatic hydrocarbons (PAHs), pharmaceutical substances. Decomposition of biodegradable substances may include both biological and abiotic steps.
Factors affecting rate.
In practice, almost all chemical compounds and materials are subject to biodegradation, the key is the relative rates of such processes - minutes, days, years, centuries... A number of factors determine the degradation rate of organic compounds. Salient factors include light, water and oxygen. Temperature is also important because chemical reactions proceed more quickly at higher temperatures. The degradation rate of many organic compounds is limited by their bioavailability. Compounds must be released into solution before organisms can degrade them.
Biodegradability can be measured in a number of ways. Respirometry tests can be used for aerobic microbes. First one places a solid waste sample in a container with microorganisms and soil, and then aerate the mixture. Over the course of several days, microorganisms digest the sample bit by bit and produce carbon dioxide – the resulting amount of CO2 serves as an indicator of degradation. Biodegradability can also be measured by anaerobic microbes and the amount of methane or alloy that they are able to produce. In formal scientific literature, the process is termed bio-remediation.
Detergents.
In advanced societies, laundry detergents are based on "linear" alkylbenzenesulfonates. Branched alkybenzenesulfonates (below right), used in former times, were abandoned because they biodegrade too slowly.
Plastics.
Plastics biodegrade at highly variable rates. PVC-based plumbing is specifically selected for handing sewage because PVC biodegrades very slowly. Some packaging materials on the other hand are being developed that would degrade readily upon exposure to the environment. Illustrative synthetic polymers that are biodegrade quickly include polycaprolactone, others are polyesters and aromatic-aliphatic esters, due to their ester bonds being susceptible to attack by water. A prominent example is poly-3-hydroxybutyrate, the renewably derived polylactic acid, and the synthetic polycaprolactone. Others are the cellulose-based cellulose acetate and celluloid (cellulose nitrate).
Under low oxygen conditions biodegradable plastics break down slower and with the production of methane, like other organic materials do. The breakdown process is accelerated in a dedicated compost heap. Starch-based plastics will degrade within two to four months in a home compost bin, while polylactic acid is largely undecomposed, requiring higher temperatures. Polycaprolactone and polycaprolactone-starch composites decompose slower, but the starch content accelerates decomposition by leaving behind a porous, high surface area polycaprolactone. Nevertheless, it takes many months.
In 2016, a bacterium named Ideonella sakaiensis was found to biodegrade PET.
Many plastic producers have gone so far even to say that their plastics are compostable, typically listing corn starch as an ingredient. However, these claims are questionable because the plastics industry operates under its own definition of compostable:
The term "composting" is often used informally to describe the biodegradation of packaging materials. Legal definitions exist for compostability, the process that leads to compost. Four criteria are offered by the European Union:
Biodegradable technology.
In 1973 it was proven for the first time that polyester degrades when disposed in bioactive material such as soil. Polyesters are water resistant and can be melted and shaped into sheets, bottles, and other products, making certain plastics now available as a biodegradable product. Following, Polyhydroxylalkanoates (PHAs) were produced directly from renewable resources by microbes. They are approximately 95% cellular bacteria and can be manipulated by genetic strategies. The composition and biodegradability of PHAs can be regulated by blending it with other natural polymers. In the 1980s the company ICI Zenecca commercialized PHAs under the name Biopol. It was used for the production of shampoo bottles and other cosmetic products. Consumer response was unusual. Consumers were willing to pay more for this product because it was natural and biodegradable, which had not occurred before.
Now biodegradable technology is a highly developed market with applications in product packaging, production and medicine. Biodegradable technology is concerned with the manufacturing science of biodegradable materials. It imposes science based mechanisms of plant genetics into the processes of today. Scientists and manufacturing corporations can help impact climate change by developing a use of plant genetics that would mimic some technologies. By looking to plants, such as biodegradable material harvested through photosynthesis, waste and toxins can be minimized.
Oxo-biodegradable technology, which has further developed biodegradable plastics, has also emerged. Oxo-biodegradation is defined by CEN (the European Standards Organisation) as "degradation resulting from oxidative and cell-mediated phenomena, either simultaneously or successively." Whilst sometimes described as "oxo-fragmentable," and "oxo-degradable" this describes only the first or oxidative phase. These descriptions should not be used for material which degrades by the process of oxo-biodegradation defined by CEN, and the correct description is "oxo-biodegradable."
By combining plastic products with very large polymer molecules, which contain only carbon and hydrogen, with oxygen in the air, the product is rendered capable of decomposing in anywhere from a week to one to two years. This reaction occurs even without prodegradant additives but at a very slow rate. That is why conventional plastics, when discarded, persist for a long time in the environment. Oxo-biodegradable formulations catalyze and accelerate the biodegradation process but it takes considerable skill and experience to balance the ingredients within the formulations so as to provide the product with a useful life for a set period, followed by degradation and biodegradation.
Biodegradable technology is especially utilized by the bio-medical community. Biodegradable polymers are classified into three groups:
medical, ecological, and dual application, while in terms of origin they are divided into two groups: natural and synthetic. The Clean Technology Group is exploiting the use of supercritical carbon dioxide, which under high pressure at room temperature is a solvent that can use biodegradable plastics to make polymer drug coatings. The polymer (meaning a material composed of molecules with repeating structural units that form a long chain) is used to encapsulate a drug prior to injection in the body and is based on lactic acid, a compound normally produced in the body, and is thus able to be excreted naturally. The coating is designed for controlled release over a period of time, reducing the number of injections required and maximizing the therapeutic benefit. Professor Steve Howdle states that biodegradable polymers are particularly attractive for use in drug delivery, as once introduced into the body they require no retrieval or further manipulation and are degraded into soluble, non-toxic by-products. Different polymers degrade at different rates within the body and therefore polymer selection can be tailored to achieve desired release rates.
Other biomedical applications include the use of biodegradable, elastic shape-memory polymers. Biodegradable implant materials can now be used for minimally invasive surgical procedures through degradable thermoplastic polymers. These polymers are now able to change their shape with increase of temperature, causing shape memory capabilities as well as easily degradable sutures. As a result, implants can now fit through small incisions, doctors can easily perform complex deformations, and sutures and other material aides can naturally biodegrade after a completed surgery.
Etymology of "biodegradable".
The first known use of the word in biological text was in 1961 when employed to describe the breakdown of material into the base components of carbon, hydrogen, and oxygen by microorganisms. Now biodegradable is commonly associated with environmentally friendly products that are part of the earth's innate cycle and capable of decomposing back into natural elements.

</doc>
<doc id="47492" url="https://en.wikipedia.org/wiki?curid=47492" title="Biomass (ecology)">
Biomass (ecology)

Biomass, in ecology, is the mass of living biological organisms in a given area or ecosystem at a given time. Biomass can refer to "species biomass", which is the mass of one or more species, or to "community biomass", which is the mass of all species in the community. It can include microorganisms, plants or animals. The mass can be expressed as the average mass per unit area, or as the total mass in the community.
How biomass is measured depends on why it is being measured. Sometimes, the biomass is regarded as the natural mass of organisms "in situ", just as they are. For example, in a salmon fishery, the salmon biomass might be regarded as the total wet weight the salmon would have if they were taken out of the water. In other contexts, biomass can be measured in terms of the dried organic mass, so perhaps only 30% of the actual weight might count, the rest being water. For other purposes, only biological tissues count, and teeth, bones and shells are excluded. In some applications, biomass is measured as the mass of organically bound carbon (C) that is present.
Apart from bacteria, the total live biomass on Earth is about 560 billion tonnes C, and the total annual primary production of biomass is just over 100 billion tonnes C/yr. The total live biomass of bacteria may be as much as that of plants and animals or may be much less. The total amount of DNA base pairs on Earth, as a possible approximation of global biodiversity, is estimated at 5.0 x 1037, and weighs 50 billion tonnes. In comparison, the total mass of the biosphere has been estimated to be as much as 4 TtC (trillion tons of carbon).
Ecological pyramids.
An ecological pyramid is a graphical representation that shows, for a given ecosystem, the relationship between biomass or biological productivity and trophic levels.
An ecological pyramid provides a snapshot in time of an ecological community.
The bottom of the pyramid represents the primary producers (autotrophs). The primary producers take energy from the environment in the form of sunlight or inorganic chemicals and use it to create energy-rich molecules such as carbohydrates. This mechanism is called primary production. The pyramid then proceeds through the various trophic levels to the apex predators at the top.
When energy is transferred from one trophic level to the next, typically only ten percent is used to build new biomass. The remaining ninety percent goes to metabolic processes or is dissipated as heat. This energy loss means that productivity pyramids are never inverted, and generally limits food chains to about six levels. However, in oceans, biomass pyramids can be wholly or partially inverted, with more biomass at higher levels.
Terrestrial biomass.
Terrestrial biomass generally decreases markedly at each higher trophic level (plants, herbivores, carnivores). Examples of terrestrial producers are grasses, trees and shrubs. These have a much higher biomass than the animals that consume them, such as deer, zebras and insects. The level with the least biomass are the highest predators in the food chain, such as foxes and eagles.
In a temperate grassland, grasses and other plants are the primary producers at the bottom of the pyramid. Then come the primary consumers, such as grasshoppers, voles and bison, followed by the secondary consumers, shrews, hawks and small cats. Finally the tertiary consumers, large cats and wolves. The biomass pyramid decreases markedly at each higher level.
Ocean biomass.
Ocean biomass, in a reversal of terrestrial biomass, can increase at higher trophic levels. In the ocean, the food chain typically starts with phytoplankton, and follows the course:
Phytoplankton → zooplankton → predatory zooplankton → filter feeders → predatory fish
Phytoplankton are the main primary producers at the bottom of the marine food chain. Phytoplankton use photosynthesis to convert inorganic carbon into protoplasm. They are then consumed by microscopic animals called zooplankton.
Zooplankton comprise the second level in the food chain, and includes small crustaceans, such as copepods and krill, and the larva of fish, squid, lobsters and crabs.
In turn, small zooplankton are consumed by both larger predatory zooplankters, such as krill, and by forage fish, which are small schooling filter feeding fish. This makes up the third level in the food chain.
The fourth trophic level consists of predatory fish, marine mammals and seabirds that consume forage fish. Examples are swordfish, seals and gannets.
Apex predators, such as orcas, which can consume seals, and shortfin mako sharks, which can consume swordfish, make up the fifth trophic level. Baleen whales can consume zooplankton and krill directly, leading to a food chain with only three or four trophic levels.
Marine environments can have inverted biomass pyramids. In particular, the biomass of consumers (copepods, krill, shrimp, forage fish) is larger than the biomass of primary producers. This happens because the ocean's primary producers are tiny phytoplankton that grow and reproduce rapidly, so a small mass can have a fast rate of primary production. In contrast, terrestrial primary producers grow and reproduce slowly.
There is an exception with cyanobacteria. Marine cyanobacteria are the smallest known photosynthetic organisms; the smallest of all, "Prochlorococcus", is just 0.5 to 0.8 micrometres across. Prochlorococcus is possibly the most plentiful species on Earth: a single millilitre of surface seawater may contain 100,000 cells or more. Worldwide, there are estimated to be several octillion (~1027) individuals. "Prochlorococcus" is ubiquitous between 40°N and 40°S and dominates in the oligotrophic (nutrient poor) regions of the oceans. The bacterium accounts for an estimated 20% of the oxygen in the Earth's atmosphere, and forms part of the base of the ocean food chain.
Bacterial biomass.
There are typically 50 million bacterial cells in a gram of soil and a million bacterial cells in a millilitre of fresh water. In a much-cited study from 1998 the world bacterial biomass was calculated to be 350 to 550 billions of tonnes of carbon, equal to between 60% and 100% of the carbon in plants. More recent studies of seafloor microbes have cast considerable doubt on that, one study in 2012 reduced the calculated microbial biomass on the seafloor from the original 303 billions of tonnes of C to just 4.1 billions of tonnes of C, reducing the global biomass of prokaryotes to 50 to 250 billions of tonnes of C. Further, if the average per cell biomass of prokaryotes is reduced from 86 to 14 femtograms C then the global biomass of prokaryotes is reduced to 13 to 44.5 billions of tonnes of C, equal to between 2.4% and 8.1% of the carbon in plants.
Global biomass.
Estimates for the global biomass of species and higher level groups are not always consistent across the literature. Apart from bacteria, the total global biomass has been estimated at about 560 billion tonnes C. Most of this biomass is found on land, with only 5 to 10 billion tonnes C found in the oceans. On land, there is about 1,000 times more plant biomass ("phytomass") than animal biomass ("zoomass"). About 18% of this plant biomass is eaten by the land animals. However, in the ocean, the animal biomass is nearly 30 times larger than the plant biomass. Most ocean plant biomass is eaten by the ocean animals.
Humans comprise about 100 million tonnes of the Earth's dry biomass, domesticated animals about 700 million tonnes, and crops about 2 billion tonnes.
The most successful animal species, in terms of biomass, may well be Antarctic krill, "Euphausia superba", with a fresh biomass approaching 500 million tonnes, although domestic cattle may also reach these immense figures. However, as a group, the small aquatic crustaceans called copepods may form the largest animal biomass on earth. A 2009 paper in "Science" estimates, for the first time, the total world fish biomass as somewhere between 0.8 and 2.0 billion tonnes. It has been estimated that about 1% of the global biomass is due to phytoplankton, and fully 25% is due to fungi.
Global rate of production.
Net primary production is the rate at which new biomass is generated, mainly due to photosynthesis. Global primary production can be estimated from satellite observations. Satellites scan the normalised difference vegetation index (NDVI) over terrestrial habitats, and scan sea-surface chlorophyll levels over oceans. This results in 56.4 billion tonnes C/yr (53.8%), for terrestrial primary production, and 48.5 billion tonnes C/yr for oceanic primary production. Thus, the total photoautotrophic primary production for the Earth is about 104.9 billion tonnes C/yr. This translates to about 426 gC/m²/yr for land production (excluding areas with permanent ice cover), and 140 gC/m²/yr for the oceans.
However, there is a much more significant difference in standing stocks—while accounting for almost half of total annual production, oceanic autotrophs account for only about 0.2% of the total biomass. Autotrophs may have the highest global proportion of biomass, but they are closely rivaled or surpassed by microbes.
Terrestrial freshwater ecosystems generate about 1.5% of the global net primary production.
Some global producers of biomass in order of productivity rates are

</doc>
<doc id="47496" url="https://en.wikipedia.org/wiki?curid=47496" title="Biota">
Biota

Biota may refer to:

</doc>
<doc id="47497" url="https://en.wikipedia.org/wiki?curid=47497" title="Doctor Doom">
Doctor Doom

Doctor Victor Von Doom, better known as Doctor Doom, is a fictional supervillain appearing in American comic books published by Marvel Comics. The son of Romani witch Cynthia Von Doom, Doctor Doom is the archenemy of the Fantastic Four, and the leader of the fictional nation of Latveria. He is both a genius inventor and a sorcerer. While his chief opponents have been the Fantastic Four, he has also come into conflict with Spider-Man, the Avengers and other superheroes in the Marvel Universe.
Doctor Doom has made many appearances in video games, television series, and merchandise such as action figures and trading cards. He was ranked as the 4th Greatest Villain by "Wizard" on its 100 Greatest Villains of All Time list. IGN's list of the Top 100 Comic Book Villains of All Time ranked Doctor Doom as #3.
Doctor Doom has also been featured in other Marvel-endorsed feature films such as Roger Corman's unreleased 1994 "The Fantastic Four" played by Joseph Culp, and the 2005 film "Fantastic Four" and its 2007 sequel "" played by Julian McMahon. Toby Kebbell portrays the character in the 2015 reboot.
Publication history.
Created by writer-editor Stan Lee and artist/co-plotter Jack Kirby, the character first appeared in "The Fantastic Four" #5 (July 1962) wearing his trademark metal mask and green cloak.
Creation and development.
Like many of Marvel's Silver Age characters, Doctor Doom was conceived by writer Stan Lee and artist Jack Kirby. With the "Fantastic Four" title performing well, Lee and Kirby were trying to dream up a "soul-stirring…super sensational new villain." Looking for a name, Lee latched onto "Doctor Doom" as "eloquent in its simplicity — magnificent in its implied menace."
Due to the rush to publish, the character was not given a full origin story until "Fantastic Four Annual" #2, two years after his debut.
Jack Kirby modeled Doctor Doom after Death, with the armor standing in for that character's skeleton; "It was the reason for the armor and the hood. Death is connected with armor and the inhuman-like steel. Death is something without mercy, and human flesh contains that mercy." Kirby further described Doctor Doom as being "paranoid", wrecked by his twisted face and wanting the whole world to be like him. Kirby went on to say that "Doctor Doom is an evil person, but he's not always been evil. He was …but through a flaw in his own character, he was a perfectionist." At one point in the 1970s, Kirby drew his interpretation of what Doctor Doom would look like under the mask, giving Doctor Doom only "a tiny scar on his cheek." Due to this slight imperfection, Doctor Doom hides his face not from the world, but from himself. To Kirby, this is the motivation for Doctor Doom's vengeance against the world; because others are superior due to this slight scar, Doom wants to elevate himself above them. Typical of Lee's writing characterization of Doctor Doom is his arrogance; his pride leads to Doctor Doom's disfigurement at the hands of his own machine, and to the failures of many of his schemes. There is also an idea that Doctor Doom placed his mask on his face before it fully cooled, burning his face. In some early stories glimpses of his face are shown, in which he appears to be bald.
While the Fantastic Four had fought various villains such as the Mole Man, Skrulls, the Miracle Man, and Namor the Sub-Mariner, Doctor Doom managed to overshadow them all and became the Fantastic Four's archnemesis.
During the 1970s, Doctor Doom branched out to more Marvel titles such as "Astonishing Tales", "The Incredible Hulk", and "Super-Villain Team-Up", starting in 1975, as well as appearances in "Marvel Team-Up", beginning with issue #42 (February 1976). Doctor Doom's origin was also a feature in "Astonishing Tales" when his ties to the villain Mephisto were revealed.
1980s–1990s.
In 1976, Marvel and DC Comics collaborated on "Superman vs. the Amazing Spider-Man", and seeking to replicate that success the two companies again teamed the characters in "Superman and Spider-Man" in 1981. Marvel editor in chief Jim Shooter co-wrote the story with Marv Wolfman, and recalled choosing Doctor Doom based on his iconic status: "I figured I needed the heaviest-duty bad guy we had to offer — Doctor Doom. Their greatest hero against our greatest villain."
The same year, John Byrne began his six-year run writing and illustrating "Fantastic Four", sparking a "second golden age" for the title but also attempting to "turn the clock back [...] get back and see fresh what it was that made the book great at its inception." Doctor Doom made his first appearance under Byrne's tenure with issue #236. Whereas Kirby had intimated that Doom's disfigurement was more a figment of Victor's vain personality, Byrne decided that Doctor Doom's face was truly ravaged: only Doctor Doom's own robot slaves are allowed to see the monarch without his helmet. Byrne emphasized other aspects of Doom's personality; despite his ruthless nature, Doctor Doom is a man of honor. Returning to Latveria after being temporarily deposed, Doctor Doom abandons a scheme to wrest mystical secrets from Doctor Strange in order to oversee his land's reconstruction. Despite a tempestuous temper, Doctor Doom occasionally shows warmth and empathy to others; he tries to free his mother from Mephisto and treats Kristoff Vernard like his own son. Byrne gave further detail regarding Doom's scarring: Byrne introduced the idea that the accident at Empire State University only left Doctor Doom with a small scar that was exaggerated into a more disfiguring accident by Doctor Doom's own arrogance—bydonning his newly-forged face mask before it had fully cooled, he caused massive irreparable damage.
After Byrne's departure Doctor Doom continued to be a major villain in "Fantastic Four", and as the 1980s continued Doom appeared in other comics such as "Punisher", "The Spectacular Spider-Man", and "Excalibur". Under Fantastic Four writer Steven Englehart, Doctor Doom became exiled from Latveria by his heir Kristoff, who was brainwashed into thinking he was Doctor Doom. Doctor Doom would spend most of his time in exile planning his return, but Englehart left the title before he could resolve the storyline. This storyline ultimately ended with the controversial "Fantastic Four" #350, where writer Walt Simonson had the Doctor Doom who had been seen in the book during the Englehart run being revealed to be a robot imposter and the real Doctor Doom, in a newly redesigned armor, returning to claim his country from his usurper. According to Simonson's retcon, the last appearance of the real Doctor Doom was in the "Battle of the Baxter Building" story arc, but Simonson's interpretation of the character was unaware of certain major changes at the time to the Fantastic Four. A rumor states that Simonson drew up a list of stories which featured the real Doctor Doom and those which did not but later writers ignored Simonson's choices, retconning these story elements as an attempt by Doom to blame his own past failures on unruly robots.
Modern depictions.
In 2003 Doctor Doom was the villain in the "Fantastic Four" story arc "Unthinkable", in which Doctor Victor Von Doom viscerally skins his childhood love Valeria alive to the bone, and turns her flesh into mystic armour, imprisons Franklin Richards in Hell, captures Valeria Richards, and succeeds in de-powering and imprisoning the Fantastic Four. Writer Mark Waid sought to redefine Doctor Doom's character in a way that had not been seen before. Waid punctuated this reinterpretation of Doctor Doom during his "Unthinkable" saga (Vol 3 #66-70 & Vol 1 (restart) #500) as an absolute sadist by having Von Doom ruthlessly murder Valeria (namesake of the Richards's daughter), his first love and granddaughter to his long serving faithful retainer Boris, in order to be granted access to powerful magic by a trio of demons, Valeria being the treasured possession that they demanded in exchange. He subsequently attempted to prove his superiority to Reed by giving him the chance to find his way out of a prison that could only be escaped by mastering magic, in the belief that Reed would fail to do so, but with the aid of the astral projection of Doctor Strange, Richards learned to utilize magic on a basic level by accepting that he could not understand it and escapes. With Doom still his superior in this particular field, Richard instead tricked him into rejecting the demons, resulting in them dragging Doom to Hell. This was the case until the events of Ragnarok, when Thor's hammer Mjolnir fell through dimensions and gave Doctor Doom a way out of Hell when it was lost after Thor's apparent death.
In 2005 and 2006, Doctor Doom was featured in his own limited series, "Books of Doom ", a retelling of the origin story by Ed Brubaker. In an interview, Brubaker said the series was a way to elaborate on the earlier portions of Doctor Doom's life which had not been seen often in the comics. The series also set out to determine if Doctor Doom's path from troubled child to dictator was fated or Doctor Doom's own faults led to his corruption — in essence, a nature versus nurture question. Brubaker's version of Doctor Doom was heavily influenced by the original Lee/Kirby version; responding to a question if he would show Doctor Doom's face, Brubaker stated "ollowing Kirby's example, I think it's better not to show it."
The Mighty Avengers invaded Latveria, Doctor Doom's nation, due to his involvement in creating a chemical bomb that would infect people with the symbiote (though it was recently revealed that this attack was actually set up by Kristoff Vernard to put Doctor Doom out of the picture prior to Kristoff's future attempt at a coup). Due to Ultron's interference, the bomb was dropped on Manhattan, but the Mighty Avengers are able to stop the effects on the people. The Mighty Avengers proceed to invade Latveria. During the invasion, the Sentry, Iron Man, and Doctor Doom are sent to the past thanks to Doctor Doom’s time platform. Eventually, the trio breaks into the Baxter Building and make use of a confiscated time machine to return to the present era, the Sentry taking advantage of the fact he will soon be forgotten by the world to easily defeat the Thing. Doctor Doom transports himself to Morgana's castle to summon up a magical army and captures the Avengers, but they free themselves and he is arrested for terrorist crimes against humanity after a brief struggle that culminated with the Sentry tearing off Doctor Doom's mask.
During Dark Reign when Norman Osborn is in charge, Doctor Doom is released and sent back to Latveria. However, Morgana le Fay engages him in a magical battle, which he is losing until the Dark Avengers rescue him. He then magically rebuilds his kingdom.
The character is also featured in "Siege" storyline and in the five issue mini-series Doomwar written by Jonathan Maberry.
Doctor Doom soon allies himself with the isolationist group known as the Desturi, to take control of Wakanda. He attacked and wounded T'Challa, the current Black Panther, maiming him enough to prevent him from holding the mantle again. Doctor Doom's main objective was to secure Wakanda's store of vibranium, which he could mystically enhance to make himself unstoppable.
In the Mark Millar penned "Fantastic Four" 566-569 Doctor Doom received a significant power upgrade. He was thrown back in time (perhaps about 50 million years) by the Marquis of Death. Doctor Doom then fought through time and space to get back to the present to seek revenge on the Marquis of Death. Doctor Doom stated, as he killed the Marquis, he had rebuilt every molecule of his being and increased his power all to destroy the Marquis. In later issues this seems to have been ignored however, with writers treating Doctor Doom the way they have always before in terms of power.
At the start of the story arc "Fantastic Four: Three," Doctor Doom feels that he needs to be "reborn" and makes plans to abdicate his throne and give it to Kristoff when Valeria teleports to his room unexpectedly asking for his assistance to help her father. Valeria quickly notices that Doctor Doom has suffered brain damage and makes a deal with him to restore his mental capacities if he helps Reed and the Fantastic Four. Doctor Doom agrees to her proposition. Later, Doctor Doom appears among those in attendance at Johnny Storm's funeral.
Due to the agreement, Doctor Doom is recommended by Nathaniel and Valeria Richards to be a member of the Future Foundation. Objecting, Thing attacks Doctor Doom out of anger, but the fight is stopped by Mister Fantastic and the Invisible Woman, who welcomes Doctor Doom to their group.
Leading to the Secret Wars, Doom usurps the power of the Beyonder once more with the aid of Doctor Strange and the Molecule Man, collecting what he can of the destroyed multiverse and forming a new Battleworld consisting of different alternate realities. He also assumes the role of God and claims complete dominion of this new world and its inhabitants, controlling them into thinking he was always the almighty force of creation; he takes Sue as his wife, Franklin and Valeria as his children, condemns the Human Torch to be the sun and Ben Grimm to be the great Siege Wall. Richards and a ragtag collection of heroes and villains that survived the destruction of all universes are able to challenge him and, with the help of Molecule Man, are able to take his power and restore the multiverse. Opting to heal rather than harm, Reed finally uses the Beyonder's power to heal Doom's face, giving Victor a happy moment despite his utter defeat.
In the All-New, All-Different Marvel, Doom returns to Latveria where he saves Iron Man by incapacitating a group of Latveria rebels with a sonic attack. Doom reveals to Tony that he is a new man and wishes to help, giving the latter one of the Wands of Watoomb to keep safe from Madame Masque. When more rebels arrive, Doom teleports Iron Man to the Bronx Zoo.
Doom finds Iron Man again and teleports the two to the Jackpot Club in Chicago to confront Madam Masque. Discovering that Madame Masque is displaying symptoms of demonic possession, Doom has Tony trap her in the Iron Man Armor then proceeds to exorcise the demon from her. Doom disappears before Tony regains consciousness. Doom appears once again and interrupts Tony's breakfast date with Amara. Doom is trying to prove to Tony that he has changed and is trying to correct the mistakes he's made, explaining that he's arrived to check up on Tony and see if he is suffering from any side-effects from being in the presence of an exorcism. Tony still refuses to trust him after what he's done and Doom leaves once again.
Fictional character biography.
Victor Von Doom was born decades ago to a tribe of Latverian Romani people under the rule of an unnamed nobleman called the Baron. Victor's mother was witch Cynthia Von Doom who died by Mephisto's hand while Von Doom was young. His father, Werner, was the leader of the tribe and a renowned medicine man who kept his wife's sorceress life quiet in order to protect Victor from a similar fate. Soon after Cynthia's death, the Baron's wife grew incurably ill from cancer and Werner was called to the capital to heal her. When she succumbed to illness, the Baron labeled Werner a murderer and called for his death. Werner escaped with young Victor, having realized the night before the woman would die. He goes on to die of exposure on the mountainside, cradling the boy in a final embrace and giving him his garments to keep him warm. Victor survived and, on return to the Romani camp, discovered his mother's occult instruments and swore revenge on the Baron. Victor grew into a headstrong and brilliant man, combining sorcery and technology to create fantastic devices to keep the Baron's men at bay and protect the Roma people. His exploits attracted the attention of the dean of Empire State University, who sent someone to the camp. Offered the chance to study in the United States, Von Doom chooses to leave his homeland and his love, Valeria, behind.
Once in the United States, Victor met fellow student and future nemesis Reed Richards, who was intended to be his roommate, but Von Doom disliked him and asked for another roommate. After a time, Victor constructed a machine intended to communicate with the dead. Though Richards tried to warn him about a flaw in the machine, seeing his calculations were a few decimals off, Victor continued on with disastrous results. The machine violently failed and the resulting explosion seemingly severely damaged his face. It is later revealed that Ben Grimm, a friend of Richards who despised Victor for his superior attitude, tampered with the machine. He would later blame himself for Doctor Doom's eventual rise to power, but never revealed this information to anyone. Expelled after the accident, Victor traveled the world until he collapsed on a Tibetan mountainside. Rescued by a clan of monks, Victor quickly mastered the monks' disciplines as well as the monks themselves. Victor then forged himself a suit of armor, complete with a scowling mask, and took the name Doctor Doom. As Doctor Doom, he would go on to menace those he felt responsible for his accident—primarily, Reed Richards of the Fantastic Four. He succeeded in taking over Latveria, taking an interest in the welfare of the Roma.
In his first appearance, Doctor Doom captures the Invisible Girl, using her as a hostage so the Fantastic Four will travel back in time to steal the enchanted treasure of Blackbeard which will help him conquer the world, but he is fooled by Reed Richards, who swaps the treasure with worthless chains. Doctor Doom then forms an alliance with the Sub-Mariner, who places a magnetic device in the Baxter Building. However Doctor Doom uses this to pull him and the Fantastic Four into space, thinking this will rid him of those capable of preventing him conquering the world. But the Sub-Mariner gets to Doctor Doom 's ship and returns the Baxter Building to New York, while Doctor Doom is left on an asteroid. Returning to Earth after learning the secrets of an advanced alien race, the Ovids, Doctor Doom exchanges consciousnesses with Mister Fantastic; Richards, inhabiting Doctor Doom 's body, switches the two back, and Doctor Doom ends up trapped in a micro-world when he is hit with a shrinking ray he had intended to use on the rest of the Fantastic Four. Doctor Doom takes over the micro-world, but leaves after the Fantastic Four end his rule. He is then thrown into space when he attempts to do this to the Fantastic Four. Doctor Doom is saved by Rama-Tut, and he returns to Earth to destroy the Fantastic Four by turning each member against the other using a special berry juice. Richards outwits Doctor Doom by using the hallucinogenic juice against the villain. Doctor Doom, believing he has killed Richards in a test of willpower, departs certain of his victory and superior intelligence.
During the 1960s, Doctor Doom attempts to recruit Spider-Man into joining forces with him, and he also menaces the Avengers when Quicksilver and Scarlet Witch travel to Latveria to find a long-lost relative. He steals the Silver Surfer's powers in 1967, but he loses them after breaching a barrier Galactus set for the Surfer on Earth.
During the 1970s, Doctor Doom branched out to more Marvel titles, with a battle between Doctor Doom and Prince Rudolfo over control of Latveria being featured in "Astonishing Tales". Doctor Doom also attempts to use the Hulk as his slave during two issues of "The Incredible Hulk". The character also made several appearances in the story arcs of "Super-Villain Team-Up", starting in 1975, as well as appearances in "Marvel Team-Up", beginning with issue #42 (February 1976). In August 1981, Doctor Doom also made an appearance in "Iron Man" when the two travel to Camelot where Stark thwarted Doctor Doom 's attempt to solicit the aide of Morgan le Fay and Doctor Doom swore deadly vengeance for that interference, which had to be indefinitely delayed in the interest of returning to the present day. A particularly detailed plan saw Doom ally with the Puppet Master to trap the Fantastic Four within the miniature artificial city of "Liddleville", their minds trapped inside tiny cybernetic, part-organic copies of their original bodies. However, Doom perverts what had been intended by the Puppet Master as a chance to give Alicia and Ben a normal life into a trap, deliberately disrupting Reed's connection to his copy to make it hard for him to concentrate while 'Vincent Vaughn'- Doom's alias as he monitors the project- belittles him, and the Puppet Master eventually helps the FF learn the truth and escape Liddleville while trapping Doom in the android body he had used to monitor the FF.
During John Byrne's run in the 1980s, Doctor Doom attempts to steal the cosmic powers of Terrax, but Doctor Doom's body is destroyed in the resulting fight between Terrax and the Silver Surfer. Doctor Doom survives by transferring his consciousness to another human, and is returned to his original body by the Beyonder (Who had reached into the relative future to 'recruit' Doom for the conflict on Battleworld that the FF had participated in a few months ago from their perspective). While on Battleworld, Doom attempts and briefly succeeds in stealing the Beyonder's power, but it proves too vast for him to control and the disembodied Beyonder is able to take his power back.
When Franklin Richards was kidnapped by Onslaught, Doctor Doom joined the Fantastic Four, Avengers and the X-Men to battle Onslaught in Central Park. An enraged Hulk was able to crack open Onslaught's shell. However, Onslaught remained as pure psionic energy, separated Hulk and Banner, planning to spread across the planet. Thor plunged into Onslaught, trying to contain him. The Fantastic Four, the majority of Avengers, and the Hulk-Less Banner followed in short order, with Doom being forced to join the sacrifice when Iron Man tackled the villain into the energy mass. Thanks to this sacrifice, the X-Men finally managed to destroy Onslaught. Doom, the Fantastic Four, and the Avengers and Banner were believed dead, but were instead saved by Franklin, who created a pocket dimension called Counter-Earth to keep them safe. After several months away, the missing heroes returned from Counter-Earth, except for Doom, who remained there for a time. Doom uncovers the secret power at the heart of the planet, an avatar of his arch-foe Reed Richards' son, Franklin, the super-powered youth who conjured this globe and left a bit of himself behind to guide it from within. Doom manages to convince the little boy to relinquish control of this world with little more than a few errant promises of a better life.
When Susan Richards experiences problems with her second pregnancy while Reed is away, Johnny contacts Doom for help, correctly guessing that Doom will be unable to pass up a chance to succeed where Reed failed (Due to the complex events involving the recent resurrection of Galactus, this pregnancy is a 'repeat' of an earlier one where Sue miscarried). Doom not only saves Sue's daughter, but also cures Johnny of a recent problem with his powers where Johnny was unable to 'flame off' without technological support after becoming overcharged with energy from the Negative Zone by channelling Johnny's excess energy into Sue to keep her alive. After the birth, Doom's only apparent condition for his aid is that he be allowed to name Sue and Reed's daughter, calling her 'Valeria' after his long-lost love. However, this inspires a new plan where Doom makes Valeria his familiar while seeking out her namesake as part of a deal with a trio of demons; by sacrificing his old lover, Doom is granted magical powers on the level he would possess if he had spent the past years studying sorcery rather than science. With this new power, Doom traps Franklin in Hell, immobilises Doctor Strange, and then neutralises the FF's powers, torturing the other three while taunting Reed by leaving him in his magical library, comparing it to giving a dog a road-map as he concludes that it will be impossible for Reed to master sufficient magical skill to be a threat to him. However, Reed is able to release Doctor Strange's astral self from Doom's traps, allowing Strange to give Reed a sufficient crash-course in magic for Reed to free the rest of the team and trick Doom into angering his demonic benefactors, prompting them to take him to Hell. Determined to ensure that Doom cannot be a further threat, Reed takes control of Latveria to dismantle all of Doom's equipment, simultaneously subtly driving his family away so that he can trap Doom and himself in a pocket dimension so that he can make sure Doom never threatens anyone again. However, this plan backfires when the rest of the team attempt to rescue Reed, resulting in Doom transferring his spirit into Sue, Johnny and Ben respectively, forcing Reed to kill his best friend to stop his greatest enemy. Doom is returned to Hell, but Reed is later able to use the same machine Doom once tried to create to travel to Heaven and restore Ben to life. Doom remains in Hell until Mjolnir falls to Earth after the events of Ragnarok as it creates a dimensional tear during its fall that allows Doom to escape, although he decides to focus on rebuilding his power base when he proves unable to even lift the hammer.
Later, a Doombot was taken down by Reed Richards, Henry Pym, Iron Man, She-Hulk and others in New York City. Whether or not it was sent by Doctor Doom himself remains to be seen, as does his role in the overall conflict. Doctor Doom was not invited to the wedding of Storm and the Black Panther. However, he did send a present: an invitation to form an alliance with Latveria, using the Civil War currently going on among the hero community as a reason to quite possibly forge an alliance between their two countries. When Black Panther, on a diplomatic mission to other countries with Storm, did show up in Latveria, he presented them with a real present, and extended another invitation to form an alliance with Black Panther. He demonstrated behavior very uncharacteristic of him, however, which may or may not become a plot point later. Panther spurned the invitation, detonating an EMP that blacked out a local portion of Latveria before Doctor Doom 's robots could destroy his ship. Later on, Doctor Doom is then shown collaborating with the Red Skull on a weapon which will only "be the beginning" of Captain America's suffering. Doctor Doom gave the Red Skull the weapon because the Red Skull gave Victor pieces of technology from an old German castle. The castle was owned by a "Baron of Iron" centuries prior, who had used his technological genius to protect himself and his people. The map the Red Skull used to find the castle bore a picture of Doctor Doom. Doctor Doom states that the technology the Red Skull gave him is more advanced than what he currently has, and that he will become the Baron of Iron in his future. The Red Skull is currently in the process of reverse-engineering Doctor Doom 's weapon for multiple uses, rather than the single use Doctor Doom agreed to.
At the end of the first chapter of the X-Men event "", Doctor Doom is among the supervillain geniuses that Beast contacts to help him reverse the effects of Decimation. He spurns Beast by stating that genetics do not number among his talents.
In "", Doctor Doom was among those that Spider-Man contacts to help save Aunt May.
Doctor Doom also makes Latveria into a refugee camp for the Atlanteans following the destruction of their underwater kingdom as well as becoming allies with Loki in his plot to manipulate Thor into unwittingly release his Asgardian enemies.
Doctor Doom later defends Latveria from the Mighty Avengers, following a revelation that it was one of Doctor Doom's satellites that carried the 'Venom Virus' released in New York City. In a battle with Iron Man and the Sentry, the time travel mechanism within his armor overloads, trapping Doctor Doom and his opponents at some point in the past. Doctor Doom continues a relationship with Morgan le Fay using his time machine. He and Iron Man managed to get back to the present, but Doctor Doom has left Iron Man in his exploding castle. Despite this, Doctor Doom ended up incarcerated at The Raft.
During the "Secret Invasion" storyline, Doctor Doom was among the villains who escaped the Raft when a virus was uploaded into its systems by the Skrulls.
In the aftermath of the Secret Invasion, Doctor Doom is a member of the Dark Illuminati alongside Norman Osborn, Emma Frost, Namor, Loki's female form, and Hood. At the end of this meeting, Namor and Doctor Doom are seen having a discussion of their own plans that have all ready been set in motion.
Doctor Doom soon allies himself with the isolationist group known as the Desturi, to take control of Wakanda. He attacked and wounded T'Challa, the current Black Panther, maiming him enough to prevent him from holding the mantle again. Doctor Doom 's main objective was to secure Wakanda's store of vibranium, which he could mystically enhance to make himself unstoppable. Doctor Doom was also a part of the supervillain group known as the Intelligencia, but was betrayed when they captured him to complete their plan. With the help of Bruce Banner, he escaped, and returned to Latveria. He appears to have been damaged by this experience.
At the start of the "Siege" storyline, Doctor Doom is with the Cabal discussing the current problems with the X-Men and both Avengers teams. Doctor Doom demands that Osborn at once reverse his course of action against his ally Namor, to which Osborn refuses, saying that he and Emma Frost had "crossed the line" with him. Doctor Doom, loathing Thor and the Asgardians all the more due to his recent defeat at their hands, claims that he will support Osborn's "madness" should Namor be returned to him, but Osborn refuses. Osborn's mysterious ally, the Void, violently attacks Doctor Doom, and an apparently amused Loki tells the Hood that he should go, as there is nothing here for either of them, which the Hood, now loyal to Loki due to his hand in the restoration of his mystical abilities, agrees. However, it is revealed that "Doctor Doom" is actually an upgraded Doctor Doom bot, which releases swarms of Doctor Doom bot nanites against the Cabal, tearing down Avengers Tower and forcing its denizens, such as the Dark Avengers, to evacuate. Osborn is rescued by the Sentry, who destroys the body. When Osborn contacts Doctor Doom, Doctor Doom tells him not to ever strike him again or he is willing to go further.
It has been revealed that the Scarlet Witch seen in Wundagore Mountain is actually a Doctor Doom bot which apparently means that the real one has been captured by Doctor Doom sometime after the House of M event. It is revealed that Wanda's enhanced powers were a result of her and Doctor Doom's combined attempt to channel the Life Force in order to resurrect her children. This proves to be too much for Wanda to contain and it overtook her. With Wiccan and Doctor Doom's help, they seek to use the entity that is possessing Wanda to restore mutantkinds' powers. This is stopped by the Young Avengers (who are concerned at the fall-out that would ensue if the powerless mutants are suddenly re-powered) only to find out Doctor Doom 's real plan: to transfer the entity into his own body and gaining Wanda's god-like powers for himself. Doctor Doom becomes omnipotent with powers surpassing those of beings as the Beyonder or the Cosmic Cube. The Young Avengers confront him, but Doctor Doom kills Cassie just before Wanda and Wiccan stole his new-found powers.
At the start of the story arc "Fantastic Four: Three", Doctor Doom felt that he needed to be "reborn" and was making plans to abdicate his throne and give it to Kristoff when Valeria teleported to his room unexpectedly asking for his assistance to help her father. Valeria quickly notices that Doctor Doom has suffered brain damage and makes a deal with him to restore his mental capacities if he helps Reed and the Fantastic Four. Doctor Doom agrees to her proposition. Later, Doctor Doom appears among those in attendance at Johnny Storm's funeral.
Due to the agreement, Doctor Doom was recommended by Nathaniel and Valeria Richards to be a member of the Future Foundation. Objecting, Thing attacks Doctor Doom out of anger, but the fight was stopped by Mister Fantastic and the Invisible Woman, who welcomes Doctor Doom to their group. When Valeria asks Doctor Doom if he has a backup for restoring his memories, he reveals that Kristoff Vernard is his backup. Afterward, Mister Fantastic, Spider-Man, Nathaniel, Valeria, and Doctor Doom head to Latveria to meet with Kristoff and request his help. Mister Fantastic sets up a brain transfer machine in order to help restore Doctor Doom's memories and knowledge, which is successful. When Kristoff wants to return the throne to him, Doctor Doom states that it is not time yet because of a promise he made to Valeria. When Mister Fantastic asks what promise Doctor Doom made to Valeria, Doctor Doom states that he made a promise to help defeat Mister Fantastic. Doctor Doom decides to hold a symposium on how to finally defeat Reed Richards. The Thing and the evolved Moloids give an invitation to the High Evolutionary. Dragon Man and Alex Power give an invitation to Diablo. Upon receiving an invitation from Spider-Man, Mad Thinker is convinced to take part in the event. Bentley 23 even gives an invitation to his creator, the Wizard, along with two A.I.M. lieutenants. However, it is subsequently revealed that the 'Richards' they have been invited to defeat are actually members of the "Council of Reeds" (alternate versions of Reed who were trapped in this universe by Valeria a while back, possessing Reed's intellect while lacking his conscience). While Spider-Man and Invisible Woman make sandwiches for the kids, Mister Fantastic, Doctor Doom, Valeria, and Nathaniel Richards meet with the supervillain geniuses and Uatu the Watcher about what to do with the Council of Reeds.
It is later revealed that, around this time, Doom performed brain surgery on Hulk to separate him from Banner, extracting the uniquely Banner elements from Hulk's brain and cloning a new body for Banner, in return for an initially-unspecified favor from the Hulk. However, when Doctor Doom demands to keep Banner for his own purposes, the Hulk reneges on the deal and flees with Banner's body, leaving his alter ego in the desert where he was created to ensure that Doctor Doom cannot use Banner's intellect. When Banner goes insane due to his separation from the Hulk, irradiating an entire tropical island trying to recreate his transformation- something he cannot do as the cloned body lacks the genetic elements of Banner that allowed him to process the gamma radiation- the Hulk is forced to destroy his other side by letting him be disintegrated by a gamma bomb, prompting the Hulk to accuse Doom of tampering with Banner's mind, only for Doom to observe that what was witnessed was simply Banner without the Hulk to use as a scapegoat for his problems.
The child members of the Future Foundation used the panic room system to teleport themselves the top of the Baxter Building to near Latveria where they help Nathaniel Richards, Kristoff Vernard, Alpha-Reed Richards, and Doctor Doom to rebuild the Bridge, and the Alpha-Reed Richards could return home. The Mad Celestials from Earth-4280 try to enter Earth through the Bridge in order to destroy it. Doctor Doom and Alpha-Reed Richards tried to stop them although Alpha-Reed Richards was killed using the Ultimate Nullifier while Doctor Doom was apparently killed by the Mad Celestials. During his absence, his Doombots attempt to use the animals on Banner's irradiated island as the basis for a new gamma army, but they are defeated by the Hulk- Banner having been absorbed back into the Hulk and recognising that he is a better person with the Hulk than he would have been on his own- using a one-of-a-kind gamma cure he had created to turn all the animals back to normal.
With no knowledge as to how he survived the blast from the Mad Celestials, Doom woke up in the middle of the ruins of the Interdimensional Council of Reeds, where Valeria had left him a present: the full army of lobotomized Doctor Dooms from alternate realities who were previously captured by the Council, along with two Infinity Gauntlets from alternate universes. With these resources, Doom created the Parliament of Doom. He later returned to again rule Latveria, and was targeted by Lucia von Bardas and the Red Ghost, who wanted to get revenge on Doom for past discretions.
Doom later journeyed to the Universe that one of his Infinity Gauntlets had belonged to, which was now empty and desolate, and used the gauntlet to create it anew. He separated magic and science, creating the basic rules for their existence, created new life, made himself its ruler. On a world where science and magic were wed, his creations turned on him and six rulers divided Doom's infinity gems between them. Reed and Nathaniel Richards entered this Universe to save Doom after being prompted by Valeria that he was in grave danger. They managed to convince five of the rulers to pardon Doom and managed to escape the clutches of the sixth, bringing Doom back to their universe. Upon their return, Doom declared that he and Richards were again even.
When Latveria became the site of an Incursion, a collision between Earth and one of its alternate universe counterparts, this Incursion was revealed to be different however, in that it was controlled by a mysterious group known as The Mapmakers, who had rigged the other Earth to explode and wished to mark Doom's Earth for potential expansion. Doom fought off the Mapmakers with the help of his adopted son Kristoff Vernard, whilst unbeknownst to him, the Illuminati blew up the other Earth. After the Incursion ended, Doom was alerted to a rock that had fallen from the sky, which was in truth the Mapmaker's beacon. Doom contacted Reed Richards and Stephen Strange in order to confront them about the Illuminati's presence in Latveria and the incursion, but to his fury, they refused to give him answers and Reed warned him not to contact him in regard to the Incursions again.
During the "AXIS" storyline, Doctor Doom appears as a member of Magneto's unnamed supervillain group during the fight against Red Skull's Red Onslaught form. Doctor Doom works with Scarlet Witch in order to use a spell to awaken the dormant part of Professor X's brain within Red Onslaught. The spell also caused some inversions which made the Avengers and X-Men evil and the bad guys good. In order to combat the now-evil Avengers and X-Men, Doctor Doom forms his team of Avengers by recruiting 3D Man, Elsa Bloodstone, Stingray, Valkyrie, and U.S. Agent. Their first mission is to fight a now-evil Scarlet Witch when she invades Latveria. It is later revealed that he intends to use Scarlet's attack for his goal to atone for his sins, by absorbing her reality-altering powers during the fight and using them to undo all his crimes; being forced to choose only one act to set straight, as he obtains only a fraction of the power, and despite being tempted to use it to repair his face and resurrect his beloved mother, Doctor Doom elects to revive Cassie Lang. He subsequently makes a Faustian deal with an unspecified demon to resurrect Brother Voodoo to take control of the Scarlet Witch and undo the inversion. Having returned to normal, Doctor Doom is shown with the Red Skull in captivity, contained by various telepathy-blocking machines, although his long-term goal is unknown.
Over the following months after the Incursion in Latveria during the "Time Runs Out" storyline, Doctor Doom had been working with a team of scientists to reverse-engineer one of the pieces of a Mapmaker he gathered from the Incursion, and successfully mapped their entire network. Doctor Doom planned to use the Molecule Man to oppose whatever was the origin of the Incursions. When Doctor Strange and the Black Priests began searching for Rabum Alal (the man whose birth was supposedly one of the main causes for the collapse and the decay of the Multiverse) the Black Priest finds a door to the Library of Worlds. Unlike before, they now have a key to get in and Doctor Strange to guide them through the Library. Inside, they are ambushed by the Black Swans in a vacuum where their words cannot be used, but Doctor Strange uses his other magics to fight back. Nevertheless, they are defeated and taken to Rabum Alal, who is revealed to be Doctor Doom himself. He then reveals to Doctor Strange that as Rabum Alal he had gathered a number of Black Swans who revere him as a mythic figure. It was also revealed that Doctor Doom has been opposing the Beyonders with the Molecule Man, by his side.
With the final Incursion imminent during the "Secret Wars" storyline, Doctor Doom, Doctor Strange, and Molecule Man leave to confront the Beyonders. In the aftermath, the multiverse is destroyed, leading Doom and Strange to try to recreate some semblance of the universe for life to exist, leading to the creation of a new Battleworld. There, God Emperor Doom is undisputed ruler with Strange serving as his personal Sheriff. Mister Sinister of Bar Sinister is brought before God Emperor Doom and Sheriff Strange attended by various other Barons where Sinister has been charged by the House of Braddock (the rulers of Higher Avalon) with secretly and illegally aligning with Baron Hyperion of Utopolis in opposition to Higher Avalon. Before Sinister can deal the killing blow to Brian Braddock, God Emperor Doom intervenes. God Emperor Doom interrogates Brian, telling him that Higher Avalon is long rumored to be the most favorable conduit for reaching the Silent Chambers, a place where "heretics and thieves" work to depose God Emperor Doom. When Brian denies any knowledge of it, God Emperor Doom declares him useless and orders that his eyes, tongue, hands and feet be removed. To save his brother, Baron James Braddock confesses that he has assisted those trying to reach the Silent Chambers. Enraged at betrayal by one of his barons, God Emperor Doom orders the entire House of Braddock to be publicly executed. Susan Storm implores him to be merciful. This causes God Emperor Doom to banish James to the Shield. Sheriff Strange informs God Emperor Doom of recent events in the domains of Battleworld. When Doom grows weary, Strange reminds Doom that he is "omnipotent but not omniscient". Before a statue of the Molecule Man, they reminisce about how Doom had battled the Beyonders and managed to salvage various fragments of the Earths that were being destroyed. God Emperor Doom and Invisible Woman later discuss a song she had heard. It tells of the man in the Sun (who is actually the Human Torch who is trapped forever in the Sun by God Emperor Doom as punishment for speaking out against him). This has caused a religion worshiping Human Torch to form among the lower classes of Battleworld. God Emperor Doom worries he is a flawed deity. Invisible Woman comforts him and encourages him to reveal more of himself to his people, including his horribly scarred face. Learning that a group of heroes from the original world have survived the incursions- including Reed Richards- Doom departs to confront them directly, but although he successfully defeats and kills the Phoenix-enhanced Cyclops, he is subsequently betrayed when Strange teleports the other heroes away, informing Doom that, regardless of his power, he will always fear Reed Richards. Angered at Strange's comment that Reed will find a way to defeat him despite his power, Doom kills Strange.
God Emperor Doom enters a realm located below Molecule Man's statue and converses with the real Molecule Man, who resides there. It is explained that the Beyonders were the originators of reality but eventually became harbingers of destruction. The Molecule Man was unique across the multiverse as only one version of him existed, a single being whose presence in each unique reality represented a sliver of a single inter-dimensional entity. The Beyonders would initiate the end of a particular reality by detonating that reality's Molecule Man. Doctor Doom, Doctor Strange and the 616 Molecule Man had gathered Molecule Men from across the multiverse and combined them into a bomb, which they detonated and directed towards the unsuspecting Beyonders. The detonation killed the Beyonders and allowed the Earth-616 Molecule Man to absorb their power and channel it to Doctor Doom, who in turn created Battleworld. God Emperor later order some of his most loyal Barons (consisting of Maestro from Dystopia, Apocalypse from the Domain of Apocalypse, Mister Sinister from Bar Sinister, and Madelyne Pryor from Limbo) to deal with the threat of the Prophet. God Emperor Doom later views the rampage of a giant Thing (who made up part of the Shield that isolated The Deadlands, Perfection, and New Xandar). After Franklin uses Galactus to destroy Thing, Susan Storm asks God Emperor Doom to end it only for him to bide his time. Thanos arrives where he is ready to wipe the floor with God Emperor Doom. God Emperor Doom offers Thanos domain over the Deadlands. Thanos is insulted by the generosity claiming that God Emperor Doom is beneath him. He has experience in divinity and mocks the metal-faced monarch on his limitedness. God Emperor Doom reaches into Thanos' heart, causing Thanos to instantly become skeletal and crumble, as God Emperor Doom wants retribution. He instructs Annihilus to eradicate the opposition when suddenly the Siege Courageous materializes and spits out zombies. God Emperor Doom is taken aback as Black Panther and Namor break through, Black Panther armed with the Infinity Gauntlet and ready to strike. Although T'Challa fails to defeat Doom, he reveals that he is merely there to distract Doom while Reed Richards makes contact with the Molecule Man, with Reece rendering Reed immune to Doom's powers as the two fight. As they right, Doom accuses Reed of simply continuing their usual vendetta because he cannot accept that Doom was right, but Reed counters that Doom establishing himself as god of Battleworld was simply Doom hampering himself, as he would rather be God of a small area that ensures his own survival than take the risk that he could recreate what was lost in the Incursions. Recognising Reed's point, Reece transfers Doom's powers to Reed, with Reed, the Future Foundation and his family going on to remake existence, although Reed also restores Doom's face when he departs to reflect his desire to fix everything, regardless of their past history.
As part of the "All-New, All-Different Marvel" event, Iron Man encountered revolutionaries at Latveria when they are suddenly knocked out by a sonic attack fired by mysterious individual behind Iron Man. When Iron Man turns around, he sees a man in a business suit who tells Iron Man they need each other. While Iron Man doesn't recognize his face, his voice sounds familiar, and according to the bioscans performed by Friday, that man is none other than Victor von Doom. Iron Man is skeptical he's standing face-to-face with Doctor Doom. It's not until the gentleman confirms the information gathered by Iron Man's scans that Iron Man tries to blast him with a repulsor. But Victor von Doom is using a mystical spell defense to shield himself from the attack. Tony keeps questioning the legitimacy of his claims, but Victor simply states that this is the new him, and that he got better from his iconic scarred face. In order to convince Iron Man, Doom reminds him of their time-traveling adventure in Camelot. Iron Man shape-shifts his armor into a Hulk-buster like suit and tries to get past Doom's magic shielding, to no avail. Doom informs Iron Man that he's present at the moment because of Madame Masque, who had started accumulating powerful items around the world. As he guides Iron Man to the ruins of his personal laboratory, Doom explains to Iron Man how different powerful items can enter the universe through dimensional rifts. Inside one crate lies a Wand of Watoomb, which Doom tries to make Iron Man accept as a sign of good faith, so Iron Man could start trusting him. Another group of rebels had entered the castle. As he prepares to deal with the intruders, Doom reveals to Iron Man that by following the energy signature of the castle, he could track down Madame Masque and the item she had taken, a decoy of the actual Wand of Watoomb instead of the real deal. Iron Man is unwilling to leave Doom alone with the rebels, but Victor clarifies that he has no intention of killing them or taking back Latveria, because he now knows he's meant for more. Against Iron Man's will, Doom uses a magic spell to teleport the Golden Avenger away. When Madame Masque attacks Mary Jane Watson's latest nightclub Jackpot in Chicago, Doctor Doom accompanies Iron Man into fighting Madame Masque. Doctor Doom and Iron Man's fight with Madame Masque causes damages to Jackpot. When Mary Jane Watson distracts Madame Masque by knocking off her mask with a microphone, Doctor Doom sees that Madame Masque has a demonic possession in her. As Iron Man holds Madame Masque down, Doctor Doom successfully performs an exorcism. Iron Man is knocked out in the process and Doctor Doom takes his leave from the damaged nightclub. Victor von Doom later pays an unexpected visit to Tony Stark and Amanda Pepara at a diner much to Tony's dismay. Victor states that he is here to see if the demonic possession that affected Madame Masque had affected Tony and/or Amara. Victor then takes his leave from the diner at Tony's suggestion.
Powers and abilities.
Doctor Doom is a polymath and scientific genius. Throughout most of his publication history, he has been depicted as one of the most intelligent humans in the Marvel Universe. This is shown on many occasions, most famously by actually curing Ben Grimm of his Thing form, which Reed Richards has never repeated. On the other hand, Reed Richards managed to process all the computer calculations necessary to save the life of a disintegrating Kitty Pryde by himself, which is a feat that Doctor Doom at the time professed to be unable to do.
Doctor Doom also possesses originally minor mystical capabilities due to teachings from Tibetan monks, but later increased them to a considerable extent due to tutoring from his lover Morgan Le Fay. He is capable of energy projection, creating protective shields, and summoning hordes of demonic creatures. Even at a time when his abilities were consistently referred to as minor, with assistance from his technology and by tag-teaming with Doctor Strange, Doctor Doom managed to come in second in a magic tournament held by the ancient sorcerer the Aged Genghis.
Doctor Doom has also used his scientific talents to steal or replicate the power of other beings such as the Silver Surfer, or in one case the entity Galactus' world-ship.
The alien Ovoids taught Doctor Doom the process of psionically transferring his consciousness into another nearby being through simple eye contact, as well as showing him other forms of technology which Doctor Doom uses to escape from incarcerations and to avoid being killed. However, if his concentration is broken, his mind can transfer back, and he rarely uses this power unless absolutely necessary due to his own ego about his appearance.
Doctor Doom can exert technopathic control over certain machines, most notably the Doom bots. In addition, Doctor Doom has a remarkably strong will, as demonstrated in the graphic novel, "Emperor Doom " when he dared his prisoner, the mind controlling Purple Man, to attempt to control him and he successfully resisted.
Without his armor he proved himself to be a skilled bare-handed fighter, even capable of killing a lion with a single punch.
Doctor Doom’s armor augments his natural physical strength to superhuman levels, to the point where he is able to hold his own against Spider-Man in hand-to-hand combat, although he tends to rely on long-range tactics when engaging physically stronger foes. It is also highly resistant to harm, sufficient to withstand blows from Iron Man’s armor. The armor can generate a defensive force field and a lethal electric shock killing anyone who might come in contact with Doctor Doom. The armor is self-supporting, equipped with internal stores and recycling systems for air, food, water, and energy, allowing the wearer to survive lengthy periods of exposure underwater or in outer space.
As the absolute monarch of Latveria, Dr. Doom has diplomatic immunity – allowing him to escape prosecution for most of his crimes – and total control of the nation’s natural and technological resources, along with its manpower, economy, and military.
Doctor Doom is known for the frequent plot device wherein it is revealed that his actions were actually those of a "Doombot", one of Doctor Doom’s many robot doubles, either working on his behalf or as a result of rogue artificial intelligence.
On many occasions, Doctor Doom’s only real weakness has been his arrogance. Layla Miller once reflected that Doctor Doom is incapable of accepting that he himself might be the reason for his failures. This is most keenly reflected in Doctor Doom’s continued refusal to accept responsibility for the accident that scarred his face, instead preferring to blame Reed Richards for sabotaging his experiment. While his high opinion of himself is generally accurate, he is generally unable to accept when others may have a better understanding of a situation than he does – with the occasional exception of hearing the recommendations of heroes such as Mister Fantastic or the Thing when it is to his advantage. Even when teaming up with others against a greater threat, Doctor Doom will often try to subvert the alliance for personal gain. For instance, while allied with Adam Warlock and other heroes against the Titan Thanos, he attempted to steal Thanos’ Infinity Gauntlet before its owner had been defeated.
Doctor Doom adheres to a strict code of honor at all times. However, Von Doom will keep his "exact" word, which may or may not be beneficial to the person to whom he has given his promise. For example, Doctor Doom may swear that he will not harm an individual, but that only means he will not personally harm that person, it does not mean he will prevent others from harming that person.
Doctor Doom’s honor code led him to save Captain America from drowning because Captain America had earlier saved his life, and on another occasion he thanked Spider-Man for saving him from terrorists attacking him in an airport by allowing him to leave alive despite Spider-Man subsequently insulting him. His code of honor also means that he will not attack a respected opponent who is weakened or at a severe disadvantage, as he regards any victory resulting from such circumstances as hollow and meaningless. He has even on several occasions battled opponents who were intent on killing the Fantastic Four, for no other reason than the fact that he does not want the ultimate defeat of the Fantastic Four to come from anyone’s hands but his own.
Doctor Doom is shown to be devoted to the welfare and well being of his subjects. In fact, one future premonition that was explored explicitly stated that of all the possible futures that could exist for Earth in the Marvel Universe, the only one where Earth is truly safe and peaceful is the one where Doctor Doom rules supreme.
Inventions.
Doctor Doom has constructed numerous devices in order to defeat his foes or gain more power including:
Other versions.
Doctor Doom's status as one of the Fantastic Four's greatest villains has led to his appearance in many of Marvel's alternate universes and spinoffs, in which the character's history, circumstances and behavior vary from the mainstream setting.
In other media.
Doctor Doom has been included in almost every media adaptation of the "Fantastic Four" franchise, including film, television, and computer and video games.
Cultural impact.
In the book "Superhero: The Secret Origin of a Genre", Peter Coogan writes that Doctor Doom's appearance was representative of a change in the portrayal of "mad scientists" to full-fledged villains, often with upgraded powers. Doctor Doom is also emblematic of a specific "subset" of supervillain, which comic book critic Peter Sanderson describes as a "megavillain". These supervillains are genre-crossing villains who exist in adventures "in a world in which the ordinary laws of nature are slightly suspended"; characters such as Professor Moriarty, Count Dracula, Auric Goldfinger, Hannibal Lecter, Lex Luthor, and Darth Vader, also fit this description. Sanderson also found traces of William Shakespeare’s characters Richard III and Iago in Doctor Doom ; all of them "are descended from the 'vice' figure of medieval drama", who address the audience in monologues detailing their thoughts and ambitions.
Described as "iconic", Doctor Doom is one of the most well-received supervillains of the Marvel Universe, as well as one of the most recurring; in his constant battles with heroes and other villains, Doctor Doom has appeared more times than any other villain. The comics site Panels of Awesome ranked Doctor Doom as the number one villain in their listing of the top ten villains in comics; "Wizard Magazine" went a step further by declaring Doctor Doom the fourth greatest villain of all time.
Comic Book Resources ranks Doctor Doom as their fourth favorite Marvel character. Journalist Brent Ecenbarger cited him being able to "stand up against entities like Mephisto, the Beyonder, and Galactus and often comes out on top", as well as the tragedy of any "other number of circumstances could have led to Doctor Doom being a savior, but as it is, instead he remains Marvel’s greatest villain." Fellow journalist Jason Stanhope called his "masterof sorcery and technology an unusual combination", and also felt "his inner sense of nobility sets him apart from lesser villains, in a similar manner to Magneto." Doctor Doom has also been favorably regarded by those who wrote for the character; Stan Lee declared Doctor Doom his favorite villain, saying that Doom "could come to the United States and he could do almost anything, and we could not arrest him because he has diplomatic immunity. Also, he wants to rule the world and if you think about it, wanting to rule the world is not a crime." Mark Waid echoed Lee's assessment of the character, stating that Doom "[has got a great look, a great visual design a dynamite origin."
A ride called "Doctor Doom's Fearfall" is located at Islands of Adventure in the Universal Orlando Resort.
Merchandise.
Since Dr. Doom is one of Marvel's most prominent supervillains, he has been featured in many forms of merchandise, including various action figures and trading cards:

</doc>
<doc id="47498" url="https://en.wikipedia.org/wiki?curid=47498" title="All your base are belong to us">
All your base are belong to us

"All your base are belong to us" is a broken English ("Engrish") phrase found in the opening cutscene of the 1991 video game "Zero Wing" which became a popular Internet meme. The quote is included in the European version of the game, which features poor English translations of the original Japanese version.
The meme developed from this as the result of a GIF animation depicting the opening text which was initially popularized on the Something Awful message forums.
Mentions in media.
The phrase or some variation of lines from the game has appeared in numerous articles, books, comics, clothing, movies, radio shows, songs, television shows, video games, webcomics, and websites.
In November 2000, Kansas City computer programmer, Something Awful forum member, and part-time DJ Jeffrey Ray Roberts of the Gabber band The Laziest Men on Mars made a techno dance track, "Invasion of the Gabber Robots", which remixed some of the "Zero Wing" video game music by Tatsuya Uemura with a voice-over phrase "All your base are belong to us.". Tribal War forums member Bad_CRC in February 2001 created a video combining Roberts' song and the various images created in a Something Awful AYB photoshop thread, which proceeded to go viral.
On February 23, 2001, "Wired" provided an early report on the phenomenon, covering it from the Flash animation to its spread through e-mail and Internet forums to T-shirts bearing the phrase.
On April 1, 2003, in Sturgis, Michigan, seven people aged 17 to 20 placed signs all over town that read: "All your base are belong to us. You have no chance to survive make your time." They claimed to be playing an April Fool's joke but most people who saw the signs were unfamiliar with the phrase. Many residents were upset that the signs appeared while the U.S. was at war with Iraq and police chief Eugene Alli said the signs could be "a borderline terrorist threat depending on what someone interprets it to mean."
In February 2004, North Carolina State University students and members of TheWolfWeb in Raleigh, North Carolina exploited a web-based service provided for local schools and businesses to report a weather-related closing to display the phrase within a news ticker on a live news broadcast on News 14 Carolina.
On June 1, 2006, YouTube, a relatively new service at the time, was taken down temporarily for maintenance. The phrase "ALL YOUR VIDEO ARE BELONG TO US" appeared below the YouTube logo as a placeholder while the site was down. Some users believed the site had been hacked, leading YouTube to add the message "No, we haven't be hacked. Get a sense of humor."
The 2012 Walt Disney Pictures animated movie Wreck-It Ralph features a Graffiti of "All Your Base Are Belong To Us" on the wall of the Game Central Station at 7:30 seconds into the movie.

</doc>
<doc id="47499" url="https://en.wikipedia.org/wiki?curid=47499" title="Boreal">
Boreal

Boreal or boreale may refer to:

</doc>
<doc id="47500" url="https://en.wikipedia.org/wiki?curid=47500" title="Northern">
Northern

Northern may refer to the following:

</doc>
<doc id="47501" url="https://en.wikipedia.org/wiki?curid=47501" title="Brightness temperature">
Brightness temperature

Brightness temperature is the temperature a black body in thermal equilibrium with its surroundings would have to be to duplicate the observed intensity of a grey body object at a frequency formula_1.
This concept is extensively used in radio astronomy and planetary science.
The brightness temperature is not a temperature as ordinarily understood. It characterizes radiation, and depending on the mechanism of radiation can differ considerably from the physical temperature of a radiating body (though it is theoretically possible to construct a device which will heat up by a source of radiation with some brightness temperature to the actual temperature equal to brightness temperature). Nonthermal sources can have very high brightness temperatures. In pulsars the brightness temperature can reach 1026 K. For the radiation of a typical helium–neon laser with a power of 60 mW and a coherence length of 20 cm, focused in a spot with a diameter of 10 µm, the brightness temperature will be nearly .
For a black body, Planck's law gives:
where
formula_3 (the Intensity or Brightness) is the amount of energy emitted per unit surface area per unit time per unit solid angle and in the frequency range between formula_1 and formula_5; formula_6 is the temperature of the black body; formula_7 is Planck's constant; formula_1 is frequency; formula_9 is the speed of light; and formula_10 is Boltzmann's constant.
For a grey body the spectral radiance is a portion of the black body radiance, determined by the emissivity formula_11.
That makes the reciprocal of the brightness temperature:
At low frequency and high temperatures, when formula_13, we can use the Rayleigh–Jeans law:
so that the brightness temperature can be simply written as:
In general, the brightness temperature is a function of formula_1, and only in the case of blackbody radiation it is the same at all frequencies. The brightness temperature can be used to calculate the spectral index of a body, in the case of non-thermal radiation.
Calculating by frequency.
The brightness temperature of a source with known spectral radiance can be expressed as:
When formula_13 we can use the Rayleigh–Jeans law:
For narrowband radiation with very low relative spectral linewidth formula_20 and known radiance formula_21 we can calculate the brightness temperature as:
Calculating by wavelength.
Spectral radiance of black-body radiation is expressed by wavelength as:
So, the brightness temperature can be calculated as:
For long-wave radiation formula_25 the brightness temperature is:
For almost monochromatic radiation, the brightness temperature can be expressed by the radiance formula_21 and the coherence length formula_28:

</doc>
<doc id="47502" url="https://en.wikipedia.org/wiki?curid=47502" title="Calibration">
Calibration

Calibration is the process of finding a relationship between two quantities that are unknown (when the measurable quantities are not given a particular value for the amount considered or found a standard for the quantity). When one of quantity is known, which is made or set with one device, another measurement is made as similar way as possible with the first device using a second device.The measurable quantities may differ in two devices which are equivalent. 
The device with the known or assigned correctness is called the standard. The second device is the unit under test, test instrument, or any of several other names for the device being calibrated.
The formal definition of calibration by the International Bureau of Weights and Measures is the following: "Operation that, under specified conditions, in a first step, establishes a relation between the quantity values with measurement uncertainties provided by measurement standards and corresponding indications with associated measurement uncertainties (of the calibrated instrument or secondary standard) and, in a second step, uses this information to establish a relation for obtaining a measurement result from an indication."
History.
Origins.
The words "calibrate" and "calibration" entered the English language as recent as the American Civil War, in descriptions of artillery. Some of the earliest known systems of measurement and calibration seem to have been created between the ancient civilizations of Egypt, Mesopotamia and the Indus Valley, with excavations revealing the use of angular gradations for construction. The term "calibration" was likely first associated with the precise division of linear distance and angles using a dividing engine and the measurement of gravitational mass using a weighing scale. These two forms of measurement alone and their direct derivatives supported nearly all commerce and technology development from the earliest civilizations until about AD 1800.
Calibration of weights and distances ().
Early measurement devices were "direct", i.e. they had the same units as the quantity being measured. Examples include length using a yardstick and mass using a weighing scale. At the beginning of the twelfth century, during the reign of Henry I (1100-1135), it was decreed that a yard be "the distance from the tip of the King's nose to the end of his outstretched thumb." However, it wasn't until the reign of Richard I (1197) that we find documented evidence.
Other standardization attempts followed, such as the Magna Carta (1225) for liquid measures, until the Mètre des Archives from France and the establishment of the Metric system.
The Industrial Revolution and the calibration of pressure ().
One of the earliest pressure measurement devices was the Mercury barometer, credited to Torricelli (1643), which read atmospheric pressure using Mercury. Soon after, hydrostatic manometers were designed, with a linear calibration for measuring lower pressures ranges. The Industrial Revolution () saw widespread use of "indirect" measuring devices, in which the quantity being measured was derived functionally based on direct measurements of dependent quantities. During this time, scientists discovered the energy stored in compressed steam and other gases, leading to the development of gauges more practical than hydrostatic manometers at measuring higher pressures. One such invention was Eugene Bourdon's indirect design Bourdon tube.
In the direct reading hydrostatic manometer design on the left, an unknown applied pressure Pa pushes the liquid down the right side of the manometer U-tube, while a length scale next to the tube measures the pressure, referenced to the other, open end of the manometer on the left side of the U-tube (P0). The resulting height difference "H" is a direct measurement of the pressure or vacuum with respect to atmospheric pressure. The absence of pressure or vacuum would make H=0. The self-applied calibration would only require the length scale to be set to zero at that same point.
This direct measurement of pressure as a height difference depends on both the density of the manometer fluid, and a calibrated means of measuring the height difference.
In a Bourdon tube (shown in the two views on the right), applied pressure entering from the bottom on the silver barbed pipe tries to straighten a curved tube (or vacuum tries to curl the tube to a greater extent), moving the free end of the tube that is mechanically connected to the pointer. This is indirect measurement that depends on calibration to read pressure or vacuum correctly. No self-calibration is possible, but generally the zero pressure state is correctable by the user, as shown below.
Even in recent times, direct measurement is used to increase confidence in the validity of the measurements.
The age of the automobiles ().
In the early days of US automobile use, people wanted to see the gasoline they were about to buy in a big glass pitcher, a direct measure of volume and quality via appearance. By 1930, rotary flowmeters were accepted as indirect substitutes. A hemispheric viewing window allowed consumers to see the blade of the flowmeter turn as the gasoline was pumped (see image on the right). By 1970, the windows were gone and the measurement was totally indirect.
Indirect measurement always involve linkages or conversions of some kind. It is seldom possible to intuitively monitor the measurement. These facts intensify the need for calibration.
Most measurement techniques used today are indirect.
Modern calibration.
In contrast, the picture on the right shows the use of a 3666C automatic pressure calibrator, which is a device that consists of a control unit housing the electronics that drive the system, a pressure intensifier used to compress a gas such as Nitrogen, a pressure transducer used to detect desired levels in a hydraulic accumulator, and accessories such as liquid traps and gauge fittings.
Basic calibration process.
Purpose and scope.
The calibration process begins with the design of the measuring instrument that needs to be calibrated. The design has to be able to "hold a calibration" through its calibration interval. In other words, the design has to be capable of measurements that are "within engineering tolerance" when used within the stated environmental conditions over some reasonable period of time. Having a design with these characteristics increases the likelihood of the actual measuring instruments performing as expected.
Basically,the purpose of calibration is for maintaining the quality of measurement as well as to ensure the proper working of particular instrument.
Frequency.
The exact mechanism for assigning tolerance values varies by country and industry type. The measuring equipment manufacturer generally assigns the measurement tolerance, suggests a calibration interval (CI) and specifies the environmental range of use and storage. The using organization generally assigns the actual calibration interval, which is dependent on this specific measuring equipment's likely usage level. The assignment of calibration intervals can be a formal process based on the results of previous calibrations. The standards themselves are not clear on recommended CI values:
Standards required and accuracy.
The next step is defining the calibration process. The selection of a standard or standards is the most visible part of the calibration process. Ideally, the standard has less than 1/4 of the measurement uncertainty of the device being calibrated. When this goal is met, the accumulated measurement uncertainty of all of the standards involved is considered to be insignificant when the final measurement is also made with the 4:1 ratio. This ratio was probably first formalized in Handbook 52 that accompanied MIL-STD-45662A, an early US Department of Defense metrology program specification. It was 10:1 from its inception in the 1950s until the 1970s, when advancing technology made 10:1 impossible for most electronic measurements.
Maintaining a 4:1 accuracy ratio with modern equipment is difficult. The test equipment being calibrated can be just as accurate as the working standard. If the accuracy ratio is less than 4:1, then the calibration tolerance can be reduced to compensate. When 1:1 is reached, only an exact match between the standard and the device being calibrated is a completely correct calibration. Another common method for dealing with this capability mismatch is to reduce the accuracy of the device being calibrated.
For example, a gage with 3% manufacturer-stated accuracy can be changed to 4% so that a 1% accuracy standard can be used at 4:1. If the gage is used in an application requiring 16% accuracy, having the gage accuracy reduced to 4% will not affect the accuracy of the final measurements. This is called a limited calibration. But if the final measurement requires 10% accuracy, then the 3% gage never can be better than 3.3:1. Then perhaps adjusting the calibration tolerance for the gage would be a better solution. If the calibration is performed at 100 units, the 1% standard would actually be anywhere between 99 and 101 units. The acceptable values of calibrations where the test equipment is at the 4:1 ratio would be 96 to 104 units, inclusive. Changing the acceptable range to 97 to 103 units would remove the potential contribution of all of the standards and preserve a 3.3:1 ratio. Continuing, a further change to the acceptable range to 98 to 102 restores more than a 4:1 final ratio.
This is a simplified example. The mathematics of the example can be challenged. It is important that whatever thinking guided this process in an actual calibration be recorded and accessible. Informality contributes to tolerance stacks and other difficult to diagnose post calibration problems.
Also in the example above, ideally the calibration value of 100 units would be the best point in the gage's range to perform a single-point calibration. It may be the manufacturer's recommendation or it may be the way similar devices are already being calibrated. Multiple point calibrations are also used. Depending on the device, a zero unit state, the absence of the phenomenon being measured, may also be a calibration point. Or zero may be resettable by the user-there are several variations possible. Again, the points to use during calibration should be recorded.
There may be specific connection techniques between the standard and the device being calibrated that may influence the calibration. For example, in electronic calibrations involving analog phenomena, the impedance of the cable connections can directly influence the result.
Process description and documentation.
All of the information above is collected in a calibration procedure, which is a specific test method. These procedures capture all of the steps needed to perform a successful calibration. The manufacturer may provide one or the organization may prepare one that also captures all of the organization's other requirements. There are clearinghouses for calibration procedures such as the Government-Industry Data Exchange Program (GIDEP) in the United States.
This exact process is repeated for each of the standards used until transfer standards, certified reference materials and/or natural physical constants, the measurement standards with the least uncertainty in the laboratory, are reached. This establishes the traceability of the calibration.
See Metrology for other factors that are considered during calibration process development.
After all of this, individual instruments of the specific type discussed above can finally be calibrated. The process generally begins with a basic damage check. Some organizations such as nuclear power plants collect "as-found" calibration data before any routine maintenance is performed. After routine maintenance and deficiencies detected during calibration are addressed, an "as-left" calibration is performed.
More commonly, a calibration technician is entrusted with the entire process and signs the calibration certificate, which documents the completion of a successful calibration.
Success factors.
The basic process outlined above is a difficult and expensive challenge. The cost for ordinary equipment support is generally about 10% of the original purchase price on a yearly basis, as a commonly accepted rule-of-thumb. Exotic devices such as scanning electron microscopes, gas chromatograph systems and laser interferometer devices can be even more costly to maintain.
The extent of the calibration program exposes the core beliefs of the organization involved. The integrity of organization-wide calibration is easily compromised. Once this happens, the links between scientific theory, engineering practice and mass production that measurement provides can be missing from the start on new work or eventually lost on old work.
The 'single measurement' device used in the basic calibration process description above does exist. But, depending on the organization, the majority of the devices that need calibration can have several ranges and many functionalities in a single instrument. A good example is a common modern oscilloscope. There easily could be 200,000 combinations of settings to completely calibrate and limitations on how much of an all inclusive calibration can be automated.
Every organization using oscilloscopes has a wide variety of calibration approaches open to them. If a quality assurance program is in force, customers and program compliance efforts can also directly influence the calibration approach. Most oscilloscopes are capital assets that increase the value of the organization, in addition to the value of the measurements they make. The individual oscilloscopes are subject to depreciation for tax purposes over 3, 5, 10 years or some other period in countries with complex tax codes. The tax treatment of maintenance activity on those assets can bias calibration decisions.
New oscilloscopes are supported by their manufacturers for at least five years, in general. The manufacturers can provide calibration services directly or through agents entrusted with the details of the calibration and adjustment processes.
Very few organizations have only one oscilloscope. Generally, they are either absent or present in large groups. Older devices can be reserved for less demanding uses and get a limited calibration or no calibration at all. In production applications, oscilloscopes can be put in racks used only for one specific purpose. The calibration of that specific scope only has to address that purpose.
This whole process in repeated for each of the basic instrument types present in the organization, such as the digital multimeter pictured below.
Also the picture above shows the extent of the integration between Quality Assurance and calibration. The small horizontal unbroken paper seals connecting each instrument to the rack prove that the instrument has not been removed since it was last calibrated. These seals are also used to prevent undetected access to the adjustments of the instrument. There also are labels showing the date of the last calibration and when the calibration interval dictates when the next one is needed. Some organizations also assign unique identification to each instrument to standardize the record keeping and keep track of accessories that are integral to a specific calibration condition.
When the instruments being calibrated are integrated with computers, the integrated computer programs and any calibration corrections are also under control.
Quality.
To improve the quality of the calibration and have the results accepted by outside organizations it is desirable for the calibration and subsequent measurements to be "traceable" to the internationally defined measurement units. Establishing traceability is accomplished by a formal comparison to a standard which is directly or indirectly related to national standards ( such as NIST in the USA), international standards, or certified reference materials. This may be done by national standards laboratories operated by the government or by private firms offering metrology services.
Quality management systems call for an effective metrology system which includes formal, periodic, and documented calibration of all measuring instruments. ISO 9000 and ISO 17025 standards require that these traceable actions are to a high level and set out how they can be quantified.
Instrument calibration.
Calibration may be called for:
In general use, calibration is often regarded as including the process of adjusting the output or indication on a measurement instrument to agree with value of the applied standard, within a specified accuracy. For example, a thermometer could be calibrated so the error of indication or the correction is determined, and adjusted (e.g. via calibration constants) so that it shows the true temperature in Celsius at specific points on the scale. This is the perception of the instrument's end-user. However, very few instruments can be adjusted to exactly match the standards they are compared to. For the vast majority of calibrations, the calibration process is actually the comparison of an unknown to a known and recording the results.
International.
In many countries a National Metrology Institute (NMI) will exist which will maintain primary standards of measurement (the main SI units plus a number of derived units) which will be used to provide traceability to customer's instruments by calibration. The NMI supports the metrological infrastructure in that country (and often others) by establishing an unbroken chain, from the top level of standards to an instrument used for measurement. Examples of National Metrology Institutes are NPL in the UK, NIST in the United States, PTB in Germany and many others. Since the Mutual Recognition Agreement was signed it is now straightforward to take traceability from any participating NMI and it is no longer necessary for a company to obtain traceability for measurements from the NMI of the country in which it is situated.
To communicate the quality of a calibration the calibration value is often accompanied by a traceable uncertainty statement to a stated confidence level. This is evaluated through careful uncertainty analysis.
Some times a DFS (Departure From Spec) is required to operate machinery in a degraded state. Whenever this does happen, it must be in writing and authorized by a manager with the technical assistance of a calibration technician.
References.
Crouch, Stanley & Skoog, Douglas A. (2007). Principles of Instrumental Analysis. Pacific Grove: Brooks Cole. ISBN 0-495-01201-7.
IS:ISO:ISI:17025:2005

</doc>
<doc id="47503" url="https://en.wikipedia.org/wiki?curid=47503" title="Carbon cycle">
Carbon cycle

The carbon cycle is the biogeochemical cycle by which carbon is exchanged among the biosphere, pedosphere, geosphere, hydrosphere, and atmosphere of the Earth. Along with the nitrogen cycle and the water cycle, the carbon cycle comprises a sequence of events that are key to making the Earth capable of sustaining life; it describes the movement of carbon as it is recycled and reused throughout the biosphere, including carbon sinks.
The "global carbon budget" is the balance of the exchanges (incomes and losses) of carbon between the carbon reservoirs or between one specific loop (e.g., atmosphere <-> biosphere) of the carbon cycle. An examination of the carbon budget of a pool or reservoir can provide information about whether the pool or reservoir is functioning as a source or sink for carbon dioxide. The carbon cycle was initially discovered by Joseph Priestley and Antoine Lavoisier, and popularized by Humphry Davy.
Global climate.
Carbon-based molecules are crucial for life on Earth, because it is the main component of biological compounds. Carbon is also a major component of many minerals. Carbon also exists in various forms in the atmosphere. Carbon dioxide (CO2) is partly responsible for the greenhouse effect and is the most important human-contributed greenhouse gas.
In the past two centuries, human activities have seriously altered the global carbon cycle, most significantly in the atmosphere. Although carbon dioxide levels have changed naturally over the past several thousand years, human emissions of carbon dioxide into the atmosphere exceed natural fluctuations. Changes in the amount of atmospheric CO2 are considerably altering weather patterns and indirectly influencing oceanic chemistry.2 levels, CO2 levels cannot change significantly without affecting global temperatures--> Current carbon dioxide levels in the atmosphere exceed measurements from the last 420,000 years and levels are rising faster than ever recorded, making it of critical importance to better understand how the carbon cycle works and what its effects are on the global climate.
Main components.
The global carbon cycle is now usually divided into the following major reservoirs of carbon interconnected by pathways of exchange:
The carbon exchanges between reservoirs occur as the result of various chemical, physical, geological, and biological processes. The ocean contains the largest active pool of carbon near the surface of the Earth.
The natural flows of carbon between the atmosphere, ocean, terrestrial ecosystems, and sediments is fairly balanced, so that carbon levels would be roughly stable without human influence.
Atmosphere.
Carbon in the Earth's atmosphere exists in two main forms: carbon dioxide and methane. Both of these gases absorb and retain heat in the atmosphere and are partially responsible for the greenhouse effect. Methane produces a large greenhouse effect per volume as compared to carbon dioxide, but it exists in much lower concentrations and is more short-lived than carbon dioxide, making carbon dioxide the more important greenhouse gas of the two.
Carbon dioxide leaves the atmosphere through photosynthesis, thus entering the terrestrial and oceanic biospheres. Carbon dioxide also dissolves directly from the atmosphere into bodies of water (oceans, lakes, etc.), as well as dissolving in precipitation as raindrops fall through the atmosphere. When dissolved in water, carbon dioxide reacts with water molecules and forms carbonic acid, which contributes to ocean acidity. It can then be absorbed by rocks through weathering. It also can acidify other surfaces it touches or be washed into the ocean.
Human activity over the past two centuries has significantly increased the amount of carbon in the atmosphere, mainly in the form of carbon dioxide, both by modifying ecosystems' ability to extract carbon dioxide from the atmosphere and by emitting it directly, e.g., by burning fossil fuels and manufacturing concrete.
Terrestrial biosphere.
The terrestrial biosphere includes the organic carbon in all land-living organisms, both alive and dead, as well as carbon stored in soils. About 500 gigatons of carbon are stored above ground in plants and other living organisms, while soil holds approximately 1,500 gigatons of carbon. Most carbon in the terrestrial biosphere is organic carbon, while about a third of soil carbon is stored in inorganic forms, such as calcium carbonate. Organic carbon is a major component of all organisms living on earth. Autotrophs extract it from the air in the form of carbon dioxide, converting it into organic carbon, while heterotrophs receive carbon by consuming other organisms.
Because carbon uptake in the terrestrial biosphere is dependent on biotic factors, it follows a diurnal and seasonal cycle. In CO2 measurements, this feature is apparent in the Keeling curve. It is strongest in the northern hemisphere, because this hemisphere has more land mass than the southern hemisphere and thus more room for ecosystems to absorb and emit carbon.
Carbon leaves the terrestrial biosphere in several ways and on different time scales. The combustion or respiration of organic carbon releases it rapidly into the atmosphere. It can also be exported into the oceans through rivers or remain sequestered in soils in the form of inert carbon. Carbon stored in soil can remain there for up to thousands of years before being washed into rivers by erosion or released into the atmosphere through soil respiration. Between 1989 and 2008 soil respiration increased by about 0.1% per year. In 2008, the global total of CO2 released from the soil reached roughly 98 billion tonnes, about 10 times more carbon than humans are now putting into the atmosphere each year by burning fossil fuel. There are a few plausible explanations for this trend, but the most likely explanation is that increasing temperatures have increased rates of decomposition of soil organic matter, which has increased the flow of CO2. The length of carbon sequestering in soil is dependent on local climatic conditions and thus changes in the course of climate change. From pre-industrial era to 2010, the terrestrial biosphere represented a net source of atmospheric prior to 1940, switching subsequently to a net sink.
Oceans.
Oceans contain the greatest quantity of actively cycled carbon in this world and are second only to the lithosphere in the amount of carbon they store. The oceans' surface layer holds large amounts of dissolved inorganic carbon that is exchanged rapidly with the atmosphere. The deep layer's concentration of dissolved inorganic carbon (DIC) is about 15% higher than that of the surface layer. DIC is stored in the deep layer for much longer periods of time. Thermohaline circulation exchanges carbon between these two layers.
Carbon enters the ocean mainly through the dissolution of atmospheric carbon dioxide, which is converted into carbonate. It can also enter the oceans through rivers as dissolved organic carbon. It is converted by organisms into organic carbon through photosynthesis and can either be exchanged throughout the food chain or precipitated into the ocean's deeper, more carbon rich layers as dead soft tissue or in shells as calcium carbonate. It circulates in this layer for long periods of time before either being deposited as sediment or, eventually, returned to the surface waters through thermohaline circulation.
Oceanic absorption of CO2 is one of the most important forms of carbon sequestering limiting the human-caused rise of carbon dioxide in the atmosphere. However, this process is limited by a number of factors. Because the rate of CO2 dissolution in the ocean is dependent on the weathering of rocks and this process takes place slower than current rates of human greenhouse gas emissions, ocean CO2 uptake will decrease in the future. CO2 absorption also makes water more acidic, which affects ocean biosystems. The projected rate of increasing oceanic acidity could slow the biological precipitation of calcium carbonates, thus decreasing the ocean's capacity to absorb carbon dioxide.
Geological carbon cycle.
The geologic component of the carbon cycle operates slowly in comparison to the other parts of the global carbon cycle. It is one of the most important determinants of the amount of carbon in the atmosphere, and thus of global temperatures.
Most of the earth's carbon is stored inertly in the earth's lithosphere. Much of the carbon stored in the earth's mantle was stored there when the earth formed. Some of it was deposited in the form of organic carbon from the biosphere. Of the carbon stored in the geosphere, about 80% is limestone and its derivatives, which form from the sedimentation of calcium carbonate stored in the shells of marine organisms. The remaining 20% is stored as kerogens formed through the sedimentation and burial of terrestrial organisms under high heat and pressure. Organic carbon stored in the geosphere can remain there for millions of years.
Carbon can leave the geosphere in several ways. Carbon dioxide is released during the metamorphosis of carbonate rocks when they are subducted into the earth's mantle. This carbon dioxide can be released into the atmosphere and ocean through volcanoes and hotspots. It can also be removed by humans through the direct extraction of kerogens in the form of fossil fuels. After extraction, fossil fuels are burned to release energy, thus emitting the carbon they store into the atmosphere.
Human influence.
Since the industrial revolution, human activity has modified the carbon cycle by changing its component's functions and directly adding carbon to the atmosphere.
The largest and most direct human influence on the carbon cycle is through direct emissions from burning fossil fuels, which transfers carbon from the geosphere into the atmosphere. Humans also influence the carbon cycle indirectly by changing the terrestrial and oceanic biosphere.
Over the past several centuries, human-caused land use and land cover change (LUCC) has led to the loss of biodiversity, which lowers ecosystems' resilience to environmental stresses and decreases their ability to remove carbon from the atmosphere. More directly, it often leads to the release of carbon from terrestrial ecosystems into the atmosphere. Deforestation for agricultural purposes removes forests, which hold large amounts of carbon, and replaces them, generally with agricultural or urban areas. Both of these replacement land cover types store comparatively small amounts of carbon, so that the net product of the process is that more carbon stays in the atmosphere.
Other human-caused changes to the environment change ecosystems' productivity and their ability to remove carbon from the atmosphere. Air pollution, for example, damages plants and soils, while many agricultural and land use practices lead to higher erosion rates, washing carbon out of soils and decreasing plant productivity.
2 levels in the atmosphere increase decomposition rates in soil, thus returning CO2 stored in plant material more quickly to the atmosphere.
However, increased levels of CO2 in the atmosphere can also lead to higher gross primary production. It increases photosynthesis rates by allowing plants to more efficiently use water, because they no longer need to leave their stomata open for such long periods of time in order to absorb the same amount of carbon dioxide. This type of carbon dioxide fertilization affects mainly C3 plants, because C4 plants can already concentrate CO2 effectively.-->
Humans also affect the oceanic carbon cycle. Current trends in climate change lead to higher ocean temperatures, thus modifying ecosystems. Also, acid rain and polluted runoff from agriculture and industry change the ocean's chemical composition. Such changes can have dramatic effects on highly sensitive ecosystems such as coral reefs, thus limiting the ocean's ability to absorb carbon from the atmosphere on a regional scale and reducing oceanic biodiversity globally.
On 12 November 2015, NASA scientists reported that human-made carbon dioxide (CO2) continues to increase above levels not seen in hundreds of thousands of years: currently, about half of the carbon dioxide released from the burning of fossil fuels remains in the atmosphere and is not absorbed by vegetation and the oceans.

</doc>
<doc id="47505" url="https://en.wikipedia.org/wiki?curid=47505" title="Queen (chess)">
Queen (chess)

The queen (♕,♛) is the most powerful piece in the game of chess, able to move any number of squares vertically, horizontally or diagonally. Each player starts the game with one queen, placed in the middle of the first rank next to the king. Because of the value of a queen, it is sometimes used as bait to lure an opponent into a trap by a queen sacrifice. Another tactic is to use the queen to threaten the opponent's queen, to either retreat or to exchange the queen (losing both of them) to reduce the game to less powerful pieces. The queen is often used in conjunction with another piece, such as teamed with a bishop or rook, where the pieces could guard each other while threatening the opponent pieces.
With the chessboard oriented correctly, the white queen starts on a white square and the black queen starts on a black square. (Thus the mnemonics "queen gets her color", or "queen on [own color", or "the dress piece matches the shoes ", Latin "servat regina colorem".) In algebraic notation, the white queen starts on d1 and the black queen starts on d8. Because the queen is the most powerful piece, when a pawn is promoted, it is almost always promoted to a queen.
In the game "shatranj", the ancestor of chess included only male figures, the closest thing to the queen being the “vizier”, a weak piece only able to move or capture one step diagonally and not at all in any other direction. The modern chess queen gained power in the 15th century in concert with traditions of queenly rule in Europe. Examples of this power shift would be the "end of chivalry" by which Queen Elizabeth I ended the tradition that any knight could create another and made it exclusively the preserve of the monarch. 
The piece is archaically known as the "minister". In Polish it is known as the "Hetman" – the name of a major historical military-political office. In Russian it is known as "ferz'" (ферзь). The Arabic name of the piece is "Wazïr", the same as in shatranj.
Movement.
The queen can be moved any number of unoccupied squares in a straight line vertically, horizontally, or diagonally, thus combining the moves of the rook and bishop. The queen captures by occupying the square on which an enemy piece sits.
Although both players start with one queen each, a player can promote a pawn to any of several types of pieces, including a queen, when the pawn is moved to the player's furthest rank (the opponent's first rank). Such a queen created by promotion can be an additional queen, or if the player's queen has been captured, a replacement queen. Pawn promotion to a queen is colloquially called "queening", which is by far the most common type of piece a pawn is promoted to because of the relative power of a queen.
Piece value.
Ordinarily the queen is slightly more powerful than a rook and a bishop together, while slightly less powerful than two rooks. It is almost always disadvantageous to exchange the queen for a single piece other than the enemy's queen.
The reason the queen is more powerful than a combination of a rook and bishop, even though they control the same number of squares, is twofold. First, the queen is a more mobile unit than the rook and bishop, as the entire power of the queen can be transferred to another location in one move, while transferring the entire firepower of a rook and bishop requires two moves, and the bishop is always restricted to squares of one color. Second, the queen is not hampered by the bishop's inability to control squares of the opposite color to the square on which it stands. A factor in favor of the rook and bishop is that they can attack (or defend) a square twice, while a queen can only do so only once, but experience has shown that this factor is usually less significant than the points favoring the queen.
The queen is at her most powerful when the board is open, when the enemy king is not well defended, or when there are "loose" (i.e. undefended) pieces in the enemy camp. Because of her long range and ability to move in multiple directions, the queen is well equipped to execute forks. Compared to other long range pieces (i.e. rooks and bishops) the queen is less restricted and more powerful also in closed positions.
Strategy.
Beginners often develop the queen early in the game, hoping to plunder the enemy position and deliver an early checkmate such as Scholar's Mate. This can expose the easily harassed queen to attacks by weaker pieces causing the player to lose time. Experienced players generally prefer to delay developing the queen, and instead develop minor pieces in the opening.
Early queen attacks are rare in high level chess, but there are some openings with early queen development that are used by high level players. For example, the Scandinavian Defense (1.e4 d5), which often features queen moves by Black on the second and third moves is considered sound, and has been played at the world championship level. Some less common examples have also been observed in high level games. The Danvers Opening (1.e4 e5 2.Qh5), which is widely considered an opening suitable only for beginners, has occasionally been played by the strong American Grandmaster Hikaru Nakamura.
A queen exchange often marks the beginning of the endgame, but there are queen endgames, and sometimes queens are exchanged in the opening, long before the endgame. A common goal in the endgame is to promote a pawn to a queen. As the queen has the largest range and mobility, queen and king vs. lone king is an easy win when compared to some other basic mates.
Queen sacrifice.
A "queen sacrifice" is the deliberate sacrifice of a queen in order to gain a more favorable tactical position.
History.
The queen was originally the counsellor or prime minister or vizier (Sanskrit "mantri", Persian "farzīn", Arabic "firzān" or "firz"). Initially it could move only one square diagonally. Around 1300 CE its move was enhanced to allow it to move two squares with jump onto a same-colored square for its first move, to help the sides to come into contact sooner.
The "fers" changed into the queen over time. The first surviving mention of this piece as a queen or similar was "regina" in the Einsiedeln Poem, written in Latin around 997 and preserved in a monastery at Einsiedeln in Switzerland. Some surviving early medieval pieces depict the piece as a queen, and the word "fers" became grammatically feminized in several languages, for example "alferza" in Spanish and "fierce" or "fierge" in French, before it was replaced with names such as "reine" or "dame" (lady). The "Carmina Burana" also refer to the queen as "femina" (woman) and "coniunx" (spouse), and the name "Amazon" has sometimes been seen.
In Russian it keeps its Persian name of "ferz" to this day; "koroleva" (queen) is colloquial and is never used by professional chess players. However, the names "korolevna" (king's daughter), "tsaritsa" (tsar's wife), and "baba" (old woman) are attested as early as 1694. In Arabic countries the queen remains termed, and in some cases depicted as, a vizier.
Historian Marilyn Yalom proposes that the prominence of medieval queens such as Eleanor of Aquitaine and Blanche of Castile and Isabella I of Castile, the cult of the Virgin Mary, and the power ascribed to women in the troubadour tradition of courtly love, might have been partly responsible for influencing the piece towards its identity as a queen and later its modern great power on the board, as might the medieval popularity of chess as a game particularly suitable for women to play on equal terms with men. She points to medieval poetry depicting the Virgin as the chess-queen of God or "Fierce Dieu". Significantly, the earliest surviving treatise to describe the modern movement of the queen (as well as the bishop and pawn), "Repetición de amores e arte de axedres con CL iuegos de partido" ("Discourses on Love and the Art of Chess with 150 Problems") by Luis Ramírez de Lucena, was published during the reign of Isabella I of Castile. Well before the queen's powers expanded, it was already being romantically described as essential to the king's survival, so that when the queen was lost, there was nothing more of value on the board.
Marilyn Yalom wrote that:
During the 16th century the queen's move took its modern form as a combination of the move of the rook and the current move of the bishop. Starting from Spain, this new version - called "queen's chess" ("scacchi de la donna"), or pejoratively "madwoman's chess" ("scacchi alla rabiosa") - spread throughout Europe rapidly, partly due to the advent of the printing press and the popularity of new books on chess. The new rules faced a backlash in some quarters, ranging from anxiety over a powerful female warrior figure to frank abuse against women in general.
At various times, the ability of pawns to be queened was restricted while the original queen was still on the board, so as not to cause scandal by providing the king with more than one queen. An early twelfth-century Latin poem refers to a queened pawn as a "ferzia", as opposed to the original queen or "regina", to account for this.
When the queen was attacked, it was customary to warn the opponent by announcing ""gardez la reine"" or simply ""gardez"", similar to the announcement of "check". Some published rules even required this announcement before the queen could be legally captured. This custom was largely abandoned in the 19th century.
In Russia for a long time the queen could also move like a knight; some players disapproved of this ability to "gallop like the horse" (knight). The book "A History of Chess" by H.J.R.Murray, page 384, says that a Mr.Coxe who was in Russia in 1772 saw chess played with the queen also moving like a knight. Such an augmented queen piece is now known as the fairy chess piece amazon.
Unicode.
Unicode defines two codepoints for queen:
♕ U+2655 White Chess Queen (HTML &#9813;)
♛ U+265B Black Chess Queen (HTML &#9819;)

</doc>
<doc id="47506" url="https://en.wikipedia.org/wiki?curid=47506" title="520s BC">
520s BC


</doc>
<doc id="47507" url="https://en.wikipedia.org/wiki?curid=47507" title="510s BC">
510s BC

Deaths.
Evans, John Karl. "War, Women, and Children in Ancient Rome" (London, 1991).

</doc>
<doc id="47510" url="https://en.wikipedia.org/wiki?curid=47510" title="Cirrus cloud">
Cirrus cloud

Cirrus (cloud classification symbol: Ci) is a genus of atmospheric cloud generally characterized by thin, wispy strands, giving the type its name from the Latin word "cirrus" meaning a ringlet or curling lock of hair. The strands of cloud sometimes appear in tufts of a distinctive form referred to by the common name of "mares' tails".
On planet Earth, cirrus generally appears white or light gray in color. It forms when water vapor undergoes deposition at altitudes above in temperate regions and above in tropical regions. It also forms from the outflow of tropical cyclones or the anvils of cumulonimbus cloud. Since cirrus clouds arrive in advance of the frontal system or tropical cyclone, it indicates that weather conditions may soon deteriorate. While it indicates the arrival of precipitation (rain), cirrus clouds only produce fall streaks (falling ice crystals that evaporate before landing on the ground).
Jet stream-powered cirrus can grow long enough to stretch across continents while remaining only a few kilometers deep. When visible light interacts with the ice crystals in cirrus cloud, it produces optical phenomena such as sun dogs and haloes. Cirrus is known to raise the temperature of the air beneath the main cloud layer by an average of 10 °C (18 °F). When the individual filaments become so extensive that they are virtually indistinguishable from one another, they form a sheet of high cloud called cirrostratus. Convection at high altitudes can produce another high-based genus called cirrocumulus, a pattern of small cloud tufts that contain droplets of supercooled water.
Cirrus clouds form on other planets, including Mars, Jupiter, Saturn, Uranus, and possibly Neptune. They have even been seen on Titan, one of Saturn's moons. Some of these extraterrestrial cirrus clouds are composed of ammonia or methane ice rather than water ice. The term "cirrus" is also used for certain interstellar clouds composed of sub-micrometer-sized dust grains.
Description.
Cirrus cloud ranges in thickness from to , with an average thickness of . There are, on average, 30 ice crystals per liter (96 ice crystals per gallon), but this ranges from one ice crystal per 10,000 liters (3.7 ice crystals per 10,000 gallons) to 10,000 ice crystals per liter (37,000 ice crystals per gallon), a difference of eight orders of magnitude. The length of each of these ice crystals is usually 0.25 millimeters long, but they range from as short as 0.01 millimeters or as long as several millimeters. The ice crystals in contrails are much smaller than those in naturally-occurring cirrus cloud, as they are around 0.001 millimeters to 0.1 millimeters in length. Cirrus can vary in temperature from to .
The ice crystals in cirrus clouds have different shapes in addition to different sizes. Some shapes include solid columns, hollow columns, plates, rosettes, and conglomerations of the various other types. The shape of the ice crystals is determined by the air temperature, atmospheric pressure, and ice supersaturation. Cirrus in temperate regions typically have the shapes segregated by type: the columns and plates tend to be at the top of the cloud, whereas the rosettes and conglomerations tend to be near the base. In the northern Arctic region, cirrus tend to be composed of only the columns, plates, and conglomerations, and these crystals tend to be at least four times larger than the minimum size. In Antarctica, cirrus are usually composed of only the columns, and these columns are much longer than normal.
Scientists have studied the characteristics of cirrus using several different methods. One, Light Detection and Ranging (LiDAR), gives highly accurate information on the cloud's altitude, length, and width. Balloon-carried hygrometers give information on the humidity of the cirrus cloud but are not accurate enough to measure the depth of the cloud. Radar units give information on the altitudes and thicknesses of cirrus clouds. Another data source is satellite measurements from the Stratospheric Aerosol and Gas Experiment (SAGE) program. These satellites measure where infrared radiation is absorbed in the atmosphere, and if it is absorbed at cirrus altitudes, then it is assumed that there are cirrus clouds in that location. The United States National Aeronautics and Space Administration's (NASA) MODerate resolution Imaging Spectroradiometer (MODIS) also gives information on the cirrus cloud cover by measuring reflected infrared radiation of various specific frequencies during the day. During the night, it determines cirrus cover by detecting the Earth's infrared emissions. The cloud reflects this radiation back to the ground, thus enabling satellites to see the "shadow" it casts into space. Visual observations from aircraft or the ground provide additional information about cirrus clouds.
Based upon data taken from the United States using these methods, cirrus cloud cover was found to vary diurnally and seasonally. The researchers found that in the summer, at noon, the cover is the lowest, with an average of 23% of the United States' land area covered by cirrus. Around midnight, the cloud cover increases to around 28%. In winter, the cirrus cloud cover did not vary appreciably from day to night. These percentages include clear days and nights, as well as days and nights with other cloud types, as lack of cirrus cloud cover. When these clouds are present, the typical coverage ranges from 30% to 50%. Based on satellite data, cirrus covers an average of 20% to 25% of the Earth's surface. In the tropical regions, this cloud covers around 70% of the region's surface area.
Cirrus clouds often produce hair-like filaments—similar to the virga produced in liquid–water clouds—called fall streaks, and they are made of heavier ice crystals that fall from the cloud. The sizes and shapes of fall streaks are determined by the wind shear.
Cirrus comes in four distinct species; Cirrus "castellanus", "fibratus", "spissatus", and "uncinus"; which are each divided into four varieties: "intortus", "vertebratus", "radiatus", and "duplicatus". "Cirrus castellanus" is a species that has cumuliform tops caused by high-altitude convection rising up from the main cloud body. "Cirrus fibratus" looks striated and is the most common cirrus species. "Cirrus uncinus" clouds are hooked and are the form that is usually called "mare's tails". Of the varieties, "Cirrus intortus" has an extremely contorted shape, and "cirrus radiatus" has large, radial bands of cirrus clouds that stretch across the sky. Kelvin–Helmholtz waves are a form of cirrus intortus that has been twisted into loops by vertical wind shear.
Formation.
Cirrus clouds are formed when water vapor undergoes deposition at high altitudes where the atmospheric pressure ranges from 600 mbar at above sea level to 200 mbar at above sea level. These conditions commonly occur at the leading edge of a warm front. Because humidity is low at such high altitudes, this genus-type tends to be very thin.
Cyclones.
Cirrus forms from tropical cyclones, and is commonly seen fanning out from the eyewalls of hurricanes. A large shield of cirrus and cirrostratus typically accompanies the high altitude outflow of hurricanes or typhoons, and these can make the underlying rain bands—and sometimes even the eye—difficult to detect in satellite photographs.
Thunderstorms.
Thunderstorms can form dense cirrus at their tops. As the cumulonimbus cloud in a thunderstorm grows vertically, the liquid water droplets freeze when the air temperature reaches the freezing point. The anvil cloud takes its shape because the temperature inversion at the tropopause prevents the warm, moist air forming the thunderstorm from rising any higher, thus creating the flat top. In the tropics, these thunderstorms occasionally produce copious amounts of cirrus from their anvils. High-altitude winds commonly push this dense mat out into an anvil shape that stretches downwind as much as several kilometers.
Individual cirrus cloud formations can be the remnants of anvil clouds formed by thunderstorms. In the dissipating stage of a cumulonimbus cloud, when the normal column rising up to the anvil has evaporated or dissipated, the mat of cirrus in the anvil is all that is left.
Contrails.
Contrails are a manmade type of cirrus cloud formed when water vapor from the exhaust of a jet engine condenses on particles, which come from either the surrounding air or the exhaust itself, and freezes, leaving behind a visible trail. The exhaust can also trigger the formation of cirrus by providing ice nuclei when there is an insufficient naturally-occurring supply in the atmosphere. One of the environmental impacts of aviation is that persistent contrails can form into large mats of cirrus, and increased air traffic has been implicated as one possible cause of the increasing frequency and amount of cirrus in Earth's atmosphere.
Subforms.
Species.
Cirrus Fibratus clouds appear as thin and fibrous looking. Noticeably, they are the most common type of cirrus cloud. They might indicate windy weather, since they are mostly formed by wind shear on higher altitudes.
Cirrus Uncinus clouds appear as thin and fibrous like, except that they always have a hook or curl on the tip.
Cirrus Spissatus clouds are the highest clouds of the main cloud genera. They may form in the higher tropopause or even at the lower stratosphere. They are dense and opaque, not allowing the light of the sun or moon to pass through. They are more common on the anvils of cumulonimbus clouds.
Cirrus Floccus is derived from Latin, which means "lock of wool" or cirrus with ragged bases. They are not to be confused with cirrocumulus floccus since they are larger than cirrocumulus and mostly are isolated. A precipitation-based supplementary feature virga is mostly visible, which makes it easier to distinguish the difference between cirrocumulus floccus and cirrus floccus.
Cirrus Castellanus is derived from Latin, which means "castle" or round turrets. They indicate that atmospheric instability is occurring on the higher altitudes of the troposphere. They appear as tall clouds that originate from a flat base.
Opacity-based Varieties.
Cirrus clouds have no opacity-based varieties, due to them being translucent, and only one species is opaque, which is cirrus spissatus.
Pattern-based Varieties.
Cirrus Intortus clouds is derived from Latin which means "twisted", or "wound". These clouds appear twisted, or simply, intorted.
Cirrus Vertebratus clouds, resembling the appearance of bones as hinted by the word "vertebratus". They look more like fish bones, with fibrous lines that originate from one which resembles the appearance of fish bones.
Cirrus Radiatus clouds appear as parallel lines that seem to originate from one point and can stretch for hundreds of miles, and they move parallel to the wind shear.
Use in forecasting.
Random, isolated cirrus do not have any particular significance. A large number of cirrus clouds can be a sign of an approaching frontal system or upper air disturbance. This signals a change in weather in the near future, which usually becomes stormier. If the cloud is a cirrus castellanus, there might be instability at the high altitude level. When the clouds deepen and spread, especially when they are of the "cirrus radiatus" variety or "cirrus fibratus" species, this usually indicates an approaching weather front. If it is a warm front, the cirrus clouds spread out into cirrostratus, which then thicken and lower into altocumulus and altostratus. The next set of clouds are the rain-bearing nimbostratus clouds. When cirrus clouds precede a cold front, squall line or multicellular thunderstorm, it is because they are blown off the anvil, and the next to arrive are the cumulonimbus clouds. Kelvin-Helmholtz waves indicate extreme wind shear at high levels.
Within the tropics, 36 hours prior to the center passage of a tropical cyclone, a veil of white cirrus clouds approaches from the direction of the cyclone. In the mid to late 19th century, forecasters used these cirrus veils to predict the arrival of hurricanes. In the early 1870s the president of Belén College in Havana, Cuba, Father Benito Viñes, developed the first hurricane forecasting system, and he mainly used the motion of these clouds in formulating his predictions. He would observe the clouds hourly from 4:00 am to 10:00 pm. After accumulating enough information, Viñes began accurately predicting the paths of hurricanes, and he eventually summarized his observations in his book, "Apuntes Relativos a los Huracanes de las Antilles".
Effects on climate.
Cirrus clouds cover up to 25% of the Earth and have a net heating effect. When they are thin and translucent, the clouds efficiently absorb outgoing infrared radiation while only marginally reflecting the incoming sunlight. When cirrus clouds are thick, they reflect only around 9% of the incoming sunlight, but they prevent almost 50% of the outgoing infrared radiation from escaping, thus raising the temperature of the atmosphere beneath the clouds by an average of 10 °C (18 °F)—a process known as the greenhouse effect. Averaged worldwide, cloud formation results in a temperature loss of 5 °C (9 °F) at the earth's surface, mainly the result of stratocumulus clouds.
As a result of their warming effects when relatively thin, cirrus clouds have been implicated as a potential partial cause of global warming. Scientists have speculated that global warming could cause high thin cloud cover to increase, thereby increasing temperatures and humidity. This, in turn, would increase the cirrus cloud cover, effectively creating a positive feedback circuit. A prediction of this hypothesis is that the cirrus would move higher as the temperatures rose, increasing the volume of air underneath the clouds and the amount of infrared radiation reflected back down to earth. In addition, the hypothesis suggests that the increase in temperature would tend to increase the size of the ice crystals in the cirrus cloud, possibly causing the reflection of solar radiation and the reflection of the Earth's infrared radiation to balance out.
A similar hypothesis put forth by Richard Lindzen is the iris hypothesis in which an increase in tropical sea surface temperatures results in less cirrus clouds and thus more infrared radiation emitted to space.
Optical phenomena.
Cirrus clouds, like cirrostratus clouds, can produce several optical effects, such as halos around the sun and moon. Halos are caused by interaction of the light with hexagonal ice crystals present in the clouds, which, depending on their shape and orientation, can result in a wide variety of white and colored rings, arcs and spots in the sky. Common halo varieties are the 22° halo, sun dogs, the circumzenithal arc and the circumhorizontal arc. Halos produced by cirrus clouds tend to be more pronounced and colorful than those caused by cirrostratus.
More rarely, cirrus clouds are capable of producing glories, more commonly associated with liquid water-based clouds such as stratus. A glory is a set of concentric, faintly-colored glowing rings that appear around the shadow of the observer, and are best observed from a high viewpoint or from a plane. Cirrus clouds only form glories when the constituent ice crystals are aspherical, and researchers suggest that the ice crystals must be between 0.009 millimeters and 0.015 millimeters in length. 
Relation to other clouds.
Cirrus clouds are one of three different genera of high-étage (high-level) clouds. High-étage clouds form at and above in temperate regions. The other two genera, cirrocumulus and cirrostratus, are also high clouds.
In the intermediate range, from to in temperate regions, are the mid-étage clouds. They comprise two or three genera depending on the system of height classification being used: altostratus, altocumulus, and, according to WMO classification, nimbostratus. These clouds are formed from ice crystals, supercooled water droplets, or liquid water droplets.
Low-étage clouds form at less than . The two genera that are strictly low-étage are stratus, and stratocumulus. These clouds are composed of water droplets, except during winter when they are formed of supercooled waterdroplets or ice crystals if the temperature at cloud level is below freezing. Two additional genera usually form in the low altitude range, but may be based at higher levels under conditions of very low humidity. They comprise the genera cumulus, and cumulonimbus, which along with nimbostratus, are often classified separately as clouds of vertical development, especially when their tops are high enough to be composed of super-cooled water droplets or ice crystals.
The altitudes of high-étage clouds like cirrus vary considerably with latitude. In the polar regions, they are at their lowest, with a minimum altitude of only to a maximum of . In tropical regions, they are at their highest, ranging in altitude from about to around . In temperate regions, they range in altitude from to —a variation in contrast to low-étage clouds, which do not appreciably change altitude with latitude.
Summary of high cloud genera.
There are three main genera in the family of high clouds: cirrus, cirrocumulus, and cirrostratus. Cirrostratus clouds commonly produce halos because they are composed almost entirely of ice crystals. Cirrocumulus and cirrostratus are sometimes informally referred to as "cirriform clouds" because of their frequent association with cirrus. They are given the prefix "cirro-", but this refers more to their altitude range than their physical structure. Cirrocumulus in its pure form is actually a high cumuliform genus, and cirrostratus is stratiform, like altostratus and lower based sheet clouds.
Cirrocumulus.
Cirrocumulus clouds form in sheets or patches and do not cast shadows. They commonly appear in regular, rippling patterns or in rows of clouds with clear areas between. Cirrocumulus are, like other members of the cumuliform category, formed via convective processes. Significant growth of these patches indicates high-altitude instability and can signal the approach of poorer weather. The ice crystals in the bottoms of cirrocumulus clouds tend to be in the form of hexagonal cylinders. They are not solid, but instead tend to have stepped funnels coming in from the ends. Towards the top of the cloud, these crystals have a tendency to clump together. These clouds do not last long, and they tend to change into cirrus because as the water vapor continues to deposit on the ice crystals, they eventually begin to fall, destroying the upward convection. The cloud then dissipates into cirrus. Cirrocumulus clouds come in four species: "stratiformis", "lenticularis", "castellanus", and "floccus". They are iridescent when the constituent supercooled water droplets are all about the same size.
Cirrostratus.
Cirrostratus clouds can appear as a milky sheen in the sky or as a striated sheet. They are sometimes similar to altostratus and are distinguishable from the latter because the sun or moon is always clearly visible through transparent cirrostratus, in contrast to altostratus which tends to be opaque or translucent. Cirrostratus come in two species, "fibratus" and "nebulosus". The ice crystals in these clouds vary depending upon the height in the cloud. Towards the bottom, at temperatures of around to , the crystals tend to be long, solid, hexagonal columns. Towards the top of the cloud, at temperatures of around to , the predominant crystal types are thick, hexagonal plates and short, solid, hexagonal columns. These clouds commonly produce halos, and sometimes the halo is the only indication that such clouds are present. They are formed by warm, moist air being lifted slowly to a very high altitude. When a warm front approaches, cirrostratus clouds become thicker and descend forming altostratus clouds, and rain usually begins 12 to 24 hours later.
Extraterrestrial.
Cirrus clouds have been observed on several other planets. On September 18, 2008, the Martian Lander "Phoenix" took a time-lapse photograph of a group of cirrus clouds moving across the Martian sky using LiDAR. Near the end of its mission, the Phoenix Lander detected more thin clouds close to the north pole of Mars. Over the course of several days, they thickened, lowered, and eventually began snowing. The total precipitation was only a few thousandths of a millimeter. James Whiteway from York University concluded that "precipitation is a component of the hydrologic cycle." These clouds formed during the Martian night in two layers, one around above ground and the other at surface level. They lasted through early morning before being burned away by the sun. The crystals in these clouds were formed at a temperature of , and they were shaped roughly like ellipsoids 0.127 millimeters long and 0.042 millimeters wide.
On Jupiter, cirrus clouds are composed of ammonia. When Jupiter's South Equatorial Belt disappeared, one hypothesis put forward by Glenn Orten was that a large quantity of ammonia cirrus clouds had formed above it, hiding it from view. NASA's Cassini probe detected these clouds on Saturn and thin water-ice cirrus on Saturn's moon Titan. Cirrus clouds composed of methane ice exist on Uranus. On Neptune, thin wispy clouds which could possibly be cirrus have been detected over the Great Dark Spot. As on Uranus, these are probably methane crystals.
Interstellar cirrus clouds are composed of tiny dust grains smaller than a micrometer and are therefore not true clouds of this genus which are composed of ice crystals or other frozen liquids. They range from a few light years to dozens of light years across. While they are not technically cirrus clouds, the dust clouds are referred to as "cirrus" because of their similarity to the clouds on Earth. They also emit infrared radiation, similar to the way cirrus clouds on Earth reflect heat being radiated out into space.
Sources.
Footnotes
Bibliography

</doc>
<doc id="47512" url="https://en.wikipedia.org/wiki?curid=47512" title="Climate change">
Climate change

Climate change is a change in the statistical distribution of weather patterns when that change lasts for an extended period of time (i.e., decades to millions of years). Climate change may refer to a change in average weather conditions, or in the time variation of weather around longer-term average conditions (i.e., more or fewer extreme weather events). Climate change is caused by factors such as biotic processes, variations in solar radiation received by Earth, plate tectonics, and volcanic eruptions. Certain human activities have also been identified as significant causes of recent climate change, often referred to as "global warming".
Scientists actively work to understand past and future climate by using observations and theoretical models. A climate record—extending deep into the Earth's past—has been assembled, and continues to be built up, based on geological evidence from borehole temperature profiles, cores removed from deep accumulations of ice, floral and faunal records, glacial and periglacial processes, stable-isotope and other analyses of sediment layers, and records of past sea levels. More recent data are provided by the instrumental record. General circulation models, based on the physical sciences, are often used in theoretical approaches to match past climate data, make future projections, and link causes and effects in climate change.
Terminology.
The most general definition of "climate change" is a change in the statistical properties (principally its mean and spread) Accordingly, fluctuations over periods shorter than a few decades, such as El Niño, do not represent climate change.
The term sometimes is used to refer specifically to climate change caused by human activity, as opposed to changes in climate that may have resulted as part of Earth's natural processes.
In this sense, especially in the context of environmental policy, the term "climate change" has become synonymous with "anthropogenic global warming". Within scientific journals, "global warming" refers to surface temperature increases while "climate change" includes global warming and everything else that increasing greenhouse gas levels affect.
Causes.
On the broadest scale, the rate at which energy is received from the Sun and the rate at which it is lost to space determine the equilibrium temperature and climate of Earth. This energy is distributed around the globe by winds, ocean currents, and other mechanisms to affect the climates of different regions.
Factors that can shape climate are called climate forcings or "forcing mechanisms". These include processes such as variations in solar radiation, variations in the Earth's orbit, variations in the albedo or reflectivity of the continents and oceans, mountain-building and continental drift and changes in greenhouse gas concentrations. There are a variety of climate change feedbacks that can either amplify or diminish the initial forcing. Some parts of the climate system, such as the oceans and ice caps, respond more slowly in reaction to climate forcings, while others respond more quickly. There are also key threshold factors which when exceeded can produce rapid change.
Forcing mechanisms can be either "internal" or "external". Internal forcing mechanisms are natural processes within the climate system itself (e.g., the thermohaline circulation). External forcing mechanisms can be either natural (e.g., changes in solar output) or anthropogenic (e.g., increased emissions of greenhouse gases).
Whether the initial forcing mechanism is internal or external, the response of the climate system might be fast (e.g., a sudden cooling due to airborne volcanic ash reflecting sunlight), slow (e.g. thermal expansion of warming ocean water), or a combination (e.g., sudden loss of albedo in the arctic ocean as sea ice melts, followed by more gradual thermal expansion of the water). Therefore, the climate system can respond abruptly, but the full response to forcing mechanisms might not be fully developed for centuries or even longer.
Internal forcing mechanisms.
Scientists generally define the five components of earth's climate system to include atmosphere, hydrosphere, cryosphere, lithosphere (restricted to the surface soils, rocks, and sediments), and biosphere. Natural changes in the climate system ("internal forcings") result in internal "climate variability". Examples include the type and distribution of species, and changes in ocean currents.
Ocean variability.
The ocean is a fundamental part of the climate system, some changes in it occurring at longer timescales than in the atmosphere, as it has hundreds of times more mass and thus very high thermal inertia, with effects such as the ocean depths still lagging today in temperature adjustment from effects of the Little Ice Age of past centuries).
Short-term fluctuations (years to a few decades) such as the El Niño-Southern Oscillation, the Pacific decadal oscillation, the North Atlantic oscillation, and the Arctic oscillation, represent climate variability rather than climate change. On longer time-scales, alterations to ocean processes such as thermohaline circulation play a key role in redistributing heat by carrying out a very slow and extremely deep movement of water and the long-term redistribution of heat in the world's oceans.
Life.
Life affects climate through its role in the carbon and water cycles and through such mechanisms as albedo, evapotranspiration, cloud formation, and weathering. Examples of how life may have affected past climate include:
External forcing mechanisms.
Orbital variations.
Slight variations in Earth's orbit lead to changes in the seasonal distribution of sunlight reaching the Earth's surface and how it is distributed across the globe. There is very little change to the area-averaged annually averaged sunshine; but there can be strong changes in the geographical and seasonal distribution. The three types of orbital variations are variations in Earth's eccentricity, changes in the tilt angle of Earth's axis of rotation, and precession of Earth's axis. Combined together, these produce Milankovitch cycles which have a large impact on climate and are notable for their correlation to glacial and interglacial periods, their correlation with the advance and retreat of the Sahara, and for their appearance in the stratigraphic record.
The IPCC notes that Milankovitch cycles drove the ice age cycles, CO2 followed temperature change "with a lag of some hundreds of years," and that as a feedback amplified temperature change. The depths of the ocean have a lag time in changing temperature (thermal inertia on such scale). Upon seawater temperature change, the solubility of CO2 in the oceans changed, as well as other factors impacting air-sea CO2 exchange.
Solar output.
The Sun is the predominant source of energy input to the Earth. Other sources include geothermal energy from the Earth's core, and heat from the decay of radioactive compounds. Both long- and short-term variations in solar intensity are known to affect global climate.
Three to four billion years ago, the Sun emitted only 70% as much power as it does today. If the atmospheric composition had been the same as today, liquid water should not have existed on Earth. However, there is evidence for the presence of water on the early Earth, in the Hadean and Archean eons, leading to what is known as the faint young Sun paradox. Hypothesized solutions to this paradox include a vastly different atmosphere, with much higher concentrations of greenhouse gases than currently exist. Over the following approximately 4 billion years, the energy output of the Sun increased and atmospheric composition changed. The Great Oxygenation Event – oxygenation of the atmosphere around 2.4 billion years ago – was the most notable alteration. Over the next five billion years, the Sun's ultimate death as it becomes a red giant and then a white dwarf will have large effects on climate, with the red giant phase possibly ending any life on Earth that survives until that time.
Solar output also varies on shorter time scales, including the 11-year solar cycle and longer-term modulations. Solar intensity variations possibly as a result of the Wolf, Spörer and Maunder Minimum are considered to have been influential in triggering the Little Ice Age, and some of the warming observed from 1900 to 1950. The cyclical nature of the Sun's energy output is not yet fully understood; it differs from the very slow change that is happening within the Sun as it ages and evolves. Research indicates that solar variability has had effects including the Maunder minimum from 1645 to 1715 A.D., part of the Little Ice Age from 1550 to 1850 A.D. that was marked by relative cooling and greater glacier extent than the centuries before and afterward. Some studies point toward solar radiation increases from cyclical sunspot activity affecting global warming, and climate may be influenced by the sum of all effects (solar variation, anthropogenic radiative forcings, etc.).
Interestingly, a 2010 study "suggests", “that the effects of solar variability on temperature throughout the atmosphere may be contrary to current expectations.”
In an Aug 2011 Press Release, CERN announced the publication in the Nature journal the initial results from its CLOUD experiment. The results indicate that ionisation from cosmic rays significantly enhances aerosol formation in the presence of sulfuric acid and water, but in the lower atmosphere where ammonia is also required, this is insufficient to account for aerosol formation and additional trace vapours must be involved. The next step is to find more about these trace vapours, including whether they are of natural or human origin.
Volcanism.
The eruptions considered to be large enough to affect the Earth's climate on a scale of more than 1 year are the ones that inject over 100,000 tons of SO2 into the stratosphere. This is due to the optical properties of SO2 and sulfate aerosols, which strongly absorb or scatter solar radiation, creating a global layer of sulfuric acid haze. On average, such eruptions occur several times per century, and cause cooling (by partially blocking the transmission of solar radiation to the Earth's surface) for a period of a few years.
The eruption of Mount Pinatubo in 1991, the second largest terrestrial eruption of the 20th century, affected the climate substantially, subsequently global temperatures decreased by about 0.5 °C (0.9 °F) for up to three years. Thus, the cooling over large parts of the Earth reduced surface temperatures in 1991-93, the equivalent to a reduction in net radiation of 4 watts per square meter. The Mount Tambora eruption in 1815 caused the Year Without a Summer. Much larger eruptions, known as large igneous provinces, occur only a few times every fifty - hundred million years - through flood basalt, and caused in Earth past global warming and mass extinctions.
Small eruptions, with injections of less than 0.1 Mt of sulfur dioxide into the stratosphere, impact the atmosphere only subtly, as temperature changes are comparable with natural variability. However, because smaller eruptions occur at a much higher frequency, they too have a significant impact on Earth's atmosphere.
Seismic monitoring maps current and future trends in volcanic activities, and tries to develop early warning systems. In climate modelling the aim is to study the physical mechanisms and feedbacks of volcanic forcing.
Volcanoes are also part of the extended carbon cycle. Over very long (geological) time periods, they release carbon dioxide from the Earth's crust and mantle, counteracting the uptake by sedimentary rocks and other geological carbon dioxide sinks. The US Geological Survey estimates are that volcanic emissions are at a much lower level than the effects of current human activities, which generate 100–300 times the amount of carbon dioxide emitted by volcanoes. A review of published studies indicates that annual volcanic emissions of carbon dioxide, including amounts released from mid-ocean ridges, volcanic arcs, and hot spot volcanoes, are only the equivalent of 3 to 5 days of human caused output. The annual amount put out by human activities may be greater than the amount released by supererruptions, the most recent of which was the Toba eruption in Indonesia 74,000 years ago.
Although volcanoes are technically part of the lithosphere, which itself is part of the climate system, the IPCC explicitly defines volcanism as an external forcing agent.
Plate tectonics.
Over the course of millions of years, the motion of tectonic plates reconfigures global land and ocean areas and generates topography. This can affect both global and local patterns of climate and atmosphere-ocean circulation.
The position of the continents determines the geometry of the oceans and therefore influences patterns of ocean circulation. The locations of the seas are important in controlling the transfer of heat and moisture across the globe, and therefore, in determining global climate. A recent example of tectonic control on ocean circulation is the formation of the Isthmus of Panama about 5 million years ago, which shut off direct mixing between the Atlantic and Pacific Oceans. This strongly affected the ocean dynamics of what is now the Gulf Stream and may have led to Northern Hemisphere ice cover. During the Carboniferous period, about 300 to 360 million years ago, plate tectonics may have triggered large-scale storage of carbon and increased glaciation. Geologic evidence points to a "megamonsoonal" circulation pattern during the time of the supercontinent Pangaea, and climate modeling suggests that the existence of the supercontinent was conducive to the establishment of monsoons.
The size of continents is also important. Because of the stabilizing effect of the oceans on temperature, yearly temperature variations are generally lower in coastal areas than they are inland. A larger supercontinent will therefore have more area in which climate is strongly seasonal than will several smaller continents or islands.
Human influences.
In the context of climate variation, anthropogenic factors are human activities which affect the climate. The scientific consensus on climate change is "that climate is changing and that these changes are in large part caused by human activities,"
and it "is largely irreversible."
Of most concern in these anthropogenic factors is the increase in CO2 levels due to emissions from fossil fuel combustion, followed by aerosols (particulate matter in the atmosphere) and the CO2 released by cement manufacture. Other factors, including land use, ozone depletion, animal agriculture and deforestation, are also of concern in the roles they play – both separately and in conjunction with other factors – in affecting climate, microclimate, and measures of climate variables.
Physical evidence.
Evidence for climatic change is taken from a variety of sources that can be used to reconstruct past climates. Reasonably complete global records of surface temperature are available beginning from the mid-late 19th century. For earlier periods, most of the evidence is indirect—climatic changes are inferred from changes in proxies, indicators that reflect climate, such as vegetation, ice cores, dendrochronology, sea level change, and glacial geology.
Temperature measurements and proxies.
The instrumental temperature record from surface stations was supplemented by radiosonde balloons, extensive atmospheric monitoring by the mid-20th century, and, from the 1970s on, with global satellite data as well. The 18O/16O ratio in calcite and ice core samples used to deduce ocean temperature in the distant past is an example of a temperature proxy method, as are other climate metrics noted in subsequent categories.
Historical and archaeological evidence.
Climate change in the recent past may be detected by corresponding changes in settlement and agricultural patterns. Archaeological evidence, oral history and historical documents can offer insights into past changes in the climate. Climate change effects have been linked to the collapse of various civilizations.
Glaciers.
Glaciers are considered among the most sensitive indicators of climate change. Their size is determined by a mass balance between snow input and melt output. As temperatures warm, glaciers retreat unless snow precipitation increases to make up for the additional melt; the converse is also true.
Glaciers grow and shrink due both to natural variability and external forcings. Variability in temperature, precipitation, and englacial and subglacial hydrology can strongly determine the evolution of a glacier in a particular season. Therefore, one must average over a decadal or longer time-scale and/or over many individual glaciers to smooth out the local short-term variability and obtain a glacier history that is related to climate.
A world glacier inventory has been compiled since the 1970s, initially based mainly on aerial photographs and maps but now relying more on satellites. This compilation tracks more than 100,000 glaciers covering a total area of approximately 240,000 km2, and preliminary estimates indicate that the remaining ice cover is around 445,000 km2. The World Glacier Monitoring Service collects data annually on glacier retreat and glacier mass balance. From this data, glaciers worldwide have been found to be shrinking significantly, with strong glacier retreats in the 1940s, stable or growing conditions during the 1920s and 1970s, and again retreating from the mid-1980s to present.
The most significant climate processes since the middle to late Pliocene (approximately 3 million years ago) are the glacial and interglacial cycles. The present interglacial period (the Holocene) has lasted about 11,700 years. Shaped by orbital variations, responses such as the rise and fall of continental ice sheets and significant sea-level changes helped create the climate. Other changes, including Heinrich events, Dansgaard–Oeschger events and the Younger Dryas, however, illustrate how glacial variations may also influence climate without the orbital forcing.
Glaciers leave behind moraines that contain a wealth of material—including organic matter, quartz, and potassium that may be dated—recording the periods in which a glacier advanced and retreated. Similarly, by tephrochronological techniques, the lack of glacier cover can be identified by the presence of soil or volcanic tephra horizons whose date of deposit may also be ascertained.
Arctic sea ice loss.
The decline in Arctic sea ice, both in extent and thickness, over the last several decades is further evidence for rapid climate change. Sea ice is frozen seawater that floats on the ocean surface. It covers millions of square miles in the polar regions, varying with the seasons. In the Arctic, some sea ice remains year after year, whereas almost all Southern Ocean or Antarctic sea ice melts away and reforms annually. Satellite observations show that Arctic sea ice is now declining at a rate of 13.3 percent per decade, relative to the 1981 to 2010 average.
Vegetation.
A change in the type, distribution and coverage of vegetation may occur given a change in the climate. Some changes in climate may result in increased precipitation and warmth, resulting in improved plant growth and the subsequent sequestration of airborne CO2. A gradual increase in warmth in a region will lead to earlier flowering and fruiting times, driving a change in the timing of life cycles of dependent organisms. Conversely, cold will cause plant bio-cycles to lag. Larger, faster or more radical changes, however, may result in vegetation stress, rapid plant loss and desertification in certain circumstances. An example of this occurred during the Carboniferous Rainforest Collapse (CRC), an extinction event 300 million years ago. At this time vast rainforests covered the equatorial region of Europe and America. Climate change devastated these tropical rainforests, abruptly fragmenting the habitat into isolated 'islands' and causing the extinction of many plant and animal species.
Pollen analysis.
Palynology is the study of contemporary and fossil palynomorphs, including pollen. Palynology is used to infer the geographical distribution of plant species, which vary under different climate conditions. Different groups of plants have pollen with distinctive shapes and surface textures, and since the outer surface of pollen is composed of a very resilient material, they resist decay. Changes in the type of pollen found in different layers of sediment in lakes, bogs, or river deltas indicate changes in plant communities. These changes are often a sign of a changing climate. As an example, palynological studies have been used to track changing vegetation patterns throughout the Quaternary glaciations and especially since the last glacial maximum.
Cloud cover and precipitation.
Past precipitation can be estimated in the modern era with the global network of precipitation gauges. Surface coverage over oceans and remote areas is relatively sparse, but, reducing reliance on interpolation, satellite clouds and precipitation data has been available since the 1970s. Quantification of climatological variation of precipitation in prior centuries and epochs is less complete but approximated using proxies such as marine sediments, ice cores, cave stalagmites, and tree rings.
Climatological temperatures substantially affect cloud cover and precipitation. For instance, during the Last Glacial Maximum of 18,000 years ago, thermal-driven evaporation from the oceans onto continental landmasses was low, causing large areas of extreme desert, including polar deserts (cold but with low rates of cloud cover and precipitation). In contrast, the world's climate was cloudier and wetter than today near the start of the warm Atlantic Period of 8000 years ago.
Estimated global land precipitation increased by approximately 2% over the course of the 20th century, though the calculated trend varies if different time endpoints are chosen, complicated by ENSO and other oscillations, including greater global land cloud cover precipitation in the 1950s and 1970s than the later 1980s and 1990s despite the positive trend over the century overall.
Similar slight overall increase in global river runoff and in average soil moisture has been perceived.
Dendroclimatology.
Dendroclimatology is the analysis of tree ring growth patterns to determine past climate variations. Wide and thick rings indicate a fertile, well-watered growing period, whilst thin, narrow rings indicate a time of lower rainfall and less-than-ideal growing conditions.
Ice cores.
Analysis of ice in a core drilled from an ice sheet such as the Antarctic ice sheet, can be used to show a link between temperature and global sea level variations. The air trapped in bubbles in the ice can also reveal the CO2 variations of the atmosphere from the distant past, well before modern environmental influences. The study of these ice cores has been a significant indicator of the changes in CO2 over many millennia, and continues to provide valuable information about the differences between ancient and modern atmospheric conditions.
Animals.
Remains of beetles are common in freshwater and land sediments. Different species of beetles tend to be found under different climatic conditions. Given the extensive lineage of beetles whose genetic makeup has not altered significantly over the millennia, knowledge of the present climatic range of the different species, and the age of the sediments in which remains are found, past climatic conditions may be inferred.
Similarly, the historical abundance of various fish species has been found to have a substantial relationship with observed climatic conditions. Changes in the primary productivity of autotrophs in the oceans can affect marine food webs.
Sea level change.
Global sea level change for much of the last century has generally been estimated using tide gauge measurements collated over long periods of time to give a long-term average. More recently, altimeter measurements — in combination with accurately determined satellite orbits — have provided an improved measurement of global sea level change. To measure sea levels prior to instrumental measurements, scientists have dated coral reefs that grow near the surface of the ocean, coastal sediments, marine terraces, ooids in limestones, and nearshore archaeological remains. The predominant dating methods used are uranium series and radiocarbon, with cosmogenic radionuclides being sometimes used to date terraces that have experienced relative sea level fall. In the early Pliocene, global temperatures were 1–2˚C warmer than the present temperature, yet sea level was 15–25 meters higher than today.

</doc>
<doc id="47513" url="https://en.wikipedia.org/wiki?curid=47513" title="Climate model">
Climate model

Climate models use quantitative methods to simulate the interactions of the important drivers of climate, including atmosphere, oceans, land surface and ice. They are used for a variety of purposes from study of the dynamics of the climate system to projections of future climate.
All climate models take account of incoming energy from the sun as short wave electromagnetic radiation, chiefly visible and short-wave (near) infrared, as well as outgoing long wave (far) infrared electromagnetic. Any imbalance results in a change in temperature.
Models vary in complexity:
Box models.
Box models are simplified versions of complex systems, reducing them to boxes (or reservoirs) linked by fluxes. The boxes are assumed to be mixed homogeneously. Within a given box, the concentration of any chemical species is therefore uniform. However, the abundance of a species within a given box may vary as a function of time due to the input to (or loss from) the box or due to the production, consumption or decay of this species within the box.
Simple box models, i.e. box model with a small number of boxes whose properties (e.g. their volume) do not change with time, are often useful to derive analytical formulas describing the dynamics and steady-state abundance of a species. More complex box models are usually solved using numerical techniques.
Box models are used extensively to model environmental systems or ecosystems and in studies of ocean circulation and the carbon cycle.
Zero-dimensional models.
A very simple model of the radiative equilibrium of the Earth is
where
and
The constant "πr"2 can be factored out, giving
Solving for the temperature,
This yields an apparent effective average earth temperature of . This is because the above equation represents the effective "radiative" temperature of the Earth (including the clouds and atmosphere). The use of effective emissivity and albedo account for the greenhouse effect.
This very simple model is quite instructive, and the only model that could fit on a page. For example, it easily determines the effect on average earth temperature of changes in solar constant or change of albedo or effective earth emissivity.
The average emissivity of the earth is readily estimated from available data. The emissivities of terrestrial surfaces are all in the range of 0.96 to 0.99 (except for some small desert areas which may be as low as 0.7). Clouds, however, which cover about half of the earth’s surface, have an average emissivity of about 0.5 (which must be reduced by the fourth power of the ratio of cloud absolute temperature to average earth absolute temperature) and an average cloud temperature of about . Taking all this properly into account results in an effective earth emissivity of about 0.64 (earth average temperature ).
This simple model readily determines the effect of changes in solar output or change of earth albedo or effective earth emissivity on average earth temperature. It says nothing, however about what might cause these things to change. Zero-dimensional models do not address the temperature distribution on the earth or the factors that move energy about the earth.
Radiative-convective models.
The zero-dimensional model above, using the solar constant and given average earth temperature, determines the effective earth emissivity of long wave radiation emitted to space. This can be refined in the vertical to a one-dimensional radiative-convective model, which considers two processes of energy transport:
The radiative-convective models have advantages over the simple model: they can determine the effects of varying greenhouse gas concentrations on effective emissivity and therefore the surface temperature. But added parameters are needed to determine local emissivity and albedo and address the factors that move energy about the earth.
Effect of ice-albedo feedback on global sensitivity in a one-dimensional radiative-convective climate model.
Higher-dimension models.
The zero-dimensional model may be expanded to consider the energy transported horizontally in the atmosphere. This kind of model may well be zonally averaged. This model has the advantage of allowing a rational dependence of local albedo and emissivity on temperature – the poles can be allowed to be icy and the equator warm – but the lack of true dynamics means that horizontal transports have to be specified.
EMICs (Earth-system models of intermediate complexity).
Depending on the nature of questions asked and the pertinent time scales, there are, on the one extreme, conceptual, more inductive models, and, on the other extreme, general circulation models operating at the highest spatial and temporal resolution currently feasible. Models of intermediate complexity bridge the gap. One example is the Climber-3 model. Its atmosphere is a 2.5-dimensional statistical-dynamical model with 7.5° × 22.5° resolution and time step of half a day; the ocean is MOM-3 (Modular Ocean Model) with a 3.75° × 3.75° grid and 24 vertical levels.
GCMs (global climate models or general circulation models).
General Circulation Models (GCMs) discretise the equations for fluid motion and energy transfer and integrate these over time. Unlike simpler models, GCMs divide the atmosphere and/or oceans into grids of discrete "cells", which represent computational units. Unlike simpler models which make mixing assumptions, processes internal to a cell—such as convection—that occur on scales too small to be resolved directly are parameterised at the cell level, while other functions govern the interface between cells.
Atmospheric GCMs (AGCMs) model the atmosphere and impose sea surface temperatures as boundary conditions. Coupled atmosphere-ocean GCMs (AOGCMs, e.g. HadCM3, EdGCM, GFDL CM2.X, ARPEGE-Climat) combine the two models. The first general circulation climate model that combined both oceanic and atmospheric processes was developed in the late 1960s at the NOAA Geophysical Fluid Dynamics Laboratory AOGCMs represent the pinnacle of complexity in climate models and internalise as many processes as possible. However, they are still under development and uncertainties remain. They may be coupled to models of other processes, such as the carbon cycle, so as to better model feedback effects. Such integrated multi-system models are sometimes referred to as either "earth system models" or "global climate models."
Research and development.
There are three major types of institution where climate models are developed, implemented and used:
The World Climate Research Programme (WCRP), hosted by the World Meteorological Organization (WMO), coordinates research activities on climate modelling worldwide.
A 2012 U.S. National Research Council report discussed how the large and diverse U.S. climate modeling enterprise could evolve to become more unified. Efficiencies could be gained by developing a common software infrastructure shared by all U.S. climate researchers, and holding an annual climate modeling forum, the report found.

</doc>
<doc id="47514" url="https://en.wikipedia.org/wiki?curid=47514" title="Cloud albedo">
Cloud albedo

Cloud albedo is a measure of the albedo of a cloud. Higher values indicate that a cloud reflects a large amount of solar radiation or transmits a small amount of radiation. 
Cloud albedo varies from less than 10% to more than 90% and depends on drop sizes, liquid water or ice content, thickness of the cloud, and the sun's zenith angle. The smaller the drops and the greater the liquid water content, the greater the cloud albedo, if all other factors are the same.
Low, thick clouds (such as stratocumulus) primarily reflect incoming solar radiation, causing it to have a high albedo, whereas high, thin clouds (such as Cirrus) tend to transmit it to the surface but then trap outgoing infrared radiation, causing it to have low albedo. It contributes to the greenhouse effect.

</doc>
<doc id="47515" url="https://en.wikipedia.org/wiki?curid=47515" title="Cloud">
Cloud

In meteorology, a cloud is an aerosol comprising a visible mass of liquid droplets or frozen crystals, both of which are made of water or various chemicals. The droplets or particles are suspended in the atmosphere above the surface of a planetary body. On Earth clouds are formed by the saturation of air in the homosphere (which includes the troposphere, stratosphere, and mesosphere) when air cools or gains water vapor. The science of clouds is nephology which is undertaken in the cloud physics branch of meteorology.
Cloud types in the troposphere, the atmospheric layer closest to Earth's surface, have Latin names due to the universal adaptation of Luke Howard's nomenclature. It was formally proposed in December 1802 and published for the first time the following year. It became the basis of a modern international system that classifies these tropospheric aerosols into five physical "forms"; stratiform, stratocumuliform, cirriform, cumuliform, and cumulonimbiform. Most of these forms can be found in the high, middle, and low altitude levels or "étages" of the troposphere, although cirriform clouds only occur in the high altitude range. The forms and étages are cross-classified to produces ten basic genus-types or "genera". A cloud genus can therefore be defined as any physical form that is particular to a given étage, or simultaneosly to more than one étage if the genus-type has significant vertical extent. Most genera can be divided into "species" that are often subdivided into "varieties" where applicable.
Clouds that form higher up in the stratosphere and mesosphere have common names for their main types, but are sub-classified "alpha-numerically" rather than with the elaborate system of Latin names given to cloud types in the troposphere. They are relatively uncommon and are mostly seen in the polar regions of Earth. Clouds have been observed on other planets and moons within the Solar System, but, due to their different temperature characteristics, they are often composed of other substances such as methane, ammonia, and sulfuric acid as well as water.
Etymology.
The origin of the term "cloud" can be found in the old English "clud" or "clod", meaning a hill or a mass of rock. Around the beginning of the 13th century, it was extended as a metaphor to include rain clouds as masses of evaporated water in the sky because of the similarity in appearance between a mass of rock and a cumulus heap cloud. Over time, the metaphoric term replaced the original old English "weolcan" to refer to clouds in general.
History of cloud science and nomenclature.
Aristotle and Theophrastus.
Ancient cloud studies were not made in isolation, but were observed in combination with other weather elements and even other natural sciences. In about 340 BC the Greek philosopher Aristotle wrote "Meteorologica", a work which represented the sum of knowledge of the time about natural science, including weather and climate. For the first time, precipitation and the clouds from which precipitation fell were called meteors, which originate from the Greek word meteoros, meaning 'high in the sky'. From that word came the modern term meteorology, the study of clouds and weather. Meteorologica was based on intuition and simple observation, but not on what is now considered the scientific method. Nevertheless, it was the first known work that attempted to treat a broad range of meteorological topics.
The magazine "De Mundo" (attributed to Pseudo-Aristotle) noted: 
Several years after Aristotle's book, his pupil Theophrastus put together a book on weather forecasting called "The Book of Signs". Various indicators such as solar and lunar halos formed by high clouds were presented as ways to forecast the weather. The combined works of Aristotle and Theophrastus had such authority they became the main influence in the study of clouds, weather and weather forecasting for nearly 2000 years.
Luke Howard, Jean-Baptiste Lamarck, and the first comprehensive classification.
After centuries of speculative theories about the formation and behavior of clouds, the first truly scientific studies were undertaken by Luke Howard in England and Jean-Baptiste Lamarck in France. Howard was a methodical observer with a strong grounding in the Latin language and used his background to classify the various tropospheric cloud types during 1802. He believed that the changing cloud forms in the sky could unlock the key to weather forecasting. Lamarck had worked independently on cloud classification the same year and had come up with a different naming scheme that failed to make an impression even in his home country of France because it used unusual French names for cloud types. His system of nomenclature included twelve categories of clouds, with such names as (translated from French) hazy clouds, dappled clouds and broom-like clouds. By contrast, Howard used universally accepted Latin, which caught on quickly after it was published in 1803. As a sign of the popularity of the naming scheme, the German dramatist and poet Johann Wolfgang von Goethe composed four poems about clouds, dedicating them to Howard. An elaboration of Howard's system was eventually formally adopted by the International Meteorological Conference in 1891.
Howard's original system established three general cloud "forms" based on physical appearance and process of formation: "cirriform" (mainly detached and wispy), "cumuliform" or convective (mostly detached and heaped, rolled, or rippled), and non-convective "stratiform" (mainly continuous layers in sheets). These were cross-classified into "lower" and "upper" étages. Cumuliform clouds forming in the lower level were given the genus name cumulus from the Latin word for "heap", and low stratiform clouds the genus name stratus from the Latin word for "sheet" or "layer". Physically similar clouds forming in the upper étage were given the genus names cirrocumulus (generally showing more limited convective activity than low level cumulus) and cirrostratus, respectively. Cirriform clouds were identified as always upper level and given the genus name cirrus from the Latin for 'fibre' or 'hair'.
In addition to these individual cloud types; Howard added two names to designate cloud systems consisting of more than one form joined together or located in very close proximity. Cumulostratus described large cumulus clouds blended with stratiform layers in the lower or upper levels. The term nimbus was given to complex systems of cirriform, cumuliform, and stratiform clouds with sufficient vertical development to produce significant precipitation, and it came to be identified as a distinct "nimbiform" physical category.
Howard's successors.
In 1840, German meteorologist Ludwig Kaemtz added stratocumulus to Howard's canon as a mostly detached low-étage genus of "limited" convection. It was defined as having cumuliform- and stratiform characteristics integrated into a single layer (in contrast to cumulostratus which was deemed to be composite in nature and could be structured into more than one layer). This led to the recognition of a "stratocumuliform" category that included rolled and rippled clouds classified separately from the more freely convective heaped cumuliform clouds. 
During the mid 1850s, Emilien Renou, director of the Parc Saint-Maur and Montsouris observatories, began work on an elaboration of Howard's classifications that would lead to the introduction during the 1870s of altocumulus (physically more closely related to stratocumulus than to cumulus) and altostratus. These were respectively stratocumuliform and stratiform cloud genera of a newly defined "middle" étage above stratocumulus and stratus but below cirrocumulus and cirrostratus.
In 1880, Philip Weilbach, secretary and librarian at the Art Academy in Copenhagen, and like Luke Howard, an amateur meteorologist, unsuccessfully proposed an alternative to Howard's classification. However, he also proposed and had accepted by the permanent committee of the International Meteorological Organization (IMO), a forerunner of the present-day World Meteorological Organization (WMO), the designation of a new free-convective vertical or multi-étage genus type, cumulonimbus, which would be distinct from cumulus and nimbus and identifiable by its often very complex structure (frequently including a cirriform top and what are now recognized as multiple accessory clouds), and its ability to produce thunder. With this addition, a canon of ten tropospheric cloud "genera" was established that came to be officially and universally accepted. Howard's cumulostratus was not included as a distinct type, having effectively been reclassified into its component cumuliform and stratiform genus types already included in the new canon.
In 1890, Otto Jesse revealed the discovery and identification of the first clouds known to form above the troposphere. He proposed the name "noctilucent" which is Latin for "night shining". Because of the extremely high altitudes of these clouds in what is now known to be the mesosphere, they could become illuminated by the a sun's rays when the sky was nearly dark after sunset and before sunrise. Three years later, Henrik Mohn revealed a similar discovery of nacreous clouds in what is now considered the stratosphere.
In 1896, the first cloud atlas sanctioned by the IMO was produced by Teisserenc de Borte based on collaborations with Hugo H. Hildebrandsson. The latter had become the first researcher to use photography for the study and classification of clouds in 1879.
Alternatives to Howard's classification system were proposed throughout the 19th century. Heinrich Dove of Germany and Elias Loomis of the United States came up with other schemes in 1828 and 1841 respectively, but neither met with international success. Additional proposals were made by Andre Poey (1863), Clemment Ley (1894), and H.H. Clayton (1896), but their systems, like earlier alternative schemes, differed too much from Howard's to have any success beyond the adoption of some secondary cloud types. However, Clayton's idea to formalize the division of clouds by their physical structures into cirriform, stratiform, "flocciform" (stratocumuliform) and cumuliform (with the later addition of cumulonimbiform), eventually found favor as an aid in the analysis of satellite cloud images.
20th-century developments.
A further modification of the genus classification system came when an IMC commission for the study of clouds put forward a refined and more restricted definition of the genus nimbus which was effectively reclassified as a stratiform cloud type. It was then renamed nimbostratus and published with the new name in the 1932 edition of the "International Atlas of Clouds and of States of the Sky". This left cumulonimbus as the only nimbiform type as indicated by its root-name.
On April 1, 1960, the first successful weather satellite, TIROS-1 (Television Infrared Observation Satellite), was launched from Cape Canaveral, Florida by the National Aeronautics and Space Administration (NASA) with the participation of The US Army Signal Research and Development Lab, RCA, the US Weather Bureau, and the US Naval Photographic Center. During its 78-day mission, it relayed thousands of pictures showing the structure of large-scale cloud regimes, and proved that satellites could provide useful surveillance of global weather conditions from space.
In 1976, the United Kingdom Department of Industry published a modification of the international cloud classification system adapted for satellite cloud observations. It was co-sponsored by NASA and showed a change in name of the nimbiform type to "cumulonimbiform", although the earlier name and original meaning pertaining to all rain clouds can still be found in some classifications.
Tropospheric clouds.
Physical forms, étages, and genera.
Clouds can be divided into five physical forms based on physical structure and process of formation. These forms are commonly used for the purpose of satellite analysis.
The individual "genus" types result from the physical forms being cross-classified by altitude level or étage within the troposphere. The base-height range for each étage varies depending on the latitudinal geographical zone. A consensus exists as to the designation of high, middle, and low étages, the makeup of the basic canon of ten cloud genera that results from this cross-classification, and the étage designations of non-vertical genus types. Clouds with significant vertical extent occupy more than one étage and are commonly, but not always, treated as a separate group or sub-group, or given separate descriptions within the context of the standard étages.
The physical forms and their constituent genera are given in approximate ascending order of instability or convective activity.
Stratiform.
The stratiform group is cross-classified into the genera cirrostratus (high-étage), altostratus (middle-étage), stratus (low-étage), and nimbostratus (multi-étage). "Non-convective" stratiform clouds appear in "stable" airmass conditions and, in general, have flat sheet-like structures that can form at any altitude in the troposphere. Very low stratiform cloud results when advection fog is lifted above surface level during breezy conditions.
Cirriform.
Cirriform clouds are generally of the genus cirrus and have the appearance of detached or semi-merged filaments. They form at high tropospheric altitudes in air that is mostly stable with little or no convective activity, although denser patches may occasionally show buildups caused by "limited" high-level convection where the air is partly "unstable".
Stratocumuliform.
The stratocumuliform group is cross-classified into the genera cirrocumulus (high-étage), altocumulus (middle-étage), and stratocumulus (low-étage). Clouds of this structure have both cumuliform and stratiform characteristics in the form of rolls or ripples. They generally form as a result of "limited convection" in an otherwise mostly stable airmass topped by an inversion layer. Layered altocumulus and cirrocumulus are physically more closely related to stratocumulus than to cumulus heaps. However, some semantic ambiguity regarding the physical relationships between these cumuliform and stratocumuliform clouds can arise because, in the case of the latter form, the strato- prefix is dropped from the names of the alto- and cirro- genus types to avoid double prefixing. Cirro-stratocumulus is therefore abbreviated to cirrocumulus and alto-stratocumulus is shortened to altocumulus.
Cumuliform.
Clouds of this form may be low or multi-étage depending on their vertical size and extent. They are typically of the genus cumulus which appear in isolated heaps. They are the product of localized but generally "free-convective" lift where there are no inversion layers in the atmosphere to limit vertical growth. In general, small cumuliform clouds tend to indicate comparatively weak instability. Larger cumulus types are a sign of moderate to strong atmospheric instability and convective activity.
Cumulonimbiform.
The largest free-convective clouds comprise the genus cumulonimbus which are multi-étage because of their great vertical extent. They occur in highly unstable air and often have complex structures that include cirriform tops and multiple accessory clouds.
Summary of étages and genera.
The forms and resultant genus types can be grouped by étage. This is generally done for the purpose of cloud atlases, surface weather observations and weather maps. These maps are produced from information in the international synoptic code (or SYNOP) that are transmitted at regular intervals by professionally trained staff at major weather stations.
Non-vertical or single-étage genera are listed and summarised below in approximate descending order of the altitude at which each normally occurs. Multi-étage clouds with significant vertical extent are separately listed and summarised in approximate ascending order of instability or convective activity.
High cirriform, stratocumuliform, and stratiform genera.
Clouds of the high étage form at altitudes of in the polar regions, in the temperate regions and in the tropical region. All cirriform clouds are classified as high and thus constitute a single genus "cirrus" (Ci). Stratocumuliform and stratiform clouds in the high étage carry the prefix "cirro-", yielding the respective genus names "cirrocumulus" (Cc) and "cirrostratus" (Cs). When satellite images of high clouds are analized without supporting data from direct human observations, it becomes impossible to distinguish between individual genus types which are then collectively identified as cirrus-type.
Middle stratocumuliform and stratiform genera.
Non-vertical clouds in the middle étage are prefixed by "alto-", yielding the genus names "altocumulus" (Ac) and "altostratus" (As). These clouds can form as low as above surface at any latitude, but may be based as high as near the poles, at mid latitudes, and in the tropics. As with high clouds, it is not always possible to distinguish between individual genera using satellite photography alone. Without the addition of human observations, these clouds are usually collectively identified as 'middle-type' on satellite images.
 
Low stratocumuliform, stratiform, and cumuliform genera.
Low-étage clouds are found from near surface up to . Genus types in this étage either have no prefix or carry one that refers to a characteristic other than altitude.
Multi-étage stratiform, cumuliform, and cumulonimbiform genera (low to middle cloud base).
These clouds have low to middle-étage bases that form anywhere from near surface to about . 
Some classifications limit the term "vertical" to upward-growing free-convective cumuliform and cumulonimbiform genera. Downward-growing nimbostratus can be as thick as most upward-growing vertical cumulus, but its horizontal extent tends to be even greater. This sometimes leads to the exclusion of this genus type from the group of vertical clouds. Classifications that follow this approach usually show nimbostratus either as low-étage to denote its normal base height range, or as middle, based on the altitude range at which it normally forms. Sometimes the terms "multi-level" or "multi-étage " are used for all very thick or tall cloud types including nimbostratus to avoid the association of 'vertical' with free-convective cumuliform only. Alternatively, some classifications do not recognize a vertical or multi-étage designation and include all vertical free-convective cumuliform and cumulonimbiform types with the low-étage clouds.
Nimbostratus and some cumulus in this group usually achieve moderate or deep vertical extent, but without towering structure. However, with sufficient airmass instability, upward-growing cumuliform clouds can grow to high towering proportions. Although genus types with vertical extent are often considered a single group, the International Civil Aviation Organization (ICAO) further distinguishes towering vertical clouds as a separate group or sub-group by specifying that these very large cumuliform and cumulonimbiform types must be identified by their standard names or abbreviations in all aviation observations (METARS) and forecasts (TAFS) to warn pilots of possible severe weather and turbulence. When towering vertical types are considered separately, they comprise the aforementioned cumulonimbus genus and one cumulus subtype or species, cumulus congestus (Cu con). This subtype is designated "towering cumulus" (Tcu) by ICAO. There is no stratiform type in this group because by definition, even very thick stratiform clouds cannot have towering vertical structure, although they may be accompanied by embedded towering cumuliform or cumulonimbiform types.
These clouds are sometimes classified separately from the other vertical or multi-étage types because of their ability to produce severe turbulence.
Species.
Genus types are commonly divided into subtypes called "species" that indicate specific structural details which can vary according to the stability and windshear characteristics of the atmosphere at any given time and location. Despite this hierarchy, a particular species may be a subtype of more than one genus, especially if the genera are of the same physical form and are differentiated from each other mainly by altitude or étage. Some species can even be subtypes of genera belonging to more than one physical form.
The species types are grouped according to the physical forms and genera with which each is normally associated. The forms, genera, and species are listed in approximate ascending order of instability or convective activity.
Stable stratiform species.
Of the stratiform group, high-level cirrostratus comprises two species. Cirrostratus "nebulosus" has a rather diffuse appearance lacking in structural detail. Cirrostratus "fibratus" is a species made of semi-merged filaments that are transitional to or from cirrus. Altostratus and multi-level nimbostratus always have a flat or diffuse appearance and are therefore not subdivided into species. Low-étage stratus is of the species nebulosus except when broken up into ragged sheets of stratus fractus (see below).
Mostly stable cirriform species.
Cirriform clouds have three non-convective species that can form in mostly "stable" airmass conditions. Cirrus fibratus comprise filaments that may be straight, wavy, or occasionally twisted by non-convective wind shear. The species "uncinus" is similar but has upturned hooks at the ends. Cirrus "spissatus" appear as opaque patches that can show light grey shading.
Mostly stable stratocumuliform species.
Stratocumuliform genus-types (cirrocumulus, altocumulus, and stratocumulus) that appear in mostly stable air have two species each that can form in the high, middle, or low étages of the troposphere. The "stratiformis" species normally occur in extensive sheets or in smaller patches where there is only minimal convective activity. Clouds of the "lenticularis" species tend to have lens-like shapes tapered at the ends. They are most commonly seen as orographic mountain-wave clouds, but can occur anywhere in the troposphere where there is strong wind shear combined with sufficient airmass stability to maintain a generally flat cloud structure.
Ragged stratiform and cumuliform species.
The species "fractus" shows "variable" instability because it can be a subdivision of genus-types of different physical forms that have different stability characteristics. This subtype can be in the form of ragged but mostly "stable" stratiform sheets (stratus fractus) or small ragged cumuliform heaps with somewhat greater unstability (cumulus fractus). Because of their relatively low altitudes, stratiform and cumuliform genus-types can be torn up into shreds by brisk low level winds that create mechanical turbulance against the ground. Fractus clouds can form in precipitation at low altitudes, with or without brisk or gusty winds. They are closely associated with precipitating cloud systems of considerable vertical and sometimes horizontal extent, so they are also classified as "accessory clouds" under the name "pannus" (see section on supplementary features).
Partly unstable cirriform and stratocumuliform species.
These species are subdivisions of genus types that occur in partly unstable air. When a mostly stable stratocumuliform or cirriform layer becomes disturbed by localized areas of airmass instability, the species "castellanus", which resemble the turrets of a castle when viewed from the side, can be found with stratocumuliform genera at any altitude level and with limited-convective patches of high-étage cirrus. Clouds of the more detached tufted "floccus" species are also subdivisions of genus-types which may be cirriform or stratocumuliform. They are sometimes seen with cirrus, cirrocumulus, and altocumulus, but not stratocumulus.
Mostly unstable cumuliform species.
More general airmass instability in the troposphere tends to produce clouds of the more freely convective cumulus genus type, whose species are mainly indicators of degrees of atmospheric instability and resultant vertical development of the clouds. A cumulus cloud initially forms as a cloudlet of the species "humilis" that shows only slight vertical development. If the air becomes more unstable, the cloud tends to grow vertically into the species "mediocris", then "congestus", the tallest cumulus species.
Unstable cumulonimbiform species.
With highly unstable atmospheric conditions, large cumulus may continue to grow into cumulonimbus "calvus" (essentially a very tall congestus cloud that produces thunder), then ultimately into the species "capillatus" when supercooled water droplets at the top turn into ice crystals giving it a cirriform appearance.
Varieties.
Genus and species types are further subdivided into "varieties" whose names can appear after the species name to provide a fuller description of a cloud. Some cloud varieties are not restricted to a specific étage or form, and can therefore be common to more than one genus or species.
Opacity-based.
All cloud varieties fall into one of two main groups. One group identifies the opacities of particular low and middle étage cloud structures and comprises the varieties "translucidus" (thin translucent), "perlucidus" (thick opaque with translucent breaks), and "opacus" (thick opaque). These varieties are always identifiable for cloud genera and species with variable opacity. All three are associated with the stratiformis species of altocumulus and stratocumulus. However, only two varieties are seen with altostratus and stratus nebulosus whose uniform structures prevent the formation of a perlucidus variety. Opacity-based varieties are not applied to high-étage clouds because they are always translucent, or in the case of cirrus spissatus, always opaque. Similarly, these varieties are also not associated with moderate and towering vertical clouds because they are always opaque.
Pattern-based.
A second group describes the occasional arrangements of cloud structures into particular patterns that are discernible by a surface-based observer (cloud fields usually being visible only from a significant altitude above the formations). These varieties are not always present with the genera and species with which they are otherwise associated, but only appear when atmospheric conditions favor their formation. "Intortus" and "vertebratus" varieties occur on occasion with cirrus fibratus. They are respectively filaments twisted into irregular shapes, and those that are arranged in fishbone patterns, usually by uneven wind currents that favor the formation of these varieties. The variety "radiatus" is associated with cloud rows of a particular type that appear to converge at the horizon. It is sometimes seen with the fibratus and uncinus species of cirrus, the stratiformis species of altocumulus and stratocumulus, the mediocris and sometimes humilis species of cumulus, and with the genus altostratus.
Another variety, "duplicatus" (closely spaced layers of the same type, one above the other), is sometimes found with cirrus of both the fibratus and uncinus species, and with altocumulus and stratocumulus of the species stratiformis and lenticularis. The variety "undulatus" (having a wavy undulating base) can occur with any clouds of the species stratiformis or lenticularis, and with altostratus. It is only rarely observed with stratus nebulosus. The variety "lacunosus" is caused by localized downdrafts that create circular holes in the form of a honeycomb or net. It is occasionally seen with cirrocumulus and altocumulus of the species stratiformis, castellanus, and floccus, and with stratocumulus of the species stratiformis and castellanus.
Combinations.
It is possible for some species to show combined varieties at one time, especially if one variety is opacity-based and the other is pattern-based. An example of this would be a layer of altocumulus stratiformis arranged in seemingly converging rows separated by small breaks. The full technical name of a cloud in this configuration would be "altocumulus stratiformis radiatus perlucidus", which would identify respectively its genus, species, and two combined varieties.
Accessory clouds, supplementary features, and other derivative formations.
Supplementary features and accessory clouds are not further subdivisions of cloud types below the species and variety level. Rather, they are either "hydrometeors" or special cloud formations with their own Latin names that form in association with certain cloud genera, species, and varieties. Supplementary features, whether in the form of clouds or precipitation, are directly attached to the main genus-cloud. Accessory clouds, by contrast, are generally detached from the main cloud.
Precipitation-based supplementary features.
One group of supplementary features are not actual cloud formations, but precipitation that falls when water droplets or ice crystals that make up visible clouds have grown too heavy to remain aloft. "Virga" is a feature seen with clouds producing precipitation that evaporates before reaching the ground, these being of the genera cirrocumulus, altocumulus, altostratus, nimbostratus, stratocumulus, cumulus, and cumulonimbus.
When the precipitation reaches the ground without completely evaporating, it is designated as the feature "praecipitatio". This normally occurs with altostratus opacus, which can produce widespread but usually light precipitation, and with thicker clouds that show significant vertical development. Of the latter, "upward-growing" cumulus mediocris produces only isolated light showers, while "downward growing" nimbostratus is capable of heavier, more extensive precipitation. Towering vertical clouds have the greatest ability to produce intense precipitation events, but these tend to be localized unless organized along fast-moving cold fronts. Showers of moderate to heavy intensity can fall from cumulus congestus clouds. Cumulonimbus, the largest of all cloud genera, has the capacity to produce very heavy showers. Low stratus clouds usually produce only light precipitation, but this always occurs as the feature praecipitatio due to the fact this cloud genus lies too close to the ground to allow for the formation of virga.
Cloud-based supplementary features.
"Incus" is the most type-specific supplementary feature, seen only with cumulonimbus of the species capillatus. A cumulonimbus incus cloud top is one that has spread out into a clear anvil shape as a result of rising air currents hitting the stability layer at the tropopause where the air no longer continues to get colder with increasing altitude.
The "mamma" feature forms on the bases of clouds as downward-facing bubble-like protuberances caused by localized downdrafts within the cloud. It is also sometimes called "mammatus", an earlier version of the term used before a standardization of Latin nomenclature brought about by the World Meterorological Organization during the 20th century. The best-known is cumulonimbus with mammatus, but the mamma feature is also seen occasionally with cirrus, cirrocumulus, altocumulus, altostratus, and stratocumulus.
A "tuba" feature is a cloud column that may hang from the bottom of a cumulus or cumulonimbus. A newly formed or poorly organized column might be comparatively benign, but can quickly intensify into a funnel cloud or tornado.
An "arcus" feature is a roll cloud with ragged edges attached to the lower front part of cumulus congestus or cumulonimbus that forms along the leading edge of a squall line or thunderstorm outflow. A large arcus formation can have the appearance of a dark menacing arch.
There are some arcus-like clouds that form as a consequence of interactions with specific geographical features rather than with a parent cloud. Perhaps the strangest geographically specific cloud of this type is the Morning Glory, a rolling cylindrical cloud that appears unpredictably over the Gulf of Carpentaria in Northern Australia. Associated with a powerful "ripple" in the atmosphere, the cloud may be "surfed" in glider aircraft. It has been officially suggested that roll clouds of this type that are not attached to a parent cloud be reclassified as a new species of stratocumulus, possibly with the Latin name "volutus".
Accessory clouds.
Supplementary cloud formations detached from the main cloud are known as accessory clouds. The heavier precipitating clouds, nimbostratus, towering cumulus (cumulus congestus), and cumulonimbus typically see the formation in precipitation of the "pannus" feature, low ragged clouds of the genera and species cumulus fractus or stratus fractus.
After the pannus types, the remaining accessory clouds comprise formations that are associated mainly with upward-growing cumuliform and cumulonimbiform clouds of free convection. "Pileus" is a cap cloud that can form over a cumulonimbus or large cumulus cloud, whereas a "velum" feature is a thin horizontal sheet that sometimes forms like an apron around the middle or in front of the parent cloud.
Under conditions of strong atmospheric wind shear and instability, wave-like undulatus formations may break into regularly spaced crests. This variant has no separate WMO Latin designation, but is sometimes known informally as a Kelvin–Helmholtz (wave) cloud. This phenomenon has also been observed in cloud formations over other planets and even in the sun's atmosphere. It has been formally suggested that this wave cloud be classified as a supplementary feature, possibly with the Latin name "fluctus". Another wave-like cloud feature that is distinct from the variety undulatus has been given the Latin name "asperatus". It has been recommended for formal classification as a supplementary feature using its suggested Latin name.
A circular fall-streak hole occasionally forms in a thin layer of supercooled altocumulus or cirrocumulus. Fall streaks consisting of virga or wisps of cirrus are usually seen beneath the hole as ice crystals fall out to a lower altitude. This type of hole is usually larger than typical lacunosus holes, and a formal recommendation has been made to classify it as a supplementary feature, possibly with the Latin name "cavus".
Mother clouds.
Clouds initially form in clear air or become clouds when fog rises above surface level. The genus of a newly formed cloud is determined mainly by air mass characteristics such as stability and moisture content. If these characteristics change over time, the genus tends to change accordingly. When this happens, the original genus is called a "mother cloud". If the mother cloud retains much of its original form after the appearance of the new genus, it is termed a "genitus" cloud. One example of this is "stratocumulus cumulogenitus", a stratocumulus cloud formed by the partial spreading of a cumulus type when there is a loss of convective lift. If the mother cloud undergoes a complete change in genus, it is considered to be a "mutatus" cloud.
It has been officially recommended that the genitus category be expanded to include certain types that do not originate from pre-existing clouds or as the result of any natural atmospheric processes. Among vertically developed clouds, these may include "flammagenitus" for cumulus congestus or cumulonimbus that are formed by large scale fires or volcanic eruptions. Smaller low-étage "pyrocumulus" or "fumulus" clouds formed by contained industrial activity could be classified as cumulus "homogenitus". Contrails formed from the exhaust of aircraft flying in the high étage can persist and spread into formations resembling any of the high cloud genus-types. These variants have no special WMO designations, but are sometimes given the faux-Latin name Aviaticus. Persistent contrails have been identified as candidates for possible inclusion in the genitus category as cirrus, cirrostratus, or cirrocumulus homogenitus
Stratocumulus fields.
Stratocumulus clouds can be organized into "fields" that take on certain specially classified shapes and characteristics. In general, these fields are more discernible from high altitudes than from ground level. They can often be found in the following forms:
Vortex streets.
These patterns are formed from a phenomenon known as a Kármán vortex which is named after the engineer and fluid dynamicist Theodore von Kármán. When wind driven clouds are forced through a mountain range, or when ocean wind driven clouds encounter a high elevation island, they can begin to circle the mountain or high land mass. They can form at any altitude in the troposphere and are not restricted to any particular cloud type.
Formation and distribution.
How the air becomes saturated.
Air can become saturated as a result of being cooled to its dew point or by having moisture added from an adjacent source.
Adiabatic cooling occurs when one or more of three possible lifting agents - cyclonic/frontal, convective, or orographic — causes air containing invisible water vapor to rise and cool to its dew point, the temperature at which the air becomes saturated. The main mechanism behind this process is adiabatic cooling. If the air is cooled to its dew point and becomes saturated, it normally sheds vapor it can no longer retain, which condenses into cloud. Water vapor in saturated air is normally attracted to condensation nuclei such as dust and salt particles that are small enough to be held aloft by normal circulation of the air.
Frontal and cyclonic lift occur when stable air is forced aloft at weather fronts and around centers of low pressure. Warm fronts associated with extratropical cyclones tend to generate mostly cirriform and stratiform clouds over a wide area unless the approaching warm airmass is unstable, in which case cumulus congestus or cumulonimbus clouds will usually be embedded in the main precipitating cloud layer. Cold fronts are usually faster moving and generate a narrower line of clouds which are mostly stratocumuliform, cumuliform, or cumulonimbiform depending on the stability of the warm air mass just ahead of the front.
Another agent is the convective upward motion of air caused by daytime solar heating at surface level. Airmass instability allows for the formation of cumuliform clouds that can produce showers if the air is sufficiently moist. On comparatively rare occasions, convective lift can be powerful enough to penetrate the tropopause and push the cloud top into the stratosphere.
A third source of lift is wind circulation forcing air over a physical barrier such as a mountain (orographic lift). If the air is generally stable, nothing more than lenticular cap clouds will form. However, if the air becomes sufficiently moist and unstable, orographic showers or thunderstorms may appear.
Along with adiabatic cooling that requires a lifting agent, there are three major non-adiabatic mechanisms for lowering the temperature of the air to its dew point. Conductive, radiational, and evaporative cooling require no lifting mechanism and can cause condensation at surface level resulting in the formation of fog.
There are several main sources of water vapor that can be added to the air as a way of achieving saturation without any cooling process: Water or moist ground, precipitation or virga, and transpiration from plants
Convergence along low-pressure zones.
Although the local distribution of clouds can be significantly influenced by topography, the global prevalence of cloud cover tends to vary more by latitude. It is most prevalent globally in and along low pressure zones of surface atmospheric convergence which encircle the Earth close to the equator and near the 50th parallels of latitude in the northern and southern hemispheres. The adiabatic cooling processes that lead to the creation of clouds by way of lifting agents are all associated with convergence; a process that involves the horizontal inflow and accumulation of air at a given location, as well as the rate at which this happens. Near the equator, increased cloudiness is due to the presence of the low-pressure Intertropical Convergence Zone (ITCZ) where very warm and unstable air promotes mostly cumuliform and cumulonimbiform clouds. Clouds of virtually any type can form along the mid-latitude convergence zones depending on the stability and moisture content of the air. These extratropical convergence zones are occupied by the polar fronts where air masses of polar origin meet and clash with those of tropical or subtropical origin. This leads to the formation of weather-making extratropical cyclones composed of cloud systems that may be stable or unstable to varying degrees according to the stability characteristics of the various airmasses that are in conflict.
Divergence along high pressure zones.
Divergence is the opposite of convergence. In the Earth's atmosphere, it involves the horizontal outflow of air from the upper part of a rising column of air, or from the lower part of a subsiding column often associated with an area or ridge of high pressure.
Formation and distribution.
Polar stratospheric clouds form in the lowest part of the stratosphere during the winter, at the altitude and during the season that produces the coldest temperatures and therefore the best chances of triggering condensation caused by adiabatic cooling. They are typically very thin with an undulating cirriform appearance. Moisture is scarce in the stratosphere, so nacreous and non-nacreous cloud at this altitude range is rare and is usually restricted to polar regions in the winter where the air is coldest.
Polar mesospheric clouds.
Polar mesospheric clouds form at a single extreme altitude range of about and are consequently not classified into more than one étage. They are given the Latin name noctilucent because of their illumination well after sunset and before sunrise. An alpha-numeric classification is used to identify variations in physical appearance.
Formation and distribution.
Polar mesospheric clouds are the highest in the atmosphere and form near the top of the mesosphere at about ten times the altitude of tropospheric high clouds. From ground level, they can occasionally be seen illuminated by the sun during deep twilight. Ongoing research indicates that convective lift in the mesosphere is strong enough during the polar summer to cause adiabatic cooling of small amount of water vapour to the point of saturation. This tends to produce the coldest temperatures in the entire atmosphere just below the mesopause. These conditions result in the best environment for the formation of polar mesospheric clouds. There is also evidence that smoke particles from burnt-up meteors provide much of the condensation nuclei required for the formation of noctilucent cloud.
Distribution in the mesosphere is similar to the stratosphere except at much higher altitudes. Because of the need for maximum cooling of the water vapor to produce noctilucent clouds, their distribution tends to be restricted to polar regions of Earth. A major seasonal difference is that convective lift from below the mesosphere pushes very scarce water vapor to higher colder altitudes required for cloud formation during the respective summer seasons in the northern and southern hemispheres. Sightings are rare more than 45 degrees south of the north pole or north of the south pole.
Clouds throughout the homosphere.
Luminance and reflectivity.
The luminance or brightness of a cloud in the homosphere (which includes the troposphere, stratosphere, and mesosphere) is determined by how light is reflected, scattered, and transmitted by the cloud's particles. Its brightness may also be affected by the presence of haze or photometeors such as halos and rainbows. In the troposphere, dense, deep clouds exhibit a high reflectance (70% to 95%) throughout the visible spectrum. Tiny particles of water are densely packed and sunlight cannot penetrate far into the cloud before it is reflected out, giving a cloud its characteristic white color, especially when viewed from the top. Cloud droplets tend to scatter light efficiently, so that the intensity of the solar radiation decreases with depth into the gases. As a result, the cloud base can vary from a very light to very-dark-grey depending on the cloud's thickness and how much light is being reflected or transmitted back to the observer. High thin tropospheric clouds reflect less light because of the comparatively low concentration of constituent ice crystals or supercooled water droplets which results in a slightly off-white appearance. However, a thick dense ice-crystal cloud appears brilliant white with pronounced grey shading because of its greater reflectivity.
As a tropospheric cloud matures, the dense water droplets may combine to produce larger droplets. If the droplets become too large and heavy to be kept aloft by the air circulation, they will fall from the cloud as rain. By this process of accumulation, the space between droplets becomes increasingly larger, permitting light to penetrate farther into the cloud. If the cloud is sufficiently large and the droplets within are spaced far enough apart, a percentage of the light that enters the cloud is not reflected back out but is absorbed giving the cloud a darker look. A simple example of this is one's being able to see farther in heavy rain than in heavy fog. This process of reflection/absorption is what causes the range of cloud color from white to black.
Coloration.
Striking cloud colorations can be seen at many altitudes in the homosphere. The color of a cloud is usually the same as the incident light.
During daytime when the sun is relatively high in the sky, tropospheric clouds generally appear bright white on top with varying shades of grey underneath. Thin clouds may look white or appear to have acquired the color of their environment or background. Red, orange, and pink clouds occur almost entirely at sunrise/sunset and are the result of the scattering of sunlight by the atmosphere. When the sun is just below the horizon, low-etage clouds are gray, middle clouds appear rose-colored, and high-etage clouds are white or off-white. Clouds at night are black or dark grey in a moonless sky, or whitish when illuminated by the moon. They may also reflect the colors of large fires, city lights, or auroras that might be present.
A cumulonimbus cloud that appears to have a greenish/bluish tint is a sign that it contains extremely high amounts of water; hail or rain which scatter light in a way that gives the cloud a blue color. A green colorization occurs mostly late in the day when the sun is comparatively low in the sky and the incident sunlight has a reddish tinge that appears green when illuminating a very tall bluish cloud. Supercell type storms are more likely to be characterized by this but any storm can appear this way. Coloration such as this does not directly indicate that it is a severe thunderstorm, it only confirms its potential. Since a green/blue tint signifies copious amounts of water, a strong updraft to support it, high winds from the storm raining out, and wet hail; all elements that improve the chance for it to become severe, can all be inferred from this. In addition, the stronger the updraft is, the more likely the storm is to undergo tornadogenesis and to produce large hail and high winds.
Yellowish clouds may be seen in the troposphere in the late spring through early fall months during forest fire season. The yellow color is due to the presence of pollutants in the smoke. Yellowish clouds caused by the presence of nitrogen dioxide are sometimes seen in urban areas with high air pollution levels.
In high latitude regions of the stratosphere, nacreous clouds occasionally found there during the polar winter tend to display quite striking displays of mother-of-pearl colorations. This is due to the refraction and diffusion of the sun's rays through thin clouds with supercooled droplets that often contain compounds other than water. At still higher altitudes up in the mesosphere, noctilucent clouds made of ice crystals are sometimes seen in polar regions in the summer. They typically have a bluish or silvery white coloration that can resemble brightly illuminated cirrus. Noctilucent clouds may occasionally take on more of a red or orange hue.
Effects on climate and the atmosphere.
The role of tropospheric clouds in regulating weather and climate remains a leading source of uncertainty in projections of global warming. This uncertainty arises because of the delicate balance of processes related to clouds, spanning scales from millimeters to planetary. Hence, interactions between the large-scale (synoptic meteorology) and clouds becomes difficult to represent in global models.
The complexity and diversity of clouds, as outlined above, adds to the problem. On the one hand, white-colored cloud tops promote cooling of Earth's surface by reflecting "short-wave" radiation from the sun. Most of the sunlight that reaches the ground is absorbed, warming the surface, which emits radiation upward at longer, "infrared", wavelengths. At these wavelengths, however, water in the clouds acts as an efficient absorber. The water reacts by radiating, also in the infrared, both upward and downward, and the downward "long-wave" radiation results in some warming at the surface. This is analogous to the greenhouse effect of greenhouse gases and water vapor.
High-étage tropospheric genus-types, "cirrus", "cirrocumulus", and "cirrostratus", particularly show this duality with both short-wave albedo cooling and long-wave greenhouse warming effects. On the whole though, "ice-crystal" clouds in the upper troposphere tend to favor net "warming". However, the "cooling" effect is dominant with lower clouds made of very small "water droplets", especially when they form in extensive sheets that block out more of the sun. These include middle-étage layers of "altocumulus" and "altostratus" as well as low "stratocumulus", and "stratus" that have droplets with an average radius of about 0.002 mm (0.00008 in).. Small-droplet aerosols are not good at absorbing long-wave radiation reflected back from Earth, so there is a net cooling with almost no long-wave effect. This effect is particularly pronounced with low-étage clouds that form over water.
Low and vertical heaps of "cumulus", "towering cumulus", and "cumulonimbus" are made of larger water droplets ranging in radius from 0.005 to about 0.015 mm. Nimbostratus cloud droplets can also be quite large, up to 0.015mm radius. These larger droplets associated with vertically developed clouds are better able to trap the long-wave radiation thus mitigating the cooling effect to some degree. However, these large often precipitating clouds are variable or unpredictable in their overall effect because of variations in their concentration, distribution, and vertical extent. Measurements taken by NASA indicate that on the whole, the effects of low and middle étage clouds that tend to promote cooling are outweighing the warming effects of high layers and the variable outcomes associated with multi-étage or vertically developed clouds.
As difficult as it is to evaluate the effects of current cloud cover characteristics on climate change, it is even more problematic to predict the outcome of this change with respect to future cloud patterns and events. As a consequence, much research has focused on the response of low and vertical clouds to a changing climate. Leading global models can produce quite different results, however, with some showing increasing low-étage clouds and others showing decreases.
In the stratosphere, Type I non-nacreous clouds are known to have harmful effects over the polar regions of Earth. They become catalysts which convert relatively benign man-made chlorine into active free radicals like chlorine monoxide which are destructive of the stratospheric ozone layer.
Polar mesospheric clouds are not common or widespread enough to have a significant effect on climate. However, an increasing frequency of occurrence of noctilucent clouds since the 19th century may be the result of climate change.
Global brightening.
New research indicates a global brightening trend. The details are not fully understood, but much of the global dimming (and subsequent reversal) is thought to be a consequence of changes in aerosol loading in the atmosphere, especially sulfur-based aerosol associated with biomass burning and urban pollution. Changes in aerosol burden can have indirect effects on clouds by changing the droplet size distribution, or the lifetime and precipitation characteristics of clouds.
Extraterrestrial.
Cloud cover has been seen on most other planets in the solar system. Venus's thick clouds are composed of sulfur dioxide and appear to be almost entirely stratiform. They are arranged in three main layers at altitudes of 45 to 65 km that obscure the planet's surface and can produce virga. No embedded cumuliform types have been identified, but broken stratocumuliform wave formations are sometimes seen in the top layer that reveal more continuous layer clouds underneath. On Mars, cirrus, cirrocumulus and stratocumulus composed of water-ice have been detected mostly near the poles. Water-ice fogs have also been detected on this planet.
Both Jupiter and Saturn have an outer cirriform cloud deck composed of ammonia, an intermediate stratiform haze-cloud layer made of ammonium hydrosulfide, and an inner deck of cumulus water clouds. Embedded cumulonimbus are known to exist near the Great Red Spot on Jupiter. The same category-types can be found covering Uranus, and Neptune, but are all composed of Methane. Saturn's moon Titan has cirrus clouds believed to be composed largely of methane. The Cassini–Huygens Saturn mission uncovered evidence of a fluid cycle on Titan, including lakes near the poles and fluvial channels on the surface of the moon.
In October 2013, the detection of high altitude optically thick clouds in the atmosphere of Kepler-7b was announced, and, in December 2013, also in the atmospheres of GJ 436 b and GJ 1214 b.

</doc>
<doc id="47517" url="https://en.wikipedia.org/wiki?curid=47517" title="Planetary body">
Planetary body

A planetary body or planetary object is any secondary body in the Solar System that is geologically differentiated or in hydrostatic equilibrium and thus has a planet-like geology: a planet, dwarf planet, or the larger moons and asteroids.
In 2002, planetary scientists Alan Stern and Harold Levison proposed the following algorithm to determine whether an object in space satisfies the definition for a planetary body. The body must:
This definition excludes brown dwarfs and stars, as well as small bodies such as planetesimals.

</doc>
<doc id="47518" url="https://en.wikipedia.org/wiki?curid=47518" title="Cloud feedback">
Cloud feedback

Cloud feedback is the coupling between cloudiness and surface air temperature in which a change in radiative forcing perturbs the surface air temperature, leading to a change in clouds, which could then amplify or diminish the initial temperature perturbation.
Global warming is expected to change the distribution and type of clouds. Seen from below, clouds emit infrared radiation back to the surface, and so exert a warming effect; seen from above, clouds reflect sunlight and emit infrared radiation to space, and so exert a cooling effect. Cloud representations vary among global climate models, and small changes in cloud cover have a large impact on the climate. Differences in planetary boundary layer cloud modeling schemes can lead to large differences in derived values of climate sensitivity. A model that decreases boundary layer clouds in response to global warming has a climate sensitivity twice that of a model that does not include this feedback. However, satellite data show that cloud optical thickness actually increases with increasing temperature. Whether the net effect is warming or cooling depends on details such as the type and altitude of the cloud; details that are difficult to represent in climate models.
In addition to how clouds themselves will respond to increased temperatures, other feedbacks affect clouds properties and formation. The amount and vertical distribution of water vapor is closely linked to the formation of clouds. Ice crystals have been shown to largely influence the amount of water vapor. Water vapor in the subtropical upper troposphere has been linked to the convection of water vapor and ice. Changes in subtropical humidity could provide a negative feedback that decreases the amount of water vapor which in turn would act to mediate global climate transitions.
Changes in cloud cover are closely coupled with other feedback, including the water vapor feedback and ice-albedo feedback. Changing climate is expected to alter the relationship between cloud ice and supercooled cloud water, which in turn would influence the microphysics of the cloud which would result in changes in the radiative properties of the cloud. Climate models suggest that a warming will increase fractional cloudiness. Increased cloudiness cools the climate, resulting in a negative feedback. Increasing temperatures in the polar regions is expected in increase the amount of low-level clouds, whose stratification prevents the convection of moisture to upper levels. This feedback would partially cancel the increased surface warming due to the cloudiness. This negative feedback has less effect than the positive feedback. The upper atmosphere more than cancels negative feedback that causes cooling, and therefore the increase of CO2 is actually exacerbating the positive feedback. Therefore as the cited paper notes, Global Warming will continue unabated as more CO2 enters the system

</doc>
<doc id="47519" url="https://en.wikipedia.org/wiki?curid=47519" title="Cloud forcing">
Cloud forcing

Cloud forcing (sometimes described as cloud radiative forcing) is, in meteorology, the difference between the radiation budget components for average cloud conditions and cloud-free conditions. Much of the interest in cloud forcing relates to its role as a feedback process in the present period of global warming. 
All global climate models used for climate change projections include the effects of water vapor and cloud forcing. The models include the effects of clouds on both incoming (solar) and emitted (terrestrial) radiation.
Clouds increase the global reflection of solar radiation from 15% to 30%, reducing the amount of solar radiation absorbed by the Earth by about 44 W/m². This cooling is offset somewhat by the greenhouse effect of clouds which reduces the outgoing longwave radiation by about 31 W/m². Thus the net cloud forcing of the radiation budget is a loss of about 13 W/m². If the clouds were removed with all else remaining the same, the Earth would gain this last amount in net radiation and begin to warm up. These numbers should not be confused with the usual radiative forcing concept, which is for the "change" in forcing related to climate change. 
Without the inclusion of clouds, water vapor alone contributes 36% to 70% of the greenhouse effect on Earth. When water vapor and clouds are considered together, the contribution is 66% to 85%. The ranges come about because there are two ways to compute the influence of water vapor and clouds: the lower bounds are the reduction in the greenhouse effect if water vapor and clouds are "removed" from the atmosphere leaving all other greenhouse gases unchanged, while the upper bounds are the greenhouse effect introduced if water vapor and clouds are "added" to an atmosphere with no other greenhouse gases. The two values differ because of overlap in the absorption and emission by the various greenhouse gases. Trapping of the long-wave radiation due to the presence of clouds reduces the radiative forcing of the greenhouse gases compared to the clear-sky forcing. However, the magnitude of the effect due to clouds varies for different greenhouse gases. Relative to clear skies, clouds reduce the global mean radiative forcing due to CO2 by about 15%, that due to CH4 and N2O by about 20%, and that due to the halocarbons by up to 30%. 
Clouds remain one of the largest uncertainties in future projections of climate change by global climate models, owing to the physical complexity of cloud processes and the small scale of individual clouds relative to the size of the model computational grid.

</doc>
<doc id="47520" url="https://en.wikipedia.org/wiki?curid=47520" title="Coccolithophore">
Coccolithophore

A coccolithophore (or coccolithophorid, from the adjective ) is a unicellular, eukaryotic phytoplankton (alga). They belong either to the kingdom Protista, according to Robert Whittaker's Five kingdom classification, or Chromalveolata, according to the newer Thomas Cavalier-Smith Biological Classification system. Within the Chromalveolata, the coccolithophorids are in the phylum or division Haptophyta, class Prymnesiophyceae (or Coccolithophyceae). Coccolithophorids are distinguished by special calcium carbonate plates (or scales) of uncertain function called "coccoliths", which are also important microfossils. However, there are Prymnesiophyceae species lacking coccoliths (e.g. in genus "Prymnesium"), so not every member of Prymnesiophyceae is coccolithophorid. Coccolithophores are almost exclusively marine and are found in large numbers throughout the sunlight zone of the ocean.
The most abundant species of coccolithophore, "Emiliania huxleyi", belongs to the order Isochrysidales and family Noëlaerhabdaceae. It is found in temperate, subtropical, and tropical oceans. This makes "E. huxleyi" an important part of the planktonic base of a large proportion of marine food webs. It is also the fastest growing coccolithophore in laboratory cultures. It is studied for the extensive blooms it forms in nutrient depleted waters after the reformation of the summer thermocline. and for its production of molecules known as alkenones that are commonly used by earth scientists as a means to estimate past sea surface temperatures. Coccolithophores are of particular interest to those studying global climate change because as ocean acidity increases, their coccoliths may become even more important as a carbon sink. Furthermore, management strategies are being employed to prevent eutrophication-related coccolithophore blooms, as these blooms lead to a decrease in nutrient flow to lower levels of the ocean.
Structure.
Coccolithophores are spherical cells about 5–100 micrometres across, enclosed by calcareous plates called coccoliths, which are about 2–25 micrometres across. Each cell contains two brown chloroplasts which surround the nucleus.
Exoskeleton (coccosphere).
Each unicellular plankton is enclosed in its own collection of coccoliths, the calcified scales, which make up its exoskeleton or coccosphere. The coccoliths are created inside the cell and while some species maintain a single layer throughout life only producing new coccoliths as the cell grows, others continually produce and shed coccoliths.
Composition.
The primary constituent of coccoliths is calcium carbonate, or chalk. Calcium carbonate is transparent, so the organisms’ photosynthetic activity is not compromised by encapsulation in a coccosphere.
Formation.
Coccoliths are produced by a biomineralization process known as coccolithogenesis. Generally, calcification of coccoliths occurs in the presence of light, and these scales are produced much more during the exponential phase of growth than the stationary phase. Although not yet entirely understood, the biomineralization process is tightly regulated by calcium signaling. Calcite formation begins in the golgi complex where protein templates nucleate the formation of CaCO3 crystals and complex acidic polysaccharides control the shape and growth of these crystals. As each scale is produced, it is exported in a Golgi-derived vesicle and added to the inner surface of the coccosphere. This means that the most recently produced coccoliths may lie beneath older coccoliths.
Depending upon the phytoplankton’s stage in the life cycle, two different types of coccoliths may be formed. Holococcoliths are produced only in the haploid phase, lack radial symmetry, and are composed of anywhere from hundreds to thousands of similar minute (ca 0.1 µm) rhombic calcite crystals. These crystals are thought to form at least partially outside the cell. Heterococcoliths occur only in the diploid phase, have radial symmetry, and are composed of relatively few complex crystal units (less than 100). Although they are rare, combination coccospheres, which contain both holococcoliths and heterococcoliths, have been observed in the plankton recording coccolithophore life cycle transitions. Finally, the coccospheres of some species are highly modified with various appendages made of specialized coccoliths.
Function.
While the exact function of the coccosphere is unclear, many potential functions have been proposed. Most obviously coccoliths may protect the phytoplankton from predators. In addition, these exoskeletons may confer an advantage in energy production, as coccolithogenesis seems highly coupled with photosynthesis. Organic precipitation of calcium carbonate from bicarbonate solution produces free carbon dioxide directly within the cellular body of the alga, this additional source of gas is then available to the Coccolithophore for photosynthesis. It has been suggested that they may provide a cell-wall like barrier to isolate intracellular chemistry from the marine environment. More specific, defensive properties of coccoliths may include protection from osmotic changes, chemical or mechanical shock, and short-wavelength light. It has also been proposed that the added weight of multiple layers of coccoliths allows the organism to sink to lower, more nutrient rich layers of the water and conversely, that coccoliths add buoyancy, stopping the cell from sinking to dangerous depths. Coccolith appendages have also been proposed to serve several functions, such as inhibiting grazing by zooplankton.
Uses.
Coccoliths are the main component of the Chalk, a Late Cretaceous rock formation which outcrops widely in southern England and forms the White Cliffs of Dover, and of other similar rocks in many other parts of the world. At the present day sedimented coccoliths are a major component of the calcareous oozes that cover up to 35% of the ocean floor and is kilometres thick in places. Because of their abundance and wide geographic ranges, the coccoliths which make up the layers of this ooze and the chalky sediment formed as it is compacted serve as valuable microfossils.
Cellular anatomy.
Enclosed in each coccosphere is a single cell with membrane bound organelles. Two large chloroplasts with brown pigment are located on either side of the cell and surround the nucleus, mitochondria, golgi apparatus, endoplasmic reticulum, and other organelles. Each cell also has two flagellar structures, which are involved not only in motility, but also in mitosis and formation of the cytoskeleton. In some species, a functional or vestigial haptonema is also present. This structure, which is unique to haptophytes, coils and uncoils in response to environmental stimuli. Although poorly understood, it has been proposed to be involved in prey capture. 
Ecology.
Life history strategy.
The life cycle of coccolithophores is characterized by an alternation of diploid and haploid phases. They alternate from the haploid to diploid phase through syngamy and from diploid to haploid through meiosis. In contrast with most organisms with alternating life cycles, asexual reproduction by mitosis is possible in both phases of the life cycle. Both abiotic and biotic factors may affect the frequency with which each phase occurs.
Coccolithophores reproduce asexually through binary fission. In this process the coccoliths from the parent cell are divided between the two daughter cells. There have been suggestions stating the possible presence of a sexual reproduction process due to the diploid stages of the coccolithophores, but this process has never been observed.
K or r- selected strategies of coccolithophores depend on their life cycle stage. When coccolithophores are diploid, they are r-selected. In this phase they tolerate a wider range of nutrient compositions. When they are haploid they are K- selected and are often more competitive in stable low nutrient environments. Most coccolithophores are K strategist and are usually found on nutrient-poor surface waters. They are poor competitors when compared to other phytoplankton and thrive in habitats where other phytoplankton would not survive.
These two stages in the life cycle of coccolithophores occur seasonally, where more nutrition is available in warmer seasons and less is available in cooler seasons. This type of life cycle is known as a complex heteromorphic life cycle.
Global distribution.
Coccolithophores occur throughout the world ocean. Their distribution varies vertically by stratified layers in the ocean and geographically by different temporal zones. While most modern coccolithophores can be located in their associated stratified oligotrophic conditions, the most abundant areas of coccolithophores where there is the highest species diversity are located in subtropical zones with a temperate climate. While water temperature and the amount of light intensity entering the water’s surface are the more influential factors in determining where species are located, the ocean currents also can determine the location where certain species of coccolithophores are found.
Although motility and colony formation vary according to the life cycle of different coccolithophore species, there is often alternation between a motile, haploid phase, and a non-motile diploid phase. In both phases, the organism’s dispersal is largely due to ocean currents and circulation patterns.
Within the Pacific Ocean, approximately 90 species have been identified with six separate zones relating to different Pacific currents that contain unique groupings of different species of coccolithophores. The highest diversity of coccolithophores in the Pacific Ocean was in an area of the ocean considered the Central North Zone which is an area between 30 oN and 5 oN, composed of the North Equatorial Current and the Equatorial Countercurrent. These two currents move in opposite directions, east and west, allowing for a strong mixing of waters and allowing a large variety of species to populate the area.
In the Atlantic Ocean, the most abundant species are "E. huxleyi" and "Florisphaera profunda" with smaller concentrations of the species "Umbellosphaera" "irregularis", "Umbellosphaera tenuis" and different species of "Gephyrocapsa". Deep-dwelling coccolithophore species abundance is greatly affected by nutricline and thermocline depths. These coccolithophores increase in abundance when the nutricline and thermocline are deep and decrease when they are shallow.
The complete distribution of coccolithophores is currently not known and some regions, such as the Indian Ocean, are not as well known as other locations in the Pacific and Atlantic Oceans. It is also very hard to explain distributions due to multiple constantly changing factors involving the ocean properties, such as coastal and equatorial upwelling, frontal systems, benthic environments, unique oceanic topography, and pockets of isolated high or low water temperatures.
Table 1 and Table 2 show simplified specific species preferences based on geographic location and water depth respectively. These do not contain all coccolithophore species and are just a simplification of major, more well-known species.
The upper photic zone is low in nutrient concentration, high in light intensity and penetration, and usually higher in temperature. The lower photic zone is high in nutrient concentration, low in light intensity and penetration and relatively cool. The middle photic zone is an area that contains the same values in between that of the lower and upper photic zones.
Effect of global climate change on distribution and abundance.
Recent studies show that climate change has direct and indirect impacts on Coccolithophore distribution and productivity. They will inevitably be affected by the increasing temperatures and thermal stratification of the top layer of the ocean, since these are prime controls on their ecology, although it is not clear whether global warming would result in net increase or decrease of coccolithophores. It has been suggested that since they calcify ocean acidification due to increasing carbon dioxide could severely affect coccolithophores. Recent CO2 increases have seen a sharp increase in the population of coccolithophores.
Role in the food web.
Coccolithophores are one of the more abundant primary producers in the ocean. As such, they are a large contributor to the primary productivity of the tropical and subtropical oceans, however, exactly how much has yet to have been recorded.
Dependence on nutrients.
The ratio between the concentrations of nitrogen, phosphorus and silicate in particular areas of the ocean dictates competitive dominance within phytoplankton communities. Each ratio essentially tips the odds in favor of either diatoms or more other groups of phytoplankton, such as coccolithophores. A low silicate to nitrogen and phosphorus ratio allows coccolithophores to outcompete other phytoplankton species; however, when silicate to phosphorus to nitrogen ratios are high coccolithophores are outcompeted by diatoms. The increase in agricultural processes lead to eutrophication of waters and thus, coccolithophore blooms in these high nitrogen and phosphorus, low silicate environments.
Impact on water column productivity.
The calcite in calcium carbonate allows coccoliths to scatter more light than they absorb. This has two important consequences: 1) Surface waters become brighter, meaning they have a higher albedo, and 2) there is induced photoinhibition, meaning deeper waters become darker. Hence, a high concentration of coccoliths leads to a simultaneous increase in surface water temperature and decrease in the temperature of deeper waters. This results in more stratification in the water column and a decrease in the vertical mixing of nutrients. However, a recent study estimated that the overall effect of coccolithophores on the increased in radiative forcing of the ocean is less than that from anthropogenic factors. Therefore, the overall result of large blooms of coccolithophores is a decrease in water column productivity, rather than a contribution to global warming.
Predator-prey interactions.
Their predators include the common predators of all phytoplankton including small fish, zooplankton, and shellfish larvae. Viruses specific to this species have been isolated from several locations worldwide and appear to play a major role in spring bloom dynamics.
Toxicity.
No environmental evidence of coccolithophore toxicity has been reported, but they belong to the class Prymnesiophyceae which contain orders with toxic species. Toxic species have been found in the genera "Prymnesium" Massart and "Chrysochromulina" Lackey. Members of the genus "Prymnesium" have been found to produce haemolytic compounds, the agent responsible for toxicity. Some of these toxic species are responsible for large fish kills and can be accumulated in organisms such as shellfish; transferring it through the food chain. In laboratory tests for toxicity members of the oceanic coccolithophore genera "Emiliania, Gephyrocapsa, Calcidiscus" and "Coccolithus" were shown to be non-toxic as were species of the coastal genus "Hymenomonas", however several species of "Pleurochrysis" and "Jomonlithus", both coastal genera were toxic to "Artemia".
Community interactions.
Coccolithophorids are predominantly found as single, free-floating haploid or diploid cells.
Competition.
Most phytoplankton need sunlight and nutrients from the ocean to survive, so they thrive in areas with large inputs of nutrient rich water upwelling from the lower levels of the ocean. Most coccolithophores, only require sunlight for energy production and have a higher ratio of nitrate uptake over ammonium uptake (nitrogen is required for growth and can be used directly from nitrate but not ammonium). Because of this they thrive in still, nutrient-poor environments where other phytoplankton are starving. Trade-offs associated with these faster growth rates, however, include a smaller cell radius and lower cell volume than other types of phytoplankton.
Viral infection and coevolution.
Giant DNA-containing viruses are known to lytically infect coccolithophores, particularly "E. huxleyi". These viruses, known as E. huxleyi viruses (EhVs), appear to infect the coccosphere coated diploid phase of the life cycle almost exclusively. It has been proposed that as the haploid organism is not infected and therefore not affected by the virus, the co-evolutionary “arms race” between coccolithophores and these viruses does not follow the classic Red Queen evolutionary framework, but instead a “Cheshire Cat” ecological dynamic. More recent work has suggested that viral synthesis of sphingolipids and induction of programmed cell death provides a more direct link to study a Red Queen-like coevolutionary arms race at least between the coccolithoviruses and diploid organism.
Importance in global climate change.
Impact on the carbon cycle.
Coccolithophores have both long and short term effects on the carbon cycle. The production of coccoliths requires the uptake of dissolved inorganic carbon and calcium. Calcium carbonate and carbon dioxide are produced from calcium and bicarbonate by the following chemical reaction:
Because coccolithophores are photosynthetic organisms, they are able to use some of the CO2 released in the calcification reaction for photosynthesis.
However, the production of calcium carbonate drives surface alkalinity down, and in conditions of low alkalinity the CO2 is instead released back into the atmosphere.
As a result of this, researchers have postulated that large blooms of coccolithophores may contribute to global warming in the short term. A more widely accepted idea, however, is that over the long term coccolithophores contribute to an overall decrease in atmospheric CO2 concentrations. During calcification two carbon atoms are taken up and one of them becomes trapped as calcium carbonate. This calcium carbonate sinks to the bottom of the ocean in the form of coccoliths and becomes part of sediment; thus, coccolithophores provide a sink for emitted carbon, mediating the effects of greenhouse gas emissions.
Evolutionary responses to ocean acidification.
Research also suggests that ocean acidification due to increasing concentrations of CO2 in the atmosphere may affect the calcification machinery of coccolithophores. This may not only affect immediate events such as increases in population or coccolith production, but also may induce evolutionary adaptation of coccolithophore species over longer periods of time. For example, coccolithophores use H+ ion channels in to constantly pump H+ ions out of the cell during coccolith production. This allows them to avoid acidosis, as coccolith production would otherwise produce a toxic excess of H+ ions. When the function of these ion channels is disrupted, the coccolithophores stop the calcification process to avoid acidosis, thus forming a feedback loop. Low ocean alkalinity, impairs ion channel function and therefore places evolutionary selective pressure on coccolithophores and makes them (and other ocean calcifiers) vulnerable to ocean acidification. In 2008, field evidence indicating an increase in calcification of newly formed ocean sediments containing coccolithophores bolstered the first ever experimental data showing that an increase in ocean CO2 concentration results in an increase in calcification of these organisms.
Decreasing coccolith mass is related to both the increasing concentrations of CO2 and decreasing concentrations of CO3 in the world’s oceans. This lower calcification is assumed to put coccolithophores at ecological disadvantage. Some species like "Calcidiscus" "leptoporus", however, are not affected in this way, while the most abundant coccolithophore species, "E. huxleyi" is. Also, highly calcified coccolithophorids have been found in conditions of low CO−3 contrary to predictions. Understanding the effects of increasing ocean acidification on coccolithophore species is absolutely essential to predicting the future chemical composition of the ocean, particularly its carbonate chemistry. Viable conservation and management measures will come from future research in this area. Groups like the European-based CALMARO are monitoring the responses of coccolithophore populations to varying pH’s and working to determine environmentally sound measures of control.
Impact on nannofossil record.
Coccolith fossils are prominent and valuable calcareous nannofossils (see Micropaleontology). Of particular interest are fossils dating back to the Palaeocene-Eocene Thermal Maximum 55 million years ago. This period is thought to correspond most directly to the current levels of CO2 in the ocean. Finally, field evidence of coccolithophore fossils in rock were used to show that the deep-sea fossil record bears a rock record bias similar to the one that is widely accepted to affect the land-based fossil record.
Impact on the oceans.
The coccolithophorids help in regulating the temperature of the oceans. They thrive in warm seas and release DMS (demethyl sulphide) into the air whose nuclei help to produce thicker clouds to block the sun. When the oceans cool, the number of coccolithophorids decrease and the amount of clouds also decrease. When there are fewer clouds blocking the sun, the temperature also rises. This, therefore, maintains the balance and equilibrium of nature.
Modern Culture.
The coccolithophores are featured on the new design for the 500 Kroner bank note.
External links.
Sources of detailed information
Introductions to coccolithophores

</doc>
<doc id="47521" url="https://en.wikipedia.org/wiki?curid=47521" title="Condensation">
Condensation

Condensation is the change of the physical state of matter from gas phase into liquid phase, and is the reverse of evaporation. The word most often refers to the water cycle. It can also be defined as the change in the state of water vapour to liquid water when in contact with a liquid or solid surface or cloud condensation nuclei within the atmosphere. When the transition happens from the gaseous phase into the solid phase directly, the change is called deposition.
Initiation.
Condensation is initiated by the formation of atomic/molecular clusters of that species within its gaseous volume—like rain drop or snow flake formation within clouds—or at the contact between such gaseous phase and a liquid or solid surface.
Reversibility scenarios.
A few distinct reversibility scenarios emerge here with respect to the nature of the surface.
Most common scenarios.
Condensation commonly occurs when a vapor is cooled and/or compressed to its saturation limit when the molecular density in the gas phase reaches its maximal threshold. Vapor cooling and compressing equipment that collects condensed liquids is called a "condenser".
How condensation is measured.
Psychrometry measures the rates of condensation from and evaporation into the air moisture at various atmospheric pressures and temperatures. Water is the product of its vapor condensation—condensation is the process of such phase conversion.
Applications of condensation.
Condensation is a crucial component of distillation, an important laboratory and industrial chemistry application.
Because condensation is a naturally occurring phenomenon, it can often be used to generate water in large quantities for human use. Many structures are made solely for the purpose of collecting water from condensation, such as air wells and fog fences. Such systems can often be used to retain soil moisture in areas where active desertification is occurring—so much so that some organizations educate people living in affected areas about water condensers to help them deal effectively with the situation.
It is also a crucial process in forming particle tracks in a cloud chamber. In this case, ions produced by an incident particle act as nucleation centers for the condensation of the vapor producing the visible "cloud" trails.
Biological adaptation.
Numerous living beings use water made accessible by condensation. A few examples of these are the Australian Thorny Devil, the darkling beetles of the Namibian coast, and the Coast Redwoods of the West Coast of the United States.
Condensation in building construction.
Condensation in building construction is an unwanted phenomenon as it may cause dampness, mold health issues, wood rot, corrosion, weakening of mortar and masonry walls, and energy penalties due to increased heat transfer. To alleviate these issues, the indoor air humidity needs to be lowered, or air ventilation in the building needs to be improved. This can be done in a number of ways, for example opening windows, turning on extractor fans, using dehumidifiers, drying clothes outside and covering pots and pans whilst cooking. Air conditioning or ventilation systems can be installed that help remove moisture from the air, and move air throughout a building. The amount of water vapour that can be stored in the air can be increased simply by increasing the temperature. However, this can be a double edged sword as most condensation in the home occurs when warm, moisture heavy air comes into contact with a cool surface. As the air is cooled, it can no longer hold as much water vapour. This leads to deposition of water on the cool surface. This is very apparent when central heating is used in combination with single glazed windows in winter.
Interstructure condensation may be caused by thermal bridges, insufficient or lacking insulation, damp proofing or insulated glazing.

</doc>
<doc id="47525" url="https://en.wikipedia.org/wiki?curid=47525" title="Contrail">
Contrail

Contrails (; short for "condensation trails") or vapor trails are line-shaped clouds sometimes produced by aircraft engine exhaust, typically at aircraft cruise altitudes several miles above the Earth’s surface. Contrails are composed primarily of water, in the form of ice crystals. The combination of water vapor in aircraft engine exhaust and the low ambient temperatures that often exists at these high altitudes allows the formation of the trails. Impurities in the jet exhaust from the fuel, including sulfur compounds (0.05% by weight in jet fuel) provide some of the particles that can serve as sites for water droplet growth in the exhaust and, if water droplets form, they might freeze to form ice particles that compose a contrail. Their formation can also be triggered by changes in air pressure in wingtip vortices or in the air over the entire wing surface.
Depending on the temperature and humidity at the altitude the contrails form, they may be visible for only a few seconds or minutes, or may persist for hours and spread to be several miles wide, eventually resembling natural cirrus or altocumulus clouds. Persistent contrails are of interest to scientists because they increase the cloudiness of the atmosphere. The resulting cloud forms may resemble cirrus, cirrocumulus, or cirrostratus, and are sometimes called cirrus aviaticus. Persistent spreading contrails are thought by some, without overwhelming scientific proof, to have a significant effect on global climate.
Condensation from engine exhaust.
The main products of hydrocarbon fuel combustion are carbon dioxide and water vapor. At high altitudes this water vapor emerges into a cold environment, and the local increase in water vapor can raise the relative humidity of the air past saturation point. The vapor then condenses into tiny water droplets which freeze if the temperature is low enough. These millions of tiny water droplets and/or ice crystals form the contrails. The time taken for the vapor to cool enough to condense accounts for the contrail forming some way behind the aircraft's engines. At high altitudes, supercooled water vapor requires a trigger to encourage deposition or condensation. The exhaust particles in the aircraft's exhaust act as this trigger, causing the trapped vapor to 
condense rapidly. Exhaust contrails usually form at high altitudes; usually above , where the air temperature is below . They can also form closer to the ground when the air is very cold and has enough moisture.
Condensation from decreases in pressure.
As a wing generates lift, it causes a vortex to form at each wingtip, and sometimes also at the tip of each wing flap. (Both wingtips and flap-boundaries are discontinuities in airflow.) These wingtip vortices persist in the atmosphere long after the aircraft has passed. The reduction in pressure and temperature across each vortex can cause water to condense and make the cores of the wingtip vortices visible. This effect is more common on humid days. Wingtip vortices can sometimes be seen behind the wing flaps of airliners during takeoff and landing, and during landing of the Space shuttle.
The visible cores of wingtip vortices contrast with the other major type of contrails which are caused by the combustion of fuel. Contrails produced from jet engine exhaust are seen at high altitude, directly behind each engine. By contrast, the visible cores of wingtip vortices are usually seen only at low altitude where the aircraft is travelling slowly after takeoff or before landing, and where the ambient humidity is higher. They trail behind the wingtips and wing flaps rather than behind the engines.
During high-thrust settings the fan blades at the intake of a turbofan engine reach transonic speeds, causing a sudden drop in air pressure. This creates the condensation fog (inside the intake) which is often observed by air travelers during takeoff. For more information see the Prandtl-Glauert singularity effect.
The tips of rotating surfaces (such as propellers and rotors) sometimes produce visible contrails.
Contrails and climate.
Contrails, by affecting the Earth's radiation balance, act as a radiative forcing. Studies have found that contrails trap outgoing longwave radiation emitted by the Earth and atmosphere (positive radiative forcing) at a greater rate than they reflect incoming solar radiation (negative radiative forcing). NASA conducted a great deal of detailed research on atmospheric and climatological effects of contrails, including effects on ozone, ice crystal formation, and particle composition, during the Atmospheric Effects of Aviation Project (AEAP). Global radiative forcing has been calculated from the reanalysis data, climatological models and radiative transfer codes. It is estimated to amount to 0.012 W/m2 (watts per square meter) for 2005, with an uncertainty range of 0.005 to 0.026 W/m2, and with a low level of scientific understanding. Therefore, the overall net effect of contrails is positive, i.e. a "warming" effect. However, the effect varies daily and annually, and overall the magnitude of the forcing is not well known: globally (for 1992 air traffic conditions), values range from 3.5 mW/m2 to 17 mW/m2. Other studies have determined that night flights are mostly responsible for the warming effect: while accounting for only 25% of daily air traffic, they contribute 60 to 80% of contrail radiative forcing. Similarly, winter flights account for only 22% of annual air traffic, but contribute half of the annual mean radiative forcing.
A 2015 study found that artificial cloudiness caused by contrail "outbreaks" reduce the difference between daytime and nighttime temperatures. The former are decreased and the latter are increased, in comparison to temperatures the day before and the day after such outbreaks. On days with outbreaks the day/night temperature difference was diminished by about 6 °F in the U.S. South and 5 °F in the Midwest.
September 11, 2001, climate impact study.
The grounding of planes for three days in the United States after September 11, 2001, provided a rare opportunity for scientists to study the effects of contrails on climate forcing. Measurements showed that without contrails, the local diurnal temperature range (difference of day and night temperatures) was about 1 °C (1.8 °F) higher than immediately before; however, it has also been suggested that this was due to unusually clear weather during the period.
Condensation trails have been suspected of causing "regional-scale surface temperature" changes for some time. Researcher David J. Travis, an atmospheric scientist at the University of Wisconsin-Whitewater, wrote in the science journal "Nature" that the effect of the change in aircraft contrail formation during the three days after the September 11th attacks was observed in surface temperature change, measured across over 4,000 reporting stations in the continental United States. Travis' research documented an "anomalous increase in the average diurnal temperature change". The diurnal temperature range (DTR) is the difference in the day's highs and lows at any weather reporting station. Travis observed a 1.8 °C (3.24 °F) departure from the two adjacent three-day periods to September 11–14. This increase was the largest recorded in 30 years, more than "2 standard deviations away from the mean DTR".
The September 2001 air closures are deeply unusual in the modern world, but similar effects have provisionally been identified from Second World War records, when flying was more tightly controlled. A 2011 study of climate records in the vicinity of large groups of airbases found a case where contrails appeared to induce a statistically significant change in local climate, with a temperature variance around 0.8 °C, suggesting that examination of historic weather data could help study these effects.
Head-on contrails.
A contrail from an airplane flying towards the observer can appear to be generated by an object moving vertically. On November 8, 2010 in California, U.S., a contrail of this type gained wide media attention as a "mystery missile" that could not be explained by U.S. military and aviation authorities, and its explanation as a contrail took more than 24 hours to become accepted by U.S. media and military institutions.
Distrails.
Where an aircraft passes through a cloud, it can clear a path through it; this is known as a distrail (short for "dissipation trail"). The plane's warm engine exhaust causes existing water droplets to evaporate, leaving a clear wake through an otherwise cloudy sky.
Clouds form when invisible water vapor ( in gas phase) condenses into microscopic water droplets ( in liquid phase) or into microscopic ice crystals ( in solid phase). This may happen when air with a high proportion of gaseous water cools. A distrail forms when the heat of engine exhaust evaporates the liquid water droplets in a cloud, turning them back into invisible, gaseous water vapor.

</doc>
