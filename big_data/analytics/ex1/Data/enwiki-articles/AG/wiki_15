<doc id="40259" url="https://en.wikipedia.org/wiki?curid=40259" title="Harthacnut">
Harthacnut

Harthacnut, Harðacnut, or Canute III (; "Tough-knot"; c.1018 – 8 June 1042) was King of Denmark from 1035 to 1042 and King of England from 1040 to 1042.
He was the son of King Cnut the Great (who ruled Denmark, Norway, and England) and Emma of Normandy. When Cnut died in 1035, Harthacnut struggled to retain his father's possessions. Magnus I took control of Norway, but Harthacnut succeeded as King of Denmark and became King of England in 1040 after the death of his half-brother Harold Harefoot.
Harthacnut died suddenly in 1042 and was succeeded by Magnus in Denmark and Edward the Confessor in England. Harthacnut was the last Scandinavian to rule England.
Early life.
Harthacnut was born shortly after the marriage of his parents in July or August 1017. Cnut had put aside his first wife Ælfgifu of Northampton to marry Emma, and according to the "Encomium Emmae Reginae", a book she inspired many years later, Cnut agreed that any sons of their marriage should take precedence over the sons of his first marriage. In 1023, Emma and Harthacnut played a leading role in the translation of the body of the martyr St Æelfheah from London to Canterbury, an occasion seen by Harthacnut's biographer, Ian Howard, as recognition of his position as Cnut's heir in England.
King of Denmark.
In the 1020s, Denmark was threatened by Norway and Sweden, and in 1026, Cnut decided to strengthen its defences by bringing over his eight-year-old son to be the future king under a council headed by his brother-in-law, Earl Ulf. However, Ulf alienated Cnut by getting the Danish provinces to acknowledge Harthacnut as king without reference to Cnut's overall authority and by failing to take vigorous measures to meet Norwegian and Swedish invasions, instead waiting for Cnut's assistance. In 1027, Cnut arrived with a fleet. He forgave Harthacnut his insubordination in view of his youth but had Ulf murdered. He drove the invaders out of Denmark and established his authority over Norway, returning to England in 1028 and leaving Denmark to be ruled by King Harthacnut.
Cnut had left Norway under the rule of Håkon Eiriksson, but he was drowned in 1029, and Cnut appointed his son Svein to rule Norway with the assistance of Ælfgifu, Cnut's first wife and Svein's mother. However, they made themselves unpopular by heavy taxation and favouring Danish advisers over the Norwegian nobles, and when King Magnus I of Norway, the son of the former King of Norway, Olaf, invaded in 1035, they were forced to flee to Harthacnut's court. Harthacnut was a close ally of Svein, but he did not feel his resources were great enough to launch an invasion of Norway, and the half-brothers looked for help from their father, but instead they received news of his death in November 1035.
England and Denmark.
In 1035, Harthacnut succeeded his father on the throne of Denmark as Cnut III. He was unable to come to England in view of the situation in Denmark, and it was agreed that Svein's full brother, Harold Harefoot, should act as regent, with Emma holding Wessex on Harthacnut's behalf. In 1037, Harold was generally accepted as king, Harthacnut being, in the words of the "Anglo-Saxon Chronicle", "forsaken because he was too long in Denmark", while Emma fled to Bruges, in Flanders. In 1039, Harthacnut sailed with ten ships to meet his mother in Bruges but delayed an invasion as it was clear Harold was sick and would soon die, which he did in March 1040. Envoys soon crossed the channel to offer Harthacnut the throne.
While the general outline of events following Cnut's death are clear, the details are obscure, and historians give differing interpretations. M. K. Lawson in his "Dictionary of National Biography" article on Harthacnut states that it is unclear whether Harthacnut was to have England as well as Denmark, but it was probably a reflection of a formal arrangement that mints south of the Thames produced silver pennies in his name, while those to the north were almost all Harold's. There might have been a division of the kingdom if Harthacnut had appeared straight away. He probably stayed in Denmark because of the threat from Magnus of Norway, but they eventually made a treaty by which if either died without an heir, his kingdom would go to the other, and this may have freed Harthacnut to pursue his claim to England.
According to Ian Howard, Harthacnut agreed to help Svein recover Norway and planned an invasion in 1036. Svein died shortly before it was to set out, but Harthacnut proceeded anyway. War was avoided by the treaty between Harthacnut and Magnus, which Harthacnut agreed to because he had no plausible candidate to rule Norway after Svein's death, and he was in any case temperamentally inclined to avoid campaigns and wars. Howard dates the treaty to 1036, whereas other historians date it to 1039 and believe it freed Harthacnut to launch an invasion of England.
In exile in Bruges, Emma plotted to gain the English throne for her son. She sponsored the "Encomium Emmae Reginae", which eulogised her and attacked Harold, especially for arranging the murder of Alfred Atheling (the younger of Emma's two sons by Æthelred) in 1036. The work describes Harthacnut's horror at hearing of his brother's murder, and in Howard's view, was probably influential in finally persuading the cautious Harthacnut to invade England. According to a later edition of the "Encomium", the English took the initiative in communicating with Harthacnut in 1039, possibly when they became aware that Harold had not long to live.
King of England.
Harthacnut travelled to England with his mother. The landing at Sandwich on 17 June 1040, "seven days before Midsummer", was a peaceful one, though he had a fleet of 62 warships. Even though he had been invited to take the throne, he was taking no chances and came as a conqueror with an invasion force. The crews had to be rewarded for their service, and to pay them, he levied a geld of more than 21,000 pounds, a huge sum of money that made him unpopular, although it was only a quarter of the amount his father had raised in similar circumstances in 1017–1018.
Harthacnut had been horrified by Harold's murder of Alfred, and his mother demanded vengeance. With the approval of Harold's former councillors, his body was disinterred from its place of honour at Westminster and publicly beheaded. It was disposed of in a sewer, but then retrieved and thrown in the Thames, from which London shipmen rescued it and had it buried in a churchyard. Godwin, the powerful earl of Wessex, had been complicit in the crime as he had handed over Alfred to Harold, and Queen Emma charged him in a trial before Harthacnut and members of his council. The king allowed Godwin to escape punishment by bringing witnesses that he had acted on Harold's orders, but Godwin then gave Harthacnut a ship so richly decorated that it amounted to the wergild that Godwin would have had to pay if he had been found guilty. Bishop Lyfing of Worcester was also charged with complicity in the crime and deprived of his see, but in 1041 he made his peace with Harthacnut and was restored to his position.
The English had become used to the king ruling in council, with the advice of his chief men, but Harthacnut had ruled autocratically in Denmark, and he was not willing to change, particularly as he did not fully trust the leading earls. At first he was successful intimidating his subjects, though less so later in his short reign. He doubled the size of the English fleet from sixteen to thirty-two ships, partly so that he had a force capable of dealing with trouble elsewhere in his empire, and to pay for it he severely increased the rate of taxation. The increase coincided with a poor harvest, causing severe hardship. In 1041 two of his tax gatherers were so harsh in dealing with people in and around Worcester that they rioted and killed the tax gatherers. Harthacnut reacted by imposing a then-legal but very unpopular punishment known as 'harrying'. He ordered his earls to burn the town and kill the population. Very few people were killed, however, as they knew what was coming and fled in all directions.
The earl of Northumbria was Siward, but Earl Eadwulf of Bernicia ruled the northern part in semi-independence, a situation which did not please the autocratic Harthacnut. In 1041 Earl Eadwulf gave offence to the king for an unknown reason but then sought reconciliation. Harthacnut promised him safe conduct but then colluded in his murder by Siward, who became earl of the whole of Northumbria. The crime was widely condemned, and the "Anglo-Saxon Chronicle" described it as "a betrayal" and the king as an "oath-breaker".
Harthacnut was generous to the church. Very few contemporary documents survive, but a royal charter of his transferred land to Bishop Ælfwine of Winchester, and he made several grants to Ramsey Abbey. The 12th-century "Ramsey Chronicle" speaks well of his generosity and of his character.
Harthacnut had suffered from bouts of illness even before he became King of England. He may have suffered from tuberculosis, and he probably knew that he had not long to live. In 1041 he invited his half-brother Edward the Confessor (his mother Emma's son by Æthelred the Unready) back from exile in Normandy and probably made him his heir. He may well have been influenced by Emma, who hoped to keep her power by ensuring that one of her sons was succeeded by another. Harthacnut was unmarried and had no known children.
Death.
On 8 June 1042, Harthacnut attended a wedding in Lambeth. The groom was Tovi the Proud, former standard-bearer to Cnut, and the bride was Gytha, daughter of the courtier Osgod Clapa. Harthacnut presumably consumed large quantities of alcohol. As he was drinking to the health of the bride, he ""died as he stood at his drink, and he suddenly fell to the earth with an awful convulsion; and those who were close by took hold of him, and he spoke no word afterwards..."" The likely cause of death was a stroke, ""brought about by an excessive intake of alcohol"".
Sten Körner noted that the death of Harthacnut could be part of a plot, but did not further explore the notion, though the implication would be that Edward the Confessor was behind this plot. In "The Death of Kings: A Medical History of the Kings and Queens of England" (2000), Clifford Brewer pointed that Edward benefited from the sudden death of Harthacnut and that while Godwin, Earl of Wessex, was the father-in-law to Edward, he had once led an uprising against his son-in-law. He died suddenly after dining with said son-in-law, again pointing suspicion at Edward as the probable culprit behind both deaths. Katherine Holman was certain that Harthacnut was poisoned but felt that the culprit will never be known with certainty due to ""no shortage of discontented candidates."" 
Succession.
The political agreement between Harthacnut and Magnus I of Norway included the appointment of the latter as heir to Harthacnut. At the time, the agreement would have only affected the throne of Denmark. The Heimskringla reports that when Harthacnut died, Magnus extended his claim to England. He reportedly sent a letter to Edward the Confessor, pressing his claim to the English throne and threatening invasion. His own heir, Harald Hardrada, would also press this claim. Both considered themselves legal heirs to Harthacnut. The Fagrskinna contains a scene where Magnus proclaims that ""I will take possession of all the Danish empire or else die in the attempt.""
According to the "Encomium", Edward the Confessor already served as co-ruler of England since 1041. There is an emphasis on Harthacnut, Edward, and Emma serving as a trinity of rulers, in emulation of the Holy Trinity. Edward, by surviving his co-ruler, would be king by default. The Heimskringla depicts Edward portraying himself as brother and legal heir to both Harold Harefoot and Harthacnut, while pointing out that he had already won the support ""of all the people of the country"". Unstated in both is that the marriage of Edward to Edith of Wessex would also support his claim by earning him both the political support of her father Godwin and an additional connection to Cnut. She was a niece to the king. The Fagrskinna has Edward point out that he was the son of Æthelred the Unready and Emma of Normandy, the brother to Edmund Ironside, the stepson of Cnut, the stepbrother of Harold Harefoot, and the half-brother of Harthacnut. In short, he had a much stronger family claim to the throne than Magnus. All the leaders of England had already acknowledged him as their king, and he was consecrated by an archbishop. England was his own heritage. Whether Magnus managed to defeat him in war or not, ""you can never be called king in England, and you will never be granted any allegiance there before you put an end to my life."" This was supposedly enough to cause Magnus to doubt the strength of his own claim.
The marriage agreement between Gunhilda of Denmark (sister of Harthacnut) and Henry III, Holy Roman Emperor would allow descendants of this marriage to claim the throne of Denmark and potentially of England. The marriage, from Henry's perspective, was probably orchestrated to allow the Holy Roman Empire to claim control of Denmark and the western areas of the Baltic Sea. However, Gunhilda had died in 1038 with no known sons. Her only daughter was Beatrice I, Abbess of Quedlinburg, who never married.
Reputation.
Apart from the "Ramsey Chronicle", medieval sources are hostile to Harthacnut. According to the "Anglo-Saxon Chronicle" he "did nothing worthy of a king as long as he ruled." Modern historians are less dismissive. In the view of M. K. Lawson, he had at least two of the requisites of a successful medieval king, he was "both ruthless and feared"; had he not died young, the Norman Conquest might not have happened. Ian Howard praises Harthacnut for keeping peace throughout his empire, benefiting trade and merchants, and ensuring a peaceful succession by inviting Edward to his court as his heir. Had he lived longer, Howard believes, his character might have enabled him to become a successful king like his father.
Henry of Huntingdon (12th century) claimed that Harthacnut ordered for the dining tables of his court to be ""laid four times a day with royal sumptuousness"" which O'Brien says is likely a popular myth. Henry of Huntingdon viewed this detail in the context of the monarch sharing these meals with the members of his household, making Harthacnut more generous than his own contemporaries, who ""through avarice, or as they pretend through disgust, ...set but one meal a day before their dependents"". His account produced the image of Harthacnut as a ""very generous Bon viveur."" Ranulf Higden (14th century) viewed the same detail in a negative light. He claimed that Harthacnut insisted on having two dinners and two suppers per day. His example influenced the English people, who supposedly were to Higden's day gluttonous and extravagant. Higden so claimed that Harthacnut had a lasting effect on the English national character. The association of Harthacnut with gluttony was well-known enough to appear in the novel Ivanhoe (1819) by Walter Scott. The character Cedric comments on his friend Athelstane, whose main character trait is a love for food and drink, that ""The soul of Hardicanute hath taken possession of him, and he hath no pleasure save to fill, to swill, and to call for more.""
The "Knýtlinga saga" treats the death of Harthacnut as the end of an ancient line of kings, and notes that he was the last Danish king to rule over England. But otherwise Harthacnut is treated as a mere footnote in the line of monarchs, while there are many observations on Cnut. Morkinskinna covers Harthacnut's death in some detail, but records next to nothing about his life, suggesting a lack of memorable details on him, presumably due to his short reign.
The "prose Brut chronicle" was an Anglo-Norman work, covering British and English monarchs from Brut (Brutus of Troy) to the death of Henry III in 1272. It was probably written during the reign of Edward I (reigned 1272–1307), though the oldest surviving manuscript dates to 1338. The text often includes notable errors. The original author remains unknown, but there were a number of continuations by different hands, continuing the story to the Battle of Halidon Hill (1333). The material on Harthacnut is largely positive. The author considered both Harold Harefoot and Harthacnut to have been sons of Cnut and Emma of Normandy. He portrays Harold as lacking in chivalry, courtesy, and honour. While Harthacnut was ""...a noble knight and stalwart of body, and he greatly loved knighthood and all virtues."" He praises Harthacnut for his generosity with food and drink, claiming that his table was open ""...for all who wished to come to his court to be richly served with royal dishes"". He concludes by portraying Harthacnut as a loyal son for accepting his mother, Emma, back to court.
Contradictory account of his death.
There is a contradictory account of Harthacnut's death featured in the "Morkinskinna" (13th century). According to this account, Magnus I of Norway (reigned 1034–1047) visited the court of Harthacnut in Denmark, received with all official honours. The two monarchs then argued on a matter of etiquette, on whether the host or the guest should drink first, each man offering the honour to the other. The two eventually agreed that the host should drink first. Then Álfífa (Ælfgifu of Northampton) entered the royal hall, welcoming Magnus. She poured a drink for him. But the guest offered the drink to Harthacnut. He drank from the drinking horn and fell dead, poisoned. Álfífa had thus intended to poison Magnus, but accidentally killed Harthacnut instead. She fled to escape punishment.
The tale is probably fictional in origin, though consistent with the villainous depiction of Ælfgifu in this work. A nearly identical story appears in the "Egils saga", though the three protagonists are different, with Egill Skallagrímsson as the intended victim, with Bárðr of Atley and Gunnhild, Mother of Kings as the would-be poisoners.

</doc>
<doc id="40261" url="https://en.wikipedia.org/wiki?curid=40261" title="Harold Harefoot">
Harold Harefoot

Harold I (c.1016 – 17 March 1040), also known as Harold Harefoot, was King of England from 1035 to 1040. Harold's nickname "Harefoot" is believed to refer to his speed, and the skill of his huntsmanship. Though it is not recorded before the late Middle Ages, it is probably contemporary. 
The son of Cnut the Great and Ælfgifu of Northampton, Harold was elected regent of England, following the death of his father in 1035. He was initially ruling England in place of his brother Harthacnut, who was stuck in Denmark due to a rebellion in Norway, which had ousted their brother Svein. Although Harold had wished to be crowned king since 1035, Æthelnoth, Archbishop of Canterbury, refused to do so. It was not until 1037 that Harold, supported by earl Leofric and many others, was officially proclaimed king. The same year Harold's two step-brothers Edward and Alfred returned to England with a considerable military force, Alfred was captured by earl Godwin who had him seized and delivered to an escort of men loyal to Harefoot. While en-route to Ely he was blinded and soon after died of his wounds.
Harold died in 1040, having ruled just five years, his brother Harthacnut soon returned and took hold of the kingdom peacefully. Harold was originally buried in Westminster but Harthacnut had his body dragged up and thrown into a "fen" (sewer), as well as then thrown into the river Thames, but was after a short time picked up by a fisherman, being immediately taken to the Danes, was honourably buried by them in their cemetery at London.
Paternity.
The "Anglo-Saxon Chronicle" reports that Harold said that he was a son of Cnut the Great and Ælfgifu of Northampton, ""although it was not true"". Florence of Worcester (12th century) elaborates on the subject. Claiming that Ælfgifu wanted to have a son by the king but was unable to, she secretly adopted the newborn children of strangers and pretended to have given birth to them. Harold was reportedly the son of a cobbler, while his brother Svein Knutsson was the illegitimate son of a priest. She deceived Cnut into recognizing both children as his own. Harriet O'Brien doubts that Cnut, the shrewd politician who ""masterminded the bloodless takeover of Norway"" could have been deceived in such a way. She suspects that the tale started out as a popular myth, or intentional defamation presumably tailored by Emma of Normandy, the other wife of Cnut and rival to Ælfgifu.
Harthacnut's reign.
Upon the death of Cnut on 12 November 1035, Harold's younger half-brother Harthacnut, the son of Cnut and his queen Emma of Normandy, was legitimate heir to the thrones of both the Danes and the English. Harthacnut, however, was unable to travel to his coronation in England because his Danish kingdom was under threat of invasion by King Magnus I of Norway and King Anund Jacob of Sweden. England's magnates favoured the idea of installing Harold Harefoot temporarily as regent or joint monarch, due to the difficulty of Harthacnut's absence, and despite the opposition of Godwin, the Earl of Wessex, and the Queen, he eventually wore the crown. There is some dispute in primary sources (the "Anglo-Saxon Chronicle") about Harold's initial role. Versions E and F mention him as regent, the others as co-ruler.
Ian Howard points out that Cnut had been survived by three sons: Svein, Harold, and Harthacnut. The "Encomium Emmae Reginae" also describes Edward the Confessor and Alfred Aetheling as the sons of Canute, though the modern term would be step-sons. Harold could claim the regency or kingship because he was the only one of the five present at England in 1035. Harthacnut was reigning in Denmark, and Svein had joined him there following his deposition from the Norwegian throne, while Edward and Alfred were in Normandy. Harold could reign in the name of his absent brothers, with Emma rivaling him as candidate for the regency.
The "Anglo-Saxon Chronicle" ignores the existence of Svein, or his claim to the throne, which Howard considers as evidence of the relative entries being unreliable, of failing to give a complete picture. The Heimskringla of Snorri Sturluson claims that Svein and Harthacnut had agreed to share the kingdom between them. This agreement would include Denmark and (probably) England. Snorri quotes older sources on the subject and could be preserving valuable details.
Reign.
Assumption of the throne.
Harold reportedly sought coronation as early as 1035. According to the "Encomium Emmae Reginae", however, Æthelnoth, Archbishop of Canterbury, refused to crown Harold Harefoot. Coronation by the Archbishop would be a legal requirement to become a king. Æthelnoth reportedly placed the sceptre and crown on the altar of a temple, possibly that of the Canterbury Cathedral. Offering to consecrate Harold without using any of the royal regalia would have been an empty honour. He refused to remove the items from the altar and forbade any other bishop from doing so. The tale goes on that Harold failed to sway Æthelnoth, as both bribes and threats proved ineffectual. The despairing Harold reportedly rejected Christianity in protest. He refused to attend church services while uncrowned, preoccupying himself with hunting and trivial matters.
The "Encomium" stays silent on an event reported by the "Anglo-Saxon Chronicle" and other sources. Harold was accepted as monarch in a Witenagemot held at Oxford. His chief supporter in the council was Leofric, Earl of Mercia, while the opposition was led by Godwin, Earl of Wessex. There is evidence that Ælfgifu of Northampton was attempting to secure her son's position through bribes to the nobles. In 1036, Gunhilda of Denmark, sister to Harthcanut and half-sister to Harold, married Henry III, King of Germany. On this occasion Immo, a priest serving at the court of the Holy Roman Empire, wrote a letter to Azecho, Bishop of Worms. It included information on the situation in England, with messengers from there reporting that Ælfgifu was gaining the support of the leading aristocrats through pleas and bribery, binding them to herself and Harold by oaths of loyalty.
Initially the Kingdom of England was divided between the two half-brothers. Harold ruled the areas north of the River Thames, supported by the local nobility. The southern nobility under Godwin and Emma continued to be ruled in the name of the absent Harthacnut. The "Anglo-Saxon Chronicle" reports that Godwin and the leading men of Wessex opposed the rule of Harold for ""...as long as they could, but they could not do anything against it. "" With the north at least on Harold's side, in adherence to the terms of a deal, which Godwin was part of, Emma was settled in Winchester, with Harthacnut's huscarls. Harold soon "sent and had taken from her all the best treasures" of Cnut the Great
The situation could not last for long, and Godwin eventually switched sides. William of Malmesbury asserts that Godwin had been overwhelmed ""in power and in numbers"" by Harold. In 1037, Emma of Normandy fled to Bruges, Flanders, and Harold "was everywhere chosen as king". The details behind the event are obscure. The account of the "Anglo-Saxon Chronicle", version E, jumps from Harold being a mere regent to Harold being the sole king. Versions C and D do not even make a distinction between the two phases. Ian Howard theorises that the death of Svein Knutsson could have strengthened Harold's position. He went from being the second surviving son of Cnut to being the eldest living, with Harthacnut still absent and unable to press his claim to the throne.
Harold himself is somewhat obscure; the historian Frank Stenton considered it probable that his mother Ælfgifu was "the real ruler of England" for part or all of his reign. Kelly DeVries points that during the High Middle Ages, royal succession in Northern Europe was determined by military power. The eldest son of a king could have a superior right of inheritance but still lose the throne to a younger brother, or other junior claimant, possessing greater military support. Harold managed to win the throne against the superior claim of Harthacnut in this way. The 11th century provides other similar examples. Magnus I of Norway (reigned 1035–1047), who wasn't a warlord, had reigned for more than a decade when his uncle Harald Hardrada (reigned 1047–1066) challenged his rule. With Harald being a famous military leader, his claim would end Magnus' reign early. Baldwin VI, Count of Flanders (reigned 1067–1070) was effectively succeeded by his brother Robert I (reigned 1071–1093), rather than his own sons. Robert Curthose, Duke of Normandy (reigned 1087–1106) lost the throne of England to his younger brothers William II (reigned 1087–1100) and Henry I (reigned 1100–1135).
With the Kingdom of England practically owned by Harold, Harthacnut could not even approach without securing sufficient military strength. His decision to remain in Denmark probably points to him lacking sufficient support, though he would certainly wait for an opportunity to forcefully assert his claim and depose his half-brother. Harold reigned as sole king from 1037 to 1040. There are few surviving documents about events of his reign. The "Anglo-Saxon Chronicle" mostly covers church matters, such as the deaths and appointments of bishops and archbishops. There is, however, record of a skirmish between the Anglo-Saxons and the Welsh in 1039. The named casualties were Eadwine (Edwin), brother to Leofric, Earl of Mercia, Thurkil, and Ælfgeat. But there are no other details concerning this event. Also in 1039, there is mention of a great gale, again with no details.
Invasion by Ælfred and Edward.
In 1036, Ælfred Ætheling, son of Emma by the long-dead Æthelred, returned to the kingdom from exile in the Duchy of Normandy with his brother Edward the Confessor, with some show of arms. Their motivation is uncertain. William of Poitiers claimed that they had come to claim the English throne for themselves. Frank Barlow suspected that Emma had invited them, possibly to use them against Harold. If so, it could mean that Emma had abandoned the cause of Harthacnut, probably to strengthen her own position. But that could have inspired Godwin to also abandon the lost cause.
The "Encomium Emmae Reginae" claims that Harold himself had lured them to England, having sent them a forged letter, supposedly written by Emma. The letter reportedly both decried Harold's behaviour against her, and urged her estranged sons to come and protect her. Barlow and other modern historians suspect that this letter was genuine. Ian Howard argued that Emma not being involved in a major political manoeuvre would be ""out of character for her"", and the Encomium was probably trying to mask her responsibility for a blunder. William of Jumièges reports that earlier in 1036, Edward had conducted a successful raid of Southampton, managing to win a victory against the troops defending the city and then sailing back to Normandy ""richly laden with booty"". But the swift retreat confirms William's assessment that Edward would need a larger army to seriously claim the throne.
With his bodyguard, according to the "Anglo-Saxon Chronicle", Ælfred intended to visit his mother, Emma, in Winchester, but he may have made this journey for reasons other than a family reunion. As the "murmur was very much in favour of Harold", on the direction of Godwin (now apparently on the side of Harold Harefoot), Ælfred was captured. Godwin had him seized and delivered to an escort of men loyal to Harefoot. He was transported by ship to Ely, blinded while on board. He died in Ely soon after due to the severity of the wounds, his bodyguard similarly treated. The event would later affect the relationship between Edward and Godwin, the Confessor holding Godwin responsible for the death of his brother.
The failed invasion shows that Harold Harefoot, as a son and successor to Cnut, had gained the support of Anglo-Danish nobility, which violently rejected the claims of Ælfred, Edward, and (by extension) the Aethelings. The House of Wessex had lost support among the nobility of the Kingdom. It might also have served as a turning point in the struggle between Harold and Emma that resulted in Emma's exile.
Death.
Harold died at Oxford on 17 March 1040 at the relatively young age of 24, just as Harthacnut was preparing an invasion force of Danes, and was buried at Westminster Abbey. His body was subsequently exhumed, beheaded, and thrown into a fen bordering the Thames when Harthacnut assumed the throne in June 1040. The body was subsequently recovered by fishermen, and resident Danes reportly had it reburied at their local cemetery in London. The body was eventually buried in a church in the City of Westminster, which was fittingly named St. Clement Danes. A contradictory account in the "Knýtlinga saga" (13th century) reports Harold buried in the city of Morstr, alongside his half-brother Harthacnut and their father Cnut. While mentioned as a great city in the text, nothing else is known of Morstr. The Heimskringla by Snorri Sturluson reports Harold Harefoot buried at Winchester, again alongside Cnut and Harthacnut.
The cause of Harold's death is uncertain. Katherine Holman attributes the death to ""a mysterious illness"". An Anglo-Saxon charter attributes the illness to divine judgment. Harold had reportedly claimed Sandwich for himself, thereby depriving the monks of Christchurch. Harold is described as lying ill and in despair at Oxford. When monks came to him to settle the dispute over Sandwich, he ""lay and grew black as they spoke"". The context of the event was a dispute between Christchurch and St Augustine's Abbey, which took over the local toll in the name of the king. There is little attention paid to the illness of the king. Harriet O'Brien feels this is enough to indicate that Harold died of natural causes, but not to determine the nature of the disease. The Anglo-Saxons themselves would consider him elf-shot (attacked by elves), their term for any number of deadly diseases. Michael Evans points out that Harold was only one of several youthful kings of pre-Conquest England to die following short reigns. Others included Edmund I (reigned 939–946), Eadred (reigned 946–955), Eadwig (reigned 955–959), Edmund Ironside (reigned 1016), and Harthacnut (reigned 1040–1042). Evans wonders whether the role of king was dangerous in this era, more so than in the period after the Conquest, or whether hereditary diseases were in effect, since most of these kings were members of the same lineage, the House of Wessex.
It is unclear why a king would have been buried at the Abbey. The only previous royals reportedly buried there were Sæberht of Essex and his wife Æthelgoda. Emma Mason speculates that Cnut had built a royal residence in the vicinity of the Abbey, or that Westminster held some significance to the Danish Kings of England, which would also explain why Harthacnut would not allow a usurper to be buried there. The lack of detail in the "Anglo-Saxon Chronicle" implies that, for its compilers, the main point of interest was not the burial site, but the exhumation of the body. Harriet O'Brien theorises that the choice of location might simply reflect the political affiliation of the area, the area of Westminster and nearby London being a power base for Harold.
A detailed account of the exhumation appears in the writings of John of Worcester (12th century). The group tasked with the mission was reportedly led by Ælfric Puttoc, Archbishop of York, and Godwin, Earl of Wessex. The involvement of such notable men would have had a significance of its own, giving the event an official nature and avoiding secrecy. Emma Mason suspects that this could also serve as a punishment for Godwine, who had served as a chief supporter of Harold, and was now tasked with the gruesome task.
Offspring.
Harold may have had a wife, Ælfgifu, and a son, Ælfwine, who became a monk on the continent when he was older – his monastic name was Alboin. Ælfwine/Alboin is recorded in 1060 and 1062 in charters from the Abbey Church of Saint Foy in Conques, which mention him as son of ""Heroldus rex fuit Anglorum"" (Latin: Harold, who was king of the English People). Harold Harefoot is the most likely father as the only other king Harold was Harold Godwinson, who would not rise to the throne until 1066. Either way, an underage boy would be unable to claim the throne in 1040. His possible hereditary claims would not be enough to gain the support of the leading nobles against the adult Harthacnut.
Ælfgifu of Northampton disappears with no trace after 1040. According to the "Anglo-Saxon Chronicle", Harold Harefoot ruled for four years and sixteen weeks, by which calculation he would have begun ruling two weeks after the death of Cnut.
Reputation.
The "Prose Brut chronicle" was an Anglo-Norman work, covering British and English monarchs from Brut (Brutus of Troy) to the death of Henry III in 1272. It was probably written during the reign of Edward I (reigned 1272–1307), though the oldest surviving manuscript dates to 1338. The text often includes notable errors. The original author remains unknown, but there were a number of continuations by different hands, extending the story to the Battle of Halidon Hill (1333). The material on Harold Harefoot is rather unflattering. The author considered both Harold and Harthacnut to have been sons of Cnut and Emma of Normandy. He proceeds to portray Harold as follows: ""...He went astray from the qualities and conduct of his father King Cnut, for he cared not at all for knighthood, for courtesy, or for honour, but only for his own will..."". He accuses Harold of driving his own mother Emma out of England, by the advice of Godwin, Earl of Wessex. He paints Harthacnut in a more favorable light.
The "Knýtlinga saga" (13th century) considers Harold Harefoot to be the oldest son of Cnut and Emma of Normandy, though its author frequently misrepresents family relationships. Harthacnut and Gunhilda of Denmark are regarded in the text as his younger siblings. The narrative has Harold and Harthacnut dividing the realms of their father in an agreement. It also features Harold offering hospitality to his half-brother Edward the Confessor, but they were actually step-brothers, and Edward only settled in England following the death of Harold.

</doc>
<doc id="40270" url="https://en.wikipedia.org/wiki?curid=40270" title="Vibrator">
Vibrator

Vibrator may refer to:

</doc>
<doc id="40272" url="https://en.wikipedia.org/wiki?curid=40272" title="Industrial sociology">
Industrial sociology

Industrial sociology, until recently a crucial research area within the field of sociology of work, examines "the direction and implications of trends in technological change, globalization, labour markets, work organization, managerial practices and employment relations to the extent to which these trends are intimately related to changing patterns of inequality in modern societies and to the changing experiences of individuals and families the ways in which workers challenge, resist and make their own contributions to the patterning of work and shaping of work institutions."
Labor process theory.
One branch of industrial sociology is Labor process theory (LPT). In 1974, Harry Braverman wrote "", which provided a critical analysis of scientific management. This book analyzed capitalist productive relations from a Marxist perspective. Following Marx, Braverman argued that work within capitalist organisations was exploitative and alienating, and therefore workers had to be coerced into servitude. For Braverman the pursuit of capitalist interests over time ultimately leads to deskilling and routinisation of the worker. The Taylorist work design is the ultimate embodiment of this tendency.
Braverman demonstrated several mechanisms of control in both the factory blue collar and clerical white collar labor force.
His key contribution is his "deskilling" thesis. Braverman argued that capitalist owners and managers were incessantly driven to deskill the labor force to lower production costs and ensure higher productivity. Deskilled labour is cheap and above all easy to control due to the workers lack of direct engagement in the production process. In turn work becomes intellectually or emotionally unfulfilling; the lack of capitalist reliance on human skill reduces the need of employers to reward workers in anything but a minimal economic way.
Braverman's contribution to the sociology of work and industry (i.e., industrial sociology) has been important and his theories of the labor process continue to inform teaching and research. Braverman's thesis has however been contested, notably by Andrew Freidman in his work "Industry and Labour" (1977). In it, Freidman suggests that whilst the direct control of labour is beneficial for the capitalist under certain circumstances, a degree of 'responsible autonomy' can be granted to unionised or 'core' workers, in order to harness their skill under controlled conditions. Also, Richard Edwards showed in 1979 that although hierarchy in organisations has remained constant, additional forms of control (such as technical control via email monitoring, call monitoring; bureaucratic control via procedures for leave, sickness etc.) has been added to gain the interests of the capitalist class versus the workers. Gallie has shown how important it is to approach the question of skill from a social class perspective. In his study, the majority of non-manual, intermediate and skilled manual workers believed that their work had come to demand a higher level of skill, but the majority of manual worker felt that the responsibility and skill needed in their work had either remained constant or declined. This means that Braverman's claims can't be applied to all social classes.
The notion the particular type of technology workers were exposed to shapes their experience was most forceufully argued in a classic study by Blauner. He argued that some work is alienating more than other types because of the different technologies workers use. Alienation, to Blauner, has four dimensions: powerlessness, meaninglessness, isolation, and self-estrangement. Individuals are powerless when they can't control their own actions or conditions of work; work is meaningless when it gives employees little or no sense of value, interest or worth; work is isolating when workers cannot identify with their workplace; and work is self-estranging when, at the subjective level, the worker has no sense of involvement in the job.
Blauner's claims however fail to recognize that the same technology can be experienced in a variety of ways. Studies have shown that cultural differences with regard to management-union relations, levels of hieararchical control, and reward and performance appraisal policies mean that the experience of the same kind of work can vary considerably between countries and firms. The individualization of work and the need for workers to have more flexible skills in order to respond to technological changes means that Blauner's characterization of work experience is no longer valid. Additionally, workers today may work in teams to alleviate workers' sense of alienation, since they are involved in the entire process, rather than just a small part of it. In conclusion, automative technologies and comupeterized work systems have typically enhanced workers' job satisfaction and skill deployment in the better-paid, secure public and private sector jobs. But, in more non-skilled manual work, they have just perpetuated job dissatisfaction, especially for the many women involved in this type of work.

</doc>
<doc id="40275" url="https://en.wikipedia.org/wiki?curid=40275" title="Kayaking">
Kayaking

Kayaking is the use of a kayak for moving across water. It is distinguished from canoeing by the sitting position of the paddler and the number of blades on the paddle. A kayak is a low-to-the-water, canoe-like boat in which the paddler sits facing forward, legs in front, using a double-bladed paddle to pull front-to-back on one side and then the other in rotation. Most kayaks have closed decks, although sit-on-top and inflatable kayaks are growing in popularity as well.
History.
Kayaks were created thousands of years ago by the Inuit, formerly known as Eskimos, of the northern Arctic regions. They used driftwood and sometimes the skeleton of whale, to construct the frame of the kayak, and animal skin, particularly seal skin was used create the body. The main purpose for creating the kayak, which literally translates to "hunter's boat" was for hunting and fishing. The kayak's stealth capabilities, allowed for the hunter to sneak up behind animals on the shoreline, and successfully catch their prey. By the mid-1800s the kayak became increasingly popular and the Europeans became interested. German and French men began kayaking for sport. In 1931, a man named Adolf Anderle became the first person to kayak down the Salzachofen Gorge, this is where the birthplace of modern day white-water kayaking is believed to have begun. Kayak races were introduced in the Berlin Olympic Games in 1936. In the 1950s fiberglass kayaks were developed and commonly used, until 1980s when polyethylene plastic kayaks came about. Kayaking progressed as a fringe sport in the U.S. until the 1970s, when it became a mainstream popular sport. Now, more than 10 white water kayaking events are featured in the Olympics.
Design.
Kayaks can also be classified by their design and the materials from which they are made. Each design has its specific advantage, including performance, manoeuvrability, stability and paddling style. Kayaks can be made of metal, fibreglass, wood, plastic, fabrics, and inflatable fabrics such as PVC or rubber, and more recently expensive but feather light carbon fiber. Each material also has its specific advantage, including strength, durability, portability, flexibility, resistance to ultraviolet and storage requirements. For example, wooden kayaks can be created from kits or built by hand. Stitch and glue, plywood kayaks can be lighter than any other material except skin-on frame. Inflatable kayaks, made from lightweight fabric, can be deflated and easily transported and stored, and considered to be remarkably tough and durable compared to some hard-sided boats.
Equipment.
There are many types of kayaks used in flat water and white water kayaking. The sizes and shapes vary drastically depending on what type of water to be paddled on and also what the paddler would like to do. The second set of essentials for kayaking is an off-set paddle where the paddle blades are tilted to help reduce wind resistance while the other blade is being used in the water. These vary in length and also shape depending on the intended use, height of the paddler and often, the paddler's preference. Kayaks should be equipped with one or more buoyancy aid (also called flotation) which creates air space that helps prevent a kayak from sinking when filled with water; life jacket should be worn at all times (also called a personal flotation device or PFD), a helmet is also often required for most kayaking and is mandatory for white water kayaking. Various other pieces of safety gear include: a whistle for signaling for help; throwing ropes to help rescue other kayakers; and, a diving knife and appropriate water shoes should used depending upon the risks the water and terrain pose. Proper clothing such as a dry suit, wet suit or spray top also help protect kayakers from cold water or air temperatures.
Types of kayaks.
"Sit on top" kayaks place the paddler in an open, shallowly-concave deck above the water level. "Cockpit style" involves sitting with the legs and hips inside the kayak hull with a spray deck or "spray skirt" that creates a water resistant seal around the waist. "Inflatables" are a hybrid of the two previous configurations; these boats have an open deck, but the paddler sits below the level of the deck. "Tandems" are configured for multiple paddlers, in contrast to the single person designs featured by most kayaks. Tandems can be used by two or even three paddlers.
Activities involving kayaks.
Because of their range and adaptability, kayaks can be useful for other outdoor activities such as diving, fishing, wilderness exploration and search and rescue during floods.
Diving.
Kayak diving is a type of recreational diving where the divers paddle to a diving site in a kayak carrying all their gear to the place they want to dive. The range can be up to several kilometres along the coastline from the launching point to a place where access would be difficult from the shore, although the sea is sheltered. It is a considerably cheaper alternative to using a powered boat, as well as combining the experience of sea kayaking at the same time.
Kayak diving gives the diver independence from dive boat operators, while allowing dives at sites which are too far to comfortably swim, but are sufficiently sheltered.
Fishing.
Kayak fishing is fishing from a kayak. The "kayak" has long been a means of transportation and a stealth means of approaching easily spooked fish, such as cobia and flounder. Kayak fishing has gained popularity in recent times due to its broad appeal as an environmentally friendly and healthy method of transportation, as well as its relatively low cost of entry compared to motorized boats.
Burnley, Ric (2007) "The Complete Kayak Fisherman." Burford Books. ISBN 978-1-58080-147-8;
Daubert, Ken (2001) "Kayakfishing : The Revolution." Coelacanth Pubns. ISBN 978-0-9678098-2-3;
Null, Scott and Mcbride, Joel (2009) "Kayak Fishing: The Ultimate Guide 2nd Edition." Heliconia Press. ISBN 978-1-896980-43-0</ref>

</doc>
<doc id="40276" url="https://en.wikipedia.org/wiki?curid=40276" title="Blackboard bold">
Blackboard bold

Blackboard bold is a typeface style that is often used for certain symbols in mathematical texts, in which certain lines of the symbol (usually vertical or near-vertical lines) are doubled. The symbols usually denote number sets. One way of producing Blackboard bold is to double-strike a character with a small offset on a typewriter. Thus they are also referred to as double struck.
Origin.
In some texts these symbols are simply shown in bold type: blackboard bold in fact originated from the attempt to write bold letters on blackboards in a way that clearly differentiated them from non-bold letters i.e. by using the edge rather than point of the chalk. It then made its way back in print form as a separate style from ordinary bold, possibly starting with the original 1965 edition of Gunning and Rossi's textbook on complex analysis. 
Rejection.
Some mathematicians, therefore, do not recognize blackboard bold as a separate style from bold: Jean-Pierre Serre, for example, uses double-struck letters when writing bold on the blackboard, whereas his published works consistently use ordinary bold for the same symbols. Donald Knuth also prefers boldface to blackboard bold, and consequently did not include blackboard bold in the Computer Modern fonts he created for the TeX mathematical typesetting system.
The Chicago Manual of Style in 1993 (14th edition) advises: "blackboard bold should be confined to the classroom" (13.14) whereas in 2003 (15th edition) it states that "open-faced (blackboard) symbols are reserved for familiar systems of numbers" (14.12).
Encoding.
TeX, the standard typesetting system for mathematical texts, does not contain direct support for blackboard bold symbols, but the add-on AMS Fonts package (codice_1) by the American Mathematical Society provides this facility; a blackboard bold R is written as codice_2. The codice_3 package loads codice_1.
In Unicode, a few of the more common blackboard bold characters (C, H, N, P, Q, R and Z) are encoded in the Basic Multilingual Plane (BMP) in the "Letterlike Symbols (2100–214F)" area, named DOUBLE-STRUCK CAPITAL C etc. The rest, however, are encoded outside the BMP, from codice_5 to codice_6 (uppercase, excluding those encoded in the BMP), codice_7 to codice_8 (lowercase) and codice_9 to codice_10 (digits). Being outside the BMP, these are relatively new and not widely supported.
Usage.
The following table shows all available Unicode blackboard bold characters.
The symbols are nearly universal in their interpretation, unlike their normally-typeset counterparts, which are used for many different purposes.
The first column shows the letter as typically rendered by the ubiquitous LaTeX markup system. The second column shows the Unicode codepoint. The third column shows the symbol itself (which will only display correctly on browsers that support Unicode and have access to a suitable font). The fourth column describes known typical (but not universal) usage in mathematical texts.
In addition, a blackboard-bold Greek letter mu (not found in Unicode) is sometimes used by number theorists and algebraic geometers (with a subscript "n") to designate the group (or more specifically group scheme) of "n"-th roots of unity.

</doc>
<doc id="40277" url="https://en.wikipedia.org/wiki?curid=40277" title="Corpus linguistics">
Corpus linguistics

Corpus linguistics is the study of language as expressed in "corpora" (samples) of "real world" text. The text-corpus method is a digestive approach for deriving a set of abstract rules, from a text, for governing a natural language, and how that language relates to and with another language; originally derived manually, corpora now are automatically derived from the source texts. Corpus linguistics proposes that reliable language analysis is more feasible with corpora collected in the field, in their natural contexts, and with minimal experimental-interference.
The field of Corpus Linguistics features divergent views about the value of corpus annotation, ranging from John McHardy Sinclair, who advocates minimal annotation, and so allow texts to speak for themselves; to the Survey of English Usage team (University College, London) who advocate annotation as allowing greater linguistic understanding, by way of rigorous recording.
History.
Some of the earliest efforts at grammatical description were based at least in part on corpora of particular religious or cultural significance. For example, Prātiśākhya literature described the sound patterns of Sanskrit as found in the Vedas, and 
Pāṇini's grammar of classical Sanskrit was based at least in part on analysis of that same corpus. Similarly, the early Arabic grammarians paid particular attention to the language of the Quran. In the Western European tradition, scholars prepared concordances to allow detailed study of the language of the Bible and other canonical texts.
A landmark in modern corpus linguistics was the publication by Henry Kučera and W. Nelson Francis of "Computational Analysis of Present-Day American English" in 1967, a work based on the analysis of the Brown Corpus, a carefully compiled selection of current American English, totalling about a million words drawn from a wide variety of sources. Kučera and Francis subjected it to a variety of computational analyses, from which they compiled a rich and variegated opus, combining elements of linguistics, language teaching, psychology, statistics, and sociology. A further key publication was Randolph Quirk's 'Towards a description of English Usage' (1960) in which he introduced The Survey of English Usage.
Shortly thereafter, Boston publisher Houghton-Mifflin approached Kučera to supply a million-word, three-line citation base for its new "American Heritage Dictionary", the first dictionary to be compiled using corpus linguistics. The AHD took the innovative step of combining prescriptive elements (how language "should" be used) with descriptive information (how it actually "is" used).
Other publishers followed suit. The British publisher Collins' COBUILD monolingual learner's dictionary, designed for users learning English as a foreign language, was compiled using the Bank of English. The Survey of English Usage Corpus was used in the development of one of the most important Corpus-based Grammars, the "Comprehensive Grammar of English" (Quirk "et al." 1985).
The Brown Corpus has also spawned a number of similarly structured corpora: the LOB Corpus (1960s British English), Kolhapur (Indian English), Wellington (New Zealand English), Australian Corpus of English (Australian English), the Frown Corpus (early 1990s American English), and the FLOB Corpus (1990s British English). Other corpora represent many languages, varieties and modes, and include the International Corpus of English, and the British National Corpus, a 100 million word collection of a range of spoken and written texts, created in the 1990s by a consortium of publishers, universities (Oxford and Lancaster) and the British Library. For contemporary American English, work has stalled on the American National Corpus, but the 400+ million word Corpus of Contemporary American English (1990–present) is now available through a web interface.
The first computerized corpus of transcribed spoken language was constructed in 1971 by the Montreal French Project, containing one million words, which inspired Shana Poplack's much larger corpus of spoken French in the Ottawa-Hull area.
Besides these corpora of living languages, computerized corpora have also been made of collections of texts in ancient languages. An example is the Andersen-Forbes database of the Hebrew Bible, developed since the 1970s, in which every clause is parsed using graphs representing up to seven levels of syntax, and every segment tagged with seven fields of information. The Quranic Arabic Corpus is an annotated corpus for the Classical Arabic language of the Quran. This is a recent project with multiple layers of annotation including morphological segmentation, part-of-speech tagging, and syntactic analysis using dependency grammar.
Methods.
Corpus Linguistics has generated a number of research methods, attempting to trace a path from data to theory. Wallis and Nelson (2001) first introduced what they called the 3A perspective: Annotation, Abstraction and Analysis.
Most lexical corpora today are part-of-speech-tagged (POS-tagged). However even corpus linguists who work with 'unannotated plain text' inevitably apply some method to isolate salient terms. In such situations annotation and abstraction are combined in a lexical search.
The advantage of publishing an annotated corpus is that other users can then perform experiments on the corpus (through corpus managers). Linguists with other interests and differing perspectives than the originators' can exploit this work. By sharing data, corpus linguists are able to treat the corpus as a locus of linguistic debate, rather than as an exhaustive fount of knowledge.
Recent studies have suggested treatment outcome in adolescents with social anxiety disorder can also be assessed by analysing language by means of Corpus Linguistics 
Notes and references.
Journals.
There are several international peer-reviewed journals dedicated to corpus linguistics, for example,
Corpora,
Corpus Linguistics and Linguistic Theory,
ICAME Journal and the
International Journal of Corpus Linguistics.
Book series.
Book series in this field include
Language and Computers,
Studies in Corpus Linguistics and English Corpus Linguistics

</doc>
<doc id="40282" url="https://en.wikipedia.org/wiki?curid=40282" title="Type theory">
Type theory

In mathematics, logic, and computer science, a type theory is any of a class of formal systems, some of which can serve as alternatives to set theory as a foundation for all mathematics. In type theory, every "term" has a "type" and operations are restricted to terms of a certain type.
Type theory is closely related to (and in some cases overlaps with) type systems, which are a programming language feature used to reduce bugs. The types of type theory were created to avoid paradoxes in a variety of formal logics and rewrite systems and sometimes "type theory" is used to refer to this broader application.
Two well-known type theories that can serve as mathematical foundations are Alonzo Church's typed λ-calculus and Per Martin-Löf's intuitionistic type theory.
History.
Between 1902 and 1908 Bertrand Russell proposed various "theories of type" in response to his discovery that Gottlob Frege's version of naive set theory was afflicted with Russell's paradox. By 1908 Russell arrived at a "ramified" theory of types together with an "axiom of reducibility" both of which featured prominently in Whitehead and Russell's "Principia Mathematica" published between 1910 and 1913. They attempt to avoid Russell's paradox by first creating a hierarchy of types, then assigning each mathematical (and possibly other) entity to a type. Objects of a given type are built exclusively from objects of preceding types (those lower in the hierarchy), thus preventing loops. In the 1920s, Leon Chwistek and Frank P. Ramsey proposed a simpler theory, now known as the "theory of simple types" or "simple type theory", that collapsed the complicated hierarchy of the "ramified theory" and did not require the axiom of reducibility.
The common usage of "type theory" is when those types are used with a term rewrite system. The most famous early example is Alonzo Church's lambda calculus. Church's theory of types helped the formal system avoid the Kleene–Rosser paradox that afflicted the original untyped lambda calculus. Church demonstrated that it could serve as a foundation of mathematics and it was referred to as a higher-order logic.
Some other type theories include Per Martin-Löf's intuitionistic type theory, which has been the foundation used in some areas of constructive mathematics and for the proof assistant Agda. Thierry Coquand's calculus of constructions and its derivatives are the foundation used by Coq and others. The field is an area of active research, as demonstrated by homotopy type theory.
Basic concepts.
In a system of type theory, each term has a type and operations are restricted to terms of a certain type. A typing judgment formula_1 describes that the term formula_2 has type formula_3. For example, formula_4 may be a type representing the natural numbers and formula_5 may be inhabitants of that type. The judgement that formula_6 has type formula_4 is written as formula_8.
A function in type theory is denoted with an arrow formula_9. The function formula_10 (commonly called successor), has the judgement formula_11. Calling or "applying" a function to an argument is usually written without parentheses, so formula_12 instead of formula_13. (This allows for consistent currying.)
Type theories also contain rules for rewriting terms. These are called conversion rules or, if the rule only works in one direction, a reduction rule. For example, formula_14 and formula_15 are syntactically different terms, but the first reduces to the latter. This reduction is denoted as formula_16.
Difference from set theory.
There are many different set theories and many different systems of type theory, so what follows are generalizations.
Optional features.
Normalization.
The term formula_14 reduces to formula_15. Since formula_15 cannot be reduced further, it is called a normal form. A system of type theory is said to be strongly normalizing if all terms have a normal form and any order of reductions reaches it. Weakly normalizing systems have a normal form but some orders of reductions may loop forever and never reach it.
For a normalizing system, some borrow the word element from set theory and use it to refer to all closed terms that can reduce to the same normal form. A closed term is one without parameters. (A term like formula_20 with its parameter formula_21 is called an open term.) Thus, formula_14 and formula_23 may be different terms but they're both from the element formula_15.
A similar idea that works for open and closed terms is convertibility. Two terms are convertible if there exists a term that they both reduce to. For example, formula_14 and formula_26 are convertible. As are formula_27 and formula_28. However, formula_20 and formula_30 (where formula_21 is a free variable) are not because both are in normal form and they are not the same. Confluent and weakly normalizing systems can test if two terms are convertible by checking if they both reduce to the same normal form.
Dependent types.
A dependent type is a type that depends on a term or on another type. Thus, the type returned by a function may depend upon the argument to the function.
For example, a list of formula_4s of length 4 may be a different type than a list of formula_4s of length 5. In a type theory with dependent types, it is possible to define a function that take a parameter "n" and returns a list containing "n" zeros. Calling the function with 4 would produce a term with a different type than if the function was called with 5.
Dependent types play a central role in intuitionistic type theory and in the design of functional programming languages like Idris, ATS, Agda and Epigram.
Equality types (or "identity types").
Many systems of type theory have a type that represents equality of types and terms. This type is different from convertibility, and is often denoted propositional equality.
In intuitionistic type theory, the equality type is known as formula_34 for identity. There is a type formula_35 when formula_3 is a type and formula_37 and formula_38 are both terms of type formula_3. A term of type formula_35 is interpreted as meaning that formula_37 is equal to formula_38.
In practice, it is possible to build a type formula_43 but there will not exist a term of that type. In intuitionistic type theory, new terms of equality start with reflexivity. If formula_15 is a term of type formula_4, then there exists a term of type formula_46. More complicated equalities can be created by creating a reflexive term and then doing a reduction on one side. So if formula_47 is a term of type formula_4, then there is a term of type formula_49 and, by reduction, generate a term of type formula_50. Thus, in this system, the equality type denotes that two values of the same type are convertible by reductions.
Having a type for equality is important because it can be manipulated inside the system. There is usually no judgement to say two terms are "not" equal; instead, as in the Brouwer–Heyting–Kolmogorov interpretation, we map formula_51 to formula_52, where formula_53 is the bottom type having no values. There exists a term with type formula_54, but not one of type formula_55.
Homotopy type theory differs from intuitionistic type theory mostly by its handling of the equality type.
Inductive types.
A system of type theory requires some basic terms and types to operate on. Some systems build them out of functions using Church encoding. Other systems have inductive types: a set of base types and a set of type constructors that generate types with well-behaved properties. For example, certain recursive functions called on inductive types are guaranteed to terminate.
Coinductive type are infinite data types created by giving a function that generates the next element(s). See Coinduction and Corecursion.
Induction induction is a feature for declaring an inductive type and a family of types that depends on the inductive type.
Induction recursion allows a wider range of well-behaved types but requires that the type and the recursive functions that operate on them be defined at the same time.
Universe types.
Types were created to prevent paradoxes, such as Russell's paradox. However, the motives that lead to those paradoxes—being able to say things about all types—still exist. So many type theories have a "universe type", which contains all "other" types (and not itself).
In systems where you might want to say something about universe types, there is a hierarchy of universe types, each containing the one below it in the hierarchy. The hierarchy is defined as being infinite, but statements must only refer to a finite number of universe levels.
Type universes are particularly tricky in type theory. The initial proposal of intuitionistic type theory suffered from Girard's paradox.
Computational component.
Many systems of type theory, such as the simply-typed lambda calculus, intuitionistic type theory, and the calculus of constructions, are also programming languages. That is, they are said to have a "computational component". The computation is the reduction of terms of the language using rewriting rules.
A system of type theory that has a well-behaved computational component also has a simple connection to constructive mathematics through the BHK interpretation.
Non-constructive mathematics in these systems is possible by adding operators on continuations such as call with current continuation. However, these operators tend to break desirable properties such as canonicity and parametricity.
Practical impact.
Programming languages.
There is extensive overlap and interaction between the fields of type theory and type systems. Type systems are a programming language feature designed to identify bugs. Any static program analysis, such as the type checking algorithms in the semantic analysis phase of compiler, has a connection to type theory.
A prime example is Agda, a programming language which uses intuitionistic type theory for its type system. The programming language ML was developed for manipulating type theories (see LCF) and its own type system was heavily influenced by them.
Mathematical foundations.
The first computer proof assistant, called Automath, used type theory to encode mathematics on a computer. Martin-Löf specifically developed intuitionistic type theory to encode "all" mathematics to serve as a new foundation for mathematics. There is current research into mathematical foundations using homotopy type theory.
Mathematicians working in category theory already had difficulty working with the widely accepted foundation of Zermelo–Fraenkel set theory. This led to proposals such as Lawvere's Elementary Theory of the Category of Sets (ETCS). Homotopy type theory continues in this line using type theory. Researchers are exploring connections between dependent types (especially the identity type) and algebraic topology (specifically homotopy).
Proof assistants.
Much of the current research into type theory is driven by proof checkers, interactive proof assistants, and automated theorem provers. Most of these systems use a type theory as the mathematical foundation for encoding proofs, which is not surprising, given the close connection between type theory and programming languages:
Multiple type theories are supported by LEGO and Isabelle. Isabelle also supports foundations besides type theories, such as ZFC. Mizar is an example of a proof system that only supports set theory.
Linguistics.
Type theory is also widely in use in formal theories of semantics of natural languages, especially Montague grammar and its descendants. In particular, categorial grammars and pregroup grammars make extensive use of type constructors to define the types ("noun", "verb", etc.) of words.
The most common construction takes the basic types formula_56 and formula_57 for individuals and truth-values, respectively, and defines the set of types recursively as follows:
A complex type formula_60 is the type of functions from entities of type formula_37 to entities of type formula_38. Thus one has types like formula_64 which are interpreted as elements of the set of functions from entities to truth-values, i.e. indicator functions of sets of entities. An expression of type formula_65 is a function from sets of entities to truth-values, i.e. a (indicator function of a) set of sets. This latter type is standardly taken to be the type of natural language quantifiers, like " everybody" or " nobody" (Montague 1973, Barwise and Cooper 1981).
Social sciences.
Gregory Bateson introduced a theory of logical types into the social sciences; his notions of double bind and logical levels are based on Russell's theory of types.
Relation to category theory.
Although the initial motivation for category theory was far removed from foundationalism, the two fields turned out to have deep connections. As John Lane Bell writes: "In fact categories can "themselves" be viewed as type theories of a certain kind; this fact alone indicates that type theory is much more closely related to category theory than it is to set theory." In brief, a category can be viewed as a type theory by regarding its objects as types (or sorts), i.e. "Roughly speaking, a category may be thought of as a type theory shorn of its syntax." A number of significant results follow in this way:
The interplay, known as categorical logic, has been a subject of active research since then; see the monograph of Jacobs (1999) for instance.

</doc>
<doc id="40283" url="https://en.wikipedia.org/wiki?curid=40283" title="Melting point">
Melting point

The melting point (or, rarely, liquefaction point) of a solid is the temperature at which it changes state from solid to liquid at atmospheric pressure. At the melting point the solid and liquid phase exist in equilibrium. The melting point of a substance depends on pressure and is usually specified at standard pressure. When considered as the temperature of the reverse change from liquid to solid, it is referred to as the freezing point or crystallization point. Because of the ability of some substances to supercool, the freezing point is not considered as a characteristic property of a substance. When the "characteristic freezing point" of a substance is determined, in fact the actual methodology is almost always "the principle of observing the disappearance rather than the formation of ice", that is, the melting point.
Examples.
For most substances, melting and freezing points are approximately equal. For example, the melting point "and" freezing point of mercury is 234.32 kelvins (−38.83 °C or −37.89 °F). However, certain substances possess differing solid-liquid transition temperatures. For example, agar melts at 85 °C (185 °F) and solidifies from 31 °C to 40 °C (89.6 °F to 104 °F); such direction dependence is known as hysteresis.
The melting point of ice at 1 atmosphere of pressure is very close to 0 °C (32 °F, 273.15 K); this is also known as the ice point. In the presence of nucleating substances the freezing point of water is the same as the melting point, but in the absence of nucleators water can supercool to −42 °C (−43.6 °F, 231 K) before freezing.
The chemical element with the highest melting point is tungsten, at 3687 K (3414 °C, 6177 °F) making it excellent for use as filaments in light bulbs. The often-cited carbon does not melt at ambient pressure but sublimes at about 4000 K; a liquid phase only exists above pressures of 10 MPa and estimated 4300–4700 K (see Carbon phase diagram). Tantalum hafnium carbide (Ta4HfC5) is a refractory compound with a very high melting point of 4215 K (3942 °C, 7128 °F). At the other end of the scale, helium does not freeze at all at normal pressure, even at temperatures very close to absolute zero; pressures over 20 times normal atmospheric pressure are necessary.
Melting point measurements.
Many laboratory techniques exist for the determination of melting points.
A Kofler bench is a metal strip with a temperature gradient (range from room temperature to 300 °C). Any substance can be placed on a section of the strip revealing its thermal behaviour at the temperature at that point. Differential scanning calorimetry gives information on melting point together with its enthalpy of fusion.
A basic melting point apparatus for the analysis of crystalline solids consists of an oil bath with a transparent window (most basic design: a Thiele tube) and a simple magnifier. The several grains of a solid are placed in a thin glass tube and partially immersed in the oil bath. The oil bath is heated (and stirred) and with the aid of the magnifier (and external light source) melting of the individual crystals at a certain temperature can be observed. In large/small devices, the sample is placed in a heating block, and optical detection is automated.
The measurement can also be made continuously with an operating process. For instance, oil refineries measure the freeze point of diesel fuel online, meaning that the sample is taken from the process and measured automatically. This allows for more frequent measurements as the sample does not have to be manually collected and taken to a remote laboratory.
Thermodynamics.
Not only is heat required to raise the temperature of the solid to the melting point, but the melting itself requires heat called the heat of fusion.
From a thermodynamics point of view, at the melting point the change in Gibbs free energy (ΔG) of the material is zero, but the enthalpy ("H") and the entropy ("S") of the material are increasing (ΔH, ΔS > 0). Melting phenomenon happens when the Gibbs free energy of the liquid becomes lower than the solid for that material. At various pressures this happens at a specific temperature. It can also be shown that:
Here "T", "ΔS" and "ΔH" are respectively the temperature at the melting point, change of entropy of melting and the change of enthalpy of melting.
The melting point is sensitive to extremely large changes in pressure, but generally this sensitivity is orders of magnitude less than that for the boiling point, because the solid-liquid transition represents only a small change in volume. If, as observed in most cases, a substance is more dense in the solid than in the liquid state, the melting point will increase with increases in pressure. Otherwise the reverse behavior occurs. Notably, this is the case of water, as illustrated graphically to the right, but also of Si, Ge, Ga, Bi. With extremely large changes in pressure, substantial changes to the melting point are observed. For example, the melting point of silicon at ambient pressure (0.1 MPa) is 1415 °C, but at pressures in excess of 10 GPa it decreases to 1000 °C.
Melting points are often used to characterize organic and inorganic compounds and to ascertain their purity. The melting point of a pure substance is always higher and has a smaller range than the melting point of an impure substance or, more generally, of mixtures. The higher the quantity of other components, the lower the melting point and the broader will be the melting point range, often referred to as the "pasty range". The temperature at which melting begins for a mixture is known as the "solidus" while the temperature where melting is complete is called the "liquidus". Eutectics are special types of mixtures that behave like single phases. They melt sharply at a constant temperature to form a liquid of the same composition. Alternatively, on cooling a liquid with the eutectic composition will solidify as uniformly dispersed, small (fine-grained) mixed crystals with the same composition.
In contrast to crystalline solids, glasses do not possess a melting point;
on heating they undergo a smooth glass transition into a viscous liquid.
Upon further heating, they gradually soften, which can be characterized by certain softening points.
Freezing-point depression.
The freezing point of a solvent is depressed when another compound is added, meaning that a solution has a lower freezing point than a pure solvent. This phenomenon is used in technical applications to avoid freezing, for instance by adding salt or ethylene glycol to water.
Carnelley's Rule.
In organic chemistry Carnelley's Rule, established in 1882 by Thomas Carnelley, stated that "high molecular symmetry is associated with high melting point". Carnelley based his rule on examination of 15,000 chemical compounds. For example, for three structural isomers with molecular formula C5H12 the melting point increases in the series isopentane −160 °C (113 K) n-pentane −129.8 °C (143 K) and neopentane −16.4 °C (256.8 K). Likewise in xylenes and also dichlorobenzenes the melting point increases in the order meta, ortho and then para. Pyridine has a lower symmetry than benzene hence its lower melting point but the melting point again increases with diazine and triazines. Many cage-like compounds like adamantane and cubane with high symmetry have relatively high melting points.
A high melting point results from a high heat of fusion, a low entropy of fusion, or a combination of both. In highly symmetrical molecules the crystal phase is densely packed with many efficient intermolecular interactions resulting in a higher enthalpy change on melting.
Predicting the melting point of substances (Lindemann's criterion).
An attempt to predict the bulk melting point of crystalline materials was first made in 1910 by Frederick Lindemann. The idea behind the theory was the observation that the average amplitude of thermal vibrations increases with increasing temperature. Melting initiates when the amplitude of vibration becomes large enough for adjacent atoms to partly occupy the same space. The Lindemann criterion states that melting is expected when the vibration root mean square amplitude exceeds a threshold value.
Assuming that all atoms in a crystal vibrate with the same frequency "ν", the average thermal energy can be estimated using the equipartition theorem as
where "m" is the atomic mass, "ν" is the frequency, "u" is the average vibration amplitude, kB is the Boltzmann constant, and "T" is the absolute temperature. If the threshold value of "u2" is "c2a2" where "c" is the Lindemann constant and "a" is the atomic spacing, then the melting point is estimated as
Several other expressions for the estimated melting temperature can be obtained depending on the estimate of the average thermal energy. Another commonly used expression for the Lindemann criterion is
From the expression for the Debye frequency for "ν", we have
where θD is the Debye temperature and "h" is the Planck constant. Values of "c" range from 0.15–0.3 for most materials.
Melting point open data.
In February 2011, Alfa Aesar released over 10,000 melting points of compounds from their catalog as open data. These data have been curated and are freely available for download. These data have been used to create a random forest model for melting point prediction which is now available as a free-to-use webservice. Highly curated and open melting point data are also available from Nature Precedings.

</doc>
<doc id="40284" url="https://en.wikipedia.org/wiki?curid=40284" title="Cam">
Cam

A cam is a rotating or sliding piece in a mechanical linkage used especially in transforming rotary motion into linear motion or vice versa. It is often a part of a rotating wheel (e.g. an eccentric wheel) or shaft (e.g. a cylinder with an irregular shape) that strikes a lever at one or more points on its circular path. The cam can be a simple tooth, as is used to deliver pulses of power to a steam hammer, for example, or an eccentric disc or other shape that produces a smooth reciprocating (back and forth) motion in the "follower", which is a lever making contact with the cam.
Overview.
The cam can be seen as a device that rotates from circular to reciprocating (or sometimes oscillating) motion. A common example is the camshaft of an automobile, which takes the rotary motion of the engine and translates it into the reciprocating motion necessary to operate the intake and exhaust valves of the cylinders.
Displacement diagram.
Certain cams can be characterized by their displacement diagrams, which reflect the changing position a roller follower (a shaft with a rotating wheel at the end) would make as the cam rotates about an axis. These diagrams relate angular position, usually in degrees, to the radial displacement experienced at that position. Displacement diagrams are traditionally presented as graphs with non-negative values. A simple displacement diagram illustrates the follower motion at a constant velocity rise followed by a similar return with a dwell in between as depicted in figure 2. The rise is the motion of the follower away from the cam center, dwell is the motion where the follower is at rest, and return is the motion of the follower toward the cam center.
However, the most common type is in the valve actuators in internal combustion engines. Here, the cam profile is commonly symmetric and at rotational speeds generally met with, very high acceleration forces develop. Ideally, a convex curve between the onset and maximum position of lift reduces acceleration, but this requires impractically large shaft diameters relative to lift. Thus, in practice, the points at which lift begins and ends mean that a tangent to the base circle appears on the profile. This is continuous with a tangent to the tip circle. In designing the cam, the lift and the dwell angle formula_1 are given. If the profile is treated as a large base circle and a small tip circle, joined by a common tangent, giving lift formula_2 , the relationship can be calculated, given the angle formula_3 between one tangent and the axis of symmetry (formula_3 being formula_5), while formula_6 is the distance between the centres of the circles (required), and formula_7 is the radius of the base (given) and formula_8 that of the tip circle (required)
formula_9
and
formula_10
Plate cam.
The most commonly used cam is the plate cam (also "disc cam" or "radial cam")
which is cut out of a piece of flat metal or plate. Here, the follower moves in a plane perpendicular to the axis of rotation of the camshaft. Several key terms are relevant in such a construction of plate cams: base circle, prime circle (with radius equal to the sum of the follower radius and the base circle radius), pitch curve which is the radial curve traced out by applying the radial displacements away from the prime circle across all angles, and the lobe separation angle (LSA - the angle between two adjacent intake and exhaust cam lobes).
The base circle is the smallest circle that can be drawn to the cam profile.
A once common, but now outdated, application of this type of cam was automatic machine tool programming cams. Each tool movement or operation was controlled directly by one or more cams. Instructions for producing programming cams and cam generation data for the most common makes of machine were included in engineering references well into the modern CNC era.
This type of cam is used in many simple electromechanical appliance controllers, such as dishwashers and clothes washing machines, to actuate mechanical switches that control the various parts.
Cylindrical cam.
A cylindrical cam or barrel cam is a cam in which the follower rides on the surface of a cylinder. In the most common type, the follower rides in a groove cut into the surface of a cylinder. These cams are principally used to convert rotational motion to linear motion parallel to the rotational axis of the cylinder. A cylinder may have several grooves cut into the surface and drive several followers. Cylindrical cams can provide motions that involve more than a single rotation of the cylinder and generally provide positive positioning, removing the need for a spring or other provision to keep the follower in contact with the control surface.
Applications include machine tool drives, such as reciprocating saws, and shift control barrels in sequential transmissions, such as on most modern motorcycles.
A special case of this cam is constant lead, where the position of the follower is linear with rotation, as in a lead screw. The purpose and detail of implementation influence whether this application is called a cam or a screw thread, but in some cases, the nomenclature may be ambiguous.
Cylindrical cams may also be used to reference an output to two inputs, where one input is rotation of the cylinder, and the second is position of the follower axially along the cam. The output is radial to the cylinder. These were once common for special functions in control systems, such as fire control mechanisms for guns on naval vessels and mechanical analog computers.
An example of a cylindrical cam with two inputs is provided by a duplicating lathe, an example of which is the Klotz axe handle lathe, which cuts an axe handle to a form controlled by a pattern acting as a cam for the lathe mechanism.
Face cam.
A face cam produces motion by using a follower riding on the face of a disk. The most common type has the follower ride in a slot so that the captive follower produces radial motion with positive positioning without the need for a spring or other mechanism to keep the follower in contact with the control surface. A face cam of this type generally has only one slot for a follower on each face. In some applications, a single element, such as a gear, a barrel cam, or other rotating element with a flat face, may do duty as a face cam in addition to other purposes.
Face cams may provide repetitive motion with a groove that forms a closed curve, or may provide function generation with a stopped groove. Cams used for function generation may have grooves that require several revolutions to cover the complete function, and in this case, the function generally needs to be invertible so that the groove does not self intersect, and the function output value must differ enough at corresponding rotations that there is sufficient material separating the adjacent groove segments. A common form is the constant lead cam, where displacement of the follower is linear with rotation, such as the scroll plate in a scroll chuck. Non-invertible functions, which require the groove to self-intersect, can be implemented using special follower designs.
A variant of the face cam provides motion parallel to the axis of cam rotation. A common example is the traditional sash window lock, where the cam is mounted to the top of the lower sash, and the follower is the hook on the upper sash. In this application, the cam is used to provide mechanical advantage in forcing the window shut, and also provides a self-locking action, like some worm gears, due to friction.
Face cams may also be used to reference a single output to two inputs, typically where one input is rotation of the cam and the other is radial position of the follower. The output is parallel to the axis of the cam. These were once common is mechanical analog computation and special functions in control systems.
A face cam that implements three outputs for a single rotational input is the stereo phonograph, where a relatively constant lead groove guides the stylus and tone arm unit, acting as either a rocker-type (tone arm) or linear (linear tracking turntable) follower, and the stylus alone acting as the follower for two orthogonal outputs to representing the audio signals. These motions are in a plane radial to the rotation of the record and at angles of 45 degrees to the plane of the disk (normal to the groove faces). The position of the tone arm was used by some turntables as a control input, such as to turn the unit off or to load the next disk in a stack, but was ignored in simple units.
Heart Shaped Cam.
This type of cam, in the form of a symmetric heart symbol, is used to return a shaft holding the cam to a set position by pressure from a roller. They were used for example on early models of Post Office Master clocks to synchronise the clock time with Greenwich Mean Time when the activating follower was pressed onto the cam automatically via a signal from an accurate time source.
Snail drop Cam.
This type of cam was used for example in mechanical time keeping clocking-in clocks to drive the day advance mechanism at precisely midnight and consisted of a follower being raised over 24 hours by the cam in a spiral path which terminated at a sharp cut off at which the follower would drop down and activate the day advance. Where timing accuracy is required as in clocking-in clocks these were typically ingeniously arranged to have a roller cam follower to raise the drop weight for most of its journey to near its full height, and only for the last portion of its travel for the weight to be taken over and supported by a solid follower with a sharp edge. This ensured that the weight dropped at a precise moment, enabling accurate timing. This was achieved by the use of two snail cams mounted coaxially with the roller initially resting on one cam and the final solid follower on the other but not in contact with its cam profile. Thus the roller cam was initially carried the weight, until at the final portion of the run the profile of the non-roller cam rose more than the other causing the solid follower to take the weight.
Linear cam.
A linear cam is one in which the cam element moves in a straight line rather than rotates. The cam element is often a plate or block, but may be any cross section. The key feature is that the input is a linear motion rather than rotational. The cam profile may be cut into one or more edges of a plate or block, may be one or more slots or grooves in the face of an element, or may even be a surface profile for a cam with more than one input. The development of a linear cam is similar to, but not identical to, that of a rotating cam.
A common example of a linear cam is a key for a pin tumbler lock. The pins act as the followers. This behavior is exemplified when the key is duplicated in a key duplication machine, where the original key acts as a control cam for cutting the new key.
History.
An early cam was built into Hellenistic water-driven automata from the 3rd century BC. Cams were later employed by Al-Jazari, who used them in his own automata.

</doc>
<doc id="40287" url="https://en.wikipedia.org/wiki?curid=40287" title="William B. Ogden">
William B. Ogden

William Butler Ogden (June 15, 1805 – August 3, 1877) was the first Mayor of Chicago.
Life.
Ogden was born on June 15, 1805, in Walton, New York. When still a teenager, his father died and Ogden took over the family real estate business. He assisted Charles Butler, his brother-in-law, with business matters related to opening a new building for New York University, attending the law school for a brief period himself. He was a member of the New York State Assembly (Delaware Co.) in 1835. He married Marianna T. Arnot.
Ogden designed the first swing bridge over the Chicago River and donated the land for Rush Medical Center. Ogden was also a founder of the Chicago Board of Trade.
Ogden was a leading promoter and investor in the Illinois and Michigan Canal, then switched his loyalty to railroads. Throughout his later life, Ogden was heavily involved in the building of several railroads. "In 1847, Ogden announced a plan to build a railway out of Chicago, but no capital was forthcoming. Eastern investors were wary of Chicago's reputation for irrational boosterism, and Chicagoans did not want to divert traffic from their profitable canal works. So Ogden and his partner J. Young Scammon solicited subscriptions from the farmers and small businessmen whose land lay adjacent to the proposed rail. Farmer's wives used the money they earned from selling eggs to buy shares of stock on a monthly payment plan. By 1848, Ogden and Scammon had raised $350,000—enough to begin laying track. The Galena and Chicago Union Railroad was profitable from the start and eventually extended out to Wisconsin, bringing grain from the Great Plains into the city. As president of Union Pacific, Ogden extended the reach of Chicago's rail lines to the West coast."
In 1853, the Chicago Land Company, of which Ogden was a trustee, purchased land at a bend in the Chicago River and began to cut a channel, formally known as North Branch Canal, but also referred to as Ogden's Canal. The resulting island is now known as Goose Island.
Post-Chicago.
Later he served on the board of the Mississippi and Missouri Railroad and lobbied with many others for congressional approval and funding of the transcontinental railroad. After the 1862 Pacific Railroad Act, Ogden was named as the first president of the Union Pacific Railroad. Ogden was a good choice for the first president, but his railroad experience was most likely not the primary reason he was chosen; Ogden was a clever man who had many political connections. When Ogden came to lead the Union Pacific, the railroad wasn't fully funded and hadn't yet laid a single mile of track—the railroad existed largely on paper created by an act of Congress. As part of the 1862 Pacific Railroad Act, Congress named several existing railroad companies to complete portions of the project. Several key areas needed to link the East (Chicago) to the West had none, and hence the Union Pacific was formed by Congress. Ogden was a fierce supporter of the transcontinental railroad at a time of great unrest for the country and was quoted as saying
As history now shows, eventually Ogden and many others got their wish. Several railroads later, Ogden Flats, Utah, where the Golden Spike was driven, was named for him.
In 1860, Ogden switched his loyalty to the Republican Party, which shared his views regarding slavery, although he left the party over a dispute with Abraham Lincoln. Ogden felt that the Emancipation Proclamation was premature. Following his defection from the Republican party, Ogden retired from politics and moved back to his native New York.
On October 8, 1871, Ogden lost most of his prized possessions in the Great Chicago Fire. He also owned a lumber company in Peshtigo, Wisconsin, which burned the same day.
He named his home in the High Bridge area of the Bronx (now called Aqueduct Bridge over the Harlem River) Villa Boscobel. He died there Friday, August 3, 1877. The funeral was held August 6, 1877, with interment following at Woodlawn Cemetery, Bronx.
Namesakes of William B. Ogden include a stretch of U.S. Highway 34, called Ogden Avenue in Chicago and its suburbs, Ogden International School of Chicago, which is located on Walton Street in Chicago, and Ogden Slip, a man-made harbor near the mouth of the Chicago River. Ogden Avenue in The Bronx is also named after him.

</doc>
<doc id="40288" url="https://en.wikipedia.org/wiki?curid=40288" title="John Wentworth (Illinois)">
John Wentworth (Illinois)

John Wentworth (nicknamed "Long John") (March 5, 1815 – October 16, 1888) was the editor of the "Chicago Democrat," publisher of an extensive Wentworth family genealogy, a two-term mayor of Chicago, and a six-term member of the United States House of Representatives, both before and after his service as mayor.
After growing up in New Hampshire, he joined the migration west and moved to the developing city of Chicago in 1836, where he made his adult life. Wentworth was affiliated with the Democratic Party until 1855; then he changed to the Republican Party. After retiring from politics, he wrote a three-volume genealogy of the Wentworth family in the United States.
Early life and education.
John Wentworth was born in Sandwich, New Hampshire. He was educated at the New Hampton Literary Institute and at the academy of Dudley Leavitt. He graduated from Dartmouth College in 1836.
Migration west and career.
Later that year, Wentworth joined a migration west and moved to Chicago, arriving in the city on October 25, 1836. He became managing editor of Chicago's first newspaper, the "Chicago Democrat," eventually becoming its owner and publisher.
He started a law practice and entered politics. He was a business partner of Illinois financier Jacob Bunn, and the two men were two of the incorporators of the Chicago Secure Depository Company.
Marriage and family.
In 1844 he married Roxanna Marie Loomis. 
In later years, his nephew Moses J. Wentworth handled his business affairs, and would eventually manage his estate as well.
Political career.
After becoming active with the Democrats, Wentworth was elected to the U.S. House of Representatives, where he served for a total of six terms, five of them as a Democrat: (March 4, 1843March 3, 1851 and March 4, 1853March 3, 1855).
He returned to Chicago and affiliated with the Republican Party. Wentworth was elected as mayor of Chicago for two terms, 1857–1858 and 1860–1861. Wentworth instituted the use of chain gangs of prisoners in the city as laborers.
In his effort to clean up the city's morals, he hired spies to determine who was frequenting Chicago's brothels. In 1857, Wentworth led a raid on "the Sands," Chicago's red-light district, which resulted in the burning of the area. 
In 1864 Wentworth ran again for Congress, as a Republican, and was elected for his last term, serving March 4, 1865March 3, 1867. While he was in the House, there was a controversial vote to settle a boundary issue between Wisconsin and Illinois, with Wisconsin claiming land as far as the tip of Lake Michigan. Wentworth was promised that if he voted to give the land including Chicago to Wisconsin, he would be appointed to the US Senate. Wentworth declined the offer.
According to city historians in Sandwich, Illinois, Wentworth was one of the key individuals who was responsible for the city getting a railroad stop. The town, which at the time, was called "Newark Station", was given the station, and in turn, the town gave Wentworth the honor of naming the town, which he subsequently named after his hometown, Sandwich, New Hampshire. It is also to note that the boundary line dispute with Wisconsin would have cut through present-day Sandwich, as it straddles the northern border with neighboring LaSalle County, which would have been the State Line had Wentworth not been successful in moving the line north.
After retiring from Congress, from 1868 Wentworth lived at his country estate at 5441 South Harlem Avenue in Chicago. He owned about of land in what is today part of the Chicago neighborhood of Garfield Ridge and suburban Summit.
When an author left a manuscript of a history of Chicago with Wentworth for his suggestions, he reportedly removed what did not refer to him and returned the manuscript to its author with the note, "Here is your expurgated and correct history of Chicago."
Family historian.
He researched and wrote "The Wentworth Genealogy – English and American" - twice, which he published privately. The first two-volume edition, also known as the "private edition", published in 1871, was followed by a second, corrected, edition in 1878, which was published in three volumes, for a total of 2241 pages. The total reported cost for both editions was $40,000. The first of the 1878 volumes chronicles the ancestry of Elder William Wentworth, the first of this family in New England, and his first five generations of New World descendants. The second and third volumes discuss the "Elder's" many descendants and others of the name. John was a fourth great grandson of William.
Death.
Wentworth died at his estate in 1888, aged 73. He was buried in Rosehill Cemetery in Chicago.

</doc>
<doc id="40289" url="https://en.wikipedia.org/wiki?curid=40289" title="Hiram College">
Hiram College

Hiram College ( ) is a private liberal arts college located in Hiram, Ohio. It was founded in 1850 as the Western Reserve Eclectic Institute by Amos Sutton Hayden and other members of the Disciples of Christ Church. The college is nonsectarian and coeducational. It is accredited by The Higher Learning Commission of the North Central Association of Colleges and Schools. Hiram's most famous alumnus is James A. Garfield, who served as a college instructor and principal, and was subsequently elected the 20th President of the United States.
History.
On June 12, 1849, representatives of the Disciples of Christ voted to establish an academic institution, which would later become Hiram College. On November 7 that year, they chose the village of Hiram as the site for the school because the founders considered this area of the Western Reserve to be "healthful and free of distractions". The following month, on December 20, the founders accepted the suggestion of Isaac Errett and named the school the Western Reserve Eclectic Institute.
The Institute's original charter was authorized by the state legislature on March 1, 1850, and the school opened several months later, on November 27. Many of the students came from the surrounding farms and villages of the Western Reserve, but Hiram soon gained a national reputation and students began arriving from other states. On February 20, 1867, the Institute incorporated as a college and changed its name to Hiram College.
During the years before it was renamed Hiram College, 1850–1867, the school had seven principals, the equivalent of today's college presidents. The two that did the most in establishing and defining the nature of the institution were Disciple minister Amos Sutton Hayden, who led the school through its first six years, and James A. Garfield, who had been a student at the Institute from 1851–1853 and then returned in 1856 as a teacher. As principal, Garfield expanded the Institute's curriculum. He left the Institute in 1861 and in 1880 was elected the 20th President of the United States.
In 1870, one of Garfield's best friends and former students, Burke A. Hinsdale, was appointed Hiram's president. Although there were two before him, Hinsdale is considered the college's first permanent president because the others served only briefly. The next president to have a major impact on the college was Ely V. Zollars, who increased enrollment significantly, established a substantial endowment and created a program for the construction of campus buildings. Later presidents who served for at least 10 years were Miner Lee Bates, Kenneth I. Brown, Paul H. Fall, Elmer Jagow, and G. Benjamin Oliver.
In 1931, shortly before Hiram celebrated the 100th anniversary of Garfield's birth, there was a debate in the community about changing the name of the school to Garfield College. There were strong advocates on both sides of the issue. Among the 2,000 guests at the centennial celebration were three generations of Garfield's family, including two of his sons. The idea of changing the college's name was not mentioned at the event and the idea was abandoned.
Principals and presidents.
The following is a list of the school's leaders since its founding in 1850.
Profile.
As of the 2011–12 academic year, Hiram's student body consists of 1,334 undergraduates from 35 states and 30 foreign countries. Of the 73 full-time faculty, 95-percent hold a Ph.D. or other terminal degree in their field.
Rankings.
Hiram was ranked #167 among National Liberal Arts Colleges by U.S. News and World Report in 2012. At the same time, Hiram is currently ranked #67 among Liberal Arts Colleges by Washington Monthly. Also, in 2012, Forbes ranked Hiram at #197 among all colleges and universities in the U.S., and #39 in the Midwest. Hiram has regularly been included in The Princeton Review Best Colleges guide, and is one of only 40 schools included in Loren Pope's book "Colleges That Change Lives".
Hiram is a member of the Annapolis Group, which has been critical of the college rankings process. Hiram is among the signatories of the "Presidents Letter". 
Academics.
Hiram specializes in the education of undergraduate students, though the college does have a small graduate program. Hiram confers the BA, BSN, and MA degrees. The college offers 30 majors and 36 minors for traditional undergraduates, in addition to pre-professional programs for specific fields. Interdisciplinary studies have also been a part of Hiram's curriculum for decades.
Hiram's curriculum requires all students to complete one course in each of eight academic areas: creative methods, interpretive methods, modeling methods, experimental scientific methods, social and cultural analysis, experiencing the world, understanding diversity at home, and ethics and social responsibility. Its education plan also includes international study and independent study opportunities, and faculty-guided research projects. Currently, almost all majors require some form of extensive independent project or apprenticeship experience.
Hiram has seven Centers of Distinction for interdisciplinary studies. They include: Center for Deciphering Life's Languages, Center for Engaged Ethics, Center for Integrated Entrepreneurship, Center for Literature and Medicine, Center for the Study of Nature and Society, Garfield Institute for Public Leadership, and Lindsay-Crane Center for Writing and Literature.
Athletics.
The school's sports teams are called the Terriers. They participate in the NCAA's Division III and the North Coast Athletic Conference. Men's sports include baseball, basketball, cross country, football, golf, lacrosse, soccer, swimming & diving, and track & field. Women's sports include basketball, cross country, golf, lacrosse, soccer, softball, swimming & diving, track & field,volleyball, and tennis.
The Hiram College basketball team won the gold medal in the collegiate division of the 1904 Summer Olympics in St. Louis. It was the first time that basketball was part of an Olympics; it was included as a demonstration sport and no foreign teams participated.
The Cleveland Browns held their training camp at Hiram College from 1952 through 1974, making it the longest-tenured training site in the team's history.
Residence life.
The college's residential complexes include The Quad (Agler Hall, Dean Hall, New Quad, Peckham Hall, and Gray-Bancroft), The Hill (Bowler Hall, Henry Hall, Miller Hall, and Whitcomb Hall), Booth-Centennial, East Hall, and the Townhouses. They are managed by resident directors (RDs), resident managers (RMs), and resident assistants (RAs).
Student clubs and organizations.
Student Senate is the elected student governing body of the college. It serves as a liaison between students and the school's administration, and oversees all student clubs and organizations, collectively called the Associated Student Organizations (ASO). The Kennedy Center Programming Board (KCPB) falls under the auspices of Student Senate, and is responsible for planning educational, social, recreational, and cultural programs.
Hiram has close to 100 registered student clubs and organizations in eight categories: Academic, Greek Social, Musical, Political and Activisim, Publications and Communications, Religious, Special Interest and Service, and Sports and Recreation. Fraternities and sororities are not permitted on campus, but there are six Greek social clubs: Delta Chi Lambda, Kappa Sigma Pi, Lambda Lambda Lambda, Phi Beta Gamma, Phi Gamma Epsilon, and Phi Kappa Chi. The school newspaper is "The Advance".
Since 1971, Hiram has maintained a chapter of Phi Beta Kappa, the national honor society for the liberal arts. The school has also had a chapter of Omicron Delta Kappa (ODK), a national leadership honor society, since 1962.

</doc>
<doc id="40290" url="https://en.wikipedia.org/wiki?curid=40290" title="Joseph Medill">
Joseph Medill

Joseph Medill (April 6, 1823March 16, 1899) was an American newspaper editor, publisher, and Republican party politician. He was co-owner and managing editor of the "Chicago Tribune", and was Mayor of Chicago after the great fire of 1871.
Personal life.
Joseph Medill was born April 6, 1823, in Saint John, New Brunswick, British North America to a Scots-Irish family. He read law in Ohio and was admitted to the Ohio Bar in 1846.
Medill married Katherine "Kitty" Patrick on September 2, 1852, and they had three daughters, Katherine, Elinor and Josephine (1866 - 1892).
Publishing career.
In 1853, Medill and Edwin Cowles started the "Leader," a newspaper in Cleveland, Ohio. (It was later absorbed by "The Plain Dealer.") In 1854, the "Tribune"'s part-owner, Captain J. D. Webster, asked Medill to become the paper's managing editor. Medill was further encouraged to come to Chicago by Dr. Charles H. Ray of Galena, Illinois, and editor Horace Greeley of the "New York Tribune".
In 1855, Medill sold his interest in the "Leader" to Cowles, and bought the "Tribune" in partnership with Dr. Ray and Cowles' brother Alfred.
Under Medill's management, the "Tribune" flourished, becoming one of the largest newspapers in Chicago. Medill served as its managing editor until 1864, when Horace White became editor-in-chief. At that time Medill left day-to-day operations of the "Tribune" for political activities.
But White clashed with Medill over the presidential election of 1872. So, in 1873 Medill bought additional equity from Cowles and from White, becoming majority owner. In 1874, he replaced White as editor-in-chief. Medill served as editor-in-chief until his death.
Political activity.
Under Medill, the "Tribune" became the leading Republican newspaper in Chicago. Medill was strongly anti-slavery, supporting both the Free-Soil cause and Abolitionism. Medill was a major supporter of Abraham Lincoln in the 1850s. Medill and the "Tribune" were instrumental in Lincoln's presidential nomination, and were equally supportive of the Union cause during the American Civil War. The "Tribune"'s chief adversary through this period was the "Chicago Times", which supported the Democrats.
In 1864, Medill left the "Tribune" editorship for political activity, which occupied him for the next ten years. He was appointed by President Grant to the first Civil Service Commission. In 1870, he was elected as a delegate to the Illinois Constitutional convention. In 1871, after the Great Chicago Fire, Medill was elected mayor of Chicago as candidate of the temporary "Fireproof" party, serving for two years. As mayor, Medill gained more power for the mayor's office, created Chicago's first public library, enforced blue laws, and reformed the police and fire departments. But the stress of the job impaired his health. In August 1873, he appointed Lester L. Bond as Acting Mayor for the remaining 3½ months of his term, and went to Europe on a convalescent tour.
Medill was a strong Republican loyalist who supported President Grant for re-election in 1872. The breach with White came because White supported the breakaway Liberal Republicans, reformists who nominated Horace Greeley for president. It was also at this time that Medill broke with Greeley.
Family tree.
The tree omits Medill's third daughter, Josephine, who died in 1892.

</doc>
<doc id="40291" url="https://en.wikipedia.org/wiki?curid=40291" title="Carter Harrison, Sr.">
Carter Harrison, Sr.

Carter Henry Harrison, Sr. (February 15, 1825October 28, 1893) was an American politician who served as mayor of Chicago, Illinois, from 1879 until 1887; he was subsequently elected to a fifth term in 1893 but was assassinated before completing his term. He previously served two terms in the United States House of Representatives. Harrison was the first cousin twice removed of President William Henry Harrison.
Born near Lexington, Kentucky, to Carter Henry Harrison II and Caroline Russell, he was only a few months old when his father died. He was educated by private tutors, and was graduated from Yale College in 1845 as a member of Scroll and Key. Following graduation, he traveled and studied in Europe from 1851 to 1853 before entering Transylvania College in Lexington, where he earned a law degree in 1855. He was admitted to the bar in 1855 and commenced practice in Chicago; Harrison came to Chicago because he saw it as a land of opportunity.
Harrison ran an unsuccessful campaign in 1872 for election to the Forty-third Congress. Beginning in 1874, he served as a member of the board of commissioners of Cook County. He was elected as a Democrat to the Forty-fourth and Forty-fifth Congresses, and delegate to the 1880 and 1884 Democratic National Conventions.
Harrison married Margarette (or Margaret) E. Stearns in 1882, following the death of his first wife in 1876. She was the daughter of Chicago pioneer Marcus C. Stearns.
"A Summer's Outing".
In 1890, Harrison and his daughter took a vacation trip from Chicago to Yellowstone National Park and Alaska. His letters from the trip were first published in the "Chicago Tribune" and later compiled into the book (1891): "A Summer's Outing and The Old Man's Story."
Assassination.
The night of the Haymarket Riot in 1886, Harrison walked unmolested through the crowd of anarchists and advised the police to leave the demonstrators alone. The riot was sparked by a bomb, reportedly thrown at police by anarchists (killing seven police officers). After leaving office, Harrison was owner and editor of the "Chicago Times" from 1891 to 1893. He was re-elected in 1893, in time for the World's Columbian Exposition. His desire was to show the world the true Chicago, and he appointed 1st Ward Alderman "Bathhouse" John Coughlin to sit on the reception committee.
On October 28, 1893, two days before the close of the Exposition, Harrison was murdered in his home by Patrick Eugene Prendergast, a disgruntled office seeker. Harrison was buried in Chicago's Graceland Cemetery. Prendergast was hanged on July 13, 1894. Harrison was Chicago's first five-time elected mayor; eventually his son, Carter Harrison, Jr., was also elected mayor five times.
Harrison's career and assassination are closely connected with the World's Columbian Exposition, and are discussed at some length as a subplot to the two main stories (about the fair and serial killer H. H. Holmes) in "The Devil in the White City". The celebration of the close of the Exposition was cancelled and replaced by a large public memorial service for Harrison.

</doc>
<doc id="40292" url="https://en.wikipedia.org/wiki?curid=40292" title="Carter Harrison, Jr.">
Carter Harrison, Jr.

Carter Henry Harrison, Jr. (April 23, 1860 – December 25, 1953) was an American politician who served as Mayor of Chicago, Illinois (1897–1905 and 1911–1915). The City's 30th mayor, he was the first actually born in Chicago.
Biography.
He was born on April 23, 1860 in Chicago.
Like his father, Carter Harrison, Sr., Carter Harrison, Jr., gained election to five terms as Chicago's mayor. Educated in Saxe-Altenburg, Germany, Harrison returned to Chicago to help his brother run the "Chicago Times", which their father bought in 1891. Under the Harrisons the paper became a resolute supporter of the Democratic Party, and was the only local newspaper to support the Pullman strikers in the mid-1890s.
As with his father, Harrison did not believe in trying to legislate morality. As mayor, Harrison believed that Chicagoans' two major desires were to make money and to spend it. During his administrations, Chicago's vice districts blossomed, and special maps were printed to enable tourists to find their way from brothel to brothel. The name of one Chicago saloon-keeper of the time supposedly entered the English language as a term for a strong or laced drink intended to render unconsciousness: Mickey Finn.
However, Harrison was seen as more of a reformer than his father, which helped him garner the middle class votes his father had lacked. One of Harrison's biggest enemies was Charles Yerkes, whose plans to monopolize Chicago's streetcar lines were vigorously attacked by the mayor. During his final term in office, Harrison established the Chicago Vice Commission and worked to close down the Levee district, starting with the Everleigh Club brothel on October 24, 1911.
Harrison was a hopeful for the 1904 Democratic nomination for President, but was unable to negotiate his way through a tangle of conflicting loyalies to different Party bosses; the nomination went to Alton B. Parker, who was soundly defeated by Theodore Roosevelt.
In 1915, when Harrison left office, Chicago had essentially reached its modern size in land area, and had a population of 2,400,000; the city was moving inexorably into its status as a major modern metropolis. He and his father had collectively been mayor of the city for 21 of the previous 36 years. 
He died in Chicago on December 25, 1953, and is buried in Graceland Cemetery.
Legacy.
Harrison wrote his autobiography, not once but twice; his wife, Edith Ogden Harrison, was a well-known writer of children's books and fairy tales in the first two decades of the twentieth century.
He was a member of many organizations including the Freemasons, Knights Templar, the Society of the Cincinnati, Sons of the Revolution, Sons of the American Revolution, Society of Colonial Wars, Veterans of Foreign Wars, American Legion and the Military Order of the World Wars. 
Ancestry.
Harrison was a descendant of Robert Carter I, Benjamin Harrison IV, William Randolph, and Isham Randolph of Dungeness.

</doc>
<doc id="40293" url="https://en.wikipedia.org/wiki?curid=40293" title="Nikkei 225">
Nikkei 225

The , more commonly called the Nikkei, the Nikkei index, or the Nikkei Stock Average (, , or ), is a stock market index for the Tokyo Stock Exchange (TSE). It has been calculated daily by the "Nihon Keizai Shimbun" ("Nikkei") newspaper since 1950. It is a price-weighted index (the unit is yen), and the components are reviewed once a year. Currently, the Nikkei is the most widely quoted average of Japanese equities, similar to the Dow Jones Industrial Average. In fact, it was known as the "Nikkei Dow Jones Stock Average" from 1975 to 1985.
The Nikkei 225 began to be calculated on September 7, 1950, retroactively calculated back to May 16, 1949. Since January 2010 the index is updated every 15 seconds during trading sessions.
The Nikkei 225 Futures, introduced at Singapore Exchange (SGX) in 1986, the Osaka Securities Exchange (OSE) in 1988, Chicago Mercantile Exchange (CME) in 1990, is now an internationally recognized futures index.
The Nikkei average has deviated sharply from the textbook model of stock averages which grow at a steady exponential rate. The average hit its all-time high on December 29, 1989, during the peak of the Japanese asset price bubble, when it reached an intra-day high of 38,957.44 before closing at 38,915.87, having grown sixfold during the decade. Subsequently, it lost nearly all these gains, closing at 7,054.98 on March 10, 2009—81.9% below its peak twenty years earlier.
Another major index for the Tokyo Stock Exchange is the Topix.
On March 15, 2011, the second working day after the massive earthquake in the northeast part of Japan, the index dropped over 10% to finish at 8605.15, a loss of 1,015 points. The index continued to drop throughout 2011, eventually bottoming out at 8160.01 on November 25, putting it at its lowest close since March 10, 2009. The Nikkei fell over 17% in 2011, finishing the year at 8455.35, its lowest year-end closing value in nearly thirty years, when the index finished at 8016.70 in 1982.
The Nikkei started 2013 near 10,600, hitting a peak of 15,942 in May. However, shortly afterward, it plunged by almost 10% before rebounding, making it the most volatile stock market index among the developed markets. By 2015, it has reached over 20,000 mark; marking a gain of over 10,000 in 2 years making it one of the fastest growing stock market indexes in the world. 
Weighting.
The index is a price-weighted index. As of late 2014, the company with the largest influence on the index is Fast Retailing.
Components.
As of December 2015, the Nikkei 225 consists of the following companies: (Japanese securities identification code in parentheses)
Components.
As of February 2015, the Nikkei 225 consists of the following companies: (Japanese securities identification code in parentheses)

</doc>
<doc id="40294" url="https://en.wikipedia.org/wiki?curid=40294" title="Stephen Smale">
Stephen Smale

Stephen Smale (born July 15, 1930) is an American mathematician from Flint, Michigan. His research concerns topology, dynamical systems and mathematical economics. He was awarded the Fields Medal in 1966, and spent more than three decades on the mathematics faculty of the University of California, Berkeley (1960–1961 and 1964–1995).
Education and career.
Smale entered the University of Michigan in 1948. Initially, he was a good student, placing into an honors calculus sequence taught by Bob Thrall and earning himself A's. However, his sophomore and junior years were marred with mediocre grades, mostly Bs, Cs and even an F in nuclear physics. However, with some luck, Smale was accepted as a graduate student at the University of Michigan's mathematics department. Yet again, Smale performed poorly in his first years, earning a C average as a graduate student. It was only when the department chair, Hildebrandt, threatened to kick out Smale, that he began to work hard. Smale finally earned his Ph.D. in 1957, under Raoul Bott.
Smale began his career as an instructor at the college at the University of Chicago. In 1958, he astounded the mathematical world with a proof of a sphere eversion. He then cemented his reputation with a proof of the Poincaré conjecture for all dimensions greater than or equal to 5, published in 1961; in 1962 he generalized the ideas in a 107 page paper that established the h-cobordism theorem.
After having made great strides in topology, he then turned to the study of dynamical systems, where he made significant advances as well. His first contribution is the Smale horseshoe that started significant research in dynamical systems. He also outlined a research program carried out by many others. Smale is also known for injecting Morse theory into mathematical economics, as well as recent explorations of various theories of computation.
In 1998 he compiled a list of 18 problems in mathematics to be solved in the 21st century, known as Smale's problems. This list was compiled in the spirit of Hilbert's famous list of problems produced in 1900. In fact, Smale's list contains some of the original Hilbert problems, including the Riemann hypothesis and the second half of Hilbert's sixteenth problem, both of which are still unsolved. Other famous problems on his list include the Poincaré conjecture (now a theorem, proved by Grigori Perelman), the P = NP problem, and the Navier–Stokes equations, all of which have been designated Millennium Prize Problems by the Clay Mathematics Institute.
Earlier in his career, Smale was involved in controversy over remarks he made regarding his work habits while proving the higher-dimensional Poincaré conjecture. He said that his best work had been done "on the beaches of Rio". This led to the withholding of his grant money from the NSF. He has been politically active in various movements in the past, such as the Free Speech movement and the movement against the Vietnam War. At one time he was subpoenaed by the House Un-American Activities Committee.
In 1960 Smale was appointed an associate professor of mathematics at the University of California, Berkeley, moving to a professorship at Columbia University the following year. In 1964 he returned to a professorship at UC Berkeley where he has spent the main part of his career. He retired from UC Berkeley in 1995 and took up a post as professor at the City University of Hong Kong. He also amassed over the years one of the finest private mineral collections in existence. Many of Smale's mineral specimens can be seen in the book—"The Smale Collection: Beauty in Natural Crystals".
Since 2002 Smale is a Professor at the Toyota Technological Institute at Chicago; starting August 1, 2009, he is also a Distinguished University Professor at the City University of Hong Kong.
In 2007, Smale was awarded the Wolf Prize in mathematics.

</doc>
<doc id="40295" url="https://en.wikipedia.org/wiki?curid=40295" title="Hawker Siddeley Nimrod">
Hawker Siddeley Nimrod

The Hawker Siddeley Nimrod was a maritime patrol aircraft developed and operated by the United Kingdom. It was an extensive modification of the de Havilland Comet, the world's first operational jet airliner. It was originally designed by de Havilland's successor firm, Hawker Siddeley; further development and maintenance work was undertaken by Hawker Siddeley's own successor companies, British Aerospace and BAE Systems, respectively.
Designed in response to a requirement issued by the Royal Air Force (RAF) to replace its fleet of ageing Avro Shackletons, the "Nimrod MR1"/"MR2"s were primarily fixed-wing aerial platforms for anti-submarine warfare (ASW) operations; secondary roles included maritime surveillance and anti-surface warfare. It served from the early 1970s until March 2010. The intended replacement was to be extensively rebuilt Nimrod MR2s, designated Nimrod MRA4; however due to considerable delays, repeated cost overruns, and financial cutbacks, the development of the MRA4 was abandoned in 2010.
In addition to the three Maritime Reconnaissance variants, two further Nimrod types were developed. The RAF operated a small number of the "Nimrod R1", an electronic intelligence gathering (ELINT) variant. A dedicated airborne early warning platform, the Nimrod AEW3 was in development from late 1970s to the mid-1980s; however, much like the MRA4, considerable problems were encountered in development and thus the project was cancelled in 1986 in favour of an off-the-shelf solution in the Boeing E-3 Sentry. All Nimrod variants had been retired by mid-2011.
Development.
MR1.
On 4 June 1964, the British Government issued Air Staff Requirement 381 to replace the Avro Shackleton. Such a replacement was necessitated by the rapidly approaching fatigue life limits of the RAF's existing Shackleton fleet. A great deal of interest in the requirement was received from both British and foreign manufacturers, offered aircraft including the Lockheed P-3 Orion, the Breguet Atlantic and derivatives of the Hawker Siddeley Trident, BAC One-Eleven, Vickers VC10 and de Havilland Comet. On 2 February 1965, British Prime Minister Harold Wilson announced the intention to order Hawker Siddeley's maritime patrol version of the Comet, the HS.801.
The Nimrod design was based on that of the Comet 4 civil airliner which had reached the end of its commercial life (the first two prototype Nimrods, XV148 & XV147 were built from two final unfinished Comet 4C airframes). The Comet's turbojet engines were replaced by Rolls-Royce Spey turbofans for better fuel efficiency, particularly at the low altitudes required for maritime patrol. Major fuselage changes were made, including an internal weapons bay, an extended nose for radar, a new tail with electronic warfare (ESM) sensors mounted in a bulky fairing, and a MAD (magnetic anomaly detector) boom. After the first flight in May 1967, the RAF ordered a total of 46 Nimrod MR1s. The first example (XV230) entered service in October 1969. A total of five squadrons using the type were established; four were permanently based in the UK and a fifth was initially based in Malta.
R1.
Three Nimrod aircraft were adapted for the signals intelligence role, replacing the Comet C2s and Canberras of No. 51 Squadron in May 1974. The R1 was visually distinguished from the MR2 by the lack of a MAD boom. It was fitted with an array of rotating dish aerials in the aircraft's bomb bay, with further dish aerials in the tailcone and at the front of the wing-mounted fuel tanks. It had a flight crew of four (two pilots, a flight engineer and one navigator) and up to 25 crew operating the SIGINT equipment.
Only since the end of the Cold War has the role of the aircraft been officially acknowledged; they were once described as "radar calibration aircraft". The R1s have not suffered the same rate of fatigue and corrosion as the MR2s. One R1 was lost in a flying accident since the type's introduction; this occurred in May 1995 during a flight test after major servicing, at RAF Kinloss. To replace this aircraft an MR2 was selected for conversion to R1 standard, and entered service in December 1996.
The Nimrod R1 was based initially at RAF Wyton, Cambridgeshire, and later at RAF Waddington in Lincolnshire, England, and flown by 51 Sqn. The two remaining Nimrod R1s were originally planned to be retired at the end of March 2011, but operational requirements forced the RAF to deploy one to RAF Akrotiri, Cyprus on 16 March in support of Operation Ellamy. The last flight of the type was on 28 June 2011 from RAF Waddington, in the presence of the Chief of the Air Staff, ACM Sir Stephen Dalton. XV 249, the former MR2, is now on display at the RAF Museum Cosford, West Midlands. The R1 is being replaced by three Boeing RC-135W "Rivet Joint" aircraft, acquired under the Airseeker project; the first aircraft was delivered in late 2013.
MR2.
Starting in 1975, 35 aircraft were upgraded to MR2 standard, being re-delivered from August 1979. The upgrade included extensive modernisation of the aircraft's electronic suite. Changes included the replacement of the obsolete ASV Mk 21 radar used by the Shackleton and Nimrod MR1 with the new EMI Searchwater radar, a new acoustic processor (GEC-Marconi AQS-901) capable of handling more modern sonobouys, a new mission data recorder (Hanbush) and a new Electronic Support Measures (Yellow Gate) which included new pods on the wingtips.
Provision for in-flight refuelling was introduced during the Falklands War (as the "MR2P"), as well as hardpoints to allow the Nimrod to carry the AIM-9 Sidewinder missile to counter enemy Argentine Air Force maritime surveillance aircraft. In preparation for operations in the Gulf War theatre, several MR2s were fitted with new communications and ECM equipment to deal with anticipated threats; at the time these modified aircraft were given the designation "MR2P(GM) (Gulf Mod)".
The Nimrod MR2 carried out three main roles – Anti-Submarine Warfare (ASW), Anti-Surface Unit Warfare (ASUW) and Search and Rescue (SAR). Its extended range enabled the crew to monitor maritime areas far to the north of Iceland and up to 4,000 km out into the Western Atlantic. With Air-to-Air Refuelling (AAR), range and endurance was greatly extended. The crew consisted of two pilots and one flight engineer, two navigators (one tactical navigator and a routine navigator), one Air Electronics Officer (AEO), the sonobuoy sensor team of two Weapon System Operators (WSOp ACO) and four Weapon System Operators (WSOp EW) to manage passive and active electronic warfare systems.
Until 1992, the Nimrod MR2 was based at RAF Kinloss in Scotland (120, 201 and 206 Squadrons), and RAF St Mawgan in Cornwall (42 and 38(R) Squadrons). Following Options for Change, 42 Squadron was disbanded and its number reassigned to 38(R) Squadron. The Nimrod MR2 aircraft was withdrawn on 31 March 2010, a year earlier than planned, for financial reasons. The last official flight of a Nimrod MR2 took place on 26 May 2010, with XV229 flying from RAF Kinloss to Kent International Airport to be used as an evacuation training airframe at the nearby MOD Defence Fire Training and Development Centre.
AEW3.
In the mid-1970s a modified Nimrod was proposed for the Airborne Early Warning (AEW) mission – again as a replacement for the Lancaster-derived, piston-engined Shackleton AEW.2. Eleven existing Nimrod airframes were to be converted by British Aerospace at the former Avro plant at Woodford to house the GEC Marconi radars in a bulbous nose and tail. The Nimrod AEW3 project was plagued by cost over-runs and problems with the GEC 4080M computer used. Eventually, the MoD recognised that the cost of developing the radar system to achieve the required level of performance was prohibitive and the probability of success very uncertain, and in December 1986 the project was cancelled. The RAF eventually received seven Boeing E-3 Sentry aircraft instead.
MRA4.
The Nimrod MRA4 was intended to replace the capability provided by the MR2. It was essentially a new aircraft, with current-generation Rolls-Royce BR710 turbofan engines, a new larger wing, and fully refurbished fuselage. However the project was subject to delays, cost over-runs, and contract re-negotiations; the type had been originally intended to enter service in 2003. The MRA4 was cancelled in 2010 as a result of the Strategic Defence and Security Review at which point it was £789 million over-budget and nine years late; the development airframes were also scrapped. The cancellation of the MRA4 marked an abortive end of the Nimrod's era; the functions it provided were largely abandoned leading to a significant UK capability gap. A few functions were dispersed to other assets, including the use of unmanned aerial vehicles (UAVs) to conduct limited maritime surveillance.
Design.
Overview.
The Nimrod was the first jet-powered maritime patrol aircraft (MPA) to enter service, being powered by the Rolls-Royce Spey turbofan engine. Aircraft in this role have been commonly propelled by piston or turboprop powerplants instead to maximise fuel economy and enable maximum patrol time on station; advantages of the Nimrod's turbofan engines included greater speed and altitude capabilities, it was also more capable of evading detection methods by submarines, whereas propeller-driven aircraft are more detectable underwater to standard acoustic sensors. Inflight, the Nimrods had a flight endurance of ten hours without aerial refuelling; the MR2s were later fitted to receive mid-air refuelling in response to demands in the Falklands War.
At the start of a patrol mission all four engines would normally be running, but, as the aircraft's weight was reduced by the consumption of onboard fuel, up to two engines could be intentionally shut down, allowing the remaining engines to be operated in a more efficient manner. Instead of relying on ram air to restart an inactive engine, compressor air could be crossfed from a live engine to a starter turbine; the crossfeed duct was later discovered to be a potential fire hazard. Similarly, the two hydraulic systems on board were designed to be powered by the two inner engines that would always be running. Electrical generation was designed to far exceed the consumption of existing equipment to accommodate additional systems installed over the Nimrod's operational service.
The standard Nimrod fleet carried out three basic operational roles during their RAF service: Anti-Submarine Warfare duties typically involved surveillance over an allocated area of the North Atlantic to detect the presence of Soviet submarines in that area and to track their movements. In the event of war, reconnaissance information gathered during these patrols would be shared with other allied aircraft to enable coordinated strikes at both submarines and surface targets. Search and rescue (SAR) missions were another important duty of the RAF's Nimrod fleet, operating under the Air Rescue Coordination Centre at RAF Kinloss, and were a common sight in both military and civil maritime incidents. Throughout the Nimrod's operational life, a minimum of one aircraft was being held in a state of readiness to respond to SAR demands at all times.
Avionics.
The Nimrod featured a large crew of up to 25 personnel, although a typical crew numbered roughly 12 members, most of which operated the various onboard sensor suites and specialist detection equipment. A significant proportion of the onboard sensor equipment was housed outside the pressure shell inside the Nimrod's distinctive pannier lower fuselage. Sensor systems included radar, sonar, and the magnetic anomaly detector; a 'sniffer' could detect exhaust fumes from diesel submarines as well. The Nimrod and its detection capabilities was an important component of Britain's military defence during the height of the Cold War.
The Nimrod's navigational functions were computerised, and were managed from a central tactical compartment housed in the forward cabin; various aircraft functions such as weapons control and information from sensors such as the large forward doppler radar were displayed and controlled at the tactical station. The flight systems and autopilot could be directly controlled by navigator's stations in the tactical compartment, giving the navigator nearly complete aircraft control. The navigational systems comprised digital, analogue, and electro-mechanical elements; the computers were directly integrated with most of the Nimrod's guidance systems such as the air data computer, astrocompass, inertial guidance and doppler radar. Navigation information could also be manually input by the operators.
Upon its introduction to service, the Nimrod was hailed as possessing advanced electronic equipment such as onboard digital computers; the increased capability of these electronic systems allowed the RAF's fleet of 46 Nimrod aircraft to provide equal coverage to that of the larger fleet of retiring Avro Shackletons. The design philosophy of these computerised systems was that of a 'man-machine partnership'; while onboard computers performed much of the data sift and analysis processes, decisions and actions on the basis of that data remained in the operator's hands. To support the Nimrod's anticipated long lifespan, onboard computers were designed to be capable of integrating with various new components, systems, and sensors that could be added in future upgrades. After a mission, gathered information could be extracted for review purposes and for further analysis.
Armaments and equipment.
The Nimrod featured a sizeable bomb bay in which, in addition to armaments such as torpedos and missiles, could be housed a wide variety of specialist equipment for many purposes, such as up to 150 sonobuoys for ASW purposes or multiple air-deployed dinghies and droppable survival packs such as Lindholme Gear for SAR missions; additional fuel tanks and cargo could also be carried in the bomb bay during ferrying flights. Other armaments equippable in the bomb bay include mines, bombs, and nuclear depth charges; later munitions included the Sting Ray torpedo and Harpoon missile for increased capabilities.
The Nimrod could also be fitted with two detachable pylons mounted underneath the wings to be used with missiles such as the Martel; two specialised pylons were later added to enable the equipping of Sidewinder missiles, used for self-defence purposes from hostile aircraft. A powerful remote-controlled searchlight was installed underneath the starboard wing for SAR operations. For reconnaissance missions, the aircraft was also equipped with a pair of downward-facing cameras suited to low and high-altitude photography. In later years a newer electro-optical camera system was installed for greater imaging quality.
Various new ECMs and electronic support systems were retrofitted onto the Nimrod fleet in response to new challenges and to increase the type's defensive capabilities; additional equipment also provided more effective means of identification and communication. A number of modifications were introduced during the 1991 Gulf War; a small number of MR2s were fitted with improved Link 11 datalinks, new defensive ECM equipment including the first operational use of a towed radar decoy, and a forward looking infrared turret under the starboard wing.
Operational history.
Introduction to service.
The Nimrod first entered squadron service with the RAF at RAF St Mawgan in October 1969. These initial aircraft, designated as Nimrod MR1, were intended as a stop-gap measure, and thus were initially equipped with many of the same sensors and equipment as the Shackletons they were supplementing. While some improvements were implemented on the MR1 fleet to enhance their detection capabilities, the improved Nimrod MR2 variant entered service in August 1979 following a lengthy development process. The majority of the Nimrod fleet operated from RAF Kinloss.
Operationally, each active Nimrod would form a single piece of a complex submarine detection and monitoring mission. An emphasis on real-time intelligence sharing was paramount to these operations; upon detecting a submarine, Nimrod aircrews would inform Royal Navy frigates and other NATO-aligned vessels to pursuit in an effort to continuously monitor Soviet submarines. The safeguarding of the Royal Navy's Resolution-class ballistic missile submarines, which were the launch platform for Britain's nuclear deterrent, was viewed as being of the utmost priority.
Falklands War.
Nimrods were first deployed to Wideawake airfield on Ascension Island on 5 April 1982, the type at first being used to fly local patrols around Ascension to guard against potential Argentine attacks, and to escort the British Task Force as it sailed south towards the Falkands, with Nimrods also being used to provide search and rescue as well as communications relay support of the Operation Black Buck bombing raids by Avro Vulcans. As the Task Force neared what would become the combat theatre and the threat from Argentine submarines rose, the more capable Nimrod MR2s took on operations initially performed by older Nimrod MR1s. Aviation author Chris Chant has claimed that the Nimrod R1 also conducted electronic intelligence missions operating from Punta Arenas in neutral Chile.
The addition of air-to-air refuelling probes allowed operations to be carried out in the vicinity of the Falklands, while the aircraft's armament was supplemented by the addition of 1,000 lb (450 kg) general purpose bombs, BL755 cluster bombs and AIM-9 Sidewinder air-to-air missiles. The use of air-to-air refuelling allowed extremely long reconnaissance missions to be mounted, one example being a 19-hour 15-minute patrol conducted on 15 May 1982, which passed within 60 miles (97 km) of the Argentine coast to confirm that Argentine surface vessels were not at sea. Another long-range flight was carried out by an MR2 on the night of 20/21 May, covering a total of 8,453 miles (13,609 km), the longest distance flight carried out during the Falklands War. In all, Nimrods flew 111 missions from Ascension in support of British operations during the Falklands War.
Gulf War.
A detachment of three Nimrod MR2s was deployed to Seeb in Oman in August 1990 as a result of the Iraqi invasion of Kuwait, carrying out patrols over the Gulf of Oman and Persian Gulf. Due to the level of threats present in the Gulf theatre, operational Nimrods were quickly retrofitted with a Marconi-towed active decoy. Once hostilities commenced, the Nimrod detachment, by now increased to five aircraft, concentrated on night patrols, with daylight patrols carried out by US Navy Lockheed P-3 Orions. Nimrods were used to guide Westland Lynx helicopters and Grumman A-6 Intruder attack aircraft against Iraqi patrol vessels, being credited with assisting in sinking or damaging 16 Iraqi vessels.
After the ground offensive against Iraqi forces had ended, Britain elected to maintain an RAF presence in the region through assets such as the Nimrod and other aircraft. Nimrod R1s operated from August 1990 to March 1991 from Cyprus, providing almost continuous flying operations from the start of the ground offensive. Each R1 was retrofitted with the same Marconi towed active decoy as well as under wing chaff/flare dispensers, reportedly sourced from the Tornado fleet.
Afghanistan and Iraq War.
Nimrods were again deployed to the Middle East as part of the British contribution to the US-led invasion of Afghanistan; missions in this theatre involved the Nimrods performing lengthy overland flights for intelligence-gathering purposes. On 2 September 2006, 12 RAF personnel were killed when a Nimrod MR2 was destroyed in a midair explosion following an onboard fire over Afghanistan, it was the single greatest loss of British life since the Falklands War. The outbreak of the Iraq War in March 2003 saw the RAF's Nimrods being used for operations over Iraq, using the aircraft's sensors to detect hostile forces and to direct attacks by friendly coalition forces.
Search and rescue.
While the Nimrod MR1/MR2 was in service, one aircraft from each of the squadrons on rotation was available for search and rescue operations at one-hour standby. The standby aircraft carried two sets of Lindholme Gear in the weapons bay. Usually one other Nimrod airborne on a training mission would also carry a set of Lindholme Gear. As well as using the aircraft sensors to find aircraft or ships in trouble, it was used to find survivors in the water, with a capability to search areas of up to . The main role would normally be to act as on-scene rescue coordinator to control ships, fixed-wing aircraft, and helicopters in the search area.
The Nimrod was most often featured in the media in relation to its search-and-rescue role, such as in the reporting of major rescue incidents. In August 1979, several Nimrods were involved in locating yachting competitors during the disaster-stricken 1979 Fastnet race and coordinated with helicopters in searches for survivors from lost vessels. In March 1980, the "Alexander L. Kielland" was a Norwegian semi-submersible drilling rig that capsized whilst working in the Ekofisk oil field killing 123 people; six different Nimrods searched for survivors and took turns to provide rescue co-ordination, involving the control of 80 surface ships and 20 British and Norwegian helicopters. In an example of the search capabilities, in September 1977 when an attempted crossing of the North Atlantic in a Zodiac inflatable dinghy went wrong, a Nimrod found the collapsed dinghy and directed a ship to it.
Operation Tapestry.
The Nimrods were often used to enforce Operation Tapestry. Tapestry is a codeword for the activities by ships and aircraft that protect the United Kingdom's Sovereign Sea Areas, including the protection of fishing rights and oil and gas extraction. Following the establishment of a Exclusive Economic Zone (EEZ) at the beginning of 1977 the Nimrod fleet was given the task of patrolling the area. The aircraft would locate, identify, and photograph vessels operating in the EEZ. The whole area was routinely patrolled; in addition to surveillance, the aircraft would communicate with all oil and gas platforms. In 1978, an airborne Nimrod arrested an illegal fishing vessel in the Western Approaches and made the vessel proceed to Milford Haven for further investigation. During the Icelandic Cod Wars of 1972 and 1975–1976, the Nimrod fleet closely cooperated with Royal Navy surface vessels to protect British civilian fishing ships.
Accidents and incidents.
Five Nimrods were lost in accidents during the type's service with the RAF:

</doc>
<doc id="40298" url="https://en.wikipedia.org/wiki?curid=40298" title="Jean-Claude Killy">
Jean-Claude Killy

Jean-Claude Killy (born 30 August 1943) also known as Gilette is a former French World Cup alpine ski racer. Born in Saint-Cloud, Hauts-de-Seine, he dominated the sport in the late 1960s. He was a triple Olympic champion, winning the three alpine events at the 1968 Winter Olympics, becoming the most successful athlete there. He also won the first two World Cup titles, in 1967 and 1968.
Early life.
Killy was born in Saint-Cloud, a suburb of Paris, during the Nazi occupation of World War II, but was brought up in Val-d'Isère in the Alps, where his family had relocated in 1945 following the war. His father, Robert, was a former Spitfire pilot for the Free French, and opened a ski shop in the Savoie village, and would later operate a hotel. In 1950, his mother Madeline abandoned the family for another man, leaving Robert to raise Jean-Claude, age 7, his older sister (France), and their infant brother (Mic). Jean-Claude was sent to boarding school in Chambéry, down the valley, but he despised being shut up in a classroom.
Early career.
Killy turned his attention to skiing rather than school. His father allowed him to drop out at age 15, and he made the French national junior team a year later. As a young racer, Killy was fast, but did not usually complete his races, and the early 1960s were not entirely successful for him.
In December 1961, at age 18, Killy won his first international race, a giant slalom. The event took place in his home village of Val-d'Isere. Killy had started 39th, a position that should have been a severe disadvantage.
The French coach picked Killy for the giant slalom in the 1962 World Championships in Chamonix, France, 50 miles (80 km) away in the shadow of Mont Blanc. But Killy, unaware of his selection, was still attempting to qualify for the downhill event in northeastern Italy at Cortina d'Ampezzo. Only three weeks before the world championships, he skied in his typical reckless style. About two hundred yards (180 m) from the finish, Killy hit a stretch of ice in a compression and went down, rose immediately, then crossed the finish on just one ski—and the fastest time. Unfortunately, his other leg was broken, and he watched the 1962 World Championships on crutches.
Two years later, at age 20, Killy was entered in all three of the men's events at the 1964 Olympics, because his coach wanted to prepare him for 1968. Unfortunately, Killy was plagued by recurrences of amoebic dysentery and hepatitis, ailments that he had contracted in 1962 during a summer of compulsory service with the French Army in Algeria. His form was definitely off, and he fell a few yards after the start of the downhill, lost a binding in the slalom, and finished fifth in the giant slalom, in which he had been the heavy favorite. Yet a few weeks later, he dominated a giant slalom race at Garmisch-Partenkirchen, in Bavaria, counting for the prestigious Arlberg-Kandahar events, the oldest 'Classic' in the sport. A year later, he also triumphed at another major competition, the slalom of the Hahnenkamm races at Kitzbühel that he clinched three times in a row until 1967.
Although the first half of the decade was a relative disappointment, Killy began to strongly improve his results afterwards to become one of the best technical ski racers. In August 1966, the Frenchman, nicknamed 'Toutoune' by some of his colleagues and friends, scored his first win in a downhill race against an international field at the 1966 World Championships in Portillo, Chile, and also took gold in the combined. Killy was peaking as the first World Cup season was launched in January 1967, with the 1968 Winter Olympics in France only a year away.
Dominance – 1967–68.
World Cup results.
Individual races.
^ Results from the 1968 Winter Olympics (and 1970 World Championships) were included in the World Cup standings.
Killy was the first World Cup champion in 1967, winning 12 of 17 races to easily take the overall title. He also won the season standings in each of the three "Classic" alpine disciplines; he won all five of the downhill races and four of the five giant slalom races.
The following year, Killy won the Triple Crown of Alpine Skiing with a sweep of all three Olympic gold medals (downhill, giant slalom, and slalom) in controversial circumstances at the 1968 Winter Olympics in Grenoble, France. By finishing first in all races, he also captured the FIS world championship title in the combined event.
Killy wasn't just faster than the other skiers, he was smarter. Electrical timing by Omega was accurate to one-hundredth of a second. The starting official counted aloud, "3-2-1-Go" and the skier's boot moved forward to push a pivoting rod aside and start the timer. Everyone knew that the closer they got to the bar, the less distance they would travel. Killy, however, relied on enormous upper-body strength and outwitted his opponents. Rather than crowd as close as possible to the bar, Killy knew that he was allowed a 6-second window to push it aside. When the official began counting, he could trip the lever any time he chose from the beginning of the "3-" call and up to 3 seconds after the "Go" signal. Therefore, he rose backward, raised his body completely off the ground with his arms and poles, pulled his feet backwards, and propelled himself forward. Instead of beginning from a standing start right at the bar, as everyone else did, he hit the bar while already moving forward, giving himself a slight edge. This spectacular start certainly helped him to beat his teammate Guy Perillat by a few hundredths in the Olympic downhill despite ruining the wax covering the base of his skis moving over a plate of icy snow an hour prior to his start.
With the Olympic events included (for the only time) in the World Cup standings, Killy easily defended his title in 1968 as the overall champion, placing first in the giant slalom and second in the downhill and slalom season standings. He retired following the 1968 season, and moved to Geneva, Switzerland, in 1969.
World Championship results.
From 1948 through 1980, the Winter Olympics were also the World Championships for alpine skiing.<br>
At the World Championships from 1954 through 1980, the combined was a "paper race" using the results of the three events (DH, GS, SL).
1962: injured
Post-Olympic career.
In May 1968, Killy signed with International Management Group, the sports management firm headed by Mark McCormack. After racing on Dynamic VR17 and Rossignol skis during the part of his career when he was dominant, Killy signed a deal with Head Skis in the fall 1968 to endorse a metal and fiberglass ski named for him, the "Killy 800". Head, which was acquired by AMF the following year, manufactured a line of Killy skis for at least two years.
In television advertisements, Killy promoted the American Express card. He also became a spokesman for Schwinn bicycles, United Airlines, and Chevrolet automobiles; the latter, a role detailed by journalist Hunter S. Thompson in his 1970 article "The Temptations of Jean-Claude Killy" for "Scanlan's Monthly".
Killy starred as a ski instructor in the 1972 crime movie "Snow Job", released in the UK as "The Ski Raiders", and US TV as "The Great Ski Caper". American children in the early 1970s knew Killy from a TV commercial where he introduces himself, his thick accent making his name into ""Chocolate Kitty."" Killy played himself in the 1983 movie Copper Mountain: A Club Med Experience, starring Jim Carrey and Alan Thicke, set at Copper Mountain, Colorado.
Jean-Claude Killy also had a short career as a racing driver between 1967 and 1970, participating in several car races including at Monza. In team with fellow Frenchman Bernard Cahier, Killy was 7th overall in the 1967 Targa-Florio in a Porsche 911 S and first in the GT classification.
In November 1972, Killy came out of ski racing retirement at age 29 to compete on the pro circuit in the U.S. for two seasons. After a spirited challenge from two-time defending champion Spider Sabich, Killy won the 1973 season title, taking $28,625 in race winnings and a $40,000 bonus for the championship.
He missed the next season, won by Hugo Nindl, due to a recurring stomach ailment, then returned in the fall of 1974. Injuries slowed him and he finished well out of the 1975 standings, won by Hank Kashiwa.
In addition to trying his skill as a car racer, Killy made two television series. One, "The Killy Style", was a thirteen-week series that showcased various ski resorts, and the other, "The Killy Challenge", featured him racing against celebrities, who were all given handicaps. He was also sponsored by a champagne company, Moët & Chandon, which paid him to be seen with a bottle of their champagne on his table everywhere he went.
In 1974 Killy, as part of this sponsorship deal was paid to ski down the previously unskied eastern slope of Mt Ngauruhoe (Peter Jackson's "Mt Doom") in New Zealand. The average slope on this side of the active volcano is 35 degrees. Radar recorded his speed at over , and it took two takes, as cloud cover spoiled the first.
From 1977 to 1994, he was a member of the Executive board of the Alpine Skiing Committee of the FIS. Killy served as co-president of the 1992 Winter Olympics, held in Albertville, France, and as the President of the Société du Tour de France cycling race between 1992 and 2001. From 1995 to 2014 he was a member of the International Olympic Committee and chaired the coordination committee for Turin 2006 and Sochi 2014. He has been an Honorary Member since then.
The ski area of Val d'Isère and Tignes in the French Alps was given the name l'Espace Killy, in his honor.
Killy became Grand Officer of the Légion d'honneur in 2000.
Intrawest credits Killy with the design of a ski trail, "Cupp Run," at their Snowshoe resort in West Virginia.
Personal life.
From 1973 to 1987, he was married to French actress Danielle Gaubert, until her death from cancer. Together they had a daughter, Emilie; he also adopted her two children from her first marriage to Rhadamés Trujillo, the son of Rafael Trujillo, the assassinated dictator of the Dominican Republic. Gaubert and Trujillo were divorced in 1968 and later that year she met Killy. He was known for being friends with Russia President, Vladimir Putin [http://edition.cnn.com/2015/12/16/sport/vladimir-putin-jean-claude-killy-russia-sochi-skiing/?iid=ob_article_organicsidebar_expansion&iref=obnetwork]

</doc>
<doc id="40302" url="https://en.wikipedia.org/wiki?curid=40302" title="William Hale Thompson">
William Hale Thompson

William Hale Thompson (May 14, 1869 – March 19, 1944) was an American politician, mayor of Chicago from 1915 to 1923 and again from 1927 to 1931. Known as "Big Bill", Thompson was the last Republican to serve as Mayor of Chicago (as of 2015). He ranks among the most unethical mayors in American history.
Biography.
Thompson was born in Boston, Massachusetts to William Hale and Mary Ann Thompson, but his family moved to Chicago when he was only nine days old. Instead of college, he travelled in Europe and then took up ranching in Texas and New Mexico, returning to Chicago in 1892 after his father's death.
Thompson began his political career in 1900, when he ran for and narrowly won a position as alderman of the 2nd Ward.
In 1915 he was elected as the 41st Mayor of Chicago, and is the last Republican to be elected to that office. Early in his mayoral career, Thompson began to amass a war chest to support an eventual run for the Presidency, by charging city drivers and inspectors $3 per month. He was mayor during the Chicago Race Riot of 1919 and was said to have had control of the 75,000 African-American voters in his day.
He declined to run for reelection in 1923 and he was succeeded by William Emmett Dever. While out of office, Thompson organized a "scientific" expedition to search for tree-climbing fish in the South Seas (actually just a crude attempt to keep his name in the public eye—the expedition never got farther than New Orleans).
He ran again in 1927 during city-wide gang war. Always a flamboyant campaigner, Thompson held a debate between himself and two live rats which he used to portray his opponents. Pledging to clean up Chicago and remove the crooks, Thompson instead turned his attention to the reformers, whom he considered the real criminals. According to Thompson, the biggest enemy the United States had was King George V of the United Kingdom. Thompson promised his supporters that if they ever met, Thompson would punch the king in the nose. Al Capone's support allowed Thompson to return to the mayor's office, using such tactics as the "Pineapple Primary" which occurred April 10, 1928, so-called because of the hand grenades thrown at polling places to disrupt voting. The St. Valentine's Day Massacre also took place while Thompson was mayor.
Thompson blamed Ruth Hanna McCormick's lack of support for his loss at the 1928 Republican National Convention, and he returned the favor during her 1930 campaign for the United States Senate. Thompson had had a longstanding rivalry with the McCormicks. He intensely disliked Robert Rutherford McCormick who published the "Chicago Tribune". U.S. Senator Joseph Medill McCormick was the publisher's brother, and after his death, his widow ran against Thompson for the vacant seat.
Amid growing discontent with Thompson's leadership, particularly in the area of cleaning up Chicago's reputation as the capital of organized crime, he was defeated in 1931 by Democrat Anton Cermak. Cermak was an immigrant from Bohemia, and Thompson used this fact to belittle him with ethnic slurs such as:
Cermak replied by saying, "He doesn't like my name...It's true I didn't come over on the Mayflower, but I came over as soon as I could," which was a sentiment to which ethnic Chicagoans could relate, so Thompson's slurs largely backfired. 
After Thompson's defeat, the "Chicago Tribune" wrote that
Upon Thompson's death, two safe deposit boxes in his name were discovered to contain nearly $1.5 million in cash.

</doc>
<doc id="40303" url="https://en.wikipedia.org/wiki?curid=40303" title="Anton Cermak">
Anton Cermak

Anton Joseph "Tony" Cermak (, ; May 9, 1873 – March 6, 1933) was an American politician of Czech origin who served as the mayor of Chicago, Illinois from 1931 until his assassination in 1933.
Life.
Born in Kladno, Austria-Hungary (now in the Czech Republic), Cermak emigrated with his parents to the United States in 1874. Cermak grew up in the town of Braidwood, Illinois, southwest of Chicago, and later moved to Chicago. He began his political career as a precinct captain and in 1902 was elected to the Illinois House of Representatives. Seven years later, he would take his place as alderman of the 12th Ward. Cermak was elected president of the Cook County Board of Commissioners in 1922, chairman of the Cook County Democratic Party in 1928, and mayor of Chicago in 1931. In 1928 he ran for the United States Senate and was defeated by Republican Otis F. Glenn, receiving 46% of the vote.
Campaign for Mayor.
His mayoral victory came in the wake of the Great Depression and the deep resentment many Chicagoans had of Prohibition and the increasing violence resulting from organized crime's control of Chicago, typified by the St. Valentine's Day Massacre.
The many ethnic groups such as Poles, Czechs, Ukrainians, Jews, Italians, and African Americans that began to settle in Chicago in the early 1900s were mostly detached from the political system, due in part to lack of organization which led to underrepresentation in the City Council. As an immigrant himself, Cermak recognized Chicago's relatively new immigrants as a significant population of disenfranchised voters and a large power base for Cermak and his local Democratic organization.
Before Cermak, the Democratic party in Cook County was run by Irish Americans. As Cermak climbed the local political ladder, the resentment of the Party leadership grew. When the bosses rejected his bid to become the mayoral candidate, Cermak swore revenge. He formed his political army from the non-Irish elements, and even persuaded black politician William L. Dawson to switch from the Republican to the Democratic Party. Dawson later became U.S. Representative (from the 1st District) and soon the most powerful black politician in Illinois.
Cermak's political and organizational skills helped create one of the most powerful political organizations of his day. With support from Franklin D. Roosevelt on the national level, Cermak gradually wooed members of Chicago's growing black community into the Democratic fold. Walter Wright, the superintendent of parks and aviation for the city of Chicago also aided Cermak in stepping into office.
When Cermak challenged the incumbent "Big Bill" Thompson in the 1931 mayor's race, Thompson, representative of Chicago's existing power structure, responded with ethnic slurs:
Cermak's replied, "He doesn't like my name... it's true I didn't come over on the "Mayflower", but I came over as soon as I could." It was a sentiment to which ethnic Chicagoans could relate and Thompson's slur largely backfired. 
The flamboyant Thompson's reputation as a buffoon, the voters' disgust with the corruption of his machine, and his inability or unwillingness to clean up organized crime in Chicago, were cited as major factors in Cermak capturing 58% of the vote in the mayoral election on April 6, 1931. Cermak's victory finished Thompson as a political power and largely ended the Republican Party's power in Chicago; indeed, all the mayors of Chicago since 1931 have been members of the Democratic Party.
Mayor.
For nearly his entire administration, Cermak had to deal with a major tax revolt. From 1931 to 1933, the Association of Real Estate Taxpayers mounted a "tax strike." At its height, ARET, which was headed by John M. Pratt and James E. Bistor, had over thirty thousand members. Much to Cermak's dismay, it successfully slowed down the collection of real estate taxes through litigation and promoting refusal to pay. In the meantime, the city found it difficult to pay teachers and maintain services.
Murder.
While shaking hands with President-elect Franklin D. Roosevelt at Bayfront Park in Miami, Florida, on February 15, 1933, Cermak was shot in the lung and mortally wounded when Giuseppe Zangara, who at the time was believed to have been engaged in an attempt to assassinate Roosevelt, hit Cermak instead. At the critical moment, Lilian Cross, a doctor's wife, hit Zangara's arm with her purse and spoiled his aim. In addition to Cermak, Zangara hit four other people, one of whom, a woman, also died of her injuries. Zangara told the police that he hated rich and powerful people, but not Roosevelt personally.
Later, rumors circulated that Cermak, not Roosevelt, had been the intended target, as his promise to clean up Chicago's rampant lawlessness posed a threat to Al Capone and the Chicago organized crime syndicate. According to Roosevelt biographer Jean Edward Smith, there is no proof for this theory. One of the first people to suggest the organized crime theory was reporter Walter Winchell, who happened to be in Miami the evening of the shooting.
Long-time Chicago newsman Len O'Connor offers a different view of the events surrounding Cermak's death. He has written that aldermen "Paddy" Bauler and Charlie Weber informed him that relations between Cermak and FDR were strained because Cermak fought FDR's nomination at the Democratic convention in Chicago, and the legend that his last words were "I'm glad it was me instead of you" was, according to O'Connor, totally fabricated by Weber and Bauler.
Author Ronald Humble offers his view as to why Cermak was killed. In his book "Frank Nitti: The True Story of Chicago's Notorious Enforcer", Humble contends that Cermak was as corrupt as Thompson and that the Chicago Outfit hired Zangara to kill Cermak in retaliation for Cermak's attempt to murder Frank Nitti.
Death.
Cermak died at Jackson Memorial Hospital in Miami on March 6, partly because of his wounds. On March 30, however, his personal physician, Dr. Karl A. Meyer, said that the primary cause of Cermak's death was ulcerative colitis, commenting, "The mayor would have recovered from the bullet wound had it not been for the complication of colitis. The autopsy disclosed the wound had healed ... the other complications were not directly due to the bullet wound." 
Cermak was interred in a mausoleum at Bohemian National Cemetery in Chicago. The mayor's death was followed by a struggle for succession to his party chairmanship and to the mayor's office.
A plaque honoring Cermak still lies at the site of the assassination in Miami's Bayfront Park. It is inscribed with Cermak's alleged words to FDR after he was shot, "I'm glad it was me instead of you."
Following Cermak's death, 22nd Street, a major east-west artery that traversed Chicago's West Side and the close-in suburbs of Cicero and Berwyn, areas with a significant Czech population, was renamed Cermak Road. Zangara was electrocuted in Florida's electric chair on March 20, 1933, for he could not be charged with murder until Cermak died.
In 1943, a Liberty ship, the SS "A. J. Cermak" was named in Cermak's honor. It was scrapped in 1964.
Family.
Cermak's son-in-law, Otto Kerner Jr., served as the 33rd Governor of Illinois, and as a federal circuit judge. His grandson, Frank J. Jirka, Jr., who was with him in Miami when he was assassinated, later became a highly decorated UDT naval officer. He was awarded the Silver Star and Purple Heart for his actions during the Battle of Iwo Jima; the wounds he suffered led to the amputation of both legs below the knee. After WWII, he became a physician, and in 1983 president of the American Medical Association. Cermak's great niece, Kajon Cermak, is a broadcaster for a Southern California radio station. His daughter Lillian was married to Richey V. Graham who served in the Illinois General Assembly.

</doc>
<doc id="40304" url="https://en.wikipedia.org/wiki?curid=40304" title="Jane Byrne">
Jane Byrne

Jane Margaret Byrne (née Burke; May 24, 1933 – November 14, 2014) was an American politician who was the 50th Mayor of Chicago from April 16, 1979 to April 29, 1983. She was the first and only female mayor of Chicago, the second largest city in the United States at the time, and the largest U.S. city to have had a female mayor to date. Byrne first entered politics to volunteer in John F. Kennedy's campaign for president in 1960. During that campaign she first met Mayor Richard J. Daley. 
In 1968, Daley appointed her head of Chicago's consumer affairs department. Byrne held that post until she was fired by Mayor Michael Bilandic in 1977. She challenged Bilandic in the 1979 Democratic mayoral primary, the real contest in this heavily Democratic city. At first, political observers believed her to have little chance of winning. A memorandum inside the Bilandic campaign said it should portray her as, “a shrill, charging, vindictive person — and nothing makes a woman look worse.” However, a series of major snowstorms in January paralyzed the city and caused Bilandic to be seen as an ineffective leader. Jesse Jackson endorsed Byrne. Many Republican voters voted in the Democratic primary to beat Bilandic and the "Machine". Infuriated voters in the North Side and Northwest Side retaliated against Bilandic for the Democratic Party's slating of only South Side candidates for the mayor, clerk, and treasurer (the outgoing city clerk, John C. Marcin, was from the Northwest Side). These four factors combined to give Byrne a razor-thin 51% to 49% victory over Bilandic in the primary. She then won the general election with 82% of the vote, still the largest margin in a Chicago mayoral election. 
Mayor of Chicago (1979–1983).
Byrne positioned herself as a reformer in her first campaign. She made inclusive moves as mayor, such as hiring the first African-American and woman school superintendent Ruth B. Love, and she was the first mayor to recognize the gay community. In March 1981, she moved into the crime-ridden Cabrini–Green Homes housing project for a 3-week period to bring attention and resources to its high crime rate. In her first three months in office, she faced strikes by labor unions as the city’s transit workers, public school teachers and firefighters all went on strike. She effectively banned handgun possession for guns unregistered or purchased after the enactment of an ordinance instituting a two-year re-registration program. Byrne used special events, such as ChicagoFest, to revitalize Navy Pier and the downtown Chicago Theatre. She endorsed Senator Edward Kennedy for president in 1980, but could not stop President Jimmy Carter from winning the Illinois Democratic Primary. However, her attempt to block the election of Richard M. Daley, the son of her late mentor, to the prominent position of Cook County States' Attorney (chief local prosecutor) in 1980 failed as Daley defeated Byrne's candidate, 14th Ward Alderman Edward M. Burke in the Democratic Primary and GOP incumbent Bernard Carey in the general election. In 1982, she supported the Cook County Democratic Party's replacement of its chairman, County Board President George Dunne, with her city-council ally, Alderman Edward Vrdolyak.
The "Chicago Sun Times" reported that her enemies publicly mocked her as “that crazy broad” and “that skinny bitch” and worse.
On November 11, 1981, Dan Goodwin, who had successfully climbed the Sears Tower, battled for his life on the side of the John Hancock Center. William Blair, Chicago's fire commissioner, had ordered the Chicago Fire Department to stop Goodwin by directing a full power fire hose at him and by using fire axes to break window glass in Goodwin's path. Mayor Byrne rushed to the scene and ordered the fire department to stand down. Then, through a smashed out 38th floor window, she told Goodwin, who was hanging from the building's side a floor below, that though she did not agree with his climbing of the John Hancock Center, she certainly opposed the fire department knocking him to the ground below. Byrne then allowed Goodwin to continue to the top.
1983 Democratic primary.
Byrne was narrowly defeated in the 1983 Democratic primary for mayor by Harold Washington; the younger Daley ran a close third. Washington won the Democratic primary with just 36% of the vote; Byrne had 33%. Washington went on to win the general election.
Later career.
Byrne ran against Washington again in the 1987 Democratic primary, but was narrowly defeated. She endorsed Washington for the general election, in which he defeated two Democrats running under other parties' banners (Edward Vrdolyak and Thomas Hynes) and a Republican.
Byrne next ran in the 1988 Democratic primary for Cook County Circuit Court Clerk. She faced the Democratic Party's slated candidate, Aurelia Pucinski (who was endorsed by Mayor Washington and is the daughter of then-Alderman Roman Pucinski). Pucinski defeated Byrne in the primary and Vrdolyak, by then a Republican, in the general election.
Byrne's fourth run for mayor involved a rematch against Daley in 1991. Byrne received only 5.9% of the vote, a distant third behind Daley and Alderman Danny K. Davis.
Personal life.
Byrne was born Jane Margaret Burke on May 24, 1933. In 1956, she married William P. Byrne, a Marine. The couple had a daughter, Katherine C. Byrne (born 1957). On May 31, 1959, while flying from Marine Corps Air Station Cherry Point to Naval Air Station Glenview in a Skyraider, Lt. Byrne attempted to land in a dense fog. After being waved off for landing twice, his plane's wing struck the porch of a nearby house and the plane crashed into Sunset Memorial Park, killing him. Byrne married journalist Jay McMullen in 1978, and they remained married until his death from lung cancer in 1992. Byrne lived in the same apartment building from the 1970s until her death in 2014. She has one grandchild, Willie. Her daughter, Kathy, is a lawyer with a Chicago firm. Mayor Byrne's book, "My Chicago" (ISBN 0-8101-2087-9), was published in 1992, and covers her life through her political career. On May 16, 2011, Byrne attended the inauguration of the city's new mayor, Rahm Emanuel.
Death and legacy.
Byrne had entered hospice care and died on November 14, 2014 in Chicago, aged 81, from complications of a stroke she suffered in January 2013. She was survived by her daughter Katherine and her grandson Willie. Her funeral Mass was held at St. Vincent de Paul on Monday, November 17, 2014. She was buried at Interment Calvary Cemetery in Evanston, Illinois. In a dedication ceremony held on August 29, 2014, Governor Pat Quinn renamed the Circle Interchange in Chicago the Jane Byrne Interchange. In July 2014, the Chicago City Council voted to rename the plaza surrounding the historic Chicago Water Tower on North Michigan Avenue the Jane M. Byrne Plaza in her honor.

</doc>
<doc id="40305" url="https://en.wikipedia.org/wiki?curid=40305" title="Harold Washington">
Harold Washington

Harold Lee Washington (April 15, 1922  – November 25, 1987) was an American lawyer and politician elected as the 51st Mayor of Chicago in February 1983. He was the first African-American to serve as Mayor of Chicago, in office from April 29, 1983 until his death on November 25, 1987. Washington was also a member of the U.S. House of Representatives from 1981 to 1983 representing the Illinois first district, and also previously served in the Illinois State Senate and the Illinois House of Representatives from 1965 until 1976.
Early years.
Harold Washington was born on April 15, 1922, to Roy and Bertha Washington. His father had been one of the first precinct captains in the city, a lawyer and a Methodist minister. His mother, Bertha, left a small farm near Centralia, Illinois, to seek her fortune in Chicago as a singer. She married Roy Washington soon after arriving in Chicago and the couple had three children, one named Kevin and the other named Ramon Price (from a later marriage), who was a former artist and eventually became chief curator of The DuSable Museum of African American History.
Washington grew up in Bronzeville, a Chicago neighborhood that was the center of black culture for the entire Midwest in the early and middle 20th century. Washington attended DuSable High School, then a newly established racially segregated public high school, and was a member of its first graduating class. In a 1939 citywide track meet, Washington placed first in the 110 meter high hurdles event, and second in the 220 meter low hurdles event. Between his junior and senior year of high school, Washington dropped out, claiming that he no longer felt challenged by the coursework.
He worked at a meat-packing plant for a time before his father helped him get a job at the U.S. Treasury branch in the city. There he met Dorothy Finch, whom he married soon after; Washington was 20 years old and Dorothy was 17 years old. Seven months later, the U.S. was drawn into World War II with the bombing of Pearl Harbor by the Japanese on Sunday, December 7, 1941.
Military service.
In 1942, Washington was drafted into the United States Army for the war effort and after basic training, sent overseas as part of a racially segregated unit of the U.S. Army Air Corps unit of Engineers. After the American invasion of the Philippines in 1944, on Leyte Island and later the main Luzon island, Washington was part of a unit building runways for bombers, protective fighter aircraft, refueling planes, and returning damaged aircraft. Eventually, Washington rose to the rank of First Sergeant in the Army Air Corps (later in the war renamed the U.S. Army Air Forces).
Roosevelt College.
In the summer of 1946, Washington, aged 24 and a war veteran, enrolled at Roosevelt College (now Roosevelt University). Washington joined other groups of students not permitted to enroll in other local colleges. Local estimates placed the student population of Roosevelt College at about 1/8 black and 1/2 Jewish. A full 75% of the students had enrolled because of the "nondiscriminatory progressive principles." He chaired a fund-raising drive by students, and then was named to a committee that supported city-wide efforts to outlaw "restrictive covenants" in housing, the legal means by which minorities (especially blacks ("negroes") and, to a lesser extent, Jews) were prohibited from purchasing real estate in predominantly white neighborhoods of the city.
In 1948, after the college had moved to the Auditorium Building, Washington was elected the third president of Roosevelt's student council. Under his leadership, the student council successfully petitioned the college to have student representation on Roosevelt's faculty committees. At the first regional meeting of the newly- founded National Student Association in the spring of 1948, Washington and nine other delegates proposed student representation on college faculties, and a "Bill of Rights" for students; both measures were roundly defeated.
The next year, Washington went to the state capital at Springfield to protest Illinois legislators' coming probe of "subversives". The probe of investigation would outlaw the Communist Party and require "loyalty oaths" for teachers. He led students' opposition to the bills, although they would pass later in 1949.
During his Roosevelt College years, Washington came to be known for his stability. His friends said that he had a "remarkable ability to keep cool", reason carefully and walk a middle line. Washington intentionally avoided extremist activities, including street actions and sit-ins against racially segregated restaurants and businesses. Overall, Washington and other radical activists ended up sharing a mutual respect for each other, acknowledging both Washington's pragmatism and the activists' idealism. With the opportunities found only at Roosevelt College in the late 1940s, Washington's time at the Roosevelt College proved to be pivotal. Washington graduated in August 1949, with a Bachelor of Arts (B.A.) degree. In addition to his activities at Roosevelt, he was a member of Phi Beta Sigma fraternity.
Northwestern University School of Law.
Washington then applied and was admitted to study law at the Northwestern University School of Law in Chicago. During this time, Washington was divorced from Dorothy Finch. By some accounts, Harold and Dorothy had simply grown apart after Washington was sent overseas during the war during the first year of his marriage. Others saw both as young and headstrong, the relationship doomed from the beginning. Another friend of Washington's deemed Harold "not the marrying kind." He would not marry again, but continued to have relationships with other women; his longtime secretary is said to have said, "If every woman Harold slept with stood at one end of City Hall, the building would sink five inches into LaSalle Street!".
At Northwestern Law School, Washington was the only black student in his class (there were six women in the class, one of them being Dawn Clark Netsch). As at Roosevelt, he entered school politics. In 1951, his last year, he was elected treasurer of the Junior Bar Association (JBA). The election was largely symbolic, however, and Washington's attempts to give the JBA more authority at Northwestern were largely unsuccessful. On campus, Washington joined the Nu Beta Epsilon fraternity, largely because he and the other minorities which constituted the fraternity were blatantly excluded from the other fraternities on campus. Overall, Washington stayed away from the activism that defined his years at Roosevelt. During the evenings and weekends, he worked to supplement his GI Bill income. He received his J.D. in 1952.
Legislative political career.
Working for Metcalfe.
From 1951 until he was first slated for election in 1965, Washington worked in the offices of the 3rd Ward Alderman, former Olympic athlete Ralph Metcalfe. Richard J. Daley was elected party chairman in 1952. Daley replaced C.C. Wimbush, an ally of William Dawson, on the party committee with Metcalfe. Under Metcalfe, the 3rd Ward was a critical factor in Mayor Daley's 1955 mayoral election victory and ranked first in the city in the size of its Democratic plurality in 1961. While working under Metcalfe, Washington began to organize the 3rd Ward's Young Democrats (YD) organization. At YD conventions, the 3rd Ward would push for numerous resolutions in the interest of blacks. Eventually, other black YD organizations would come to the 3rd Ward headquarters for advice on how to run their own organizations. Like he had at Roosevelt College, Washington avoided radicalism and preferred to work through the party to engender change. While working with the Young Democrats, Washington met Mary Ella Smith. They dated for the next 20 years, and in 1983 Washington proposed to Smith. In an interview with the Chicago Sun-Times, Smith said that she never pressed Washington for marriage because she knew Washington's first love was politics, saying, "He was a political animal. He thrived on it, and I knew any thoughts of marriage would have to wait. I wasn't concerned about that. I just knew the day would come."
In 1960, with Lemuel Bentley, Bennett Johnson, Luster Jackson and others, Washington founded the Chicago League of Negro Voters, one of the first African-American political organizations in the city. In its first election, Bentley drew 60,000 votes for city clerk. After dropping out of view after the elections, it resurfaced as the group Protest at the Polls in 1963. Washington participated in the planning process to further the goals of 3rd Ward YDs. By 1967, the independent candidates had gained traction within the black community, winning several aldermanic seats; in 1983, the League of Negro Voters were instrumental in Washington's run for mayor. By then, the YDs were losing influence in the party, as more black voters supported independent candidates.
Illinois House (1965–1976).
After the state legislature failed to reapportion districts as required by the census every ten years, an at-large election was held in January 1965 to elect 177 representatives. With the Republicans and Democrats combining to slate only 118 candidates, independent voting groups seized the opportunity to slate candidates. The League of Negro Voters created a "Third Slate" of 59 candidates, announcing the slate on June 27, 1964. Shortly afterwards, Daley put together a slate including Adlai Stevenson III and Washington. The Third Slate was then thrown out by the Illinois Election Board because of "insufficient signatures" on the nominating petitions. In the election, Washington received the second-largest amount of ballots, behind Stevenson.
Washington's years in the Illinois House were marked by tension with Democratic Party leadership. In 1967, he was ranked by the Independent Voters of Illinois (IVI) as the fourth-most independent legislator in the Illinois House and named Best Legislator of the Year. His defiance of the "idiot card", a sheet of paper that directed legislators' votes on every issue, attracted the attention of party leaders, who moved to remove Washington from his legislative position. Daley often told Metcalfe to dump Washington as a candidate, but Metcalfe did not want to risk losing the 3rd Ward's Young Democrats, who were mostly aligned with Washington. Washington backed Renault Robinson, a black police officer and one of the founders of the Afro-American Patrolmen’s League (AAPL). The aim of the AAPL was to fight racism directed against minority officers by the rest of the predominantly white department. Soon after the creation of the group, Robinson was written up for minor infractions, suspended, reinstated, and then placed on the graveyard shift to a single block behind central police headquarters. Robinson approached Washington to fashion a bill creating a civilian review board, consisting of both patrolmen and officers, to monitor police brutality. Both black independent and white liberal legislators refused to back the bill, afraid to challenge Daley's grip on the police force.
After Washington announced he would support the AAPL, Metcalfe refused to protect him from Daley. Washington believed he had the support of John Touhy, Speaker of the House and a former party chair. Instead, Touhy criticized Washington and then allayed Daley's anger. In exchange for the party's backing, Washington would serve on the Chicago Crime Commission, the group Daley formed to investigate the AAPL's charges. The commission promptly found the AAPL's charges "unwarranted". An angry and humiliated Washington admitted that on the commission, he felt like Daley's "showcase nigger". In 1969, Daley removed Washington's name from the slate; only by the intervention of Cecil Partee, a party loyalist, was Washington reinstated. The Democratic Party supported Jim Taylor, a former professional boxer, Streets and Sanitation worker, over Washington. With Partee and his own ward's support, Washington defeated Taylor.
His years in the House of Representatives were focused on becoming an advocate for black rights. He continued work on the Fair Housing Act, and worked to strengthen the state's Fair Employment Practices Commission (FEPC). In addition, he worked on a state Civil Rights Act, which would strengthen employment and housing provisions in the federal Civil Rights Act of 1964. In his first session, all of his bills were sent to committee or tabled. Like his time in Roosevelt College, Washington relied on parliamentary tactics (e.g., writing amendments guaranteed to fail in a vote) to enable him to bargain for more concessions.
Washington also passed bills honoring civil rights figures. He passed a resolution honoring Metcalfe, his mentor. He also passed a resolution honoring James J. Reeb, a Unitarian minister who was beaten to death in Selma, Alabama by a segregationist mob. After the assassination of Martin Luther King, Jr., he introduced a bill aimed at making King's birthday a state holiday; it was tabled and later vetoed. It was not until 1973 that Washington was able, with Partee's help in the Senate, to have the bill enacted and signed by the governor. In 1975, Washington was named chairman of the Judiciary Committee with the election of William A. Redmond as Speaker of the House. The same year, Partee, now President of the Senate and eligible for his pension, decided to retire from the Senate. Although Daley and Taylor declined at first, at Partee's insistence, Washington was slated for the seat and received the party's support. In 1976, Washington was elected to the Illinois Senate.
Legal issues.
In addition to Daley's strong-armed tactics, Washington's time in the Illinois House was also marred by problems with tax returns and allegations of not performing services owed to his clients. In her biography, Levinsohn questions whether the timing of Washington's legal troubles was politically motivated. In November 1966, Washington was re-elected to the house over Daley's strong objections; the first complaint was filed in 1964; the second was filed by January 1967.
A letter asking Washington to explain the matter was sent on January 5, 1967. After failing to respond to numerous summons and subpoenas, the commission recommend a five-year suspension on March 18, 1968. A formal response to the charges did not occur until July 10, 1969. In his reply, Washington said that "sometimes personal problems are enlarged out of proportion to the entire life picture at the time and the more important things are abandoned." In 1970, the Board of Managers of the Chicago Bar Association ruled that Washington's license be suspended for only one year, not the five recommended; the total amount in question between all six clients was $205.
In 1971, Washington was charged with failure to file tax returns for four years, although the Internal Revenue Service (IRS) claimed to have evidence for nineteen years; top campaign aides later said that nineteen was closer to the truth. Judge Sam Perry noted that he was "disturbed that this case ever made it to my courtroom" — while Washington had paid his taxes, he ended up owing the government a total of $508 as a result of not filing his returns. Typically, the IRS handled such cases in civil court, or within its bureaucracy. Washington pleaded "no contest" and was sentenced to forty days in Cook County Jail, a $1,000 fine, and three years probation.
Illinois Senate (1976–1980).
Human Rights Act of 1980.
In the Illinois Senate, Washington's main focus worked to pass 1980's Illinois Human Rights Act. Legislators rewrote all of the human rights laws in the state, restricting discrimination based on "race, color, religion, sex, national origin, ancestry, age, marital status, physical or mental disability, military status, sexual orientation, or unfavorable discharge from military service in connection with employment, real estate transactions, access to financial credit, and the availability of public accommodations." The bill's origins began in 1970 with the rewriting of the Illinois Constitution. The new constitution required all governmental agencies and departments to be reorganized for efficiency. Republican governor James R. Thompson reorganized low-profile departments before his re-election in 1978. In 1979, during the early stages of his second term and immediately in the aftermath of the largest vote for a gubernatorial candidate in the state's history, Thompson called for human rights reorganization. The bill would consolidate and remove some agencies, eliminating a number of political jobs. Some Democratic legislators would oppose any measure backed by Washington, Thompson and other Republican legislators.
For many years, human rights had been a campaign issue brought up and backed by Democrats. Thompson's staffers brought the bill to Washington and other black legislators before it was presented to the legislature. Washington made adjustments in anticipation of some legislators' concerns regarding the bill, before speaking for it in April 1979. On May 24, 1979, the bill passed the Senate by a vote of 59 to one, with two voting present and six absent. The victory in the Senate was attributed by a Thompson staffer to Washington's "calm noncombative presentation".
However, the bill stalled in the house. State Representative Susan Catania insisted on attaching an amendment to allow women guarantees in the use of credit cards. This effort was assisted by Carol Moseley Braun, a representative from Hyde Park. State Representatives Jim Taylor and Larry Bullock introduced over one hundred amendments, including the text of the first ten amendments to the U.S. Constitution, to try to stall the bill. With Catania's amendment, the bill passed the House, but the Senate refused to accept the amendment. On June 30, 1979, the legislature adjourned.
U.S. House (1980–1983).
In 1980, Washington was elected to the U.S. House of Representatives in Illinois' 1st Congressional District. He defeated incumbent Representative Bennett Stewart in the Democratic primary. Anticipating that the Democratic Party would challenge him in his bid for re-nomination in 1982, Washington spent much of his first term campaigning for re-election, often travelling back to Chicago to campaign. Washington missed many House votes, an issue that would come up in his campaign for mayor in 1983. Washington's major congressional accomplishment involved legislation to extend the Voting Rights Act, legislation that opponents had argued was only necessary in an emergency. Others, including Congressman Henry Hyde, had submitted amendments designed to seriously weaken the power of the Voting Rights Act.
Although he had been called "crazy" for railing in the House of Representatives against deep cuts to social programs, Associated Press political reporter Mike Robinson noted that Washington worked "quietly and thoughtfully" as the time came to pass the act. During hearings in the South regarding the Voting Rights Act, Washington asked questions that shed light on tactics used to prevent African Americans from voting (among them, closing registration early, literacy tests, and gerrymandering). After the amendments were submitted on the floor, Washington spoke from prepared speeches that avoided rhetoric and addressed the issues. As a result, the amendments were defeated, and Congress passed the Voting Rights Act Extension. By the time Washington faced re-election in 1982, he had cemented his popularity in the 1st Congressional District. Jane Byrne could not find one serious candidate to run against Washington for his re-election campaign. He had collected 250,000 signatures to get on the ballot, although only 610 signatures (0.5% of the voters in the previous election) were required. With his re-election to Congress locked up, Washington turned his attention to the next Chicago mayoral election.
Mayor of Chicago (1983–1987).
In the February 22, 1983, Democratic mayoral primary, more than 100,000 new voters registered to vote. On the North and Northwest Sides, the incumbent mayor Jane Byrne led and future mayor Richard M. Daley, son of the late Mayor Richard J. Daley, finished a close second. Harold Washington had massive majorities on the South and West Sides. Southwest Side voters overwhelmingly supported Daley. Washington won with 37% of the vote, versus 33% for Byrne and 30% for Daley.
Although winning the Democratic primary is normally tantamount to election in heavily Democratic Chicago, after his primary victory Washington found that his Republican opponent, former state legislator Bernard Epton (earlier considered a nominal stand-in), was supported by many high-ranking Democrats and their ward organizations, including the chairman of the Cook County Democratic Party, Alderman Edward "Fast Eddie" Vrdolyak. Epton's campaign referred to, among other things, Washington's conviction for failure to file income tax returns (he had paid the taxes, but had not filed a return). Washington, on the other hand, stressed reforming the Chicago patronage system and the need for a jobs program in a tight economy. In the April 12, 1983, mayoral general election, Washington defeated Epton by 3.7%, 51.7% to 48.0%, to become mayor of Chicago. Washington was sworn in as mayor on April 29, 1983, and resigned his Congressional seat the following day.
During his tenure as mayor, Washington lived at the Hampton House apartments in the Hyde Park neighborhood of Chicago. He created the city's first environmental-affairs department under the management of longtime Great Lakes environmentalist Lee Botts. Washington's first term in office was characterized by conflict with the city council dubbed "Council Wars", referring to the then-recent "Star Wars" films and caused Chicago to be nicknamed "Beirut on the Lake". A 29-alderman City Council majority refused to enact Washington's legislation and prevented him from appointing nominees to boards and commissions. First-term challenges included city population loss, increased crime, and a massive decrease in ridership on the Chicago Transit Authority (CTA).
The 29, also known as the "Vrdolyak 29", were led by Alderman Ed Vrdolyak and Finance Chair Edward Burke. Parks superintendent Edmund Kelly also opposed the mayor. The three were known as "the Eddies" and were supported by the younger Daley (now State's Attorney), U.S. Congressmen Dan Rostenkowski and William Lipinski, and much of the Democratic Party. During his first city council meeting, Washington and the 21 supportive aldermen walked out of the meeting after a quorum had been established. Vrdolyak and the other 28 then chose committee chairmen and assigned aldermen to the various committees. Later lawsuits submitted by Washington and others were dismissed because it was determined that the appointments were legally made.
Washington ruled by veto. The 29 lacked the 30th vote they needed to override Washington's veto; female and African American aldermen supported Washington despite pressure from the Eddies. Meanwhile, in the courts, Washington kept the pressure on to reverse the redistricting of city council wards that the city council had created during the Byrne years. During special elections in 1986, victorious Washington-backed candidates in the first round ensured at least 24 supporters in the city council. Six weeks later, when Marlene Carter and Luís Gutiérrez won run-off elections, Washington had the 25 aldermen he needed. His vote as chairman of the City Council enabled him to break 25-25 tie-votes and enact his programs.
Washington defeated former mayor Jane Byrne in the February 24, 1987 Democratic mayoral primary by 7.2%, 53.5% to 46.3%, and in the April 7, 1987 mayoral general election defeated Vrdolyak (Illinois Solidarity Party) by 11.8%, 53.8% to 42.8%, with Northwestern University business professor Donald Haider (Republican) getting 4.3%, to win reelection to a second term as mayor. Cook County Assessor Thomas Hynes (Chicago First Party), a Daley ally, dropped out of the race 36 hours before the mayoral general election. During Washington's short second term, the Eddies fell from power: Vrdolyak became a Republican, Kelly was removed from his powerful parks post, and Burke lost his Finance Committee chairmanship.
Harold Washington's Political Education Project.
From 1984 to 1987, Harold Washington's Political Education Project (PEP) served as Washington’s political arm, organizing both Washington’s campaigns and the campaigns of his political allies.
Harold Washington established the Political Education Project in 1984. This organization supported Washington’s interests in electoral politics beyond the Office of the Mayor. PEP helped organize political candidates for statewide elections in 1984 and managed Washington's participation in the 1984 Democratic National Convention as a "favorite son" presidential candidate.
PEP used its political connections to support candidates such as Luís Gutiérrez and Jesús "Chuy" García through field operations, voter registration and Election Day poll monitoring. Once elected, these aldermen helped break the stalemate between Washington and his opponents in the city council. Due to PEP’s efforts, Washington’s City Council legislation gained ground and his popularity grew as the 1987 mayoral election approached.
In preparation for the 1987 mayoral election, PEP formed the Committee to Re-Elect Mayor Washington. This organization carried out fundraising for the campaign, conducted campaign events, and coordinated volunteers. PEP staff members, such as Joseph Gardner and Helen Shiller, went on to play leading roles in Chicago politics. The organization disbanded upon Harold Washington’s death.
Harold Washington's Political Education Project Records is an archival collection detailing the organization's work. It is located in the Chicago Public Library Special Collections, Harold Washington Library Center, Chicago, Illinois.
Death.
On November 25, 1987, at 11:00 a.m., Chicago Fire Department paramedics were called to City Hall. Washington's press secretary, Alton Miller, had been discussing school board issues with the mayor when Washington suddenly slumped over on his desk, falling unconscious. After failing to revive Washington in his office, paramedics rushed him to Northwestern Memorial Hospital. Further resuscitation attempts failed, and Washington was pronounced dead at 1:36 p.m. At Daley Plaza, Patrick Keen, project director for the Westside Habitat for Humanity, announced Washington's official time of death to a separate gathering of Chicagoans. Initial reactions to the pronouncement of his death were of shock and sadness, as many blacks believed that Washington was the only top Chicago official who would address their concerns. Thousands of Chicagoans attended his wake in the lobby of City Hall between November 27 and November 29, 1987. On December 1, 1987, Rev. B. Herbert Martin officiated Washington's funeral service in Christ Universal Temple at 119th Street and Ashland Avenue in Chicago. After the service, Washington was buried in Oak Woods Cemetery on the South Side of Chicago.
Immediately after Washington's death, rumors about how Washington died began to surface. On January 6, 1988, Dr. Antonio Senat, Washington's personal physician, denied "unfounded speculations" that Washington had cocaine in his system at the time of his death, or that foul play was involved. Cook County Medical Examiner Robert J. Stein performed an autopsy on Washington and concluded that Washington had died of a heart attack. Washington had weighed , and suffered from hypertension, high cholesterol levels, and an enlarged heart. On June 20, 1988, Alton Miller again indicated that drug reports on Washington had come back negative, and that Washington had not been poisoned prior to his death. Dr. Stein stated that the only drug in Washington's system had been lidocaine, which is used to stabilize the heart after a heart attack takes place. The drug was given to Washington either by paramedics, or by doctors at Northwestern Memorial Hospital.
School of the Art Institute of Chicago student David Nelson painted "Mirth & Girth", a caricature that depicted Washington wearing women's lingerie and holding a pencil, which was briefly displayed in a hallway at the school on May 11, 1988. The painting kicked off a First Amendment and civil rights controversy between Art Institute students and black aldermen. Nelson and the ACLU eventually split a US$95,000 (1994, US$138,000 in 2008) settlement from the city. Coincidentally, Bernard Epton, Washington's opponent in the 1983 general election, followed him in death just 18 days later, on December 13, 1987.
Legacy.
Despite the bickering in City Council, Washington seemed to relish his role as Chicago's ambassador to the world. At a party held shortly after his re-election on April 7, 1987, he said to a group of supporters, "In the old days, when you told people in other countries that you were from Chicago, they would say, 'Boom-boom! Rat-a-tat-tat!' Nowadays, they say joins with him, 'How's Harold?'!"
In later years, various city facilities and institutions were named or renamed after the late mayor to commemorate his legacy. The new building housing the main branch of the Chicago Public Library, located at 400 South State Street, was named the Harold Washington Library Center. The Chicago Public Library Special Collections, located on the building's 9th floor, house the Harold Washington Archives and Collections. These archives hold numerous collections related to Harold Washington's life and political career.
Five months after Mayor Washington's sudden death in office, a ceremony was held on April 19, 1988, changing the name of Loop College, one of the City Colleges of Chicago, to Harold Washington College. Harold Washington Elementary School in Chicago's Chatham neighborhood is also named after the former mayor. In August 2004, the Harold Washington Cultural Center was opened to the public in the Bronzeville neighborhood. Across from the Hampton House apartments where Washington lived, a city park was renamed Harold Washington Park, which was known for "Harold's Parakeets", a colony of feral monk parakeets that inhabited Ash Trees in the park. A building on the campus of Chicago State University is named Harold Washington Hall.
External links.
88558

</doc>
<doc id="40307" url="https://en.wikipedia.org/wiki?curid=40307" title="Plasma stability">
Plasma stability

An important field of plasma physics is the stability of the plasma. It usually only makes sense to analyze the stability of a plasma once it has been established that the plasma is in equilibrium. "Equilibrium" asks whether there are not forces that will accelerate any part of the plasma. If there are not, then "stability" asks whether a small perturbation will grow, oscillate, or be damped out.
In many cases a plasma can be treated as a fluid and its stability analyzed with magnetohydrodynamics (MHD). MHD theory is the simplest representation of a plasma, so MHD stability is a necessity for stable devices to be used for nuclear fusion, specifically magnetic fusion energy. There are, however, other types of instabilities, such as velocity-space instabilities in magnetic mirrors and systems with beams. There are also rare cases of systems, e.g. the Field-Reversed Configuration, predicted by MHD to be unstable, but which are observed to be stable, probably due to kinetic effects.
Plasma instabilities.
Plasma instabilities can be divided into two general groups:
Plasma instabilities are also categorised into different modes:
Source: Andre Gsponer, ""Physics of high-intensity high-energy particle beam propagation in open air and outer-space plasmas"" (2004)
MHD Instabilities.
Beta is a ratio of the plasma pressure over the magnetic field strength. 
formula_1 
MHD stability at high beta is crucial for a compact, cost-effective magnetic fusion reactor. Fusion power density varies roughly as formula_2 at constant magnetic field, or as formula_3 at constant bootstrap fraction in configurations with externally driven plasma current. (Here formula_4 is the normalized beta.) In many cases MHD stability represents the primary limitation on beta and thus on fusion power density. MHD stability is also closely tied to issues of creation and sustainment of certain magnetic configurations, energy confinement, and steady-state operation. Critical issues include understanding and extending the stability limits through the use of a
variety of plasma configurations, and developing active means for reliable operation near those limits. Accurate predictive capabilities are needed, which will require the addition of new physics to existing MHD models. Although a wide range of magnetic configurations exist, the underlying MHD physics is common to all. Understanding of MHD stability gained in one configuration can benefit others, by verifying analytic theories, providing benchmarks for predictive MHD stability codes, and advancing the development of active control techniques.
The most fundamental and critical stability issue for magnetic fusion is simply that MHD instabilities often limit performance at high beta. In most cases the important instabilities are long wavelength, global modes, because of their ability to cause severe degradation of energy confinement or termination of the plasma. Some important examples that are common to many magnetic configurations are ideal kink modes, resistive wall modes, and neoclassical tearing modes. A possible consequence of violating stability boundaries is a disruption, a sudden loss of thermal energy often followed by termination of the discharge. The key issue thus includes understanding the nature of the beta limit in the various configurations, including the associated thermal and magnetic stresses, and finding ways to avoid the limits or mitigate the consequences. A wide range of approaches to preventing such instabilities is under investigation, including optimization of the configuration of the plasma and its confinement device, control of the internal structure of the plasma, and active control of the MHD instabilities.
Ideal Instabilities.
Ideal MHD instabilities driven by current or pressure gradients represent
the ultimate operational limit for most configurations. The long-wavelength kink mode and short-wavelength
ballooning mode limits are generally well understood and can in principle be avoided.
Intermediate-wavelength modes (n ~ 5–10 modes encountered in tokamak edge plasmas, for
example) are less well understood due to the computationally intensive nature of the stability
calculations. The extensive beta limit database for tokamaks is consistent with ideal MHD stability limits, yielding agreement to within about 10% in beta for cases where the internal profiles of the
plasma are accurately measured. This good agreement provides confidence in ideal stability
calculations for other configurations and in the design of prototype fusion reactors.
Resistive Wall Modes.
Resistive wall modes (RWM) develop in plasmas that require the presence of a perfectly conducting wall for stability. RWM stability is a key issue for many magnetic configurations. Moderate beta values are possible without a nearby wall in the tokamak, stellarator, and other configurations, but a nearby conducting wall can significantly improve ideal kink mode stability in most configurations, including the tokamak, ST, reversed field pinch (RFP), spheromak, and possibly the FRC. In the advanced tokamak and ST, wall stabilization is critical for operation with a large bootstrap fraction. The spheromak requires wall stabilization to avoid the low-m,n tilt and shift modes, and possibly bending modes. However, in the presence of a non-ideal wall, the slowly growing RWM is unstable. The resistive wall mode has been a long-standing issue for the RFP, and has more recently been observed in tokamak experiments. Progress in understanding the physics of the RWM and developing the means to stabilize it could be directly applicable to all magnetic configurations. A closely related issue is to understand plasma rotation, its sources and sinks, and its role in stabilizing the RWM.
Resistive instabilities.
Resistive instabilities are an issue for all magnetic configurations, since the onset can occur at beta values well below the ideal limit. The stability of neoclassical tearing modes (NTM) is a key issue for magnetic configurations with a strong bootstrap current. The NTM is a metastable mode; in certain plasma configurations, a sufficiently large deformation of the bootstrap current produced by a “seed island” can contribute to the growth of the island. The NTM is already an important performance-limiting factor in many tokamak experiments, leading to degraded confinement or disruption. Although the basic mechanism is well established, the capability to predict the onset in present and future devices requires better understanding of the damping mechanisms which determine the threshold island size, and of the mode coupling by which other instabilities (such as sawteeth in tokamaks) can generate seed islands. Resistive Ballooning Mode, similar to ideal ballooning, but with finite resistivity taken into consideration, provides another example of a resistive instability.
Opportunities for Improving MHD Stability.
Configuration.
The configuration of the plasma and its confinement device represent an
opportunity to improve MHD stability in a robust way. The benefits of discharge shaping and low
aspect ratio for ideal MHD stability have been clearly demonstrated in tokamaks and STs, and will
continue to be investigated in experiments such as DIII-D, Alcator C-Mod, NSTX, and MAST. New
stellarator experiments such as NCSX (proposed) will test the prediction that addition of
appropriately designed helical coils can stabilize ideal kink modes at high beta, and lower-beta tests
of ballooning stability are possible in HSX. The new ST experiments provide an opportunity to
test predictions that a low aspect ratio yields improved stability to tearing modes, including
neoclassical, through a large stabilizing “Glasser effect” term associated with a large Pfirsch-Schlüter
current. Neoclassical tearing modes can be avoided by minimizing the bootstrap current in
quasi-helical and quasi-omnigenous stellarator configurations. Neoclassical tearing modes are also
stabilized with the appropriate relative signs of the bootstrap current and the magnetic shear; this
prediction is supported by the absence of NTMs in central negative shear regions of tokamaks.
Stellarator configurations such as the proposed NCSX, a quasi-axisymmetric stellarator design,
can be created with negative magnetic shear and positive bootstrap current to achieve stability to the
NTM. Kink mode stabilization by a resistive wall has been demonstrated in RFPs and tokamaks,
and will be investigated in other configurations including STs (NSTX) and spheromaks (SSPX).
A new proposal to stabilize resistive wall modes by a flowing liquid lithium wall needs further
evaluation.
Internal Structure.
Control of the internal structure of the plasma allows more active
avoidance of MHD instabilities. Maintaining the proper current density profile, for example, can
help to maintain stability to tearing modes. Open-loop optimization of the pressure and current
density profiles with external heating and current drive sources is routinely used in many devices.
Improved diagnostic measurements along with localized heating and current drive sources, now
becoming available, will allow active feedback control of the internal profiles in the near future.
Such work is beginning or planned in most of the large tokamaks (JET, JT–60U, DIII–D,
C–Mod, and ASDEX–U) using RF heating and current drive. Real-time analysis of profile data
such as MSE current profile measurements and real-time identification of stability boundaries are
essential components of profile control. Strong plasma rotation can stabilize resistive wall modes,
as demonstrated in tokamak experiments, and rotational shear is also predicted to stabilize resistive
modes. Opportunities to test these predictions are provided by configurations such as the ST,
spheromak, and FRC, which have a large natural diamagnetic rotation, as well as tokamaks with
rotation driven by neutral beam injection. The Electric Tokamak experiment is intended to have a
very large driven rotation, approaching Alfvénic regimes where ideal stability may also be
influenced. Maintaining sufficient plasma rotation, and the possible role of the RWM in damping
the rotation, are important issues that can be investigated in these experiments.
Feedback Control.
Active feedback control of MHD instabilities should allow operation
beyond the “passive” stability limits. Localized rf current drive at the rational surface is predicted
to reduce or eliminate neoclassical tearing mode islands. Experiments have begun in ASDEX–U
and COMPASS-D with promising results, and are planned for next year in DIII–D. Routine use
of such a technique in generalized plasma conditions will require real-time identification of the
unstable mode and its radial location. If the plasma rotation needed to stabilize the resistive wall
mode cannot be maintained, feedback stabilization with external coils will be required. Feedback
experiments have begun in DIII–D and HBT-EP, and feedback control should be explored for the
RFP and other configurations. Physics understanding of these active control techniques will be
directly applicable between configurations.
Disruption Mitigation.
The techniques discussed above for improving MHD stability are the
principal means of avoiding disruptions. However, in the event that these techniques do not
prevent an instability, the effects of a disruption can be mitigated by various techniques.
Experiments in
JT–60U have demonstrated reduction of electromagnetic stresses through operation at a neutral
point for vertical stability. Pre-emptive removal of the plasma energy by injection of a large gas
puff or an impurity pellet has been demonstrated in tokamak experiments, and ongoing
experiments in C–Mod, JT–60U, ASDEX–U, and DIII–D will improve the understanding and
predictive capability. Cryogenic liquid jets of helium are another proposed technique, which may
be required for larger devices. Mitigation techniques developed for tokamaks will be directly
applicable to other configurations.

</doc>
<doc id="40310" url="https://en.wikipedia.org/wiki?curid=40310" title="Magnetohydrodynamics">
Magnetohydrodynamics

Magnetohydrodynamics (MHD) ("magneto fluid dynamics" or "hydromagnetics") is the study of the magnetic properties of electrically conducting fluids. Examples of such magneto-fluids include plasmas, liquid metals, and salt water or electrolytes. The word "magnetohydrodynamics (MHD)" is derived from "magneto-" meaning magnetic field, "hydro-" meaning water, and "-dynamics" meaning movement. The field of MHD was initiated by Hannes Alfvén, for which he received the Nobel Prize in Physics in 1970.
The fundamental concept behind MHD is that magnetic fields can induce currents in a moving conductive fluid, which in turn polarizes the fluid and reciprocally changes the magnetic field itself. The set of equations that describe MHD are a combination of the Navier-Stokes equations of fluid dynamics and Maxwell's equations of electromagnetism. These differential equations must be solved simultaneously, either analytically or numerically.
History.
The first recorded use of the word "magnetohydrodynamics" is by Hannes Alfvén in 1942:
The ebbing salty water flowing past London's Waterloo Bridge interacts with the Earth's magnetic field to produce a potential difference between the two river-banks. Michael Faraday tried this experiment in 1832 but the current was too small to measure with the equipment at the time, and the river bed contributed to short-circuit the signal. However, by a similar process the voltage induced by the tide in the English Channel was measured in 1851.
Ideal and resistive MHD.
The simplest form of MHD, Ideal MHD, assumes that the fluid has so little resistivity that it can be treated as a perfect conductor. This is the limit of infinite magnetic Reynolds number. In ideal MHD, Lenz's law dictates that the fluid is in a sense "tied" to the magnetic field lines. To explain, in ideal MHD a small rope-like volume of fluid surrounding a field line will continue to lie along a magnetic field line,
even as it is twisted and distorted by fluid flows in the system. This is sometimes referred to as the magnetic field lines being "frozen" in the fluid.
The connection between magnetic field lines and fluid in ideal MHD fixes the topology of the magnetic field in the fluid—for example, if a set of magnetic field lines are tied into a knot, then they will remain so as long as the fluid/plasma has negligible resistivity. This difficulty in reconnecting magnetic field lines makes it possible to store energy by moving the fluid or the source of the magnetic field. The energy can then become available if the conditions for ideal MHD break down, allowing magnetic reconnection that releases the stored energy from the magnetic field.
Ideal MHD equations.
The ideal MHD equations consist of the continuity equation, the Cauchy momentum equation, Ampere's Law neglecting displacement current, and a temperature evolution equation. As with any fluid description to a kinetic system, a closure approximation must be applied to highest moment of the particle distribution equation. This is often accomplished with approximations to the heat flux through a condition of adiabaticity or isothermality.
In the following, formula_1 is the magnetic field, formula_2 is the electric field, formula_3 is the bulk plasma velocity, formula_4 is the current density, formula_5 is the mass density, formula_6 is the plasma pressure, ∇⋅ is divergence, and formula_7 is time. The continuity equation is
The Cauchy momentum equation is
The Lorentz force term formula_10 can be expanded using Ampere's law and the identity formula_11 to give
where the first term on the right hand side is the magnetic tension force and the second term is the magnetic pressure force.
The ideal Ohm's law for a plasma is given by
Faraday's law is
The low-frequency Ampere's law neglects displacement current and is given by
The magnetic divergence constraint is
The energy equation is given by
where formula_18 is the ratio of specific heats for an adiabatic equation of state. This energy equation is, of course, only applicable in the absence of shocks or heat conduction as it assumes that the entropy of a fluid element does not change.
Applicability of ideal MHD to plasmas.
Ideal MHD is only strictly applicable when:
Importance of resistivity.
In an imperfectly conducting fluid the magnetic field can generally move through the fluid following a diffusion law with the resistivity of the plasma serving as a diffusion constant. This means that solutions to the ideal MHD equations are only applicable for a limited time for a region of a given size before diffusion becomes too important to ignore. One can estimate the diffusion time across a solar active region (from collisional resistivity) to be hundreds to thousands of years, much longer than the actual lifetime of a sunspot—so it would seem reasonable to ignore the resistivity. By contrast, a meter-sized volume of seawater has a magnetic diffusion time measured in milliseconds.
Even in physical systems – which are large and conductive enough that simple estimates of the Lundquist number suggest that the resistivity can be ignored – resistivity may still be important: many instabilities exist that can increase the effective resistivity of the plasma by factors of more than a billion. The enhanced resistivity is usually the result of the formation of small scale structure like current sheets or fine scale magnetic turbulence, introducing small spatial scales into the system over which ideal MHD is broken and magnetic diffusion can occur quickly. When this happens, magnetic reconnection may occur in the plasma to release stored magnetic energy as waves, bulk mechanical acceleration of material, particle acceleration, and heat.
Magnetic reconnection in highly conductive systems is important because it concentrates energy in time and space, so that gentle forces applied to a plasma for long periods of time can cause violent explosions and bursts of radiation.
When the fluid cannot be considered as completely conductive, but the other conditions for ideal MHD are satisfied, it is possible to use an extended model called resistive MHD. This includes an extra term in Ohm's Law which models the collisional resistivity. Generally MHD computer simulations are at least somewhat resistive because their computational grid introduces a numerical resistivity.
Importance of kinetic effects.
Another limitation of MHD (and fluid theories in general) is that they depend on the assumption that the plasma is strongly collisional (this is the first criterion listed above), so that the time scale of collisions is shorter than the other characteristic times in the system, and the particle distributions are Maxwellian. This is usually not the case in fusion, space and astrophysical plasmas. When this is not the case, or the interest is in smaller spatial scales, it may be necessary to use a kinetic model which properly accounts for the non-Maxwellian shape of the distribution function. However, because MHD is relatively simple and captures many of the important properties of plasma dynamics it is often qualitatively accurate and is therefore often the first model tried.
Effects which are essentially kinetic and not captured by fluid models include double layers, Landau damping, a wide range of instabilities, chemical separation in space plasmas and electron runaway. In the case of ultra-high intensity laser interactions, the incredibly short timescales of energy deposition mean that hydrodynamic codes fail to capture the essential physics.
Structures in MHD systems.
In many MHD systems most of the electric current is compressed into thin nearly-two-dimensional ribbons termed current sheets. These can divide the fluid into magnetic domains, inside of which the currents are relatively weak. Current sheets in
the solar corona are thought to be between a few meters and a few kilometers in thickness, which is quite thin compared to the magnetic domains (which are thousands to hundreds of thousands of kilometers across). Another example is in the Earth's magnetosphere, where current sheets separate topologically distinct domains, isolating most of the Earth's ionosphere from the solar wind.
Waves.
The wave modes derived using MHD plasma theory are called magnetohydrodynamic waves or MHD waves. In general there are three MHD wave modes:
All these waves have constant phase velocities for all frequencies, and hence there is no dispersion. At the limits when the angle between the wave propagation vector k and magnetic field B is either 0 (180) or 90 degrees, the wave modes are called:
The phase velocity depends on the angle between the wave vector k and the magnetic field B. An MHD wave propagating at an arbitrary angle formula_19 with respect to the time independent or bulk field formula_20 will satisfy the dispersion relation
formula_21where formula_22is the Alfvén speed. This branch corresponds to the shear Alfvén mode. Additionally the dispersion equation givesformula_23whereformula_24is the ideal gas sound speed. The plus branch corresponds to the fast-MHD wave mode and the minus branch corresponds to the slow-MHD wave mode.
The MHD oscillations will be damped if the fluid is not perfectly conducting but has a finite conductivity, or if viscous effects are present.
MHD waves and oscillations are a popular tool for the remote diagnostics of laboratory and astrophysical plasmas, e.g. the corona of the Sun (Coronal seismology).
Applications.
Geophysics.
Beneath the Earth's mantle lies the core, which is made up of two parts: the solid inner core and liquid outer core. Both have significant quantities of iron. The liquid outer core moves in the presence of the magnetic field and eddies are set up into the same due to the Coriolis effect. These eddies develop a magnetic field which boosts Earth's original magnetic field—a process which is self-sustaining and is called the geomagnetic dynamo.
Based on the MHD equations, Glatzmaier and Paul Roberts have made a supercomputer model of the Earth's interior. After running the simulations for thousands of years in virtual time, the changes in Earth's magnetic field can be studied. The simulation results are in good agreement with the observations as the simulations have correctly predicted that the Earth's magnetic field flips every few hundred thousands of years. During the flips, the magnetic field does not vanish altogether—it just gets more complicated.
Earthquakes.
Some monitoring stations have reported that earthquakes are sometimes preceded by a spike in ULF activity. A remarkable example of this occurred before the 1989 Loma Prieta earthquake in California, although a subsequent study indicates that this was little more than a sensor malfunction. On December 9, 2010, geoscientists announced that the DEMETER satellite observed a dramatic increase in ULF radio waves over Haiti in the month before the magnitude 7.0 Mw 2010 earthquake. Researchers are attempting to learn more about this correlation to find out whether this method can be used as part of an early warning system for earthquakes.
Astrophysics.
MHD applies to astrophysical, including stars, the interplanetary medium (space between the planets), and possibly within the interstellar medium (space between the stars) and jets. Most astrophysical systems are not in local thermal equilibrium, and therefore require an additional kinematic treatment to describe all the phenomena within the system (see Astrophysical plasma).
Sunspots are caused by the Sun's magnetic fields, as Joseph Larmor theorized in 1919. The solar wind is also governed by MHD. The differential solar rotation may be the long-term effect of magnetic drag at the poles of the Sun, an MHD phenomenon due to the Parker spiral shape assumed by the extended magnetic field of the Sun.
Previously, theories describing the formation of the Sun and planets could not explain how the Sun has 99.87% of the mass, yet only 0.54% of the angular momentum in the solar system. In a closed system such as the cloud of gas and dust from which the Sun was formed, mass and angular momentum are both conserved. That conservation would imply that as the mass concentrated in the center of the cloud to form the Sun, it would spin faster, much like a skater pulling their arms in. The high speed of rotation predicted by early theories would have flung the proto-Sun apart before it could have formed. However, magnetohydrodynamic effects transfer the Sun's angular momentum into the outer solar system, slowing its rotation.
Breakdown of ideal MHD (in the form of magnetic reconnection) is known to be the likely cause of solar flares. The magnetic field in a solar active region over a sunspot can storing energy that is released suddenly as a burst of motion, X-rays, and radiation when the main current sheet collapses, reconnecting the field.
Sensors.
Magnetohydrodynamic sensors are used for precision measurements of angular velocities in inertial navigation systems such as in aerospace engineering. Accuracy improves with the size of the sensor. The sensor is capable of surviving in harsh environments.
Engineering.
MHD is related to engineering problems such as plasma confinement, liquid-metal cooling of nuclear reactors, and electromagnetic casting (among others).
A magnetohydrodynamic drive or MHD propulsor is a method for propelling seagoing vessels using only electric and magnetic fields with no moving parts, using magnetohydrodynamics. The working principle involves electrification of the propellant (gas or water) which can then be directed by a magnetic field, pushing the vehicle in the opposite direction. Although some working prototypes exist, MHD drives remain impractical.
The first prototype of this kind of propulsion was built and tested in 1965 by Steward Way, a professor of mechanical engineering at the University of California, Santa Barbara. Way, on leave from his job at Westinghouse Electric, assigned his senior-year undergraduate students to develop a submarine with this new propulsion system. In the early 1990s, Mitsubishi built a boat, the 'Yamato,' which used a magnetohydrodynamic drive incorporating a superconductor cooled by liquid helium, and could travel at 15 km/h.
MHD power generation fueled by potassium-seeded coal combustion gas showed potential for more efficient energy conversion (the absence of solid moving parts allows operation at higher temperatures), but failed due to cost-prohibitive technical difficulties. One major engineering problem was the failure of the wall of the primary-coal combustion chamber due to abrasion.
In microfluidics, MHD is studied as a fluid pump for producing a continuous, nonpulsating flow in a complex microchannel design.
MHD can be implemented in the continuous casting process of metals to suppress instabilities and control the flow.
Magnetic drug targeting.
An important task in cancer research is developing more precise methods for delivery of medicine to affected areas. One method involves the binding of medicine to biologically compatible magnetic particles (e.g. ferrofluids), which are guided to the target via careful placement of permanent magnets on the external body. Magnetohydrodynamic equations and finite element analysis are used to study the interaction between the magnetic fluid particles in the bloodstream and the external magnetic field.

</doc>
<doc id="40311" url="https://en.wikipedia.org/wiki?curid=40311" title="Great Chicago Fire">
Great Chicago Fire

The Great Chicago Fire was a conflagration that burned from Sunday, October 8, to early Tuesday, October 10, 1871. The fire killed up to 300 people, destroyed roughly of Chicago, Illinois, and left more than 100,000 residents homeless.
Origin.
The fire started at about 9:00 p.m. on October 8, in or around a small barn belonging to the O'Leary family that bordered the alley behind 137 DeKoven Street. The shed next to the barn was the first building to be consumed by the fire, but city officials never determined the exact cause of the blaze. There has, however, been much speculation over the years. The most popular tale blames Mrs. O'Leary's cow; others state that a group of men were gambling inside the barn and knocked over a lantern. Still other speculation suggests that the blaze was related to other fires in the Midwest that day.
The fire's spread was aided by the city's use of wood as the predominant building material in a style called balloon frame; a drought before the fire; and strong southwest winds that carried flying embers toward the heart of the city. More than two thirds of the structures in Chicago at the time of the fire were made entirely of wood. Most houses and buildings were topped with highly flammable tar or shingle roofs. All the city's sidewalks and many roads were made of wood. Compounding this problem, Chicago had only received an inch of rain from July 4 to October 9 causing severe drought conditions.
In 1871, the Chicago Fire Department had 185 firefighters with just 17 horse-drawn steam engines to protect the entire city. The initial response by the fire department was quick, but due to an error by the watchman, Matthias Schaffer, the firefighters were sent to the wrong place, allowing the fire to grow unchecked. An alarm sent from the area near the fire also failed to register at the courthouse where the fire watchmen were. Also, the firefighters were tired from having fought numerous small fires and one large fire in the week before. These factors combined to turn a small barn fire into a conflagration.
Spread of the blaze.
When firefighters finally arrived at DeKoven Street, the fire had grown and spread to neighboring buildings and was progressing towards the central business district. Firefighters had hoped that the South Branch of the Chicago River and an area that had previously thoroughly burned would act as a natural firebreak. All along the river, however, were lumber yards, warehouses, and coal yards, and barges and numerous bridges across the river. As the fire grew, the southwest wind intensified and became superheated, causing structures to catch fire from the heat and from burning debris blown by the wind. Around 11:30 p.m., flaming debris blew across the river and landed on roofs and the South Side Gas Works.
With the fire across the river and moving rapidly towards the heart of the city, panic set in. About this time, Mayor Roswell B. Mason sent messages to nearby towns asking for help. When the courthouse caught fire, he ordered the building to be evacuated and the prisoners jailed in the basement to be released. At 2:20 a.m. on the 9th, the cupola of the courthouse collapsed, sending the great bell crashing down. Some witnesses reported hearing the sound from a mile away.
As more buildings succumbed to the flames, a major contributing factor to the fire’s spread was a meteorological phenomenon known as a fire whirl. As overheated air rises, it comes into contact with cooler air and begins to spin creating a tornado-like effect. These fire whirls are likely what drove flaming debris so high and so far. Such debris was blown across the main branch of the Chicago River to a railroad car carrying kerosene. The fire had jumped the river a second time and was now raging across the city’s north side.
Despite the fire spreading and growing rapidly, the city's firefighters continued to battle the blaze. A short time after the fire jumped the river, a burning piece of timber lodged on the roof of the city’s waterworks. Within minutes, the interior of the building was engulfed in flames and the building was destroyed. With it, the city’s water mains went dry and the city was helpless. The fire burned unchecked from building to building, block to block.
Finally, late into the evening of the 9th, it started to rain, but the fire had already started to burn itself out. The fire had spread to the sparsely populated areas of the north side, having consumed the densely populated areas thoroughly.
Aftermath.
Once the fire had ended, the smoldering remains were still too hot for a survey of the damage to be completed for many days. Eventually, the city determined that the fire destroyed an area about long and averaging wide, encompassing an area of more than . Destroyed were more than of roads, of sidewalk, 2,000 lampposts, 17,500 buildings, and $222 million in property—about a third of the city's valuation (more than $4 billion in 2015 dollars). Of the 300,000 inhabitants, 100,000 were left homeless. 120 bodies were recovered, but the death toll may have been as high as 300. The county coroner speculated that an accurate count was impossible as some victims may have drowned or had been incinerated leaving no remains.
In the days and weeks following the fire, monetary donations flowed into Chicago from around the country and abroad, along with donations of food, clothing, and other goods. These donations came from individuals, corporations, and cities. New York City gave $450,000 along with clothing and provisions, St. Louis gave $300,000, and the Common Council of London gave 1,000 guineas, as well as ₤7,000 from private donations. Cincinnati, Cleveland, and Buffalo, all commercial rivals, donated hundreds and thousands of dollars. Milwaukee, along with other nearby cities, helped by sending fire-fighting equipment. Additionally, food, clothing and books were brought by train from all over the continent. Mayor Mason placed the Chicago Relief and Aid Society in charge of the city’s relief efforts.
Operating from the First Congregational Church, city officials and aldermen began taking steps to preserve order in Chicago. Price gouging was a key concern, and in one ordinance, the city set the price of bread at 8¢ for a loaf. Public buildings were opened as places of refuge, and saloons closed at 9 in the evening for the week following the fire.
The fire also led to questions about development in the United States. Due to Chicago’s rapid expansion at that time, the fire led to Americans reflecting on industrialization. Based on a religious point of view, some said that Americans should return to a more old-fashioned way of life, and that the fire was caused by people ignoring traditional morality. On the other hand, others believed that a lesson to be learned from the fire was that cities needed to improve their building techniques. Frederick Law Olmsted observed that poor building practices in Chicago were a problem:
""Chicago had a weakness for “big things,” and liked to think that it was outbuilding New York. It did a great deal of commercial advertising in its house-tops. The faults of construction as well as of art in its great showy buildings must have been numerous. Their walls were thin, and were overweighted with gross and coarse misornamentation.""
Olmsted also believed that with brick walls, and disciplined firemen and police, the deaths and damage caused would have been much less.
Almost immediately, the city began to rewrite its fire standards, spurred by the efforts of leading insurance executives, and fire-prevention reformers such as Arthur C. Ducat. Chicago soon developed one of the country's leading fire-fighting forces.
Business owners, and land speculators such as Gurdon Saltonstall Hubbard, quickly set about rebuilding the city. The first load of lumber for rebuilding was delivered the day the last burning building was extinguished. By the World's Columbian Exposition 22 years later, Chicago hosted more than 21 million visitors. The Palmer House hotel burned to the ground in the fire 13 days after its grand opening. Its developer, Potter Palmer, secured a loan and rebuilt the hotel to higher standards across the street from the original, proclaiming it to be "The World's First Fireproof Building".
In 1956, the remaining structures on the original O'Leary property at 558 W. DeKoven Street were torn down for construction of the Chicago Fire Academy, a training facility for Chicago firefighters. A bronze sculpture of stylized flames, entitled "Pillar of Fire" by sculptor Egon Weiner, was erected on the point of origin in 1961.
Rumors about the fire.
Almost from the moment the fire broke out, various theories about its cause began to circulate.
The most popular and enduring legend maintains that the fire began in the O'Leary barn, as Mrs. O’Leary was milking her cow. The cow kicked over a lantern (or an oil lamp in some versions), setting fire to the barn. The O'Leary family denied this, stating that they were in bed before the fire started, but stories of the cow began to spread across the city. Catherine O'Leary seemed the perfect scapegoat: she was a poor, Irish Catholic immigrant. During the latter half of the 19th century, anti-Irish sentiment was strong throughout the United States and in Chicago. This was intensified as a result of the growing political power of the city's Irish population. This story was circulating in Chicago even before the flames had died out, and it was noted in the "Chicago Tribune"'s first post-fire issue. In 1893 the reporter Michael Ahern retracted the "cow-and-lantern" story, admitting it was fabricated, but even his confession was unable to put the legend to rest. Although the O'Learys were never officially charged with starting the fire, the story became so engrained in local lore that Chicago's city council officially exonerated them—and the cow—in 1997.
Amateur historian Richard Bales has suggested the fire started when Daniel "Pegleg" Sullivan, who first reported the fire, ignited hay in the barn while trying to steal milk. Part of Bales' evidence includes an account by Sullivan who claimed in an inquiry before the Fire Department of Chicago on November 25, 1871 that he saw the fire coming through the side of the barn and ran across DeKoven Street to free the animals from the barn, one of which included a cow owned by Sullivan's mother. Bales' account does not have consensus. The Chicago Public Library staff criticized his account in their web page on the fire. Despite this, the Chicago city council was convinced of Bales' argument and stated that the actions of Sullivan on that day should be scrutinized after the O'Learys were exonerated in 1997.
Anthony DeBartolo reported evidence in the "Chicago Tribune" suggesting that Louis M. Cohn may have started the fire during a craps game. According to Cohn, on the night of the fire, he was gambling in the O'Learys' barn with one of their sons and some other neighborhood boys. When Mrs. O'Leary came out to the barn to chase the kids away at around 9:00, they knocked over a lantern in their flight, although Cohn states that he paused long enough to scoop up the money. Following his death in 1942, Cohn bequeathed $35,000 which was assigned by his executors to the Medill School of Journalism at Northwestern University. The bequest was given to the school on September 28, 1944, along with his confession.
An alternative theory, first suggested in 1882 by Ignatius L. Donnelly in "", is that the fire was caused by a meteor shower. At a 2004 conference of the Aerospace Corporation and the American Institute of Aeronautics and Astronautics, engineer and physicist Robert Wood suggested that the fire began when Biela's Comet broke up over the Midwest. That four large fires took place, all on the same day, all on the shores of Lake Michigan (see Related Events), suggests a common root cause. Eyewitnesses reported sighting spontaneous ignitions, lack of smoke, "balls of fire" falling from the sky, and blue flames. According to Wood, these accounts suggest that the fires were caused by the methane that is commonly found in comets.
But as meteorites are not known to start or spread fires and are cool to the touch after reaching the ground, this theory has not found favor in the scientific community. A common cause for the fires in the Midwest can be found in the fact that the area had suffered through a tinder-dry summer, so that winds from the front that moved in that evening were capable of generating rapidly expanding blazes from available ignition sources, which were plentiful in the region. Methane-air mixtures become flammable only when the methane concentration exceeds 5%, at which point the mixtures also become explosive. Methane gas is lighter than air and thus does not accumulate near the ground; any localized pockets of methane in the open air would rapidly dissipate. Moreover, if a fragment of an icy comet were to strike the Earth, the most likely outcome, due to the low tensile strength of such bodies, would be for it to disintegrate in the upper atmosphere, leading to an air burst explosion analogous to that of the Tunguska event.
Surviving structures.
The following structures are the only structures from the burnt district still standing:
St. Michael's Church and the Pumping Station were both gutted in the fire, but their exteriors survived, and the buildings were rebuilt using the surviving walls. Additionally, though the inhabitable portions of the building were destroyed, the bell tower of St. James Cathedral survived the fire and was incorporated into the rebuilt church. The stones near the top of the tower are still blackened from the soot and smoke. A couple of wooden cottages on North Cleveland Avenue also survived the blaze.
Related events.
On that hot, dry, and windy autumn day, three other major fires occurred along the shores of Lake Michigan at the same time as the Great Chicago Fire. Some to the north, the Peshtigo Fire consumed the town of Peshtigo, Wisconsin, along with a dozen other villages. It killed 1,200 to 2,500 people and charred approximately 1.5 million acres (6,000 km²). The Peshtigo Fire remains the deadliest in American history but the remoteness of the region meant it was little noticed at the time, due to the fact that one of the first things that burned were the telegraph lines to Green Bay.
Across the lake to the east, the town of Holland, Michigan, and other nearby areas burned to the ground. Some to the north of Holland, the lumbering community of Manistee also went up in flames in what became known as The Great Michigan Fire.
Farther east, along the shore of Lake Huron, the Port Huron Fire swept through Port Huron, Michigan and much of Michigan's "Thumb". On October 9, 1871, a fire swept through the city of Urbana, Illinois, south of Chicago, destroying portions of its downtown area. Windsor, Ontario, likewise burned on October 12.
The city of Singapore, Michigan, provided a large portion of the lumber to rebuild Chicago. As a result, the area was so heavily deforested that the land deteriorated into barren sand dunes and the town had to be abandoned.

</doc>
<doc id="40313" url="https://en.wikipedia.org/wiki?curid=40313" title="Universal Grammar">
Universal Grammar

Universal Grammar (UG) is a theory in linguistics, usually credited to Noam Chomsky, proposing that the ability to learn grammar is hard-wired into the brain. It is sometimes known as 'Mental Grammar', and stands opposed to other 'grammars', e.g. prescriptive, descriptive and pedagogical. The theory suggests that linguistic ability becomes manifest without being taught (see the poverty of the stimulus argument), and that there are properties that all natural human languages share. It is a matter of observation and experimentation to determine precisely what abilities are innate and what properties are shared by all languages.
Argument.
The theory of Universal Grammar proposes that if human beings are brought up under normal conditions (not those of extreme sensory deprivation), then they will always develop language with a certain property X (e.g., distinguishing nouns from verbs, or distinguishing function words from lexical words). As a result, property X is considered to be a property of universal grammar in the most general sense (here not capitalized).
There are theoretical senses of the term Universal Grammar as well (here capitalized). The most general of these is that Universal Grammar is encapsulated in those properties of a normally developing human brain which cause it to learn languages that conform to universal grammar (the non-capitalized, pre-theoretical sense). Using the above examples, Universal Grammar would be the innate property of the human brain which causes it to posit a difference between nouns and verbs, whenever presented with linguistic data.
As Chomsky puts it, "Evidently, development of language in the individual must involve three factors: (1) genetic endowment, which sets limits on the attainable languages, thereby making language acquisition possible; (2) external data, converted to the experience that selects one or another language within a narrow range; (3) principles not specific to the Faculty of Language." properties of the brain cause the person to learn language. So (1) is Universal Grammar in the first theoretical sense, (2) is the linguistic data to which the child is exposed.
Occasionally, aspects of Universal Grammar seem describable in terms of general details regarding cognition.
For example, if a predisposition to categorize events and objects as different classes of things, is part of human cognition and directly results in nouns and verbs showing up in all languages, then it could be assumed that rather than this aspect of Universal Grammar being specific to language, it is more generally a part of human cognition. To distinguish properties of languages that can be traced to other facts regarding cognition from properties of languages that cannot, the abbreviation UG* can be used. UG is the term often used by Chomsky for those aspects of the human brain which cause language to be the way that it is (i.e. are Universal Grammar in the sense used here) but here for discussion, it is used for those aspects which are furthermore specific to language (thus UG, as Chomsky uses it, is just an abbreviation for Universal Grammar, but UG* as used here is a subset of Universal Grammar).
In the same article, Chomsky casts the theme of a larger research program in terms of the following question: "How little can be attributed to UG while still accounting for the variety of 'I-languages' attained, relying on third factor principles?" (I-languages meaning internal languages, the brain states that correspond to knowing how to speak and understand a particular language, and third factor principles meaning (3) in the previous quote).
Chomsky has speculated that UG might be extremely simple and abstract, for example only a mechanism for combining symbols in a particular way, which he calls Merge. To see that Chomsky does not use the term "UG" in the narrow sense UG* suggested above, consider the following quote from the same article:
"The conclusion that Merge falls within UG holds whether such recursive generation is unique to FL (Faculty of Language) or is
appropriated from other systems."
I.e. Merge is part of UG because it causes language to be the way it is, is universal, and is not part of (2) (the environment) or (3) (general properties independent of genetics and environment). Merge is part of Universal Grammar whether it is specific to language, or whether, as Chomsky suggests, it is also used for an example in mathematical thinking.
The distinction is important because there is a long history of argument about UG*, whereas most people working on language agree that there is Universal Grammar. Many people assume that Chomsky means UG* when he writes UG (and in some cases he might actually mean UG* not in the passage quoted above).
Some students of universal grammar study a variety of grammars to extract generalizations called linguistic universals, often in the form of "If X holds true, then Y occurs." These have been extended to a variety of traits, such as the phonemes found in languages, the word orders which languages choose, and the reasons why children exhibit certain linguistic behaviors.
Later linguists who have influenced this theory include Noam Chomsky and Richard Montague, developing their version of this theory as they considered issues of the Argument from poverty of the stimulus to arise from the constructivist approach to linguistic theory. The application of the idea of Universal Grammar to the study of second language acquisition (SLA) is represented mainly in the work of McGill linguist Lydia White.
Syntacticians generally hold that there are parametric points of variation between languages, although heated debate occurs over whether UG constraints are essentially universal due to being "hard-wired" (Chomsky's Principles and Parameters approach), a logical consequence of a specific syntactic architecture (the Generalized Phrase Structure approach) or the result of functional constraints on communication (the functionalist approach).
Relation to the evolution of language.
In an article titled, "The Faculty of Language: What Is It, Who Has It, and How Did It Evolve?" Hauser, Chomsky, and Fitch present the three leading hypotheses for how language evolved and brought humans to the point where we have a Universal Grammar.
Hypothesis 1 states that FLb (the Faculty of Language in the broad sense) is strictly homologous to animal communication.
This means that homologous aspects of the Faculty of Language exist in non-human animals.
Hypothesis 2 states that FLb "is a derived, uniquely human adaptation for language". This hypothesis holds that individual traits were subject to natural selection and came to be very specialized for humans.
Hypothesis 3 states that only FLn (the Faculty of Language in the narrow sense) is unique to humans. It holds that while mechanisms of FLb are present in both humans and non-human animals, the computational mechanism of recursion is recently evolved solely in humans. This is the hypothesis which most closely aligns to the typical theory of Universal Grammar championed by Chomsky.
History.
The idea of a universal grammar can be traced back to Roger Bacon's observations in his "Overview of Grammar" and "Greek Grammar" that all languages are built upon a common grammar, even though it may undergo incidental variations; and the 13th century speculative grammarians who, following Bacon, postulated universal rules underlying all grammars. The concept of a universal grammar or language was at the core of the 17th century projects for philosophical languages. There is a Scottish school of universal grammarians from the 18th century, as distinguished from the philosophical language project, which included authors such as James Beattie, Hugh Blair, James Burnett, James Harris, and Adam Smith. The article on "Grammar" in the first edition of the Encyclopædia Britannica (1771) contains an extensive section titled "Of Universal Grammar".
The idea rose to prominence and influence, in modern linguistics with theories from Noam Chomsky and Richard Montague in the 1950s-1970s, as part of the "Linguistics Wars".
During the early 20th century, in contrast, language was usually understood from a behaviourist perspective, suggesting that language acquisition, like any other kind of learning, could be explained by a succession of trials, errors, and rewards for success. In other words, children learned their mother tongue by simple imitation, through listening and repeating what adults said. For example, when a child says "milk" and the mother will smile and give her some as a result, the child will find this outcome rewarding, thus enhancing the child's language development.
Chomsky's theory.
Chomsky argued that the human brain contains a limited set of constraints for organizing language. This implies in turn that all languages have a common structural basis: the set of rules known as "universal grammar".
Speakers proficient in a language know which expressions are acceptable in their language and which are unacceptable. The key puzzle is how speakers come to know these restrictions of their language, since expressions that violate those restrictions are not present in the input, indicated as such. Chomsky argued that this poverty of stimulus means Skinner's behaviorist perspective cannot explain language acquisition. The absence of negative evidence—evidence that an expression is part of a class of ungrammatical sentences in one's language—is the core of his argument. For example, in English one cannot relate a question word like "what" to a predicate within a relative clause:
Such expressions are not available to language learners: they are, by hypothesis, ungrammatical. Speakers of the local language do not use them, nor note them as unacceptable to language learners. Universal grammar offers an explanation for the presence of the poverty of the stimulus, by making certain restrictions into universal characteristics of human languages. Language learners are consequently never tempted to generalize in an illicit fashion.
Presence of creole languages.
The presence of creole languages is sometimes cited as further support for this theory, especially by Bickerton's controversial language bioprogram theory. Creoles are languages that are develop and form when disparate societies come together and are forced to devise a new system of communication. The system used by the original speakers is typically an inconsistent mix of vocabulary items, known as a pidgin. As these speakers' children begin to acquire their first language, they use the pidgin input to effectively create their own original language, known as a creole. Unlike pidgins, creoles have native speakers (those with acquisition from early childhood) and make use of a full, systematic grammar.
According to Bickerton, the idea of universal grammar is supported by creole languages because certain features are shared by virtually all in the category. For example, their default point of reference in time (expressed by bare verb stems) is not the present moment, but the past. Using pre-verbal auxiliaries, they uniformly express tense, aspect, and mood. Negative concord occurs, but it affects the verbal subject (as opposed to the object, as it does in languages like Spanish). Another similarity among creoles can be seen in the fact that questions are created simply by changing the intonation of a declarative sentence, not its word order or content.
However, extensive work by Carla Hudson-Kam and Elissa Newport suggests that creole languages may not support a universal grammar at all. In a series of experiments, Hudson-Kam and Newport looked at how children and adults learn artificial grammars. They found, notably, that children tend to ignore minor variations in the input when those variations are infrequent, and reproduce only the most frequent forms. In doing so, they tend to standardize the language that they hear around them. Hudson-Kam and Newport hypothesize that in a pidgin-development situation (and in the real-life situation of a deaf child whose parents are/were disfluent signers), children systematize the language they hear, based on the probability and frequency of forms, and not that which has been suggested on the basis of a universal grammar. Further, it seems unsurprising that creoles would share features with the languages from which they are derived, and thus look similar in terms of grammar.
Many researchers of Universal Grammar argue against a concept of Relexification, which says that a language replaces its lexicon almost entirely with that of another. This goes against universalist ideas of a Universal Grammar, which has an innate grammar.
Criticisms.
Geoffrey Sampson maintains that universal grammar theories are not falsifiable and are therefore pseudoscientific theory. He argues that the grammatical "rules" linguists posit are simply post-hoc observations about existing languages, rather than predictions about what is possible in a language. Similarly, Jeffrey Elman argues that the unlearnability of languages assumed by Universal Grammar is based on a too-strict, "worst-case" model of grammar, that is not in keeping with any actual grammar. In keeping with these points, James Hurford argues that the postulate of a language acquisition device (LAD) essentially amounts to the trivial claim that languages are learnt by humans, and thus, that the LAD is less a theory than an explanandum looking for theories.
Morten H. Christiansen and Nick Chater have argued that the relatively fast-changing nature of language would prevent the slower-changing genetic structures from ever catching up, undermining the possibility of a genetically hard-wired universal grammar. Instead of an innate Universal Grammar, they claim, "apparently arbitrary aspects of linguistic structure may result from general learning and processing biases deriving from the structure of thought processes, perceptuo-motor factors, cognitive limitations, and pragmatics."
Hinzen summarizes the most common criticisms of Universal Grammar:
Other researchers have come to some of the same conclusions as Hinzen. Christiannsen and Chater note that there was not a stable environment across all populations, cultures, and languages, in which a language acquisition gene could have adapted. Instead, Christiansen and Chater focus on the relationship between language and the learner, claiming that language has been shaped to fit the human brain. According to Christiansen and Chater, "apparently arbitrary aspects of linguistic structure may result from general learning and processing biases deriving from the structure of thought processes, perceptuo-motor factors, cognitive limitations, and pragmatics." (489).
In addition, it has been suggested that people learn about probabilistic patterns of word distributions in their language, rather than hard and fast rules (see Distributional hypothesis). For example, children overgeneralize the past tense marker "ed" and mispronounce the irregular verbs, producing forms like "goed" and "eated "and correct these errors over time. It has also been proposed that the poverty of the stimulus problem can be largely avoided, if we assume that children employ "similarity-based generalization" strategies in language learning, generalizing about the usage of new words from similar words that they already know how to use.
Language acquisition researcher Michael Ramscar has suggested that when children erroneously expect an ungrammatical form that then never occurs, the repeated failure of expectation serves as a form of implicit negative feedback that allows them to correct their errors over time such as how children correct grammar generalizations like "goed" to "went" through repetitive failure. This implies that word learning is a probabilistic, error-driven process, rather than a process of fast mapping, as many nativists assume.
In the domain of field research, the Pirahã language is claimed to be a counterexample to the basic tenets of Universal Grammar. This research has been led by Daniel Everett. Among other things, this language is alleged to lack all evidence for recursion, including embedded clauses, as well as quantifiers and color terms. According to the writings of Dr. Everett, the Pirahã showed these linguistic shortcomings not because they were simple-minded, but because their culture — which emphasized concrete matters in the present and also lacked creation myths and traditions of art making — did not necessitate it. Some other linguists have argued, however, that some of these properties have been misanalyzed, and that others are actually expected under current theories of Universal Grammar. Other linguists have attempted to reassess the Pirahã to see if it did indeed use recursion. In a corpus analysis of the Pirahã language, linguists failed to disprove Everett's arguments against Universal Grammar and the lack of recursion in Pirahã. However, they also stated that there was "no strong evidence for the lack of recursion either" and they provided "suggestive evidence that Pirahã may have sentences with recursive
structures".
Daniel Everett has gone as far as claiming that universal grammar does not exist. In his words, "universal grammar doesn't seem to work, there doesn't seem to be much evidence for . And what can we put in its place? A complex interplay of factors, of which culture, the values human beings share, plays a major role in structuring the way that we talk and the things that we talk about." Michael Tomasello, a developmental psychologist, also supports this claim, arguing that "although many aspects of human linguistic competence have indeed evolved biologically, specific grammatical principles and constructions have not. And universals in the grammatical structure of different languages have come from more general processes and constraints of human cognition, communication, and vocal-auditory processing, operating during the conventionalization and transmission of the particular grammatical constructions of particular linguistic communities."

</doc>
<doc id="40314" url="https://en.wikipedia.org/wiki?curid=40314" title="Munro">
Munro

A Munro () is a mountain in Scotland with a height over . Munros are named after Sir Hugh Munro, 4th Baronet (1856–1919), who produced the first list of such hills, known as "Munros Tables", in 1891. A Munro top is a summit that is not regarded as a separate mountain and which is over 3,000 feet. In the 2012 revision of the tables, published by the Scottish Mountaineering Club, there are 282 Munros and 227 further subsidiary tops. The best known Munro is Ben Nevis, the highest mountain in the British Isles.
The Munros of Scotland present challenging conditions to hikers, particularly in winter. Each year, people die on the mountains. Nevertheless, a popular practice amongst hillwalkers is "Munro bagging", the aim being to climb all of the listed Munros. As of 2009, more than 4,000 had reported completing their round. The first continuous round of the Munros was completed by Hamish Brown in 1974, whilst the current holder of the record for the fastest continuous round is Stephen Pyke, who completed his 2010 round in just under 40 days.
History.
Before the publication of "Munros Tables" in 1891, there was much uncertainty about the number of Scottish peaks over 3,000 feet. Estimates ranged from 31 (in M.J.B. Baddeley's guides) to 236 (listed in Robert Hall's third edition of "The Highland Sportsman and Tourist", published in 1884). When the Scottish Mountaineering Club was formed in 1889, one of its aims was to remedy this by accurately documenting all of Scotland's mountains over 3,000 feet. Sir Hugh Munro, a founding member of the Club, took on the task using his own experience as a mountaineer, as well as detailed study of the Ordnance Survey Six-inch to the mile (1:10,560) and One-inch to the mile (1:63,360) maps.
Munro researched and produced a set of tables that were published in the Scottish Mountaineering Club Journal in September 1891. The tables listed 538 summits over 3,000 feet, 282 of which were regarded as "separate mountains" The term "Munro" applies to separate mountains, while the lesser summits are known as "tops". Munro did not set any measure of topographic prominence by which a peak qualified as a separate mountain, so there has been much debate about how distinct two hills must be if they are to be counted as two separate Munros.
The Scottish Mountaineering Club has revised the tables, both in response to new height data on Ordnance Survey maps and to address the perceived inconsistency as to which peaks qualify for Munro status. In 1992, the publication of Alan Dawson's book "Relative Hills of Britain", showed that three tops not already considered summits, had a prominence of more than . Given this they would have qualified as Corbett summits had they been under 3,000 feet. In the 1997 tables these three tops, on Beinn Alligin, Beinn Eighe and Buachaille Etive Beag, gained full Munro summit status. Dawson's book also highlighted a number of significant tops with as much as of prominence which were not listed as Munro subsidiary tops. The 1997 tables promoted five of these to full Munro status.
Other classification schemes in Scotland, such as the Corbetts and Grahams , require a peak to have a prominence of at least for inclusion. The Munros, however, lack a rigid set of criteria for inclusion, with many summits of lesser prominence listed, principally because their summits are hard to reach. The 1997 tables ironed out many anomalies, but despite it being the highest-profile hill list in UK, some still consider it not wholly satisfactory.
During May and July 2009 the Munro Society re-surveyed several mountains that are known to be close to the 3,000 ft figure to determine their height more accurately. On 10 September 2009 the society announced that the mountain Sgùrr nan Ceannaichean, south of Glen Carron, had a height of . Therefore, the Scottish Mountaineering Club removed the Munro status of Sgùrr nan Ceannaichean and this mountain is now a Corbett. In a Summer 2011 height survey by The Munro Society, Beinn a' Chlaidheimh was found to be and thus short of the Munro mark. In September 2012, the Scottish Mountaineering Club demoted it from Munro to Corbett status.
As of September 2012, the Scottish Mountaineering Club lists 282 Munros and 227 further subsidiary tops.
Munros.
Perhaps the most famous Munro is Ben Nevis in the Lochaber area. It is the highest peak in the British Isles, with an elevation of .
Other well-known Munros include:
Bagging the Munros.
Compared with some continental ranges, Scottish mountains might be modest in height, but walking and climbing in them can be treacherous because of their latitude and exposure to Atlantic and Arctic weather systems. Even in summer, conditions can be atrocious; thick fog, strong winds, driving rain and freezing summit temperatures are not unusual.
Winter ascents of some Munros are serious undertakings due to the unpredictable weather, the likelihood of ice and snow, and poor visibility. Some walkers are unprepared for extreme weather on the exposed tops and fatalities are recorded every year, often resulting from slips on wet rock or ice.
Some hillwalkers aim to climb every Munro, known as "Munro bagging". Munro-bagging is a form of peak bagging. A walker who has climbed all Munros is entitled to be called a Munroist.
Notable completions.
By May 2013, more than 5,000 people had completed the Munros. The Scottish Mountaineering Club, who maintain a list of those Munroists who have reported completing the Munros, have attempted to popularise the archaic spelling of "compleation".
Hugh Munro never completed his own list, missing out on Càrn an Fhidhleir and Càrn Cloich-mhuillin (downgraded to a "top" in 1981). Sir Hugh is said to have missed the Inaccessible Pinnacle of Sgùrr Dearg, on the Isle of Skye, which he never climbed. However the "In Pinn", as it is known colloquially within Scottish mountaineering, was only listed as a subsidiary top on his list (despite being several metres higher than Sgùrr Dearg, which was listed as the main top).
The first "compleationist" was to be the Reverend A. E. Robertson, in 1901, later minister at Braes of Rannoch from 1907. However, research has cast doubt on this claim, and it is not certain that he reached the summit of Ben Wyvis. Also it is known that Robertson did not climb the Inaccessible Peak of Sgùrr Dearg. If Robertson is discounted, the first Munroist is Ronald Burn, who completed in 1923. Burn is also (indisputably) the first person to climb all the subsidiary "tops".
The person with the most rounds of Munros is Steven Fallon from Edinburgh, who has completed 13 rounds as of 2006.
Chris Smith became the first Member of Parliament to complete the Munros when he reached the summit of Sgùrr nan Coireachan on 27 May 1989.
Ben Fleetwood is probably the youngest person to have completed a round. He climbed the final Munro of his round – Ben More – on 30 August 2011 at the age of 10 years and 3 months. The youngest compleationist to have done the round without the presence of a parent or a guardian is probably Andy Nisbet, who finished his round in 1972 aged 18 years and 1 month.
Continuous rounds.
Hamish Brown did the first continuous self-propelled round of the Munros (except for the Skye and Mull ferries) between 4 April and 24 July 1974 with of ascent and mostly walking – just were on a bicycle. The journey is fully documented in his book "Hamish's Mountain Walk", which is credited with kick-starting the popularity of Munro-bagging as a hobby. The average time taken to bag all the Munros is eight years.
The first reported completion of all the Munros plus the subsidiary tops in one continuous expedition was by Chris Townsend in 1996. His trip lasted between 18 May and 12 September (118 days), he covered a distance of ( by bicycle) with of ascent. The round was broken twice for spells at the office, which could be regarded as stretching the meaning of "continuous".
The first person to complete a winter round (all the Munros in one winter season) was Martin Moran in 1984/85. His journey lasted between 21 December 1984 and 13 March 1985 (83 days), he walked with of ascent. He used motor transport (Campervan) to link his walk.
In the winter of 2005/06, Steve Perry completed a continuous unsupported round entirely on foot (and ferry). He is also the first person to have completed two continuous Munro rounds, having also walked Land's End to John O'Groats via every mainland 3,000 ft mountain between 18 February 2003 and 30 September 2003.
Fastest round.
Charlie Campbell, a former postman from Glasgow, held the record for the fastest round of the Munros between 2000 and 2010. He completed his round in 48 days 12 hours, finishing on 16 July 2000, on Ben Hope. He cycled and swam between Munros; no motorised transport was used. His record was broken by Stephen Pyke of Stone, Staffordshire, in 2010 who completed the round in 39 days, 9 hours. Pyke's round started on the island of Mull on 25 April 2010 and finished on Ben Hope in Sutherland on 3 June 2010. He cycled and kayaked between Munros; no motorised transport was used. He was backed by a support team in a motor home, but had to camp out in the more remote areas.
On 18 September 2011 Alex Robinson and Tom O'Connell finished their self-propelled continuous round on Ben Hope in a time of 48 days 6 hours and 56 minutes setting the second fastest time ever. At the age of just 21, Alex also became the youngest person to have completed a continuous round without the use of any motorised transport.
See also.
The SMC recognises six peaks in England, fifteen in Wales and thirteen in Ireland that would be Munros or Munro Tops if they were in Scotland. These are referred to as Furth Munros, i.e. the Munros furth of Scotland.
The first recorded Furthist is James Parker, who completed on Tryfan (Snowdonia) on 19 April 1929.

</doc>
<doc id="40316" url="https://en.wikipedia.org/wiki?curid=40316" title="Kurdish languages">
Kurdish languages

Kurdish (, ) is a continuum of Northwestern Iranian languages spoken by the Kurds in Western Asia. Kurdish forms three dialect groups known as Northern Kurdish (Kurmanji), Central Kurdish (Sorani), and Southern Kurdish (Pehlewani). A separate group of languages, Zaza-Gorani, is also spoken by several million Kurds, but is linguistically not Kurdish. Recent (as of 2009) studies estimate between 20 and 30 million native speakers of Kurdish in total. The majority of the Kurds speak Kurmanji. 
The literary output in Kurdish was mostly confined to poetry until the early 20th century, when more general literature began to be developed. Today, there are two principal written Kurdish dialects, namely Kurmanji in the northern parts of the geographical region of Kurdistan, and Sorani further east and south. The standard Sorani form of Central Kurdish is, along with Arabic, one of the two official languages of Iraq and is in political documents simply referred to as "Kurdish".
Classification and origin.
The Kurdish languages belong to the Iranian branch of the Indo-European family. They are generally classified as Northwestern Iranian languages, or by some scholars as intermediate between Northwestern and Southwestern Iranian. Martin van Bruinessen notes that "Kurdish has a strong south-western Iranian element", whereas "Zaza and Gurani [...] do belong to the north-west Iranian group".
Ludwig Paul concludes that Kurdish seems to be a Northwestern Iranian language in origin, but acknowledges that it shares many traits with Southwestern Iranian languages like Persian, apparently due to longstanding and intense historical contacts.
Windfuhr identified Kurdish dialects as Parthian, albeit with a Median substratum.
The present state of knowledge about Kurdish allows, at least roughly, drawing the approximate borders of the areas where the main ethnic core of the speakers of the contemporary Kurdish dialects was formed. The most argued hypothesis on the localisation of the ethnic territory of the Kurds remains D.N. Mackenzie's theory, proposed in the early 1960s (Mackenzie 1961). Developing the ideas of P. Tedesco (1921: 255) and regarding the common phonetic isoglosses shared by Kurdish, Persian, and Baluchi, Mackenzie concluded that the speakers of these three languages may once have been in closer contact.
He has tried to reconstruct the alleged Persian-Kurdish-Baluchi linguistic unity presumably in the central parts of Iran. According to Mackenzie's theory, the Persians (or Proto-Persians) occupied the province of Fars in the southwest (proceeding from the assumption that the Achaemenids spoke Persian), the Baluchis (Proto-Baluchis) inhabited the central areas of Western Iran, and the Kurds (Proto-Kurds), in the wording of G. Windfuhr (1975: 459), lived either in northwestern Luristan or in the province of Isfahan.
Subdivisions.
Kurdish is divided into three groups, where dialects from different groups are not mutually intelligible without acquired bilingualism.
In historical evolution terms, Kurmanji is less modified than Sorani and Pehlewani in both phonetic and morphological structure. The Sorani group has been influenced by among other things its closer cultural proximity to the other languages spoken by Kurds in the region including the Gorani language in parts of Iranian Kurdistan and Iraqi Kurdistan. The Kermanshahi group has been influenced by among other things its closer cultural proximity to Persian.
Philip G. Kreyenbroek, an expert writing in 1992, says:
According to "Encyclopaedia of Islam", although Kurdish is not a unified language, its many dialects are interrelated and at the same time distinguishable from other Western Iranian languages. The same source classifies different Kurdish dialects as two main groups, northern and central. The reality is that the average Kurmanji speaker does not find it easy to communicate with the inhabitants of Suleymania or Halabja.
Some linguistic scholars assert that the term "Kurdish" has been applied extrinsically in describing the language the Kurds speak, whereas ethnic Kurds have used the word term to simply describe their ethnic or national identity and refer to their language as "Kurmanji", "Sorani", "Hewrami", "Kermanshahi", "Kalhori" or whatever other dialect or language they speak. Some historians have noted that it is only recently that the Kurds who speak the Sorani dialect have begun referring to their language as "Kurdî", in addition to their identity, which is translated to simply mean Kurdish.
Zazaki and Gorani.
The Zaza–Gorani languages, spoken by communities in the wider area who identify as ethnic Kurds, are not linguistically classified as Kurdish. 
They are classified as adjunct to Kurdish within the Northwestern Iranian languages, although authorities differ in the details. 
Windfuhr 2009 groups Kurdish with Zaza Gorani within a "Northwestern I" group, while Glottolog based on "Encyclopedia Iranica"
prefers an areal grouping of "Central dialects" (or "Kermanic") within Northwest Iranic, with Kurdish but not Zaza-Gorani grouped with "Kermanic".
Gorani appears to be distinct from Kurmanji and Sorani, yet shares vocabulary with both of them and some grammatical similarities with Sorani. Hewrami, a dialect of Gorani, was an important literary language since the fourteenth century but was replaced by Sorani in the twentieth.
European scholars have maintained that Gorani is separate from Kurdish and that Kurdish is synonymous with the Kurmanji-language group, whereas ethnic Kurds maintain that Kurdish encompasses any of the unique languages or dialects spoken by Kurds that are not spoken by neighboring ethnic groups.
Gorani is often classified as part of the Zaza–Gorani branch of Indo-Iranian languages. The Zaza language, spoken in the northernmost parts of Kurdistan differs both grammatically and in vocabulary and is generally not understandable by Gorani speakers but it is considered related to Gorani. Almost all Zaza-speaking communities, as well as speakers of another closely related language spoken in parts of Iraqi Kurdistan called Shabaki, identify themselves as ethnic Kurds.
Geoffrey Haig and Ergin Öpengin in their recent study suggest grouping the Kurdish languages into Northern Kurdish, Central Kurdish, Southern Kurdish, Zaza, and Gorani, and avoid the subgrouping Zaza–Gorani.
History.
During his stay in Damascus, historian Ibn Wahshiyya came across two books on agriculture written in Kurdish, one on the culture of the vine and the palm tree, and the other on water and the means of finding it out in unknown ground. He translated both from Kurdish into Arabic in the early 9th century AD.
Among the earliest Kurdish religious texts is the "Yazidi Black Book", the sacred book of Yazidi faith. It is considered to have been authored sometime in the 13th century AD by "Hassan bin Adi" (b. 1195 AD), the great-grandnephew of Sheikh Adi ibn Musafir (d. 1162), the founder of the faith. It contains the Yazidi account of the creation of the world, the origin of man, the story of Adam and Eve and the major prohibitions of the faith. From the 15th to 17th centuries, classical Kurdish poets and writers developed a literary language. The most notable classical Kurdish poets from this period were Ali Hariri, Ahmad Khani, Malaye Jaziri and Faqi Tayran.
The Italian priest Maurizio Garzoni published the first Kurdish grammar titled "Grammatica e Vocabolario della Lingua Kurda" in Rome in 1787 after eighteen years of missionary work among the Kurds of Amadiya.
This work is very important in Kurdish history as it is the first acknowledgment of the widespread use of a distinctive Kurdish language. Garzoni was given the title "Father of Kurdology" by later scholars. The Kurdish language was banned in a large portion of Kurdistan for some time. After the 1980 Turkish coup d'état until 1991 the use of the Kurdish language was illegal in Turkey.
Current status.
Today, Central Kurdish is an official language in Iraq. In Syria, on the other hand, publishing materials in Kurdish is forbidden, though this prohibition is not enforced anymore due to the civil war.
Before August 2002, the Turkish government placed severe restrictions on the use of Kurdish, prohibiting the language in education and broadcast media. The Kurdish alphabet is not recognized in Turkey, and the use of Kurdish names containing the letters "X", "W", and "Q", which do not exist in the Turkish alphabet, is not allowed. In 2012, Kurdish-language lessons became an elective subject in public schools. Previously, Kurdish education had only been possible in private institutions.
In Iran, though it is used in some local media and newspapers, it is not used in public schools. In 2005, 80 Iranian Kurds took part in an experiment and gained scholarships to study in Kurdish in Iraqi Kurdistan.
In March 2006, Turkey allowed private television channels to begin airing programming in Kurdish. However, the Turkish government said that they must avoid showing children's cartoons, or educational programs that teach Kurdish, and could broadcast only for 45 minutes a day or four hours a week. However, most of these restrictions on private Kurdish television channels were relaxed in September 2009.
In 2010, Kurdish municipalities in the southeast decided to begin printing water bills, marriage certificates and construction and road signs, as well as emergency, social and cultural notices in Kurdish alongside Turkish. Friday sermons by Imams began to be delivered in the language, and Esnaf provided Kurdish price tags.
The state-run Turkish Radio and Television Corporation (TRT) started its 24-hour Kurdish television station on 1 January 2009 with the motto "we live under the same sky". The Turkish Prime Minister sent a video message in Kurdish to the opening ceremony, which was attended by Minister of Culture and other state officials. The channel uses the "X", "W", and "Q" letters during broadcasting.
In Kyrgyzstan, 96.4% of the Kurdish population speak Kurdish as their native language. In Kazakhstan, the corresponding percentage is 88.7%.
Phonology.
According to the Kurdish Academy of Language, Kurdish has the following phonemes:
Vowels.
According to the Kurdish Academy of Language, vowel phonemes of Kurdish are as follows:
As in most modern Iranian languages, Kurdish vowels contrast in quality; they often carry a secondary length distinction that does not affect syllabic weight. This distinction appears in the writing systems developed for Kurdish. The four "short" vowels are , , , and . The five long vowels are , , , and . Variations occur, e.g. in Southern Kurdish where long vowel is used instead of and short vowel is used instead of .
Indo-European linguistic comparison.
Because Kurdish is an Indo-European language, there are many words that are cognates in Kurdish and other Indo-European languages such as Avestan, Persian, Sanskrit, German, English, Norwegian, Latin and Greek. (Source: "Altiranisches Wörterbuch (1904)" for the first two and last six.)
Vocabulary.
The bulk of the vocabulary in Kurdish is of Iranian origin, especially of northwestern Iranian. A considerable number of loanwords come from Semitic, mainly Arabic, which entered through Islam and historical relations with Arab tribes. Yet, a smaller group of loanwords which are of Armenian, Caucasian, and Turkic origins are used in Kurdish, besides some European words. There are also Kurdish words with no clear etymology.
Writing system.
The Kurdish language has been written using four different writing systems. In Iraq and Iran it is written using an Arabic script, composed by Sa'id Kaban Sedqi. More recently, it is sometimes written with a Latin alphabet in Iraq. In Turkey, Syria, and Armenia, it is now written using a Latin script. Kurdish was also written in the Arabic script in Turkey and Syria until 1932. There is a proposal for a unified international recognized Kurdish alphabet based on ISO-8859-1 called "Yekgirtú". Kurdish in the former USSR is written with a Cyrillic alphabet. Kurdish has even been written in the Armenian alphabet in Soviet Armenia and in the Ottoman Empire (a translation of the Gospels in 1857 and of all New Testament in 1872).

</doc>
<doc id="40317" url="https://en.wikipedia.org/wiki?curid=40317" title="UTF-16">
UTF-16

UTF-16 (16-bit Unicode Transformation Format) is a character encoding capable of encoding all 1,112,064 possible characters in Unicode. The encoding is variable-length, as code points are encoded with one or two 16-bit "code units". (also see Comparison of Unicode encodings for a comparison of UTF-8, -16 & -32)
UTF-16 developed from an earlier fixed-width 16-bit encoding known as UCS-2 (for 2-byte Universal Character Set) once it became clear that a fixed-width 2-byte encoding could not encode enough characters to be truly universal.
History.
In the late 1980s, work began on developing a uniform encoding for a "Universal Character Set" (UCS) that would replace earlier language-specific encodings with one coordinated system. The goal was to include all required characters from most of the world's languages, as well as symbols from technical domains such as science, mathematics, and music. The original idea was to expand the typical 256-character encodings requiring 1 byte per character with an encoding using 216 = 65,536 values requiring 2 bytes per character. Two groups worked on this in parallel, the IEEE and the Unicode Consortium, the latter representing mostly manufacturers of computing equipment. The two groups attempted to synchronize their character assignments, so that the developing encodings would be mutually compatible. The early 2-byte encoding was usually called "Unicode", but is now called "UCS-2".
Early in this process, however, it became increasingly clear that 216 characters would not suffice, and IEEE introduced a larger 31-bit space with an encoding (UCS-4) that would require 4 bytes per character. This was resisted by the Unicode Consortium, both because 4 bytes per character wasted a lot of disk space and memory, and because some manufacturers were already heavily invested in 2-byte-per-character technology. The UTF-16 encoding scheme was developed as a compromise to resolve this impasse in version 2.0 of the Unicode standard in July 1996 and is fully specified in RFC 2781 published in 2000 by the IETF.
In UTF-16, code points greater or equal to 216 are encoded using "two" 16-bit code units. The standards organizations chose the largest block available of un-allocated 16-bit code points to use as these code units, and code points from this range are not individually encodable in UTF-16 (and not legally encodable in any UTF encoding).
UTF-16 is specified in the latest versions of both the international standard ISO/IEC 10646 and the Unicode Standard.
Description.
U+0000 to U+D7FF and U+E000 to U+FFFF.
Both UTF-16 and UCS-2 encode code points in this range as single 16-bit code units that are numerically equal to the corresponding code points. These code points in the BMP are the "only" code points that can be represented in UCS-2. Modern text almost exclusively consists of these code points.
U+10000 to U+10FFFF.
Code points from the other planes (called Supplementary Planes) are encoded as two 16-bit code units called "surrogate pairs", by the following scheme:
There was an attempt to rename "high" and "low" surrogates to "leading" and "trailing" due to their numerical values not matching their names. This appears to have been abandoned in recent Unicode standards.
Since the ranges for the high surrogates, low surrogates, and valid BMP characters are disjoint, it is not possible for a surrogate to match a BMP character, or for (parts of) two adjacent characters to look like a legal surrogate pair. This simplifies searches a great deal. It also means that UTF-16 is "self-synchronizing" on 16-bit words: whether a code unit starts a character can be determined without examining earlier code units. UTF-8 shares these advantages, but many earlier multi-byte encoding schemes (such as Shift JIS and other Asian multi byte encodings) did not allow unambiguous searching and could only be synchronized by re-parsing from the start of the string (UTF-16 is not self-synchronizing if one byte is lost or if traversal starts at a random byte).
Because the most commonly used characters are all in the Basic Multilingual Plane, handling of surrogate pairs is often not thoroughly tested. This leads to persistent bugs and potential security holes, even in popular and well-reviewed application software (e.g. CVE-2008-2938, CVE-2012-2135).
The Supplementary Planes contain Emoji, historic scripts, less used symbols, less used Chinese ideographs and some more.
U+D800 to U+DFFF.
The Unicode standard permanently reserves these code point values for UTF-16 encoding of the high and low surrogates, and they will never be assigned a character, so there should be no reason to encode them. The official Unicode standard says that no UTF forms, including UTF-16, can encode these code points.
However UCS-2, UTF-8, and UTF-32 can encode these code points in trivial and obvious ways, and large amounts of software does so even though the standard states that such arrangements should be treated as encoding errors. It is possible to unambiguously encode them in UTF-16 by using a code unit equal to the code point, as long as no sequence of two code units can be interpreted as a legal surrogate pair (that is, as long as a high surrogate is never followed by a low surrogate). The majority of UTF-16 encoder and decoder implementations translate between encodings as though this were the case and Windows allows such sequences in filenames.
Examples.
Consider the encoding of U+10437 (𐐷):
The following table summarizes this conversion, as well as others. The colors indicate how bits from the code point are distributed among the UTF-16 bytes. Additional bits added by the UTF-16 encoding process are shown in black.
Note UTF-16LE is middle-endian with a byte granularity, because the most significant byte is the second and the least significant byte is the third, for U+10437 (𐐷):
Byte order encoding schemes.
UTF-16 and UCS-2 produce a sequence of 16-bit code units. Since most communication and storage protocols are defined for bytes, and each unit thus takes two 8-bit bytes, the order of the bytes may depend on the endianness (byte order) of the computer architecture.
To assist in recognizing the byte order of code units, UTF-16 allows a Byte Order Mark (BOM), a code point with the value U+FEFF, to precede the first actual coded value. (U+FEFF is the invisible zero-width non-breaking space/ZWNBSP character.) If the endian architecture of the decoder matches that of the encoder, the decoder detects the 0xFEFF value, but an opposite-endian decoder interprets the BOM as the non-character value U+FFFE reserved for this purpose. This incorrect result provides a hint to perform byte-swapping for the remaining values. If the BOM is missing, RFC 2781 says that big-endian encoding should be assumed. (In practice, due to Windows using little-endian order by default, many applications similarly assume little-endian encoding by default.) If there is no BOM, one method of recognizing a UTF-16 encoding is searching for the space character (U+0020) which is very common in texts in most languages.
The standard also allows the byte order to be stated explicitly by specifying UTF-16BE or UTF-16LE as the encoding type. When the byte order is specified explicitly this way, a BOM is specifically "not" supposed to be prepended to the text, and a U+FEFF at the beginning should be handled as a ZWNBSP character. Many applications ignore the BOM code at the start of any Unicode encoding. Web browsers often use a BOM as a hint in determining the character encoding.
For Internet protocols, IANA has approved "UTF-16", "UTF-16BE", and "UTF-16LE" as the names for these encodings. (The names are case insensitive.) The aliases UTF_16 or UTF16 may be meaningful in some programming languages or software applications, but they are not standard names in Internet protocols.
Similar designations, UCS-2, UCS-2BE and UCS-2LE, are used to imitate the UTF-16 labels and behaviour. However, "UCS-2 should now be considered obsolete. It no longer refers to an encoding form in either 10646 or the Unicode Standard."
Usage.
UTF-16 is used for text in the OS API in Microsoft Windows 2000/XP/2003/Vista/7/8/CE. Older Windows NT systems (prior to Windows 2000) only support UCS-2. In Windows XP, no code point above U+FFFF is included in any font delivered with Windows for European languages. Files and network data tend to be a mix of UTF-16, UTF-8, and legacy byte encodings.
IBM iSeries systems designate code page CCSID 13488 for UCS-2 character encoding, CCSID 1200 for UTF-16 encoding, and CCSID 1208 for UTF-8 encoding.
UTF-16 is used by the Qualcomm BREW operating systems; the .NET environments; and the Qt cross-platform graphical widget toolkit.
Symbian OS used in Nokia S60 handsets and Sony Ericsson UIQ handsets uses UCS-2. iPhone handsets use UTF-16 for Short Message Service instead of UCS-2 described in the 3GPP TS 23.038 (GSM) and IS-637 (CDMA) standards.
The Joliet file system, used in CD-ROM media, encodes file names using UCS-2BE (up to sixty-four Unicode characters per file name).
The Python language environment officially only uses UCS-2 internally since version 2.0, but the UTF-8 decoder to "Unicode" produces correct UTF-16. Since Python 2.2, "wide" builds of Unicode are supported which use UTF-32 instead; these are primarily used on Linux. Python 3.3 no longer ever uses UTF-16, instead an encoding that gives the most compact representation for the given string is chosen from ASCII/Latin-1, UCS-2, and UTF-32.
Java originally used UCS-2, and added UTF-16 supplementary character support in J2SE 5.0.
JavaScript uses UCS-2.
In many languages, quoted strings need a new syntax for quoting non-BMP characters, if the language's codice_1 syntax explicitly limits itself to 4 hex digits. The most common (used by C#, D and several other languages) is to use an upper-case 'U' with 8 hex digits such as codice_2 In Java 7, regular expressions and ICU and Perl, the syntax codice_3 must be used. In many other cases (such as Java outside of regular expressions), the only way to get non-BMP characters is to enter the surrogate halves individually, for example: codice_4 for U+1D11E.
These implementations "all" return the number of 16-bit code units rather than the number of Unicode code points when the corresponding string-length function call is used on their strings, and indexing into a string returns the indexed code unit, not the indexed code point, this leads some people to claim that UTF-16 is not supported. However, the term "character" is defined and used in multiple ways within the Unicode terminology, so an unambiguous count is not possible. Most of the confusion is due to obsolete ASCII-era documentation using the term "character" when a fixed-size "byte" or "octet" was intended.

</doc>
<doc id="40318" url="https://en.wikipedia.org/wiki?curid=40318" title="Portland cement">
Portland cement

Portland cement is the most common type of cement in general use around the world, used as a basic ingredient of concrete, mortar, stucco, and most non-speciality grout. It was developed from other types of hydraulic lime in England in the mid 19th century and usually originates from limestone. It is a fine powder produced by heating materials in a kiln to form what is called clinker, grinding the clinker, and adding small amounts of other materials. Several types of Portland cement are available with the most common being called ordinary Portland cement (OPC) which is grey in color, but a white Portland cement is also available.
Portland cement is caustic, so it can cause chemical burns, the powder can cause irritation or with severe exposure lung cancer, and can contain some hazardous components such as crystalline silica and hexavalent chromium. Environmental concerns are the high energy consumption required to mine, manufacture, and transport the cement and the related air pollution including the release of greenhouse gases (e.g., carbon dioxide), dioxin, NOx, SO2, and particulates.
The low cost and widespread availability of the limestone, shales, and other naturally occurring materials used in Portland cement make it one of the lowest-cost materials widely used over the last century throughout the world. Concrete produced from Portland cement is one of the most versatile construction materials available in the world.
History.
Portland cement was developed from natural cements made in Britain beginning in the middle of the 18th century. Its name is derived from its similarity to Portland stone, a type of building stone quarried on the Isle of Portland in Dorset, England.
The development of modern Portland cement (sometimes called ordinary or normal Portland cement) began in 1756 when John Smeaton experimented with combinations of different limestones and additives including trass and pozzolanas relating to the planned construction of a lighthouse now known as Smeaton's Tower. In the late 18th century, Roman cement was developed and patented in 1796 by James Parker; Roman cement quickly became popular, but was largely replaced by Portland cement in the 1850s. In 1811 James Frost produced a cement he called British cement. James Frost is reported to have erected a manufactory for making of an artificial cement in 1826. In 1843, Aspdin's son William improved their cement, which was initially called "Patent Portland cement", although he had no patent. In 1818, French engineer Louis Vicat invented an artificial hydraulic lime considered the "principal forerunner" of Portland cement and "... Edgar Dobbs of Southwark patented a cement of this kind in 1811." Portland cement was used by Joseph Aspdin in his cement patent in 1824 because of the cements' resemblance to Portland stone. The name "Portland cement" is also recorded in a directory published in 1823 being associated with a William Lockwood, Dave Stewart, and possibly others. However, Aspdins' cement was nothing like modern Portland cement, but was a first step in the development of modern Portland cement, called a 'proto-Portland cement'. William Aspdin had left his father's company and in his cement manufacturing apparently accidentally produced calcium silicates in the 1840s, a middle step in the development of Portland cement. In 1848, William Aspdin further improved his cement; in 1853, he moved to Germany, where he was involved in cement making. William Aspdin made what could be called 'meso-Portland cement' (a mix of Portland cement and hydraulic lime). Isaac Charles Johnson further refined the production of 'meso-Portland cement' (middle stage of development) and claimed to be the real father of Portland cement. John Grant of the Metropolitan Board of Works in 1859 set out requirements for cement to be used in the London sewer project. This became a specification for Portland cement. The next development with the manufacture of Portland cement was the introduction of the rotary kiln patented by German Friedrich Hoffmann called a Hoffmann kiln for brick making in 1858 and then Frederick Ransome in 1885 (U.K.) and 1886 (U.S.) which allowed a stronger, more homogeneous mixture and a continuous manufacturing process. The Hoffman "endless" kiln which gave "perfect control over combustion" was tested in 1860 and showed the process produced a better grade of cement. This cement was made at the Portland Cementfabrik Stern at Stettin, which was the first to use a Hoffman kiln. It is thought that the first modern Portland cement was made there. The Association of German Cement Manufacturers issued a standard on Portland cement in 1878.
Portland cement had been imported into the United States from Germany and England and in the 1870s and 1880s it was being produced by Eagle Portland cement near Kalamazoo, Michigan, and in 1875, the first Portland cement was produced by Coplay Cement Company under the direction of David O. Saylor in Coplay, Pennsylvania. By the early 20th century American made Portland cement had displaced most of the imported Portland cement.
Manufacturing.
ASTM C150 defines Portland cement as "hydraulic cement (cement that not only hardens by reacting with water but also forms a water-resistant product) produced by pulverizing clinkers which consist essentially of hydraulic calcium silicates, usually containing one or more of the forms of calcium sulphate as an inter ground addition." The European Standard EN 197-1 uses the following definition:
(The last two requirements were already set out in the German Standard, issued in 1909).
Clinkers make up more than 90% of the cement along with a limited amount of calcium sulfate (which controls the set time) and up to 5% minor constituents (fillers) as allowed by various standards. Clinkers are nodules (diameters, 0.2–1.0 inch [5–25 mm]) of a sintered material that is produced when a raw mixture of predetermined composition is heated to high temperature. The key chemical reaction which defines Portland cement from other hydraulic limes occurs at these high temperatures (> and is when the belite (Ca2SiO4) combines with calcium oxide (CaO) to form alite (Ca3SiO5).
Portland cement clinker is made by heating, in a cement kiln, a mixture of raw materials to a calcining temperature of above and then a fusion temperature, which is about for modern cements, to sinter the materials into clinker. The materials in cement clinker are alite, belite, tri-calcium aluminate, and tetra-calcium alumino ferrite. The aluminium, iron, and magnesium oxides are present as a flux allowing the calcium silicates to form at a lower temperature and contribute little to the strength. For special cements, such as Low Heat (LH) and Sulfate Resistant (SR) types, it is necessary to limit the amount of tricalcium aluminate (3 CaO·Al2O3) formed. The major raw material for the clinker-making is usually limestone (CaCO3) mixed with a second material containing clay as source of alumino-silicate. Normally, an impure limestone which contains clay or SiO2 is used. The CaCO3 content of these limestones can be as low as 80%. Secondary raw materials (materials in the rawmix other than limestone) depend on the purity of the limestone. Some of the materials used are clay, shale, sand, iron ore, bauxite, fly ash, and slag. When a cement kiln is fired by coal, the ash of the coal acts as a secondary raw material.
Cement grinding.
To achieve the desired setting qualities in the finished product, a quantity (2–8%, but typically 5%) of calcium sulfate (usually gypsum or anhydrite) is added to the clinker and the mixture is finely ground to form the finished cement powder. This is achieved in a cement mill. The grinding process is controlled to obtain a powder with a broad particle size range, in which typically 15% by mass consists of particles below 5 μm diameter, and 5% of particles above 45 μm. The measure of fineness usually used is the "specific surface area", which is the total particle surface area of a unit mass of cement. The rate of initial reaction (up to 24 hours) of the cement on addition of water is directly proportional to the specific surface area. Typical values are 320–380 m2·kg−1 for general purpose cements, and 450–650 m2·kg−1 for "rapid hardening" cements. The cement is conveyed by belt or powder pump to a silo for storage. Cement plants normally have sufficient silo space for one to 20 weeks of production, depending upon local demand cycles. The cement is delivered to end users either in bags or as bulk powder blown from a pressure vehicle into the customer's silo. In industrial countries, 80% or more of cement is delivered in bulk.
Setting and hardening.
Cement sets when mixed with water by way of a complex series of chemical reactions still only partly understood. The different constituents slowly crystallise and the interlocking of their crystals gives cement its strength. Carbon dioxide is slowly absorbed to convert the portlandite (Ca(OH)2) into insoluble calcium carbonate. After the initial setting, immersion in warm water will speed up setting. Gypsum is added as an inhibitor to prevent flash setting.
Use.
The most common use for Portland cement is in the production of concrete. Concrete is a composite material consisting of aggregate (gravel and sand), cement, and water. As a construction material, concrete can be cast in almost any shape desired, and once hardened, can become a structural (load bearing) element. Concrete can be used in the construction of structural elements like panels, beams, street furniture, or may make cast-"in situ" concrete for building superstructures like roads and dams. These may be supplied with concrete mixed on site, or may be provided with "ready-mixed" concrete made at permanent mixing sites. Portland cement is also used in mortars (with sand and water only) for plasters and screeds, and in grouts (cement/water mixes squeezed into gaps to consolidate foundations, road-beds, etc.).
When water is mixed with Portland cement, the product sets in a few hours and hardens over a period of weeks. These processes can vary widely depending upon the mix used and the conditions of curing of the product, but a typical concrete sets in about 6 hours and develops a compressive strength of 8 MPa in 24 hours. The strength rises to 15 MPa at 3 days, 23 MPa at 1 week, 35 MPa at 4 weeks and 41 MPa at 3 months. In principle, the strength continues to rise slowly as long as water is available for continued hydration, but concrete is usually allowed to dry out after a few weeks and this causes strength growth to stop.
Types.
General.
Different standards are used for classification of Portland cement. The two major standards are the ASTM C150 used primarily in the USA and European EN 197. EN 197 cement types CEM I, II, III, IV, and V do not correspond to the similarly named cement types in ASTM C150.
ASTM C150.
The five types of Portland cements exist, with variations of the first three according to ASTM C150.
Type I Portland cement is known as common or general-purpose cement. It is generally assumed unless another type is specified. It is commonly used for general construction especially when making precast and precast-prestressed concrete that is not to be in contact with soils or ground water. The typical compound compositions of this type are:
55% (C3S), 19% (C2S), 10% (C3A), 7% (C4AF), 2.8% MgO, 2.9% (SO3), 1.0% ignition loss, and 1.0% free CaO
A limitation on the composition is that the (C3A) shall not exceed 15%.
Type II gives off less heat during hydration. This type of cement costs about the same as type I. Its typical compound composition is:
51% (C3S), 24% (C2S), 6% (C3A), 11% (C4AF), 2.9% MgO, 2.5% (SO3), 0.8% ignition loss, and 1.0% free CaO
A limitation on the composition is that the (C3A) shall not exceed 8%, which reduces its vulnerability to sulfates. This type is for general construction exposed to moderate sulfate attack and is meant for use when concrete is in contact with soils and ground water, especially in the western United States due to the high sulfur content of the soils. Because of similar price to that of type I, type II is much used as a general purpose cement, and the majority of Portland cement sold in North America meets this specification.
Note: Cement meeting (among others) the specifications for types I and II has become commonly available on the world market.
Type III has relatively high early strength. Its typical compound composition is: 57% (C3S), 19% (C2S), 10% (C3A), 7% (C4AF), 3.0% MgO, 3.1% (SO3), 0.9% Ignition loss, and 1.3% free CaO. This cement is similar to type I, but ground finer. Some manufacturers make a separate clinker with higher C3S and/or C3A content, but this is increasingly rare, and the general purpose clinker is usually used, ground to a specific surface area typically 50–80% higher. The gypsum level may also be increased a small amount. This gives the concrete using this type of cement a three-day compressive strength equal to the seven-day compressive strength of types I and II. Its seven-day compressive strength is almost equal to 28-day compressive strengths of types I and II. The only downside is that the six-month strength of type III is the same or slightly less than that of types I and II. Therefore, the long-term strength is sacrificed a little. It is usually used for precast concrete manufacture, where high one-day strength allows fast turnover of molds. It may also be used in emergency construction and repairs and construction of machine bases and gate installations.
Type IV Portland cement is generally known for its low heat of hydration. Its typical compound composition is: 28% (C3S), 49% (C2S), 4% (C3A), 12% (C4AF), 1.8% MgO, 1.9% (SO3), 0.9% Ignition loss, and 0.8% free CaO. The percentages of (C2S) and (C4AF) are relatively high and (C3S) and (C3A) are relatively low. A limitation on this type is that the maximum percentage of (C3A) is seven, and the maximum percentage of (C3S) is thirty-five. This causes the heat given off by the hydration reaction to develop at a slower rate. However, as a consequence the strength of the concrete develops slowly. After one or two years the strength is higher than the other types after full curing. This cement is used for very large concrete structures, such as dams, which have a low surface to volume ratio. This type of cement is generally not stocked by manufacturers but some might consider a large special order. This type of cement has not been made for many years, because Portland-pozzolan cements and ground granulated blast furnace slag addition offer a cheaper and more reliable alternative.
Type V is used where sulfate resistance is important. Its typical compound composition is: 38% (C3S), 43% (C2S), 4% (C3A), 9% (C4AF), 1.9% MgO, 1.8% (SO3), 0.9% Ignition loss, and 0.8% free CaO. This cement has a very low (C3A) composition which accounts for its high sulfate resistance. The maximum content of (C3A) allowed is 5% for type V Portland cement. Another limitation is that the (C4AF) + 2(C3A) composition cannot exceed 20%. This type is used in concrete to be exposed to alkali soil and ground water sulfates which react with (C3A) causing disruptive expansion. It is unavailable in many places, although its use is common in the western United States and Canada. As with type IV, type V Portland cement has mainly been supplanted by the use of ordinary cement with added ground granulated blast furnace slag or tertiary blended cements containing slag and fly ash.
Types Ia, IIa, and IIIa have the same composition as types I, II, and III. The only difference is that in Ia, IIa, and IIIa, an air-entraining agent is ground into the mix. The air-entrainment must meet the minimum and maximum optional specification found in the ASTM manual. These types are only available in the eastern United States and Canada, only on a limited basis. They are a poor approach to air-entrainment which improves resistance to freezing under low temperatures.
Types II(MH) and II(MH)a have a similar composition as types II and IIa, but with a mild heat.
EN 197.
EN 197-1 defines five classes of common cement that comprise Portland cement as a main constituent. These classes differ from the ASTM classes.
Constituents that are permitted in Portland-composite cements are artificial pozzolans (blastfurnace slag, silica fume, and fly ashes) or natural pozzolans (siliceous or siliceous aluminous materials such as volcanic ash glasses, calcined clays and shale).
CSA A3000-08.
Canada - http://www.lafarge-na.com/Portland_Cement_PDS_v6.pdf
GU, GUL* (a.k.a Type 10 (GU) cement) > General use cement, suitable for all
applications where the special properties of any
other type of Portland cement are not required.
MS > Moderate sulphate resistant cement for
use in applications requiring moderate levels of
sulphate resistance.
MH, MHL* > Moderate heat cement for use in
applications requiring moderately low levels of
heat generation during the hydration process.
HE, HEL* > High early strength cement for use
where high early strengths are required.
LH, LHL* > Low heat cement for use in
applications that require significantly low levels
of heat generation during the hydration process.
HS > High sulphate resistant cement for use in
applications that require high levels of sulphate
resistance. HS cement generally gains strength
more slowly than the other types.
White Portland cement.
White Portland cement or white ordinary Portland cement (WOPC) is similar to ordinary, grey Portland cement in all respects except for its high degree of whiteness. Obtaining this colour requires some modification to the method of manufacture; because of this, it is somewhat more expensive than the grey product. The main requirement is to have low iron content which should be less than 0.5% expressed as Fe2O3 for white cement and less than 0.9% for off-white cement. It helps to have the iron oxide as ferrous oxide (FeO) which is obtained via slight reducing conditions i.e. operating with zero excess oxygen at the kiln exit. This gives the clinker and cement a green tinge. Other metals such as Cr, Mn, Ti, etc. in trace content can also give color tinges, so for a project it is best to use cement from a single source.
Safety issues.
Bags of cement routinely have health and safety warnings printed on them because not only is cement highly alkaline, but the setting process is also exothermic. As a result, wet cement is strongly caustic and can easily cause severe skin burns if not promptly washed off with water. Similarly, dry cement powder in contact with mucous membranes can cause severe eye or respiratory irritation. The reaction of cement dust with moisture in the sinuses and lungs can also cause a chemical burn as well as headaches, fatigue, and lung cancer.
The production of comparatively low-alkalinity cements (pH<11) is an area of ongoing investigation.
In Scandinavia, France, and the UK, the level of chromium(VI), which is considered to be toxic and a major skin irritant, may not exceed 2 ppm.
The Occupational Safety and Health Administration (OSHA) has set the legal limit (permissible exposure limit) for Portland cement exposure in the workplace as 50 mppcf over an 8-hour workday. The National Institute for Occupational Safety and Health (NIOSH) has set a Recommended exposure limit (REL) of 10 mg/m3 total exposure and 5 mg/m3 respiratory exposure over an 8-hour workday. At levels of 5000 mg/m3, Portland cement is immediately dangerous to life and health.
Environmental effects.
Portland cement manufacture can cause environmental impacts at all stages of the process. These include emissions of airborne pollution in the form of dust; gases; noise and vibration when operating machinery and during blasting in quarries; consumption of large quantities of fuel during manufacture; release of CO2 from the raw materials during manufacture, and damage to countryside from quarrying. Equipment to reduce dust emissions during quarrying and manufacture of cement is widely used, and equipment to trap and separate exhaust gases are coming into increased use. Environmental protection also includes the re-integration of quarries into the countryside after they have been closed down by returning them to nature or re-cultivating them.
"The Arizona Department of Environmental Quality was informed this week that the Arizona Portland Cement Co. failed a second round of testing for emissions of hazardous air pollutants at the company's Rillito plant near Tucson. The latest round of testing, performed in January 2003 by the company, is designed to ensure that the facility complies with federal standards governing the emissions of dioxins and furans, which are byproducts of the manufacturing process." Cement Reviews' "Environmental News" web page details case after case of environmental problems with cement manufacturing.
An independent research effort of AEA Technology to identify critical issues for the cement industry today concluded the most important environment, health and safety performance issues facing the cement industry are atmospheric releases (including greenhouse gas emissions, dioxin, NOx, SO2, and particulates), accidents, and worker exposure to dust.
The CO2 associated with Portland cement manufacture comes from 3 sources:
Source 1 is fairly constant: minimum around CO2 per kg of cement, maximum 0.54, typical value around 0.50 worldwide. Source 2 varies with plant efficiency: efficient precalciner plant CO2 per kg cement, low-efficiency wet process as high as 0.65, typical modern practices (e.g. UK) averaging around 0.30. Source 3 is almost insignificant at 0.002–0.005. So typical total CO2 is around CO2 per kg finished cement. This omits the CO2 associated with electric power consumption because this varies according to the local generation type and efficiency. Typical electrical energy consumption is of the order of 90–150 kWh per tonne cement, equivalent to CO2 per kg finished cement if the electricity is coal-generated.
Overall, with nuclear- or hydroelectric power and efficient manufacturing, CO2 generation can be reduced to per kg cement, but can be twice as high. The thrust of innovation for the future is to reduce sources 1 and 2 by modification of the chemistry of cement, by the use of wastes, and by adopting more efficient processes. Although cement manufacturing is clearly a very large CO2 emitter, concrete (of which cement makes up about 15%) compares quite favourably with other building systems in this regard.
Cement plants used for waste disposal or processing.
Due to the high temperatures inside cement kilns, combined with the oxidizing (oxygen-rich) atmosphere and long residence times, cement kilns are used as a processing option for various types of waste streams: indeed, they efficiently destroy many hazardous organic compounds. The waste streams also often contain combustible materials which allow the substitution of part of the fossil fuel normally used in the process.
Waste materials used in cement kilns as a fuel supplement:
Portland cement manufacture also has the potential to benefit from using industrial byproducts from the waste stream. These include in particular:

</doc>
<doc id="40319" url="https://en.wikipedia.org/wiki?curid=40319" title="Sarah Hughes">
Sarah Hughes

Sarah Elizabeth Hughes (born May 2, 1985) is an American figure skater. She is the 2002 Olympic Champion and 2001 World bronze medalist in ladies' singles.
Personal life.
Hughes was born in Great Neck, New York. Her father, John Hughes, is a Canadian of Irish descent and was one of the captains of the undefeated and untied NCAA champion 1969–70 Cornell University ice hockey team. Her mother, Amy Pastarnack, is Jewish and is a breast cancer survivor. This led Sarah Hughes to become an advocate for breast cancer awareness. She appeared in a commercial for General Electric promoting breast cancer awareness and research. Hughes stated: ""I always said that if I can get one person to get a mammogram, I've accomplished something."" Among the other causes Hughes supports are Figure Skating in Harlem, which provides free ice skating lessons and academic tutoring for girls in the Harlem community in New York City. Hughes has supported this program for over ten years.
Hughes attended Great Neck North High School. In 2003, she began her studies at Yale University. On May 25, 2009, Hughes graduated from Yale and received a bachelor's degree in American studies with a concentration in U.S. politics and communities. She currently is a student at the University of Pennsylvania Law School.
Sarah Hughes is the fourth of six children. One of her younger sisters, Emily, is also a figure skater and competed at the 2006 Winter Olympics. In December 2012, her older brother Matt, graduated from the police academy and is currently an NYPD officer. She is the cousin of Gregg "Opie" Hughes, from the Opie & Anthony show.
Career.
Sarah Hughes began skating at the age of three. Robin Wagner, who also choreographed for her from 1994, became her head coach in January 1998.
Hughes won the junior title at the 1998 U.S. Championships in the 1997–1998 season. The following season, she competed on the ISU Junior Grand Prix and won the silver medal at the 1998–1999 Junior Grand Prix Final. She also took silver at the 1999 World Junior Championships held in November 1998. At the 1999 U.S. Championships, Hughes won the pewter medal in her senior-level debut. As the fourth-place finisher, Hughes would not normally have received one of the three spots for U.S. ladies at the 1999 World Championships, however, Naomi Nari Nam, the silver medalist, was not age-eligible for the event according to ISU rules. Hughes was likewise not age-eligible but at the time a loophole existed for skaters who had medaled at Junior Worlds. Hughes was sent to senior Worlds and finished 7th in her debut.
In the 1999–2000 season, Hughes made her Grand Prix debut, winning the bronze medal at the 1999 Trophée Lalique. She won the bronze medal at the 2000 U.S. Championships and was credited with a triple-salchow-triple-loop combination. She placed 5th at the 2000 World Championships.
In the 2000–2001 season, she won three medals on the Grand Prix circuit and won the bronze medal at the 2000–2001 Grand Prix of Figure Skating Final. She won the silver medal at the 2001 U.S. Championships. At the 2001 Worlds, she won the bronze medal.
In the 2001–2002 season, Hughes competed again on the Grand Prix, winning the 2001 Skate Canada International and placing second at her other two events. She won her second consecutive bronze medal at the Grand Prix Final and then won the bronze medal at the 2002 U.S. Championships to qualify for the 2002 Winter Olympics.
The week before the opening of the 2002 Olympics, Hughes appeared on the cover of "Time" magazine.
At the 2002 Olympics, Hughes placed fourth in the short program after being penalized for underrotating her triple flip and lutz. In her long program, she landed seven triple jumps, including a triple toe loop-triple loop and a triple salchow-triple loop combination. She won the long program, as the three contenders ahead of her after the short program all made mistakes in their respective long programs. Figure Skating rules at the time dictated that if someone placed fourth in the short program, but won the free skate, they could not automatically win the event. Michelle Kwan, who was in first place after the short program would have to lose the free program to Hughes and one other skater as well. Hughes won the free skate, with Irina Slutskaya placing second in that portion, ahead of Kwan. Therefore, the final standings were Hughes in first, Slutskaya in second and Kwan in third. She is the only American woman to have won the Olympic title without ever having won either a World or U.S. senior national title.
After her Olympic win, Hughes was honored with a parade in her hometown of Great Neck. Senator Hillary Rodham Clinton spoke at the event and declared it Sarah Hughes Day. She received the James E. Sullivan Award as the top amateur athlete in the U.S.
Hughes did not compete at the 2002 Worlds. For the 2002–2003 season, she won the silver medal at the 2003 U.S. Championships and placed sixth at the 2003 World Championships.
Hughes took the 2004–2005 year off from college to skate professionally with the Smuckers Stars on Ice tour company. She was inducted into the International Jewish Sports Hall of Fame in 2005.
Hughes' biography, "Sudden Champion: The Sarah Hughes Story", was written by Richard Krawiec in 2002.
Skating technique.
Hughes had a variety of triple-triple jump combinations, including a triple loop-triple loop, triple salchow-triple loop, and a triple toe-triple loop. Her best jump was perhaps the triple loop which she often completed out of a back spiral. She was also known for her camel spin with change of edge, and her spiral position.

</doc>
<doc id="40321" url="https://en.wikipedia.org/wiki?curid=40321" title="Soundgarden">
Soundgarden

Soundgarden is an American rock band formed in Seattle, Washington, in 1984 by singer and rhythm guitarist Chris Cornell, lead guitarist Kim Thayil, and bassist Hiro Yamamoto. Matt Cameron became the band's full-time drummer in 1986, while bassist Ben Shepherd became a permanent replacement for Yamamoto in 1990.
Soundgarden was one of the seminal bands in the creation of grunge, a style of alternative rock that developed in Seattle, and was one of a number of grunge bands signed to the record label Sub Pop. Soundgarden was the first grunge band to sign to a major label (A&M Records, in 1988), though the band did not achieve commercial success until they popularized the genre in the early 1990s with Seattle contemporaries Pearl Jam, Nirvana, and Alice in Chains.
Soundgarden achieved its biggest success with the 1994 album "Superunknown", which debuted at number one on the "Billboard" charts and yielded the Grammy Award-winning singles "Black Hole Sun" and "Spoonman". In 1997, the band broke up due to internal strife over its creative direction. After more than a decade of working on projects and other bands, Soundgarden reunited in 2010 and their sixth studio album, "King Animal", was released two years later.
As of 2012, Soundgarden had sold more than 10.5 million records in the United States, and an estimated 22.5 million worldwide. VH1 ranked Soundgarden at number 14 in their special "100 Greatest Artists of Hard Rock".
History.
Formation and early recordings (1984–1988).
Soundgarden's origins can be found in a band called The Shemps, which performed around Seattle in the early 1980s, and featured bassist Hiro Yamamoto and drummer and singer Chris Cornell. Following Yamamoto's departure, the band recruited guitarist Kim Thayil as its new bassist. Thayil had moved to Seattle from Park Forest, Illinois, with Yamamoto and Bruce Pavitt, who would later start the independent record label Sub Pop. Cornell and Yamamoto stayed in contact, and after The Shemps broke up Cornell and Yamamoto started jamming together, and were eventually joined by Thayil.
Soundgarden was formed in 1984 by Cornell (drums and vocals), Yamamoto (bass), and Thayil (guitar). The band named themselves after a wind-channeling pipe sculpture, "A Sound Garden", located on National Oceanic and Atmospheric Administration property at 7600 Sand Point Way next to Magnuson Park, Seattle. Cornell originally played drums while singing, but in 1985 the band enlisted Scott Sundquist to allow Cornell to concentrate on vocals. The band traveled around playing various concerts with this line-up for about a year. Their first recordings were three songs that appeared on a 1986 compilation album for C/Z Records called "Deep Six". It also featured songs by fellow grunge pioneers Green River, Skin Yard, Malfunkshun, The U-Men, and The Melvins. In 1986, Sundquist left the band to spend time with his family, and was replaced by Matt Cameron, the drummer from Skin Yard.
KCMU DJ Jonathan Poneman was impressed after seeing Soundgarden perform one night, later saying, "I saw this band that was everything rock music should be." Poneman offered to fund a release by the band, so Thayil told him to team up with Bruce Pavitt. Poneman offered to contribute $20,000 in funding for Sub Pop, effectively turning it into a full-fledged record label. Soundgarden signed to Sub Pop, and the label released "Hunted Down" in 1987 as the band's first single. The B-side of the "Hunted Down" single, "Nothing to Say", appeared on the KCMU compilation tape "Bands That Will Make Money", which was distributed to record companies, many of whom showed interest in Soundgarden. Through Sub Pop, the band released the "Screaming Life" EP in 1987, and the "Fopp" EP in 1988. A combination of the two was issued as "Screaming Life/Fopp" in 1990.
Debut album, major label signing, and rift with audience (1988–1990).
Though the band was being courted by major labels, in 1988 it signed to the smaller label SST Records for its debut album, "Ultramega OK", released on October 31, 1988. Cornell said that the band "made a huge mistake with "Ultramega OK"" since they used a producer suggested by SST who "didn't know what was happening in Seattle." On that album, Soundgarden demonstrates, according to Steve Huey of AllMusic, a "Stooges/MC5-meets-Zeppelin/Sabbath sound." The band's first music video, "Flower", was directed by Mark Miremont, and aired regularly on MTV's "120 Minutes". Soundgarden supported "Ultramega OK" with a tour in the United States in the spring of 1989 and a tour in Europe, which began in May 1989 and was the band's first overseas tour. "Ultramega OK" earned the band a Grammy Award nomination for Best Metal Performance in 1990.
After touring in support of "Ultramega OK" the band signed with A&M Records, which caused a rift between Soundgarden and its traditional audience. Thayil said, "In the beginning, our fans came from the punk rock crowd. They abandoned us when they thought we had sold out the punk tenets, getting on a major label and touring with Guns N' Roses. There were fashion issues and social issues, and people thought we no longer belonged to their scene, to their particular sub-culture." The band subsequently began work on its first album for a major label, and personnel difficulties caused a shift in the band's songwriting process, according to Cornell: "At the time Hiro had excommunicated himself from the band and there wasn't a free-flowing system as far as music went, so I ended up writing a lot of it." On September 5, 1989, the band released its second album, "Louder Than Love", which saw the band take "a step toward the metal mainstream," according to Steve Huey of Allmusic, describing "a slow, grinding, detuned mountain of Sabbath/Zeppelin riffs and Chris Cornell wailing." Because of some of the lyrics, most notably on "Hands All Over" and "Big Dumb Sex", the band faced various retail and distribution problems upon the album's release. "Louder Than Love" became the band's first album to chart on the "Billboard" 200, peaking at number 108 on the chart in 1990.
A month before touring for "Louder Than Love" commenced, bassist Hiro Yamamoto, who was becoming frustrated that he wasn't contributing much, left to go back to college. He was replaced by Jason Everman, formerly of Nirvana. The band toured North America from December 1989 to March 1990, opening for Voivod, which was supporting their album "Nothingface" tour, with Faith No More and The Big F also serving as opening acts at the beginning and end of the tour. The band then went on to tour Europe. Bassist Jason Everman was fired immediately after Soundgarden completed its promotional tour for "Louder Than Love" in mid-1990; Thayil said that "Jason just "didn't" work out." "Louder Than Love" spawned the EP "Loudest Love" and the video compilation "Louder Than Live", both released in 1990.
Established lineup, censorship, and rise in popularity (1991–1993).
Bassist Ben Shepherd replaced previous bassist Jason Everman and the new line-up recorded Soundgarden's third album in 1991. Cornell said that Shepherd brought a "fresh and creative" approach to the recording sessions, and the band as a whole said that his knowledge of music and writing skills redefined the band. The resulting album, "Badmotorfinger", was released on October 8, 1991. Steve Huey of Allmusic said that the songwriting on "Badmotorfinger" "takes a quantum leap in focus and consistency." He added, "It's surprisingly cerebral and arty music for a band courting mainstream metal audiences." Thayil suggested that the album's lyrics are "like reading a novel man's conflict with himself and society, or the government, or his family, or the economy, or anything." The first single from "Badmotorfinger", "Jesus Christ Pose", garnered attention when MTV decided to ban its corresponding music video in 1991. Many listeners were outraged by the song and its video, perceiving it as anti-Christian. The band received death threats while on tour in the United Kingdom in support of the album. Cornell explained that the lyrics criticize public figures who use religion (particularly the image of Jesus Christ) to portray themselves as being persecuted. Although overshadowed at the time of its release by the sudden popularity of Nirvana's "Nevermind", the focus of attention brought by "Nevermind" to the Seattle scene helped Soundgarden gain wider attention. The singles "Outshined" and "Rusty Cage" were able to find an audience at alternative rock radio and MTV. "Badmotorfinger" was nominated for a Grammy Award for Best Metal Performance in 1992. The album was among the 100 top selling albums of 1992.
Following the release of "Badmotorfinger", Soundgarden went on a tour in North America that ran from October 1991 to November 1991. Afterward, the band took a slot opening for Guns N' Roses in North America on the band's Use Your Illusion Tour. Soundgarden was personally selected by Guns N' Roses as its opening band. The band took a slot opening for Skid Row in North America in February 1992 on the band's "Slave to the Grind" tour, and then headed to Europe for a month-long headlining theater tour. The band returned for a tour in the United States and subsequently rejoined Guns N' Roses in the summer of 1992 in Europe as part of the Use Your Illusion Tour along with fellow opening act Faith No More. Regarding the time spent opening for Guns N' Roses, Cornell said, "It wasn't a whole lot of fun going out in front of 40,000 people for 35 minutes every day. Most of them hadn't heard our songs and didn't care about them. It was a bizarre thing." The band would go on to play the 1992 Lollapalooza tour with the Red Hot Chili Peppers, Pearl Jam, Ministry and the Jim Rose Circus among others. In anticipation of the band's appearance at Lollapalooza, a limited edition of "Badmotorfinger" was released in 1992 with a second disc containing the EP "Satanoscillatemymetallicsonatas" (a palindrome), featuring Soundgarden's cover of Black Sabbath's "Into the Void", titled "Into the Void (Sealth)", which was nominated for a Grammy Award for Best Metal Performance in 1993. The band later released the video compilation "Motorvision", which was filmed at the Paramount Theatre in 1992. The band also made an appearance in the movie "Singles" performing "Birth Ritual". The song appeared on the , as did a Cornell solo song, "Seasons".
In 1993, the band contributed the track "Show Me" to the AIDS-Benefit Album No Alternative produced by the Red Hot Organization.
Breakthrough album and mainstream success (1994–1995).
Soundgarden began working on its fourth album after touring in support of "Badmotorfinger". Cornell said that while working on the album, the band members allowed each other more freedom than on past records, and Thayil observed that the band spent a lot more time working on the actual recording of the songs than on previous records. Released on March 8, 1994, "Superunknown" became the band's breakthrough album, driven by the singles "Spoonman", "The Day I Tried to Live", "Black Hole Sun", "My Wave", and "Fell on Black Days"; "Superunknown" debuted at number one on the "Billboard" 200 album chart.
The songs on "Superunknown" captured the creativity and heaviness of the band's earlier works, while showcasing the group's newly evolving style. Lyrically, the album was quite dark and mysterious, and it is often interpreted to be dealing with substance abuse, suicide, and depression. Cornell was inspired by the writings of Sylvia Plath at the time. The album was also more experimental than previous releases, with some songs incorporating Middle-Eastern or Indian music. J. D. Considine of "Rolling Stone" said "Superunknown" "demonstrates far greater range than many bands manage in an entire career." He also stated, "At its best, "Superunknown" offers a more harrowing depiction of alienation and despair than anything on "In Utero"." The music video for "Black Hole Sun" became a hit on MTV and received the award for Best Metal/Hard Rock Video at the 1994 MTV Video Music Awards and in 1995 it received the Clio Award for Alternative Music Video. Soundgarden won two Grammy Awards in 1995; "Black Hole Sun" received the award for Best Hard Rock Performance and "Spoonman" received the award for Best Metal Performance. "Superunknown" was nominated for a Grammy Award for Best Rock Album in 1995. "Superunknown" has been certified five times platinum in the United States and remains Soundgarden's most successful album.
The band began touring in January 1994 in Oceania and Japan, areas where the record came out early and where the band had never toured before. This round of touring ended in February 1994, and then in March 1994 the band moved on to Europe. They began a theater tour of the United States on May 27, 1994, with the opening acts Tad and Eleven. In late 1994, after touring in support of "Superunknown", doctors discovered that Cornell had severely strained his vocal cords, and Soundgarden canceled several shows to avoid causing any permanent damage. Cornell said, "I think we kinda overdid it! We were playing five or six nights a week and my voice pretty much took a beating. Towards the end of the American tour I felt like I could still kinda sing, but I wasn't really giving the band a fair shake. You don't buy a ticket to see some guy croak for two hours! That seemed like kind of a rip off." The band would make up the dates later in 1995. "Superunknown" spawned the EP "Songs from the Superunknown" and the CD-ROM "Alive in the Superunknown", both released in 1995.
"Down on the Upside", internal conflicts and breakup (1996–1997).
Following the worldwide tour in support of "Superunknown", the band members began working on what would become their last studio album for over 15 years. The band chose to produce the record themselves. However, tensions within the group reportedly arose during the sessions, with Thayil and Cornell allegedly clashing over Cornell's desire to shift away from the heavy guitar riffing that had become the band's trademark. Cornell said, "By the time we were finished, it felt like it had been kind of hard, like it was a long, hard haul. But there was stuff we were discovering." The band's fifth album, "Down on the Upside", was released on May 21, 1996. The album was notably less heavy than the group's preceding albums, and marked a further departure from the band's grunge roots; Soundgarden explained at the time that it wanted to experiment with other sounds, which included acoustic instrumentation: David Browne of "Entertainment Weekly" said, "Few bands since Led Zeppelin have so crisply mixed instruments both acoustic and electric." The overall mood of the album's lyrics is less dark than on previous Soundgarden albums, with Cornell describing some songs as "self-affirming." The album spawned several singles, including "Pretty Noose", "Burden in My Hand", and "Blow Up the Outside World". "Pretty Noose" was nominated for a Grammy Award for Best Hard Rock Performance in 1997. Despite favorable reviews and modest sales, the album did not match the sales or praise of "Superunknown".
The band took a slot on the 1996 Lollapalooza tour with Metallica, who had insisted on Soundgarden's appearance on the tour. After Lollapalooza, the band embarked on a worldwide tour, and already-existing tensions increased during that tour. When asked whether the band hated touring, Cornell replied "We really enjoy it to a point, and then it gets tedious, because it becomes repetitious. You feel like fans have paid their money and they expect you to come out and play them your songs like the first time you ever played them. That's the point where we hate touring." At the tour's final stop in Honolulu, Hawaii on February 9, 1997, Shepherd threw his bass into the air in frustration after suffering equipment failure, and subsequently stormed off the stage. The band retreated, with Cornell returning to conclude the show with a solo encore. On April 9, 1997, the band announced it was disbanding. Thayil said, "It was pretty obvious from everybody's general attitude over the course of the previous half year that there was some dissatisfaction." Cameron later said that Soundgarden was "eaten up by the business." Soundgarden released a greatest hits collection entitled "A-Sides" on November 4, 1997. The album was composed of 17 songs, including the previously-unreleased "Bleed Together", which had been recorded during the "Down on the Upside" recording sessions.
Post-breakup activities (1998–2009).
Cornell released a solo album in September 1999, entitled "Euphoria Morning", which featured Matt Cameron on the track "Disappearing One". Later, in 2001, Cornell formed the platinum-selling supergroup Audioslave with Tom Morello, Tim Commerford and Brad Wilk, then-former members of Rage Against the Machine, which recorded three albums (US:Triple-Platinum) "Audioslave" (2002), (US:Platinum) "Out of Exile" (2005), and (US:Gold) "Revelations" (2006)). Cornell left Audioslave in early 2007, resulting in the band's break-up. His second solo album, "Carry On", was released in June 2007 and his third solo album, "Scream", produced by Timbaland, was released in March 2009, both to mixed commercial and critical success. In 2009 Cornell also provided the vocals for "Promise" on Slash's debut solo album "Slash".
Thayil joined forces with former Dead Kennedys singer Jello Biafra and former Nirvana bassist Krist Novoselic and drummer Gina Mainwal for one show, performing as The No WTO Combo during the WTO ministerial conference in Seattle on December 1, 1999. Thayil later contributed guitar tracks to Steve Fisk's 2001 album, "999 Levels of Undo", as well as Dave Grohl's 2004 side-project album, "Probot". In 2006, Thayil played guitar on the album "Altar", the collaboration between the bands Sunn O))) and Boris.
Cameron initially turned his efforts to his side-project Wellwater Conspiracy, to which both Shepherd and Thayil have contributed. He then worked briefly with The Smashing Pumpkins on the band's 1998 album, "Adore". In 1998, he stepped in on drums for Pearl Jam's Yield Tour following Jack Irons's health problems, and subsequently joined Pearl Jam as an official member; he has recorded five albums as the band's drummer ("Binaural" (2000), "Riot Act" (2002), "Pearl Jam" (2006), "Backspacer" (2009) and "Lightning Bolt" (2013). Cameron also played percussion on Geddy Lee's album "My Favourite Headache".
Shepherd was the singer on Wellwater Conspiracy's 1997 debut studio album, "Declaration of Conformity", but left the band in 1998. He has toured with Mark Lanegan and played bass on two of Lanegan's albums, "I'll Take Care of You" (1999) and "Field Songs" (2001). Shepherd and Cameron lent a hand with recording Tony Iommi's album "IOMMI" (2000); they were part of the side-project band Hater while they were members of Soundgarden and in 2005 Shepherd released the band's long-delayed second album, "The 2nd".
In a July 2009 interview with "Rolling Stone", Cornell shot down rumors of a reunion, saying that conversations between the band members had been limited to discussion about the release of a box set or B-sides album of Soundgarden rarities, and that there had been no discussion of a reunion at all. The band's interest in new releases emerged from a 2008 reunion regarding their shared properties, both financial and legal, where they realized Soundgarden lacked online presence such as a website or Facebook page. As Thayil summed up, "we kind of had neglected our merchandise over the last decade." Eventually the musicians decided to create an official site handled by Pearl Jam's Ten Club, relaunch their catalog, and according to Cameron, seek "a bunch of unreleased stuff we wanted to try to put out.” On March 2009, Thayil, Shepherd and Cameron got onstage during a concert by Tad Doyle in Seattle and played some Soundgarden songs. Cornell stated that the moment "sort of sparked the idea: If Matt, Kim, and Ben can get in a room, rehearse a couple songs, and play, maybe we all could do that as Soundgarden.”
On October 6, 2009, all the members of Soundgarden attended Night 3 of Pearl Jam's four-night stand at the Gibson Amphitheatre in Universal City, CA. During an encore, Temple of the Dog reunited for the first time since Pearl Jam's show at the Santa Barbara Bowl on October 28, 2003. Chris Cornell joined the band to sing "Hunger Strike". It was the first public appearance of Soundgarden together since their breakup in April 1997. Consequently, rumors of an impending reunion were circulating on the internet.
Reunion, "Telephantasm" and "King Animal" (2010–2013).
On January 1, 2010, Cornell alluded to a Soundgarden reunion via his Twitter, writing: "The 12-year break is over and school is back in session. Sign up now. Knights of the Soundtable ride again!" The message linked to a website that features a picture of the group performing live and a place for fans to enter their e-mail addresses to get updates on the reunion. Entering that information unlocks an archival video for the song "Get on the Snake", from Soundgarden's second studio album, 1989's "Louder Than Love". On March 1, 2010, Soundgarden announced to the people who signed their e-mail subscribers that they are re-releasing an old single "Hunted Down" with the song "Nothing to Say" on a 7-inch vinyl released on April 17 only at Record Store Day. Also, they released "Spoonman" live at the Del Mar Fairgrounds in San Diego, California from 1996. Soundgarden played its first show since 1997 on April 16 at the Showbox at the Market in the band's hometown of Seattle. The band headlined Lollapalooza on August 8.
"Telephantasm: A Retrospective", a new Soundgarden compilation album, was packaged with initial shipments of the "" video game and released on September 28, 2010, one week before the CD's availability in stores on October 5, 2010. An expanded version of "Telephantasm" consisting of two CDs and one DVD is currently available for sale. A previously unreleased Soundgarden song—"Black Rain"—debuted on the "Guitar Hero" video game and appears on the compilation album. The compilation album achieved platinum certification status after its first day of retail availability. "Black Rain" hit rock radio stations on August 10, 2010. It became the band's first single since 1997. In November 2010, Soundgarden was the second musical guest on the show "Conan", making it their first television appearance in 13 years, and issued a 7-inch vinyl, "The Telephantasm", for Black Friday Record Store Day. In March 2011, Soundgarden released their first live album, "Live on I-5".
In February 2011 it was announced on Soundgarden's homepage that they had started recording a new album. On March 1, 2011, Chris Cornell confirmed that Adam Kasper would produce the new album. Four days later, the band stated it would consist of material that was "90 percent new" and the rest consisting of updated versions of older ideas. They also noted that they had 12 to 14 songs that were "kind of ready to go". Although Cameron claimed the album would be released in 2011, the recording was prolonged as Thayil said that "the more we enjoy it, the more our fans should end up enjoying it.". Thayil also reported that some songs sound "similar in a sense to "Down on the Upside"" and that the album would be "picking up where we left off. There are some heavy moments, and there are some fast songs." The next day, Cornell reported that the new album would not be released until the spring of 2012.
In April 2011, Soundgarden announced a summer tour consisting of four dates in July, and was also headliner for Voodoo Experience at City Park in New Orleans Halloween weekend 2011. It was announced in March 2012 via the band's official Facebook page that they would be including a new song on the soundtrack of the upcoming movie "The Avengers", based on the franchise by Marvel Comics. The song was titled "Live to Rise" and marked the first newly recorded song that the band have released since reforming in 2010. "Live to Rise" was released as a free download on iTunes April 17. Also in March it was announced that Soundgarden would headline the Friday night of the Hard Rock Calling Festival the following July. In April, Soundgarden announced the release of a box set titled ""Classic Album Selection"" for Europe, containing all of their studio albums (except for "Ultramega OK") and live album "Live on I-5". On May 5, just before The Offspring began playing their set, the band appeared as a special guest at the 20th annual KROQ Weenie Roast in Irvine, California. Later that month, Soundgarden told Rolling Stone they were eyeing an October release for their new album. That June, the band appeared at Download Festival in Donington, England. The band released "Been Away Too Long", the first single from their new album "King Animal" on September 27. "King Animal" was released on November 13, 2012. The band released a video for "By Crooked Steps", directed by Dave Grohl, in early 2013. "Halfway There" was the third single to be released from the album.
"Echo of Miles..." and next album (2014–present).
On November 15, 2013, drummer Matt Cameron announced that he would not be touring with Soundgarden in 2014, due to prior commitments promoting Pearl Jam's album "Lightning Bolt". On March 16, 2014, it was announced that Soundgarden and Nine Inch Nails, along with opening act Death Grips, were going to tour together in North America. On March 27, 2014, former Pearl Jam drummer Matt Chamberlain replaced Cameron for live shows in South America and Europe.
On October 28, 2014, Soundgarden announced that they would release the 3CD compilation box set, "", on November 24. The set is a collection of rarities, live tracks, and unreleased material spanning the group's history. It includes previously released songs, such as "Live to Rise", "Black Rain", "Birth Ritual", and others, as well as a newly recorded rendition of a song from the band's pre-Matt Cameron 1985 demo, "The Storm", now simply titled "Storm", which was produced by Jack Endino. One day prior to the announcement, on October 27, the band posted a copy of "Storm" to YouTube unannounced.
Thayil had mentioned in several interviews that it was highly likely that the band would start working on material for a new album in 2015. In August 2015, Cornell stated that they were working on songs for the album. On January 19, 2016, it was announced that Soundgarden has returned to the studio to continue working on their new album.
Musical style and influences.
Soundgarden were pioneers of the grunge music genre, which mixed elements of punk rock and metal into a dirty, aggressive sound. "Soundgarden are quite good..." remarked Black Sabbath's Tony Iommi, "It's very much like the same sort of stuff that we would have done." Soundgarden's sound during the early years of the Seattle grunge scene has been described as consisting of "gnarled neo-Zeppelinisms." The influence of Led Zeppelin was evident, with "Q" magazine noting that Soundgarden were "in thrall to '70s rock, but contemptuous of the genre's overt sexism and machismo." According to Sub Pop, the band had "a hunky lead singer and fused Led Zeppelin and the Butthole Surfers." The Butthole Surfers' mix of punk, heavy metal and noise rock was a major influence on the early work of Soundgarden.
The name of the band, according to Thayil, was supposed to include the many roots of their style: that included "a virtual plethora of cutting edge rock that spans Velvet Underground, Meat Puppets and Killing Joke". They also mentioned "Metallica; Gothicism and sublime poetry. The almost ethereal flavour of the name betrays the brutality of the music but never pins Soundgarden in one corner".
Black Sabbath also had a huge impact on the band's sound, especially on the guitar riffs and tunings. Joel McIver stated: "Soundgarden are one of the bands I've heard closest to the original Sabbath sound". Soundgarden, like other early grunge bands, were also influenced by British post-punk bands such as Gang of Four and Bauhaus which were popular in the early 1980s Seattle scene. Cornell himself said: "When Soundgarden formed we were post-punk – pretty quirky. Then somehow we found this neo-Sabbath psychedelic rock that fitted well with who we were". Thayil described the band's sound as a "Sabbath-influenced punk".
Soundgarden broadened its musical range with its later releases. By 1994's "Superunknown", the band began to incorporate more psychedelic influences into its music. As a member of Soundgarden, Cornell became known for his wide vocal range and his dark, existentialist lyrics.
Soundgarden often uses alternative tunings in its songs. Many Soundgarden songs are performed in drop D tuning, including "Jesus Christ Pose", "Outshined", "Spoonman", "Black Hole Sun" and "Black Rain". The E strings of the instruments were at times tuned even lower, such as on "Rusty Cage", where the lower E is tuned all the way down to B. Some songs use more unorthodox tunings: "Been Away Too Long", "My Wave" and "The Day I Tried to Live" are all in a E–E–B–B–B–B tuning and "Burden in My Hand", "Head Down" and "Pretty Noose" in a tuning of C-G-C-G-G-E".
Soundgarden also uses unorthodox time signatures; while such songs as "Jesus Christ Pose", "4th of July", and "Blow Up the Outside World" are in typical 4/4 time, "Outshined" is in 7/4, "My Wave" is in 5/4 and 4/4, "He Didn't" is in 5/4 and 6/4, "Black Hole Sun" is in 4/4 and 9/8, "The Day I Tried to Live" is in 15/8 for its verses and switches to 4/4 for the second half of its choruses. "Fell on Black Days" and "Somewhere" are in 6/4, "Never the Machine Forever" and "Black Rain" are in 9/8, "Beyond the Wheel", "Get on the Snake" and "New Damage" are in 9/4, "Face Pollution" uses 9/8 and 6/4, "Rusty Cage" is in 4/4, 7/4, and 19/8, "Ugly Truth" is in 4/4 and 6/8, "Limo Wreck" alternates between 12/8, 15/8, 9/8, and 6/8, "Half" is in 5/8 with a measure of 11/16 before a 4/4 section, and "Spoonman" alternates between 7/4 verses and 4/4 choruses with a section in 6/4.
Thayil has said that Soundgarden usually did not consider the time signature of a song until after the band had written it, and said that the use of odd meters was "a total accident." Thayil also used the meters as an example of the band's anti-commercial stance, saying that if Soundgarden "were in the business of hit singles, we'd at least write songs in 4/4 so you could dance to them."
Legacy.
Soundgarden was one of the early bands of the 1980s Seattle music scene and is regarded as being one of the originators of the genre later known as grunge. The development of the Seattle independent record label Sub Pop is tied closely to Soundgarden, since Sub Pop co-founder Jonathan Poneman funded Soundgarden's early releases, and the band's success led to the expansion of Sub Pop as a serious record label. Nirvana frontman Kurt Cobain was a fan of Soundgarden's early music, and reportedly Soundgarden's involvement with Sub Pop influenced Cobain to sign Nirvana with the label. Soundgarden was the first grunge band to sign to a major label when the band joined the roster of A&M Records in 1989. Soundgarden, however, did not achieve initial success, and only with successive album releases did the band meet with increased sales and wider attention. Bassist Ben Shepherd has not been receptive to the grunge label, saying in a 2013 interview "That's just marketing. It's called rock and roll, or it's called punk rock or whatever. We never were Grunge, we were just a band from Seattle." They were ranked No. 14 on VH1's "100 Greatest Artists of Hard Rock".
Regarding Soundgarden's legacy, in a 2007 interview Cornell said, 
I think, and this is now with some distance in listening to the records, but on the outside looking in with all earnestness I think Soundgarden made the best records out of that scene. I think we were the most daring and experimental and genre pushing really and I'm really proud of it. And I guess that's why I have trepidation about the idea of reforming. I don't know what it would mean. I guess I just have this image of who we were and I had probably a lot of anxiety during the period of being Soundgarden, as we all did, that it was a responsibility and it was an important band of music and we didn't want to mess it up and we managed to not, which I felt is a great achievement.
Soundgarden has been praised for its technical musical ability and the expansion of its sound as the band's career progressed. "Heavy yet ethereal, powerful yet always-in-control, Soundgarden's music was a study in contrasts," said Henry Wilson of "Hit Parader". Wilson proclaimed the band's music as "a brilliant display of technical proficiency tempered by heart-felt emotion." Soundgarden is one of the bands credited with the development of the genre of alternative metal, with Stephen Thomas Erlewine of AllMusic stating that "Soundgarden made a place for heavy metal in alternative rock." Ben Ratliff of "Rolling Stone" defined Soundgarden as the "standard-bearers of stoner rock" during the 1990s. The band inspired and influenced a number of bands, such as metalcore bands like Between the Buried and Me and The Dillinger Escape Plan.
Band members.
Current members
Former members
Touring members
Awards and nominations.
Grammy Awards

</doc>
<doc id="40322" url="https://en.wikipedia.org/wiki?curid=40322" title="Theodore Beza">
Theodore Beza

Theodore Beza (; or "de Besze"; June 24, 1519 – October 13, 1605) was a French Protestant Christian theologian and scholar who played an important role in the Reformation. A member of the monarchomaque movement who opposed absolute monarchy, he was a disciple of John Calvin and lived most of his life in Switzerland.
Biography.
Early life.
Theodore Beza was born at Vézelay, in Burgundy, France. His father, Pierre de Beze, royal governor of Vézelay, descended from a Burgundian family of distinction; his mother, Marie Bourdelot, was known for her generosity. Beza's father had two brothers; Nicholas, who was member of Parliament at Paris; and Claude, who was abbot of the Cistercian monastery Froimont in the diocese of Beauvais.
Nicholas, who was unmarried, during a visit to Vézelay was so pleased with Theodore that, with the permission of his parents, he took him to Paris to educate him there. From Paris, Theodore was sent to Orléans in December 1528 to receive instruction from the famous German teacher Melchior Wolmar. He was received into Wolmar's house, and the day on which this took place was afterward celebrated as a second birthday.
Young Beza soon followed his teacher to Bourges, where the latter was called by the duchess Margaret of Angoulême, sister of Francis I. At the time, Bourges was the focus of the Reformation movement in France. In 1534, after Francis I issued his edict against ecclesiastical innovations, Wolmar returned to Germany. Beza, in accordance with the wish of his father, went back to Orléans to study law, and spent four years there (1535–39). The pursuit of law had little attraction for him; he enjoyed more the reading of the ancient classics, especially Ovid, Catullus, and Tibullus.
He received the degree of licentiate in law August 11, 1539, and, as his father desired, went to Paris, where he began to practice. To support him, his relatives had obtained for him two benefices, the proceeds of which amounted to 700 golden crowns a year; and his uncle had promised to make him his successor.
Beza spent two years in Paris and gained a prominent position in literary circles. To escape the many temptations to which he was exposed, with the knowledge of two friends, he became engaged in the year 1544 to a young girl of humble descent, Claudine Denoese, promising to publicly marry her as soon as his circumstances would allow it.
In 1548 he published a collection of Latin poetry, "Juvenilia", which made him famous, and he was widely considered one of the best writers of Latin poetry of his time. Some cautioned against reading biographical details in his writings. Philip Schaff argued that it was a mistake to "read between his lines what he never intended to put there" or to imagine "offences of which he was not guilty even in thought."
Shortly after the publication of his book, he fell ill and his illness, it is reported, revealed to him his spiritual needs. Gradually he came to accept salvation in Christ, which lifted his spirits. He then resolved to sever his connections of the time, and went to Geneva, the French city of refuge for Evangelicals (adherents of the Reformation movement), where he arrived with Claudine on October 23, 1548.
Teacher at Lausanne.
He was received by John Calvin, who had met him already in Wolmar's house, and was married in the church. Beza was at a loss for immediate occupation so he went to Tübingen to see his former teacher Wolmar. On his way home, he visited Pierre Viret at Lausanne, who brought about his appointment as professor of Greek at the academy there in November 1549.
Beza found time to write a Biblical drama, "Abraham Sacrifiant ", in which he contrasted Catholicism with Protestantism, and the work was well received. The text of some verses includes directions for musical performance, but no music survives.
After Clément Marot's death in 1544, John Calvin asked Beza to complete his French metrical translations of the Psalms. Thirty-four of his translations were published in the 1551 edition of the Genevan Psalter, and six more were added to later editions.
About the same time he published "Passavantius, " a satire directed against Pierre Lizet, the former president of the Parliament of Paris, and principal originator of the "fiery chamber" ("chambre ardente"), who, at the time (1551), was abbot of St. Victor near Paris and publishing a number of polemical writings.
Of a more serious character were two controversies in which Beza was involved at this time. The first concerned the doctrine of predestination and the controversy of Calvin with Jerome Hermes Bolsec. The second referred to the burning of Michael Servetus at Geneva on October 27, 1553. In defense of Calvin and the Genevan magistrates, Beza published, in 1554, the work "De haereticis a civili magistratu puniendis" (translated into French in 1560).
Journeys on behalf of the Protestants.
In 1557, Beza took a special interest in the Waldensians of Piedmont, who were being harassed by the French government. On their behalf, he went with William Farel to Bern, Zürich, Basel, and Schaffhausen, then to Strasburg, Mömpelgard, Baden, and Göppingen. In Baden and Göppingen, Beza and Farel made a declaration concerning the Waldensians' views on the sacrament on May 14, 1557. The written declaration clearly stated their position and was well received by the Lutheran theologians, but was strongly disapproved of in Bern and Zurich.
In the autumn of 1558, Beza undertook a second journey with Farel to Worms by way of Strasburg in the hopes of bringing about an intercession by the Evangelical princes of the empire in favor of the persecuted brethren at Paris. 
With Melanchthon and other theologians then assembled at the Colloquy of Worms, Beza proposed a union of all Protestant Christians, but the proposal was decidedly denied by Zurich and Bern.
False reports reached the German princes that the hostilities against the Huguenots in France had ceased and no embassy was sent to the court of France. 
As a result, Beza undertook another journey with Farel, Johannes Buddaeus, and Gaspard Carmel to Strasburg and Frankfort, where the sending of an embassy to Paris was resolved upon.
Settles in Geneva.
Upon his return to Lausanne, Beza was greatly disturbed. In union with many ministers and professors in city and country, Viret at last thought of establishing a consistory and of introducing a church discipline which should apply excommunication especially at the celebration of the communion. 
But the Bernese, then in control of Lausanne, would have no Calvinistic church government. This caused many difficulties, and Beza thought it best in 1558, to settle at Geneva.
Here he was given chair of Greek in the newly established academy, and after Calvin's death also that of theology. 
He was also obliged to preach.
He completed the revision of Pierre Olivetan's translation of the New Testament, begun some years before. 
In 1559, he undertook another journey in the interest of the Huguenots, this time to Heidelberg. At about the same time, he had to defend Calvin against Joachim Westphal in Hamburg and Tilemann Heshusius.
More important than this polemical activity was Beza's statement of his own confession. It was originally prepared for his father in justification of his actions and published in revised form to promote Evangelical knowledge among Beza's countrymen. It was printed in Latin in 1560 with a dedication to Wolmar. An English translation was published at London 1563, 1572, and 1585. Translations into German, Dutch, and Italian were also issued.
Events of 1560–63.
In the mean time, things took such shape in France that the happiest future for Protestantism seemed possible. King Anthony of Navarre, yielding to the urgent requests of Evangelical noblemen, declared his willingness to listen to a prominent teacher of the Church. Beza, a French nobleman and head of the academy in the metropolis of French Protestantism, was invited to Castle Nerac, but he could not plant the seed of Evangelical faith in the heart of the king.
In the following year, 1561, Beza represented the Evangelicals at the Colloquy of Poissy, and in an eloquent manner defended the principles of the Evangelical faith. 
The colloquy was without result, but Beza as the head and advocate of all Reformed congregations of France was revered and hated at the same time. 
The queen insisted upon another colloquy, which was opened at St. Germain Jan. 28, 1562, eleven days after the proclamation of the famous January edict, which granted important privileges to those of the Reformed faith. 
But the colloquy was broken off when it became evident that the Catholic party was preparing (after the Massacre of Vassy, on March 1) to overthrow Protestantism.
Beza hastily issued a circular letter (March 25) to all Reformed congregations of the empire, and went to Orléans with the Huguenot leader Conde and his troops. It was necessary to proceed quickly and energetically. But there were neither soldiers nor money. At the request of Conde, Beza visited all Huguenot cities to obtain both. He also wrote a manifesto in which he argued the justice of the Reformed cause. As one of the messengers to collect soldiers and money among his coreligionists, Beza was appointed to visit England, Germany, and Switzerland. He went to Strasburg and Basel, but met with failure. He then returned to Geneva, which he reached September 4. He had hardly been there fourteen days when he was called once more to Orléans by D'Andelot. The campaign was becoming more successful; but the publication of the unfortunate edict of pacification which Conde accepted (Mar. 12,1563) filled Beza and all Protestant France with horror.
Calvin's successor.
For twenty-two months Beza had been absent from Geneva, and the interests of school and Church there and especially the condition of Calvin made it necessary for him to return. For there was no one to take the place of Calvin, who was sick and unable longer to bear the burden resting on him. Calvin and Beza arranged to perform their duties jointly in alternate weeks, but the death of Calvin occurred soon afterward (May 27, 1564). As a matter of course Beza was his successor.
Until 1580, Beza was not only moderator of the Company of Pastors, but also the real soul of the great institution of learning at Geneva which Calvin had founded in 1559, consisting of a gymnasium and an academy. 
As long as he lived, Beza was interested in higher education. 
The Protestant youth for nearly forty years thronged his lecture-room to hear his theological lectures, in which he expounded the purest Calvinistic orthodoxy. 
As a counselor he was listened to by both magistrates and pastors. 
Geneva is indebted to him for the founding of a law school in which François Hotman, Jules Pacius, and Denys Godefroy, the most eminent jurists of the century, lectured in turn (cf. Charles Borgeaud, "L'Academie de Calvin, " Geneva, 1900).
Course of events after 1564.
As Calvin's successor, Beza was very successful, not only in carrying on his work but also in giving peace to the Church at Geneva. The magistrates had fully appropriated the ideas of Calvin, and the direction of spiritual affairs, the organs of which were the "ministers of the word" and "the consistory", was founded on a solid basis. No doctrinal controversy arose after 1564. The discussions concerned questions of a practical, social, or ecclesiastical nature, such as the supremacy of the magistrates over the pastors, freedom in preaching, and the obligation of the pastors to submit to the majority of the Company of Pastors.
Beza obtruded his will in no way upon his associates, and took no harsh measures against injudicious or hot-headed colleagues, though sometimes he took their cases in hand and acted as mediator; and yet he often experienced an opposition so extreme that he threatened to resign. Although he was inclined to take the part of the magistrates, he knew how to defend the rights and independence of the spiritual power when occasion arose, without, however, conceding to it such a preponderating influence as did Calvin.
Beza did not believe it wise for the Company of Pastors to have a permanent head. He convinced the Company to petition the Small Council to have limited terms for the position of moderator. In 1580 the Council agreed to a system of weekly rotating presidency.
His activity was great. He mediated between the "compagnie" and the magistracy; the latter continually asked his advice even in political questions. He corresponded with all the leaders of the Reformed party in Europe. After the St. Bartholomew's Day Massacre (1572), he used his influence to give to the refugees a hospitable reception at Geneva.
In 1574, he wrote his "De jure magistratuum" (Right of Magistrates), in which he emphatically protested against tyranny in religious matters, and affirmed that it is legitimate for a people to oppose an unworthy magistracy in a practical manner and if necessary to use weapons and depose them.
To sum up: Without being a great dogmatician like his master, nor a creative genius in the ecclesiastical realm, Beza had qualities which made him famous as humanist, exegete, orator, and leader in religious and political affairs, and qualified him to be the guide of the Calvinists in all Europe. In the various controversies into which he was drawn, Beza often showed an excess of irritation and intolerance, from which Bernardino Ochino, pastor of the Italian congregation at Zurich (on account of a treatise which contained some objectionable points on polygamy), and Sebastian Castellio at Basel (on account of his Latin and French translations of the Bible) had especially to suffer.
With Reformed France, Beza continued to maintain the closest relations. He was the moderator of the general synod which met in April, 1571, at La Rochelle and decided not to abolish church discipline or to acknowledge the civil government as head of the Church, as the Paris minister Jean Morel and the philosopher Pierre Ramus demanded; it also decided to confirm anew the Calvinistic doctrine of the Lord's Supper (by the expression: "substance of the body of Christ") against Zwinglianism, which caused a very unpleasant discussion between Beza and Ramus and Heinrich Bullinger.
In the following year (May, 1572) he took an important part in the national synod at Nîmes. He was also interested in the controversies which concerned the Augsburg Confession in Germany, especially after 1564, on the doctrine of the Person of Christ and the sacrament, and published several works against Westphal, Hesshusen, Selnecker, Johannes Brenz, and Jakob Andrea. This made him, especially after 1571, hated by all those who adhered to Lutheranism in opposition to Melanchthon.
The Colloquy of Montbéliard.
The last polemical conflict of importance Beza encountered from the Lutherans was at the Colloquy of Montbéliard, Mar. 14-27, 1586, to which he had been invited by the Lutheran Count Frederick of Württemberg at the wish of the French-speaking and Reformed residents as well as by French noblemen who had fled to Montbéliard. As a matter of course the intended union which was the purpose of the colloquy was not brought about; nevertheless it called forth serious developments within the Reformed Church.
When the edition of the acts of the colloquy, as prepared by Jakob Andrea, was published, Samuel Huber, of Burg near Bern, who belonged to the Lutheranizing faction of the Swiss clergy, took so great offense at the supralapsarian doctrine of predestination propounded at Montbéliard by Beza and Musculus that he felt it to be his duty to denounce Musculus to the magistrates of Bern as an innovator in doctrine. To adjust the matter, the magistrates arranged a colloquy between Huber and Musculus (September 2, 1587), in which the former represented the universalism, the latter the particularism, of grace.
As the colloquy was resultless, a debate was arranged at Bern, Apr. 15-18, 1588, at which the defense of the accepted system of doctrine was at the start put into Beza's hands. The three delegates of the Helvetic cantons who presided at the debate declared in the end that Beza had substantiated the teaching propounded at Montbéliard as the orthodox one, and Huber was dismissed from his office.
Last days.
After that time Beza's activity was confined more and more to the affairs of his home. His faithful wife Claudine had died childless in 1588, a few days before he went to the Bern Disputation. Forty years they had lived happily together. He contracted, on the advice of his friends, a second marriage with Catharina del Piano, a Genoese widow, in order to have a helpmate in his declining years. Up to his sixty-fifth year he enjoyed excellent health, but after that a gradual sinking of his vitality became perceptible. He was active in teaching until January 1597.
The saddest experience in his old days was the conversion of King Henry IV to Catholicism, in spite of his most earnest exhortations (1593). Strange to say, in 1596 the report was spread by the Jesuits in Germany, France, England, and Italy that Beza and the Church of Geneva had returned into the bosom of Rome, and Beza replied in a satire that revealed the possession still of his old fire of thought and vigor of expression.
He died in Geneva. He was not buried, like Calvin, in the general cemetery, Plain-Palais (for the Savoyards had threatened to abduct his body to Rome), but at the direction of the magistrates, in the monastery of St. Pierre.
Literary works.
Humanistic and historical writings.
In Beza's literary activity as well as in his life, distinction must be made between the period of the humanist (which ended with the publication of his "Juvenilia") and that of the ecclesiastic. Combining his pastoral and literary gifts, Beza wrote the first drama produced in French, Abrahm Sacrifiant; a play that is an antecedent to the work of Racine and is still occasionally produced today. Later productions like the humanistic, biting, satirical "Passavantius" and his "Complainte de Messire Pierre Lizet..." prove that in later years he occasionally went back to his first love. In his old age he published his "Cato censorius " (1591), and revised his "Poemata", from which he purged juvenile eccentricities.
Of his historiographical works, aside from his "Icones" (1580), which have only an iconographical value, mention may be made of the famous "Histoire ecclesiastique des Eglises reformes au Royaume de France" (1580), and his biography of Calvin, with which must be named his edition of Calvin's "Epistolae et responsa" (1575).
Theological works.
But all these humanistic and historical studies are surpassed by his theological productions (contained in "Tractationes theologicae"). In these Beza appears the perfect pupil or the "alter ego " of Calvin. His view of life is deterministic and the basis of his religious thinking is the predestinate recognition of the necessity of all temporal existence as an effect of the absolute, eternal, and immutable will of God, so that even the fall of the human race appears to him essential to the divine plan of the world. Beza, in tabular form, thoroughly elucidates the religious views which emanated from a fundamental supralapsarian mode of thought. This he added to his highly instructive treatise "Summa totius Christianismi."
Beza's "De vera excommunicatione et Christiano presbyterio" (1590), written as a response to Thomas Erastus's "Explicatio gravissimae quaestionis utrum excommunicatio" (1589) contributed an important defense of the right of ecclesiastical authorities (rather than civil authorities) to excommunicate.
Beza's Greek New Testament.
Of no less importance are the contributions of Beza to Biblical scholarship. In 1565 he issued an edition of the Greek New Testament, accompanied in parallel columns by the text of the Vulgate and a translation of his own (already published as early as 1556). Annotations were added, also previously published, but now he greatly enriched and enlarged them.
In the preparation of this edition of the Greek text, but much more in the preparation of the second edition which he brought out in 1582, Beza may have availed himself of the help of two very valuable manuscripts. One is known as the "Codex Bezae" or "Cantabrigensis, " and was later presented by Beza to the University of Cambridge; the second is the "Codex Claromontanus", which Beza had found in Clermont (now in the National Library at Paris).
It was not, however, to these sources that Beza was chiefly indebted, but rather to the previous edition of the eminent Robert Estienne (1550), itself based in great measure upon one of the later editions of Erasmus. Beza's labors in this direction were exceedingly helpful to those who came after. The same thing may be asserted with equal truth of his Latin version and of the copious notes with which it was accompanied. The former is said to have been published over a hundred times.
Although some contend that Beza's view of the doctrine of predestination exercised an overly dominant influence upon his interpretation of the Scriptures, there is no question that he added much to a clear understanding of the New Testament.

</doc>
<doc id="40323" url="https://en.wikipedia.org/wiki?curid=40323" title="Inertial confinement fusion">
Inertial confinement fusion

Inertial confinement fusion (ICF) is a type of fusion energy research that attempts to initiate nuclear fusion reactions by heating and compressing a fuel target, typically in the form of a pellet that most often contains a mixture of deuterium and tritium.
To compress and heat the fuel, energy is delivered to the outer layer of the target using high-energy beams of laser light, electrons or ions, although for a variety of reasons, almost all ICF devices have used lasers. The heated outer layer explodes outward, producing a reaction force against the remainder of the target, accelerating it inwards, compressing the target. This process is designed to create shock waves that travel inward through the target. A sufficiently powerful set of shock waves can compress and heat the fuel at the center so much that fusion reactions occur.
The energy released by these reactions will then heat the surrounding fuel, and if the heating is strong enough this could also begin to undergo fusion. The aim of ICF is to produce a condition known as "ignition", where this heating process causes a chain reaction that burns a significant portion of the fuel. Typical fuel pellets are about the size of a pinhead and contain around 10 milligrams of fuel: in practice, only a small proportion of this fuel will undergo fusion, but if all this fuel were consumed it would release the energy equivalent to burning a barrel of oil.
14 J/kg, the maximum efficiency of the hydrogen burning fusion reaction and the standard accepted value of burning a barrel of oil of ~6 gigajoules-->
ICF is one of two major branches of fusion energy research, the other being magnetic confinement fusion. When it was first proposed in the early 1970s, ICF appeared to be a practical approach to fusion power production and the field flourished. Experiments during the 1970s and '80s demonstrated that the efficiency of these devices was much lower than expected, and reaching ignition would not be easy. Throughout the 1980s and '90s, many experiments were conducted in order to understand the complex interaction of high-intensity laser light and plasma. These led to the design of newer machines, much larger, that would finally reach ignition energies.
The largest operational ICF experiment is the National Ignition Facility (NIF) in the US, designed using all of the decades-long experience of earlier experiments. Like those earlier experiments, however, NIF has failed to reach ignition and is, as of 2015, generating about of the required energy levels. As of October 7, 2013, this facility is understood to have achieved an important milestone towards commercialization of fusion, namely, for the first time a fuel capsule gave off more energy than was applied to it. This is a major step forward. A similar large-scale device in France, Laser Mégajoule, finished construction in Dec 2014, and was officially opened in 2015, but has not begun operation.
Description.
Basic fusion.
Fusion reactions combine lighter atoms, such as hydrogen, together to form larger ones. Generally the reactions take place at such high temperatures that the atoms have been ionized, their electrons stripped off by the heat; thus, fusion is typically described in terms of "nuclei" instead of "atoms".
Nuclei are positively charged, and thus repel each other due to the electrostatic force. Overcoming this repulsion costs a considerable amount of energy, which is known as the "Coulomb barrier" or "fusion barrier energy". Generally, less energy will be needed to cause lighter nuclei to fuse, as they have less charge and thus a lower barrier energy, and when they do fuse, more energy will be released. As the mass of the nuclei increase, there is a point where the reaction no longer gives off net energy—the energy needed to overcome the energy barrier is greater than the energy released in the resulting fusion reaction. The crossover point is iron, Fe56.
The best fuel from an energy perspective is a one to one mix of deuterium and tritium; both are heavy isotopes of hydrogen. The D-T (deuterium & tritium) mix has a low barrier because of its high ratio of neutrons to protons. The presence of neutral neutrons in the nuclei helps pull them together via the nuclear force, while the presence of positively charged protons pushes the nuclei apart via electrostatic force. Tritium has one of the highest ratios of neutrons to protons of any stable or moderately unstable nuclide—two neutrons and one proton. Adding protons or removing neutrons increases the energy barrier.
A mix of D-T at standard conditions does not undergo fusion; the nuclei must be forced together before the nuclear force can pull them together into stable collections. Even in the hot, dense center of the sun, the average proton will exist for billions of years before it fuses. For practical fusion power systems, the rate must be dramatically increased; heated to tens of millions of degrees, and/or compressed to immense pressures. The temperature and pressure required for any particular fuel to fuse is known as the Lawson criterion. These conditions have been known since the 1950s when the first H-bombs were built. To meet the Lawson Criterion is extremely difficult on Earth, which explains why fusion research has taken many years to reach the current high state of technical prowess.
ICF mechanism of action.
In a hydrogen bomb, the fusion fuel is compressed and heated with a separate fission bomb (see Teller-Ulam design). A variety of mechanisms transfers the energy of the fission "trigger"'s explosion into the fusion fuel. The requirement of a fission bomb makes the method impractical for power generation. Not only would the triggers be prohibitively expensive to produce, but there is a minimum size that such a bomb can be built, defined roughly by the critical mass of the plutonium fuel used. Generally it seems difficult to build nuclear devices smaller than about 1 kiloton in yield, which would make it a difficult engineering problem to extract power from the resulting explosions.
As the explosion size is scaled down, so too is the amount of energy needed to start the reaction off. Studies from the late 1950s and early 1960s suggested that scaling down into the megajoule energy range would require energy levels that could be delivered by any number of means. This led to the idea of using a device that would "beam" the energy at the fusion fuel, ensuring mechanical separation. By the mid-1960s, it appeared that the laser would develop to the point where the required energy levels would be available.
Generally ICF systems use a single laser, the "driver", whose beam is split up into a number of beams which are subsequently individually amplified by a trillion times or more. These are sent into the reaction chamber (called a target chamber) by a number of mirrors, positioned in order to illuminate the target evenly over its whole surface. The heat applied by the driver causes the outer layer of the target to explode, just as the outer layers of an H-bomb's fuel cylinder do when illuminated by the X-rays of the fission device.
The material exploding off the surface causes the remaining material on the inside to be driven inwards with great force, eventually collapsing into a tiny near-spherical ball. In modern ICF devices the density of the resulting fuel mixture is as much as one-hundred times the density of lead, around 1000 g/cm3. This density is not high enough to create any useful rate of fusion on its own. However, during the collapse of the fuel, shock waves also form and travel into the center of the fuel at high speed. When they meet their counterparts moving in from the other sides of the fuel in the center, the density of that spot is raised much further.
Given the correct conditions, the fusion rate in the region highly compressed by the shock wave can give off significant amounts of highly energetic alpha particles. Due to the high density of the surrounding fuel, they move only a short distance before being "thermalised", losing their energy to the fuel as heat. This additional energy will cause additional fusion reactions in the heated fuel, giving off more high-energy particles. This process spreads outward from the centre, leading to a kind of self-sustaining burn known as "ignition".
Issues with successful achievement.
The primary problems with increasing ICF performance since the early experiments in the 1970s have been of energy delivery to the target, controlling symmetry of the imploding fuel, preventing premature heating of the fuel (before maximum density is achieved), preventing premature mixing of hot and cool fuel by hydrodynamic instabilities and the formation of a 'tight' shockwave convergence at the compressed fuel center.
In order to focus the shock wave on the center of the target, the target must be made with extremely high precision and sphericity with aberrations of no more than a few micrometres over its surface (inner and outer). Likewise the aiming of the laser beams must be extremely precise and the beams must arrive at the same time at all points on the target. Beam timing is a relatively simple issue though and is solved by using delay lines in the beams' optical path to achieve picosecond levels of timing accuracy. The other major problem plaguing the achievement of high symmetry and high temperatures/densities of the imploding target are so called "beam-beam" imbalance and beam anisotropy. These problems are, respectively, where the energy delivered by one beam may be higher or lower than other beams impinging on the target and of "hot spots" within a beam diameter hitting a target which induces uneven compression on the target surface, thereby forming Rayleigh–Taylor instabilities in the fuel, prematurely mixing it and reducing heating efficacy at the time of maximum compression. The Richtmyer-Meshkov instability is also formed during the process due to shock waves being formed.
All of these problems have been substantially mitigated to varying degrees in the past two decades of research by using various beam smoothing techniques and beam energy diagnostics to balance beam to beam energy; however, RT instability remains a major issue. Target design has also improved tremendously over the years. Modern cryogenic hydrogen ice targets tend to freeze a thin layer of deuterium just on the inside of a plastic sphere while irradiating it with a low power IR laser to smooth its inner surface while monitoring it with a microscope equipped camera, thereby allowing the layer to be closely monitored ensuring its "smoothness". Cryogenic targets filled with a deuterium tritium (D-T) mixture are "self-smoothing" due to the small amount of heat created by the decay of the radioactive tritium isotope. This is often referred to as "beta-layering".
Certain targets are surrounded by a small metal cylinder which is irradiated by the laser beams instead of the target itself, an approach known as ""indirect drive"". In this approach the lasers are focused on the inner side of the cylinder, heating it to a superhot plasma which radiates mostly in X-rays. The X-rays from this plasma are then absorbed by the target surface, imploding it in the same way as if it had been hit with the lasers directly. The absorption of thermal x-rays by the target is more efficient than the direct absorption of laser light, however these "hohlraums" or "burning chambers" also take up considerable energy to heat on their own thus significantly reducing the overall efficiency of laser-to-target energy transfer. They are thus a debated feature even today; the equally numerous ""direct-drive"" design does not use them. Most often, indirect drive hohlraum targets are used to simulate thermonuclear weapons tests due to the fact that the fusion fuel in them is also imploded mainly by X-ray radiation.
A variety of ICF drivers are being explored. Lasers have improved dramatically since the 1970s, scaling up in energy and power from a few joules and kilowatts to megajoules (see NIF laser) and hundreds of terawatts, using mostly frequency doubled or tripled light from neodymium glass amplifiers.
Heavy ion beams are particularly interesting for commercial generation, as they are easy to create, control, and focus. On the downside, it is very difficult to achieve the very high energy densities required to implode a target efficiently, and most ion-beam systems require the use of a hohlraum surrounding the target to smooth out the irradiation, reducing the overall efficiency of the coupling of the ion beam's energy to that of the imploding target further.
History of ICF.
First conception.
In the US.
Inertial confinement feasibility began in the mid-1950s by the inventor of modern television, Dr. Philo Farnsworth. During this time period, he began concerted efforts to recreate the generation of high energy plasma he found in his earlier "Multipactor" tube design. With scant funding, he was successful in developing three generations of his "Fusor" tube. During the last several tests of this device, notable levels of excess output were produced. Patents were approved, and the science of this type of "inertial confinement" fusion is well documented. While it has been relatively dormant, it remains the first true experimentation into the concept.
A second stream of inertial confinement fusion history can be traced back to a seminal meeting called by Edward Teller in 1957 on the topic of peaceful uses of atomic explosions. Among the many topics covered during the event, some consideration was given to using a hydrogen bomb to heat a water-filled underground cavern. The resulting steam would then be used to power conventional generators, and thereby provide electrical power.
This meeting led to the Operation Plowshare efforts, given this name in 1961. Three primary concepts were studied as part of Plowshare; energy generation under Project PACER, the use of large nuclear explosions for excavation, and as a sort of nuclear fracking for the natural gas industry. PACER was directly tested in December 1961 when the 3 kt Project Gnome device was emplaced in bedded salt in New Mexico. In spite of all theorizing and attempts to stop it, radioactive steam was released from the drill shaft, some distance from the test site. Further studies as part of Project PACER led to a number of engineered cavities replacing natural ones, but through this period the entire Plowshare efforts turned from bad to worse, especially after the failure of 1962's Sedan which released huge quantities of fallout. PACER nevertheless continued to receive some funding until 1975, when a 3rd party study demonstrated that the cost of electricity from PACER would be the equivalent to conventional nuclear plants with fuel costs over ten times as great as they were.
Another outcome of the Teller meeting was to prompt John Nuckolls to start considering what happens when the fusion side of the bomb, the "secondary," was scaled down to very small size. His earliest work concerned the study of how small a fusion bomb could be made while still having a large "gain" to provide net energy output. This work suggested that at very small sizes, on the order of milligrams, very little energy would be needed to ignite it, much less than a fission "primary". He proposed building, in effect, tiny all-fusion explosives using a tiny drop of D-T fuel suspended in the center of a metal shell, today known as a hohlraum. The shell provided the same effect as the bomb casing in an H-bomb, trapping x-rays inside so they irradiated the fuel. The main difference is that the x-rays would not be supplied by a primary within the shell, but some sort of external device that heated the shell from the outside until it was glowing in the x-ray region (see thermal radiation). The power would be delivered by a then-unidentified pulsed power source he referred to using bomb terminology, the "primary".
The main advantage to this scheme is the efficiency of the fusion process at high densities. According to the Lawson criterion, the amount of energy needed to heat the D-T fuel to break-even conditions at ambient pressure is perhaps 100 times greater than the energy needed to compress it to a pressure that would deliver the same rate of fusion. So, in theory, the ICF approach would be dramatically more efficient in terms of gain. This can be understood by considering the energy losses in a conventional scenario where the fuel is slowly heated, as in the case of magnetic fusion energy; the rate of energy loss to the environment is based on the temperature difference between the fuel and its surroundings, which continues to increase as the fuel is heated. In the ICF case, the entire hohlraum is filled with high-temperature radiation, limiting losses.
In Germany.
Around the same time (in 1956) a meeting was organized at the Max Planck Institute in Germany by the fusion pioneer Carl Friedrich von Weizsäcker. At this meeting Friedwardt Winterberg proposed the non-fission ignition of a thermonuclear micro-explosion by a convergent shock wave driven with high explosives. Further reference about Winterberg's work in Germany on nuclear micro explosions (mininukes) is contained in a declassified report of the former East German Stasi (Staatsicherheitsdienst).
In 1964 Winterberg proposed that ignition could be achieved by an intense beam of microparticles accelerated to a velocity of 1000 km/s. And in 1968, he proposed to use intense electron and ion beams, generated by Marx generators, for the same purpose. The advantage of this proposal is that the generation of charged particle beams is not only less expensive than the generation of laser beams but also can entrap the charged fusion reaction products due to the strong self-magnetic beam field, drastically reducing the compression requirements for beam ignited cylindrical targets.
Early research.
Through the late 1950s, Nuckolls and collaborators at the Lawrence Livermore National Laboratory (LLNL) ran a number of computer simulations of the ICF concept. In early 1960 this produced a full simulation of the implosion of 1 mg of D-T fuel inside a dense shell. The simulation suggested that a 5 MJ power input to the hohlraum would produce 50 MJ of fusion output, a gain of 10. At the time the laser had not yet been invented, and a wide variety of possible drivers were considered, including pulsed power machines, charged particle accelerators, plasma guns, and hypervelocity pellet guns.
Through the year two key theoretical advances were made. New simulations considered the timing of the energy delivered in the pulse, known as "pulse shaping", leading to better implosion. Additionally, the shell was made much larger and thinner, forming a thin shell as opposed to an almost solid ball. These two changes dramatically increased the efficiency of the implosion, and thereby greatly lowered the energy required to compress it. Using these improvements, it was calculated that a driver of about 1 MJ would be needed, a five-fold improvement. Over the next two years several other theoretical advancements were proposed, notably Ray Kidder's development of an implosion system without a hohlraum, the so-called "direct drive" approach, and Stirling Colgate and Ron Zabawski's work on very small systems with as little as 1 μg of D-T fuel.
The introduction of the laser in 1960 at Hughes Research Laboratories in California appeared to present a perfect driver mechanism. Starting in 1962, Livermore's director John S. Foster, Jr. and Edward Teller began a small-scale laser study effort directed toward the ICF approach. Even at this early stage the suitability of the ICF system for weapons research was well understood, and the primary reason for its ability to gain funding. Over the next decade, LLNL made several small experimental devices for basic laser-plasma interaction studies.
Development begins.
In 1967 Kip Siegel started KMS Industries using the proceeds of the sale of his share of an earlier company, Conductron, a pioneer in holography. In the early 1970s he formed KMS Fusion to begin development of a laser-based ICF system. This development led to considerable opposition from the weapons labs, including LLNL, who put forth a variety of reasons that KMS should not be allowed to develop ICF in public. This opposition was funnelled through the Atomic Energy Commission, who demanded funding for their own efforts. Adding to the background noise were rumours of an aggressive Soviet ICF program, new higher-powered CO2 and glass lasers, the electron beam driver concept, and the 1970s energy crisis which added impetus to many energy projects.
In 1972 Nuckolls wrote an influential public paper in "Nature" introducing ICF and suggesting that testbed systems could be made to generate fusion with drivers in the kJ range, and high-gain systems with MJ drivers.
In spite of limited resources and numerous business problems, KMS Fusion successfully demonstrated fusion from the ICF process on 1 May 1974. However, this success was followed not long after by Siegel's death, and the end of KMS fusion about a year later, having run the company on Siegel's life insurance policy. By this point several weapons labs and universities had started their own programs, notably the solid-state lasers (Nd:glass lasers) at LLNL and the University of Rochester, and krypton fluoride excimer lasers systems at Los Alamos and the Naval Research Laboratory.
Although KMS's success led to a major development effort, the advances that followed were, and still are, hampered by the seemingly intractable problems that characterize fusion research in general.
High-Energy ICF.
High energy ICF experiments (multi-hundred joules per shot and greater experiments) began in earnest in the early-1970s, when lasers of the required energy and power were first designed. This was some time after the successful design of magnetic confinement fusion systems, and around the time of the particularly successful tokamak design that was introduced in the early '70s. Nevertheless, high funding for fusion research stimulated by the multiple energy crises during the mid to late 1970s produced rapid gains in performance, and inertial designs were soon reaching the same sort of "below break-even" conditions of the best magnetic systems.
LLNL was, in particular, very well funded and started a major laser fusion development program. Their Janus laser started operation in 1974, and validated the approach of using Nd:glass lasers to generate very high power devices. Focusing problems were explored in the Long path laser and Cyclops laser, which led to the larger Argus laser. None of these were intended to be practical ICF devices, but each one advanced the state of the art to the point where there was some confidence the basic approach was valid. At the time it was believed that making a much larger device of the Cyclops type could both compress and heat the ICF targets, leading to ignition in the "short term". This was a misconception based on extrapolation of the fusion yields seen from experiments utilizing the so-called "exploding pusher" type of fuel capsules. During the period spanning the years of the late '70s and early '80s the estimates for laser energy on target needed to achieve ignition doubled almost yearly as the various plasma instabilities and laser-plasma energy coupling loss modes were gradually understood. The realization that the simple exploding pusher target designs and mere few kilojoule (kJ) laser irradiation intensities would never scale to high gain fusion yields led to the effort to increase laser energies to the 100 kJ level in the UV and to the production of advanced ablator and cryogenic DT ice target designs.
Shiva and Nova.
One of the earliest serious and large scale attempts at an ICF driver design was the Shiva laser, a 20-beam neodymium doped glass laser system built at the Lawrence Livermore National Laboratory (LLNL) that started operation in 1978. Shiva was a "proof of concept" design intended to demonstrate compression of fusion fuel capsules to many times the liquid density of hydrogen. In this, Shiva succeeded and compressed its pellets to 100 times the liquid density of deuterium. However, due to the laser's strong coupling with hot electrons, premature heating of the dense plasma (ions) was problematic and fusion yields were low. This failure by Shiva to efficiently heat the compressed plasma pointed to the use of optical frequency multipliers as a solution which would frequency triple the infrared light from the laser into the ultraviolet at 351 nm. Newly discovered schemes to efficiently frequency triple high intensity laser light discovered at the Laboratory for Laser Energetics in 1980 enabled this method of target irradiation to be experimented with in the 24 beam OMEGA laser and the NOVETTE laser, which was followed by the Nova laser design with 10 times the energy of Shiva, the first design with the specific goal of reaching ignition conditions.
Nova also failed in its goal of achieving ignition, this time due to severe variation in laser intensity in its beams (and differences in intensity between beams) caused by filamentation which resulted in large non-uniformity in irradiation smoothness at the target and asymmetric implosion. The techniques pioneered earlier could not address these new issues. But again this failure led to a much greater understanding of the process of implosion, and the way forward again seemed clear, namely the increase in uniformity of irradiation, the reduction of hot-spots in the laser beams through beam smoothing techniques to reduce Rayleigh–Taylor instability imprinting on the target and increased laser energy on target by at least an order of magnitude. Funding for fusion research was severely constrained in the 80's, but Nova nevertheless successfully gathered enough information for a next generation machine.
National Ignition Facility.
The resulting design, now known as the National Ignition Facility, started construction at LLNL in 1997. NIF's main objective will be to operate as the flagship experimental device of the so-called nuclear stewardship program, supporting LLNLs traditional bomb-making role. Completed in March 2009, NIF has now conducted experiments using all 192 beams, including experiments that set new records for power delivery by a laser.
The first credible attempts at ignition were initially scheduled for 2010, but ignition was not achieved as of September 30, 2012. As of October 7, 2013, the facility is understood to have achieved an important milestone towards commercialization of fusion, namely, for the first time a fuel capsule gave off more energy than was applied to it. This is still a long way from satisfying the Lawson criterion, but is a major step forward.
Fast ignition.
A more recent development is the concept of "fast ignition," which may offer a way to directly heat the high density fuel after compression, thus decoupling the heating and compression phases of the implosion. In this approach the target is first compressed "normally" using a driver laser system, and then when the implosion reaches maximum density (at the stagnation point or "bang time"), a second ultra-short pulse ultra-high power petawatt (PW) laser delivers a single pulse focused on one side of the core, dramatically heating it and hopefully starting fusion ignition. The two types of fast ignition are the "plasma bore-through" method and the "cone-in-shell" method. In the first method the petawatt laser is simply expected to bore straight through the outer plasma of an imploding capsule and to impinge on and heat the dense core, whereas in the cone-in-shell method, the capsule is mounted on the end of a small high-z (high atomic number) cone such that the tip of the cone projects into the core of the capsule. In this second method, when the capsule is imploded, the petawatt has a clear view straight to the high density core and does not have to waste energy boring through a 'corona' plasma; however, the presence of the cone affects the implosion process in significant ways that are not fully understood. Several projects are currently underway to explore the fast ignition approach, including upgrades to the OMEGA laser at the University of Rochester, the GEKKO XII device in Japan, and an entirely new £500 million facility, known as HiPER, proposed for construction in the European Union. If successful, the fast ignition approach could dramatically lower the total amount of energy needed to be delivered to the target; whereas NIF uses UV beams of 2 MJ, HiPER's driver is 200 kJ and heater 70 kJ, yet the predicted fusion gains are nevertheless even higher than on NIF.
Other projects.
Laser Mégajoule, the French project, has seen its first experimental line achieved in 2002, and was finally completed in 2014, but has not yet began operations.
Using a different approach entirely is the "z"-pinch device. "Z"-pinch uses massive amounts of electric current which is switched into a cylinder comprising many of extremely fine wires. The wires vaporize to form an electrically conductive plasma that carries a very high current; the resulting circumferential magnetic field squeezes the plasma cylinder, imploding it and thereby generating a high-power x-ray pulse that can be used to drive the implosion of a fuel capsule. Challenges to this approach include relatively low drive temperatures, resulting in slow implosion velocities and potentially large instability growth, and preheat caused by high-energy x-rays.
Most recently, Winterberg has proposed the ignition of a deuterium microexplosion, with a gigavolt super-Marx generator, which is a Marx generator driven by up to 100 ordinary Marx generators.
As an energy source.
Practical power plants built using ICF have been studied since the late 1970s when ICF experiments were beginning to ramp up to higher powers; they are known as inertial fusion energy, or IFE plants. These devices would deliver a successive stream of targets to the reaction chamber, several a second typically, and capture the resulting heat and neutron radiation from their implosion and fusion to drive a conventional steam turbine.
Technical challenges.
IFE faces continued technical challenges in reaching the conditions needed for ignition. But even if these were all to be solved, there are a significant number of practical problems that seem just as difficult to overcome. Laser-driven systems were initially believed to be able to generate commercially useful amounts of energy. However, as estimates of the energy required to reach ignition grew dramatically during the 1970s and '80s, these hopes were abandoned. Given the low efficiency of the laser amplification process (about 1 to 1.5%), and the losses in generation (steam-driven turbine systems are typically about 35% efficient), fusion gains would have to be on the order of 350 just to energetically break even. These sorts of gains appeared to be impossible to generate, and ICF work turned primarily to weapons research.
With the recent introduction of fast ignition and similar approaches, things have changed dramatically. In this approach gains of 100 are predicted in the first experimental device, HiPER. Given a gain of about 100 and a laser efficiency of about 1%, HiPER produces about the same amount of "fusion" energy as electrical energy was needed to create it. It also appears that an order of magnitude improvement in laser efficiency may be possible through the use of newer designs that replace the flash lamps with laser diodes that are tuned to produce most of their energy in a frequency range that is strongly absorbed. Initial experimental devices offer efficiencies of about 10%, and it is suggested that 20% is a real possibility with some additional development.
With "classical" devices like NIF about 330 MJ of electrical power are used to produce the driver beams, producing an expected yield of about 20 MJ, with the maximum credible yield of 45 MJ. Using the same sorts of numbers in a reactor combining fast ignition with newer lasers would offer dramatically improved performance. HiPER requires about 270 kJ of laser energy, so assuming a first-generation diode laser driver at 10% the reactor would require about 3 MJ of electrical power. This is expected to produce about 30 MJ of fusion power. Even a very poor conversion to electrical energy appears to offer real-world power output, and incremental improvements in yield and laser efficiency appear to be able to offer a commercially useful output.
Practical problems.
ICF systems face some of the same secondary power extraction problems as magnetic systems in generating useful power from their reactions. One of the primary concerns is how to successfully remove heat from the reaction chamber without interfering with the targets and driver beams. Another serious concern is that the huge number of neutrons released in the fusion reactions react with the plant, causing them to become intensely radioactive themselves, as well as mechanically weakening metals. Fusion plants built of conventional metals like steel would have a fairly short lifetime and the core containment vessels will have to be replaced frequently.
One current concept in dealing with both of these problems, as shown in the HYLIFE-II baseline design, is to use a "waterfall" of FLiBe, a molten mix of fluoride salts of lithium and beryllium, which both protect the chamber from neutrons and carry away heat. The FLiBe is then passed into a heat exchanger where it heats water for use in the turbines. Another, Sombrero, uses a reaction chamber built of Carbon-fiber-reinforced polymer which has a very low neutron cross section. Cooling is provided by a molten ceramic, chosen because of its ability to stop the neutrons from traveling any further, while at the same time being an efficient heat transfer agent.
Economic viability.
Even if these technical advances solve the considerable problems in IFE, another factor working against IFE is the cost of the fuel. Even as Nuckolls was developing his earliest detailed calculations on the idea, co-workers pointed this out: if an IFE machine produces 50 MJ of fusion energy, one might expect that a shot could produce perhaps 10 MJ of power for export. Converted to better known units, this is the equivalent of 2.8 kWh of electrical power. Wholesale rates for electrical power on the grid were about 0.3 cents/kWh at the time, which meant the monetary value of the shot was perhaps one cent. In the intervening 50 years the price of power has remained about even with the rate of inflation, and the rate in 2012 in Ontario, Canada was about 2.8 cents/kWh
Thus, in order for an IFE plant to be economically viable, fuel shots would have to cost considerably less than ten cents in year 2012 dollars. At the time this objection was first noted, Nuckolls suggested using liquid droplets sprayed into the hohlraum from an eye-dropper-like apparatus. Given the ever-increasing demands for higher uniformity of the targets, this approach does not appear practical, as even the inner ablator and fuel itself currently costs several orders of magnitude more than this. Moreover, Nuckolls' solution had the fuel dropped into a fixed hohlraum that would be re-used in a continual cycle, but at current energy levels the hohlraum is destroyed with every shot.
Direct-drive systems avoid the use of a hohlraum and thereby may be less expensive in fuel terms. However, these systems still require an ablator, and the accuracy and geometrical considerations are even more important. They are also far less developed than the indirect-drive systems, and face considerably more technical problems in terms of implosion physics. Currently there is no strong consensus whether a direct-drive system would actually be less expensive to operate.
Projected development.
The various phases of such a project are the following, the sequence of inertial confinement fusion development follows much the same outline:
At the moment, according to the available data, inertial confinement fusion experiments have not gone beyond the first phase, although Nova and others have repeatedly demonstrated operation within this realm.
In the short term a number of new systems are expected to reach the second stage.
For a true industrial demonstration, further work is required. In particular, the laser systems need to be able to run at high operating frequencies, perhaps one to ten times a second. Most of the laser systems mentioned in this article have trouble operating even as much as once a day. Parts of the HiPER budget are dedicated to research in this direction as well. Because they convert electricity into laser light with much higher efficiency, diode lasers also run cooler, which in turn allows them to be operated at much higher frequencies. HiPER is currently studying devices that operate at 1 MJ at 1 Hz, or alternately 100 kJ at 10 Hz.
Nuclear weapons program.
The very hot and dense conditions encountered during an Inertial Confinement Fusion experiment are similar to those created in a thermonuclear weapon, and have applications to the nuclear weapons program. ICF experiments might be used, for example, to help determine how warhead performance will degrade as it ages, or as part of a program of designing new weapons. Retaining knowledge and corporate expertise in the nuclear weapons program is another motivation for pursuing ICF. Funding for the NIF in the United States is sourced from the 'Nuclear Weapons Stockpile Stewardship' program, and the goals of the program are oriented accordingly. It has been argued that some aspects of ICF research may violate the Comprehensive Test Ban Treaty or the Nuclear Non-Proliferation Treaty. In the long term, despite the formidable technical hurdles, ICF research might potentially lead to the creation of a "pure fusion weapon".
Neutron source.
Inertial confinement fusion has the potential to produce orders of magnitude more neutrons than spallation. Neutrons are capable of locating hydrogen atoms in molecules, resolving atomic thermal motion and studying collective excitations of photons more effectively than X-rays. Neutron scattering studies of molecular structures could resolve problems associated with protein folding, diffusion through membranes, proton transfer mechanisms, dynamics of molecular motors, etc. by modulating thermal neutrons into beams of slow neutrons. In combination with fissionable materials, neutrons produced by ICF can potentially be used in Hybrid Nuclear Fusion designs to produce electric power.

</doc>
<doc id="40324" url="https://en.wikipedia.org/wiki?curid=40324" title="Quadratic programming">
Quadratic programming

Quadratic programming (QP) is a special type of mathematical optimization problem. It is the problem of optimizing (minimizing or maximizing) a quadratic function of several variables subject to linear constraints on these variables.
Problem formulation.
The quadratic programming problem with variables and constraints can be formulated as follows.
Given:
the objective of quadratic programming is to find an -dimensional vector , that
where denotes the vector transpose of formula_1. The notation means that every entry of the vector is less than or equal to the corresponding entry of the vector .
A related programming problem, quadratically constrained quadratic programming, can be posed by adding quadratic constraints on the variables.
Solution methods.
For general problems a variety of methods are commonly used, including
Convex quadratic programming is a special case of the more general field of convex optimization.
Equality constraints.
Quadratic programming is particularly simple when there are only equality constraints; specifically, the problem is linear. By using Lagrange multipliers and seeking the extremum of the Lagrangian, it may be readily shown that the solution to the equality constrained problem 
is given by the linear system
where formula_5 is a set of Lagrange multipliers which come out of the solution alongside formula_6.
The easiest means of approaching this system is direct solution (for example, LU factorization), which for small problems is very practical. For large problems, the system poses some unusual difficulties, most notably that problem is never positive definite (even if formula_7 is), making it potentially very difficult to find a good numeric approach, and there are many approaches to choose from dependent on the problem.
If the constraints don't couple the variables too tightly, a relatively simple attack is to change the variables so that constraints are unconditionally satisfied. For example, suppose formula_8 (generalizing to nonzero is straightforward). Looking at the constraint equations:
introduce a new variable formula_10 defined by
where formula_10 has dimension of formula_6 minus the number of constraints. Then
and if formula_15 is chosen so that formula_16 the constraint equation will be always satisfied. Finding such formula_15 entails finding the null space of formula_18, which is more or less simple depending on the structure of formula_18. Substituting into the quadratic form gives an unconstrained minimization problem:
the solution of which is given by:
Under certain conditions on formula_7, the reduced matrix formula_23 will be positive definite. It's possible to write a variation on the conjugate gradient method which avoids the explicit calculation of formula_15.
Lagrangian duality.
The Lagrangian dual of a QP is also a QP. To see that let us focus on the case where formula_25 and Q is positive definite. We write the Lagrangian function as 
Defining the (Lagrangian) dual function formula_27, defined as formula_28, we find an infimum of formula_29, using formula_30
formula_31
hence the dual function is 
hence the Lagrangian dual of the QP is
maximize: formula_33
subject to: formula_34.
Besides the Lagrangian duality theory, there are other duality pairings (e.g. Wolfe, etc.).
Complexity.
For positive definite "Q", the ellipsoid method solves the problem in polynomial time. If, on the other hand, "Q" is indefinite, then the problem is NP-hard. In fact, even if "Q" has only one negative eigenvalue, the problem is NP-hard.

</doc>
<doc id="40325" url="https://en.wikipedia.org/wiki?curid=40325" title="Positive semidefinite">
Positive semidefinite

In mathematics, positive semidefinite may refer to:

</doc>
<doc id="40326" url="https://en.wikipedia.org/wiki?curid=40326" title="Positive-definite matrix">
Positive-definite matrix

In linear algebra, a symmetric real matrix formula_1 is said to be positive definite if the scalar formula_2 is positive for every non-zero column vector formula_3 of formula_4 real numbers. Here formula_5 denotes the transpose of formula_3.
More generally, an Hermitian matrix formula_1 is said to be positive definite if the scalar formula_8 is real and positive for all non-zero column vectors formula_3 of formula_4 complex numbers. Here formula_11 denotes the conjugate transpose of formula_3.
The negative definite, positive semi-definite, and negative semi-definite matrices are defined in the same way, except that the expression formula_2 or formula_8 is required to be always negative, non-negative, and non-positive, respectively.
Positive definite matrices are closely related to positive-definite symmetric bilinear forms (or sesquilinear forms in the complex case), and to inner products of vector spaces.
Some authors use more general definitions of "positive definite" that include some non-symmetric real matrices, or non-Hermitian complex ones.
Examples.
The examples "M" and "N" above show that a matrix in which some elements are negative may still be positive-definite, and conversely a matrix whose entries are all positive may not be positive definite.
Connections.
A general purely quadratic real function "f"("z") on "n" real variables "z"1, ..., "zn" can always be written as "z"T"Mz" where "z" is the column vector with those variables, and "M" is a symmetric real matrix. Therefore, the matrix being positive definite means that "f" has a unique minimum (zero) when "z" is zero, and is strictly positive for any other "z".
More generally, a twice-differentiable real function "f" on "n" real variables has an isolated local minimum at arguments "z"1, ..., "zn" if its gradient is zero and its Hessian (the matrix of all second derivatives) is positive semi-definite at that point. Similar statements can be made for negative definite and semi-definite matrices.
In statistics, the covariance matrix of a multivariate probability distribution is always positive semi-definite; and it is positive definite unless one variable is an exact linear combination of the others. Conversely, every positive semi-definite matrix is the covariance matrix of some multivariate distribution.
Characterizations.
Let "M" be an "n" × "n" Hermitian matrix. The following properties are equivalent to "M" being positive definite:
Quadratic forms.
The (purely) quadratic form associated with a real matrix "M" is the function "Q" : R"n" → R such that "Q"("x") = "x"T"Mx" for all "x". It turns out that the matrix "M" is positive definite if and only if it is symmetric and its quadratic form is a strictly convex function.
More generally, any quadratic function from R"n" to R can be written as "x"T"Mx" + "x"T"b" + "c" where "M" is a symmetric "n" × "n" matrix, "b" is a real "n"-vector, and "c" a real constant. This quadratic function is strictly convex when "M" is positive definite, and hence has a unique finite global minimum, if and only if "M" is positive definite. For this reason, positive definite matrices play an important role in optimization problems.
Simultaneous diagonalization.
A symmetric matrix and another symmetric and positive-definite matrix can be simultaneously diagonalized, although not necessarily via a similarity transformation. This result does not extend to the case of three or more matrices. In this section we write for the real case. Extension to the complex case is immediate.
Let "M" be a symmetric and "N" a symmetric and positive-definite matrix. Write the generalized eigenvalue equation as ("M"−λ"N")"x" = 0 where we impose that "x" be normalized, i.e. "x"T"Nx" = 1. Now we use Cholesky decomposition to write the inverse of "N" as "Q"T"Q". Multiplying by "Q" and letting "x" → "Q"T"y", we get "Q"("M"−λ"N")"Q"T"y" = 0, which can be rewritten as ("QMQ"T)"y" = λ"y" where "y"T"y" = 1. Manipulation now yields "MX" = "NX"Λ where "X" is a matrix having as columns the generalized eigenvectors and Λ is a diagonal matrix with the generalized eigenvalues. Now premultiplication with "X"T gives the final result: "X"T"MX" = Λ and "X"T"NX" = "I", but note that this is no longer an orthogonal diagonalization with respect to the inner product where "y"T"y" = 1. In fact, we diagonalized "M" with respect to the inner product induced by "N".
Note that this result does not contradict what is said on simultaneous diagonalization in the article Diagonalizable matrix, which refers to simultaneous diagonalization by a similarity transformation. Our result here is more akin to a simultaneous diagonalization of two quadratic forms, and is useful for optimization of one form under conditions on the other. For this result see Horn&Johnson, 1985, page 218 and following.
Negative-definite, semidefinite and indefinite matrices.
A Hermitian matrix is negative-definite, negative-semidefinite, or positive-semidefinite if and only if all of its eigenvalues are negative, non-positive, or non-negative, respectively.
Negative-definite.
The Hermitian matrix "M" is said to be "negative-definite" if
for all non-zero "x" in C"n" (or, all non-zero "x" in R"n" for the real matrix), where "x*" is the conjugate transpose of "x".
A matrix is negative definite if its "k-"th order leading principal minor is negative when "k" is odd, and positive when "k" is even.
Positive-semidefinite.
M is called "positive-semidefinite" (or sometimes "nonnegative-definite") if
for all "x" in C"n" (or, all "x" in R"n" for the real matrix).
A matrix "M" is positive-semidefinite if and only if it arises as the Gram matrix of some set of vectors. In contrast to the positive-definite case, these vectors need not be linearly independent.
For any matrix "A", the matrix "A*A" is positive semidefinite, and rank("A") = rank("A*A"). 
Conversely, any Hermitian positive semi-definite matrix "M" can be written as "M" = "LL*", where "L" is lower triangular; this is the Cholesky decomposition. If "M" is not positive definite, then some of the diagonal elements of "L" may be zero.
A Hermitian matrix is positive semidefinite if and only if all of its principal minors are nonnegative. It is however not enough to consider the leading principal minors only, as is checked on the diagonal matrix with entries 0 and -1.
Negative-semidefinite.
It is called "negative-semidefinite" if
for all "x" in C"n" (or, all "x" in R"n" for the real matrix).
Indefinite.
A Hermitian matrix which is neither positive definite, negative definite, positive-semidefinite, nor negative-semidefinite is called "indefinite". Indefinite matrices are also characterized by having both positive and negative eigenvalues.
Further properties.
If "M" is a Hermitian positive-semidefinite matrix, one sometimes writes "M" ≥ 0 and if "M" is positive-definite one writes "M" > 0. The notion comes from functional analysis where positive-semidefinite matrices define positive operators.
For arbitrary square matrices "M", "N" we write "M" ≥ "N" if "M" − "N" ≥ 0; i.e., "M" − "N" is positive semi-definite. This defines a partial ordering on the set of all square matrices. One can similarly define a strict partial ordering "M" > "N".
Block matrices.
A positive 2"n" × 2"n" matrix may also be defined by blocks:
where each block is "n" × "n". By applying the positivity condition, it immediately follows that "A" and "D" are hermitian, and "C" = "B*".
We have that "z*Mz" ≥ 0 for all complex "z", and in particular for "z" = ( "v", 0)T. Then
A similar argument can be applied to "D", and thus we conclude that both "A" and "D" must be positive definite matrices, as well.
Converse results can be proved with stronger conditions on the blocks, for instance using the Schur complement.
On the definition.
Consistency between real and complex definitions.
Since every real matrix is also a complex matrix, the definitions of "positive definite" for the two classes must agree.
For complex matrices, the most common definition says that ""M" is positive definite if and only if "z*Mz" is real and positive for all non-zero "complex" column vectors "z"". This condition implies that "M" is Hermitian, that is, its transpose is equal to its conjugate. To see this, consider the matrices "A" = ("M"+"M*")/2 and "B" = ("M"−"M*")/(2"i"), so that "M" = "A"+"iB" and "z*Mz" = "z*Az" + "iz*Bz". The matrices "A" and "B" are Hermitian, therefore "z*Az" and "z*Bz" are individually real. If "z*Mz" is real, then "z*Bz" must be zero for all "z". Then "B" is the zero matrix and "M" = "A", proving that "M" is Hermitian.
By this definition, a positive definite "real" matrix "M" is Hermitian, hence symmetric; and "z"T"Mz" is positive for all non-zero "real" column vectors "z". However the last condition alone is not sufficient for "M" to be positive definite. For example, if
then for any real vector "z" with entries "a" and "b" we have "z"T"Mz" = ("a"−"b")"a" + ("a"+"b")"b" = "a"2 + "b"2, which is always positive if "z" is not zero. However, if "z" is the complex vector with entries 1 and "i", one gets
which is not real. Therefore, "M" is not positive definite.
On the other hand, for a "symmetric" real matrix "M", the condition ""z"T"Mz" > 0 for all nonzero real vectors "z"" "does" imply that "M" is positive definite in the complex sense.
Extension for non symmetric matrices.
Some authors choose to say that a complex matrix "M" is positive definite if Re("z*Mz") > 0 for all non-zero complex vectors "z", where Re("c") denotes the real part of a complex number "c". This weaker definition encompasses some non-Hermitian complex matrices, including some non-symmetric real ones, such as formula_52.
Indeed, with this definition, a real matrix is positive definite if and only if "z"T"Mz" > 0 for all nonzero real vectors "z", even if "M" is not symmetric.
In general, we have Re("z*Mz") > 0 for all complex nonzero vectors "z" if and only if the Hermitian part ("M" + "M*")/2 of "M" is positive definite in the narrower sense. Similarly, we have "x"T"Mx" > 0 for all real nonzero vectors "x" if and only if the symmetric part ("M" + "M"T)/2 of "M" is positive definite in the narrower sense.
In summary, the distinguishing feature between the real and complex case is that, a bounded positive operator on a complex Hilbert space is necessarily Hermitian, or self adjoint. The general claim can be argued using the polarization identity. That is no longer true in the real case.

</doc>
<doc id="40330" url="https://en.wikipedia.org/wiki?curid=40330" title="Magnoliaceae">
Magnoliaceae

The Magnoliaceae are a flowering plant family, the magnolia family, in the order Magnoliales. It consists of two subfamilies:
Magnolioideae, of which "Magnolia" is the most well-known genus, and Liriodendroidae, a monogeneric subfamily, of which "Liriodendron" (tulip trees) is the only genus.
Unlike most angiosperms, whose flower parts are in whorls (rings), the Magnoliaceae have their stamens and pistils in spirals on a conical receptacle. This arrangement is found in some fossil plants and is believed to be a basal or early condition for angiosperms. The flowers also have parts not distinctly differentiated into sepals and petals, while angiosperms that evolved later tend to have distinctly differentiated sepals and petals. The poorly differentiated perianth parts that occupy both positions are known as tepals.
The family has about 219 species in seven genera, although some classification systems include all of subfamily Magnoioideae in genus "Magnolia". The family ranges across subtropical eastern North America, Mexico and Central America, the West Indies, tropical South America, southern and eastern India, Sri Lanka, Indochina, Malesia, China, Japan, and Korea.
Genera.
The genera in the family include "Lirianthe", "Liriodendron", "Magnolia", "Michelia", and "Yulania".
Description.
The monophyly of Magnoliaceae is supported by a number of shared morphological characters among the various genera in the family. Most have bisexual flowers (with the exception of "Kmeria" and some species of "Magnolia" section "Gynopodium"), showy, fragrant, radial, and with an elongated receptacle. Leaves are alternate, simple, and sometimes lobed. The inflorescence is a solitary, showy flower with indistinguishable petals and sepals. Sepals range from six to many; stamens are numerous and feature short filaments which are poorly differentiated from the anthers. Carpels are usually numerous, distinct, and on an elongated receptacle or torus. The fruit is an etario of follicles which usually become closely appressed as they mature and open along the abaxial surface. Seeds have a fleshy coat and color that ranges from red to orange (except "Liriodendron"). Magnoliaceae flowers are beetle pollinated, except for "Liriodendron", which is bee pollinated. The carpels of" Magnolia" flowers are especially thick to avoid damage by beetles that land, crawl, and feast on them. The seeds of Magnolioideae are bird dispersed, while the seeds of "Liriodendron" are wind dispersed.
Biogeography.
Due to its great age, the geographical distribution of the Magnoliaceae has become disjunct or fragmented as a result of major geologic events such as ice ages, continental drift, and mountain formation. This distribution pattern has isolated some species, while keeping others in close contact.
Extant species of the Magnoliaceae are widely distributed in temperate and tropical Asia from the Himalayas to Japan and southwest through Malaysia and New Guinea. Asia is home to about two-thirds of the species in Magnoliaceae, with the remainder of the family spread across the Americas with temperate species extending into southern Canada and tropical elements extending into Brazil and the West Indies.
Systematics.
Due to the family-wide morphological similarity, no consensus has yet emerged on the number of genera in the family. The development of DNA sequencing at the end of the 20th century had a profound impact on the research of phylogenetic relationships within the family.
The employment of ndhF and cpDNA sequences has refuted many of the traditionally accepted phylogenetic relationships within the Magnoliaceae. For example, the genera "Magnolia" and "Michelia" were shown to be paraphyletic when the remaining four genera of the Magnolioideae are split out. In fact, even many of the subgenera ("Magnolia" subg. "Magnolia", "Magnolia" subg. "Talauma") have been found to be paraphyletic. Although no completely resolved phylogeny for the family has yet been determined, these technological advances have allowed systematists to broadly circumscribe major lineages.
Economic significance.
As a whole, the Magnoliaceae are not an economically significant family. With the exception of ornamental cultivation, the economic significance of magnolias is generally confined to the use of wood from certain timber species and the use of bark and flowers from several species believed to possess medicinal qualities. The wood of the American tuliptree, "Liriodendron tulipifera" and the wood of the cucumbertree magnolia, "Magnolia acuminata", and, to a lesser degree, that of the Frasier magnolia, "Magnolia fraseri", are harvested and marketed collectively as "yellow poplar." This is a lightweight and exceptionally fine-grained wood, lending itself to precision woodworking for purposes such as pipe organ building.
Magnolias have a rich cultural tradition in China, where references to their healing qualities go back thousands of years. The Chinese have long used the bark of "Magnolia officinalis", a magnolia native to the mountains of China with large leaves and fragrant white flowers, as a remedy for cramps, abdominal pain, nausea, diarrhea, and indigestion. Certain magnolia flowers, such as the buds of "Magnolia liliiflora", have been used to treat chronic respiratory and sinus infections and lung congestion. Recently, magnolia bark has become incorporated into alternative medicine in the west, where tablets made from the bark of "M. officinalis" have been marketed as an aid for anxiety, allergies, asthma, and weight loss. Compounds found in magnolia bark might have antibacterial and antifungal properties, but no large-scale study on the health effects of magnolia bark or flowers has yet been conducted.

</doc>
<doc id="40331" url="https://en.wikipedia.org/wiki?curid=40331" title="Cannoli">
Cannoli

__NOTOC__
Cannoli (; ) is an Italian pastry dessert of the Sicily region. The singular is "cannolo" (; in the Sicilian language "cannolu"), meaning "little tube", with the etymology stemming from the Arabic "Qanawat". Cannoli originated in Sicily and are a staple of Sicilian cuisine. They are also popular in Italian-American cuisine. In Italy, they are commonly known as "cannoli siciliani", "Sicilian cannoli".
Cannoli consist of tube-shaped shells of fried pastry dough, filled with a sweet, creamy filling usually containing ricotta. They range in size from "cannulicchi", no bigger than a finger, to the fist-sized proportions typically found south of Palermo, Sicily, in Piana degli Albanesi.
History.
Cannoli have been traced to the Arabs during the Emirate of Sicily, with a possible origin for the word and recipe deriving directly from "qanawāt". These were deep fried dough tubes filled with various sweets, which were a popular pastry across the Islamic world at the time, from Al-Andalus to Iraq.
Cannoli come from the Palermo and Messina areas and were historically prepared as a treat during Carnevale season, possibly as a fertility symbol; one legend assigns their origin to the harem of Caltanissetta. The dessert eventually became a year-round staple throughout Italy.
Variants.
The cannoli sold in Italian-American bakeries today usually still contain ricotta, but mascarpone is a less common alternative. Rarely, the filling is a simple custard of sugar, milk, and cornstarch. These changes became standardized when Italians who immigrated to the United States in the early 1900s and discovered limited availability of certain ingredients.
The cream is often flavored with vanilla or orange flower water and a small amount of cinnamon. Chopped pistachios, semi-sweet chocolate pieces, and candied citrus peel or cherries are often still included, dotting the open ends of the pastries.
In some Italian-American families a variant using pizzelle cookies as a shell instead of fried dough has been popular since the 1930s. This variant comes from the regions of Lazio and Abruzzo close to the Monte Cassino which is the area attributed to the development of the pizzelle. In this variant, the cookies while still hot from the press are wrapped around a dowel or other handy round item like a rolling pin, to form the tube and allowed to cool at which point the cookie hardens. This is then filled with a sweetened ricotta filling.

</doc>
<doc id="40333" url="https://en.wikipedia.org/wiki?curid=40333" title="Magnolia">
Magnolia

Magnolia is a large genus of about 210 flowering plant species in the subfamily Magnolioideae of the family Magnoliaceae. It is named after French botanist Pierre Magnol.
"Magnolia" is an ancient genus. Appearing before bees did, the flowers are theorized to have evolved to encourage pollination by beetles. To avoid damage from pollinating beetles, the carpels of "Magnolia" flowers are extremely tough. Fossilised specimens of "M. acuminata" have been found dating to 20 million years ago, and of plants identifiably belonging to the Magnoliaceae date to 95 million years ago. Another aspect of "Magnolia" considered to represent an ancestral state is that the flower bud is enclosed in a bract rather than in sepals; the perianth parts are undifferentiated and called tepals rather than distinct sepals and petals. "Magnolia" shares the tepal characteristic with several other flowering plants near the base of the flowering plant lineage such as "Amborella" and "Nymphaea" (as well as with many more recently derived plants such as "Lilium").
The natural range of "Magnolia" species is a disjunct distribution, with a main centre in east and southeast Asia and a secondary centre in eastern North America, Central America, the West Indies, and some species in South America.
Description.
As with all Magnoliaceae, the perianth is undifferentiated, with 9–15 tepals in 3 or more whorls. The flowers are bisexual with numerous adnate carpels and stamens are arranged in a spiral fashion on the elongated receptacle. The fruit dehisces along their dorsal sutures. The pollen is monocolpate, and the embryo development is of the Polygonium type.(Kapil 1964)(Xu and Rudall 2006)
Taxonomy.
History.
Early.
The name "Magnolia" first appeared in 1703 in the "Genera" of Charles Plumier (1646–1704), for a flowering tree from the island of Martinique ("talauma"). English botanist William Sherard, who studied botany in Paris under Joseph Pitton de Tournefort, a pupil of Magnol, was most probably the first after Plumier to adopt the genus name "Magnolia". He was at least responsible for the taxonomic part of Johann Jacob Dillenius's "Hortus Elthamensis" and of Mark Catesby's "Natural History of Carolina, Florida and the Bahama Islands". These were the first works after Plumier's "Genera" that used the name "Magnolia", this time for some species of flowering trees from temperate North America. The species that Plumier originally named "Magnolia" was later described as "Annona dodecapetala" by Lamarck, and has since been named "Magnolia plumieri" and "Talauma plumieri" (and still a number of other names) but is now known as "Magnolia dodecapetala".
Carl Linnaeus, who was familiar with Plumier's "Genera", adopted the genus name "Magnolia" in 1735 in his first edition of "Systema Naturae", without a description, but with a reference to Plumier's work. In 1753, he took up Plumier's "Magnolia" in the first edition of "Species Plantarum". There he described a monotypic genus, with the sole species being "Magnolia virginiana". Since Linnaeus never saw a herbarium specimen (if there ever was one) of Plumier's "Magnolia" and had only his description and a rather poor picture at hand, he must have taken it for the same plant which was described by Catesby in his 1730 "Natural History of Carolina". He placed it in the synonymy of "Magnolia virginiana" var. "fœtida", the taxon now known as "Magnolia grandiflora". Under "Magnolia virginiana" Linnaeus described five varieties ("glauca", "fœtida", "grisea", "tripetala", and "acuminata"). In the tenth edition of "Systema Naturae" (1759), he merged "grisea" with "glauca", and raised the four remaining varieties to species status.
By the end of the 18th century, botanists and plant hunters exploring Asia began to name and describe the "Magnolia" species from China and Japan. The first Asiatic species to be described by western botanists were "Magnolia denudata" and "Magnolia liliiflora", and "Magnolia coco" and "Magnolia figo". Soon after that, in 1794, Carl Peter Thunberg collected and described "Magnolia obovata" from Japan and at roughly the same time "Magnolia kobus" was also first collected.
Recent.
With the number of species increasing, the genus was divided into the two subgenera "Magnolia" and "Yulania". "Magnolia" contains the American evergreen species "M. grandiflora", which is of horticultural importance, especially in the southeastern United States, and "M. virginiana", the type species. "Yulania" contains several deciduous Asiatic species, such as "M. denudata" and "M. kobus", which have become horticulturally important in their own right and as parents in hybrids. Classified in "Yulania", is also the American deciduous "M. acuminata" (cucumber tree), which has recently attained greater status as the parent responsible for the yellow flower colour in many new hybrids.
Relations in the family Magnoliaceae have been puzzling taxonomists for a long time. Because the family is quite old and has survived many geological events (such as ice ages, mountain formation, and continental drift), its distribution has become scattered. Some species or groups of species have been isolated for a long time, while others could stay in close contact. To create divisions in the family (or even within the genus "Magnolia"), solely based upon morphological characters, has proven to be a nearly impossible task.
Phylogenetic era.
By the end of the 20th century, DNA sequencing had become available as a method of large-scale research on phylogenetic relationships. Several studies, including studies on many species in the family Magnoliaceae, were carried out to investigate relationships. What these studies all revealed was that genus "Michelia" and "Magnolia" subgenus "Yulania" were far more closely allied to each other than either one of them was to "Magnolia" subgenus "Magnolia". These phylogenetic studies were supported by morphological data.
As nomenclature is supposed to reflect relationships, the situation with the species names in "Michelia" and "Magnolia" subgenus "Yulania" was undesirable. Taxonomically, three choices are available: 1 to join "Michelia" and "Yulania" species in a common genus, not being "Magnolia" (for which the name "Michelia" has priority), 2 to raise subgenus "Yulania" to generic rank, leaving "Michelia" names and subgenus "Magnolia" names untouched, or 3 to join "Michelia" with genus "Magnolia" into genus "Magnolia" s.l. (a big genus). "Magnolia" subgenus "Magnolia" cannot be renamed because it contains "M. virginiana", the type species of the genus and of the family.
Not many "Michelia" species have so far become horticulturally or economically important, apart for their wood. Both subgenus "Magnolia" and subgenus "Yulania" include species of major horticultural importance, and a change of name would be very undesirable for many people, especially in the horticultural branch. In Europe, "Magnolia" even is more or less a synonym for "Yulania", since most of the cultivated species on this continent have "Magnolia (Yulania) denudata" as one of their parents. Most taxonomists who acknowledge close relations between "Yulania" and "Michelia" therefore support the third option and join "Michelia" with "Magnolia".
The same goes, "mutatis mutandis", for the (former) genera "Talauma" and "Dugandiodendron", which are then placed in subgenus "Magnolia", and genus "Manglietia", which could be joined with subgenus "Magnolia" or may even earn the status of an extra subgenus. "Elmerrillia" seems to be closely related to "Michelia" and "Yulania", in which case it will most likely be treated in the same way as "Michelia" is now. The precise nomenclatural status of small or monospecific genera like "Kmeria", "Parakmeria", "Pachylarnax", "Manglietiastrum", "Aromadendron", "Woonyoungia", "Alcimandra", "Paramichelia" and "Tsoongiodendron" remains uncertain. Taxonomists who merge "Michelia" into "Magnolia" tend to merge these small genera into "Magnolia" s.l. as well. Botanists do not yet agree on whether to recognize a big "Magnolia" genus or the different small genera. For example, "Flora of China" offers two choices: a large "Magnolia" which includes about 300 species, everything in the Magnoliaceae except "Liriodendron" (tulip tree), or 16 different genera, some of them recently split out or re-recognized, each of which contains up to 50 species. The western co-author favors the big "Magnolia" genus, whereas the Chinese recognize the different small genera.
Subdivision.
Species of Magnolia are most commonly listed under three subgenera, 12 sections, and 13 subsections, such as that used here, following the classification of the Magnolia Society. It does not represent the last word on the subclassification of the genus "Magnolia" (see above), as a clear consensus has not yet been reached. Each species entry follows this pattern: "Botanical name" Naming auth. - common name(s), if any (REGION FOUND)
The subdivision structure is as follows:
Subgenus "Magnolia".
Anthers open by splitting at the front facing the centre of the flower, deciduous or evergreen, flowers produced after the leaves.
Subgenus "Yulania".
Anthers open by splitting at the sides, deciduous, flowers mostly produced before leaves (except "M. acuminata")
Etymology.
Charles Plumier (1646–1704) described a flowering tree from the island of Martinique in his "Genera", giving it the name "Magnolia", after the French botanist Pierre Magnol.
Uses.
Horticultural uses.
In general, the "Magnolia" genus has attracted horticultural interest. Some, such as the star magnolia ("M. stellata") and the saucer magnolia ("Magnolia × soulangeana") flower quite early in the spring, before the leaves open. Others flower in late spring or early summer, including the sweetbay magnolia ("M. virginiana") and the southern magnolia ("M. grandiflora").
Hybridisation has been immensely successful in combining the best aspects of different species to give plants which flower at an earlier age than the parent species, as well as having more impressive flowers. One of the most popular garden magnolias, saucer magnolia ("Magnolia × soulangeana"), is a hybrid of "M. liliiflora" and "M. denudata".
In the eastern United States, five native species are frequently in cultivation: "M. acuminata" (as a shade tree), "M. grandiflora", "M. virginiana", "M. tripetala", and "M. macrophylla". The last two species must be planted where high winds are not a frequent problem because of the size of their leaves.
Traditional medicine.
The bark and flower buds of "M. officinalis" have long been used in traditional Chinese medicine, where they are known as "hou po" (厚朴). In Japan, "kōboku", "M. obovata", has been used in a similar manner.
Timber.
The cucumbertree, "M. acuminata", grows to large size and is harvested as a timber tree in northeastern US forests. Its wood is sold as "yellow poplar" along with that of the tuliptree, "Liriodendron tulipifera". The Fraser magnolia, "M. fraseri", also attains enough size sometimes to be harvested, as well.
Other uses.
In parts of Japan, the leaves of "M. obovata" are used for wrapping food and as cooking dishes.
Magnolias are used as food plants by the larvae of some Lepidoptera species, including the giant leopard moth.
Chemical compounds and bioeffects.
The aromatic bark contains magnolol, honokiol, 4-O-methylhonokiol, and obovatol. Magnolol and honokiol activate the nuclear receptor peroxisome proliferator-activated receptor gamma.
Culture.
Arts.
Visual arts.
The Canadian artist, Sarah Maloney, has created a series of sculptures of Magnolia flowers in bronze and steel, entitled "First Flowers", in which she draws our attention to the dual symbols of beginnings in the flower, as both an evolutionary archetype and also one of the first trees to flower in spring (see illustration).
See also.
List of AGM magnolias

</doc>
<doc id="40335" url="https://en.wikipedia.org/wiki?curid=40335" title="Salzburg">
Salzburg

Salzburg (; ; literally: "Salt Fortress") is the fourth-largest city in Austria and the capital of the federal state of Salzburg.
Salzburg's "Old Town" ("Altstadt") is internationally renowned for its baroque architecture and is one of the best-preserved city centers north of the Alps. It was listed as a UNESCO World Heritage Site in 1997. The city has three universities and a large population of students. Tourists also frequent the city to tour the city's historic center and the scenic Alpine surroundings.
Salzburg was the birthplace of 18th-century composer Wolfgang Amadeus Mozart. In the mid‑20th century, the city was the setting for the musical play and film "The Sound of Music".
History.
Antiquity to the High Middle Ages.
Traces of human settlements have been found in the area, dating to the Neolithic Age. The first settlements in Salzburg continuous with the present were apparently by the Celts around the 5th century BC.
Around 15 BC the separate settlements were merged into one city by the Roman Empire. At this time, the city was called "Juvavum" and was awarded the status of a Roman "municipium" in 45 AD. Juvavum developed into an important town of the Roman province of Noricum. After the collapse of the Norican frontier, Juvavum declined so sharply that by the late 7th century it nearly became a ruin.
The "Life of Saint Rupert" credits the 8th-century saint with the city's rebirth. When Theodo of Bavaria asked Rupert to become bishop c. 700, Rupert reconnoitered the river for the site of his basilica. Rupert chose Juvavum, ordained priests, and annexed the manor Piding. Rupert named the city "Salzburg". He traveled to evangelise among pagans.
The name Salzburg means "Salt Castle" (Latin: "Salis Burgium"). The name derives from the barges carrying salt on the Salzach River, which were subject to a toll in the 8th century and was customary for many communities and cities on European rivers. The Festung Hohensalzburg, the city's fortress, was built in 1077 by Archbishop Gebhard, who made it his residence. It was greatly expanded during the following centuries.
Independence.
Independence from Bavaria was secured in the late 14th century. Salzburg was the seat of the Archbishopric of Salzburg, a prince-bishopric of the Holy Roman Empire. As the reformation movement gained steam, riots broke out among peasants in the areas in and around Salzburg. The city was occupied during the German Peasants' War, and the archbishop had to flee to the safety of the fortress It was besieged for three months in 1525.
Eventually, tensions were quelled, and the independence of the city led to an increase in wealth and prosperity, culminating in the late 16th to 18th centuries under the Prince Archbishops Wolf Dietrich von Raitenau, Markus Sittikus, and Paris Lodron. It was in the 17th century that Italian architects (and Austrians who had studied the Baroque style) rebuilt the city center as it is today along with many palaces.
Modern era.
Religious conflict.
On 31 October 1731, the 214th anniversary of the 95 Theses, Archbishop Count Leopold Anton von Firmian signed an Edict of Expulsion, the "Emigrationspatent", directing all Protestant citizens to recant their non-Catholic beliefs. A total of 21,475 citizens refused to recant their beliefs and were expelled from Salzburg. Most of them accepted an offer by King Friedrich Wilhelm I of Prussia, traveling the length and breadth of Germany to their new homes in East Prussia. The rest settled in other Protestant states in Europe and the British colonies in America.
Illuminism.
In 1772-1803, under archbishop Hieronymus Graf von Colloredo, Salzburg was a centre of late Illuminism.
Electorate of Salzburg.
In 1803, the archbishopric was secularised by Emperor Napoleon; he transferred the territory to Ferdinando III of Tuscany, former Grand Duke of Tuscany, as the Electorate of Salzburg.
Austrian annexation of Salzburg.
In 1805, Salzburg was annexed to the Austrian Empire, along with the Berchtesgaden Provostry.
Salzburg under Bavarian rule.
In 1809, the territory of Salzburg was transferred to the Kingdom of Bavaria after Austria's defeat at Wagram.
Division of Salzburg and annexation by Austria and Bavaria.
At the Congress of Vienna in 1815, Salzburg was definitively returned to Austria, but without Rupertigau and Berchtesgaden, which remained with Bavaria. Salzburg was integrated into the Salzach province and Salzburgerland was ruled from Linz.
In 1850, Salzburg's status was once more restored as the capital of the Duchy of Salzburg, a crownland of the Austrian Empire. The city became part of Austria-Hungary in 1866 as the capital of a crownland into the Austrian Empire. The nostalgia of the Romantic Era led to increased tourism. In 1892, a funicular was installed to facilitate tourism to the fortress of Hohensalzburg
20th century.
First republic.
Following World War I and the dissolution of the Austro-Hungarian Empire; Salzburg, as the capital of one of the Austro-Hungarian territories, became part of the new German Austria. In 1918, it represented the residual German-speaking territories of the Austrian heartlands. This was replaced by the First Austrian Republic in 1919, after the Treaty of Versailles.
Annexation by German Third Reich.
The Anschluss (the occupation and annexation of Austria, including Salzburg, into German Third Reich) took place the 12 March 1938, one day before a scheduled referendum about Austria's independence. German troops moved to the city. Political opponents, Jewish citizens and other minorities were subsequently arrested and deported to concentration camps. The synagogue was destroyed. Later after Germany invaded the Soviet Union, several POW camps for prisoners from the Soviet Union and other enemy nations were organized in the city.
During the Nazi occupation, a Roma camp was built in Salzburg-Maxglan. It was an Arbeitserziehungslager (work 'education' camp), which provided slave labour to local industry. It also operated as a Zwischenlager (transit camp), holding Roma before their deportation to German extermination camps or ghettos in German-occupied territories in eastern Europe.
World War II.
Allied bombing destroyed 7,600 houses and killed 550 inhabitants. A total of 15 strikes destroyed 46 percent of the city's buildings especially around Salzburg train station. Although the town's bridges and the dome of the cathedral were destroyed, much of its Baroque architecture remained intact. As a result, it is one of the few remaining examples of a town of its style. American troops entered Salzburg on 5 May 1945.
In the city of Salzburg, there were several DP Camps following World War II. Among these were Riedenburg, Camp Herzl (Franz-Josefs-Kaserne), Camp Mülln, Bet Bialik, Bet Trumpeldor, and New Palestine. Salzburg was the centre of the American-occupied area in Austria.
Present day.
After World War II, Salzburg became the capital city of the State of Salzburg ("Land Salzburg"). On 27 January 2006, the 250th anniversary of the birth of Wolfgang Amadeus Mozart, all 35 churches of Salzburg rang their bells a little after 8:00 p.m. (local time) to celebrate the occasion. Major celebrations took place throughout the year.
Geography.
Salzburg is on the banks of the Salzach River, at the northern boundary of the Alps. The mountains to Salzburg's south contrast with the rolling plains to the north. The closest alpine peak, the 1,972‑metre-high Untersberg, is less than from the city centre. The "Altstadt", or "old town", is dominated by its baroque towers and churches and the massive "Festung Hohensalzburg". This area is surrounded by two smaller mountains, the "Mönchsberg" and "Kapuzinerberg", which offer green relief within the city. Salzburg is approximately east of Munich, northwest of Ljubljana, Slovenia, and west of Vienna.
Climate.
Salzburg is part of the temperate zone. The Köppen climate classification specifies the climate as humid continental (Dfb). Due to the location at the northern rim of the Alps, the amount of precipitation is comparatively high, mainly in the summer months. The specific drizzle is called "Schnürlregen" in the local dialect. In winter and spring, pronounced foehn winds regularly occur.
Population development.
Salzburg's official population significantly increased in 1935 when the city absorbed adjacent municipalities. After World War II, numerous refugees found a new home in the city. New residential space was constructed for American soldiers of the postwar occupation, and could be used for refugees when they left. Around 1950, Salzburg passed the mark of 100,000 citizens, and in 2006, it reached the mark of 150,000 citizens.
Architecture.
Romanesque and Gothic.
The Romanesque and Gothic churches, the monasteries and the early carcass houses dominated the medieval city for a long time. The Cathedral of Archbishop Conrad of Wittelsbach was the largest basilica north of the Alps. The choir of the , construction was began by Hans von Burghausen and completed by , is one of the most prestigious religious gothic constructions of southern Germany. At the end of the Gothic era the Collegiate church "Nonnberg", Margaret Chapel in St. Peter's Cemetery, the St. George's Chapel and the stately halls of the "Hoher Stock" in the Hohensalzburg Castle were constructed.
Renaissance and baroque.
Inspired by Vincenzo Scamozzi, Prince Archbishop Wolf Dietrich von Raitenau began transforming the medieval town to the architectural ideals of the late Renaissance. Plans for a massive cathedral by Scamozzi failed to materialize upon the fall of the archbishop. A second cathedral planned by Santino Solari rose as the first early Baroque church in Salzburg. It served as an example for many other churches in Southern Germany and Austria. and continued the rebuilding of the city with major projects such as Hellbrunn Palace, the prince archbishop's residence, the university buildings, fortifications, and many other buildings. Giovanni Antonio Daria managed by order of Prince Archbishop Guido von Thun the construction of the residential well. Giovanni , by order of the same archbishop, created the Erhard and the Kajetan church in the south of the town. The redesign of the city was completed with buildings designed by Johann Bernhard Fischer von Erlach, donated by Prince Archbishop Johann Ernst von Thun. After the era of Ernst von Thun the expansion of the city came to a halt, which is the reason why there are no churches built in the rococo style. Sigismund von Schrattenbach continued with the construction of "Sigmundstor" and the statue of holy Maria on the cathedral square. With the fall and division of the former "Fürsterzbistums Salzburg" (Archbishopric) to Upper Austria, Bavaria (Rupertigau) and Tyrol (Zillertal Matrei) began a long period of urban stagnancy. This era didn't end before the period of promoterism ("Gründerzeit") brought new life into urban development. The builder dynasty and filled major positions in shaping the city in this era.
Classical modernism and post-war modernism.
Buildings of classical modernism and in particular the post-war modernism are frequently encountered in Salzburg. Examples are the Zahnwurzen house (a house in the Linzergasse 22 in the right center of the old town), the "Lepi" (a public baths in "Leopoldskron") (built 1964) and the original 1957 constructed congress center of Salzburg, which got replaced by a new building in 2001. An important and famous example of architecture of this era is the 1960 opening of the Großes Festspielhaus by Clemens Holzmeister.
Contemporary architecture.
Adding contemporary architecture to Salzburg's old town without risking its UNESCO World Heritage status is problematic. Yet some new structures have been added: the Mozarteum at the baroque Mirabell garden (Architecture Robert Rechenauer), the 2001 Congress house (Architecture: Freemasons), the 2011 Unipark Nonntal (Architecture: Storch Ehlers partners), the 2001 "Makartsteg" bridge (Architecture: HALLE1), and the "Residential and studio house" of the architects and in the middle of Salzburg's old town (winner of the ). Other examples of contemporary architecture exist outside the old town: the Faculty of Science building (Universität Salzburg – Architecture ) built on the edge of free green space, the blob architecture of Red Bull Hangar‑7 (Architecture: Volkmar Burgstaller) at Salzburg Airport, home to Dietrich Mateschitz's Flying Bulls and the Europark shopping mall. (Architecture: Massimiliano Fuksas)
Districts.
Salzburg has twenty-four urban districts and three extra-urban populations.
Urban districts ("Stadtteile"):
Extra-urban populations ("Landschaftsräume"):
Main sights.
Salzburg is a tourist favorite, with the number of tourists outnumbering locals by a large margin in peak times. In addition to Mozart's birthplace noted above, other notable places include:
Old Town
Outside the Old Town
Greater Salzburg area
Education.
Salzburg is a centre of education and home to three universities, as well as several professional colleges and gymnasiums (high schools).
Transport.
The city is served by comprehensive rail connections, with frequent east-west trains serving Vienna, Munich, Innsbruck, and Zürich, including daily high-speed ICE services. The city acts as a hub for south-bound trains through the Alps into Italy.
Salzburg Airport has scheduled flights to European cities such as Frankfurt, Vienna, London, Rotterdam, Amsterdam, Brussels, Düsseldorf, and Zürich, as well as Hamburg and Dublin. In addition to these, there are numerous charter flights.
In the main city, there is the Salzburg trolleybus system and bus system with a total of more than 20 lines, and service every 10 minutes. Salzburg has an S-Bahn system with four Lines (S1, S2, S3, S11), trains depart from the main station every 30 minutes, and they are part of the ÖBB network. Suburb line number S1 reaches the world famous Silent Night chapel in Oberndorf in about 25 minutes.
Popular culture.
In the 1960s, the movie "The Sound of Music" used some locations in and around Salzburg and the state of Salzburg. The movie was based on the true story of Maria von Trapp who took up with an aristocratic family and fled the German Anschluss. Although the film is not particularly popular nor well known among Austrians, the town draws many visitors who wish to visit the filming locations, alone or on tours.
Salzburg is the setting for the Austrian crime series Stockinger.
In the 2010 film "Knight & Day", Salzburg serves as the backdrop for a large portion of the film.
Language.
Austrian German is widely written. Austro-Bavarian is the German dialect of this territory and widely spoken.
Sports.
Football.
The former SV Austria Salzburg reached the UEFA Cup final in 1994. On 6 April 2005 Red Bull bought the club and changed its name into FC Red Bull Salzburg. The home stadium of Red Bull Salzburg is the Wals Siezenheim Stadium in a suburb in the agglomeration of Salzburg and was one of the venues for the 2008 European Football Championship. The FC Red Bull Salzburg plays in the Bundesliga.
After Red Bull had bought the SV Austria Salzburg and changed its name and team colors, some supporters of the club decided to leave and form a new club with the old name and old colors, wanting to preserve the traditions of their club. The reformed SV Austria Salzburg was founded in 2005 and currently plays in the Erste Liga, only one tier below the Bundesliga.
Ice hockey.
Red Bull also sponsors the local ice hockey team, the EC Salzburg Red Bulls. The team plays in the Erste Bank Eishockey Liga, an Austria-headquartered crossborder league featuring the best teams from Austria, Hungary, Slovenia and Italy, as well as one Czech team.
Other sports.
Salzburg was a candidate city for the 2010 & 2014 Winter Olympics, but lost to Vancouver and Sochi respectively.
International relations.
Twin towns—sister cities.
Salzburg is twinned with:

</doc>
